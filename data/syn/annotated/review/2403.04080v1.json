{
    "title": "Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis",
    "abstract": "This paper presents a comprehensive survey of research works on the topic of form understanding in the context of scanned documents. We delve into recent advancements and breakthroughs in the field, highlighting the significance of language models and transformers in solving this challenging task. Our research methodology involves an in-depth analysis of popular documents and forms of understanding of trends over the last decade, enabling us to offer valuable insights into the evolution of this domain. Focusing on cutting-edge models, we showcase how transformers have propelled the field forward, revolutionizing form-understanding techniques. Our exploration includes an extensive examination of state-of-the-art language models designed to effectively tackle the complexities of noisy scanned documents. Furthermore, we present an overview of the latest and most relevant datasets, which serve as essential benchmarks for evaluating the performance of selected models. By comparing and contrasting the capabilities of these models, we aim to provide researchers and practitioners with useful guidance in choosing the most suitable solutions for their specific form understanding tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Digitization has become ubiquitous in various aspects of our lives, where physical objects like music discs and analog documents are being replaced with digital, machine-readable counterparts. This transformation enables downstream applications and historical preservation through efficient storage in databases [1  ###reference_b1###, 2  ###reference_b2###, 3  ###reference_b3###].\nAmong the crucial tasks in digitization, extracting form information from scanned documents stands out as a critical challenge. Digitizing form documents is non-trivial due to their inherent complexity, including intricate structures, non-textual elements like graphics, and combinations of hand- and machine-written content [4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###]. Moreover, scanned documents may suffer from noise, resulting in unclear or distorted images, further complicating the form extraction process. This survey paper focuses on the methodologies employed for extracting form information from noisy scanned documents.\nDocument understanding [6  ###reference_b6###, 7  ###reference_b7###, 8  ###reference_b8###, 9  ###reference_b9###] is the field of analyzing the content and structure of documents with various formats and modalities, such as text, images, tables, and graphs. Language models and transformers [10  ###reference_b10###] are advanced neural network architectures that have demonstrated remarkable capabilities in various natural language processing tasks, as well as computer vision and audio processing tasks. The transformative potential of these models in document understanding has been widely recognised [11  ###reference_b11###].\nForm understanding is essentially a sequence-labeling task akin to named entity recognition (NER), often referred to as key information extraction (KIE) [12  ###reference_b12###]. The task poses unique challenges due to the diverse shapes and formats in which these documents appear (see Fig 1  ###reference_### for examples). Unlike traditional NER tasks that deal with 1D text information, form understanding involves multiple modalities. In addition to textual information, the position and layout of text segments play a vital role in semantic interpretation. An effective language model for form understanding must comprehend document entities within the context of multiple modalities and adapt to various types of document formatting.\nOne of the challenges in form understanding is the integration of visual information from scanned documents. Vision-language models, such as ViT (Vision Transformer) [13  ###reference_b13###, 14  ###reference_b14###] and CLIP (Contrastive Language\u2013Image Pretraining)[15  ###reference_b15###] have demonstrated the potential to bridge the gap between text and images. These models enable the understanding of visual elements within the context of textual content, offering new ways for improved form understanding. Transformer-based models [16  ###reference_b16###, 17  ###reference_b17###] belong to a type of deep learning approach that is very effective in a wide range of natural language processing [18  ###reference_b18###, 19  ###reference_b19###] and computer vision [20  ###reference_b20###, 21  ###reference_b21###] tasks. In the domain of form understanding, Transformer models have been particularly transformative. They excel in interpreting the complex, often unstructured data found in forms and documents, thanks to their ability to capture contextual relationships in text. For instance, BERT\u2019s bidirectional training mechanism enables it to understand the context of a word based on its surrounding text, which is crucial in deciphering the semantics of information in forms.\nThe key feature of these models is the use of self-attention mechanisms, which enables them to effectively weigh the importance of different parts of the input when making predictions. This allows for understanding the context of a given question and providing a relevant answer.\nA notable example of the fusion of language models and transformers in document understanding is the LayoutLM model [22  ###reference_b22###]. This model extends BERT by incorporating layout information, enabling it to consider the spatial arrangement of tokens in addition to their textual context. LayoutLM demonstrated superior performance in tasks like document image classification and form understanding, where both textual and spatial information are crucial.\nIt is a challenging and important problem in the field of document analysis and understanding. While there are several surveys that cover general aspects of document understanding and layout analysis [23  ###reference_b23###, 24  ###reference_b24###], as well as specific topics such as visual document analysis [25  ###reference_b25###], Camera-based analysis of text and documents [26  ###reference_b26###], and deep learning for document image understanding [27  ###reference_b27###], there is currently no comprehensive survey focusing specifically on the task of form understanding. Our survey bridges this gap and provides an extensive overview of recent technologies dedicated to the form understanding task.\nThe main contributions of this paper can be summarized as follows:\nComprehensive survey of recent advancements in form understanding for scanned documents, highlighting the role of language models and transformers.\nRigorous methodology for literature collection, encompassing historical evolution of form understanding methodologies and specialized techniques.\nIn-depth analysis of cutting-edge transformer models, focusing on their effectiveness in handling noisy scanned documents.\nOverview of essential datasets serving as benchmarks for evaluating model performance. Identification of research gap and contribution through a dedicated form understanding survey.\nThe structure of our paper is as follows: In Section 2  ###reference_###, we present our research methodology, detailing how we collected and curated papers for this survey. Section 3  ###reference_### provides an in-depth exploration of previously employed methods in the domain of form and document understanding. We delve into different approaches to address form understanding in noisy scanned documents in Section 4  ###reference_###. In Section 5  ###reference_###, we describe commonly used datasets, which serve as the foundation for comparing various approaches. The comparative analysis is presented in Section 6.2  ###reference_###. Finally, we offer an outlook on potential future directions and conclude in Section 7  ###reference_###.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Literature Collection Methodology",
            "text": "To ensure the comprehensiveness of our survey, we employed a rigorous methodology for collecting relevant literature in the field of form understanding in noisy scanned documents. Our goal was to cover recent advancements and capture the latest trends in this rapidly evolving domain.\nWe conducted searches in prominent scientific databases, namely Scopus, on Aug 6th, 2023. The search query was designed to be comprehensive, combining relevant terms to target the topic effectively: \"document understanding\" AND \"form\" OR \"information extraction\" AND \"Invoices\"). By incorporating \"document understanding\" AND \"form\" in the search string, we ensured a strong connection between the topics of form understanding and document understanding.\nThe search results revealed substantial publication numbers across the databases. Springer Link returned the highest number of publications (162), with 50 of them published in 2021 alone. ACM yielded 75 relevant publications, with a noticeable increase in publications after 2018. IEEE returned 8 relevant publications.\nIn addition to the initial search results, we included the LayoutLMv2 approach [28  ###reference_b28###] as it represented one of the state-of-the-art models during our survey\u2019s timeline.\nTo ensure the thoroughness of our literature collection, we employed the backward-snowballing method on the initially collected papers. This approach helped us identify additional relevant papers that may not have appeared in the initial search results. Through this process, we selected the publications that showcased the latest advances in form understanding and introduced datasets dedicated to form understanding or document understanding as a whole.\nThe chart shown in Figure 2  ###reference_### depicts the number of relevant publications per year. The x-axis represents the years from 2014 to 2023, while the y-axis denotes the count of publications. The bars provide a clear visual representation of the distribution of relevant literature over the years, with a notable surge in publications from 2019 to 2022.\nAs illustrated in Figure 2  ###reference_###, the majority of the relevant literature emerged within the last three years. This trend aligns with the rise in popularity of the topic, primarily driven by the introduction of pre-trained language models like BERT [24  ###reference_b24###].\nOur literature collection methodology ensures the inclusion of recent breakthroughs and facilitates a comprehensive survey that captures the most current state of form understanding in noisy scanned documents. The selected papers will serve as the foundation for the subsequent sections of this survey, where we delve into various approaches, datasets, and comparisons to shed light on the advancements in this vibrant research area.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Early Approaches in Document Understanding",
            "text": "The evolution of document understanding witnessed the use of early models, predominantly based on rule-based algorithms. These methods can be broadly categorized into top-down and bottom-up approaches, each with its strengths and limitations [29  ###reference_b29###, 30  ###reference_b30###, 24  ###reference_b24###].\nTop-down approaches, being efficient and effective, operate well when the document structure is fixed and well-defined. Typically, this method requires a high-level description of the logical structure of the document. However, its limitations become evident in scenarios with complex and varied document layouts. Therefore, top-down approaches are best suited for situations where document structures are known in advance.\nOn the other hand, bottom-up approaches are more time-consuming but offer the capability to analyze previously unknown document structures. These methods analyze the image to extract connected components, progressively merging them into coherent blocks (e.g., characters to words, words to lines, lines to paragraphs). The bottom-up strategy is more versatile, making it suitable for handling diverse document layouts, but it comes at the cost of increased computational overhead.\nForm document processing, in particular, has seen the development of specialized methods based on form description [31  ###reference_b31###, p. 1941]. Researchers have explored form description languages (FDL) to represent the layout and logical structure of form documents using rectangular regions [29  ###reference_b29###, 32  ###reference_b32###, 31  ###reference_b31###]. FDL enables a comprehensive representation of the geometric and logical aspects of form documents, facilitating their automated processing.\nFor document understanding in general, earlier techniques included tree transformation to represent multi-article documents and the utilization of form definition languages [31  ###reference_b31###, p. 1939]. Tree transformations aimed to organize document content into a logical tree structure, aiding in content categorization and extraction. For instance, the separation of a document into header and body blocks involves labeling each block according to its type and then moving the blocks in the tree structure based on predefined rules.\nThese older models paved the way for document understanding, offering valuable insights and techniques that laid the foundation for subsequent research. However, the limitations of rule-based approaches and the demand for higher accuracy and adaptability led to the development of more sophisticated methods, including the recent advancements we explore in the subsequent sections of this survey."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Multi-Modal Transformer Approaches",
            "text": "In recent years, the trajectory of models in the document understanding domain has been characterized by a paradigm shift towards enriching transformer-based architectures with additional modalities beyond text. This evolution has led to breakthroughs in capturing layout, formatting, and contextual cues present in scanned documents.\nThis section introduces a series of significant models that have emerged in recent years, contributing to the advancement of document understanding. We present these models chronologically, illustrating their innovative integration of multi-modal information and their unique contributions to the field. A comprehensive overview of the covered models, including the modalities they leverage and the types of approaches they embody, is provided in Table 1  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Graph-Based Models",
            "text": "Graph-Based Text Relationship Modeling is a technique that employs graph neural networks (GNNs) to capture intricate relationships between text segments in documents. GNNs can process data that can be represented as graphs, such as nodes and edges333https://distill.pub/2021/gnn-intro/  ###reference_###444https://en.wikipedia.org/wiki/Graph_neural_network  ###reference__network###. These models use an encoder to process both textual and visual features, followed by a graph module that constructs a soft adjacency matrix to represent pairwise relationships between segments. This approach is particularly effective for capturing long-range dependencies and context in documents with complex textual interactions, such as relation extraction and entity linking [37  ###reference_b37###, 38  ###reference_b38###].\nPICK [39  ###reference_b39###] uses text and layout features in conjunction with a graph learning module to capture relationships between text segments.\nThe architecture of PICK555https://github.com/wenwenyu/PICK-pytorch  ###reference_### is built on three separate modules:\nThe encoder module takes the original document as input. For each sentence of a document, the input vector consists of the text embeddings of characters, the bounding box, and the image embedding. Text segments are encoded using a transformer [40  ###reference_b40###] and image segments are encoded using a pre-trained ResNet CNN [41  ###reference_b41###]. The text and image features are then combined by element-wise addition.\nThe graph module takes the combined representation of the encoder module and layout bounding boxes of the original document as input. It is trained to generate a soft adjacent matrix that captures pairwise relationships between two graphs nodes.\nThe usage of graph learning allows this model to capture long-distance relationships between text segments, which is traditionally a problem for transformer-based language models.\nThe decoder module that takes both the results of the encoder and the results of the graph module as input and forms the final output. The decoder uses a bidirectional long short-term memory network (BiLSTM) and a conditional random field algorithm (CRF) to perform sequence tagging on the union non-local sentence at a character level.\nThe Text Reading and Information Extraction (TRIE) model [42  ###reference_b42###] is unique in that the usually separated tasks of text-reading and text-understanding are fused into an end-to-end trained network. The general assumption is that both tasks can mutually benefit from each other during pre-training. The model consists of a text-reading module, a multi-modal context block and an information-extraction module. The text-reading module uses a ResNet CNN [43  ###reference_b43###] in combination with a Feature Pyramid Network (FPN) [44  ###reference_b44###] to extract the convolutional features  for an input image. This image embedding is passed to a  function (usually anchor-based or segmentation-based approaches), which determines possible text regions  as bounding boxes. The regions  are fused with the image features  using RoIAlign [45  ###reference_b45###] to get the text-region (or visual) features . Each  is fed into an encoder/decode model to produce the corresponding characters  where  is the length of the label sequence. These textual features are represented in textual feature vector  which is used together with visual feature vector  for information extraction. The multi-modal context block fuses textual, positional, and visual features created by the previous task. It makes use of a multi-head self-attention mechanism for the text embeddings, creating a text-context vector . The information extraction module uses the text-context  and the visual context  to separate entities and the text-embedding  to extract their entity values. First, the final context vector  is created as a weighted sum of the context vectors  and . This fused context vector is concatenated with the text-embedding, and the final feature vector is passed to bidirectional LSTM."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Multi-Modal Fusion Models",
            "text": "Multi-Modal Transformer Approaches are a type of large models that advance the fusion of modalities by introducing novel mechanisms to better integrate text, layout, and visual information. These models often propose modifications to the transformer architecture, such as spatial-aware self-attention mechanisms, to capture multi-dimensional relationships between tokens. They also introduce new pre-training tasks, like text-image matching and sequence length prediction, to enhance the understanding of document content and layout. These models are suitable for various multimodal tasks, such as document classification, image captioning, and video understanding [46  ###reference_b46###, 47  ###reference_b47###, 48  ###reference_b48###].\nSelfDoc [49  ###reference_b49###] is a novel model in the way the textual features are encoded.\nInstead of working on word-level embeddings, SelfDoc receives text grouped by semantically relevant components also referred to as document object proposals (text-block, title, list, table, figure) as model input.\nFor the component-specific text extraction, the authors train a Faster R-CNN [50  ###reference_b50###] on public document datasets PubLayNet [3  ###reference_b3###] and DocVQA [51  ###reference_b51###] on a document object detection task.\nThe categorized regions are cropped from the document and passed to an OCR engine.\nThe encoded visual and textual feature vectors are then passed separately through two distinct multi-head attention encoders.\nSince those encoders do not share information between the visual and textual modality, they are referred to as single-modality encoders.\nThe resulting attention matrix of both modalities is then combined using a third encoder, making the system a cross-modality encoder. This mechanism allows the SelfDoc model to use multi-modal document features during the pre-training phase.\nUniLMv2 [52  ###reference_b52###] allows for pre-training of a unified language model consisting of an auto-encoding and and a partially auto-regressive model.\nThe auto-encoding model predicts tokens by conditioning on context just like BERT [53  ###reference_b53###].\nThe partially auto-regressive model is based on masked- and pseudo-masked tokens. It can predict one or more masked (or pseudo-masked) tokens per factorization step. This approach allows the model to train on masked-token spans instead of single tokens. The authors refer to this as Pseudo-Masked Language Modelling.\nMasking of the data is done as follows: 15% of original tokens are masked. 40% of the time n-gram blocks are masked and 60% of the time single tokens are masked. The pre-training objective is to minimize the summed cross-entropy loss for the reconstruction of masked- and pseudo-masked-tokens in both models.\nThe model\u2019s input consists of word token-embeddings, their absolute position embedding, and their segment embedding.\nDocFormer [54  ###reference_b54###] improves on multi-modal training by fusing vision and language network layers.\nThe visual features are obtained by using a ResNet CNN [43  ###reference_b43###] at a lower-resolution embedding level.\nThe visual embedding is then reduced in dimensionality by applying a 1 x 1 convolution. For the text embedding, DocFormer uses the OCR result from the document image and tokenizes it using WordPiece [36  ###reference_b36###]. The tokenized text  is in the form of  where N = 511 and all further tokens  with  are ignored. The text embedding is padded using a [PAD] token that is ignored by the self-attention mechanism. The resulting text input feature is of the same shape as the visual feature and formally defined as\n\nwhere the weight  is initialized from LayoutLM pre-training weights.\nFor the spatial features the authors use the top-left and bottom-right coordinate of each word\u2019s bounding box in addition to the box\u2019s width , height , the euclidean distance from each corner of a box to each corner of the next box, and the centroid distance between boxes.\nThe latter two can be defined as  where  defines the direction of the relation ( stands for the centroid coordinate).\nSimilar to many comparable models DocFormer also encodes a 1D position encoding  which defines the token sequence.\nStrucText [55  ###reference_b55###] model uses a multi-modal combination of visual and textual document features.\nThe textual embedding consists of a language token embedding where each text sentence is processed coherently to preserve semantic context.\nThis is different from other models where word tokens of a sentence are processed individually. A sentence S is a sorted set of words, with the line delimiters [CLS] and [SEP] inserted at the beginning () and the at the end () of a sentence respectively.\nThe final textual embedding is achieved by combining the language token embedding with the layout embedding (i.e. bounding box of token).\nFor extractions of features of an image , ResNet [43  ###reference_b43###] is used which creates the visual embedding. Additionally, a sequence id embedding is used to capitalize on text segment order.\nFor pre-training, the authors used MVLM of LayoutLM in combination with two additional tasks.\nThe first novel task is called sequence length prediction (SLP) where the length of a text sequence has to be predicted based on the visual features of the document.\nThis task forces the model to learn from the image embedding.\nThe second additional task is paired box direction (PBD).\nFor two text segments  and , the model needs to predict in which direction segment  is located with respect to .\nHere the 360-degree field is divided into eight buckets of identical size.\nPBD is therefore a classification task that forces the model to learn spatial text segment relationships.\nBROS [56  ###reference_b56###] model does not incorporate the image of the text for pre-training, which is considered SOTA.\nThe authors claim that it is more effective to focus on the combination of text and position-information, ignoring the layout modality.\nBROS introduces a custom text serializer that sorts the tokens in their respective reading order.\nThe model\u2019s features do not include absolute 2D positions of text, but a relative position representation based on the position of neighboring tokens as shown in Fig. 5  ###reference_###. This has the advantage that the model can capture relations between similar key-value pairs better compared to absolute spatial information."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Cross-Modal Interaction Models",
            "text": "Cross-Modal Interaction Models are a type of artificial intelligence model that facilitate interactions between different modalities like text and layout in multimodal data. They employ attention mechanisms and fusion strategies to enable cross-modal information exchange and enhance the perception and understanding of the data. These models are suitable for various cross-modal tasks, such as document analysis, image-text retrieval, and video question answering.\nUDoc [57  ###reference_b57###] is a framework that employs a multi-layer gated cross-attention encoder [58  ###reference_b58###, 59  ###reference_b59###], enabling effective cross-modal interactions within a single model. Unlike traditional methods, UDoc takes into account the complex and diverse nature of documents by incorporating both textual and visual features. This approach addresses the challenges of hierarchical document structures, multimodal content, and spatial layout. UDoc employs three pretraining tasks: Masked Sentence Modeling (MSM) [50  ###reference_b50###], Visual Contrastive Learning (VCL), and Vision-Language Alignment (VLA) [60  ###reference_b60###]. MSM predicts masked sentence embeddings, VCL focuses on quantized visual features, and VLA enforces alignment between textual and visual elements.\nThe document images are processed through feature extraction and quantization, providing a structured representation. The pretraining tasks, including MSM, VCL, and VLA, contribute to enhanced embeddings, capturing both local and global dependencies.\nTILT [61  ###reference_b61###] addresses the complex challenge of natural language comprehension in documents that extend beyond plain text by introducing the Text-Image-Layout Transformer (TILT) neural network architecture. TILT is designed to simultaneously capture layout information, visual features, and textual semantics, making it capable of handling documents with rich spatial layouts. TILT model integrates layout representation through attention biases and contextualized visual information, leveraging pre-trained encoder-decoder Transformers [62  ###reference_b62###, 63  ###reference_b63###].\nIn the proposed TILT architecture, the model\u2019s strength lies in its ability to combine contextualized image embeddings [64  ###reference_b64###] with semantic embeddings. By incorporating both layout and visual information, TILT enhances its understanding of documents, particularly those with complex structures like forms and tables."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Sequence-to-Sequence Models",
            "text": "Encoder-Decoder and Sequence-to-Sequence Models involve encoders to process input data and decoders to generate sequential output. The encoders and decoders can be composed of different architectures, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), or transformers. They are used for tasks like text generation, translation, and sequence prediction, where the input and output have different lengths or structures [65  ###reference_b65###, 66  ###reference_b66###, 67  ###reference_b67###, 68  ###reference_b68###].\nGenDoc [69  ###reference_b69###] model employs an encoder-decoder architecture, allowing adaptability to diverse downstream tasks with varying output formats, which differentiates it from common encoder-only models. GenDoc\u2019s pre-training tasks encompass masked text prediction, masked image token prediction, and masked coordinate prediction, catering to multiple modalities. The model integrates modality-specific instructions, disentangled attention [70  ###reference_b70###], and a mixture-of-modality-experts(MoE) [71  ###reference_b71###] approach to effectively capture information from each modality.\nThe fusion of modalities within the encoder is achieved through disentangled attention, leading to improved layout effectiveness. The authors employ disentangled attentions for various interactions, including content-to-content, content-to-layout in the x-dimension, and content-to-layout in the y-dimension. This is represented by the following equations:\nHere,  represents the resulting attention score between positions  and ;\n is the conventional attention score from content query  to content key  for positions  and ;\n and  denote the disentangled attention scores from content to layout;\n denotes the relative distance function between positions  and .\nIn contrast, modality separation within the decoder employs the MoE strategy, which activates task-specific experts for natural language, image token prediction, and coordinate prediction tasks. The pre-training tasks include text infilling, masked image token prediction, and masked coordinate prediction, employing a shared vocabulary.\nDocFormerv2 [72  ###reference_b72###] comprises a multi-modal encoder [73  ###reference_b73###] tailored to process both textual and spatial characteristics inherent in documents. The model adopts traditional transformer-based language modeling to handle textual content while introducing inventive pre-training tasks like token-to-line and token-to-grid predictions to capture document layout and structure as shown in Fig 4  ###reference_### and 4  ###reference_###. This approach enables the model to understand the spatial arrangement of document elements.\nTo effectively incorporate spatial information from document images, the authors introduce spatial features that encode critical elements such as position, size, and category. By encoding these elements, DocFormerv2 gains an understanding of the document\u2019s structure and the relationships between different components.\nA key strength of DocFormerv2 lies in its adept multi-modal fusion techniques [74  ###reference_b74###]. By employing self-attention mechanisms, the model seamlessly integrates textual and spatial features, enabling it to comprehensively understand document content and layout. This fusion facilitates improved performance in a range of document understanding tasks.\nFormNet [75  ###reference_b75###] is designed to improve the extraction of information from complex form-like documents. These documents, which often contain tables, columns, and intricate layouts, pose challenges for traditional sequence models. FormNet addresses this issue by incorporating structural information into the sequence modeling process. FormNet contains two components: Rich Attention and Super-Tokens. Rich Attention enhances attention score calculation by considering both semantic relationships and spatial distances between tokens, enabling the model to capture the underlying layout patterns more effectively. Super-Tokens are constructed for each word in the form through graph convolutions, recovering local syntactic relationships that may have been lost during serialization. The information extraction problem is formulated as sequential tagging for tokenized words, with the goal of predicting key entity classes for each token. The authors build upon the Extended Transformer Construction (ETC) framework, which efficiently handles long sequences. ETC employs sparse global-local attention, and FormNet enhances positional encoding through Rich Attention."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Layout Representation and Language-Independent Models",
            "text": "Layout Representation Models [76  ###reference_b76###, 77  ###reference_b77###, 78  ###reference_b78###] focus on capturing and representing the spatial arrangement of components in documents. They encode layout information for tasks like structure analysis and extraction. Language-Independent Models [79  ###reference_b79###, 80  ###reference_b80###, 81  ###reference_b81###] designed to work across multiple languages. They often utilize pre-training in one language and fine-tuning in others, enabling document understanding in various languages.\nMCLR [82  ###reference_b82###] proposed method employs a cell-based layout representation. This representation utilizes row and column indexes to define the position of components within a document, aligning with human reading habits and facilitating a more intuitive understanding of layout structures. The MCLR architecture comprises three main components: the cell-based layout, the multi-scale layout, and data augmentation.\nIn the cell-based layout, the traditional challenge of learning spatial relationships from coordinates is addressed by using row and column indexes. This enables the model to learn whether components share the same row or column and the number of components in each row or column. The approach also provides a more human-centered representation, aligning with natural reading patterns and enhancing the detection of latent rules within the document layout. Furthermore, the authors introduce a multi-scale layout representation by employing word- and token-level cells.\nLiLT [78  ###reference_b78###] introduces an innovative solution to a critical challenge in structured document understanding (SDU). The authors propose the Language-independent Layout Transformer (LiLT), a novel framework that enables pre-training on documents from a single language and subsequent fine-tuning for use with multiple languages.\nThe core architecture of LiLT is founded upon a parallel dual-stream Transformer model that efficiently processes both text and layout information. During pre-training, LiLT decouples the text and layout aspects of the document and employs a bi-directional attention complementation mechanism (BiACM) to facilitate interaction between these modalities. The model is then fine-tuned with a focus on downstream tasks using off-the-shelf pre-trained textual models. This process ensures that LiLT learns and generalizes the layout knowledge from monolingual documents to multilingual contexts."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Hybrid Transformer Architectures",
            "text": "Hybrid Transformer Architectures [83  ###reference_b83###, 84  ###reference_b84###] leverage innovative designs to address the challenges of computational complexity and diverse structural representations in document understanding. These models often incorporate novel attention mechanisms, such as Symmetry Cross-Attention (SCA) [85  ###reference_b85###], and utilize hourglass transformer architectures with merging and extension blocks to efficiently process multi-modal information. These designs enhance both performance and efficiency in analyzing complex documents.\nStructuralLM [83  ###reference_b83###] is a successor to the LayoutLM model.\nIt differs from LayoutLM by operating on 2D cell position embeddings instead of 2D word embeddings.\nThe core idea is to better capture word relations by grouping them.\nEach document  is represented by a sequence of cells  and each cell is represented by a sequence of words , where each word  of cell  has the same 2D position embedding.\nIn addition to this, the 2D embedding receives 1D position information per token (i.e. the sequence number of the token) to leverage word order within a cell, and the embedding of the word itself using WordPiece [36  ###reference_b36###].\n\nThe first pre-training task for the StructuralLM model is the MVLM task introduced by LayoutLM [22  ###reference_b22###].\nSince positional information is not dropped during MVLM and words of the same cell share common position information, StructuralLM can leverage the cell grouping during this task.\nThe second pre-training task is called cell position classification (CPC).\nHere, a set of scanned documents is split into  areas. For a random cell of the documents, the model needs to predict where the cell belongs using its center coordinate.\nThis is in essence a multinomial classification task where each label  belongs to a document section.\nStrucTexTv2 [84  ###reference_b84###] a model that combines visual and textual information to effectively analyze document images. The proposed approach employs a unique pre-training strategy with two self-supervised tasks: Mask Language Modeling (MLM) [16  ###reference_b16###] and Mask Image Modeling (MIM) [86  ###reference_b86###, 87  ###reference_b87###], where text region-level masking is utilized to predict both visual and textual content. The core architecture of StrucTexTv2 comprises two main components: a visual extractor (CNN) and a semantic module (Transformer). The visual extractor processes the input document image using convolutional layers, extracting visual features from different stages of downsampling. The semantic module employs the Transformer architecture, which takes the flattened visual features and transforms them into a 1D sequence of patch token embeddings. A relative position embedding is added to the token embeddings, and the standard transformer processes these embeddings to generate enhanced semantic features. The resulting feature maps are reshaped to 2D context feature maps and up-sampled with a Feature Pyramid Network (FPN) strategy [44  ###reference_b44###].\nFast-StrucTexT [85  ###reference_b85###] is an innovative and efficient transformer-based framework designed for document understanding tasks. The proposed model addresses the challenges posed by the computational complexity of traditional transformers while efficiently handling the diverse structural representations within documents. Fast-StrucTexT incorporates a novel hourglass transformer architecture, modality-guided dynamic token merging, and Symmetry Cross-Attention (SCA) [88  ###reference_b88###] to achieve both high performance and efficiency.\nThe SCA module functions as a two-way cross-attention mechanism that facilitates interaction between different modes. In this module, one set of model features serves as the query, while another set of features from a different mode serves as both the key and value. This arrangement enables the calculation of cross-attention in both visual and textual modalities.\nThe SCA is defined by the following equations:\n1. Cross-Attention (CA) between feature sets  and  is given by:\nwhere , , and  are linear projections for query, key, and value respectively.  represents the softmax function,  is the hidden size, and  is the output weight.\n2. SCA between feature sets  and  is defined as:\nFast-StrucTexT leverages an hourglass transformer design, integrating Merging-Blocks and Extension-Blocks to efficiently process multi-modal information and handle complex document structures. The Merging-Blocks aggregate tokens to capture higher-level context, enhancing efficiency without compromising semantic richness. Complementarily, Extension-Blocks recover merged tokens, ensuring fine-grained details are preserved. The modality-guided dynamic token merging mechanism selectively merges tokens based on relevance, enabling diverse structural representations.\nModalities\n\n\nModel\n\n\nText\n\n\n\n1D Pos.\n\n\n\n2D Pos.\n\n\n\nImage\n\nApproach type\n\n\n\nBERT\n\u2713\n\u2713\n\n\n\n\nLayoutLM\n\u2713\n\u2713\n\u2713\n\u2713\nimage-based fine-tuning\n\nUniLMv2\n\u2713\n\u2713\n\u2713\n\nseq2seq\n\nPICK\n\u2713\n\u2713\n\u2713\n\u2713\ngraph-learning\n\nTRIE\n\u2713\n\u2713\n\u2713\n\u2713\nmulti-task (OCR + IE)\n\nLayoutLMv2\n\u2713\n\u2713\n\u2713\n\u2713\nspatial-aware self-attention\n\nStructuralLM\n\u2713\n\u2713\n\u2713\n\u2713\ntext-segment grouping\n\nSelfDoc\n\u2713\n\u2713\n\u2713\n\u2713\ntext-segment grouping\n\nDocFormer\n\u2713\n\u2713\n\u2713\n\u2713\ndiscrete multi-modal\n\nStrucText\n\u2713\n\u2713\n\u2713\n\u2713\ntext-segment grouping\n\nBROS\n\u2713\n\u2713\n\u2713\n\nusing relative 2D positions\n\nStrucTexTv2\n\u2713\n\u2713\n\u2713\n\u2713\nmulti-modal fusion\n\nFast-StrucTexT\n\u2713\n\u2713\n\u2713\n\u2713\nefficient transformer\n\nDocFormerv2\n\u2713\n\u2713\n\u2713\n\u2713\nmulti-modal encoder\n\nUDoc\n\u2713\n\u2713\n\u2713\n\u2713\ngated cross-attention\n\nTILT\n\u2713\n\u2713\n\u2713\n\u2713\nText-Image-Layout Transformer\n\nGenDoc\n\u2713\n\u2713\n\u2713\n\u2713\nmodality fusion, disentangled attention\n\nMCLR\n\u2713\n\u2713\n\u2713\n\u2713\ncell-based layout, multi-scale layout\n\nLiLT\n\u2713\n\u2713\n\u2713\n\u2713\nlanguage-independent layout\n###figure_3### ###figure_4### ###figure_5### ###figure_6###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Datasets",
            "text": "In this section, we introduce commonly used datasets for form understanding tasks.\nFor a comparison of basic features of the datasets, see Tab. 2  ###reference_###.\nDataset\nDocument Content\nColor\nUsed Languages\n#Documents\nDocument Type\n\n\n\nFUNSD\nMachine Written / Handwritten\nB/W\nEN\n199\nForm Documents\n\nXFUND\nMachine Written / Handwritten\nB/W\nMulti-lingual\n199/language\nForm Documents\n\nNAF\nMachine written pre-printed text and mostly handwritten input text\nB/W\nEN\n865\nMultiple\n\nIIT-CDIP\nMachine Written, Handwritten\nB/W\nEN\n\nMultiple\n\nRVL-CDIP\nMachine Written, Handwritten\nB/W\nEN\n\nMultiple\n\nPubLayNet\nMachine Written\nColor\nEN\n\nMedical Literature\n\nSROIE\nMachine Written with occasional handwriting\nColor, B/W\nEN\n\nReceipts\n\nCORD\nMachine Written with occasional handwriting\nColor\nEN\n released,  total\nReceipts\n\nDocVQA\nMachine Written, Handwritten\nColor, B/W\nEN\n\nMultiple\n\nForm-NLU\nMachine Written, Handwritten\nColor, B/W\nEN\n\nMultiple\n\nVRDU\nMachine Written\nColor, B/W\nEN\n\nAd-buy\nRVL-CDIP and IIT-CDIP: The Ryerson Vision Lab Complex Document Information Processing RVL-CDIP dataset [89  ###reference_b89###] was created to evaluate Deep CNNs666https://www.cs.cmu.edu/~aharley/rvl-cdip/  ###reference_###.\nThe dataset is a subset of the IIT Complex Document Information Processing Test Collection IIT-CDIP [4  ###reference_b4###, 90  ###reference_b90###] dataset, which itself contains a collection of documents from the Legacy Tobacco Documents Library (LTDL) created by the University of California San Francisco [91  ###reference_b91###].\nIIT-CDIP consists of  document records in XML format, each of which contains text recognized by OCR and metadata information (document names, geographical information, etc.) of various structure and quality.\nAdditionally, IIT-CDIP contains source images of scanned documents [92  ###reference_b92###].\nIn total, the RVL-CDIP dataset consists of  grayscale images separated into 16 classes (including form documents) with  images per class.\nThe dataset is split into 320k training, 40k validation, and 40k test images.\nFUNSD:\nThe dataset for Form Understanding in Noisy Scanned Documents (FUNSD) [93  ###reference_b93###] is a collection of 199 fully annotated forms from various fields like marketing, science, and more777https://guillaumejaume.github.io/FUNSD/  ###reference_###.\nAll forms are rasterized, low-resolution one-page forms containing noise.\nMore specifically, the authors manually selected  eligible documents from the RVL-CDIP dataset [4  ###reference_b4###] form category and used random sampling to gain a final set of  documents.\nMost of the textual content is machine-written, which might not reflect real-life scenarios where handwritten content can be more prominent.\nAll annotations used for text detection were done by external contractors.\nThe remaining tasks were annotated using an undisclosed annotation tool.\nEvery form in the dataset is encoded in a JSON file where each form is considered as a list of interlinked semantic entities (i.e. a name field that is interlinked with its corresponding answer box).\nEach semantic entity contains an identifier, a label, a bounding box, a list of links with other entities, and a list of words.\nIn total, the dataset contains  semantic entities and  relations.\nThe authors of [94  ###reference_b94###] have found several errors and inconsistencies in the FUNSD dataset including incorrect textual contents and bounding boxes as well as inconsistencies in annotating relations. They released an updated version of the dataset that fixes the discovered issues888https://drive.google.com/drive/folders/1HjJyoKqAh-pvtg3eQAmrbfzPccQZ48rz?fbclid=IwAR2ouj5Sh0vkcKAMNfSoZjSM7vSpGnbK-AowWZZ8_Lltcn34hr7_nVcazu0  ###reference_JyoKqAh-pvtg3eQAmrbfzPccQZ48rz?fbclid=IwAR2ouj5Sh0vkcKAMNfSoZjSM7vSpGnbK-AowWZZ8_Lltcn34hr7_nVcazu0###.\nXFUND [95  ###reference_b95###] is an extension to FUNSD where the dataset is translated into seven other languages, namely Chinese, Japanese, Spanish, French, Italian, German, and Portuguese.\nLike FUNSD, the dataset contains information on both semantic entities in a document and their relations to one another999https://github.com/doc-analysis/XFUND  ###reference_###.\nThe dataset was manually created by gathering form templates in the above-mentioned languages from the internet.\nEach form is filled out exactly once with synthetic information to prevent repetition of forms and avoid sharing sensitive information. Forms are filled out either digitally or with handwriting and then scanned to again be similar to the original FUNSD dataset.\nTo generate bounding boxes for each semantic entity, the authors use the Microsoft Read API [96  ###reference_b96###].\nRelation extraction is then done manually by annotators.\nIn total, the XFUND dataset contains  forms per language ( total) split into training/test splits of / documents respectively.\nNAF [1  ###reference_b1###, 97  ###reference_b97###] is an annotated dataset created from historical form images of the United States National Archives.\nThe forms have varied layouts and are noisy from degradation and machinery that was used to print them.\nA form in the dataset typically contains English pre-printed text and English input text consisting of handwriting, typed text, and stamped content.\nIn total, the dataset contains  annotated grayscale form images.\nIt is undisclosed how they were annotated.\nThe NAF dataset is manually split into a training, a validation, and a test set such that each set contains images with distinct form layouts.\nWith this, the authors create a scenario where a classifier trained with the training set will not have seen the form layouts of the test set.\nPubLayNet [3  ###reference_b3###] is a dataset that was automatically created by matching the XML representations and the content of  scientific PDF articles from PubMed CentralTM.\nThe dataset was created primarily to advance research in document layout analysis, meaning it is not specifically made for form detection101010https://github.com/ibm-aur-nlp/PubLayNet  ###reference_###.\nThe annotations were created by parsing the PDF representation of the article using the PDFMiner [98  ###reference_b98###] software and then matching the generated layout with the respective XML representation.\nWhile not mentioned in the paper, most of the\nannotated documents seem to have been removed during the quality control procedure as only a total of  documents remain in the dataset after this step.\nThe dataset is split into  pages for training,  pages for development, and  pages for testing.\nBoth development and testing sets have been selected from multiple different journals and contain a sufficient number of tables, lists, and figures.\nThe SROIE [99  ###reference_b99###] dataset was created as part of the ICDAR 2019 competition on Scanned Receipt OCR and Information Extraction111111https://drive.google.com/drive/folders/1ShItNWXyiY1tFDM5W02bceHuJjyeeJl2  ###reference_ItNWXyiY1tFDM5W02bceHuJjyeeJl2###.\nThe dataset contains 1000 scanned receipt images with different annotations depending on one of three competition tasks (Text Localisation, OCR, and Key-Information Extraction).\nReceipts in the dataset may have poor paper, ink, or print quality, be of low resolution, and contain scanning artifacts. Further, receipts may contain unneeded interfering texts in complex layouts, long texts, and small font sizes.\nEach receipt contains English character text fields like goods name and digit fields like unit price and total cost.\nThe paper does not mention the origin of the receipts.\nCORD: The Consolidated Receipt Dataset for post-OCR parsing [100  ###reference_b100###] was created to be the first public dataset containing box-level text and parsing class annotations.\nParsing class annotations are represented as a combination of one of eight superclasses (i.e. store, payment, and menu) and their respective subclasses (i.e. name, address, and price);\ne.g.,\nmenu.price.\nThe dataset itself contains  Indonesian receipts from shops and restaurants gathered through crowdsourcing.\nImages were manually annotated using a web-based annotation tool where each image is first annotated and then checked for correctness and compliance with the annotation guidelines.\nSensitive information like credit card numbers or a person\u2019s full name have been blurred in the final set of receipts.\nOut of the  receipts, only  have been made available to the public due to concerns of accidentally publishing sensitive personal data.\nAccording to one of the authors of the paper, future data release is unclear [101  ###reference_b101###].\nDocVQA: [51  ###reference_b51###, 102  ###reference_b102###] is a dataset of  document images of varied types for the task of Visual Question Answering.\nThe documents are sourced from documents of the UCSF Industry Documents Library [103  ###reference_b103###].\nIndividual documents were hand-picked to provide suitable image quality and tables, forms, lists, and figures over long-running text fields.\nThe documents are split randomly in an 80/10/10 ratio of train/validation/test splits respectively.\nA total of  questions and answers have been defined on the documents by remote workers using a three-step web-based annotation tool.\nIn the first step, a worker defines up to ten question-answer pairs for a document.\nIn the second step, another worker will try to answer the defined questions without seeing the pre-defined answers.\nIf no answers of the first step match with any of the answers of the second step, the question is moved to a third step where the authors of the paper can manually edit the question-answer pairs.\nBesides question-answer pairs, the dataset also contains one of nine question types per question.\nAn example of a question type is table/list which specifies if table or list understanding is required to answer the question.\nA question can be assigned more than one question type.\nForm-NLU: [104  ###reference_b104###] is a comprehensive dataset designed to advance the field of Natural Language Understanding (NLU) in the context of form-based documents. The dataset comprises a total of  document images, 6k form keys and values, and 4k table keys and values. These documents encompass a variety of form types, including digital, printed, and handwritten. Form-NLU constitutes a portion of the publicly accessible financial form dataset originating from Form 604, which was amassed by SIRCA121212https://asic.gov.au/regulatory-resources/forms/forms-folder/604-notice-of-changeof-interests-of-substantial-holder/  ###reference_orms/forms-folder/604-notice-of-changeof-interests-of-substantial-holder/###. It provides text records of substantial shareholder notifications filed with the Australian Stock Exchange (ASX)131313https://www2.asx.com.au/  ###reference_www2.asx.com.au/### from 2003 to 2015. The authors segmented the annotation into three specialized sub-tasks: Key annotation for 12 types of Keys, Value annotation for paired Values, and Non-Key-Value annotation for other components like Title, Section, and Others. Each annotator handled one sub-task.\nVRDU: is a benchmark dataset for Visually-rich Document Understanding, consisting of two distinct datasets: Ad-buy Forms, comprising 641 documents, and Registration Forms, comprising 1,915 documents. These datasets are designed to address challenges in extracting structured data from visually complex documents. The benchmark incorporates five key desiderata: rich schema, layout-rich documents, diverse templates, high-quality OCR results, and token-level annotation.\nThe benchmark focuses on three tasks:\nSingle Template Learning (STL): the model is trained and evaluated on documents that belong to a single template. This means that the training, validation, and testing sets all contain documents that share the same structural layout or template. The goal of this task is to assess the model\u2019s ability to extract structured data when presented with a consistent and familiar document layout.\nMixed Template Learning (MTL): the model is trained and evaluated on documents that come from a set of templates. The training, validation, and testing sets include documents from different but predefined templates. This task evaluates the model\u2019s capacity to generalize its learning to diverse document layouts and templates that it has encountered during training.\nUnseen Template Learning (UTL): The Unseen Template Learning task challenges the model to generalize beyond its training experience. In this task, the model is trained on documents from a subset of templates and is then evaluated on documents with templates it has never encountered during training. The goal is to assess the model\u2019s ability to adapt to new, previously unseen document layouts and templates.\nThe evaluation toolkit includes a type-aware fuzzy matching algorithm to assess extraction performance. Repeated, unrepeated, and hierarchical entity names are included, with post-processing heuristics to handle unrepeated and hierarchical entities."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments Results",
            "text": "In this section, we provide a comprehensive comparison of selected models on various datasets, including FUNSD, SROIE, CORD, RVL-CDIP, DocVQA, and Publaynet. We evaluate the models\u2019 performance using a range of metrics, including Precision, Recall, F1 Score, Accuracy, Average Normalized Line Similarity (Anls), and Mean Average Precision (MAP)."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Evaluation Metrics",
            "text": "We utilize several evaluation metrics to assess the effectiveness of the models in extracting structured information from scanned documents. The key evaluation metrics are as follows:"
        },
        {
            "section_id": "6.1.1",
            "parent_section_id": "6.1",
            "section_name": "6.1.1 Precision, Recall, F1 Score, and Accuracy",
            "text": "Precision, Recall, F1 Score, and Accuracy are standard metrics used to evaluate the performance of models in classification tasks. These metrics are calculated as follows:\nWhere:\nTrue Positives (TP) are the correctly predicted positive samples.\nFalse Positives (FP) are the incorrectly predicted positive samples.\nFalse Negatives (FN) are the incorrectly predicted negative samples.\nTrue Negatives (TN) are the correctly predicted negative samples.\nTotal Samples is the total number of samples in the dataset."
        },
        {
            "section_id": "6.1.2",
            "parent_section_id": "6.1",
            "section_name": "6.1.2  Average Normalized Levenshtein Similarity (Anls)",
            "text": "Anls [105  ###reference_b105###] captures the OCR mistakes applying a slight penalization in case of correct intended responses, but badly recognized. It also makes use of a threshold of value 0.5 that dictates whether the output of the metric will be the ANLS if its value is equal to or bigger than 0.5 or 0 otherwise. The key point of this threshold is to determine if the answer has been correctly selected but not properly recognized, or on the contrary, the output is a wrong text selected from the options and given as an answer.\nMore formally, the ANLS between the net output and the ground truth answers is given by the following equation. Where  is the total number of questions,  total number of GT answers per question,  the ground truth answers where , and , and  be the network\u2019s answer for the ith question :\nWhere:\nwhere  is defined as follows:"
        },
        {
            "section_id": "6.1.3",
            "parent_section_id": "6.1",
            "section_name": "6.1.3 Mean Average Precision (MAP)",
            "text": "MAP is a metric commonly used to evaluate the quality of ranked lists in information retrieval tasks. It involves calculating the Average Precision (AP) for each query and then averaging these values across all queries. MAP is also used in object detection to evaluate the performance of a model. In object detection, the goal is to identify and localize objects in an image or video. The MAP metric takes into account both the accuracy of the detections and the precision of the detections. The formula for calculating MAP is as follows:\nWhere:\nis the total number of queries.\nis the Average Precision for query , calculated as the average precision of relevant documents at each position in the ranked list:\nWhere:\nis the total number of retrieved documents for query .\nis the precision at position  in the ranked list.\nis an indicator function that takes the value 1 if the document at position  is relevant to query , and 0 otherwise."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Model Comparison",
            "text": "In this section, We present a comparison of selected models on the FUNSD, SROIE, CORD, RVL-CDIP, DocVQA, and Publaynet datasets in Table 3  ###reference_###, 4  ###reference_###, 5  ###reference_###, 6  ###reference_###, 7  ###reference_###,and 8  ###reference_###. The table provides the Precision, Recall, F1 Score, Accuracy, Anls, MAP, and the number of parameters for each model on each dataset."
        },
        {
            "section_id": "6.2.1",
            "parent_section_id": "6.2",
            "section_name": "6.2.1 FUNSD Dataset",
            "text": "Model\nDataset\nPrecision\nRecall\nF1\n#Params\n\n\n\nLayoutLMBase\nFUNSD\n0.7597\n0.8155\n0.7866\n113M\n\nLayoutLMLarge\nFUNSD\n0.7596\n0.8219\n0.7895\n343M\n\nLayoutLMv2Base\nFUNSD\n0.8029\n0.8539\n0.8276\n200M\n\nBROSBase\nFUNSD\n0.8116\n0.8502\n0.8305\n110M\n\nStrucText\nFUNSD\n0.8568\n0.8097\n0.8309\n107M\n\nDocFormerBase\nFUNSD\n0.8076\n0.8609\n0.8334\n183M\n\nSelfDoc\nFUNSD\n-\n-\n0.8336\n-\n\nLayoutLMv2Large\nFUNSD\n0.8324\n0.8519\n0.8420\n426M\n\nBROSLarge\nFUNSD\n0.8281\n0.8631\n0.8452\n340M\n\nDocFormerLarge\nFUNSD\n0.8229\n0.8694\n0.8455\n536M\n\nFormNet\nFUNSD\n0.8521\n0.8418\n0.8469\n217M\n\nStructuralLMLarge\nFUNSD\n0.8352\n0.8681\n0.8514\n355M\n\nUDoc\nFUNSD\n-\n-\n0.8720\n274M\n\nDocFormerv2Base\nFUNSD\n0.8915\n0.876\n0.8837\n232M\n\nDocFormerv2Large\nFUNSD\n0.8988\n0.8792\n0.8889\n750M\n\nStrucTexTv2Small\nFUNSD\n-\n-\n0.8923\n107M\n\nFast-StrucTexTLinear\nFUNSD\n-\n-\n0.8950\n-\n\nLayoutLMv3Base\nFUNSD\n-\n-\n0.9029\n133M\n\nFast-StrucTexTResNet-18\nFUNSD\n-\n-\n0.9035\n-\n\nStrucTexTv2Large\nFUNSD\n-\n-\n0.9182\n238M\n\nLayoutLMv3Large\nFUNSD\n-\n-\n0.9208\n368M\n\nMCLRLarge\nFUNSD\n-\n-\n0.9352\n368M\n\nMCLRBase\nFUNSD\n-\n-\n0.9376\n133M\n\nDiTBase\nFUNSD\n0.9470\n0.9307\n0.9388\n87M\n\nDiTLarge\nFUNSD\n0.9452\n0.9336\n0.9393\n304M\nThe FUNSD dataset, renowned for form understanding tasks, serves as a crucial benchmark for evaluating various models\u2019 performance. A comprehensive analysis of the comparison results presented in Table 3  ###reference_### yields valuable insights into the models\u2019 abilities. Notably, the transition from LayoutLM to LayoutLMv2 highlights the impact of introducing spatial-aware self-attention, leading to improved F1 scores. Noteworthy performance is observed from LayoutLMv3 BASE, which outperforms its predecessors with an impressive F1 score of 0.9029. StructuralLM LARGE stands out with an F1 score of 0.8514, indicating the success of its approach. StrucTexTv2 LARGE, despite its lower parameter count, achieves a competitive F1 score of 0.9182, demonstrating its efficiency in form understanding tasks. Additionally, DiT BASE and DiT LARGE showcase promising results with F1 scores of 0.9388 and 0.9393, respectively."
        },
        {
            "section_id": "6.2.2",
            "parent_section_id": "6.2",
            "section_name": "6.2.2 SROIE Dataset",
            "text": "Model\nDataset\nPrecision\nRecall\nF1\n#Params\n\n\n\nLayoutLMBase\nSROIE\n0.9438\n0.9438\n0.9438\n113M\n\nLayoutLMLarge\nSROIE\n0.9524\n0.9524\n0.9524\n343M\n\nPICK\nSROIE\n0.9679\n0.9546\n0.9612\n-\n\nTRIE\nSROIE\n-\n-\n0.9618\n-\n\nLayoutLMv2Base\nSROIE\n0.9625\n0.9625\n0.9625\n200M\n\nLayoutLMv2Large\nSROIE\n0.9661\n0.9661\n0.9661\n426M\n\nStrucText\nSROIE\n0.9584\n0.9852\n0.9688\n107M\n\nFast-StrucTexTLinear\nSROIE\n-\n-\n0.9712\n-\n\nFast-StrucTexTResNet-18\nSROIE\n-\n-\n0.9755\n-\n\nTILTBase\nSROIE\n-\n-\n0.9765\n230M\n\nTILTLarge\nSROIE\n-\n-\n0.9810\n780M\nThe evaluation of various models on the SROIE dataset, which is centered around receipt understanding and key-information extraction tasks, provides insights into the capabilities of these models for form-based understanding. Table 4  ###reference_### presents a comprehensive comparison of the performance achieved by different models on this dataset.\nLayoutLMv2 BASE and LayoutLMv2 LARGE demonstrate their prowess in the receipt understanding task. Both models achieve high precision, recall, and F1 scores of 0.9625 and 0.9661 respectively. StrucText exhibits strong recall with a score of 0.9852, showcasing its effectiveness in capturing key information from receipts. Its F1 score of 0.9688 indicates a balanced trade-off between precision and recall, making it a viable option for the SROIE dataset."
        },
        {
            "section_id": "6.2.3",
            "parent_section_id": "6.2",
            "section_name": "6.2.3 Publaynet Dataset",
            "text": "The PubLayNet dataset, despite its primary focus on advancing document layout analysis, has garnered attention from researchers in the field of form understanding due to its extensive annotations and diverse scientific content. As showcased in Table 5  ###reference_###, various models have been evaluated on the PubLayNet dataset. Among the evaluated models, LayoutLMv3 Base stands out with a high Mean Average Precision (MAP) of 0.9510. This suggests that the model\u2019s spatial-aware self-attention mechanism and architecture contribute to effectively capturing layout information, even in the context of intricate scientific documents. Similarly, DiT LARGE showcases promising results with a MAP of 0.9490, demonstrating its capability in processing diverse document layouts for form understanding. GenDoc Base, while not specifically designed for layout analysis, still achieves a competitive MAP of 0.9440, indicating its generalizability to document understanding tasks. UDoc also demonstrates its effectiveness in this domain, achieving a MAP of 0.9390. StrucTexTv2 models, operating at different resolutions (224x224, 512x512, and 960x960), exhibit varying levels of performance, with the 960x960 variant achieving the highest MAP of 0.9490."
        },
        {
            "section_id": "6.2.4",
            "parent_section_id": "6.2",
            "section_name": "6.2.4 DocVQA Dataset",
            "text": "The evaluation of various models on the DocVQA dataset provides insights into their effectiveness in addressing the challenges posed by document-based visual question answering. Table 6  ###reference_### presents a comprehensive comparison of the performance achieved by different models on this dataset. StructuralLM LARGE stands out as a strong performer, boasting an analysis score of . This achievement underscores the model\u2019s ability to integrate structural information from documents and effectively answer questions based on visual and textual cues. Both LayoutLMv3 Large and LayoutLMv2 Large demonstrate their prowess in document understanding, achieving scores of  and  respectively. These models leverage advanced self-attention mechanisms to capture contextual and spatial relationships in documents, enabling them to tackle complex questions that involve both textual and visual components."
        },
        {
            "section_id": "6.2.5",
            "parent_section_id": "6.2",
            "section_name": "6.2.5 RVL-CDIP Dataset",
            "text": "The RVL-CDIP dataset offers a distinct dimension in the realm of form understanding, featuring multi-class single-label classification tasks. The comprehensive model comparison presented in Table 7  ###reference_### provides valuable insights into various models\u2019 performances. Among these models, the DocFormer architecture emerges as a standout performer, particularly DocFormer Base, which attains an exceptional accuracy score of 0.9617, positioning itself at the forefront. This accomplishment underscores the efficacy of the DocFormer approach in tackling the challenges posed by multi-class classification tasks within the form understanding domain.\nA noteworthy observation arises from the juxtaposition of DocFormer LARGE and DocFormer BASE, accentuating the interplay between model complexity and performance enhancement. Although DocFormer LARGE achieves marginally lower accuracy than its BASE counterpart, it does so with a notable increase in the number of model parameters. This trade-off underlines the nuanced relationship between model size and the achieved accuracy, showcasing the balance that must be struck when considering practical deployment and computational efficiency.\nMoreover, this dataset comparison affirms the competitive capabilities of other models, such as StructuralLM LARGE, LayoutLMv3, LayoutLMv2, and StrucTexTv2 LARGE, each contributing to the diverse landscape of form understanding. Furthermore, the inclusion of various baseline models like BEiT and DiT provides a comprehensive perspective on the state-of-the-art advancements in multi-class classification on the RVL-CDIP dataset."
        },
        {
            "section_id": "6.2.6",
            "parent_section_id": "6.2",
            "section_name": "6.2.6 CORD Dataset",
            "text": "Model\nDataset\nPrecision\nRecall\nF1\n#Params\n\n\n\nLayoutLMBase\nCORD\n0.9437\n0.9508\n0.9472\n113M\n\nLayoutLMLarge\nCORD\n0.9432\n0.9554\n0.9493\n343M\n\nLayoutLMv2Base\nCORD\n0.9453\n0.9539\n0.9495\n200M\n\nTILTBase\nCORD\n-\n-\n0.9511\n230M\n\nLayoutLMv2Large\nCORD\n0.9565\n0.9637\n0.9601\n426M\n\nDocFormerBase\nCORD\n0.9652\n0.9614\n0.9633\n183M\n\nTILTLarge\nCORD\n-\n-\n0.9633\n780M\n\nLayoutLMv3Base\nCORD\n-\n-\n0.9656\n133M\n\nGenDocBase\nCORD\n-\n-\n0.9659\n-\n\nFast-StrucTexTLinear\nCORD\n-\n-\n0.9665\n-\n\nDocFormerv2Base\nCORD\n0.9751\n0.9610\n0.9680\n232M\n\nGenDocLarge\nCORD\n-\n-\n0.9697\n-\n\nDocFormerLarge\nCORD\n0.9725\n0.9674\n0.9699\n536M\n\nFast-StrucTexTResNet-18\nCORD\n-\n-\n0.9715\n-\n\nMCLRBase\nCORD\n-\n-\n0.9723\n133M\n\nFormNet\nCORD\n0.9802\n0.9655\n0.9728\n345M\n\nLayoutLMv3Large\nCORD\n-\n-\n0.9746\n368M\n\nMCLRBase\nCORD\n-\n-\n0.9749\n368M\n\nDocFormerv2Large\nCORD\n0.9771\n0.9770\n0.9770\n750M\n\nUDocLarge\nCORD\n-\n-\n0.9813\n274M\nWhile the CORD dataset is more oriented towards receipt understanding, The models demonstrate a range of strengths in capturing different aspects of the documents, including textual content, spatial arrangement, and overall structure. A comprehensive analysis of the comparison results presented in Table 8  ###reference_###. Notably, the LayoutLM variants, LayoutLM BASE and LayoutLM LARGE exhibit strong performance with F1 scores of 0.9472 and 0.9493, respectively, benefiting from their layout-aware self-attention mechanisms. DocFormer models, particularly DocFormer BASE and DocFormer LARGE, excel in capturing complex structural patterns and linguistic context, achieving impressive F1 scores of 0.9633 and 0.9699. The LayoutLMv2 models also perform well, with LayoutLMv2 LARGE achieving an F1 score of 0.9601. The Fast-StrucTexT models, including Fast-StrucTexT Linear and Fast-StrucTexT ResNet-18, demonstrate solid performance with F1 scores of 0.9665 and 0.9715, respectively, indicating the efficiency of their hourglass transformer architecture. UDoc LARGE emerges as a top performer, achieving an impressive F1 score of 0.9813, showcasing the effectiveness of its gated cross-attention mechanism in capturing both textual and spatial information. Models like TILT Base and GenDoc Base also show competitive performance, highlighting their potential in addressing document understanding challenges. MCLR base and MCLR base achieve strong F1 scores of 0.9723 and 0.9749, respectively, emphasizing the success of their cell-based layout representation. FormNet achieves a high accuracy of 0.9728, showcasing its proficiency in extracting information from complex form-like documents."
        },
        {
            "section_id": "6.2.7",
            "parent_section_id": "6.2",
            "section_name": "6.2.7 Overall Insights",
            "text": "We explored different datasets to understand various forms, along with comparisons of the models used, which have provided insights into the current state of this field. Through this analysis, we were able to uncover both opportunities and challenges that extend beyond individual instances, shedding light on the broader landscape of form understanding. A key observation is the rich diversity of approaches employed to tackle the form understanding tasks, as revealed by model comparisons across different datasets. Notably, the consistent performance of LayoutLMv2 models highlights a wide range of spatial-aware self-attention mechanisms. One standout during these comparisons is the diversity of using DocFormer models across various datasets. Their consistent ability to deliver competitive results underscores their potential to adapt and excel across of form understanding tasks. This adaptability positions the DocFormer models as a strong contender for addressing a range of document-related challenges.\nThe balance between model complexity, quantified by the number of parameters, and the resulting performance gains. While larger models tend to yield slightly improved results, the considerable increase in resource requirements highlights the need for thoughtful evaluation of the trade-offs between computational demands and performance enhancements when selecting models for specific tasks. The distinct focus of each dataset opens up avenues for the development of specialized models. For example, the SROIE dataset, centered around receipts, showcases how models like TILT Large can excel in context-specific scenarios. This underscores the potential for efficient form understanding even in domain-specific contexts. The real-world datasets introduced challenges inherent to complex and unstructured data, such as noise, layout variations, and diverse document formats. These hurdles underscore the necessity for models that can robustly handle such complexities, emphasizing the importance of building solutions that are both resilient and adaptable."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In this paper, we provided an extensive overview of modern approaches for form understanding. We showed that current state-of-the-art models are all based on transformer models, which makes them easy to use and adapt to a variety of tasks due to the pre-training / fine-tuning approach. MVLM is used by 6 models (LayoutLM, UniLMv2, LayoutLMv2, StructuralLM, StrucText, BROS). All of the presented models used multiple document modalities. The textual baseline transformer is always extended with text-position information on either word, sentence, or text-segment level. Especially the relative position encoding used by the BROS model seems promising. The layout information, which is provided by extracting text-image features, also seems crucial for good performance, as shown by the performance increase of LayoutLMv2 compared to its predecessor."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>An overview of the covered models in terms of their used modalities.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.1\" style=\"width:216.8pt;height:301.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-36.5pt,50.7pt) scale(0.74819,0.74819) ;\">\n<p class=\"ltx_p\" id=\"S4.T1.1.1\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.1.1.1.1\" style=\"width:289.8pt;height:403pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1.1.1.1\" style=\"color:#000000;\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1.1.1.1.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_th ltx_th_column ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.1.1.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_colspan ltx_colspan_4\" id=\"S4.T1.1.1.1.1.1.1.1.1.1.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.1.1.1.1.1.2.1\">Modalities</span></span>\n<span class=\"ltx_td ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.1.1.1.1.1.1.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.2.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Model</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.2.1\" style=\"width:6.8pt;height:23.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:23.3pt;transform:translate(-8.25pt,-8.25pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.2.1.1\">Text</span>\n</span></span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.3.1\" style=\"width:6.8pt;height:37.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:37.6pt;transform:translate(-15.36pt,-15.36pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.3.1.1\">1D Pos.</span>\n</span></span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.4.1\" style=\"width:6.8pt;height:37.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:37.6pt;transform:translate(-15.36pt,-15.36pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.4.1.1\">2D Pos.</span>\n</span></span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.5.1\" style=\"width:8.8pt;height:29.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:29.7pt;transform:translate(-10.47pt,-9.5pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.5.1.1\">Image</span>\n</span></span></span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.2.2.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Approach type</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.3.1\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.3.1.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">BERT</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.3.1.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.3.1.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.3.1.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></span>\n<span class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.3.1.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></span>\n<span class=\"ltx_td ltx_border_t\" id=\"S4.T1.1.1.1.1.1.1.1.3.1.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.4.2\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.4.2.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LayoutLM</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.4.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.4.2.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.4.2.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.4.2.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.4.2.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">image-based fine-tuning</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.5.3\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.5.3.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">UniLMv2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.5.3.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.5.3.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.5.3.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.5.3.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.5.3.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">seq2seq</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.6.4\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.6.4.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">PICK</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.6.4.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.6.4.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.6.4.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.6.4.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.6.4.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">graph-learning</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.7.5\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.7.5.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">TRIE</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.7.5.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.7.5.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.7.5.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.7.5.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.7.5.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">multi-task (OCR + IE)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.8.6\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.8.6.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LayoutLMv2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.8.6.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.8.6.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.8.6.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.8.6.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.8.6.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">spatial-aware self-attention</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.9.7\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.9.7.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">StructuralLM</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.9.7.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.9.7.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.9.7.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.9.7.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.9.7.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">text-segment grouping</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.10.8\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.10.8.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SelfDoc</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.10.8.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.10.8.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.10.8.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.10.8.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.10.8.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">text-segment grouping</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.11.9\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.11.9.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DocFormer</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.11.9.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.11.9.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.11.9.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.11.9.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.11.9.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">discrete multi-modal</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.12.10\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.12.10.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">StrucText</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.12.10.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.12.10.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.12.10.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.12.10.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.12.10.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">text-segment grouping</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.13.11\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.13.11.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">BROS</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.13.11.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.13.11.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.13.11.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.13.11.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.13.11.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">using relative 2D positions</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.14.12\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.14.12.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">StrucTexTv2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.14.12.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.14.12.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.14.12.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.14.12.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.14.12.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">multi-modal fusion</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.15.13\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.15.13.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Fast-StrucTexT</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.15.13.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.15.13.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.15.13.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.15.13.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.15.13.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">efficient transformer</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.16.14\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.16.14.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DocFormerv2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.16.14.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.16.14.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.16.14.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.16.14.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.16.14.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">multi-modal encoder</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.17.15\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.17.15.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">UDoc</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.17.15.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.17.15.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.17.15.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.17.15.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.17.15.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">gated cross-attention</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.18.16\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.18.16.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">TILT</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.18.16.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.18.16.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.18.16.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.18.16.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.18.16.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text-Image-Layout Transformer</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.19.17\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.19.17.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">GenDoc</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.19.17.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.19.17.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.19.17.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.19.17.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.19.17.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">modality fusion, disentangled attention</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.20.18\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.20.18.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MCLR</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.20.18.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.20.18.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.20.18.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.20.18.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.20.18.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">cell-based layout, multi-scale layout</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.1.1.1.1.1.1.1.21.19\">\n<span class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.21.19.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LiLT</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.21.19.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.21.19.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.21.19.4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T1.1.1.1.1.1.1.1.21.19.5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\u2713</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.1.1.1.1.1.1.21.19.6\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">language-independent layout</span></span>\n</span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n</figure>",
            "capture": "Table 1: An overview of the covered models in terms of their used modalities."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Comparison between individual datasets. All listed datasets are publicly available.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T2.9\" style=\"width:677.0pt;height:217pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<p class=\"ltx_p\" id=\"S5.T2.9.9\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.9.9.9.9\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S5.T2.9.9.9.9.10.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.9.9.9.9.10.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.9.9.9.9.10.1.1.1\" style=\"color:#000000;\">Dataset</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.9.9.9.9.10.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.9.9.9.9.10.1.2.1\" style=\"color:#000000;\">Document Content</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.9.9.9.9.10.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.9.9.9.9.10.1.3.1\" style=\"color:#000000;\">Color</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.9.9.9.9.10.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.9.9.9.9.10.1.4.1\" style=\"color:#000000;\">Used Languages</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.9.9.9.9.10.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.9.9.9.9.10.1.5.1\" style=\"color:#000000;\">#Documents</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.9.9.9.9.10.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.9.9.9.9.10.1.6.1\" style=\"color:#000000;\">Document Type</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S5.T2.9.9.9.9.11.1\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.9.9.9.9.11.1.1\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.11.1.1.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.9.9.9.9.11.1.2\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.11.1.2.1\" style=\"color:#000000;\">Machine Written / Handwritten</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.9.9.9.9.11.1.3\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.11.1.3.1\" style=\"color:#000000;\">B/W</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.9.9.9.9.11.1.4\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.11.1.4.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.9.9.9.9.11.1.5\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.11.1.5.1\" style=\"color:#000000;\">199</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.9.9.9.9.11.1.6\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.11.1.6.1\" style=\"color:#000000;\">Form Documents</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.9.9.9.9.12.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.9.9.9.9.12.2.1\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.12.2.1.1\" style=\"color:#000000;\">XFUND</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.12.2.2\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.12.2.2.1\" style=\"color:#000000;\">Machine Written / Handwritten</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.12.2.3\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.12.2.3.1\" style=\"color:#000000;\">B/W</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.12.2.4\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.12.2.4.1\" style=\"color:#000000;\">Multi-lingual</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.12.2.5\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.12.2.5.1\" style=\"color:#000000;\">199/language</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.12.2.6\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.12.2.6.1\" style=\"color:#000000;\">Form Documents</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.9.9.9.9.13.3\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.9.9.9.9.13.3.1\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.13.3.1.1\" style=\"color:#000000;\">NAF</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.13.3.2\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.13.3.2.1\" style=\"color:#000000;\">Machine written pre-printed text and mostly handwritten input text</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.13.3.3\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.13.3.3.1\" style=\"color:#000000;\">B/W</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.13.3.4\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.13.3.4.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.13.3.5\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.13.3.5.1\" style=\"color:#000000;\">865</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.9.9.9.9.13.3.6\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.13.3.6.1\" style=\"color:#000000;\">Multiple</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.1.1.1.2\"><span class=\"ltx_text\" id=\"S5.T2.1.1.1.1.1.2.1\" style=\"color:#000000;\">IIT-CDIP</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.1.1.1.3\"><span class=\"ltx_text\" id=\"S5.T2.1.1.1.1.1.3.1\" style=\"color:#000000;\">Machine Written, Handwritten</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.1.1.1.4\"><span class=\"ltx_text\" id=\"S5.T2.1.1.1.1.1.4.1\" style=\"color:#000000;\">B/W</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.1.1.1.5\"><span class=\"ltx_text\" id=\"S5.T2.1.1.1.1.1.5.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.1.1.1.1\"></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.1.1.1.6\"><span class=\"ltx_text\" id=\"S5.T2.1.1.1.1.1.6.1\" style=\"color:#000000;\">Multiple</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.2.2.2.2.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.2.2.2.2.2\"><span class=\"ltx_text\" id=\"S5.T2.2.2.2.2.2.2.1\" style=\"color:#000000;\">RVL-CDIP</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.2.2.3\"><span class=\"ltx_text\" id=\"S5.T2.2.2.2.2.2.3.1\" style=\"color:#000000;\">Machine Written, Handwritten</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.2.2.4\"><span class=\"ltx_text\" id=\"S5.T2.2.2.2.2.2.4.1\" style=\"color:#000000;\">B/W</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.2.2.5\"><span class=\"ltx_text\" id=\"S5.T2.2.2.2.2.2.5.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.2.2.1\"></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.2.2.2.2.6\"><span class=\"ltx_text\" id=\"S5.T2.2.2.2.2.2.6.1\" style=\"color:#000000;\">Multiple</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.3.3.3.3.3\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.3.3.3.3.3.2\"><span class=\"ltx_text\" id=\"S5.T2.3.3.3.3.3.2.1\" style=\"color:#000000;\">PubLayNet</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.3.3.3.3.3\"><span class=\"ltx_text\" id=\"S5.T2.3.3.3.3.3.3.1\" style=\"color:#000000;\">Machine Written</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.3.3.3.3.4\"><span class=\"ltx_text\" id=\"S5.T2.3.3.3.3.3.4.1\" style=\"color:#000000;\">Color</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.3.3.3.3.5\"><span class=\"ltx_text\" id=\"S5.T2.3.3.3.3.3.5.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.3.3.3.3.1\"></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.3.3.3.3.6\"><span class=\"ltx_text\" id=\"S5.T2.3.3.3.3.3.6.1\" style=\"color:#000000;\">Medical Literature</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.4.4.4.4.4\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.4.4.4.4.4.2\"><span class=\"ltx_text\" id=\"S5.T2.4.4.4.4.4.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.4.4.4.4.3\"><span class=\"ltx_text\" id=\"S5.T2.4.4.4.4.4.3.1\" style=\"color:#000000;\">Machine Written with occasional handwriting</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.4.4.4.4.4\"><span class=\"ltx_text\" id=\"S5.T2.4.4.4.4.4.4.1\" style=\"color:#000000;\">Color, B/W</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.4.4.4.4.5\"><span class=\"ltx_text\" id=\"S5.T2.4.4.4.4.4.5.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.4.4.4.4.1\"></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.4.4.4.4.6\"><span class=\"ltx_text\" id=\"S5.T2.4.4.4.4.4.6.1\" style=\"color:#000000;\">Receipts</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.6.6.6.6.6\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.6.6.6.6.6.3\"><span class=\"ltx_text\" id=\"S5.T2.6.6.6.6.6.3.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.6.6.6.6.6.4\"><span class=\"ltx_text\" id=\"S5.T2.6.6.6.6.6.4.1\" style=\"color:#000000;\">Machine Written with occasional handwriting</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.6.6.6.6.6.5\"><span class=\"ltx_text\" id=\"S5.T2.6.6.6.6.6.5.1\" style=\"color:#000000;\">Color</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.6.6.6.6.6.6\"><span class=\"ltx_text\" id=\"S5.T2.6.6.6.6.6.6.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.6.6.6.6.6.2\"><span class=\"ltx_text\" id=\"S5.T2.6.6.6.6.6.2.1\" style=\"color:#000000;\"> released, </span><span class=\"ltx_text\" id=\"S5.T2.6.6.6.6.6.2.2\" style=\"color:#000000;\"> total</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.6.6.6.6.6.7\"><span class=\"ltx_text\" id=\"S5.T2.6.6.6.6.6.7.1\" style=\"color:#000000;\">Receipts</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.7.7.7.7.7\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.7.7.7.7.7.2\"><span class=\"ltx_text\" id=\"S5.T2.7.7.7.7.7.2.1\" style=\"color:#000000;\">DocVQA</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.7.7.7.7.7.3\"><span class=\"ltx_text\" id=\"S5.T2.7.7.7.7.7.3.1\" style=\"color:#000000;\">Machine Written, Handwritten</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.7.7.7.7.7.4\"><span class=\"ltx_text\" id=\"S5.T2.7.7.7.7.7.4.1\" style=\"color:#000000;\">Color, B/W</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.7.7.7.7.7.5\"><span class=\"ltx_text\" id=\"S5.T2.7.7.7.7.7.5.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.7.7.7.7.7.1\"></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.7.7.7.7.7.6\"><span class=\"ltx_text\" id=\"S5.T2.7.7.7.7.7.6.1\" style=\"color:#000000;\">Multiple</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.8.8.8.8.8\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T2.8.8.8.8.8.2\"><span class=\"ltx_text\" id=\"S5.T2.8.8.8.8.8.2.1\" style=\"color:#000000;\">Form-NLU</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.8.8.8.8.8.3\"><span class=\"ltx_text\" id=\"S5.T2.8.8.8.8.8.3.1\" style=\"color:#000000;\">Machine Written, Handwritten</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.8.8.8.8.8.4\"><span class=\"ltx_text\" id=\"S5.T2.8.8.8.8.8.4.1\" style=\"color:#000000;\">Color, B/W</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.8.8.8.8.8.5\"><span class=\"ltx_text\" id=\"S5.T2.8.8.8.8.8.5.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.8.8.8.8.8.1\"></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.8.8.8.8.8.6\"><span class=\"ltx_text\" id=\"S5.T2.8.8.8.8.8.6.1\" style=\"color:#000000;\">Multiple</span></span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.9.9.9.9.9\">\n<span class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T2.9.9.9.9.9.2\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.9.2.1\" style=\"color:#000000;\">VRDU</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.9.9.9.9.9.3\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.9.3.1\" style=\"color:#000000;\">Machine Written</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.9.9.9.9.9.4\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.9.4.1\" style=\"color:#000000;\">Color, B/W</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.9.9.9.9.9.5\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.9.5.1\" style=\"color:#000000;\">EN</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.9.9.9.9.9.1\"></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T2.9.9.9.9.9.6\"><span class=\"ltx_text\" id=\"S5.T2.9.9.9.9.9.6.1\" style=\"color:#000000;\">Ad-buy</span></span></span>\n</span>\n</span><span class=\"ltx_text\" id=\"S5.T2.9.9.9.10\" style=\"color:#000000;\"></span></span></p>\n</span></div>\n</figure>",
            "capture": "Table 2: Comparison between individual datasets. All listed datasets are publicly available."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Performance Comparison of Document Layout Analysis Models on the FUNSD Dataset.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T3.1\" style=\"width:287.8pt;height:468pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<p class=\"ltx_p\" id=\"S6.T3.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S6.T3.1.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S6.T3.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.1.1.1.1\" style=\"color:#000000;\">Model</span><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.1.1.1.2\" style=\"color:#000000;\"></span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T3.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.1.1.2.1\" style=\"color:#000000;\">Dataset</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T3.1.1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.1.1.3.1\" style=\"color:#000000;\">Precision</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T3.1.1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.1.1.4.1\" style=\"color:#000000;\">Recall</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T3.1.1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.1.1.5.1\" style=\"color:#000000;\">F1</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T3.1.1.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.1.1.6.1\" style=\"color:#000000;\">#Params</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.2.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T3.1.1.1.1.2.1.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.2.1.1.1\" style=\"color:#000000;\">LayoutLM</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.2.1.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.2.1.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.1.1.2.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.2.1.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.1.1.2.1.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.2.1.3.1\" style=\"color:#000000;\">0.7597</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.1.1.2.1.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.2.1.4.1\" style=\"color:#000000;\">0.8155</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.1.1.2.1.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.2.1.5.1\" style=\"color:#000000;\">0.7866</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.1.1.1.2.1.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.2.1.6.1\" style=\"color:#000000;\">113M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.3.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.3.2.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.3.2.1.1\" style=\"color:#000000;\">LayoutLM</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.3.2.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.3.2.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.3.2.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.3.2.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.3.2.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.3.2.3.1\" style=\"color:#000000;\">0.7596</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.3.2.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.3.2.4.1\" style=\"color:#000000;\">0.8219</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.3.2.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.3.2.5.1\" style=\"color:#000000;\">0.7895</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.3.2.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.3.2.6.1\" style=\"color:#000000;\">343M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.4.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.4.3.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.4.3.1.1\" style=\"color:#000000;\">LayoutLMv2</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.4.3.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.4.3.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.4.3.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.4.3.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.4.3.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.4.3.3.1\" style=\"color:#000000;\">0.8029</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.4.3.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.4.3.4.1\" style=\"color:#000000;\">0.8539</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.4.3.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.4.3.5.1\" style=\"color:#000000;\">0.8276</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.4.3.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.4.3.6.1\" style=\"color:#000000;\">200M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.5.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.5.4.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.5.4.1.1\" style=\"color:#000000;\">BROS</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.5.4.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.5.4.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.5.4.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.5.4.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.5.4.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.5.4.3.1\" style=\"color:#000000;\">0.8116</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.5.4.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.5.4.4.1\" style=\"color:#000000;\">0.8502</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.5.4.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.5.4.5.1\" style=\"color:#000000;\">0.8305</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.5.4.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.5.4.6.1\" style=\"color:#000000;\">110M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.6.5\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.6.5.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.6.5.1.1\" style=\"color:#000000;\">StrucText</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.6.5.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.6.5.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.6.5.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.6.5.3.1\" style=\"color:#000000;\">0.8568</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.6.5.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.6.5.4.1\" style=\"color:#000000;\">0.8097</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.6.5.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.6.5.5.1\" style=\"color:#000000;\">0.8309</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.6.5.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.6.5.6.1\" style=\"color:#000000;\">107M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.7.6\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.7.6.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.7.6.1.1\" style=\"color:#000000;\">DocFormer</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.7.6.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.7.6.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.7.6.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.7.6.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.7.6.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.7.6.3.1\" style=\"color:#000000;\">0.8076</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.7.6.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.7.6.4.1\" style=\"color:#000000;\">0.8609</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.7.6.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.7.6.5.1\" style=\"color:#000000;\">0.8334</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.7.6.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.7.6.6.1\" style=\"color:#000000;\">183M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.8.7\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.8.7.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.8.7.1.1\" style=\"color:#000000;\">SelfDoc</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.8.7.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.8.7.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.8.7.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.8.7.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.8.7.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.8.7.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.8.7.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.8.7.5.1\" style=\"color:#000000;\">0.8336</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.8.7.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.8.7.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.9.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.9.8.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.9.8.1.1\" style=\"color:#000000;\">LayoutLMv2</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.9.8.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.9.8.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.9.8.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.9.8.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.9.8.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.9.8.3.1\" style=\"color:#000000;\">0.8324</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.9.8.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.9.8.4.1\" style=\"color:#000000;\">0.8519</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.9.8.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.9.8.5.1\" style=\"color:#000000;\">0.8420</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.9.8.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.9.8.6.1\" style=\"color:#000000;\">426M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.10.9\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.10.9.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.10.9.1.1\" style=\"color:#000000;\">BROS</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.10.9.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.10.9.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.10.9.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.10.9.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.10.9.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.10.9.3.1\" style=\"color:#000000;\">0.8281</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.10.9.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.10.9.4.1\" style=\"color:#000000;\">0.8631</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.10.9.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.10.9.5.1\" style=\"color:#000000;\">0.8452</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.10.9.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.10.9.6.1\" style=\"color:#000000;\">340M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.11.10\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.11.10.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.11.10.1.1\" style=\"color:#000000;\">DocFormer</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.11.10.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.11.10.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.11.10.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.11.10.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.11.10.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.11.10.3.1\" style=\"color:#000000;\">0.8229</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.11.10.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.11.10.4.1\" style=\"color:#000000;\">0.8694</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.11.10.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.11.10.5.1\" style=\"color:#000000;\">0.8455</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.11.10.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.11.10.6.1\" style=\"color:#000000;\">536M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.12.11\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.12.11.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.12.11.1.1\" style=\"color:#000000;\">FormNet</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.12.11.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.12.11.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.12.11.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.12.11.3.1\" style=\"color:#000000;\">0.8521</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.12.11.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.12.11.4.1\" style=\"color:#000000;\">0.8418</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.12.11.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.12.11.5.1\" style=\"color:#000000;\">0.8469</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.12.11.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.12.11.6.1\" style=\"color:#000000;\">217M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.13.12\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.13.12.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.13.12.1.1\" style=\"color:#000000;\">StructuralLM</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.13.12.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.13.12.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.13.12.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.13.12.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.13.12.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.13.12.3.1\" style=\"color:#000000;\">0.8352</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.13.12.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.13.12.4.1\" style=\"color:#000000;\">0.8681</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.13.12.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.13.12.5.1\" style=\"color:#000000;\">0.8514</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.13.12.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.13.12.6.1\" style=\"color:#000000;\">355M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.14.13\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.14.13.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.14.13.1.1\" style=\"color:#000000;\">UDoc</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.14.13.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.14.13.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.14.13.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.14.13.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.14.13.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.14.13.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.14.13.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.14.13.5.1\" style=\"color:#000000;\">0.8720</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.14.13.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.14.13.6.1\" style=\"color:#000000;\">274M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.15.14\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.15.14.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.15.14.1.1\" style=\"color:#000000;\">DocFormerv2</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.15.14.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.15.14.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.15.14.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.15.14.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.15.14.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.15.14.3.1\" style=\"color:#000000;\">0.8915</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.15.14.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.15.14.4.1\" style=\"color:#000000;\">0.876</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.15.14.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.15.14.5.1\" style=\"color:#000000;\">0.8837</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.15.14.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.15.14.6.1\" style=\"color:#000000;\">232M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.16.15\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.16.15.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.16.15.1.1\" style=\"color:#000000;\">DocFormerv2</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.16.15.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.16.15.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.16.15.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.16.15.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.16.15.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.16.15.3.1\" style=\"color:#000000;\">0.8988</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.16.15.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.16.15.4.1\" style=\"color:#000000;\">0.8792</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.16.15.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.16.15.5.1\" style=\"color:#000000;\">0.8889</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.16.15.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.16.15.6.1\" style=\"color:#000000;\">750M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.17.16\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.17.16.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.17.16.1.1\" style=\"color:#000000;\">StrucTexTv2</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.17.16.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.17.16.1.2.1\" style=\"color:#000000;\">Small</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.17.16.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.17.16.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.17.16.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.17.16.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.17.16.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.17.16.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.17.16.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.17.16.5.1\" style=\"color:#000000;\">0.8923</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.17.16.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.17.16.6.1\" style=\"color:#000000;\">107M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.18.17\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.18.17.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.18.17.1.1\" style=\"color:#000000;\">Fast-StrucTexT</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.18.17.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.18.17.1.2.1\" style=\"color:#000000;\">Linear</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.18.17.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.18.17.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.18.17.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.18.17.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.18.17.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.18.17.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.18.17.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.18.17.5.1\" style=\"color:#000000;\">0.8950</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.18.17.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.18.17.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.19.18\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.19.18.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.19.18.1.1\" style=\"color:#000000;\">LayoutLMv3</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.19.18.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.19.18.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.19.18.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.19.18.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.19.18.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.19.18.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.19.18.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.19.18.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.19.18.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.19.18.5.1\" style=\"color:#000000;\">0.9029</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.19.18.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.19.18.6.1\" style=\"color:#000000;\">133M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.20.19\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.20.19.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.20.19.1.1\" style=\"color:#000000;\">Fast-StrucTexT</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.20.19.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.20.19.1.2.1\" style=\"color:#000000;\">ResNet-18</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.20.19.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.20.19.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.20.19.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.20.19.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.20.19.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.20.19.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.20.19.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.20.19.5.1\" style=\"color:#000000;\">0.9035</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.20.19.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.20.19.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.21.20\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.21.20.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.21.20.1.1\" style=\"color:#000000;\">StrucTexTv2</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.21.20.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.21.20.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.21.20.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.21.20.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.21.20.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.21.20.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.21.20.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.21.20.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.21.20.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.21.20.5.1\" style=\"color:#000000;\">0.9182</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.21.20.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.21.20.6.1\" style=\"color:#000000;\">238M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.22.21\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.22.21.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.22.21.1.1\" style=\"color:#000000;\">LayoutLMv3</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.22.21.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.22.21.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.22.21.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.22.21.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.22.21.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.22.21.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.22.21.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.22.21.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.22.21.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.22.21.5.1\" style=\"color:#000000;\">0.9208</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.22.21.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.22.21.6.1\" style=\"color:#000000;\">368M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.23.22\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.23.22.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.23.22.1.1\" style=\"color:#000000;\">MCLR</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.23.22.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.23.22.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.23.22.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.23.22.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.23.22.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.23.22.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.23.22.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.23.22.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.23.22.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.23.22.5.1\" style=\"color:#000000;\">0.9352</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.23.22.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.23.22.6.1\" style=\"color:#000000;\">368M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.24.23\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.24.23.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.24.23.1.1\" style=\"color:#000000;\">MCLR</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.24.23.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.24.23.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.24.23.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.24.23.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.24.23.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.24.23.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.24.23.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.24.23.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.24.23.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.24.23.5.1\" style=\"color:#000000;\">0.9376</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.24.23.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.24.23.6.1\" style=\"color:#000000;\">133M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.25.24\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T3.1.1.1.1.25.24.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.25.24.1.1\" style=\"color:#000000;\">DiT</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.25.24.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.25.24.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.25.24.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.25.24.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.25.24.3\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.25.24.3.1\" style=\"color:#000000;\">0.9470</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.25.24.4\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.25.24.4.1\" style=\"color:#000000;\">0.9307</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.25.24.5\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.25.24.5.1\" style=\"color:#000000;\">0.9388</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.1.1.1.25.24.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.25.24.6.1\" style=\"color:#000000;\">87M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T3.1.1.1.1.26.25\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S6.T3.1.1.1.1.26.25.1\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.26.25.1.1\" style=\"color:#000000;\">DiT</span><sub class=\"ltx_sub\" id=\"S6.T3.1.1.1.1.26.25.1.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.26.25.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T3.1.1.1.1.26.25.2\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.26.25.2.1\" style=\"color:#000000;\">FUNSD</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T3.1.1.1.1.26.25.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.26.25.3.1\" style=\"color:#000000;\">0.9452</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T3.1.1.1.1.26.25.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.26.25.4.1\" style=\"color:#000000;\">0.9336</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T3.1.1.1.1.26.25.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.1.1.1.26.25.5.1\" style=\"color:#000000;\">0.9393</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T3.1.1.1.1.26.25.6\"><span class=\"ltx_text\" id=\"S6.T3.1.1.1.1.26.25.6.1\" style=\"color:#000000;\">304M</span></span></span>\n</span>\n</span><span class=\"ltx_text\" id=\"S6.T3.1.1.1.2\" style=\"color:#000000;\"></span></span></p>\n</span></div>\n</figure>",
            "capture": "Table 3: Performance Comparison of Document Layout Analysis Models on the FUNSD Dataset."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Performance Comparison of Document Layout Analysis Models on SROIE Dataset.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T4.1\" style=\"width:286.9pt;height:216pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<p class=\"ltx_p\" id=\"S6.T4.1.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S6.T4.1.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S6.T4.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.1.1.1.1\" style=\"color:#000000;\">Model</span><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.1.1.1.2\" style=\"color:#000000;\"></span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T4.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.1.1.2.1\" style=\"color:#000000;\">Dataset</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T4.1.1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.1.1.3.1\" style=\"color:#000000;\">Precision</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T4.1.1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.1.1.4.1\" style=\"color:#000000;\">Recall</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T4.1.1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.1.1.5.1\" style=\"color:#000000;\">F1</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T4.1.1.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.1.1.6.1\" style=\"color:#000000;\">#Params</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.2.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T4.1.1.1.1.2.1.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.2.1.1.1\" style=\"color:#000000;\">LayoutLM</span><sub class=\"ltx_sub\" id=\"S6.T4.1.1.1.1.2.1.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.2.1.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.1.1.2.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.2.1.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.1.1.2.1.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.2.1.3.1\" style=\"color:#000000;\">0.9438</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.1.1.2.1.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.2.1.4.1\" style=\"color:#000000;\">0.9438</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.1.1.2.1.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.2.1.5.1\" style=\"color:#000000;\">0.9438</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.1.1.1.1.2.1.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.2.1.6.1\" style=\"color:#000000;\">113M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.3.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.3.2.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.3.2.1.1\" style=\"color:#000000;\">LayoutLM</span><sub class=\"ltx_sub\" id=\"S6.T4.1.1.1.1.3.2.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.3.2.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.3.2.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.3.2.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.3.2.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.3.2.3.1\" style=\"color:#000000;\">0.9524</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.3.2.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.3.2.4.1\" style=\"color:#000000;\">0.9524</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.3.2.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.3.2.5.1\" style=\"color:#000000;\">0.9524</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.3.2.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.3.2.6.1\" style=\"color:#000000;\">343M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.4.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.4.3.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.4.3.1.1\" style=\"color:#000000;\">PICK</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.4.3.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.4.3.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.4.3.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.4.3.3.1\" style=\"color:#000000;\">0.9679</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.4.3.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.4.3.4.1\" style=\"color:#000000;\">0.9546</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.4.3.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.4.3.5.1\" style=\"color:#000000;\">0.9612</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.4.3.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.4.3.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.5.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.5.4.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.5.4.1.1\" style=\"color:#000000;\">TRIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.5.4.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.5.4.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.5.4.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.5.4.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.5.4.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.5.4.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.5.4.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.5.4.5.1\" style=\"color:#000000;\">0.9618</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.5.4.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.5.4.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.6.5\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.6.5.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.6.5.1.1\" style=\"color:#000000;\">LayoutLMv2</span><sub class=\"ltx_sub\" id=\"S6.T4.1.1.1.1.6.5.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.6.5.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.6.5.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.6.5.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.6.5.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.6.5.3.1\" style=\"color:#000000;\">0.9625</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.6.5.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.6.5.4.1\" style=\"color:#000000;\">0.9625</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.6.5.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.6.5.5.1\" style=\"color:#000000;\">0.9625</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.6.5.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.6.5.6.1\" style=\"color:#000000;\">200M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.7.6\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.7.6.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.7.6.1.1\" style=\"color:#000000;\">LayoutLMv2</span><sub class=\"ltx_sub\" id=\"S6.T4.1.1.1.1.7.6.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.7.6.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.7.6.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.7.6.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.7.6.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.7.6.3.1\" style=\"color:#000000;\">0.9661</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.7.6.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.7.6.4.1\" style=\"color:#000000;\">0.9661</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.7.6.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.7.6.5.1\" style=\"color:#000000;\">0.9661</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.7.6.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.7.6.6.1\" style=\"color:#000000;\">426M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.8.7\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.8.7.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.8.7.1.1\" style=\"color:#000000;\">StrucText</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.8.7.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.8.7.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.8.7.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.8.7.3.1\" style=\"color:#000000;\">0.9584</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.8.7.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.8.7.4.1\" style=\"color:#000000;\">0.9852</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.8.7.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.8.7.5.1\" style=\"color:#000000;\">0.9688</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.8.7.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.8.7.6.1\" style=\"color:#000000;\">107M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.9.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.9.8.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.9.8.1.1\" style=\"color:#000000;\">Fast-StrucTexT</span><sub class=\"ltx_sub\" id=\"S6.T4.1.1.1.1.9.8.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.9.8.1.2.1\" style=\"color:#000000;\">Linear</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.9.8.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.9.8.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.9.8.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.9.8.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.9.8.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.9.8.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.9.8.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.9.8.5.1\" style=\"color:#000000;\">0.9712</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.9.8.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.9.8.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.10.9\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.10.9.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.10.9.1.1\" style=\"color:#000000;\">Fast-StrucTexT</span><sub class=\"ltx_sub\" id=\"S6.T4.1.1.1.1.10.9.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.10.9.1.2.1\" style=\"color:#000000;\">ResNet-18</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.10.9.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.10.9.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.10.9.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.10.9.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.10.9.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.10.9.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.10.9.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.10.9.5.1\" style=\"color:#000000;\">0.9755</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.10.9.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.10.9.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.11.10\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T4.1.1.1.1.11.10.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.11.10.1.1\" style=\"color:#000000;\">TILT</span><sub class=\"ltx_sub\" id=\"S6.T4.1.1.1.1.11.10.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.11.10.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.11.10.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.11.10.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.11.10.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.11.10.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.11.10.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.11.10.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.11.10.5\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.11.10.5.1\" style=\"color:#000000;\">0.9765</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T4.1.1.1.1.11.10.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.11.10.6.1\" style=\"color:#000000;\">230M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T4.1.1.1.1.12.11\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S6.T4.1.1.1.1.12.11.1\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.12.11.1.1\" style=\"color:#000000;\">TILT</span><sub class=\"ltx_sub\" id=\"S6.T4.1.1.1.1.12.11.1.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.12.11.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T4.1.1.1.1.12.11.2\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.12.11.2.1\" style=\"color:#000000;\">SROIE</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T4.1.1.1.1.12.11.3\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.12.11.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T4.1.1.1.1.12.11.4\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.12.11.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T4.1.1.1.1.12.11.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T4.1.1.1.1.12.11.5.1\" style=\"color:#000000;\">0.9810</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T4.1.1.1.1.12.11.6\"><span class=\"ltx_text\" id=\"S6.T4.1.1.1.1.12.11.6.1\" style=\"color:#000000;\">780M</span></span></span>\n</span>\n</span><span class=\"ltx_text\" id=\"S6.T4.1.1.1.2\" style=\"color:#000000;\"></span></span></p>\n</span></div>\n</figure>",
            "capture": "Table 4: Performance Comparison of Document Layout Analysis Models on SROIE Dataset."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Performance Comparison of Document Layout Analysis Models on Publaynet Dataset.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S6.T5.1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T5.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.1.1.2.1\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T5.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.1.1.3.1\">MAP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T5.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.1.1.4.1\">#Params</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T5.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T5.1.2.1.1\">StrucTexTv2<sub class=\"ltx_sub\" id=\"S6.T5.1.2.1.1.1\">224*224</sub>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.1.2.1.2\">Publaynet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.1.2.1.3\">0.8590</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.1.2.1.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.1.3.2.1\">StrucTexTv2<sub class=\"ltx_sub\" id=\"S6.T5.1.3.2.1.1\">512*512</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.3.2.2\">Publaynet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.3.2.3\">0.9210</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.3.2.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.1.4.3.1\">UDoc</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.4.3.2\">Publaynet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.4.3.3\">0.9390</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.4.3.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.1.5.4.1\">GenDoc<sub class=\"ltx_sub\" id=\"S6.T5.1.5.4.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.5.4.2\">Publaynet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.5.4.3\">0.9440</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.5.4.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.1.6.5.1\">StrucTexTv2<sub class=\"ltx_sub\" id=\"S6.T5.1.6.5.1.1\">960*960</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.6.5.2\">Publaynet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.6.5.3\">0.9490</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.6.5.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.1.7.6.1\">DiT<sub class=\"ltx_sub\" id=\"S6.T5.1.7.6.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.7.6.2\">Publaynet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.7.6.3\">0.9450</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.7.6.4\">87M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T5.1.8.7.1\">DiT<sub class=\"ltx_sub\" id=\"S6.T5.1.8.7.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.8.7.2\">Publaynet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.8.7.3\">0.9490</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.1.8.7.4\">304M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S6.T5.1.9.8.1\">LayoutLMv3<sub class=\"ltx_sub\" id=\"S6.T5.1.9.8.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T5.1.9.8.2\">Publaynet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T5.1.9.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.9.8.3.1\">0.9510</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T5.1.9.8.4\">133M</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 5: Performance Comparison of Document Layout Analysis Models on Publaynet Dataset."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T6\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Performance Comparison of Document Layout Analysis Models on DocVQA Dataset.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S6.T6.1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T6.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.2.1\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T6.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.3.1\">Anls</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T6.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.4.1\">#Params</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T6.1.2.1.1\">LayoutLMv2<sub class=\"ltx_sub\" id=\"S6.T6.1.2.1.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.2.1.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.2.1.3\">0.7808</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.2.1.4\">200M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.3.2.1\">LayoutLMv3<sub class=\"ltx_sub\" id=\"S6.T6.1.3.2.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.3.2.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.3.2.3\">0.7876</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.3.2.4\">133M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.4.3.1\">GenDoc<sub class=\"ltx_sub\" id=\"S6.T6.1.4.3.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.4.3.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.4.3.3\">0.7883</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.4.3.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.5.4.1\">LayoutLMv3<sub class=\"ltx_sub\" id=\"S6.T6.1.5.4.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.5.4.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.5.4.3\">0.8337</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.5.4.4\">368M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.6.5.1\">TILT<sub class=\"ltx_sub\" id=\"S6.T6.1.6.5.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.6.5.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.6.5.3\">0.8392</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.6.5.4\">230M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.7.6.1\">GenDoc<sub class=\"ltx_sub\" id=\"S6.T6.1.7.6.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.7.6.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.7.6.3\">0.8449</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.7.6.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.8.7.1\">StructuralLM<sub class=\"ltx_sub\" id=\"S6.T6.1.8.7.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.8.7.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.8.7.3\">0.8610</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.8.7.4\">355M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.9.8.1\">LayoutLMv2<sub class=\"ltx_sub\" id=\"S6.T6.1.9.8.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.9.8.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.9.8.3\">0.8672</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.9.8.4\">426M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.10.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.10.9.1\">TILT<sub class=\"ltx_sub\" id=\"S6.T6.1.10.9.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.10.9.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.10.9.3\">0.8705</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.10.9.4\">780M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.11.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T6.1.11.10.1\">DocFormerv2<sub class=\"ltx_sub\" id=\"S6.T6.1.11.10.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.11.10.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.11.10.3\">0.8720</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.11.10.4\">232M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.12.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S6.T6.1.12.11.1\">DocFormerv2<sub class=\"ltx_sub\" id=\"S6.T6.1.12.11.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T6.1.12.11.2\">DocVQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T6.1.12.11.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.12.11.3.1\">0.8784</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T6.1.12.11.4\">750M</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 6: Performance Comparison of Document Layout Analysis Models on DocVQA Dataset."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T7\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Performance Comparison of Document Layout Analysis Models on RVL-CDIP Dataset.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T7.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T7.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S6.T7.1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T7.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.1.1.2.1\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T7.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.1.1.3.1\">Accuracy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T7.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.1.1.4.1\">#Params</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T7.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T7.1.2.1.1\">MCLR<sub class=\"ltx_sub\" id=\"S6.T7.1.2.1.1.1\">base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T7.1.2.1.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T7.1.2.1.3\">0.9070</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T7.1.2.1.4\">133M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.3.2.1\">BEiT<sub class=\"ltx_sub\" id=\"S6.T7.1.3.2.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.3.2.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.3.2.3\">0.9109</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.3.2.4\">87M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.4.3.1\">DiT<sub class=\"ltx_sub\" id=\"S6.T7.1.4.3.1.1\">Base</sub><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04080v1#bib.bib106\" title=\"\">106</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.4.3.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.4.3.3\">0.9211</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.4.3.4\">87M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.5.4.1\">DiT<sub class=\"ltx_sub\" id=\"S6.T7.1.5.4.1.1\">Large</sub><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.04080v1#bib.bib106\" title=\"\">106</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.5.4.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.5.4.3\">0.9269</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.5.4.4\">304M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.6.5.1\">StrucTexTv2<sub class=\"ltx_sub\" id=\"S6.T7.1.6.5.1.1\">Small</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.6.5.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.6.5.3\">0.9340</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.6.5.4\">28M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.7.6.1\">UDoc</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.7.6.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.7.6.3\">0.9364</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.7.6.4\">274M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.8.7.1\">GenDoc<sub class=\"ltx_sub\" id=\"S6.T7.1.8.7.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.8.7.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.8.7.3\">0.9380</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.8.7.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.9.8.1\">SelfDoc</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.9.8.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.9.8.3\">0.9381</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.9.8.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.10.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.10.9.1\">LayoutLM<sub class=\"ltx_sub\" id=\"S6.T7.1.10.9.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.10.9.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.10.9.3\">0.9442</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.10.9.4\">160M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.11.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.11.10.1\">LayoutLM<sub class=\"ltx_sub\" id=\"S6.T7.1.11.10.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.11.10.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.11.10.3\">0.9443</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.11.10.4\">390M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.12.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.12.11.1\">GenDoc<sub class=\"ltx_sub\" id=\"S6.T7.1.12.11.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.12.11.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.12.11.3\">0.9450</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.12.11.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.13.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.13.12.1\">StrucTexTv2<sub class=\"ltx_sub\" id=\"S6.T7.1.13.12.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.13.12.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.13.12.3\">0.9462</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.13.12.4\">238M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.14.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.14.13.1\">TILT<sub class=\"ltx_sub\" id=\"S6.T7.1.14.13.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.14.13.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.14.13.3\">0.9525</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.14.13.4\">230M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.15.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.15.14.1\">LayoutLMv2<sub class=\"ltx_sub\" id=\"S6.T7.1.15.14.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.15.14.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.15.14.3\">0.9525</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.15.14.4\">200M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.16.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.16.15.1\">LayoutLMv3<sub class=\"ltx_sub\" id=\"S6.T7.1.16.15.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.16.15.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.16.15.3\">0.9544</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.16.15.4\">133M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.17.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.17.16.1\">DocFormer<sub class=\"ltx_sub\" id=\"S6.T7.1.17.16.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.17.16.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.17.16.3\">0.9550</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.17.16.4\">536M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.18.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.18.17.1\">TILT<sub class=\"ltx_sub\" id=\"S6.T7.1.18.17.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.18.17.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.18.17.3\">0.9552</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.18.17.4\">780M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.19.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.19.18.1\">LayoutLMv2<sub class=\"ltx_sub\" id=\"S6.T7.1.19.18.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.19.18.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.19.18.3\">0.9564</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.19.18.4\">426M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.20.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.20.19.1\">LayoutLMv3<sub class=\"ltx_sub\" id=\"S6.T7.1.20.19.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.20.19.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.20.19.3\">0.9593</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.20.19.4\">368M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.21.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T7.1.21.20.1\">StructuralLM<sub class=\"ltx_sub\" id=\"S6.T7.1.21.20.1.1\">Large</sub>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.21.20.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.21.20.3\">0.9608</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.21.20.4\">355M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.22.21\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S6.T7.1.22.21.1\">DocFormer<sub class=\"ltx_sub\" id=\"S6.T7.1.22.21.1.1\">Base</sub>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T7.1.22.21.2\">RVL-CDIP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T7.1.22.21.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.22.21.3.1\">0.9617</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T7.1.22.21.4\">183M</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 7: Performance Comparison of Document Layout Analysis Models on RVL-CDIP Dataset."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T8\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Performance Comparison of Document Layout Analysis Models on CORD Dataset.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T8.1\" style=\"width:286.9pt;height:378pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<p class=\"ltx_p\" id=\"S6.T8.1.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S6.T8.1.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S6.T8.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.1.1.1.1.1.1\" style=\"color:#000000;\">Model</span><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.1.1.1.2\" style=\"color:#000000;\"></span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T8.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.1.1.1.1.2.1\" style=\"color:#000000;\">Dataset</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T8.1.1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.1.1.1.1.3.1\" style=\"color:#000000;\">Precision</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T8.1.1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.1.1.1.1.4.1\" style=\"color:#000000;\">Recall</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T8.1.1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.1.1.1.1.5.1\" style=\"color:#000000;\">F1</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T8.1.1.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.1.1.1.1.6.1\" style=\"color:#000000;\">#Params</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.2.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T8.1.1.1.1.2.1.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.2.1.1.1\" style=\"color:#000000;\">LayoutLM</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.2.1.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.2.1.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.1.1.2.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.2.1.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.1.1.2.1.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.2.1.3.1\" style=\"color:#000000;\">0.9437</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.1.1.2.1.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.2.1.4.1\" style=\"color:#000000;\">0.9508</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.1.1.2.1.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.2.1.5.1\" style=\"color:#000000;\">0.9472</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.1.1.2.1.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.2.1.6.1\" style=\"color:#000000;\">113M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.3.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.3.2.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.3.2.1.1\" style=\"color:#000000;\">LayoutLM</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.3.2.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.3.2.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.3.2.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.3.2.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.3.2.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.3.2.3.1\" style=\"color:#000000;\">0.9432</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.3.2.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.3.2.4.1\" style=\"color:#000000;\">0.9554</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.3.2.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.3.2.5.1\" style=\"color:#000000;\">0.9493</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.3.2.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.3.2.6.1\" style=\"color:#000000;\">343M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.4.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.4.3.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.4.3.1.1\" style=\"color:#000000;\">LayoutLMv2</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.4.3.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.4.3.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.4.3.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.4.3.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.4.3.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.4.3.3.1\" style=\"color:#000000;\">0.9453</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.4.3.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.4.3.4.1\" style=\"color:#000000;\">0.9539</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.4.3.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.4.3.5.1\" style=\"color:#000000;\">0.9495</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.4.3.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.4.3.6.1\" style=\"color:#000000;\">200M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.5.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.5.4.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.5.4.1.1\" style=\"color:#000000;\">TILT</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.5.4.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.5.4.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.5.4.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.5.4.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.5.4.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.5.4.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.5.4.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.5.4.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.5.4.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.5.4.5.1\" style=\"color:#000000;\">0.9511</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.5.4.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.5.4.6.1\" style=\"color:#000000;\">230M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.6.5\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.6.5.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.6.5.1.1\" style=\"color:#000000;\">LayoutLMv2</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.6.5.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.6.5.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.6.5.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.6.5.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.6.5.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.6.5.3.1\" style=\"color:#000000;\">0.9565</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.6.5.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.6.5.4.1\" style=\"color:#000000;\">0.9637</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.6.5.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.6.5.5.1\" style=\"color:#000000;\">0.9601</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.6.5.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.6.5.6.1\" style=\"color:#000000;\">426M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.7.6\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.7.6.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.7.6.1.1\" style=\"color:#000000;\">DocFormer</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.7.6.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.7.6.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.7.6.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.7.6.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.7.6.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.7.6.3.1\" style=\"color:#000000;\">0.9652</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.7.6.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.7.6.4.1\" style=\"color:#000000;\">0.9614</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.7.6.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.7.6.5.1\" style=\"color:#000000;\">0.9633</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.7.6.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.7.6.6.1\" style=\"color:#000000;\">183M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.8.7\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.8.7.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.8.7.1.1\" style=\"color:#000000;\">TILT</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.8.7.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.8.7.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.8.7.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.8.7.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.8.7.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.8.7.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.8.7.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.8.7.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.8.7.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.8.7.5.1\" style=\"color:#000000;\">0.9633</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.8.7.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.8.7.6.1\" style=\"color:#000000;\">780M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.9.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.9.8.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.9.8.1.1\" style=\"color:#000000;\">LayoutLMv3</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.9.8.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.9.8.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.9.8.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.9.8.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.9.8.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.9.8.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.9.8.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.9.8.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.9.8.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.9.8.5.1\" style=\"color:#000000;\">0.9656</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.9.8.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.9.8.6.1\" style=\"color:#000000;\">133M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.10.9\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.10.9.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.10.9.1.1\" style=\"color:#000000;\">GenDoc</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.10.9.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.10.9.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.10.9.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.10.9.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.10.9.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.10.9.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.10.9.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.10.9.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.10.9.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.10.9.5.1\" style=\"color:#000000;\">0.9659</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.10.9.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.10.9.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.11.10\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.11.10.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.11.10.1.1\" style=\"color:#000000;\">Fast-StrucTexT</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.11.10.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.11.10.1.2.1\" style=\"color:#000000;\">Linear</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.11.10.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.11.10.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.11.10.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.11.10.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.11.10.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.11.10.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.11.10.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.11.10.5.1\" style=\"color:#000000;\">0.9665</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.11.10.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.11.10.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.12.11\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.12.11.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.12.11.1.1\" style=\"color:#000000;\">DocFormerv2</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.12.11.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.12.11.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.12.11.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.12.11.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.12.11.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.12.11.3.1\" style=\"color:#000000;\">0.9751</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.12.11.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.12.11.4.1\" style=\"color:#000000;\">0.9610</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.12.11.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.12.11.5.1\" style=\"color:#000000;\">0.9680</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.12.11.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.12.11.6.1\" style=\"color:#000000;\">232M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.13.12\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.13.12.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.13.12.1.1\" style=\"color:#000000;\">GenDoc</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.13.12.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.13.12.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.13.12.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.13.12.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.13.12.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.13.12.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.13.12.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.13.12.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.13.12.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.13.12.5.1\" style=\"color:#000000;\">0.9697</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.13.12.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.13.12.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.14.13\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.14.13.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.14.13.1.1\" style=\"color:#000000;\">DocFormer</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.14.13.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.14.13.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.14.13.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.14.13.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.14.13.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.14.13.3.1\" style=\"color:#000000;\">0.9725</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.14.13.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.14.13.4.1\" style=\"color:#000000;\">0.9674</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.14.13.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.14.13.5.1\" style=\"color:#000000;\">0.9699</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.14.13.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.14.13.6.1\" style=\"color:#000000;\">536M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.15.14\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.15.14.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.15.14.1.1\" style=\"color:#000000;\">Fast-StrucTexT</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.15.14.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.15.14.1.2.1\" style=\"color:#000000;\">ResNet-18</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.15.14.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.15.14.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.15.14.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.15.14.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.15.14.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.15.14.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.15.14.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.15.14.5.1\" style=\"color:#000000;\">0.9715</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.15.14.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.15.14.6.1\" style=\"color:#000000;\">-</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.16.15\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.16.15.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.16.15.1.1\" style=\"color:#000000;\">MCLR</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.16.15.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.16.15.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.16.15.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.16.15.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.16.15.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.16.15.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.16.15.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.16.15.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.16.15.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.16.15.5.1\" style=\"color:#000000;\">0.9723</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.16.15.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.16.15.6.1\" style=\"color:#000000;\">133M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.17.16\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.17.16.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.17.16.1.1\" style=\"color:#000000;\">FormNet</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.17.16.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.17.16.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.17.16.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.17.16.3.1\" style=\"color:#000000;\">0.9802</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.17.16.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.17.16.4.1\" style=\"color:#000000;\">0.9655</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.17.16.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.17.16.5.1\" style=\"color:#000000;\">0.9728</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.17.16.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.17.16.6.1\" style=\"color:#000000;\">345M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.18.17\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.18.17.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.18.17.1.1\" style=\"color:#000000;\">LayoutLMv3</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.18.17.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.18.17.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.18.17.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.18.17.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.18.17.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.18.17.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.18.17.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.18.17.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.18.17.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.18.17.5.1\" style=\"color:#000000;\">0.9746</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.18.17.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.18.17.6.1\" style=\"color:#000000;\">368M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.19.18\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.19.18.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.19.18.1.1\" style=\"color:#000000;\">MCLR</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.19.18.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.19.18.1.2.1\" style=\"color:#000000;\">Base</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.19.18.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.19.18.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.19.18.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.19.18.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.19.18.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.19.18.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.19.18.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.19.18.5.1\" style=\"color:#000000;\">0.9749</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.19.18.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.19.18.6.1\" style=\"color:#000000;\">368M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.20.19\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S6.T8.1.1.1.1.20.19.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.20.19.1.1\" style=\"color:#000000;\">DocFormerv2</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.20.19.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.20.19.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.20.19.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.20.19.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.20.19.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.20.19.3.1\" style=\"color:#000000;\">0.9771</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.20.19.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.20.19.4.1\" style=\"color:#000000;\">0.9770</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.20.19.5\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.20.19.5.1\" style=\"color:#000000;\">0.9770</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1.1.20.19.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.20.19.6.1\" style=\"color:#000000;\">750M</span></span></span>\n<span class=\"ltx_tr\" id=\"S6.T8.1.1.1.1.21.20\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S6.T8.1.1.1.1.21.20.1\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.21.20.1.1\" style=\"color:#000000;\">UDoc</span><sub class=\"ltx_sub\" id=\"S6.T8.1.1.1.1.21.20.1.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.21.20.1.2.1\" style=\"color:#000000;\">Large</span></sub></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T8.1.1.1.1.21.20.2\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.21.20.2.1\" style=\"color:#000000;\">CORD</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T8.1.1.1.1.21.20.3\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.21.20.3.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T8.1.1.1.1.21.20.4\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.21.20.4.1\" style=\"color:#000000;\">-</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T8.1.1.1.1.21.20.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.1.1.21.20.5.1\" style=\"color:#000000;\">0.9813</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T8.1.1.1.1.21.20.6\"><span class=\"ltx_text\" id=\"S6.T8.1.1.1.1.21.20.6.1\" style=\"color:#000000;\">274M</span></span></span>\n</span>\n</span><span class=\"ltx_text\" id=\"S6.T8.1.1.1.2\" style=\"color:#000000;\"></span></span></p>\n</span></div>\n</figure>",
            "capture": "Table 8: Performance Comparison of Document Layout Analysis Models on CORD Dataset."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.04080v1_figure_1.png",
            "caption": "Figure 1: Examples of scanned documents with different layouts [22]."
        },
        "2": {
            "figure_path": "2403.04080v1_figure_2.png",
            "caption": "Figure 2: Number of relevant publications per year."
        },
        "3": {
            "figure_path": "2403.04080v1_figure_3.png",
            "caption": "Figure 3: Token-to-Line [72]"
        },
        "4": {
            "figure_path": "2403.04080v1_figure_4.png",
            "caption": "Figure 3: Token-to-Line [72]"
        },
        "5": {
            "figure_path": "2403.04080v1_figure_5.png",
            "caption": "Figure 5: Absolute (top) vs. relative (bottom) 2D position representation [56]."
        },
        "6": {
            "figure_path": "2403.04080v1_figure_6.png",
            "caption": "Figure 5: Absolute (top) vs. relative (bottom) 2D position representation [56]."
        }
    },
    "references": [
        {
            "1": {
                "title": "Deep visual template-free form parsing, 2019.",
                "author": "Brian Davis, Bryan Morse, Scott Cohen, Brian Price, and Chris Tensmeyer.",
                "venue": null,
                "url": null
            }
        },
        {
            "2": {
                "title": "Tncr: Table net detection and classification dataset.",
                "author": "Abdelrahman Abdallah, Alexander Berendeyev, Islam Nuradin, and Daniyar Nurseitov.",
                "venue": "Neurocomputing, 473:79\u201397, 2022.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Publaynet: largest dataset ever for document layout analysis, 2019.",
                "author": "Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes.",
                "venue": null,
                "url": null
            }
        },
        {
            "4": {
                "title": "Building a test collection for complex document information processing.",
                "author": "David D. Lewis, Gady Agam, Shlomo Engelson Argamon, Ophir Frieder, David A. Grossman, and Jefferson Heard.",
                "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, 2006.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Handwritten kazakh and russian (hkr) database for text recognition.",
                "author": "Daniyar Nurseitov, Kairat Bostanbekov, Daniyar Kurmankhojayev, Anel Alimova, Abdelrahman Abdallah, and Rassul Tolegenov.",
                "venue": "Multimedia Tools and Applications, 80:33075\u201333097, 2021.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Classification of handwritten names of cities and handwritten text recognition using various deep learning models.",
                "author": "Daniyar Nurseitov, Kairat Bostanbekov, Maksat Kanatov, Anel Alimova, Abdelrahman Abdallah, and Galymzhan Abdimanap.",
                "venue": "arXiv preprint arXiv:2102.04816, 2021.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Ocr-free document understanding transformer, 2022.",
                "author": "Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.",
                "venue": null,
                "url": null
            }
        },
        {
            "8": {
                "title": "Kohtd: Kazakh offline handwritten text dataset.",
                "author": "Nazgul Toiganbayeva, Mahmoud Kasem, Galymzhan Abdimanap, Kairat Bostanbekov, Abdelrahman Abdallah, Anel Alimova, and Daniyar Nurseitov.",
                "venue": "Signal Processing: Image Communication, 108:116827, 2022.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Deep learning for table detection and structure recognition: A survey.",
                "author": "Mahmoud Kasem, Abdelrahman Abdallah, Alexander Berendeyev, Ebrahem Elkady, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Hamada, Daniyar Nurseitov, and Islam Taj-Eddin.",
                "venue": "arXiv preprint arXiv:2211.08469, 2022.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Language modeling with deep transformers.",
                "author": "Kazuki Irie, Albert Zeyer, Ralf Schl\u00fcter, and Hermann Ney.",
                "venue": "arXiv preprint arXiv:1905.04226, 2019.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Language models with transformers, 2019.",
                "author": "Chenguang Wang, Mu Li, and Alexander J. Smola.",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "Amurd: annotated multilingual receipts dataset for cross-lingual key information extraction and classification.",
                "author": "Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser Elbendary, and Adam Jatowt.",
                "venue": "arXiv preprint arXiv:2309.09800, 2023.",
                "url": null
            }
        },
        {
            "13": {
                "title": "A survey on vision transformer.",
                "author": "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao.",
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(1):87\u2013110, 2023.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Cvt: Introducing convolutions to vision transformers, 2021.",
                "author": "Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.",
                "venue": null,
                "url": null
            }
        },
        {
            "15": {
                "title": "Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.",
                "author": "Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan.",
                "venue": "arXiv preprint arXiv:2110.05208, 2021.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
                "venue": "arXiv preprint arXiv:1810.04805, 2018.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.",
                "venue": "2019.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Generator-retriever-generator: A novel approach to open-domain question answering.",
                "author": "Abdelrahman Abdallah and Adam Jatowt.",
                "venue": "arXiv preprint arXiv:2307.11278, 2023.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Exploring the state of the art in legal qa systems.",
                "author": "Abdelrahman Abdallah, Bhawna Piryani, and Adam Jatowt.",
                "venue": "arXiv preprint arXiv:2304.06623, 2023.",
                "url": null
            }
        },
        {
            "20": {
                "title": "A survey on vision transformer.",
                "author": "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al.",
                "venue": "IEEE transactions on pattern analysis and machine intelligence, 45(1):87\u2013110, 2022.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Vivit: A video vision transformer.",
                "author": "Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid.",
                "venue": "In Proceedings of the IEEE/CVF international conference on computer vision, pages 6836\u20136846, 2021.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Layoutlm: Pre-training of text and layout for document image understanding.",
                "author": "Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.",
                "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Document layout analysis: A comprehensive survey.",
                "author": "Galal M Binmakhashen and Sabri A Mahmoud.",
                "venue": "ACM Computing Surveys (CSUR), 52(6):1\u201336, 2019.",
                "url": null
            }
        },
        {
            "24": {
                "title": "A survey of deep learning approaches for ocr and document understanding.",
                "author": "Nishant Subramani, Alexandre Matton, Malcolm Greaves, and Adrian Lam.",
                "venue": "arXiv preprint arXiv:2011.13534, 2020.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Document image analysis and recognition: a survey.",
                "author": "Andreeva Elena Igorevna, Bulatov Konstantin Bulatovich, Nikolaev Dmitry Petrovich, Petrova Olga Olegovna, Savelev Boris Igorevich, and Slavin Oleg Anatolevich.",
                "venue": "computer optics, 46(4):567\u2013589, 2022.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Camera-based analysis of text and documents: a survey.",
                "author": "Jian Liang, David Doermann, and Huiping Li.",
                "venue": "International Journal of Document Analysis and Recognition (IJDAR), 7:84\u2013104, 2005.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Table understanding: Problem overview.",
                "author": "Alexey Shigarov.",
                "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 13(1):e1482, 2023.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Layoutlmv2: Multi-modal pre-training for visually-rich document understanding.",
                "author": "Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei A. F. Flor\u00eancio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou.",
                "venue": "In ACL/IJCNLP, 2021.",
                "url": null
            }
        },
        {
            "29": {
                "title": "A knowledge-based segmentation method for document understanding.",
                "author": "Jun\u2019ichi Higashino, Hiromichi Fujisawa, Yasuaki Nakano, and Masakazu Ejiri.",
                "venue": "space, 50(60):10, 1986.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Analysis of form images.",
                "author": "Dacheng Wang and Sargur N Srihari.",
                "venue": "International journal of pattern recognition and artificial intelligence, 8(05):1031\u20131052, 1994.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Automatic document processing: A survey.",
                "author": "Yuan Y. Tang, Seong-Whan Lee, and Ching Y. Suen.",
                "venue": "Pattern Recognition, 29(12):1931\u20131952, 1996.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Automatic Analysis and Understanding of Documents, chapter 3.6, pages 625\u2013654.",
                "author": "Yuan Y. Tang, Chang D. Yan, M. Cheriet, and Ching Y. Suen.",
                "venue": "1993.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Vtlayout: Fusion of visual and text features for document layout analysis.",
                "author": "Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang.",
                "venue": "In Duc Nghia Pham, Thanaruk Theeramunkong, Guido Governatori, and Fenrong Liu, editors, PRICAI 2021: Trends in Artificial Intelligence, pages 308\u2013322, Cham, 2021. Springer International Publishing.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Layoutdiffusion: Controllable diffusion model for layout-to-image generation, 2023.",
                "author": "Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li.",
                "venue": null,
                "url": null
            }
        },
        {
            "35": {
                "title": "Fusion of visual representations for multimodal information extraction from unstructured transactional documents.",
                "author": "Berke Oral and G\u00fcl\u015fen Eryi\u011fit.",
                "venue": "International Journal on Document Analysis and Recognition (IJDAR), 25(3):187\u2013205, 2022.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation, 2016.",
                "author": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean.",
                "venue": null,
                "url": null
            }
        },
        {
            "37": {
                "title": "GraphRel: Modeling text as relational graphs for joint entity and relation extraction.",
                "author": "Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1409\u20131418, Florence, Italy, July 2019. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Deep learning, graph-based text representation and classification: a survey, perspectives and challenges.",
                "author": "Phu Pham, Loan TT Nguyen, Witold Pedrycz, and Bay Vo.",
                "venue": "Artificial Intelligence Review, 56(6):4893\u20134927, 2023.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Pick: Processing key information extraction from documents using improved graph learning-convolutional networks, 2020.",
                "author": "Wenwen Yu, Ning Lu, Xianbiao Qi, Ping Gong, and Rong Xiao.",
                "venue": null,
                "url": null
            }
        },
        {
            "40": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Deep residual learning for image recognition, 2015.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
                "venue": null,
                "url": null
            }
        },
        {
            "42": {
                "title": "Trie: End-to-end text reading and information extraction for document understanding, 2020.",
                "author": "Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, and Fei Wu.",
                "venue": null,
                "url": null
            }
        },
        {
            "43": {
                "title": "Aggregated residual transformations for deep neural networks, 2017.",
                "author": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He.",
                "venue": null,
                "url": null
            }
        },
        {
            "44": {
                "title": "Feature pyramid networks for object detection, 2017.",
                "author": "Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.",
                "venue": null,
                "url": null
            }
        },
        {
            "45": {
                "title": "Mask r-cnn.",
                "author": "Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick.",
                "venue": "In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Multimodal learning with transformers: A survey, 2023.",
                "author": "Peng Xu, Xiatian Zhu, and David A. Clifton.",
                "venue": null,
                "url": null
            }
        },
        {
            "47": {
                "title": "Multi-modal transformer for accelerated mr imaging, 2022.",
                "author": "Chun-Mei Feng, Yunlu Yan, Geng Chen, Yong Xu, Ling Shao, and Huazhu Fu.",
                "venue": null,
                "url": null
            }
        },
        {
            "48": {
                "title": "Mutualformer: Multi-modality representation learning via cross-diffusion attention, 2023.",
                "author": "Xixi Wang, Xiao Wang, Bo Jiang, Jin Tang, and Bin Luo.",
                "venue": null,
                "url": null
            }
        },
        {
            "49": {
                "title": "Selfdoc: Self-supervised document representation learning, 2021.",
                "author": "Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, and Hongfu Liu.",
                "venue": null,
                "url": null
            }
        },
        {
            "50": {
                "title": "Faster r-cnn: Towards real-time object detection with region proposal networks.",
                "author": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.",
                "venue": "Advances in neural information processing systems, 28, 2015.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Docvqa: A dataset for vqa on document images.",
                "author": "Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.",
                "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2200\u20132209, 2021.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Unilmv2: Pseudo-masked language models for unified language model pre-training, 2020.",
                "author": "Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.",
                "venue": null,
                "url": null
            }
        },
        {
            "53": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
                "venue": null,
                "url": null
            }
        },
        {
            "54": {
                "title": "Docformer: End-to-end transformer for document understanding, 2021.",
                "author": "Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha.",
                "venue": null,
                "url": null
            }
        },
        {
            "55": {
                "title": "Structext: Structured text understanding with multi-modal transformers, 2021.",
                "author": "Yulin Li, Yuxi Qian, Yuchen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, and Errui Ding.",
                "venue": null,
                "url": null
            }
        },
        {
            "56": {
                "title": "Bros: A pre-trained language model focusing on text and layout for better key information extraction from documents, 2021.",
                "author": "Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, and Sungrae Park.",
                "venue": null,
                "url": null
            }
        },
        {
            "57": {
                "title": "Unified pretraining framework for document understanding, May 18 2023.",
                "author": "Jiuxiang Gu, Ani Nenkova Nenkova, Nikolaos Barmpalios, Vlad Ion Morariu, Tong Sun, Rajiv Bhawanji Jain, Jason Wen Yong Kuen, and Handong Zhao.",
                "venue": "US Patent App. 17/528,061.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.",
                "author": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.",
                "venue": "Advances in neural information processing systems, 32, 2019.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Self-supervised relationship probing.",
                "author": "Jiuxiang Gu, Jason Kuen, Shafiq Joty, Jianfei Cai, Vlad Morariu, Handong Zhao, and Tong Sun.",
                "venue": "Advances in Neural Information Processing Systems, 33:1841\u20131853, 2020.",
                "url": null
            }
        },
        {
            "60": {
                "title": "A simple framework for contrastive learning of visual representations.",
                "author": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.",
                "venue": "In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Going full-tilt boogie on document understanding with text-image-layout transformer.",
                "author": "Rafa\u0142 Powalski, \u0141ukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Micha\u0142 Pietruszka, and Gabriela Pa\u0142ka.",
                "venue": "In Document Analysis and Recognition\u2013ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5\u201310, 2021, Proceedings, Part II 16, pages 732\u2013747. Springer, 2021.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Wikireading: A novel large-scale language understanding task over wikipedia.",
                "author": "Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, and David Berthelot.",
                "venue": "arXiv preprint arXiv:1608.03542, 2016.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.",
                "venue": "The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.",
                "url": null
            }
        },
        {
            "64": {
                "title": "How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings.",
                "author": "Kawin Ethayarajh.",
                "venue": "arXiv preprint arXiv:1909.00512, 2019.",
                "url": null
            }
        },
        {
            "65": {
                "title": "Enforcing encoder-decoder modularity in sequence-to-sequence models, 2019.",
                "author": "Siddharth Dalmia, Abdelrahman Mohamed, Mike Lewis, Florian Metze, and Luke Zettlemoyer.",
                "venue": null,
                "url": null
            }
        },
        {
            "66": {
                "title": "Evaluating sequence-to-sequence models for handwritten text recognition, 2019.",
                "author": "Johannes Michael, Roger Labahn, Tobias Gr\u00fcning, and Jochen Z\u00f6llner.",
                "venue": null,
                "url": null
            }
        },
        {
            "67": {
                "title": "Distillation of encoder-decoder transformers for sequence labelling, 2023.",
                "author": "Marco Farina, Duccio Pappadopulo, Anant Gupta, Leslie Huang, Ozan \u0130rsoy, and Thamar Solorio.",
                "venue": null,
                "url": null
            }
        },
        {
            "68": {
                "title": "Decoder-only or encoder-decoder? interpreting language model as a regularized encoder-decoder, 2023.",
                "author": "Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho So, Shengding Hu, Zhiyuan Liu, and Nigel Collier.",
                "venue": null,
                "url": null
            }
        },
        {
            "69": {
                "title": "Sequence-to-sequence pre-training with unified modality masking for visual document understanding.",
                "author": "Shuwei Feng, Tianyang Zhan, Zhanming Jie, Trung Quoc Luong, and Xiaoran Jin.",
                "venue": "arXiv preprint arXiv:2305.10448, 2023.",
                "url": null
            }
        },
        {
            "70": {
                "title": "Ernie-layout: Layout knowledge enhanced pre-training for visually-rich document understanding.",
                "author": "Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo, Zhenyu Zhang, Zhengjie Huang, Teng Hu, Weichong Yin, Yongfeng Chen, Yin Zhang, et al.",
                "venue": "arXiv preprint arXiv:2210.06155, 2022.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts.",
                "author": "Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei.",
                "venue": "Advances in Neural Information Processing Systems, 35:32897\u201332912, 2022.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Docformerv2: Local features for document understanding.",
                "author": "Srikar Appalaraju, Peng Tang, Qi Dong, Nishant Sankaran, Yichu Zhou, and R Manmatha.",
                "venue": "arXiv preprint arXiv:2306.01733, 2023.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Transformer encoder with multi-modal multi-head attention for continuous affect recognition.",
                "author": "Haifeng Chen, Dongmei Jiang, and Hichem Sahli.",
                "venue": "IEEE Transactions on Multimedia, 23:4171\u20134183, 2020.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Two-stage multimodality fusion for high-performance text-based visual question answering.",
                "author": "Bingjia Li, Jie Wang, Minyi Zhao, and Shuigeng Zhou.",
                "venue": "In Proceedings of the Asian Conference on Computer Vision, pages 4143\u20134159, 2022.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Formnet: Structural encoding beyond sequential modeling in form document information extraction, 2022.",
                "author": "Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, and Tomas Pfister.",
                "venue": null,
                "url": null
            }
        },
        {
            "76": {
                "title": "Flat2layout: Flat representation for estimating layout of general room types, 2019.",
                "author": "Chi-Wei Hsiao, Cheng Sun, Min Sun, and Hwann-Tzong Chen.",
                "venue": null,
                "url": null
            }
        },
        {
            "77": {
                "title": "Unifying vision, text, and layout for universal document processing, 2023.",
                "author": "Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal.",
                "venue": null,
                "url": null
            }
        },
        {
            "78": {
                "title": "Lilt: A simple yet effective language-independent layout transformer for structured document understanding, 2022.",
                "author": "Jiapeng Wang, Lianwen Jin, and Kai Ding.",
                "venue": null,
                "url": null
            }
        },
        {
            "79": {
                "title": "Language independent neuro-symbolic semantic parsing for form understanding, 2023.",
                "author": "Bhanu Prakash Voutharoja, Lizhen Qu, and Fatemeh Shiri.",
                "venue": null,
                "url": null
            }
        },
        {
            "80": {
                "title": "Enabling large language models to generate text with citations, 2023.",
                "author": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.",
                "venue": null,
                "url": null
            }
        },
        {
            "81": {
                "title": "Navigation-based candidate expansion and pretrained language models for citation recommendation.",
                "author": "Rodrigo Nogueira, Zhiying Jiang, Kyunghyun Cho, and Jimmy Lin.",
                "venue": "Scientometrics, 125:3001\u20133016, 2020.",
                "url": null
            }
        },
        {
            "82": {
                "title": "Multi-scale cell-based layout representation for document understanding.",
                "author": "Yuzhi Shi, Mijung Kim, and Yeongnam Chae.",
                "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 3670\u20133679, January 2023.",
                "url": null
            }
        },
        {
            "83": {
                "title": "StructuralLM: Structural pre-training for form understanding.",
                "author": "Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6309\u20136318, Online, August 2021. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "84": {
                "title": "Structextv2: Masked visual-textual prediction for document image pre-training.",
                "author": "Yuechen Yu, Yulin Li, Chengquan Zhang, Xiaoqiang Zhang, Zengyuan Guo, Xiameng Qin, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang.",
                "venue": "arXiv preprint arXiv:2303.00289, 2023.",
                "url": null
            }
        },
        {
            "85": {
                "title": "Fast-structext: An efficient hourglass transformer with modality-guided dynamic token merge for document understanding.",
                "author": "Mingliang Zhai, Yulin Li, Xiameng Qin, Chen Yi, Qunyi Xie, Chengquan Zhang, Kun Yao, Yuwei Wu, and Yunde Jia.",
                "venue": "arXiv preprint arXiv:2305.11392, 2023.",
                "url": null
            }
        },
        {
            "86": {
                "title": "Beit: Bert pre-training of image transformers.",
                "author": "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.",
                "venue": "arXiv preprint arXiv:2106.08254, 2021.",
                "url": null
            }
        },
        {
            "87": {
                "title": "Context autoencoder for self-supervised representation learning.",
                "author": "Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang.",
                "venue": "arXiv preprint arXiv:2202.03026, 2022.",
                "url": null
            }
        },
        {
            "88": {
                "title": "Rtformer: Efficient design for real-time semantic segmentation with transformer.",
                "author": "Jian Wang, Chenhui Gou, Qiman Wu, Haocheng Feng, Junyu Han, Errui Ding, and Jingdong Wang.",
                "venue": "Advances in Neural Information Processing Systems, 35:7423\u20137436, 2022.",
                "url": null
            }
        },
        {
            "89": {
                "title": "Evaluation of deep convolutional nets for document image classification and retrieval.",
                "author": "Adam W Harley, Alex Ufkes, and Konstantinos G Derpanis.",
                "venue": "In International Conference on Document Analysis and Recognition (ICDAR).",
                "url": null
            }
        },
        {
            "90": {
                "title": "Cdip dataset.",
                "author": "David D. Lewis, Gady Agam, Shlomo Engelson Argamon, Ophir Frieder, David A. Grossman, and Jefferson Heard.",
                "venue": "https://ir.nist.gov/cdip/, 2011.",
                "url": null
            }
        },
        {
            "91": {
                "title": "Building digital tobacco industry document libraries at the university of california, san francisco library/center for knowledge management.",
                "author": "Heidi Schmidt, Karen Butter, and Cynthia Rider.",
                "venue": "D-Lib Magazine, 8(9):1082\u20139873, 2002.",
                "url": null
            }
        },
        {
            "92": {
                "title": "The iit cdip test collection.",
                "author": "David D. Lewis, Gady Agam, Shlomo Engelson Argamon, Ophir Frieder, David A. Grossman, and Jefferson Heard.",
                "venue": "https://ir.nist.gov/cdip/README.txt, 2021.",
                "url": null
            }
        },
        {
            "93": {
                "title": "Funsd: A dataset for form understanding in noisy scanned documents.",
                "author": "Jean-Philippe Thiran Guillaume Jaume, Hazim Kemal Ekenel.",
                "venue": "In Accepted to ICDAR-OST, 2019.",
                "url": null
            }
        },
        {
            "94": {
                "title": "Revising funsd dataset for key-value detection in document images, 2020.",
                "author": "Hieu M. Vu and Diep Thi-Ngoc Nguyen.",
                "venue": null,
                "url": null
            }
        },
        {
            "95": {
                "title": "Layoutxlm: Multimodal pre-training for multilingual visually-rich document understanding, 2021.",
                "author": "Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, and Furu Wei.",
                "venue": null,
                "url": null
            }
        },
        {
            "96": {
                "title": "What is optical character recognition?",
                "author": "Microsoft.",
                "venue": "https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview-ocr#read-api, 2022.",
                "url": null
            }
        },
        {
            "97": {
                "title": "National archives forms dataset.",
                "author": "Brian Davis, Bryan Morse, Scott Cohen, Brian Price, and Chris Tensmeyer.",
                "venue": "https://github.com/herobd/NAF_dataset, 2019.",
                "url": null
            }
        },
        {
            "98": {
                "title": "Pdfminer.",
                "author": "Yusuke Shinyama et al.",
                "venue": "https://github.com/euske/pdfminer, 2019.",
                "url": null
            }
        },
        {
            "99": {
                "title": "Icdar2019 competition on scanned receipt ocr and information extraction.",
                "author": "Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar.",
                "venue": "2019 International Conference on Document Analysis and Recognition (ICDAR), Sep 2019.",
                "url": null
            }
        },
        {
            "100": {
                "title": "Cord: A consolidated receipt dataset for post-ocr parsing.",
                "author": "Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee.",
                "venue": "In Workshop on Document Intelligence at NeurIPS 2019, 2019.",
                "url": null
            }
        },
        {
            "101": {
                "title": "Availability of full dataset.",
                "author": "Fabrizio Primerano.",
                "venue": "https://github.com/clovaai/cord/issues/4, 2021.",
                "url": null
            }
        },
        {
            "102": {
                "title": "Docvqa: A dataset for vqa on document images.",
                "author": "Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.",
                "venue": "https://www.docvqa.org/datasets, 2021.",
                "url": null
            }
        },
        {
            "103": {
                "title": "Industry documents library.",
                "author": "UCSF Library.",
                "venue": "https://www.industrydocuments.ucsf.edu/.",
                "url": null
            }
        },
        {
            "104": {
                "title": "Form-nlu: Dataset for the form language understanding.",
                "author": "Yihao Ding, Siqu Long, Jiabin Huang, Kaixuan Ren, Xingxiang Luo, Hyunsuk Chung, and Soyeon Caren Han.",
                "venue": "arXiv preprint arXiv:2304.01577, 2023.",
                "url": null
            }
        },
        {
            "105": {
                "title": "Scene text visual question answering, 2019.",
                "author": "Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar\u00e7al Rusi\u00f1ol, Ernest Valveny, C. V. Jawahar, and Dimosthenis Karatzas.",
                "venue": null,
                "url": null
            }
        },
        {
            "106": {
                "title": "Dit: Self-supervised pre-training for document image transformer.",
                "author": "Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei.",
                "venue": "In Proceedings of the 30th ACM International Conference on Multimedia, pages 3530\u20133539, 2022.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.04080v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "3"
        ],
        "methodology_sections": [
            "2",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6",
            "4.7"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "6",
            "6.1",
            "6.1.1",
            "6.1.2",
            "6.1.3",
            "6.2",
            "6.2.1",
            "6.2.2",
            "6.2.3",
            "6.2.4",
            "6.2.5",
            "6.2.6",
            "6.2.7"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6",
            "4.7"
        ]
    },
    "research_context": {
        "paper_id": "2403.04080v1",
        "paper_title": "Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis",
        "research_background": "## Motivation and Research Problem\nThe paper titled \"Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis\" is motivated by the increasing ubiquity of digitization in our lives, where physical objects, such as music discs and analog documents, are being replaced by digital, machine-readable counterparts. This shift facilitates downstream applications and historical preservation through efficient database storage. Among the digitization tasks, extracting form information from scanned documents stands out as a critical challenge due to the inherent complexity of these documents. Form documents often involve intricate structures, non-textual elements such as graphics, and a mix of hand-written and machine-written content, further complicated by noise and distortions in scanned images. This paper focuses on providing a comprehensive survey of methodologies employed in extracting form information from noisy scanned documents.\n\nThe primary research problem addressed in this survey revolves around the complexities associated with form understanding from scanned documents. Key challenges include integrating visual information, accounting for the diverse shapes and formats of documents, and understanding multi-modal content involving text, images, and layout information. The paper aims to explore and aggregate advanced techniques, particularly the role of language models and transformers, in overcoming these challenges.\n\n## Relevant Prior Work\nPrior work in the field of form and document understanding has highlighted several key methods and approaches:\n\n1. **Document Understanding**:\n   - Document understanding [6, 7, 8, 9] involves analyzing the content and structure of documents with various formats and modalities. This field encompasses a wide range of tasks including text, image, table, and graph analysis.\n\n2. **Language Models and Transformers**:\n   - Advanced neural network architectures like language models and transformers [10] have shown impressive capabilities in natural language processing (NLP), computer vision, and audio processing tasks.\n\n3. **Form Understanding**:\n   - Form understanding is akin to sequence-labeling tasks such as named entity recognition (NER), often referred to as key information extraction (KIE) [12]. This field requires models to comprehend document entities within the context of multiple modalities.\n   - Multi-modal nature of form understanding involves text, positional layout, and graphical information, making the task more challenging compared to traditional NER.\n\n4. **Vision-Language Models**:\n   - Vision-language models like ViT (Vision Transformer) [13, 14] and CLIP (Contrastive Language\u2013Image Pretraining) [15] have demonstrated potential in bridging text and image understanding, critical for complex document forms.\n\n5. **Transformer Models**:\n   - Transformer-based models [16, 17] have proven very effective in both NLP [18, 19] and computer vision tasks [20, 21]. Their ability to understand context and interpret unstructured data makes them suitable for form understanding tasks.\n\n6. **LayoutLM Model**:\n   - LayoutLM [22] extends BERT by incorporating layout information, focusing on both textual and spatial contexts, showing superior performance in document image classification and form understanding tasks.\n\nDespite advancements in general document understanding, layout analysis, and visual document analysis, there is a lack of comprehensive surveys focusing specifically on the task of form understanding. This paper aims to bridge this gap by providing an extensive overview of recent technologies dedicated to form understanding, particularly emphasizing the role of language models and transformers.",
        "methodology": "In the \"Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis\" paper, the proposed methodology for surveying the field of form understanding in noisy scanned documents is detailed as follows:\n\n1. **Search Strategy**: \n   - The research primarily leverages searches in prominent scientific databases, specifically using Scopus as of August 6th, 2023.\n   - A comprehensive search query was formulated to effectively target the topic. This query combined terms such as \"document understanding\" AND \"form\" OR \"information extraction\" AND \"Invoices\" to ensure strong relevance to both form understanding and document understanding.\n\n2. **Database Searches**:\n   - The search results indicated significant publication volumes across different databases:\n     - Springer Link had the highest number of publications at 162, with 50 from 2021 alone.\n     - ACM yielded 75 relevant publications, especially noting a post-2018 publication surge.\n     - IEEE returned 8 relevant publications.\n\n3. **Inclusion of Key Models**:\n   - The LayoutLMv2 approach was specifically included as one of the state-of-the-art models during the survey timeline, ensuring the survey captured leading-edge advancements.\n\n4. **Backward-Snowballing Method**:\n   - To ensure the thoroughness of the literature collection, a backward-snowballing method was employed. This technique involves reviewing the references of initially collected papers to identify additional relevant literature that was not captured in the initial search results.\n   - This method helped in selecting publications showcasing recent advances and introducing important datasets dedicated to form understanding or document understanding.\n\n5. **Publication Distribution Analysis**:\n   - Publications were charted from 2014 to 2023 to depict the number of relevant publications per year.\n\n6. **Literature Collection Criteria**:\n   - The chosen methodology ensures the inclusion of recent breakthroughs and trends, facilitating a comprehensive survey.\n   - The selected papers form the foundation for analyzing approaches, datasets, and making comparisons to elucidate advancements in the domain of form understanding in noisy scanned documents.\n\nThis methodology effectively captures the latest advancements, ensures comprehensive coverage of the literature, and highlights key trends and innovations in the field.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\nThe main experiments use a variety of well-established datasets designed for form understanding tasks. Here is a brief overview:\n\n- **FUNSD**: 199 form documents in English, containing both machine-written and handwritten text.\n- **XFUND**: Multilingual equivalent of FUNSD with each language having 199 documents.\n- **NAF**: 865 grayscale forms with pre-printed text and handwritten input from the US National Archives.\n- **IIT-CDIP**: A large collection of documents, mainly in grayscale, used for multiple document processing tasks.\n- **RVL-CDIP**: A subset of IIT-CDIP used extensively in document image classification, particularly assessing deep CNNs with 16 document classes.\n- **PubLayNet**: Contains  scientific PDF articles used primarily for document layout analysis.\n- **SROIE**: 1000 scanned receipts, used in OCR and key information extraction.\n- **CORD**:  receipt images from Indonesian shops, annotated for OCR parsing tasks.\n- **DocVQA**: Visual Question Answering dataset with  document images and  K question-answer pairs.\n- **Form-NLU**:  document images from financial forms, annotated into keys, values, and non-key-value entities.\n- **VRDU**: Visually-rich Document Understanding benchmark with  documents split between Ad-buy and Registration forms, capturing complex layouts.\n\n#### Baselines\nThe paper benchmarks multiple architectures in form understanding tasks, comparing:\n- Traditional machine learning models\n- Various Transformer-based models such as BERT, RoBERTa, and specialized models for document understanding like LayoutLM.\n\n#### Evaluation Metrics\nThe performance is evaluated using standard metrics:\n- **Precision, Recall, and F1-score** for specific tasks like entity extraction.\n- **Exact Match (EM)** and **F1-score** for question-answering tasks.\n- **Intersection-over-Union (IoU)** for bounding box evaluation in detection tasks.\n- **Overall Accuracy** for classification tasks, particularly useful for datasets like RVL-CDIP.\n\n#### Main Experimental Results\n- **Model Performance on FUNSD**: Transformer-based models like LayoutLM significantly outperform traditional baselines. Specific improvements in entity extraction tasks demonstrate the effectiveness of attention mechanisms in understanding document layout.\n  \n- **Multilingual Analysis with XFUND**: LayoutLM multilingual variants also exhibit strong performance across different languages, showing the model's robustness in handling diverse scripts and text layouts.\n\n- **RVL-CDIP and IIT-CDIP Classification Tasks**: Deep CNN architectures have shown superior accuracy in classifying document types within RVL-CDIP, setting a high standard for efficient feature extraction and representation learning.\n\n- **SROIE and CORD for Receipt Understanding**: The advanced Transformer models yield better F1-scores in OCR parsing and key information extraction tasks, handling noise and complex layouts more effectively than traditional approaches.\n\n- **DocVQA Performance**: On DocVQA, models like LayoutLM with fine-tuning achieve higher Exact Match and F1-scores, proving suitable for Visual Question Answering due to their ability to learn visual and textual features jointly.\n\n- **VRDU Benchmark**: Experiments on VRDU demonstrate significant challenges when dealing with unseen templates, highlighting areas for future research to improve generalization. Models perform well under Single Template Learning but find Mixed Template Learning and Unseen Template Learning tasks more challenging.\n\nOverall, the main results indicate that Transformer-based approaches, specifically those tailored for document understanding, achieve state-of-the-art results across different datasets and tasks, surpassing traditional methodologies in handling complex document analysis."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the impact of integrating textual and 2D position information with image embeddings in document understanding tasks.",
            "experiment_process": "The LayoutLM model was pre-trained on text embedding of words combined with their 2D position embeddings and fine-tuned using image embeddings of document sections. Two pre-training tasks were used: Masked Visual-Language Modeling (MVLM) and optional Multi-label Document Classification. The pre-trained model was fine-tuned on form understanding, receipt understanding, and document image classification tasks.",
            "result_discussion": "LayoutLM achieved new state-of-the-art results for several downstream tasks including form understanding. The model showed that incorporating the 2D positional information and image embeddings during fine-tuning significantly improved the model's understanding of document layouts.",
            "ablation_id": "2403.04080v1.No1"
        },
        {
            "research_objective": "To enhance the relation between text, layout, and visual information for better document understanding.",
            "experiment_process": "LayoutLMv2 incorporated image information during pre-training and introduced new pre-training tasks: text image alignment (TIA) and text image matching (TIM). The spatial aware self-attention mechanism extended the original attention by including 1D and 2D relations.",
            "result_discussion": "LayoutLMv2's integration of spatial information and new pre-training tasks improved its ability to understand complex document layouts, outperforming its predecessor in various form and document understanding tasks.",
            "ablation_id": "2403.04080v1.No2"
        },
        {
            "research_objective": "To improve the capabilities of document understanding models in relation extraction and entity linking tasks involving long-range dependencies and context.",
            "experiment_process": "PICK used text and layout features combined with a graph learning module to capture relationships between text segments. The encoder module processed the original document text and image features, the graph module created a soft adjacency matrix for pairwise relationships, and the decoder module used BiLSTM and CRF for sequence tagging.",
            "result_discussion": "PICK excelled in capturing long-distance relationships, addressing a key limitation of transformer-based models. Its effective use of graph learning significantly enhanced performance on tasks requiring strong contextual understanding.",
            "ablation_id": "2403.04080v1.No3"
        },
        {
            "research_objective": "To enable integrated text-reading and text-understanding through an end-to-end trained network.",
            "experiment_process": "The TRIE model fused text-reading and text-understanding tasks into a unified network. It used a ResNet CNN with FPN to extract convolutional features, identified text regions, and fused these with visual features using RoIAlign. These features were processed through a text-context block using multi-head self-attention, followed by an information extraction module with bidirectional LSTM.",
            "result_discussion": "TRIE's integrated approach allowed it to mutually benefit from both text-reading and text-understanding tasks during pre-training, leading to improved performance in extracting text and recognizing context in complex documents.",
            "ablation_id": "2403.04080v1.No4"
        }
    ]
}