{
    "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
    "abstract": "The growing number of parameter-efficient adaptations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise techniques for both zero-shot and supervised task generalization through routing in such library. We benchmark existing approaches to build this library and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Tailoring large language models (LLMs) towards downstream tasks, domains, or user profiles is of paramount importance given the recent democratization of their usage, catalyzed by the release of open-source LLMs (Zhang et al., 2023b  ###reference_b96###; Microsoft Research, 2023  ###reference_b54###, inter alia). This process often relies on an adapter, such as LoRA (Hu et al., 2022  ###reference_b34###), a parameter-efficient fine-tuning (PEFT) of a pre-trained LLM (Hu et al., 2022  ###reference_b34###; Liu et al., 2022  ###reference_b49###; Li & Liang, 2021  ###reference_b47###).\n###figure_1### LLM adapters are increasingly available as part of online hubs (Beck et al., 2021  ###reference_b3###; Mangrulkar et al., 2022  ###reference_b52###). These adapters are developed independently and asynchronously by users across the globe. Hence, they implicitly constitute a library built on top of multi-task data (Pfeiffer et al., 2023  ###reference_b63###).\nPrior works show that mixtures of pretrained trained adapters can facilitate few-shot adaptation of LLMs to unseen tasks (Ponti et al., 2023  ###reference_b64###; Vu et al., 2021  ###reference_b77###; Huang et al., 2024  ###reference_b36###). Reusing pre-existing adapters in a zero-shot fashion remains less explored (Jang et al., 2023  ###reference_b38###; Belofsky, 2023  ###reference_b5###). In contrast to standard mixture-of-experts approaches (Fedus et al., 2022  ###reference_b26###), in this setting, new inputs must be routed to independently trained experts without requiring joint training of the routing mechanism and expert parameters.\nThis leads to the question: how to create a modular LLM end-to-end by first building and then reusing a library of adapters for supervised adaptation and zero-shot generalization? First, given a base LLM, such as Phi-2 (Microsoft Research, 2023  ###reference_b54###) or Mistral (Jiang et al., 2023  ###reference_b39###), we investigate building a library of adapters by leveraging 256 tasks from Flan-v2 (Longpre et al., 2023  ###reference_b50###).111We held out SNI tasks to test supervised adaptation. We focus on LoRA (Hu et al., 2022  ###reference_b34###) and leave the extension to other adapter types for future work. Once the adapter library has been built, we devise routing strategies to evaluate zero-shot generalization on 10 downstream tasks comprising common-sense reasoning and coding (ARC (Clark et al., 2018  ###reference_b18###), MBPP (Austin et al., 2021  ###reference_b2###), inter alia) and supervised adaptation on 12 SuperNatural Instructions (SNI) tasks (Wang et al., 2022b  ###reference_b80###).\nHow to build the adapter library? One straightforward approach is to operate in a private scenario, in which one trains one adapter per task on the multi-task data and mix those adapters for unseen tasks (Chronopoulou et al., 2023a  ###reference_b15###; Vu et al., 2021  ###reference_b77###; Huang et al., 2024  ###reference_b36###). This method is useful when the multi-task data cannot be shared for joint training  (Mireshghallah et al., 2020  ###reference_b56###) but trained adapters can.\nTo favour transfer between training tasks, recent approaches compress the multi-task data into a smaller set of reusable, composable adapters (Ponti et al., 2023  ###reference_b64###; Caccia et al., 2023  ###reference_b9###). In this shared data setting, we propose model-based clustering (MBC), a simple two-stage approach to build a library of adapters. We find a positive correlation between the similarity of the LoRA weights of a pair of tasks and the transfer between the two tasks. Building on this intuition, we first exploit LoRA similarity in weight space between privately trained adapters as a proxy for detecting clusters of similar tasks, then train one adapter per cluster. Our approach empirically improves performance while matching the compute budget.\nHow to reuse the library for new scenarios? Given a library of trained LoRAs, we examine strategies of how to reuse the library in two settings: zero-shot generalization and parameter-efficient supervised adaptation to new tasks.\nReusing LoRAs in a zero-shot manner is challenging because there is no labelled data to learn a routing mechanism. We propose Arrow (\\contourblack), a routing mechanism that automatically selects relevant LoRAs without requiring i) joint training and ii) access to the data used to train each LoRA. This facilitates the vision of a decentralized system where LoRAs can be trained asynchronously and be readily reused with minimal assumptions. Arrow computes a representation for each LoRA as the direction of maximum variance induced by the LoRA parameters. At inference time, Arrow routes per token and per layer, i.e. each hidden state is routed by computing its alignment with each LoRA representation.\nIn summary, our contributions are: i) we study how to create LoRA-based modular multi-task LLM in a setting where experts are trained independently and the router is created after the training of the experts; ii) assuming shared multi-task data, we propose a clustering approach (MBC) to train a library of adapters; and, iii) we propose Arrow, a zero-shot routing method to select which adapters to use from a library of LoRAs. This allows for routing to independently trained experts without accessing their training data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "We are given a set of tasks , where each task  is associated with a dataset containing a set of samples . The union of the training sets constitutes our multi-task dataset ; in our case, it is Flan (Longpre et al., 2023  ###reference_b50###).\nIn order to create our library of task adapters, we use LoRA (Hu et al., 2022  ###reference_b34###). LoRA achieves competitive trade-offs between performance and parameter efficiency (Karimi Mahabadi et al., 2021  ###reference_b43###) by modifying the linear transformations in a base LM. For each linear transformation in the base LM, LoRA modifies the base model parameters as follows:\nwhere  are the (frozen) weights of the base LM,  are low-rank learnable parameters and  is a tunable scalar hyperparameter. LoRA achieves parameter efficiency because of the reduced rank  ()."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Building the LoRA Library",
            "text": "We propose different alternatives for building a library  of adapters that perform well on the tasks they were trained on and are versatile enough to be effective on other unseen downstream tasks. To do so, we seek methods that enhance multi-task transfer while reducing task interference (Wang et al., 2021  ###reference_b82###; Chen et al., 2022  ###reference_b13###).\nOne straightforward solution is to train separate adapters on each training task, i.e. the library will be composed of  adapters (see Fig. 1  ###reference_###). Several existing methods operate in this setting, such as LoraHub (Huang et al., 2024  ###reference_b36###), AdapterSoup (Chronopoulou et al., 2023a  ###reference_b15###) and SPoT (Vu et al., 2021  ###reference_b77###). Although this solution does not exploit multi-task training,\nit is required in settings where the task data is private, e.g., user data, and cannot be shared. Moreover, this setting reflects well the scenario in which adapters are trained by end users in a decentralized fashion and added asynchronously to the library.\nTo encourage transfer, another solution is to train a single adapter on all the multi-task training data. One possible shortcoming is the reduced capacity to fit the multi-task training data and the possibility of interference between the multitude of training tasks (Ponti et al., 2023  ###reference_b64###). Training a single adapter may result in negative transfer because task gradients are misaligned (Wang et al., 2021  ###reference_b82###). An obvious solution to reduce the amount of interference is to increase the number of trainable parameters, e.g. to fine-tune the whole base LM on the multi-task data (Liu et al., 2022  ###reference_b49###).\nPolytropon (Poly) and Multi-Head Routing (MHR) (Ponti et al., 2023  ###reference_b64###; Caccia et al., 2023  ###reference_b9###) explore intermediate approaches between private and shared, where  \u201cbasis\u201d adapters are trained on the multi-task training data. These  adapters can be considered \u201clatent skills\u201d, as each task adapter in the multi-task training set can be expressed as a linear combination of these basis adapters. If private training for all the tasks learns a matrix of parameters , where  is the dimensionality of the LoRA adapters, Poly decomposes , where ,  storing the latent skills and  the linear combination coefficients for each task which specify the task-specific routing w.r.t. the latent skills. Both  and  are trained jointly on the multi-task training set by gradient descent. Note that the skills  do not correspond to specific tasks and therefore it is not clear how to reuse them for zero-shot generalization (Caccia et al., 2023  ###reference_b9###).\n###figure_2### While Polytropon and MHR reduce the inventory size, they require joint training of experts and the router on the combined dataset of all tasks. Here, we propose another approach to compress multi-task data into a set of reusable adapters; we cluster tasks based on their similarity and then train one adapter per task cluster. Ideally, the similarity between two tasks should correlate with the benefit of training a single model on both tasks compared to having two independent models  (Fifty et al., 2021  ###reference_b27###; Vu et al., 2020a  ###reference_b75###).\nMotivated by (Zhou et al., 2022  ###reference_b98###), we rely on the intuition that LoRA parameter vectors similarity can approximate the amount of transfer between a pair of tasks. To confirm this, we devise the following experiment: we sample pairs of tasks  from the multi-task dataset, and we train both a) a LoRA on each task independently b) a LoRA on the union of the training datasets for the two tasks. We then compute the cosine similarity between the flattened LoRA parameters. We quantify transfer as the difference in the average log-likelihood induced by the joint and private models when evaluated on the test set of the two tasks. In Fig. 2  ###reference_###, we observe that, for two different base models (GPT-Neo and Phi-2), the closer the tasks are in LoRA parameter space, the more performance delta is when we train on the joint dataset.\nThe previous observation warrants our simple two-stage training procedure illustrated in Fig. 1  ###reference_### (top). Given a fixed computation training budget of  training steps per task, we use the first  steps to train private LoRAs. We then use these LoRA parameters to group tasks into  clusters by running a standard clustering algorithm (K-Means). In the second stage of training, we train one adapter per cluster for an additional  training steps, which keeps the total amount of computation similar to other approaches. We refer to this method as Model-Based Clustering (MBC) as it uses the model-based information encoded in the weights to determine a similarity metric between tasks (see Alg. 1  ###reference_###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Reusing the LoRA Library",
            "text": "Next, we study the reuse of a trained library  in two scenarios: for new inputs , i.e. zero-shot, and in a supervised adaptation setting, where new tasks  come equipped with their training data . While the latter has been addressed in recent works (Huang et al., 2024  ###reference_b36###; Caccia et al., 2023  ###reference_b9###; Vu et al., 2021  ###reference_b77###), the former scenario remains less explored (Jang et al., 2023  ###reference_b38###; Belofsky, 2023  ###reference_b5###).\nWe first devise routing strategies in the zero-shot and supervised settings and then describe how to aggregate the contributions of adapters selected by the routing strategies."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Routing",
            "text": "We denote the hidden state for any token at a given transformer layer produced by the input token  as . Similar to MoE approaches, we seek to parameterize a layer-specific routing distribution that prescribes which adapters to use. We denote this categorical distribution over  outcomes as , where we drop the dependence on the layer for simplicity.\nFor example, in standard MoE approaches (Fedus et al., 2022  ###reference_b26###), . Given that we relax the assumption that the routing and the library should be trained together, we must devise ways to learn such routing distribution a posteriori."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Zero-Shot Routing",
            "text": "Routing One straightforward method to route to existing experts is to set the routing distribution to uniform for all layers, . Despite its simplicity,  routing was shown to be quite effective in recent work (Caccia et al., 2023  ###reference_b9###; Chronopoulou et al., 2023a  ###reference_b15###) and, due to the linearity of the LoRA adapters, effectively boils down to averaging the weights of the trained adapters uniformly.\nTP Routing\nAnother variant treats routing as an -way classification problem. Specifically, given an input  belonging to task  in our multi-task training set, we train a task predictor  by minimizing the categorical cross-entropy loss , where  is a probability distribution obtained by learning a classifier on top of a T5 encoder (Raffel et al., 2020  ###reference_b66###). We then set  at inference time. Note that the routing decisions are not dependent on the hidden state , so this is a router dependent on the whole input but independent of the particular token or layer in the Transformer. We call this predictor TP (Task Predictor).\nCM Routing Centroid Matching (CM) computes a prototype for every expert (and for each layer) by averaging the hidden representations obtained by a forward pass of the LLM on each expert dataset. These prototypes can be stored in the columns of the routing matrix . Once the prototypes for each expert have been obtained, the routing distribution is calculated by taking the cosine similarity between  and each expert prototype and finally applying softmax. This routing is similar in spirit to Jang et al. (2023  ###reference_b38###) and Belofsky (2023  ###reference_b5###).\nArrow Routing \\contourblack The rows of every routing matrix  of standard MoE routing can be interpreted as expert \u201cprototypes\u201d. Arrow prescribes a way to estimate such routing matrix in a 0-shot fashion without requiring data access. Let\u2019s denote by  the parameters for expert  at layer , where we drop the dependency on . The -th LoRA expert transforms each token\u2019s hidden state  as .\nArrow finds a prototype for the expert  by decomposing the outer product  with SVD and taking the right first singular vector of this transformation (see Alg. 2  ###reference_###). The prototype determines the direction of most variance induced by expert  in the space of hidden states . If the LoRA adapters are of rank 1, i.e.  the prototype for the expert  will be equal to the normalized  vector, i.e. .\nIn Section 10.1  ###reference_1###, we provide empirical evidence that indeed,  is larger when  belongs to task , thus motivating this routing approach. Given that both  and  are valid singular vectors, we compute expert logits as the absolute value of the dot product between prototypes and inputs. Alg. 2  ###reference_### details the prototype initialization and the routing step of Arrow.\nArrow offers several advantages: a) it doesn\u2019t require access to training data; b) it routes differently in every layer and token, increasing the overall model expressivity, and c) it is compute efficient since it requires no further training and SVD decomposition can be computed efficiently for low-rank matrices (Elhage et al., 2021  ###reference_b24###; Nakatsukasa, 2019  ###reference_b59###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "LoRA Composition",
            "text": "Given a routing distribution  obtained either using the previously presented zero-shot or supervised routing, we linearly combine adapters in the library, i.e. ,  and use the resulting adapter to perform inference at every layer of the base LLM (Ponti et al., 2023  ###reference_b64###; Huang et al., 2024  ###reference_b36###). For 0-shot task generalization, we employ top- routing, composing the  experts with the highest routing logits."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Our experimental evaluation aims to answer the following questions: 1) How does building a LoRA library compare to non-modular methods (e.g. full fine-tuning)? 2) How large is the gap between privately trained libraries (similar to online hubs) and libraries which assume access to multi-task data? 3) To what extent does routing facilitate reusing a library of LoRA adapters?\nMulti-Task Dataset We train expert modules on 256 tasks from the original Flan v2 dataset (Longpre et al., 2023  ###reference_b50###). We exclude the SNI tasks ( tasks) (Wang et al., 2022b  ###reference_b80###) from training for computational reasons. We reserved 12 SNI tasks for downstream out-of-domain evaluation. Similarly to Wang et al. (2023  ###reference_b81###), we sub-sampled 10,000 examples per task to ensure computational feasibility. Within these samples, 1,000 are allocated for validation and early-stopping. We will release our dataset for reproducibility.\nEvaluation For our supervised adaptation study, we use 12 held-out SNI tasks, each corresponding to a different SNI category. We threshold the number of training examples to 10,000 examples per task. We evaluate performance with Rouge-L scores (Lin & Hovy, 2003  ###reference_b48###). For zero-shot evaluation, we mainly use ten tasks widespread in the literature, including 1) common-sense reasoning: WinoGrande (Sakaguchi et al., 2021  ###reference_b69###), HellaSwag (Zellers et al., 2019  ###reference_b94###), PIQA (Bisk et al., 2020  ###reference_b6###); 2) question answering: boolQ (Clark et al., 2019  ###reference_b17###), OpenbookQA (Mihaylov et al., 2018  ###reference_b55###), ARC-easy and hard (Clark et al., 2018  ###reference_b18###); 3) coding: (HumanEval (Chen et al., 2021  ###reference_b12###), MBPP (Austin et al., 2021  ###reference_b2###); 4) general-purpose reasoning: BBH. (Suzgun et al., 2022  ###reference_b73###)222We test on a subset of 1000 randomly sampled examples to reduce evaluation costs. We remove overlaps between the evaluation tasks and the Flan multi-task training set (boolQ, ARC, WinoGrande, HellaSwag, OpenbookQA and PIQA). We also include zero-shot results on the 12 held-out SNI tasks in the appendix.\n###table_1### Models and Training This work focuses on augmenting LLMs with a library of adapters to transform them into modular architectures. Our primary focus is on Phi-2 (Microsoft Research, 2023  ###reference_b54###), a state-of-the-art model (as of March 2024) with 2.8 billion parameters, leading its class of models with parameter counts below 3 billion, according to the open leaderboard (Beeching et al., 2023  ###reference_b4###). Additionally, we conducted experiments using the larger Mistral 7B (Jiang et al., 2023  ###reference_b39###) model, given its widespread use in the community. For all models, we only patch attention layers with LoRA adapters. Unless stated otherwise, for all our multi-task training and single-task adaptation scenarios, we use LoRA rank of 4, dropout of 0.05 and learning rate of 1e-4. Unless specified, we set the number of clusters for MBC to 10, resulting in the best upstream validation loss and downstream performance for Phi-2, as demonstrated in Fig. 4  ###reference_###.\nMethods We consider the following methods in both zero-shot and supervised scenarios (except for FullFT):\nBase: the base model tested without any adaptation;\nShared: a single expert LoRA finetuned on the joint training set of all tasks (256 tasks unless stated otherwise) on top of the base model with multi-task learning;\nFullFT: like Shared but the full model is finetuned.\nWe adopt the following naming convention for the models using a library of experts: library\u2013routing. For the library type, we consider Poly, MHR, Private and MBC libraries described in Sec. 3  ###reference_###. For MBC, we match the total amount of compute, meaning that we use 40% of the training steps to compute the LoRA clustering and the other 60% to compute the final cluster adapters. For routing, we use , TP, CM and Arrow in the zero-shot scenario and Poly and LoraHub333For LoraHub, we match the amount of compute used by SGD. Assuming the backward pass is twice the compute of a forward pass, and since nevergrad (NG; Rapin & Teytaud, 2018  ###reference_b68###) only does forward passes, to match the compute of 5 SGD training epochs, we perform 30 epochs of NG with 1/2 of the training data used by SGD methods. for the supervised scenario, described in Sec. 4  ###reference_###.\n###figure_3###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Zero-Shot Results",
            "text": "In the zero-shot scenario, downstream tasks are evaluated without further fine-tuning. Tab. 1  ###reference_### presents the mean downstream accuracy for 10 held-out tasks. First, we analyze Phi-2 results. We observe that MHR- achieves strong zero-shot performance, competitive with Shared and FullFT, in line with the results of Caccia et al. (2023  ###reference_b9###). Interestingly, training one adapter per task and then taking the average, Private-, still achieves gains w.r.t. Base, albeit falling short of multi-task training (FullFT and Shared), highlighting the competitiveness of uniform () adapter routing (Chronopoulou et al., 2023a  ###reference_b15###). Comparing the performance of our proposed MBC approach for library construction (MBC-) to previous approaches, we notice a sizable bump in performance of 1.2% absolute accuracy points over the strongest baseline (MHR). Similarly, when studying the zero-shot performance of Phi-2 on 12 SNI tasks in Tab. 5  ###reference_### we observe that MBC- strongly outperforms other baselines. Importantly, both Shared and FullFT methods, as well as Poly and MHR libraries assume simultaneous access to the full dataset of all tasks. In contrast, Private and MBC libraries can be trained in an embarrassingly parallel manner and therefore do not require any distributed training infrastructure (Li et al., 2022  ###reference_b45###).\nNext, we analyze whether more informed routing strategies can improve performance beyond the routing. The full results are reported in Figure 3  ###reference_### (Left & Middle). We see that TP, CM and Arrow routing improve the performance over  routing for the Private Phi-2 library, gaining 0.9%, 0.9% and 1.8% points respectively.\nThis highlights the importance of routing for larger libraries. Notably, Arrow (66.6%) can surpass the performance of FullFT (65.5%) when applied to the Private library.\nOn the MBC library, TP routing decreases performance when compared to uniform routing, while MBC-\\contourblack improves over MBC- by 0.3% points and proves itself as a more robust routing method for both Private and MBC libraries. Overall, MBC-\\contourblack improves 3.6 points over the base model and 1.8% absolute over FullFT.\nFor Mistral, we find a similar trend with MBC libraries achieving the best performance. Arrow routing results in a 2.5% increase in average performance over  routing when used with the Private library (Private-\\contourblack vs. Private-). Arrow is able to narrow the performance gap with MBC, without requiring simultaneous data access across tasks. We do not see any gains from using other routing methods for 10 experts in the MBC library in this case. We make similar observations analyzing 0-shot SNI-12 results presented in Table 5  ###reference_###, where Private-\\contourblack attains notable gains of 10 Rouge-L points over Private- while MBC- strongly outperforms all other baselines.\n###figure_4### MBC Analysis Overall, MBC enhances the performance of the library across all our results. To investigate this further, we compare different clustering techniques. First, we compare to clusters obtained by randomly selecting examples (RandomExamples). This is equivalent to randomly partitioning the joint multi-task dataset. Then, we compare to clusters obtained by randomly choosing tasks from the entire set of training tasks (RandomTask). Finally, we cluster task embeddings, which are obtained by forwarding task-specific examples through the model and averaging their representation at the model\u2019s penultimate layer (Embeddings). For all these methods, we set the number of clusters to 10.\nThe results are shown in Table 2  ###reference_###.\nRandomTask surpasses RandomExamples by 1.6%, which indicates that grouping tasks rather than task examples is crucial for positive transfer.\nEmbeddings underperforms MBC and supports our observation that the cosine similarity between the weights of privately-trained LoRA correlates better than using representation similarity for 0-shot generalization. Additionally, we also report average pairwise cluster \u201csimilarity\u201d (as measured by the cosine similarity of the LoRA weights for each cluster) and observe a tendency that expert clusters with lower similarity, i.e. higher diversity, tend to result in higher performance. We conjecture that this stems from different clusters contributing distinct features to the joint model; however, we leave further investigation in this direction to future work (Jolicoeur-Martineau et al., 2023  ###reference_b42###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Upstream Performance",
            "text": "We further assess the efficacy of Arrow routing by looking at the upstream in-distribution performance, measured as the average of the Rouge-L on the validation sets of the 256 training tasks. Within this setting, we can compute the performance of the Oracle routing, which selects for each task the corresponding expert. In Fig. 3  ###reference_### (Right) we report the results for Arrow and  routing with both MBC and Private libraries.\nFor both libraries,  increases performance w.r.t.  and almost matches Oracle performance in the Private setting. This demonstrates Arrow\u2019s ability to correctly select the most relevant modules from a large library of experts."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Supervised Adaptation",
            "text": "In Table 3  ###reference_###, we present the supervised adaptation results for Phi-2 on the full (100% of training data) and limited (10% of training data) data regimes. The detailed per-task performance as well as the adaptation results for the Mistral model are presented in Table 8  ###reference_### and 7  ###reference_###. First, for all models (Phi-2, Mistral) we observe a notable performance boost coming from using Private and MBC libraries compared to No Library, which optimizes a LoRA for each downstream task by starting from a random initialization, and Shared, which starts from the multi-task trained LoRA solution. Secondly, similarly to zero-shot results, we observe that MBC can boost the performance with both Poly and  routing: for Phi-2 the performance of MBC- tops Private-. Additionally, we see that randomly grouping tasks RandomTask-Poly outperforms the non-library baselines but does not quite match MBC-based clustering for all the models.\nThe low performance of LoraHub can be attributed to the fact that LoraHub does not fine-tune the LoRA experts\u2019 weights but only their routing coefficients (due to gradient free optimization). Refer to App. 10.2  ###reference_2### for more insights onto this point. Finally, MBC- performs similarly to MBC-Poly, echoing results in (Caccia et al., 2023  ###reference_b9###)."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Summary of Results",
            "text": "Mirroring the questions at the start of this section, we list our main takeaway messages below:\nWhen appropriately routed, independently trained experts (Private-\\contourblack) can match and surpass the zero-shot performance of full fine-tuning (for Phi-2) and shared tuning (for Mistral 7B). This is a rather surprising result given that experts are independently trained and routing is learned post-hoc. These results show promise for building collaboratively and asynchronously trained LMs.\nIf data sharing is possible, then clustering tasks by their similarity with MBC constitutes a very effective strategy. In this case, simply averaging the LoRA adapters obtained through MBC (MBC-) is sufficient compared to more sophisticated routing. Our zero-shot and supervised adaptation results underscore the superiority of task-based over example-based clustering.\nArrow appears to be a very performant zero-shot routing strategy while requiring minimal information about the trained LoRAs and none about the training data. For supervised adaptation, training both adapters and the routing coefficients appears to be crucial. Overall, if routing seems beneficial for large libraries of adapters, the gains for smaller libraries are diminishing. This appears to stand in contrast with sparse MoE models, where (non-uniform) routing is crucial (Jiang et al., 2024  ###reference_b40###). This may be due to the linearity of LoRA experts, which stands in contrast with MLP experts in sparse MoEs (Fedus et al., 2022  ###reference_b26###); however, we leave this investigation for future work.\nOur main finding is that adapter parameters are suitable both to inform task clustering, and thus guide library building, and to route new inputs, thus facilitating library reuse."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Multi-task learning involves training on a joint set of all tasks (Caruana, 1997  ###reference_b11###), potentially leading to performance degradation due to task interference (Zhao et al., 2018  ###reference_b97###). An extensive literature studies how to partition learnable parameters into shared and task-specific ones (Ding et al., 2023  ###reference_b21###; Strezoski et al., 2019  ###reference_b72###; Bragman et al., 2019  ###reference_b7###; Zaremoodi et al., 2018  ###reference_b93###; Wallingford et al., 2022  ###reference_b78###; Fifty et al., 2021  ###reference_b27###). We operate in the parameter-efficient multi-task learning setting (Ponti et al., 2023  ###reference_b64###; Vu et al., 2021  ###reference_b77###; Chronopoulou et al., 2023a  ###reference_b15###; Pfeiffer et al., 2021  ###reference_b62###). Vu et al. (2021  ###reference_b77###) train one prefix adapter (Li & Eisner, 2019  ###reference_b46###) per task and learn to re-use them for other tasks based on the adapter similarities. MBC can be seen as an extension of this approach where we cluster tasks based on their weight similarity to ensure more transfer during multi-task pre-training.\nMixture of experts (MoEs), when coupled with sparse routing, are notable for augmenting model capacity with minimal computational overhead (Fedus et al., 2022  ###reference_b26###). Among the most important differences in this work: i) adapter experts are not trained during base model pre-training, ii) they are parameter-efficient and iii) they are tailored to specific tasks instead of being opaque computation units at the token level whose specialization is not easily interpretable (Jiang et al., 2024  ###reference_b40###). Regarding ii), Wang et al. (2022a  ###reference_b79###); Zadouri et al. (2023  ###reference_b92###); Muqeeth et al. (2023  ###reference_b57###) employs routing each example to a set of experts, showcasing enhanced performance on unseen tasks. Gupta et al. (2022  ###reference_b31###) trains a separate router for each task and picks a router from a similar task based on domain knowledge. Ye et al. (2022  ###reference_b91###) proposes task-level MoEs that treat a collection of transformer layers as experts and a router chooses from these experts dynamically. Recent work by Caccia et al. (2023  ###reference_b9###); Ponti et al. (2023  ###reference_b64###); Ostapenko et al. (2023  ###reference_b61###) investigate the effectiveness of densely routed adapter experts trained end-to-end with an expert library for MTL fine-tuning. For expert aggregation, we employ parameter-space weighted averaging (Wortsman et al., 2022  ###reference_b86###; Zhang et al., 2023a  ###reference_b95###; Ram\u00e9 et al., 2023  ###reference_b67###) with weights induced by a learned router, a technique akin to those in previous works (Ostapenko et al., 2023  ###reference_b61###; Zadouri et al., 2023  ###reference_b92###). Several recent works have also proposed techniques for learning how to route queries to specialized pretrained open-source LLMs (Lu et al., 2023  ###reference_b51###; Shnitzer et al., 2023  ###reference_b71###).\nModel ensembling techniques aim to enhance model robustness and generalization by integrating multiple distinct models (Frankle et al., 2020  ###reference_b28###; Wortsman et al., 2022  ###reference_b86###; Ram\u00e9 et al., 2023  ###reference_b67###; Jin et al., 2022  ###reference_b41###; Matena & Raffel, 2022  ###reference_b53###; Chronopoulou et al., 2023b  ###reference_b16###; Yang et al., 2023  ###reference_b90###). Parameter space averaging of independent models serves as an efficient ensembling method for full models (Ilharco et al., 2022  ###reference_b37###; Ainsworth et al., 2022  ###reference_b1###; Jin et al., 2022  ###reference_b41###) and adapters (Zhang et al., 2023a  ###reference_b95###; Yadav et al., 2024  ###reference_b89###), requiring only a single forward pass through the model, unlike output space ensembling (Dietterich, 2000  ###reference_b20###; Breiman, 1996  ###reference_b8###), that requires many forward passes. Efficient output ensembling techniques that can be applied in conjunction with our work are in (Wen et al., 2020  ###reference_b84###). Similarly, Pfeiffer et al. (2021  ###reference_b62###) proposes ensembling bottleneck style adapters with the subsequent fine-tuning step. Tam et al. (2023  ###reference_b74###) presents a merging framework called MaTs using the conjugate gradient method. Yadav et al. (2024  ###reference_b89###) proposes Ties-Merging to mitigate interference due to redundant parameter values. Daheim et al. (2024  ###reference_b19###) merge models by reducing their individual gradient mismatch with an ideal joint model, weighting their parameters with normalized Fisher Information.\nData Clustering for LMs have been proposed to improve performance and decrease task interference (Fifty et al., 2021  ###reference_b27###; Gururangan et al., 2023  ###reference_b32###; Gou et al., 2023  ###reference_b29###). These methods include clustering using similarities computed by tf-idf and neural embeddings, K-means clustering with balanced linear assignment, and soft clustering with GMMs (Gross et al., 2017  ###reference_b30###; Chronopoulou et al., 2023a  ###reference_b15###, 2021  ###reference_b14###; Gururangan et al., 2023  ###reference_b32###; Duan et al., 2021  ###reference_b22###; Caron et al., 2018  ###reference_b10###). Recent work by Zhou et al. (2022  ###reference_b98###) observes the potential of adapter parameters as effective task embeddings for clustering purposes, a concept we leverage in this work. A similar observation, but regarding task gradients, has been made by Vu et al. (2020b  ###reference_b76###).\nBuilding libraries of composable experts has been envisioned in several previous works (Pfeiffer et al., 2021  ###reference_b62###; Wu et al., 2023  ###reference_b87###; Huang et al., 2023  ###reference_b35###; Shah et al., 2023  ###reference_b70###; Xun Wu, 2024  ###reference_b88###). Beck et al. (2021  ###reference_b3###); Poth et al. (2023  ###reference_b65###) orchestrated a framework for assembling diverse adapters, offering flexibility in both training and inference. Most related to this work, Huang et al. (2023  ###reference_b35###) build LoRAHub, a library of task-specific LoRAs that can be combined for few-shot generalization. Pfeiffer et al. (2021  ###reference_b62###) introduce a two-stage learning algorithm that leverages knowledge from multiple tasks. They first learn task-specific experts and then combine the experts in a separate knowledge composition step. Xun Wu (2024  ###reference_b88###) introduces a learnable gating function to combine multiple LoRAs, called Mixture of LoRA Experts (MoLE). Wu et al. (2023  ###reference_b87###) presents -tuning for vision, language, and vision-language few-shot tasks. -tuning trains task-specific experts and then uses task embedding based on the diagonal of the Fisher information matrix to retrieve the top-k most similar tasks to a target task. We extend and complement these works by i) proposing novel methods to build a library, and ii) proposing techniques for zero-shot post-hoc routing independently trained adapters.\nRelated to ii), in a concurrent work,  Muqeeth et al. (2024  ###reference_b58###) learns a sigmoid gate for each expert, which is later used as expert prototype for zero-shot transfer.\nNotably, this method is applicable to the same setting as Arrow, and generalizes beyond linear adapters. However, in contrast to Arrow, obtaining the expert prototypes requires additional training after the experts are learned."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "We investigate how to build and reuse a library of adapters \u201cend-to-end\u201d. We show the potential of reusing independently (or partially independently) trained adapters with a zero-shot routing strategy. Overall, we strategically investigate the modular augmentation of smaller (language) models, offering a promising direction for research that prioritizes efficiency, flexibility, and performance.\nThe current investigation focuses on LoRA adapters. For future work, we are excited by the exploration of a heterogeneous \u201cuniverse\u201d of adapters\u2014including soft and hard prompts (Lester et al., 2021  ###reference_b44###; Wen et al., 2023  ###reference_b85###), MLPs (Houlsby et al., 2019  ###reference_b33###), etc.\u2014and combinations thereof. Whether our approach can result in encouraging results at a greater scale (both in terms of data and model size) remains open to further investigation. Using the proposed routing strategy for modular continual learning (Ostapenko et al., 2021  ###reference_b60###; Ermis et al., 2022  ###reference_b25###; Wang et al., 2022c  ###reference_b83###) is another promising direction for future work, especially given the fact that the Arrow router is local to each expert. In principle, it may be less susceptible to catastrophic forgetting as no gradient-based training is required to incorporate new experts into the library."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Broader Impact",
            "text": "This work sheds light on different ways of extending the capabilities of language models by surrounding them with a universe of lightweight adapters that can be trained on conventional hardware.\nAllowing the reuse of adapters might enable systems that are trained in a collaborative and distributed fashion and that use less total energy, with positive ramifications for the environment, but still attain the performance of vanilla systems. Further, this might allow users with smaller computational resources to more easily use and customize LLMs. There are also many potential societal consequences of improving LLMs, some being less desirable and even undesirable, but none of which we feel must be specifically highlighted here."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Summary of Contributions",
            "text": "OO led the effort on library compression and adaptation baselines. conceptualized and implemented MBC clustering, designed and implemented various experiments, and contributed to paper writing and codebase.\nZS worked on 0-shot and supervised adaptation, designed the task predictor routing, implemented the -tuning baseline, and contributed to the codebase and writing.\nEP, LCh, NLR were involved in the project after its start, and contributed to the general vision and to proof-writing.\nMP maintained and optimized the code, and prepared the code release.\nLCa led the efforts on 0-shot routing; designed, conceptualized, and implemented Arrow routing; implemented the CM baseline and contributed to the codebase.\nAS led the project and conceived its idea, worked on the codebase, data generation and evaluations, and wrote the paper."
        }
    ],
    "url": "http://arxiv.org/html/2405.11157v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6"
        ],
        "methodology_sections": [
            "3",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4",
            "4.1.1",
            "4.1.2",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.11157v1",
        "paper_title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
        "research_background": "### Motivation\nGiven the democratization of large language models (LLMs) spurred by the release of open-source versions, the adaptation of these models to specific downstream tasks, domains, or user profiles has become increasingly important. This adaptation typically hinges on parameter-efficient fine-tuning techniques such as LoRA (Low-Rank Adaptation). The proliferation of adapters, contributed by users globally, has introduced the implicit formation of a vast, yet unstructured, library of adapters. While there's been notable success in using mixtures of pretrained adapters for few-shot adaptation to unseen tasks, the reuse of these pre-existing adapters in a zero-shot manner remains relatively uncharted. This paper aims to explore creating a modular LLM system by developing and reusing an extensive library of these adapters.\n\n### Research Problem\nThe primary research question driving this paper revolves around how to build and efficiently reuse a modular LLM system structured around a library of independently trained LoRA-based adapters. Specifically, the issues at hand are:\n1. **Building the Adapter Library:** Creating a comprehensive library of adapters by leveraging multi-task data effectively, within computational constraints.\n2. **Reusing the Library:** Developing mechanisms for the zero-shot application of these adapters to new tasks without requiring labeled data or joint training of routing mechanisms.\n\n### Relevant Prior Work\nThe research builds on several key contributions in the domain:\n1. **Parameter-Efficient Fine-Tuning (PEFT):**\n   - **LoRA (Hu et al., 2022 ###reference_b34###):** A prominent method for PEFT, applied to pre-trained LLMs.\n   - **Other Related PEFT Methods:** Contributions by Liu et al. (2022 ###reference_b49###) and Li & Liang (2021 ###reference_b47###) describe similar approaches.\n   \n2. **Adapters and Online Hubs:**\n   - **Increasing Availability of Adapters:** Efforts such as Beck et al. (2021 ###reference_b3###) and Mangrulkar et al. (2022 ###reference_b52###) highlight the growing public repositories of such adapters.\n\n3. **Mixture of Experts and Task Transfer:**\n   - **Mixtures for Few-Shot Adaptation:** Previous works (Ponti et al., 2023 ###reference_b64###; Vu et al., 2021 ###reference_b77###; Huang et al., 2024 ###reference_b36###) show positive outcomes in few-shot learning scenarios using a mixture of pretrained adapters.\n   - **Zero-Shot Adapter Reuse:** Less explored in literature, with Jang et al. (2023 ###reference_b38###) and Belofsky (2023 ###reference_b5###) being notable exceptions.\n\n4. **Clustering and Multi-Task Data Handling:**\n   - **Data Compression for Transfer Learning:** Approaches by Ponti et al. (2023 ###reference_b64###) and Caccia et al. (2023 ###reference_b9###) that focus on data compression to improve transfer among tasks and make efficient use of computing resources.\n   - **Private Scenario Training:** Frameworks proposed by Chronopoulou et al. (2023a ###reference_b15###) and others emphasize the importance of adapter mixing when multi-task data cannot be jointly trained due to privacy or other constraints.\n\nThese prior works provide a foundational understanding of PEFT, adapter utility, and task transfer mechanisms, enabling this paper to propose new strategies for building and reusing a library of adapters for modular LLMs. The novel contributions here include the clustering technique (MBC) for constructing the adapter library and the zero-shot routing mechanism (Arrow) for efficient adapter reuse.",
        "methodology": "In this methodology, we propose several alternatives for building a library of adapters that perform well on the tasks they were initially trained on while being versatile enough to be effective on unseen downstream tasks. The aim is to enhance multi-task transfer while reducing task interference.\n\n### Methods\n\n1. **Separate Adapters for Each Task:**\n   - Train individual adapters for each task.\n   - These adapters are added asynchronously to the library, which reflects scenarios where task data is private or decentralized.\n   - Examples: LoraHub, AdapterSoup, SPoT.\n\n2. **Single Adapter for Multi-task Training:**\n   - Train a single adapter on all multi-task training data.\n   - Challenges include reduced capacity for fitting multi-task data and potential negative transfer due to task gradient misalignment.\n   - Suggested solution: increase trainable parameters by fine-tuning the entire base LM.\n\n3. **Intermediate Approaches - Polytropon (Poly) and Multi-Head Routing (MHR):**\n   - Train \"basis\" adapters on multi-task training data.\n   - These adapters serve as \"latent skills,\" where each task adapter can be expressed as a linear combination of these basis adapters.\n   - Parameters are decomposed to store latent skills and linear combination coefficients, which are jointly trained on multi-task data.\n   - Limitation: Skills don't correspond to specific tasks, which complicates zero-shot generalization.\n\n### Proposed Approach: Model-Based Clustering (MBC)\n\nTo address the limitations of the above methods, we propose a novel two-stage training procedure:\n\n1. **Initial Training (Stage 1):**\n   - Train private adapters (LoRAs) for each task using a first portion of the training budget (\\(m\\) steps).\n\n2. **Task Clustering:**\n   - Use the similarity of LoRA parameter vectors to cluster tasks.\n   - Similarity correlates with the benefit of joint training over independent models.\n   - Experiment demonstrated that tasks closer in LoRA parameter space show a greater performance delta when trained jointly.\n\n3. **Cluster-based Training (Stage 2):**\n   - Cluster tasks using a standard clustering algorithm like K-Means.\n   - Train one adapter per cluster for additional training steps (\\(n_2\\) steps), maintaining a fixed total computation budget.\n\nThis method compresses multi-task data into a set of reusable adapters by leveraging task similarity derived from model-based information encoded in the LoRA weights.\n\nBy clustering tasks and training one adapter per cluster, we can effectively manage computational resources while promoting generalization and transfer abilities across multiple tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n\n1. **Datasets:**\n   - **Multi-Task Training Dataset:** Expert modules are trained on 256 tasks from the original Flan v2 dataset (Longpre et al., 2023). The SNI tasks are excluded from training for computational reasons, with 12 SNI tasks reserved for downstream out-of-domain evaluation. Each task in the dataset has 10,000 examples, with 1,000 used for validation and early stopping.\n   - **Evaluation Dataset:**\n      - **Supervised Adaptation Evaluation:** 12 held-out SNI tasks with 10,000 examples per task.\n      - **Zero-Shot Evaluation:** 10 tasks including common-sense reasoning (WinoGrande, HellaSwag, PIQA), question answering (boolQ, OpenbookQA, ARC-easy, and ARC-hard), coding (HumanEval, MBPP), and general-purpose reasoning (BBH). A subset of 1,000 randomly sampled examples is used for evaluation to reduce costs.\n\n2. **Evaluation Metrics:**\n   - Rouge-L scores are used for performance evaluation.\n\n3. **Models and Training:**\n   - **Primary Models:** Phi-2 (a state-of-the-art model with 2.8 billion parameters) and Mistral 7B.\n   - **Adapters:** Attention layers are patched with LoRA adapters, using a LoRA rank of 4, dropout of 0.05, and learning rate of 1e-4.\n   - **Clustering:** For MBC (Modular Bayesian Clustering), the number of clusters is set to 10 for optimal performance.\n\n4. **Methods:**\n   - **Base:** The base model without any adaptation.\n   - **Shared:** A single expert LoRA finetuned on a joint training set of all tasks.\n   - **FullFT:** The full model finetuned on the joint training set.\n   - **Library-Routing Naming Conventions:** Libraries (Poly, MHR, Private, MBC) combined with routing methods (TP, CM, Arrow for zero-shot; Poly, LoraHub for supervised).\n\n#### Main Experimental Results\n\n1. **Comparison to Non-Modular Methods:** The performance of models built using a library of LoRA adapters is compared to non-modular methods, such as full fine-tuning (FullFT). The results show how modular methods handle various tasks compared to traditional full fine-tuning methods.\n\n2. **Gap Between Private and Multi-Task Libraries:** The results indicate the performance gap between libraries trained privately (similar to online hubs) versus those assuming access to multi-task data.\n\n3. **Effectiveness of Routing:** Routing methods are evaluated on their effectiveness in reusing a library of LoRA adapters. The results highlight the impact of different routing strategies on performance, particularly under zero-shot and supervised scenarios.\n\nOverall, the main experiment demonstrates the viability of modular approaches using LoRA adapters, showcasing specific comparisons and effectiveness under varied scenarios and datasets."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare various methods of building a library of adapters that perform well on the tasks they were trained on and are also effective on unseen downstream tasks.",
            "experiment_process": "Several methods are proposed for building the library. The first method involves training separate adapters for each training task, while the second method trains a single adapter on all multi-task training data. Other intermediate approaches include Polytropon (Poly) and Multi-Head Routing (MHR), which train 'basis' adapters on multi-task training data that act as 'latent skills.' The proposed Model-Based Clustering (MBC) method clusters tasks based on their similarity in LoRA parameter space and trains one adapter per task cluster. Similarity is measured through the cosine similarity between flattened LoRA parameters. The MBC training process involves a two-stage training procedure where private LoRAs are first trained for each task and then tasks are clustered using K-Means. A final adapter is then trained for each cluster.",
            "result_discussion": "MBC improves multi-task transfer and reduces task interference, outperforming other methods by a notable margin in terms of mean downstream accuracy for zero-shot performance. MBC-based clustering leads to better performance than random clustering methods. Using LoRA parameter vectors as the similarity metric for task clustering correlates better with beneficial task transfer compared to using representation similarity. The results indicate that grouping tasks into clusters and using these clusters to train adapters enhances performance and reduces task interference.",
            "ablation_id": "2405.11157v1.No1"
        },
        {
            "research_objective": "To evaluate the efficacy of various routing strategies for reusing a library of trained adapters, in both zero-shot and supervised adaptation settings.",
            "experiment_process": "For zero-shot routing, the strategies evaluated include uniform routing (\\contourblack), Task Predictor routing (TP), Centroid Matching routing (CM), and the proposed Arrow routing. These methods vary in how they predict and utilize the most relevant adapters for new inputs. For supervised task routing, Poly Routing, LoraHub Routing, and P-tuning Routing are compared, which further tune task-specific routing and adapters using new task data. The experiments are conducted using Phi-2 and Mistral LLMs on various held-out tasks from the SNI dataset and other common benchmarks like WinoGrande and HellaSwag. The evaluation metrics involve mean downstream accuracy and Rouge-L scores.",
            "result_discussion": "Arrow routing shows significant improvement in zero-shot performance, especially with the Private library setup, outperforming the uniform routing method by 1.8%. For supervised adaptation, MBC-Poly routing results in better performance, highlighting the importance of task-specific tuning of both router and adapters. The results indicate that informed routing strategies can enhance the reuse of adapter libraries, improving performance in both zero-shot and supervised settings significantly over simpler or uninformed routing methods.",
            "ablation_id": "2405.11157v1.No2"
        }
    ]
}