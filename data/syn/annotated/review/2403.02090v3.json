{
    "title": "Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations",
    "abstract": "Understanding social interactions involving both verbal and non-verbal cues is essential for effectively interpreting social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not aligned to utterances in multi-party environments. Consequently, they are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates concurrently capturing verbal and non-verbal cues pertinent to social reasoning. Experiments demonstrate the effectiveness of the proposed approach with densely aligned multimodal representations in modeling fine-grained social interactions. Project website: https://sangmin-git.github.io/projects/MMSI.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Real-world social interactions involve intricate behaviors between multiple people. People communicate not only through verbal cues (e.g., language) but also through non-verbal cues (e.g., gesture, gaze). While spoken language conveys explicit meaning, inferring the full social context from language alone can sometimes be ambiguous. Non-verbal cues can often play a crucial role in clarifying these subtle social nuances and providing additional context. Consequently, comprehensively understanding social interactions involving multimodal social cues is essential to interpret social situations appropriately.\n\n###figure_1### There have been attempts to investigate social behaviors in multimodal aspects by jointly considering language and visual cues. Some works tried to learn the relationships between spoken language and visual gestures for gesture generation [1  ###reference_b1###, 32  ###reference_b32###, 2  ###reference_b2###] and gesture-language grounding [29  ###reference_b29###]. Other multimodal approaches utilized the interconnection between spoken language and visual expressions to recognize human sentiment and emotion [55  ###reference_b55###, 43  ###reference_b43###, 22  ###reference_b22###, 39  ###reference_b39###, 21  ###reference_b21###, 9  ###reference_b9###]. However, these works mainly focus on single-person behaviors or rely on holistic visual representations in multi-party settings, rather than modeling the fine-grained dynamics of social interactions among multiple people. Modeling such intricate multi-party dynamics requires understanding the relationships between language and visual cues at an individual level, rather than using global representations.\nRecently, a multimodal work [28  ###reference_b28###] addressed social behaviors in social deduction games, which provide an effective testbed for studying multimodal social interactions. In these games, players take on roles and attempt to deduce the roles of their opponents by engaging in communication, deception, inference, and collaboration. These games encompass rich social interactions including verbal and non-verbal cues in multi-party settings. Lai et al. [28  ###reference_b28###] leveraged language and visual cues to predict persuasion strategies at the utterance level. However, their work has limitations in modeling multi-party interactions in terms of its task and methodology. Although persuasion strategies emerge in communication, the task primarily focuses on understanding the social behaviors of a single person rather than the dynamics among people. Moreover, their approach is limited in distinguishing and recognizing fine-grained interactions because it utilizes holistic visual representations for the entire scene, despite the presence of multiple people.\n\n###figure_2### To address these issues, we introduce three new multimodal tasks that encompass the dynamics of multi-party interactions, along with a novel baseline model. We extend the social deduction game datasets [28  ###reference_b28###] with extensive data annotations to curate new social tasks focused on identifying referents in multi-party conversations. Appropriately identifying referents is important because it significantly affects interpreting the social intent and context of conversations. Figure 1  ###reference_### shows the overall concepts of our curated social tasks. The three tasks are as follows.\nSpeaking target identification: Identifying who a speaker is talking to in a conversation.\nPronoun coreference resolution: Determining who a pronoun refers to in a conversation.\nMentioned player prediction: Predicting who is mentioned by name in a conversation.\nThese tasks are challenging as they require understanding the fine-grained dynamics of player interactions. For instance, when an utterance is accompanied by visual cues like pointing gestures, it is necessary to comprehensively interpret the utterance content and the speaker\u2019s gestures for holistic reasoning. To this end, we need to figure out who the speaker is and who is being pointed at within the visual scene based on the utterance context. This involves matching utterances with the visually identified individuals. Thus, it is required to align language references with player visuals and to derive densely aligned multimodal representations from such aligned features. Figure 2  ###reference_### shows the concept of densely aligned language-visual representations.\nTo this end, we propose a novel baseline model leveraging densely aligned language-visual representations. We detect and visually track each player in the video to distinguish individual players. By initially aligning player visual positions with their language references, we continuously track player visuals in sync with the dialogue. This alignment allows the model to visually identify the speaker and the other players (i.e., listeners) for given utterances. Encoding speaker gestures and the relative positions of the listeners enables deciphering visual relationships for understanding non-verbal dynamics. We then combine this with the linguistic context of the conversation to obtain densely aligned representations. Consequently, we can effectively predict referents by leveraging densely aligned representations containing both verbal and non-verbal dynamics.\nThe major contributions of this paper are as follows.\nWe introduce new social tasks in multi-party settings via extensive data annotations: speaking target identification, pronoun coreference resolution, and mentioned player prediction. These tasks are challenging as they require understanding the fine-grained dynamics of interactions.\nWe propose a novel multimodal baseline model leveraging language and visual cues for understanding multi-party social interactions. To the best of our knowledge, this is the first work to address the multimodal dense alignment between language and visual social cues."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Social Deduction Game Modeling",
            "text": "There have been works investigating computational models for social deduction games where players actively communicate and strategize with one another. Some prior studies have focused on developing game-playing agents and analyzing optimal strategies using game theory [6  ###reference_b6###, 35  ###reference_b35###, 5  ###reference_b5###, 44  ###reference_b44###, 12  ###reference_b12###]. These works aim to model the state of the game computationally but do not address understanding the dialogue and behaviors of players. Chittaranjan et al. [10  ###reference_b10###] modeled game outcomes from communication patterns such as player speaking and interrupting behaviors. Bakhtin et al. [14  ###reference_b14###] built an agent that can play diplomacy games by utilizing language models with strategic reasoning. These approaches do not capture verbal and non-verbal multimodal aspects of modeling social behaviors. Recently, Lai et al. [28  ###reference_b28###] addressed social behaviors in social deduction games using multimodal representations. They leveraged language and visual cues to predict persuasion strategies at the utterance level such as identity declaration and interrogation.\nHowever, this multimodal work is limited in addressing multi-party dynamics due to the lack of person-level feature recognition. To address this gap, we introduce new tasks in social deduction games that explicitly demand recognizing person-level features. We also propose the corresponding baseline model that captures the multi-party dynamics across both language and visual representations.\n\n###figure_3###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Proposed Benchmark",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Base Datasets",
            "text": "We extend two social deduction game datasets [28  ###reference_b28###]: YouTube and Ego4D with additional extensive data annotations for curating new social tasks.\nYouTube dataset. This dataset was collected from the YouTube video platform by searching keywords of Werewolf social deduction game. It contains 151 games of One Night Ultimate Werewolf, which corresponds to 151 separate videos with 14.8 hours. It consists of videos, transcripts, player roles, voting outcomes, and persuasion strategy labels. The transcripts comprise 20,832 utterances.\nEgo4D dataset. This Ego4D dataset is a subset of Ego4D Social dataset [19  ###reference_b19###]. It has 40 games of One Night Ultimate Werewolf and 8 games of The Resistance: Avalon. It contains 101 separate videos with 7.3 hours. Among them, we leverage 83 videos where we can visually identify individuals for new data annotations. To guarantee the visibility of all players within the frame, this dataset adopts third-person view videos instead of first-person view videos. It also consists of videos, transcripts, player roles, voting outcomes, and persuasion strategy labels. The transcripts contain 5,815 utterances during the games."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Data Annotation",
            "text": "To address the fine-grained dynamics of interactions, we design three new tasks in social deduction games: speaking target identification, pronoun coreference resolution, and mentioned player prediction. Annotators reference both transcripts and videos comprehensively to make their annotations in the transcripts. To achieve reliable annotation quality, we initially request three annotators to label subsets of data and measure their annotation agreement using Krippendorff\u2019s alpha [26  ###reference_b26###]. After we train the annotators sufficiently with  larger than 0.8, we request the three annotators to label independently for the rest of the data. Note that Krippendorff\u2019s alpha  0.8 is generally considered to indicate good reliability with a high level of agreement [7  ###reference_b7###].\nTable 1  ###reference_### shows the summary of our data annotation results. We achieve sufficiently high  for both speaking target identification and pronoun coreference resolution. Note that we could obtain the annotations for the mentioned player prediction automatically. In the training and testing process, we anonymize all names in transcripts (e.g., [\u201cDavid\u201d, \u201cAlice\u201d, \u201cThomas\u201d]  [\u201cPlayer1\u201d, \u201cPlayer2\u201d, \u201cPlayer3\u201d]). For each task, a test set is constructed using about 20% of the annotations. We split the training and test sets at the video level rather than at the individual utterance level to ensure no overlap in terms of contextual utterances. Annotation details for each task are as follows.\nTask1: Speaking target identification. This task aims to predict who a speaker is talking to in a dialogue. To this end, we annotate the labels of who a speaker is speaking to at the utterance level. Since utterances are often directed to all players, we perform the annotation only on the utterances that include [\u201cyou\u201d, \u201cyour\u201d]. We give our annotators three labeling options: (To Player#), (To Everyone), and (To Unknown). Based on the annotations, we add \u201c(To [MASK])\u201d at the end of a target utterance.\nTask2: Pronoun coreference resolution. This task aims to predict who a pronoun refers to in a dialogue. We conduct the annotation on the third-person pronouns that are used in our dataset, which are [\u201che\u201d, \u201cshe\u201d, \u201chim\u201d, \u201cher\u201d, \u201chis\u201d] in utterances. We give two options to annotators: [Player#] and [Unknown]. We only target the pronouns that indicate a specific player in the game. In terms of modeling the task, we change a pronoun to [MASK] in a target utterance.\nTask3: Mentioned player prediction. This task aims to predict who is referred to by their name in a dialogue. Since we know the ground truth names in utterances, it is possible to annotate these mentioned player labels automatically. We modify a mentioned player name with [MASK] in utterances and predict who is referred to in that part."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed Approach",
            "text": "The introduced social tasks can be formulated as follows. Let  denote utterance texts that include the -th target utterance containing a [MASK] token representing an unknown player, while  indicates the corresponding  video frames aligned with the utterance timeline. Given  and , our objective is to optimize the multimodal predictive function  to effectively classify the identity of the player associated with the [MASK].\nTo this end, we introduce a novel multimodal baseline that leverages densely aligned representations between language and visual cues to capture the fine-grained dynamics of interactions. Figure 3  ###reference_### shows the overall framework of the proposed multimodal baseline. The proposed model consists of four main parts: (i) language-visual alignment, (ii) visual interaction modeling, (iii) conversation context modeling, and (iv) aligned multimodal fusion for prediction."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Language-Visual Alignment",
            "text": "To achieve densely aligned representations containing both verbal and non-verbal dynamics properly, it is necessary to first establish fine-grained alignments between visual and language cues. To this end, we distinguish individual visuals and match language references with them.\nWe detect and track players visually in video frames over time using AlphaPose framework [17  ###reference_b17###]. Once we initially match player visuals with the player references in the utterances (i.e., assigning each tracking ID to Player#), we can continuously identify players in both visual and language domains. It enables the model to identify the speaker in the video based on the corresponding utterance and to determine the spatial arrangement of the listeners. Building upon this alignment, we can achieve densely aligned language-visual representations from verbal and non-verbal cues in a comprehensive manner. It enables us to tackle our social tasks effectively, allowing for a more nuanced and holistic understanding of multi-party interactions."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Visual Interaction Modeling",
            "text": "To distinguish individual players in video frames , we use the human pose keypoints from AlphaPose. Specifically, we extract 17 body keypoints (, ) for each player. Figure 3  ###reference_### shows the procedure of encoding visual interactions. The upper path (green) of Figure 3  ###reference_### indicates encoding a kinesics feature of a speaker, while the middle path (purple) represents encoding spatial positions of all players.\nFirst, we use the keypoints of a speaker in the upper path of Figure 3  ###reference_###. Among the 17 part keypoints, we leverage [nose, l-eye, r-eye, l-shoulder, r-shoulder, l-elbow, r-elbow, l-wrist, r-wrist] closely related to gaze and gesture characteristics. Let ,  denote the image coordinates of a part at time . For example, ,  indicates the nose point. To represent human motion in a unified coordinate, we normalize speaker keypoints by subtracting the speaker nose point from each part point. Each point vector ,  is encoded by an MLP point encoder  into a part point feature  ( is channel dim). These part point features are concatenated and processed by an MLP kinesics encoder  to obtain a speaker kinesics feature  as follows.\nSince we have multiple time steps, we can obtain .\nIn the meantime, the middle path (purple) of Figure 3  ###reference_### receives the position of each player. We consider the nose point of each player as their representative position. We normalize their nose points by subtracting the speaker\u2019s nose point from them to get their relative positions from the speaker. We utilize their representative positions at a single time step corresponding to the start of the utterance. Let ,  denote the representative position of Player#. Each point vector ,  is independently fed to an MLP point encoder  to get a player point feature . We concatenate the player point features and feed them to an MLP position encoder  to get . We then make  aware of speaker knowledge. To this end, we make a speaker-label feature  by passing a speaker-label one-hot vector through an FC layer. We combine  with  to obtain a player position feature . These procedures are formulated as follows.\nwhere  indicates the maximum player number in the datasets (). If the number of players is less than  for the current input data, we apply zero padding to the excess. If a player is temporarily undetected (e.g., offscreen for a short time), we proceed with position encoding by substituting the corresponding player position stored in a buffer to correct the player position.\nBased on the speaker kinesics features  and player position feature , we encode the visual interaction by capturing speaker kinesics motion with the context of player visual positions.  and  are passed through a visual interaction encoder  sequentially, which has the form of the transformer [50  ###reference_b50###].  allows modeling dependencies between the speaker kinesics and player positions across time via self-attention. Finally, we can obtain visual interaction features  that represent dynamics between the speaker and players based on the speaker kinesics and listener positions."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Conversation Context Modeling",
            "text": "The lower path (red) of Figure 3  ###reference_### shows encoding spoken utterances from players. To incorporate conversation context, we use surrounding utterances including the target utterance. The input to the language path is formulated as.\nwhere  denotes the target -th utterance, and the others indicate the preceding and following utterances. Note that the target utterance is the one that contains [MASK]. A [CLS] token is inserted in front of  while a [SEP] token is inserted at the end of each utterance in  for language tokenization processing. Note that all player names in utterances are anonymized as \u201c[Player#]\u201d. We leverage pre-trained language models based on masked-language modeling such as BERT [24  ###reference_b24###]. The tokenized sequence of the utterances is fed into the language model. The output feature corresponding to the index of the [MASK] token is then retrieved. After passing it through an FC layer to match the channel dimension of the visual interaction features, we get a conversation context feature  which contains the context around the [MASK]."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Aligned Multimodal Fusion",
            "text": "To fuse the aligned visual interaction features  and conversation context feature , we first concatenate them in the sequence dimension along with an [AGG] token for feature aggregation. It can be formulated as follows.\nNote that positional encoding [50  ###reference_b50###] for transformers is applied to the  parts. Then,  is processed with a multimodal transformer to encode their joint relationships. We leverage an output multimodal feature  from the [AGG] token. Finally, a densely aligned multimodal feature  is passed through a classification head consisting of an FC layer and softmax to predict the anonymized player identity  (e.g., Player#) for the target [MASK]. We optimize the model using cross-entropy loss between the predicted player  and the ground-truth label .\nAt training time, we apply permutations to anonymized identities to prevent the model from relying on consistent identities. Specifically, we randomly shuffle the mapping from player names to the anonymized player identities in utterances for every iteration. For example, [\u201cDavid\u201d, \u201cAlice\u201d, \u201cThomas\u201d]  [\u201cPlayer1\u201d, \u201cPlayer2\u201d, \u201cPlayer3\u201d]  [\u201cPlayer3\u201d, \u201cPlayer1\u201d, \u201cPlayer2\u201d]. This mapping permutation from the text domain is also applied to the visual position encoding and ground truth label  to ensure that language and visual cues are consistently aligned. This player permutation learning forces the model to learn more generalizable representations of player interactions that do not depend on specific identifiers during the training time."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Implementation",
            "text": "We adopt the language model as pre-trained BERT [24  ###reference_b24###], RoBERTa [33  ###reference_b33###], and ELECTRA [13  ###reference_b13###] which are based on masked-language modeling. The proposed model is trained by Adam optimizer [25  ###reference_b25###] with a learning rate of 5e-6 for the language model and 5e-5 for the other parts. We use a batch size of 16. We leverage about 3 seconds of video frames (frame interval 0.4s) that correspond to the timeline of the utterance. We use the preceding and following 5 utterances for encoding conversation context. The detailed network structures are described in the supplementary material."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Performance Comparison",
            "text": "We measure the identity classification accuracies for our curated tasks: speaking target identification, pronoun coreference resolution, and mentioned player prediction.\nTable 2  ###reference_### shows the experimental results for speaking target identification on YouTube and Ego4D datasets with different language models. We compare our proposed baselines with the recent multimodal model [28  ###reference_b28###] (i.e., Language Model + MViT [15  ###reference_b15###]) for social deduction games. In addition, we further adopt DINOv2 [38  ###reference_b38###] which is a powerful versatile visual feature generally used for various downstream tasks. Note that both comparison methods cannot leverage densely aligned language-visual representations. As shown in the table, these methods are not effective in improving upon the language models alone. This reflects that they are not able to figure out who the speaker is and who their gestures are directed at, in correspondence with the language domain. In contrast, our baselines leveraging densely aligned language-visual representations consistently enhance the language models for this task.\nTable 3  ###reference_### and 4  ###reference_### show the performance comparison results for pronoun coreference resolution and mentioned player prediction, respectively. We follow a similar experimental setup, evaluating our baselines against the language models and the multimodal methods (i.e., Language Models + DINOv2/MViT) across three different language models and two datasets. The results show that the competing multimodal methods fail to achieve substantial improvements over the language baselines. In contrast, our proposed multimodal baseline consistently outperforms both the language models and the other multimodal methods. Our multimodal approach demonstrates the effectiveness of aligned multimodal cues in addressing these social tasks."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Effects of Visual Features",
            "text": "We conduct ablation studies on visual feature types to analyze the contribution of each component in our baseline model. Table 5  ###reference_### shows the performance results according to the types of encoded non-verbal cues (i.e., gesture and gaze features) for our social tasks. Our final baseline model encodes speaker kinesics using keypoints related to gaze and gesture, specifically [nose, l-eye, r-eye, l-shoulder, r-shoulder, l-elbow, r-elbow, l-wrist, r-wrist]. \u201cw/o gesture feature\u201d indicates the model utilizing only the head-related keypoints of [nose, l-eye, r-eye] while \u201cw/o gaze feature\u201d employs only the gesture-related keypoints of [l-shoulder, r-shoulder, l-elbow, r-elbow, l-wrist, r-wrist]. We adopt the BERT-based baseline for evaluation on YouTube dataset. As shown in the table, the results show that the gesture features are more dominant compared to the gaze features in our setting. The proposed baselines using both gesture and gaze features generally achieve good performances.\n\n###figure_4###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Effects of Conversation Context",
            "text": "Conversational context plays an important role in understanding the meaning and intent behind individual utterances. To analyze the effects of conversation context on our social tasks, we conduct ablation experiments according to the preceding and following contexts of the target utterance. Table 6  ###reference_### presents the results of different context settings using the BERT-based baseline model on YouTube dataset. We compare three variations of the model: one that uses only the target utterance without any additional context, another that incorporates only the preceding context, and our proposed baseline which utilizes both preceding and following contexts. As shown in the table, leveraging both contexts shows the best results for all social tasks. It is noteworthy that the advantage of using the following context is relatively small in the pronoun coreference resolution task compared to the other tasks. This is because the nature of this task is to resolve the reference of pronouns against people that usually appear in the preceding context."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Effects of Permutation Learning",
            "text": "To validate the effectiveness of our player permutation learning which shuffles anonymized player identities, we conduct ablation experiments by training models with and without permutation. Table 7  ###reference_### shows the experiment results for three tasks with our BERT-based baseline on YouTube dataset. As shown in the table, the permutation learning approach consistently improves the performances for all tasks, implying it helps the model learn more generalizable representations of player interactions. Note that we apply this player permutation learning to all comparison methods in Tables 2  ###reference_###, 3  ###reference_###, and 4  ###reference_### for fair performance comparisons."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Qualitative Results",
            "text": "Figure 4  ###reference_### shows examples of three social tasks and their qualitative results according to the use of visual cues. We utilize BERT as the language model for this experiment. As shown in the figure, our multimodal baseline leveraging both language and visual cues in a dense alignment manner can correct the inference when the language model alone fails. The qualitative results show that visual features aligned to utterances provide complementary information to disambiguate referents in social interactions."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce three new challenging tasks in social deduction games: speaking target identification, pronoun coreference resolution, and mentioned player prediction - all of which require understanding the fine-grained verbal and non-verbal dynamics between multiple people. We curate extensive dataset annotations for our new social tasks and further propose a novel multimodal baseline that establishes dense language-visual alignments between spoken utterances and player visual features. This approach enables modeling multi-party social interactions through verbal and non-verbal communication channels simultaneously. Experiments show consistent and considerable performance improvements of our multimodal baselines over other approaches without both modalities and without multimodal dense alignment. Furthermore, extensive ablation studies are conducted to validate the effectiveness of our baseline components. We release the benchmarks and source code to facilitate further research in this direction.\nAcknowledgement.\nPortions of this project were supported in part by a gift from Meta."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Network Structure Details",
            "text": "Table 8  ###reference_### shows the network structure details of the proposed baseline. The point, kinesics, position, and visual interaction encoders are included in the visual interaction modeling part. The multimodal transformer is used for aligned multimodal fusion. The point, kinesics, and position encoders are implemented with Multilayer Perceptron (MLP) structures, comprising fully connected (FC) layers. The visual interaction encoder and the multimodal transformer are based on typical transformer architectures [50  ###reference_b50###]. For the MLP structures, \u201cMLP Size\u201d denotes the output dimension of each FC layer. ReLU activation is applied between FC layers in MLP structures. For transformers, \u201cHidden Size\u201d refers to the feature size after passing through the feed-forward network, while \u201cMLP Size\u201d represents the intermediate feature size in the Multi-Head Attention (MHA) mechanism. The channel dimensions  and  are set to 64 and 512, respectively."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Implementation Details",
            "text": "Language Models. We employ \u201dbert-base-uncased\u201d, \u201croberta-base\u201d, and \u201celectra-base-discriminator\u201d as pre-trained BERT, RoBERTa, and ELECTRA language models, respectively. We leverage models and weights of them from Hugging Face [52  ###reference_b52###].\nComparison Methods. For comparative analysis, we utilize (Language Model + MViT) and (Language Model + DINOv2). In the case of (Language Model + MViT), we leverage the visual features from the 24-layer multiscale vision transformer (MViT) [15  ###reference_b15###], pre-trained on the Kinetics-400 video dataset, following the approach in [28  ###reference_b28###]. For (Language Model + DINOv2), we use the visual features pooled from a 3-second window of DINOv2 features [38  ###reference_b38###] (interval 0.5s) along the time axis. Both visual features are integrated with the language feature (i.e., conversation context feature) through FC layers for task-specific predictions according to [28  ###reference_b28###]."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Effects of Conversation Context Length",
            "text": "We conduct experiments to investigate the effects of conversation context length  on the performance of each task. Figures 5  ###reference_###, 6  ###reference_###, and 7  ###reference_### show the validation results for speaking target identification, pronoun coreference resolution, and mentioned player prediction, respectively. When a context length of  is employed, the target utterance is concatenated with  preceding and  following utterances. As shown in the figures, we consistently obtain low performances with the shortest context length of , while achieving fairly good performances with a context length of  for all tasks. Note that we adopt  as our default setting for the baselines. These evaluations were conducted on YouTube dataset using the BERT-based model.\n###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Effects of Video Length",
            "text": "We conduct experiments to investigate the effects of video length on the performance of each task. Figures 8  ###reference_###, 9  ###reference_###, and 10  ###reference_### show the validation results for speaking target identification, pronoun coreference resolution, and mentioned player prediction, respectively. We achieve fairly good performance with a video length of 3 seconds for all tasks. Note that we adopt 3 seconds as our default setting for the baselines. These evaluations were conducted on YouTube dataset using the BERT-based model.\n###figure_8### ###figure_9### ###figure_10###"
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E Effects of Player Position Correction",
            "text": "We address scenarios where a player is temporarily undetected, such as being offscreen for a short time. In such cases, we proceed with player position encoding by leveraging the corresponding player position stored in a buffer to correct the missing player position. Table 9  ###reference_### shows the experimental results demonstrating the impact of missing player correction on the performance of all three social tasks. As shown in the table, the position correction contributes to improved performance. These experiments were conducted using the BERT-based model on YouTube dataset."
        },
        {
            "section_id": "Appendix 6",
            "parent_section_id": null,
            "section_name": "Appendix F Data Domain Generalization",
            "text": "We conduct experiments to validate the generalization capability of our proposed approach across two data domains. To this end, we train our baseline on YouTube dataset and evaluate its performance on Ego4D dataset. Table 10  ###reference_### shows the performance results according to the training data. As shown in the table, the model trained on YouTube data performs well on the Ego4D domain and even achieves better results compared to the model trained on Ego4D for all three social tasks. This improvement can be attributed to the larger amount of training data available in YouTube dataset. The experimental results demonstrate the generalization capability of our approach between different data domains and its potential to work in generalized environments. We adopt the BERT-based model for this experiment."
        },
        {
            "section_id": "Appendix 7",
            "parent_section_id": null,
            "section_name": "Appendix G Additional Quantitative Results",
            "text": "Utilization of Cropped Visual Features. We conduct experiments using cropped visual image features for visual interaction modeling. This approach is based on our dense alignment framework but utilizes cropped CLIP [42  ###reference_b42###] features instead of keypoint features for the speaker kinesics part (green) in Figure 2 of the main paper. The performances achieved with the cropped CLIP features are 71.0% for Speaking Target Identification, 63.6% for Pronoun Coreference Resolution, and 57.7% for Mentioned Player Prediction. These results are lower compared to our proposed baseline with keypoint features, which achieves 72.7%, 65.9%, and 58.8% for the respective tasks. These evaluations are conducted on YouTube dataset using the BERT-based model.\nMeasurement of Recall and Precision. In addition to accuracy, we further measure the macro-precision and macro-recall performance for our proposed approach. It is worth noting that in the multi-class setting, accuracy represents both micro-precision and micro-recall. The results show that our approach achieves (macro-precision / macro-recall / accuracy) performances of (74.8% / 74.7% / 72.7%) for Speaking Target Identification, (64.9% / 63.3% / 65.9%) for Pronoun Coreference Resolution, and (61.9% / 60.6% / 58.8%) for Mentioned Player Prediction. These results are obtained using the BERT-based model on YouTube dataset. We could achieve the balanced performances across precision, recall, and accuracy metrics in our environment, considering precision vs recall and macro vs micro aspects."
        },
        {
            "section_id": "Appendix 8",
            "parent_section_id": null,
            "section_name": "Appendix H Additional Qualitative Results",
            "text": "Figure 11  ###reference_### illustrates examples where our multimodal baseline model utilizing aligned language and visual cues outperforms the language-only model across the three social tasks. Our baseline with densely aligned multimodal representations enables corrected inferences compared to relying solely on language input. The BERT model is employed for these experiments. When we make inferences for these samples with RoBERTa language-only model, the inference results are #3 for STI, #1 for PCR, and #2 for MPP, which means it fails on 2nd and 3rd samples. These results demonstrate cases where the multimodal model (i.e., BERT) with visual reasoning surpasses the more powerful language-only model (i.e., RoBERTa).\n\n###figure_11###"
        },
        {
            "section_id": "Appendix 9",
            "parent_section_id": null,
            "section_name": "Appendix I Social Deduction Game Details",
            "text": "We leverage two social deduction game datasets: YouTube and Ego4D. Note that the data collections and annotations have been approved by the Institutional Review Board (IRB). YouTube dataset includes the games of One Night Ultimate Werewolf while Ego4D dataset contains the games of One Night Ultimate Werewolf and The Resistance: Avalon. Below are the details for each social deduction game: Werewolf and Avalon.\nOne Night Ultimate Werewolf. One Night Werewolf is a social deduction game in which players are secretly assigned to one of two primary factions - the villager team or the werewolf team. During the night phase, players close their eyes and characters with special abilities perform actions like swapping cards before opening their eyes again. The night phase may alter players\u2019 roles, though most remain unaware of these changes. Subsequently, players engage in discussion and negotiation to deduce the werewolf\u2019s identity. Werewolves, on their part, strive to conceal their identity and mislead others. At the end, everyone votes on who they believe is most suspicious. If at least one werewolf is eliminated, the village team wins, but if no werewolves are eliminated, the werewolf team wins. We refer One Night Ultimate Werewolf game\u2019s rules on Wikipedia https://en.wikipedia.org/wiki/Ultimate_Werewolf  ###reference_ewolf###.\nThe Resistance: Avalon. This game splits players into two groups: the Minions and the Loyal Servants of Arthur. After roles are assigned secretly via card distribution, players commence with a round where each assumes the Leader role in turns. The Leader\u2019s role involves proposing a team for a Quest and all players discuss and vote on approving or rejecting the team assignment. Post the Team Building phase, the designated team decides the Quest\u2019s outcome. The Good Team is restricted to using only the Success card in the Quest phase, whereas the Evil Team has the option to use either the Success or Fail card. The Good Team claims victory upon completing three successful Quests, while the Evil Team wins either by causing three Quests to fail or by correctly identifying the character Merlin among the Good Team. We refer The Resistance: Avalon game\u2019s rules on Wikipedia https://en.wikipedia.org/wiki/The_Resistance_(game)  ###reference_ce_(game)###."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel ltx_align_center\" id=\"S1.T1\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" id=\"S1.3.3\" style=\"width:424.9pt;height:149.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-23.2pt,8.1pt) scale(0.90165576011304,0.90165576011304) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3\">\n<tr class=\"ltx_tr\" id=\"S1.3.3.3.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.3.3.3.4.1\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.3.3.3.4.1.1\">Annotation Type</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.3.3.3.4.2\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.3.3.3.4.2.1\">Utterance Example</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S1.3.3.3.4.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S1.3.3.3.4.3.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S1.3.3.3.4.3.2\"> <span class=\"ltx_text\" id=\"S1.3.3.3.4.3.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3.4.3.2.1.1\">\n<span class=\"ltx_tr\" id=\"S1.3.3.3.4.3.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.4.3.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">YouTube DB</span></span>\n</span></span><span class=\"ltx_text\" id=\"S1.3.3.3.4.3.2.2\"></span></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S1.3.3.3.4.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S1.3.3.3.4.4.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S1.3.3.3.4.4.2\"> <span class=\"ltx_text\" id=\"S1.3.3.3.4.4.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3.4.4.2.1.1\">\n<span class=\"ltx_tr\" id=\"S1.3.3.3.4.4.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.4.4.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">Ego4D DB</span></span>\n</span></span><span class=\"ltx_text\" id=\"S1.3.3.3.4.4.2.2\"></span></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.2.2.2.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.2.2.2.2.3\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.2.2.2.2.3.1\">Count</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.1.1.1.1.1\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.1.1.1.1.1.1\">Krippendorff\u2019s </span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.2.2.2.2.4\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.2.2.2.2.4.1\">Count</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.2.2.2.2.2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.2.2.2.2.2.1\">Krippendorff\u2019s </span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.3.3.3.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.3.3.3.5.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S1.3.3.3.5.1.1\"></span> <span class=\"ltx_text\" id=\"S1.3.3.3.5.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3.5.1.2.1\">\n<span class=\"ltx_tr\" id=\"S1.3.3.3.5.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.5.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">Speaking Target</span></span>\n<span class=\"ltx_tr\" id=\"S1.3.3.3.5.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.5.1.2.1.2.1\" style=\"padding:1pt 0.0pt;\">Identification</span></span>\n</span></span><span class=\"ltx_text\" id=\"S1.3.3.3.5.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.3.3.3.5.2\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S1.3.3.3.5.2.1\"></span> <span class=\"ltx_text\" id=\"S1.3.3.3.5.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3.5.2.2.1\">\n<span class=\"ltx_tr\" id=\"S1.3.3.3.5.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.5.2.2.1.1.1\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S1.3.3.3.5.2.2.1.1.1.1\">Why are you helping the Werewolves</span></span></span>\n<span class=\"ltx_tr\" id=\"S1.3.3.3.5.2.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.5.2.2.1.2.1\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S1.3.3.3.5.2.2.1.2.1.1\">out?</span> (To <span class=\"ltx_text ltx_font_bold\" id=\"S1.3.3.3.5.2.2.1.2.1.2\" style=\"color:#FF3333;\">[Name]</span>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S1.3.3.3.5.2.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.3.3.3.5.3\" style=\"padding:1pt 0.0pt;\">3,255</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.3.3.3.5.4\" style=\"padding:1pt 0.0pt;\">0.922</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.3.3.3.5.5\" style=\"padding:1pt 0.0pt;\">832</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.3.3.3.5.6\" style=\"padding:1pt 0.0pt;\">0.907</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.3.3.3.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.3.3.3.3.2\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S1.3.3.3.3.2.1\"></span> <span class=\"ltx_text\" id=\"S1.3.3.3.3.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3.3.2.2.1\">\n<span class=\"ltx_tr\" id=\"S1.3.3.3.3.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.3.2.2.1.1.1\" style=\"padding:1pt 0.0pt;\">Pronoun Coreference</span></span>\n<span class=\"ltx_tr\" id=\"S1.3.3.3.3.2.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.3.2.2.1.2.1\" style=\"padding:1pt 0.0pt;\">Resolution</span></span>\n</span></span><span class=\"ltx_text\" id=\"S1.3.3.3.3.2.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.3.3.3.3.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S1.3.3.3.3.1.2\"></span> <span class=\"ltx_text\" id=\"S1.3.3.3.3.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3.3.1.1.1\">\n<span class=\"ltx_tr\" id=\"S1.3.3.3.3.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.3.1.1.1.2.1\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S1.3.3.3.3.1.1.1.2.1.1\">I\u2019m a Villager which makes me think</span></span></span>\n<span class=\"ltx_tr\" id=\"S1.3.3.3.3.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.3.1.1.1.1.1\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" id=\"S1.3.3.3.3.1.1.1.1.1.1\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S1.3.3.3.3.1.1.1.1.1.1.1\">he</span></span> ( <span class=\"ltx_text ltx_font_bold\" id=\"S1.3.3.3.3.1.1.1.1.1.2\" style=\"color:#FF3333;\">[Name]</span>) <span class=\"ltx_text ltx_font_italic\" id=\"S1.3.3.3.3.1.1.1.1.1.3\">was the Werewolf</span></span></span>\n</span></span><span class=\"ltx_text\" id=\"S1.3.3.3.3.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.3.3.3.3.3\" style=\"padding:1pt 0.0pt;\">2,679</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.3.3.3.3.4\" style=\"padding:1pt 0.0pt;\">0.962</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.3.3.3.3.5\" style=\"padding:1pt 0.0pt;\">503</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.3.3.3.3.6\" style=\"padding:1pt 0.0pt;\">0.846</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.3.3.3.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.3.3.3.6.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S1.3.3.3.6.1.1\"></span> <span class=\"ltx_text\" id=\"S1.3.3.3.6.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3.6.1.2.1\">\n<span class=\"ltx_tr\" id=\"S1.3.3.3.6.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.6.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">Mentioned Player</span></span>\n<span class=\"ltx_tr\" id=\"S1.3.3.3.6.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.6.1.2.1.2.1\" style=\"padding:1pt 0.0pt;\">Prediction</span></span>\n</span></span><span class=\"ltx_text\" id=\"S1.3.3.3.6.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.3.3.3.6.2\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S1.3.3.3.6.2.1\"></span> <span class=\"ltx_text\" id=\"S1.3.3.3.6.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S1.3.3.3.6.2.2.1\">\n<span class=\"ltx_tr\" id=\"S1.3.3.3.6.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.6.2.2.1.1.1\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\" id=\"S1.3.3.3.6.2.2.1.1.1.1\">I\u2019m the troublemaker and I switched</span></span></span>\n<span class=\"ltx_tr\" id=\"S1.3.3.3.6.2.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.3.3.3.6.2.2.1.2.1\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.3.3.3.6.2.2.1.2.1.1\" style=\"color:#FF3333;\">[Name]</span> <span class=\"ltx_text ltx_font_italic\" id=\"S1.3.3.3.6.2.2.1.2.1.2\">with somebody</span></span></span>\n</span></span><span class=\"ltx_text\" id=\"S1.3.3.3.6.2.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.3.3.3.6.3\" style=\"padding:1pt 0.0pt;\">3,360</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.3.3.3.6.4\" style=\"padding:1pt 0.0pt;\">N/A</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.3.3.3.6.5\" style=\"padding:1pt 0.0pt;\">472</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.3.3.3.6.6\" style=\"padding:1pt 0.0pt;\">N/A</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S1.T1.5.2.1\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text\" id=\"S1.T1.2.1\" style=\"font-size:90%;\">Summary of annotation details for three new social tasks. We achieve sufficiently high <span class=\"ltx_text ltx_font_italic\" id=\"S1.T1.2.1.1\">Krippendorff\u2019s alpha</span> values () for both speaking target identification and pronoun coreference resolution tasks, which indicates the high reliability of our data annotations.</span></figcaption>\n</figure>",
            "capture": "Table 1: Summary of annotation details for three new social tasks. We achieve sufficiently high Krippendorff\u2019s alpha values () for both speaking target identification and pronoun coreference resolution tasks, which indicates the high reliability of our data annotations."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:433.2pt;height:265.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-52.0pt,31.9pt) scale(0.8064202343749,0.8064202343749) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1.1\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.2\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.2.1.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.2.1.2\"> <span class=\"ltx_text\" id=\"S4.T2.1.1.1.2.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.1.2.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.2.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.1.2.1.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">Densely</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.2.1.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.1.2.1.2.1.1.2.1\" style=\"padding:1pt 0.0pt;\">Aligned?</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T2.1.1.1.2.1.2.2\"></span></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"2\" id=\"S4.T2.1.1.1.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.1.3.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.3.2\"> <span class=\"ltx_text\" id=\"S4.T2.1.1.1.3.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.1.3.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.3.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.1.3.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">Speaking Target</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.3.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.1.3.2.1.1.2.1\" style=\"padding:1pt 0.0pt;\">Identification (%)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.1.3.2.2\"></span></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.1.2.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.2.1.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.2.1.1.1.1\"></span><span class=\"ltx_text ltx_font_bold ltx_align_center\" id=\"S4.T2.1.1.2.1.1.1.2\"> <span class=\"ltx_text\" id=\"S4.T2.1.1.2.1.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.2.1.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.2.1.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.2.1.1.1.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">YouTube</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.2.1.1.1.2.2\"></span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.1.2.2\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.2.2.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.2.2.1.1.1\"></span><span class=\"ltx_text ltx_font_bold ltx_align_center\" id=\"S4.T2.1.1.2.2.1.1.2\"> <span class=\"ltx_text\" id=\"S4.T2.1.1.2.2.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.2.2.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.2.2.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.2.2.1.1.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">Ego4D</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.2.2.1.1.2.2\"></span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.3.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.3.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.3.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.3.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.3.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.3.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.3.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.3.2\" style=\"padding:1pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T2.1.1.3.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.3.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.3.3.1.1\" style=\"width:39.8pt;\">65.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T2.1.1.3.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.3.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.3.4.1.1\" style=\"width:39.8pt;\">56.8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.4.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.4.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.4.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.4.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.4.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.4.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">BERT + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.4.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.4.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.4.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.4.3.1.1\" style=\"width:39.8pt;\">66.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.4.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.4.4.1.1\" style=\"width:39.8pt;\">58.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.5.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.5.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.5.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.5.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.5.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.5.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">BERT + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.1.1.5.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.5.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.5.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.5.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.5.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.5.3.1.1\" style=\"width:39.8pt;\">66.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.5.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.5.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.5.4.1.1\" style=\"width:39.8pt;\">57.4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.6.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.6.1.1\" style=\"background-color:#EBEBEB;\">BERT-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.6.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.6.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.6.3\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.6.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.6.3.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.6.3.1.1.1\" style=\"background-color:#EBEBEB;\">72.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.6.4\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.6.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.6.4.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.6.4.1.1.1\" style=\"background-color:#EBEBEB;\">61.9</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.7.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.7.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.7.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.7.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.7.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.7.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.7.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.7.2\" style=\"padding:1pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.1.7.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.7.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.7.3.1.1\" style=\"width:39.8pt;\">72.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.1.7.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.7.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.7.4.1.1\" style=\"width:39.8pt;\">63.6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.8\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.8.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.8.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.8.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.8.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.8.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.8.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">RoBERTa + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.8.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.8.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.8.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.8.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.8.3.1.1\" style=\"width:39.8pt;\">72.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.8.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.8.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.8.4.1.1\" style=\"width:39.8pt;\">62.5</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.9\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.9.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.9.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.9.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.9.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.9.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.9.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">RoBERTa + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.1.1.9.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.9.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.9.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.9.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.9.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.9.3.1.1\" style=\"width:39.8pt;\">73.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.9.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.9.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.9.4.1.1\" style=\"width:39.8pt;\">64.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.10\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.10.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.1.1\" style=\"background-color:#EBEBEB;\">RoBERTa-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.10.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.10.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.10.3\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.10.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.10.3.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.3.1.1.1\" style=\"background-color:#EBEBEB;\">74.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.10.4\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.10.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.10.4.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.4.1.1.1\" style=\"background-color:#EBEBEB;\">66.5</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.11\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.11.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.11.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.11.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.11.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.11.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.11.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">ELECTRA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.11.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.2\" style=\"padding:1pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.1.11.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.11.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.11.3.1.1\" style=\"width:39.8pt;\">65.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T2.1.1.11.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.11.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.11.4.1.1\" style=\"width:39.8pt;\">60.8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.12\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.12.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.12.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.12.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.12.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.12.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.12.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">ELECTRA + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.12.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.12.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.12.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.12.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.12.3.1.1\" style=\"width:39.8pt;\">65.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.12.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.12.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.12.4.1.1\" style=\"width:39.8pt;\">60.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.13\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.13.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T2.1.1.13.1.1\"></span><span class=\"ltx_text\" id=\"S4.T2.1.1.13.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.13.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.13.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T2.1.1.13.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">ELECTRA + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.1.1.13.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T2.1.1.13.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.1.13.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.13.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.13.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.13.3.1.1\" style=\"width:39.8pt;\">64.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T2.1.1.13.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.13.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.13.4.1.1\" style=\"width:39.8pt;\">60.8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.14\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" id=\"S4.T2.1.1.14.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.1.1\" style=\"background-color:#EBEBEB;\">ELECTRA-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S4.T2.1.1.14.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.14.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T2.1.1.14.3\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.14.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.14.3.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.3.1.1.1\" style=\"background-color:#EBEBEB;\">69.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T2.1.1.14.4\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T2.1.1.14.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.14.4.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.4.1.1.1\" style=\"background-color:#EBEBEB;\">64.8</span></span>\n</span>\n</td>\n</tr>\n</table>\n</span></div>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" id=\"S4.T2.4.2\" style=\"font-size:90%;\">Performance comparison results for the speaking target identification task on YouTube and Ego4D datasets.</span></figcaption>\n</figure>",
            "capture": "Table 2: Performance comparison results for the speaking target identification task on YouTube and Ego4D datasets."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" id=\"S4.T3.1\" style=\"width:433.2pt;height:256pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-57.7pt,34.1pt) scale(0.789687907069678,0.789687907069678) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S4.T3.1.1.1.1\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.1.2\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.1.1.1.2.1\"><span class=\"ltx_text\" id=\"S4.T3.1.1.1.2.1.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1.2\"> <span class=\"ltx_text\" id=\"S4.T3.1.1.1.2.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.1.2.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.2.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.1.2.1.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">Densely</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.2.1.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.1.2.1.2.1.1.2.1\" style=\"padding:1pt 0.0pt;\">Aligned?</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T3.1.1.1.2.1.2.2\"></span></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"2\" id=\"S4.T3.1.1.1.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.1.3.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.2\"> <span class=\"ltx_text\" id=\"S4.T3.1.1.1.3.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.1.3.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.3.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.1.3.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">Pronoun Coreference</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.1.3.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.1.3.2.1.1.2.1\" style=\"padding:1pt 0.0pt;\">Resolution (%)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.1.3.2.2\"></span></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.1.2.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.2.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.2.1.1.1.1\">YouTube</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.1.2.2\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.2.2.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.2.2.1.1.1\">Ego4D</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S4.T3.1.1.3.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.3.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.3.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.3.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.3.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.3.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.3.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.3.2\" style=\"padding:1pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T3.1.1.3.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.3.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.3.3.1.1\" style=\"width:45.5pt;\">60.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T3.1.1.3.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.3.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.3.4.1.1\" style=\"width:45.5pt;\">47.3</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.4.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.4.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.4.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.4.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.4.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.4.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">BERT + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.4.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.4.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.4.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.4.3.1.1\" style=\"width:45.5pt;\">58.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.4.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.4.4.1.1\" style=\"width:45.5pt;\">46.4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.5.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.5.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.5.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.5.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.5.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.5.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">BERT + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S4.T3.1.1.5.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.5.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.5.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.5.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.5.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.5.3.1.1\" style=\"width:45.5pt;\">59.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.5.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.5.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.5.4.1.1\" style=\"width:45.5pt;\">46.4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.6.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.6.1.1\" style=\"background-color:#EBEBEB;\">BERT-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.6.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.1.1.6.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.6.3\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.6.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.6.3.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.6.3.1.1.1\" style=\"background-color:#EBEBEB;\">65.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.6.4\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.6.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.6.4.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.6.4.1.1.1\" style=\"background-color:#EBEBEB;\">49.1</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S4.T3.1.1.7.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.7.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.7.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.7.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.7.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.7.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.7.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.7.2\" style=\"padding:1pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.1.7.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.7.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.7.3.1.1\" style=\"width:45.5pt;\">69.0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.1.7.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.7.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.7.4.1.1\" style=\"width:45.5pt;\">48.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.8\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.8.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.8.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.8.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.8.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.8.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.8.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">RoBERTa + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.8.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.8.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.8.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.8.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.8.3.1.1\" style=\"width:45.5pt;\">68.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.8.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.8.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.8.4.1.1\" style=\"width:45.5pt;\">46.4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.9\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.9.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.9.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.9.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.9.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.9.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.9.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">RoBERTa + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S4.T3.1.1.9.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.9.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.9.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.9.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.9.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.9.3.1.1\" style=\"width:45.5pt;\">69.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.9.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.9.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.9.4.1.1\" style=\"width:45.5pt;\">49.1</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.10\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.10.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.10.1.1\" style=\"background-color:#EBEBEB;\">RoBERTa-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.10.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.1.1.10.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.10.3\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.10.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.10.3.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.10.3.1.1.1\" style=\"background-color:#EBEBEB;\">73.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.10.4\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.10.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.10.4.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.10.4.1.1.1\" style=\"background-color:#EBEBEB;\">52.7</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.11\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S4.T3.1.1.11.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.11.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.11.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.11.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.11.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.11.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">ELECTRA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.11.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.11.2\" style=\"padding:1pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.1.11.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.11.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.11.3.1.1\" style=\"width:45.5pt;\">62.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.1.11.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.11.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.11.4.1.1\" style=\"width:45.5pt;\">44.6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.12\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.12.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.12.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.12.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.12.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.12.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.12.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">ELECTRA + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.12.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.12.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.12.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.12.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.12.3.1.1\" style=\"width:45.5pt;\">61.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.12.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.12.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.12.4.1.1\" style=\"width:45.5pt;\">42.9</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.13\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.13.1\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S4.T3.1.1.13.1.1\"></span><span class=\"ltx_text\" id=\"S4.T3.1.1.13.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.1.1.13.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.1.1.13.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S4.T3.1.1.13.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">ELECTRA + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S4.T3.1.1.13.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S4.T3.1.1.13.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S4.T3.1.1.13.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.13.3\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.13.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.13.3.1.1\" style=\"width:45.5pt;\">62.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S4.T3.1.1.13.4\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.13.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.13.4.1.1\" style=\"width:45.5pt;\">43.8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.14\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" id=\"S4.T3.1.1.14.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.14.1.1\" style=\"background-color:#EBEBEB;\">ELECTRA-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.14.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S4.T3.1.1.14.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T3.1.1.14.3\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.14.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.14.3.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.14.3.1.1.1\" style=\"background-color:#EBEBEB;\">67.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T3.1.1.14.4\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.14.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.14.4.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.14.4.1.1.1\" style=\"background-color:#EBEBEB;\">46.4</span></span>\n</span>\n</td>\n</tr>\n</table>\n</span></div>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T3.3.1.1\" style=\"font-size:90%;\">Table 3</span>: </span><span class=\"ltx_text\" id=\"S4.T3.4.2\" style=\"font-size:90%;\">Performance comparison results for the pronoun coreference resolution task on YouTube and Ego4D datasets.</span></figcaption>\n</figure>",
            "capture": "Table 3: Performance comparison results for the pronoun coreference resolution task on YouTube and Ego4D datasets."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"S5.T4\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" id=\"S5.T4.1\" style=\"width:433.2pt;height:265.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-52.0pt,31.9pt) scale(0.8064202343749,0.8064202343749) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S5.T4.1.1.1.1\" rowspan=\"2\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.1.2\" rowspan=\"2\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T4.1.1.1.2.1\"><span class=\"ltx_text\" id=\"S5.T4.1.1.1.2.1.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.2.1.2\"> <span class=\"ltx_text\" id=\"S5.T4.1.1.1.2.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.1.2.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.1.2.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.1.2.1.2.1.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Densely</span></span>\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.1.2.1.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.1.2.1.2.1.1.2.1\" style=\"padding:0.75pt 0.0pt;\">Aligned?</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T4.1.1.1.2.1.2.2\"></span></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"2\" id=\"S5.T4.1.1.1.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.1.3.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.3.2\"> <span class=\"ltx_text\" id=\"S5.T4.1.1.1.3.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.1.3.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.1.3.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.1.3.2.1.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Mentioned Player</span></span>\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.1.3.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.1.3.2.1.1.2.1\" style=\"padding:0.75pt 0.0pt;\">Prediction (%)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.1.3.2.2\"></span></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T4.1.1.2.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.2.1.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.2.1.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text\" id=\"S5.T4.1.1.2.1.1.1.1\"></span><span class=\"ltx_text ltx_font_bold ltx_align_center\" id=\"S5.T4.1.1.2.1.1.1.2\"> <span class=\"ltx_text\" id=\"S5.T4.1.1.2.1.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.2.1.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.2.1.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.2.1.1.1.2.1.1.1.1\" style=\"padding:0.75pt 0.0pt;\">YouTube</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.2.1.1.1.2.2\"></span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T4.1.1.2.2\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.2.2.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.2.2.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text\" id=\"S5.T4.1.1.2.2.1.1.1\"></span><span class=\"ltx_text ltx_font_bold ltx_align_center\" id=\"S5.T4.1.1.2.2.1.1.2\"> <span class=\"ltx_text\" id=\"S5.T4.1.1.2.2.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.2.2.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.2.2.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.2.2.1.1.2.1.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Ego4D</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.2.2.1.1.2.2\"></span></span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S5.T4.1.1.3.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.3.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.3.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.3.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.3.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.3.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.3.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.3.2\" style=\"padding:0.75pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S5.T4.1.1.3.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.3.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.3.3.1.1\" style=\"width:39.8pt;\">54.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S5.T4.1.1.3.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.3.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.3.4.1.1\" style=\"width:39.8pt;\">46.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.4.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.4.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.4.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.4.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.4.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.4.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">BERT + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.4.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.4.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.4.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.4.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.4.3.1.1\" style=\"width:39.8pt;\">54.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.4.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.4.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.4.4.1.1\" style=\"width:39.8pt;\">47.4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.5.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.5.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.5.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.5.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.5.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.5.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">BERT + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.1.1.5.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.5.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.5.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.5.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.5.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.5.3.1.1\" style=\"width:39.8pt;\">53.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.5.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.5.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.5.4.1.1\" style=\"width:39.8pt;\">46.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.6.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.6.1.1\" style=\"background-color:#EBEBEB;\">BERT-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.6.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T4.1.1.6.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.6.3\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.6.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.6.3.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.6.3.1.1.1\" style=\"background-color:#EBEBEB;\">58.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.6.4\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.6.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.6.4.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.6.4.1.1.1\" style=\"background-color:#EBEBEB;\">50.0</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S5.T4.1.1.7.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.7.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.7.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.7.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.7.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.7.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib33\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.7.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.7.2\" style=\"padding:0.75pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T4.1.1.7.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.7.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.7.3.1.1\" style=\"width:39.8pt;\">59.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T4.1.1.7.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.7.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.7.4.1.1\" style=\"width:39.8pt;\">50.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.8\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.8.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.8.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.8.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.8.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.8.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.8.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">RoBERTa + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.8.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.8.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.8.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.8.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.8.3.1.1\" style=\"width:39.8pt;\">60.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.8.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.8.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.8.4.1.1\" style=\"width:39.8pt;\">50.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.9\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.9.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.9.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.9.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.9.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.9.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.9.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">RoBERTa + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.1.1.9.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.9.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.9.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.9.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.9.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.9.3.1.1\" style=\"width:39.8pt;\">60.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.9.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.9.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.9.4.1.1\" style=\"width:39.8pt;\">51.3</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.10\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.10.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.10.1.1\" style=\"background-color:#EBEBEB;\">RoBERTa-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.10.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T4.1.1.10.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.10.3\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.10.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.10.3.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.10.3.1.1.1\" style=\"background-color:#EBEBEB;\">62.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.10.4\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.10.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.10.4.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.10.4.1.1.1\" style=\"background-color:#EBEBEB;\">55.1</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.11\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S5.T4.1.1.11.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.11.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.11.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.11.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.11.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.11.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">ELECTRA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.11.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.11.2\" style=\"padding:0.75pt 0.0pt;\">-</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T4.1.1.11.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.11.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.11.3.1.1\" style=\"width:39.8pt;\">55.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T4.1.1.11.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.11.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.11.4.1.1\" style=\"width:39.8pt;\">42.3</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.12\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.12.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.12.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.12.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.12.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.12.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.12.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">ELECTRA + DINOv2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib38\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>]</cite></span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.12.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.12.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.12.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.12.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.12.3.1.1\" style=\"width:39.8pt;\">56.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.12.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.12.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.12.4.1.1\" style=\"width:39.8pt;\">43.6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.13\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.13.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.13.1.1\"></span><span class=\"ltx_text\" id=\"S5.T4.1.1.13.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.13.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.13.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T4.1.1.13.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">ELECTRA + MViT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite> (Lai <span class=\"ltx_text ltx_font_italic\" id=\"S5.T4.1.1.13.1.2.1.1.1.1\">et al.</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.02090v3#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T4.1.1.13.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T4.1.1.13.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.13.3\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.13.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.13.3.1.1\" style=\"width:39.8pt;\">55.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top\" id=\"S5.T4.1.1.13.4\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.13.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.13.4.1.1\" style=\"width:39.8pt;\">41.0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.14\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" id=\"S5.T4.1.1.14.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.14.1.1\" style=\"background-color:#EBEBEB;\">ELECTRA-based Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T4.1.1.14.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T4.1.1.14.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T4.1.1.14.3\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.14.3.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.14.3.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.14.3.1.1.1\" style=\"background-color:#EBEBEB;\">61.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T4.1.1.14.4\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.14.4.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.14.4.1.1\" style=\"width:39.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.14.4.1.1.1\" style=\"background-color:#EBEBEB;\">46.2</span></span>\n</span>\n</td>\n</tr>\n</table>\n</span></div>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T4.3.1.1\" style=\"font-size:90%;\">Table 4</span>: </span><span class=\"ltx_text\" id=\"S5.T4.4.2\" style=\"font-size:90%;\">Performance comparison results for the mentioned player prediction task on YouTube and Ego4D datasets.</span></figcaption>\n</figure>",
            "capture": "Table 4: Performance comparison results for the mentioned player prediction task on YouTube and Ego4D datasets."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel ltx_align_center\" id=\"S5.T5\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" id=\"S5.T5.1\" style=\"width:433.2pt;height:469.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(88.7pt,-96.1pt) scale(1.69313025384995,1.69313025384995) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T5.1.1.1.1\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.1\">Target Task</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S5.T5.1.1.1.2\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.2.1\">Method</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T5.1.1.1.3\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.3.1\">Accuracy (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T5.1.1.2.1\" rowspan=\"4\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T5.1.1.2.1.1\"><span class=\"ltx_text\" id=\"S5.T5.1.1.2.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T5.1.1.2.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.2.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.2.1.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Speaking Target</span></span>\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.2.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.2.1.1.2.1.2.1\" style=\"padding:0.75pt 0.0pt;\">Identification</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T5.1.1.2.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S5.T5.1.1.2.2\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T5.1.1.2.2.1\"></span><span class=\"ltx_text\" id=\"S5.T5.1.1.2.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.2.2.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.2.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.2.2.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">w/o Visual Features</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T5.1.1.2.2.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T5.1.1.2.3\" style=\"padding:0.75pt 0.0pt;\">65.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.3.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T5.1.1.3.1.1\"></span><span class=\"ltx_text\" id=\"S5.T5.1.1.3.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.3.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.3.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.3.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">w/o Gesture Feature</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T5.1.1.3.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.3.2\" style=\"padding:0.75pt 0.0pt;\">69.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.4.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T5.1.1.4.1.1\"></span><span class=\"ltx_text\" id=\"S5.T5.1.1.4.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.4.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.4.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.4.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">w/o Gaze Feature</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T5.1.1.4.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.4.2\" style=\"padding:0.75pt 0.0pt;\">70.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.5.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.5.1.1\" style=\"background-color:#EBEBEB;\">Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.5.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.5.2.1\" style=\"background-color:#EBEBEB;\">72.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.1\" rowspan=\"4\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T5.1.1.6.1.1\"><span class=\"ltx_text\" id=\"S5.T5.1.1.6.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T5.1.1.6.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.6.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.6.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.6.1.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Pronoun Coreference</span></span>\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.6.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.6.1.1.2.1.2.1\" style=\"padding:0.75pt 0.0pt;\">Resolution</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T5.1.1.6.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.6.2\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T5.1.1.6.2.1\"></span><span class=\"ltx_text\" id=\"S5.T5.1.1.6.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.6.2.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.6.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.6.2.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">w/o Visual Features</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T5.1.1.6.2.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.6.3\" style=\"padding:0.75pt 0.0pt;\">60.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.7.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T5.1.1.7.1.1\"></span><span class=\"ltx_text\" id=\"S5.T5.1.1.7.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.7.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.7.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.7.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">w/o Gesture Feature</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T5.1.1.7.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.7.2\" style=\"padding:0.75pt 0.0pt;\">64.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.8\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.8.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T5.1.1.8.1.1\"></span><span class=\"ltx_text\" id=\"S5.T5.1.1.8.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.8.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.8.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.8.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">w/o Gaze Feature</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T5.1.1.8.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.8.2\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.8.2.1\">66.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.9\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.9.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.9.1.1\" style=\"background-color:#EBEBEB;\">Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.9.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S5.T5.1.1.9.2.1\" style=\"background-color:#EBEBEB;\">65.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.10\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.1.1.10.1\" rowspan=\"4\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T5.1.1.10.1.1\"><span class=\"ltx_text\" id=\"S5.T5.1.1.10.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T5.1.1.10.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.10.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.10.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.10.1.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Mentioned Player</span></span>\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.10.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.10.1.1.2.1.2.1\" style=\"padding:0.75pt 0.0pt;\">Prediction</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T5.1.1.10.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.10.2\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T5.1.1.10.2.1\"></span><span class=\"ltx_text\" id=\"S5.T5.1.1.10.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.10.2.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.10.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.10.2.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">w/o Visual Features</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T5.1.1.10.2.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.10.3\" style=\"padding:0.75pt 0.0pt;\">54.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.11\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.11.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T5.1.1.11.1.1\"></span><span class=\"ltx_text\" id=\"S5.T5.1.1.11.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1.11.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T5.1.1.11.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.11.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">w/o Gesture Feature</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T5.1.1.11.1.3\"></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.11.2\" style=\"padding:0.75pt 0.0pt;\">55.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.12\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S5.T5.1.1.12.1\" style=\"padding:0.75pt 0.0pt;\">w/o Gaze Feature</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T5.1.1.12.2\" style=\"padding:0.75pt 0.0pt;\">56.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.13\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" id=\"S5.T5.1.1.13.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.13.1.1\" style=\"background-color:#EBEBEB;\">Our Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T5.1.1.13.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.13.2.1\" style=\"background-color:#EBEBEB;\">58.8</span></td>\n</tr>\n</table>\n</span></div>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T5.3.1.1\" style=\"font-size:90%;\">Table 5</span>: </span><span class=\"ltx_text\" id=\"S5.T5.4.2\" style=\"font-size:90%;\">Effects of non-verbal visual feature types on the performances for three social tasks. We adopt the BERT-based model and conduct evaluations on YouTube dataset.</span></figcaption>\n</figure>",
            "capture": "Table 5: Effects of non-verbal visual feature types on the performances for three social tasks. We adopt the BERT-based model and conduct evaluations on YouTube dataset."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel ltx_align_center\" id=\"S5.T6\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer\" id=\"S5.T6.1\" style=\"width:433.2pt;height:348.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(86.9pt,-69.9pt) scale(1.66965463344127,1.66965463344127) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.1\" rowspan=\"2\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.1.1\">Target Task</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S5.T6.1.1.1.2\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T6.1.1.1.2.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.2.2\"> <span class=\"ltx_text\" id=\"S5.T6.1.1.1.2.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1.1.2.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.1.2.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.1.2.2.1.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Conversation Context</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T6.1.1.1.2.2.2\"></span></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.3\" rowspan=\"2\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.3.1\">Accuracy (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.2.1\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T6.1.1.2.1.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.2.1.2\"> <span class=\"ltx_text\" id=\"S5.T6.1.1.2.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1.2.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.2.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.2.1.2.1.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Preceding</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T6.1.1.2.1.2.2\"></span></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.2.2\" style=\"padding:0.75pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T6.1.1.2.2.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.2.2.2\"> <span class=\"ltx_text\" id=\"S5.T6.1.1.2.2.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1.2.2.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.2.2.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.2.2.2.1.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Following</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T6.1.1.2.2.2.2\"></span></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.3.1\" rowspan=\"3\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.3.1.1\"><span class=\"ltx_text\" id=\"S5.T6.1.1.3.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T6.1.1.3.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1.3.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.3.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.3.1.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Speaking Target</span></span>\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.3.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.3.1.1.2.1.2.1\" style=\"padding:0.75pt 0.0pt;\">Identification</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T6.1.1.3.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.3.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.3.3\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.3.4\" style=\"padding:0.75pt 0.0pt;\">40.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.4.1\" style=\"padding:0.75pt 0.0pt;\">\u2713</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.4.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.4.3\" style=\"padding:0.75pt 0.0pt;\">59.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.5.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.5.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.5.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.5.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.5.3\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.5.3.1\" style=\"background-color:#EBEBEB;\">72.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.6.1\" rowspan=\"3\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.6.1.1\"><span class=\"ltx_text\" id=\"S5.T6.1.1.6.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T6.1.1.6.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1.6.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.6.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.6.1.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Pronoun Coreference</span></span>\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.6.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.6.1.1.2.1.2.1\" style=\"padding:0.75pt 0.0pt;\">Resolution</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T6.1.1.6.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.6.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.6.3\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.6.4\" style=\"padding:0.75pt 0.0pt;\">51.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.7.1\" style=\"padding:0.75pt 0.0pt;\">\u2713</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.7.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.7.3\" style=\"padding:0.75pt 0.0pt;\">63.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.8\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.8.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.8.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.8.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.8.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.8.3\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.8.3.1\" style=\"background-color:#EBEBEB;\">65.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.9\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T6.1.1.9.1\" rowspan=\"3\" style=\"padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.9.1.1\"><span class=\"ltx_text\" id=\"S5.T6.1.1.9.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T6.1.1.9.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1.9.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.9.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.9.1.1.2.1.1.1\" style=\"padding:0.75pt 0.0pt;\">Mentioned Player</span></span>\n<span class=\"ltx_tr\" id=\"S5.T6.1.1.9.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.9.1.1.2.1.2.1\" style=\"padding:0.75pt 0.0pt;\">Prediction</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T6.1.1.9.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.3\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.4\" style=\"padding:0.75pt 0.0pt;\">36.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.10\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.10.1\" style=\"padding:0.75pt 0.0pt;\">\u2713</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.10.2\" style=\"padding:0.75pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T6.1.1.10.3\" style=\"padding:0.75pt 0.0pt;\">47.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.11\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.11.1\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.11.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.11.2\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T6.1.1.11.2.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T6.1.1.11.3\" style=\"background-color:#EBEBEB;padding:0.75pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.11.3.1\" style=\"background-color:#EBEBEB;\">58.8</span></td>\n</tr>\n</table>\n</span></div>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T6.3.1.1\" style=\"font-size:90%;\">Table 6</span>: </span><span class=\"ltx_text\" id=\"S5.T6.4.2\" style=\"font-size:90%;\">Effects of the language conversation contexts on the performances for three social tasks. We adopt the BERT-based model and conduct evaluations on YouTube dataset.</span></figcaption>\n</figure>",
            "capture": "Table 6: Effects of the language conversation contexts on the performances for three social tasks. We adopt the BERT-based model and conduct evaluations on YouTube dataset."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel ltx_align_center\" id=\"S5.T7\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" id=\"S5.T7.1\" style=\"width:433.2pt;height:254.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(89.6pt,-52.7pt) scale(1.70600298905515,1.70600298905515) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T7.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.1.1\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.1.1.1\">Target Task</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.1.2\" style=\"padding:1pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"S5.T7.1.1.1.2.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.1.2.2\"> <span class=\"ltx_text\" id=\"S5.T7.1.1.1.2.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T7.1.1.1.2.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T7.1.1.1.2.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.1.2.2.1.1.1.1\" style=\"padding:1pt 0.0pt;\">Player Permutation</span></span>\n<span class=\"ltx_tr\" id=\"S5.T7.1.1.1.2.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.1.2.2.1.1.2.1\" style=\"padding:1pt 0.0pt;\">Learning</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T7.1.1.1.2.2.2\"></span></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.1.3\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.1.3.1\">Accuracy (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.2.1\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.1.1.2.1.1\"><span class=\"ltx_text\" id=\"S5.T7.1.1.2.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T7.1.1.2.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T7.1.1.2.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T7.1.1.2.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.2.1.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">Speaking Target</span></span>\n<span class=\"ltx_tr\" id=\"S5.T7.1.1.2.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.2.1.1.2.1.2.1\" style=\"padding:1pt 0.0pt;\">Identification</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T7.1.1.2.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.2.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T7.1.1.2.3\" style=\"padding:1pt 0.0pt;\">70.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.3.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.1.1.3.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.3.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.3.2.1\" style=\"background-color:#EBEBEB;\">72.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.4.1\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.1.1.4.1.1\"><span class=\"ltx_text\" id=\"S5.T7.1.1.4.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T7.1.1.4.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T7.1.1.4.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T7.1.1.4.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.4.1.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">Pronoun Coreference</span></span>\n<span class=\"ltx_tr\" id=\"S5.T7.1.1.4.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.4.1.1.2.1.2.1\" style=\"padding:1pt 0.0pt;\">Resolution</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T7.1.1.4.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.4.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.4.3\" style=\"padding:1pt 0.0pt;\">51.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.5.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.1.1.5.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.5.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.5.2.1\" style=\"background-color:#EBEBEB;\">65.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.1.1.6.1\" rowspan=\"2\" style=\"padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.1.1.6.1.1\"><span class=\"ltx_text\" id=\"S5.T7.1.1.6.1.1.1\"></span> <span class=\"ltx_text\" id=\"S5.T7.1.1.6.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T7.1.1.6.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T7.1.1.6.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.6.1.1.2.1.1.1\" style=\"padding:1pt 0.0pt;\">Mentioned Player</span></span>\n<span class=\"ltx_tr\" id=\"S5.T7.1.1.6.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T7.1.1.6.1.1.2.1.2.1\" style=\"padding:1pt 0.0pt;\">Prediction</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T7.1.1.6.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.6.2\" style=\"padding:1pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.6.3\" style=\"padding:1pt 0.0pt;\">52.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T7.1.1.7.1\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text\" id=\"S5.T7.1.1.7.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T7.1.1.7.2\" style=\"background-color:#EBEBEB;padding:1pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.7.2.1\" style=\"background-color:#EBEBEB;\">58.8</span></td>\n</tr>\n</table>\n</span></div>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T7.3.1.1\" style=\"font-size:90%;\">Table 7</span>: </span><span class=\"ltx_text\" id=\"S5.T7.4.2\" style=\"font-size:90%;\">Effects of player permutation learning on the performances for three social tasks. We adopt the BERT-based model and conduct evaluations on YouTube dataset.</span></figcaption>\n</figure>",
            "capture": "Table 7: Effects of player permutation learning on the performances for three social tasks. We adopt the BERT-based model and conduct evaluations on YouTube dataset."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T8\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T8.2\" style=\"width:325.2pt;height:904.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(23.4pt,-65.2pt) scale(1.16843495604134,1.16843495604134) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A1.T8.2.1\">\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_align_middle ltx_border_tt\" colspan=\"5\" id=\"A1.T8.2.1.1.1\" style=\"padding:-2pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T8.2.1.1.1.1\">Network Structures</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" id=\"A1.T8.2.1.2.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.2.1.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T8.2.1.2.1.1.1.1\">Module</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" id=\"A1.T8.2.1.2.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.2.2.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.2.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T8.2.1.2.2.1.1.1\">Layers</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" id=\"A1.T8.2.1.2.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.2.3.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.2.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T8.2.1.2.3.1.1.1\">Hidden Size</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" id=\"A1.T8.2.1.2.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.2.4.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.2.4.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T8.2.1.2.4.1.1.1\">MLP Size</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t\" id=\"A1.T8.2.1.2.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.2.5.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.2.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T8.2.1.2.5.1.1.1\">Multi-Heads</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.3.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.3.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.3.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.3.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.3.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" id=\"A1.T8.2.1.4.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.4.1.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.4.1.1.1\"><span class=\"ltx_text\" id=\"A1.T8.2.1.4.1.1.1.1\"></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.4.1.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A1.T8.2.1.4.1.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"A1.T8.2.1.4.1.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T8.2.1.4.1.1.1.2.1.1.1\" style=\"padding:-2pt 0.0pt;\">Point Encoder</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.4.1.1.1.3\"></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" id=\"A1.T8.2.1.4.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.4.2.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.4.2.1.1\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" id=\"A1.T8.2.1.4.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.4.3.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.4.3.1.1\">\u2013</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" id=\"A1.T8.2.1.4.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.4.4.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.4.4.1.1\">64</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_tt\" id=\"A1.T8.2.1.4.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.4.5.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.4.5.1.1\">\u2013</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.5.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.5.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.5.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.5.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.5.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.6.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.6.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.6.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.6.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.6.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.7.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.7.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.7.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.7.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.7.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.8\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.8.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.8.1.1\"><span class=\"ltx_ERROR undefined\" id=\"A1.T8.2.1.8.1.1.1\">\\hdashline</span>\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T8.2.1.8.1.1.2\"><span class=\"ltx_text\" id=\"A1.T8.2.1.8.1.1.2.1\"></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.8.1.1.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A1.T8.2.1.8.1.1.2.2.1\">\n<span class=\"ltx_tr\" id=\"A1.T8.2.1.8.1.1.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T8.2.1.8.1.1.2.2.1.1.1\" style=\"padding:-2pt 0.0pt;\">Kinesics Encoder</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.8.1.1.2.3\"></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.8.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.8.2.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.8.2.1.1\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.8.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.8.3.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.8.3.1.1\">\u2013</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.8.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.8.4.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.8.4.1.1\">512</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.8.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.8.5.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.8.5.1.1\">\u2013</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.9\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.9.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.9.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.9.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.9.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.9.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.10\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.10.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.10.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.10.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.10.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.10.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.11\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.11.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.11.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.11.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.11.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.11.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.12\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.12.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.12.1.1\"><span class=\"ltx_ERROR undefined\" id=\"A1.T8.2.1.12.1.1.1\">\\hdashline</span>\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T8.2.1.12.1.1.2\"><span class=\"ltx_text\" id=\"A1.T8.2.1.12.1.1.2.1\"></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.12.1.1.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A1.T8.2.1.12.1.1.2.2.1\">\n<span class=\"ltx_tr\" id=\"A1.T8.2.1.12.1.1.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T8.2.1.12.1.1.2.2.1.1.1\" style=\"padding:-2pt 0.0pt;\">Position Encoder</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.12.1.1.2.3\"></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.12.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.12.2.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.12.2.1.1\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.12.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.12.3.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.12.3.1.1\">\u2013</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.12.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.12.4.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.12.4.1.1\">512</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.12.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.12.5.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.12.5.1.1\">\u2013</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.13\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.13.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.13.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.13.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.13.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.13.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.14\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.14.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.14.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.14.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.14.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.14.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.15\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.15.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.15.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.15.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.15.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.15.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.16\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.16.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.16.1.1\"><span class=\"ltx_ERROR undefined\" id=\"A1.T8.2.1.16.1.1.1\">\\hdashline</span>\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T8.2.1.16.1.1.2\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:10.0pt;background:black;display:inline-block;\"></span><span class=\"ltx_text\" id=\"A1.T8.2.1.16.1.1.2.1\"><span class=\"ltx_text\" id=\"A1.T8.2.1.16.1.1.2.1.1\"></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.16.1.1.2.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A1.T8.2.1.16.1.1.2.1.2.1\">\n<span class=\"ltx_tr\" id=\"A1.T8.2.1.16.1.1.2.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T8.2.1.16.1.1.2.1.2.1.1.1\" style=\"padding:-2pt 0.0pt;\">Visual Interaction</span></span>\n<span class=\"ltx_tr\" id=\"A1.T8.2.1.16.1.1.2.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T8.2.1.16.1.1.2.1.2.1.2.1\" style=\"padding:-2pt 0.0pt;\">Encoder</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.16.1.1.2.1.3\"></span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.16.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.16.2.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.16.2.1.1\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.16.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.16.3.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.16.3.1.1\">512</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.16.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.16.4.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.16.4.1.1\">1024</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.16.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.16.5.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.16.5.1.1\">8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.17\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.17.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.17.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.17.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.17.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.17.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.18\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.18.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.18.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.18.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.18.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.18.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.19\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.19.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.19.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.19.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.19.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.19.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.20\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.20.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.20.1.1\"><span class=\"ltx_ERROR undefined\" id=\"A1.T8.2.1.20.1.1.1\">\\hdashline</span>\n<span class=\"ltx_p ltx_align_center\" id=\"A1.T8.2.1.20.1.1.2\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:10.0pt;background:black;display:inline-block;\"></span><span class=\"ltx_text\" id=\"A1.T8.2.1.20.1.1.2.1\"><span class=\"ltx_text\" id=\"A1.T8.2.1.20.1.1.2.1.1\"></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.20.1.1.2.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A1.T8.2.1.20.1.1.2.1.2.1\">\n<span class=\"ltx_tr\" id=\"A1.T8.2.1.20.1.1.2.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T8.2.1.20.1.1.2.1.2.1.1.1\" style=\"padding:-2pt 0.0pt;\">Aligned Multimodal</span></span>\n<span class=\"ltx_tr\" id=\"A1.T8.2.1.20.1.1.2.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A1.T8.2.1.20.1.1.2.1.2.1.2.1\" style=\"padding:-2pt 0.0pt;\">Transformer</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A1.T8.2.1.20.1.1.2.1.3\"></span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.20.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.20.2.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.20.2.1.1\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.20.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.20.3.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.20.3.1.1\">512</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.20.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.20.4.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.20.4.1.1\">1024</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.20.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A1.T8.2.1.20.5.1\">\n<span class=\"ltx_p\" id=\"A1.T8.2.1.20.5.1.1\">8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.21\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.21.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.21.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.21.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.21.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.21.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.22\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.22.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.22.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.22.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.22.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle\" id=\"A1.T8.2.1.22.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.2.1.23\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" id=\"A1.T8.2.1.23.1\" style=\"width:113.8pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" id=\"A1.T8.2.1.23.2\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" id=\"A1.T8.2.1.23.3\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" id=\"A1.T8.2.1.23.4\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb\" id=\"A1.T8.2.1.23.5\" style=\"width:56.9pt;padding:-2pt 0.0pt;\"></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T8.3.1.1\" style=\"font-size:90%;\">Table 8</span>: </span><span class=\"ltx_text\" id=\"A1.T8.4.2\" style=\"font-size:90%;\">Network structure details of the proposed baseline model including MLP and transformer structures.</span></figcaption>\n</figure>",
            "capture": "Table 8: Network structure details of the proposed baseline model including MLP and transformer structures."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel ltx_align_center\" id=\"A5.T9\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" id=\"A5.T9.1\" style=\"width:260.2pt;height:136.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-12.1pt,6.4pt) scale(0.914764301584141,0.914764301584141) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A5.T9.1.1\">\n<tr class=\"ltx_tr\" id=\"A5.T9.1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A5.T9.1.1.1.1\" style=\"padding:1.5pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.1.1.1.1.1\">Target Task</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A5.T9.1.1.1.2\" style=\"padding:1.5pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"A5.T9.1.1.1.2.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.1.1.1.2.2\"> <span class=\"ltx_text\" id=\"A5.T9.1.1.1.2.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A5.T9.1.1.1.2.2.1.1\">\n<span class=\"ltx_tr\" id=\"A5.T9.1.1.1.2.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.1.2.2.1.1.1.1\" style=\"padding:1.5pt 0.0pt;\">Player Position</span></span>\n<span class=\"ltx_tr\" id=\"A5.T9.1.1.1.2.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.1.2.2.1.1.2.1\" style=\"padding:1.5pt 0.0pt;\">Correction</span></span>\n</span></span><span class=\"ltx_text\" id=\"A5.T9.1.1.1.2.2.2\"></span></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A5.T9.1.1.1.3\" style=\"padding:1.5pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.1.1.1.3.1\">Accuracy (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A5.T9.1.1.2.1\" rowspan=\"2\" style=\"padding:1.5pt 0.0pt;\"><span class=\"ltx_text\" id=\"A5.T9.1.1.2.1.1\"><span class=\"ltx_text\" id=\"A5.T9.1.1.2.1.1.1\"></span> <span class=\"ltx_text\" id=\"A5.T9.1.1.2.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A5.T9.1.1.2.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"A5.T9.1.1.2.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.2.1.1.2.1.1.1\" style=\"padding:1.5pt 0.0pt;\">Speaking Target Identification</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A5.T9.1.1.2.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A5.T9.1.1.2.2\" style=\"padding:1.5pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A5.T9.1.1.2.3\" style=\"padding:1.5pt 0.0pt;\">70.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.3.1\" style=\"background-color:#EBEBEB;padding:1.5pt 0.0pt;\"><span class=\"ltx_text\" id=\"A5.T9.1.1.3.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.3.2\" style=\"background-color:#EBEBEB;padding:1.5pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.1.1.3.2.1\" style=\"background-color:#EBEBEB;\">72.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.1.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A5.T9.1.1.4.1\" rowspan=\"2\" style=\"padding:1.5pt 0.0pt;\"><span class=\"ltx_text\" id=\"A5.T9.1.1.4.1.1\"><span class=\"ltx_text\" id=\"A5.T9.1.1.4.1.1.1\"></span> <span class=\"ltx_text\" id=\"A5.T9.1.1.4.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A5.T9.1.1.4.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"A5.T9.1.1.4.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.4.1.1.2.1.1.1\" style=\"padding:1.5pt 0.0pt;\">Pronoun Coreference Resolution</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A5.T9.1.1.4.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A5.T9.1.1.4.2\" style=\"padding:1.5pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A5.T9.1.1.4.3\" style=\"padding:1.5pt 0.0pt;\">65.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.1.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.5.1\" style=\"background-color:#EBEBEB;padding:1.5pt 0.0pt;\"><span class=\"ltx_text\" id=\"A5.T9.1.1.5.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.5.2\" style=\"background-color:#EBEBEB;padding:1.5pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.1.1.5.2.1\" style=\"background-color:#EBEBEB;\">65.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.1.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"A5.T9.1.1.6.1\" rowspan=\"2\" style=\"padding:1.5pt 0.0pt;\"><span class=\"ltx_text\" id=\"A5.T9.1.1.6.1.1\"><span class=\"ltx_text\" id=\"A5.T9.1.1.6.1.1.1\"></span> <span class=\"ltx_text\" id=\"A5.T9.1.1.6.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A5.T9.1.1.6.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"A5.T9.1.1.6.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A5.T9.1.1.6.1.1.2.1.1.1\" style=\"padding:1.5pt 0.0pt;\">Mentioned Player Prediction</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A5.T9.1.1.6.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A5.T9.1.1.6.2\" style=\"padding:1.5pt 0.0pt;\">\u2717</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A5.T9.1.1.6.3\" style=\"padding:1.5pt 0.0pt;\">58.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.1.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"A5.T9.1.1.7.1\" style=\"background-color:#EBEBEB;padding:1.5pt 0.0pt;\"><span class=\"ltx_text\" id=\"A5.T9.1.1.7.1.1\" style=\"background-color:#EBEBEB;\">\u2713</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"A5.T9.1.1.7.2\" style=\"background-color:#EBEBEB;padding:1.5pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.1.1.7.2.1\" style=\"background-color:#EBEBEB;\">58.8</span></td>\n</tr>\n</table>\n</span></div>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A5.T9.3.1.1\" style=\"font-size:90%;\">Table 9</span>: </span><span class=\"ltx_text\" id=\"A5.T9.4.2\" style=\"font-size:90%;\">Effects of player position correction on the performances for three social tasks.</span></figcaption>\n</figure>",
            "capture": "Table 9: Effects of player position correction on the performances for three social tasks."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel ltx_align_center\" id=\"A6.T10\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer\" id=\"A6.T10.1\" style=\"width:303.5pt;height:120.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-13.2pt,5.3pt) scale(0.919871974654629,0.919871974654629) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A6.T10.1.1\">\n<tr class=\"ltx_tr\" id=\"A6.T10.1.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A6.T10.1.1.1.1\" style=\"padding:2pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T10.1.1.1.1.1\">Target Task</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A6.T10.1.1.1.2\" style=\"padding:2pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"A6.T10.1.1.1.2.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"A6.T10.1.1.1.2.2\"> <span class=\"ltx_text\" id=\"A6.T10.1.1.1.2.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A6.T10.1.1.1.2.2.1.1\">\n<span class=\"ltx_tr\" id=\"A6.T10.1.1.1.2.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.1.2.2.1.1.1.1\" style=\"padding:2pt 0.0pt;\">Test Data</span></span>\n</span></span><span class=\"ltx_text\" id=\"A6.T10.1.1.1.2.2.2\"></span></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A6.T10.1.1.1.3\" style=\"padding:2pt 0.0pt;\">\n<span class=\"ltx_text\" id=\"A6.T10.1.1.1.3.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"A6.T10.1.1.1.3.2\"> <span class=\"ltx_text\" id=\"A6.T10.1.1.1.3.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A6.T10.1.1.1.3.2.1.1\">\n<span class=\"ltx_tr\" id=\"A6.T10.1.1.1.3.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.1.3.2.1.1.1.1\" style=\"padding:2pt 0.0pt;\">Training Data</span></span>\n</span></span><span class=\"ltx_text\" id=\"A6.T10.1.1.1.3.2.2\"></span></span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A6.T10.1.1.1.4\" style=\"padding:2pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T10.1.1.1.4.1\">Accuracy (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T10.1.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A6.T10.1.1.2.1\" rowspan=\"2\" style=\"padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.2.1.1\"><span class=\"ltx_text\" id=\"A6.T10.1.1.2.1.1.1\"></span> <span class=\"ltx_text\" id=\"A6.T10.1.1.2.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A6.T10.1.1.2.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"A6.T10.1.1.2.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.2.1.1.2.1.1.1\" style=\"padding:2pt 0.0pt;\">Speaking Target Identification</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A6.T10.1.1.2.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A6.T10.1.1.2.2\" rowspan=\"2\" style=\"padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.2.2.1\">Ego4D</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A6.T10.1.1.2.3\" style=\"padding:2pt 0.0pt;\">Ego4D</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A6.T10.1.1.2.4\" style=\"padding:2pt 0.0pt;\">61.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T10.1.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.3.1\" style=\"background-color:#EBEBEB;padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.3.1.1\" style=\"background-color:#EBEBEB;\">YouTube</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.3.2\" style=\"background-color:#EBEBEB;padding:2pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T10.1.1.3.2.1\" style=\"background-color:#EBEBEB;\">70.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T10.1.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A6.T10.1.1.4.1\" rowspan=\"2\" style=\"padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.4.1.1\"><span class=\"ltx_text\" id=\"A6.T10.1.1.4.1.1.1\"></span> <span class=\"ltx_text\" id=\"A6.T10.1.1.4.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A6.T10.1.1.4.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"A6.T10.1.1.4.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.4.1.1.2.1.1.1\" style=\"padding:2pt 0.0pt;\">Pronoun Coreference Resolution</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A6.T10.1.1.4.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A6.T10.1.1.4.2\" rowspan=\"2\" style=\"padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.4.2.1\">Ego4D</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A6.T10.1.1.4.3\" style=\"padding:2pt 0.0pt;\">Ego4D</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A6.T10.1.1.4.4\" style=\"padding:2pt 0.0pt;\">49.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T10.1.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.5.1\" style=\"background-color:#EBEBEB;padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.5.1.1\" style=\"background-color:#EBEBEB;\">YouTube</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.5.2\" style=\"background-color:#EBEBEB;padding:2pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T10.1.1.5.2.1\" style=\"background-color:#EBEBEB;\">58.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T10.1.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"A6.T10.1.1.6.1\" rowspan=\"2\" style=\"padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.6.1.1\"><span class=\"ltx_text\" id=\"A6.T10.1.1.6.1.1.1\"></span> <span class=\"ltx_text\" id=\"A6.T10.1.1.6.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A6.T10.1.1.6.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"A6.T10.1.1.6.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"A6.T10.1.1.6.1.1.2.1.1.1\" style=\"padding:2pt 0.0pt;\">Mentioned Player Prediction</span></span>\n</span></span> <span class=\"ltx_text\" id=\"A6.T10.1.1.6.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"A6.T10.1.1.6.2\" rowspan=\"2\" style=\"padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.6.2.1\">Ego4D</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A6.T10.1.1.6.3\" style=\"padding:2pt 0.0pt;\">Ego4D</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"A6.T10.1.1.6.4\" style=\"padding:2pt 0.0pt;\">50.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T10.1.1.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"A6.T10.1.1.7.1\" style=\"background-color:#EBEBEB;padding:2pt 0.0pt;\"><span class=\"ltx_text\" id=\"A6.T10.1.1.7.1.1\" style=\"background-color:#EBEBEB;\">YouTube</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"A6.T10.1.1.7.2\" style=\"background-color:#EBEBEB;padding:2pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T10.1.1.7.2.1\" style=\"background-color:#EBEBEB;\">57.3</span></td>\n</tr>\n</table>\n</span></div>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A6.T10.3.1.1\" style=\"font-size:90%;\">Table 10</span>: </span><span class=\"ltx_text\" id=\"A6.T10.4.2\" style=\"font-size:90%;\">Performance results according to the training data types for three social tasks.</span></figcaption>\n</figure>",
            "capture": "Table 10: Performance results according to the training data types for three social tasks."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.02090v3_figure_1.png",
            "caption": "Figure 1: Concepts of the proposed three social tasks in multi-party environments: speaking target identification, pronoun coreference resolution, and mentioned player prediction."
        },
        "2": {
            "figure_path": "2403.02090v3_figure_2.png",
            "caption": "Figure 2: Concept of densely aligned language-visual representations. People are matched in the language and visual domains."
        },
        "3": {
            "figure_path": "2403.02090v3_figure_3.png",
            "caption": "Figure 3: Proposed baseline model for understanding multimodal social interactions to tackle our new social tasks via densely aligned language-visual representations. The model consists of four main parts: language-visual alignment (grey), visual interaction modeling (green & purple), conversation context modeling (red), and aligned multimodal fusion for prediction (blue)."
        },
        "4": {
            "figure_path": "2403.02090v3_figure_4.png",
            "caption": "Figure 4: Qualitative results demonstrating the benefit of visual cues for three social tasks. The examples show cases where the language model alone fails, but the proposed multimodal baseline leveraging both language and visual cues correctly predicts the right person. Note that Player# are assigned in ascending order from left to right in the visual scenes of this figure."
        },
        "5": {
            "figure_path": "2403.02090v3_figure_5.png",
            "caption": "Figure 5: Effects of conversation context length on the performance for speaking target identification."
        },
        "6": {
            "figure_path": "2403.02090v3_figure_6.png",
            "caption": "Figure 6: Effects of conversation context length on the performance for pronoun coreference resolution."
        },
        "7": {
            "figure_path": "2403.02090v3_figure_7.png",
            "caption": "Figure 7: Effects of conversation context length on the performance for mentioned player prediction."
        },
        "8": {
            "figure_path": "2403.02090v3_figure_8.png",
            "caption": "Figure 8: Effects of video length on the performance for speaking target identification."
        },
        "9": {
            "figure_path": "2403.02090v3_figure_9.png",
            "caption": "Figure 9: Effects of video length on the performance for pronoun coreference resolution."
        },
        "10": {
            "figure_path": "2403.02090v3_figure_10.png",
            "caption": "Figure 10: Effects of video length on the performance for mentioned player prediction."
        },
        "11": {
            "figure_path": "2403.02090v3_figure_11.png",
            "caption": "Figure 11: Qualitative results demonstrating the benefit of visual cues in multimodal analysis for three social tasks. Note that Player# are assigned in ascending order from left to right in the visual scenes of this figure."
        }
    },
    "references": [
        {
            "1": {
                "title": "No gestures left behind: Learning relationships between spoken language and freeform gestures.",
                "author": "Chaitanya Ahuja, Dong Won Lee, Ryo Ishii, and Louis-Philippe Morency.",
                "venue": "In Findings of Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP), pages 1884\u20131895, 2020.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Continual learning for personalized co-speech gesture generation.",
                "author": "Chaitanya Ahuja, Pratik Joshi, Ryo Ishii, and Louis-Philippe Morency.",
                "venue": "In IEEE/CVF International Conference on Computer Vision (ICCV), pages 20893\u201320903, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Data-free class-incremental hand gesture recognition.",
                "author": "Shubhra Aich, Jesus Ruiz-Santaquiteria, Zhenyu Lu, Prachi Garg, KJ Joseph, Alvaro Fernandez Garcia, Vineeth N Balasubramanian, Kenrick Kin, Chengde Wan, Necati Cihan Camgoz, et al.",
                "venue": "In IEEE/CVF International Conference on Computer Vision (ICCV), pages 20958\u201320967, 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Emotional speech corpus for persuasive dialogue system.",
                "author": "Sara Asai, Koichiro Yoshino, Seitaro Shinagawa, Sakriani Sakti, and Satoshi Nakamura.",
                "venue": "In Language Resources and Evaluation Conference (LREC), pages 491\u2013497, 2020.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Human-side strategies in the werewolf game against the stealth werewolf strategy.",
                "author": "Xiaoheng Bi and Tetsuro Tanaka.",
                "venue": "In International Conference on Computers and Games (ICCG), pages 93\u2013102, 2016.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Mafia: A theoretical study of players and coalitions in a partial information environment.",
                "author": "Mark Braverman, Omid Etesami, and Elchanan Mossel.",
                "venue": "The Annals of Applied Probability, 18(3):825\u2013846, 2008.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Assessing agreement on classification tasks: the kappa statistic.",
                "author": "Jean Carletta.",
                "venue": "Computational Linguistics, 22(2):249\u2013254, 1996.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Casino: A corpus of campsite negotiation dialogues for automatic negotiation systems.",
                "author": "Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May, and Jonathan Gratch.",
                "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT), pages 3167\u20133185, 2021.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation.",
                "author": "Feiyu Chen, Jie Shao, Shuyuan Zhu, and Heng Tao Shen.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10761\u201310770, 2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game.",
                "author": "Gokul Chittaranjan and Hayley Hung.",
                "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5334\u20135337, 2010.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Detecting attended visual targets in video.",
                "author": "Eunji Chong, Yongxin Wang, Nataniel Ruiz, and James M Rehg.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5396\u20135406, 2020.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Training an assassin ai for the resistance: Avalon.",
                "author": "Robert Chuchro.",
                "venue": "arXiv preprint arXiv:2209.09331, 2022.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Electra: Pre-training text encoders as discriminators rather than generators.",
                "author": "Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning.",
                "venue": "In International Conference on Learning Representations (ICLR), 2020.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Human-level play in the game of diplomacy by combining language models with strategic reasoning.",
                "author": "Meta Fundamental AI Research Diplomacy Team (FAIR)\u2020, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al.",
                "venue": "Science, 378(6624):1067\u20131074, 2022.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Multiscale vision transformers.",
                "author": "Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.",
                "venue": "In IEEE/CVF International Conference on Computer Vision (ICCV), pages 6824\u20136835, 2021.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Inferring shared attention in social scene videos.",
                "author": "Lifeng Fan, Yixin Chen, Ping Wei, Wenguan Wang, and Song-Chun Zhu.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6460\u20136468, 2018.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time.",
                "author": "Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu.",
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Dual attention guided gaze target detection in the wild.",
                "author": "Yi Fang, Jiapeng Tang, Wang Shen, Wei Shen, Xiao Gu, Li Song, and Guangtao Zhai.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11390\u201311399, 2021.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Ego4d: Around the world in 3,000 hours of egocentric video.",
                "author": "Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18995\u201319012, 2022.",
                "url": null
            }
        },
        {
            "20": {
                "title": "A probabilistic model of gaze imitation and shared attention.",
                "author": "Matthew W Hoffman, David B Grimes, Aaron P Shon, and Rajesh PN Rao.",
                "venue": "Neural Networks, 19(3):299\u2013310, 2006.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations.",
                "author": "Dou Hu, Xiaolong Hou, Lingwei Wei, Lianxin Jiang, and Yang Mo.",
                "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7037\u20137041, 2022a.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition.",
                "author": "Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, and Yongbin Li.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7837\u20137851, 2022b.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition.",
                "author": "Wenxiang Jiao, Haiqin Yang, Irwin King, and Michael R Lyu.",
                "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT), pages 397\u2013406, 2019.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova.",
                "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT), pages 4171\u20134186, 2019.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Adam: A method for stochastic optimization.",
                "author": "Diederik P Kingma and Jimmy Ba.",
                "venue": "In International Conference on Learning Representations (ICLR), 2015.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Content analysis: An introduction to its methodology.",
                "author": "Klaus Krippendorff.",
                "venue": "Sage publications, 2018.",
                "url": null
            }
        },
        {
            "27": {
                "title": "In the eye of transformer: Global-local correlation for egocentric gaze estimation.",
                "author": "Bolin Lai, Miao Liu, Fiona Ryan, and James M Rehg.",
                "venue": "In The British Machine Vision Conference (BMVC), 2022.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Werewolf among us: Multimodal resources for modeling persuasion behaviors in social deduction games.",
                "author": "Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati, James Rehg, and Diyi Yang.",
                "venue": "In Findings of the Association for Computational Linguistics (Findings of ACL), pages 6570\u20136588, 2023.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Crossmodal clustered contrastive learning: Grounding of spoken language to gesture.",
                "author": "Dong Won Lee, Chaitanya Ahuja, and Louis-Philippe Morency.",
                "venue": "In Companion Publication of International Conference on Multimodal Interaction (ICMI), pages 202\u2013210, 2021.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Learning robust representations with information bottleneck and memory network for rgb-d-based gesture recognition.",
                "author": "Yunan Li, Huizhou Chen, Guanwen Feng, and Qiguang Miao.",
                "venue": "In IEEE/CVF International Conference on Computer Vision (ICCV), pages 20968\u201320978, 2023.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Decoupled representation learning for skeleton-based gesture recognition.",
                "author": "Jianbo Liu, Yongcheng Liu, Ying Wang, Veronique Prinet, Shiming Xiang, and Chunhong Pan.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5751\u20135760, 2020.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Learning hierarchical cross-modal association for co-speech gesture generation.",
                "author": "Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei Zhou.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10462\u201310472, 2022.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Roberta: A robustly optimized bert pretraining approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.",
                "venue": "arXiv preprint arXiv:1907.11692, 2019.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Speaker and time-aware joint contextual learning for dialogue-act classification in counselling conversations.",
                "author": "Ganeshan Malhotra, Abdul Waheed, Aseem Srivastava, Md Shad Akhtar, and Tanmoy Chakraborty.",
                "venue": "In ACM International Conference on Web Search and Data Mining (WSDM), pages 735\u2013745, 2022.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Constructing a human-like agent for the werewolf game using a psychological model based multiple perspectives.",
                "author": "Noritsugu Nakamura, Michimasa Inaba, Kenichi Takahashi, Fujio Toriumi, Hirotaka Osawa, Daisuke Katagami, and Kousuke Shinoda.",
                "venue": "In IEEE Symposium Series on Computational Intelligence (SSCI), pages 1\u20138, 2016.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Interaction-aware joint attention estimation using people attributes.",
                "author": "Chihiro Nakatani, Hiroaki Kawashima, and Norimichi Ukita.",
                "venue": "In IEEE/CVF International Conference on Computer Vision (ICCV), pages 10224\u201310233, 2023.",
                "url": null
            }
        },
        {
            "37": {
                "title": "External commonsense knowledge as a modality for social intelligence question-answering.",
                "author": "Sanika Natu, Shounak Sural, and Sulagna Sarkar.",
                "venue": "In IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3044\u20133050, 2023.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Dinov2: Learning robust visual features without supervision.",
                "author": "Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al.",
                "venue": "Transactions on Machine Learning Research, 2023.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Mmlatch: Bottom-up top-down fusion for multimodal sentiment analysis.",
                "author": "Georgios Paraskevopoulos, Efthymios Georgiou, and Alexandras Potamianos.",
                "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4573\u20134577, 2022.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Dcr-net: A deep co-interactive relation network for joint dialog act recognition and sentiment classification.",
                "author": "Libo Qin, Wanxiang Che, Yangming Li, Mingheng Ni, and Ting Liu.",
                "venue": "In AAAI Conference on Artificial Intelligence (AAAI), pages 8665\u20138672, 2020.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Co-gat: A co-interactive graph attention network for joint dialog act recognition and sentiment classification.",
                "author": "Libo Qin, Zhouyang Li, Wanxiang Che, Minheng Ni, and Ting Liu.",
                "venue": "In AAAI Conference on Artificial Intelligence (AAAI), pages 13709\u201313717, 2021.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.",
                "venue": "In International Conference on Machine Learning (ICML), pages 8748\u20138763, 2021.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Towards emotion-aided multi-modal dialogue act classification.",
                "author": "Tulika Saha, Aditya Patra, Sriparna Saha, and Pushpak Bhattacharyya.",
                "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL), pages 4361\u20134372, 2020.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Finding friend and foe in multi-agent games.",
                "author": "Jack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum.",
                "venue": "Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Directed acyclic graph network for conversational emotion recognition.",
                "author": "Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun Quan.",
                "venue": "In Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 1551\u20131560, 2021.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Attention flow: End-to-end joint attention estimation.",
                "author": "Omer Sumer, Peter Gerjets, Ulrich Trautwein, and Enkelejda Kasneci.",
                "venue": "In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 3327\u20133336, 2020.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Dialogue act classification with context-aware self-attention.",
                "author": "Vipul Raheja Joel Tetreault.",
                "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT), pages 3727\u20133733, 2019.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Object-aware gaze target detection.",
                "author": "Francesco Tonini, Nicola Dall\u2019Asen, Cigdem Beyan, and Elisa Ricci.",
                "venue": "In IEEE/CVF International Conference on Computer Vision (ICCV), pages 21860\u201321869, 2023.",
                "url": null
            }
        },
        {
            "49": {
                "title": "End-to-end human-gaze-target detection with transformers.",
                "author": "Danyang Tu, Xiongkuo Min, Huiyu Duan, Guodong Guo, Guangtao Zhai, and Wei Shen.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2192\u20132200, 2022.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.",
                "venue": "Advances in Neural Information Processing Systems (NeurIPS), 30, 2017.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Persuasion for good: Towards a personalized persuasive dialogue system for social good.",
                "author": "Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu.",
                "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL), pages 5635\u20135649, 2019.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Transformers: State-of-the-art natural language processing.",
                "author": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al.",
                "venue": "In Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP-Demos), pages 38\u201345, 2020.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Multi-modal correlated network with emotional reasoning knowledge for social intelligence question-answering.",
                "author": "Baijun Xie and Chung Hyuk Park.",
                "venue": "In IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3075\u20133081, 2023.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Social-iq: A question answering benchmark for artificial social intelligence.",
                "author": "Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8807\u20138817, 2019.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.",
                "author": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency.",
                "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL), pages 2236\u20132246, 2018.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Attention in convolutional lstm for gesture recognition.",
                "author": "Liang Zhang, Guangming Zhu, Lin Mei, Peiyi Shen, Syed Afaq Ali Shah, and Mohammed Bennamoun.",
                "venue": "Advances in Neural Information Processing Systems (NeurIPS), 31, 2018.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Knowledge-bridged causal interaction network for causal emotion entailment.",
                "author": "Weixiang Zhao, Yanyan Zhao, Zhuojun Li, and Bing Qin.",
                "venue": "In AAAI Conference on Artificial Intelligence (AAAI), pages 14020\u201314028, 2023.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Decoupling and recoupling spatiotemporal representation for rgb-d-based motion recognition.",
                "author": "Benjia Zhou, Pichao Wang, Jun Wan, Yanyan Liang, Fan Wang, Du Zhang, Zhen Lei, Hao Li, and Rong Jin.",
                "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20154\u201320163, 2022.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection.",
                "author": "Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou, and Yulan He.",
                "venue": "In Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 1571\u20131582, 2021.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.02090v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.02090v3",
        "paper_title": "Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations",
        "research_background": "The paper \"Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations\" delves into the complexities of real-world social interactions by integrating verbal and non-verbal cues to more accurately interpret social contexts. The motivation behind this study stems from the limitations of previous research in capturing the nuanced dynamics involved in multi-party social settings. The authors aim to address a key research problem: the need for fine-grained modeling of interactions among multiple individuals by jointly considering language and visual cues at an individual level rather than holistic global representations.\n\nTo tackle this problem, the paper introduces three specific tasks centered around multi-party interactions:\n1. **Speaking target identification**: Identifying who a speaker is addressing in a conversation.\n2. **Pronoun coreference resolution**: Determining the referent of pronouns used in the conversation.\n3. **Mentioned player prediction**: Predicting which individual is mentioned by name in a conversation.\n\nThese tasks necessitate understanding the intricate details of player interactions, such as comprehensively interpreting both verbal content and accompanying visual gestures to parse out social intents and contexts.\n\n**Relevant Prior Work**: The paper builds upon and critiques existing works in multimodal social behavior analysis. Previous efforts, such as those focused on gesture generation and grounding [1, 2, 29, 32], as well as emotion and sentiment recognition through language and visual cues [9, 21, 22, 39, 43, 55], predominantly either concentrated on single-person behaviors or used holistic visual representations in multi-party scenarios. A notable exception is the work by Lai et al. [28], which investigated social behaviors in social deduction games. However, it primarily targeted single-person social behaviors and utilized holistic scene representations, thus falling short in capturing the finer dynamics of interactions among multiple individuals.\n\nThe paper sets a precedent by introducing a more granular, multimodal approach that aligns individual language references with corresponding visual cues. This representation enables the simultaneous tracking and decoding of who is speaking, who is being spoken to, and the relational dynamics conveyed through gestures within visual scenes.\n\n**Contributions**: The paper's contributions include:\n- Introducing new tasks that delve into the fine-grained dynamics of multi-party social interactions with extensive data annotations.\n- Proposing a novel baseline model that leverages densely aligned multimodal representations to better understand and predict the intricacies of multi-party social interactions, setting it apart as pioneering work in the domain of multimodal dense alignment.\n\nIn summary, the paper addresses a gap in multi-party social interaction modeling by proposing new tasks and a baseline model capable of densely aligning language and visual cues to capture the detailed social dynamics among individuals.",
        "methodology": "The introduced social tasks can be formulated as follows. Let \\( T \\) denote utterance texts that include the \\( i \\)-th target utterance containing a [MASK] token representing an unknown player, while \\( V \\) indicates the corresponding video frames aligned with the utterance timeline. Given \\( T \\) and \\( V \\), our objective is to optimize the multimodal predictive function \\( F(\\cdot) \\) to effectively classify the identity of the player associated with the [MASK].\n\nTo this end, we introduce a novel multimodal baseline that leverages densely aligned representations between language and visual cues to capture the fine-grained dynamics of interactions. The proposed model consists of four main parts:\n  (i) **language-visual alignment**,\n  (ii) **visual interaction modeling**,\n  (iii) **conversation context modeling**, and\n  (iv) **aligned multimodal fusion for prediction**.",
        "main_experiment_and_results": "### Main Experiment Setup\n**Datasets:**\n1. **YouTube Dataset:**\n   - Source: YouTube video platform.\n   - Content: 151 games of One Night Ultimate Werewolf (151 videos, 14.8 hours).\n   - Annotations: Videos, transcripts, player roles, voting outcomes, persuasion strategy labels.\n   - Transcripts: 20,832 utterances.\n\n2. **Ego4D Dataset:**\n   - Source: Subset of Ego4D Social dataset.\n   - Content: 40 games of One Night Ultimate Werewolf and 8 games of The Resistance: Avalon (101 videos, 7.3 hours; used 83 videos).\n   - Annotations: Videos, transcripts, player roles, voting outcomes, persuasion strategy labels.\n   - Transcripts: 5,815 utterances during the games.\n\n### Main Results\nThe results section is not provided in the text snippet. Please refer to the original document for detailed metrics and outcome analysis to understand the performance and impact of various models and representations on these datasets."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Analyze the contribution of different visual features to the performance of a multimodal baseline model on social tasks.",
            "experiment_process": "The study involves evaluating the model performance with different types of encoded non-verbal cues (gesture and gaze features). The final baseline model uses keypoints related to gaze and gesture (nose, l-eye, r-eye, l-shoulder, r-shoulder, l-elbow, r-elbow, l-wrist, r-wrist). Ablation experiments include two settings: one without gesture features, using only head-related keypoints (nose, l-eye, r-eye), and another without gaze features, using only gesture-related keypoints (l-shoulder, r-shoulder, l-elbow, r-elbow, l-wrist, r-wrist). A BERT-based baseline is used for evaluation on the YouTube dataset.",
            "result_discussion": "The results indicate that gesture features are more dominant than gaze features in this setting. The models using both gesture and gaze features generally achieve good performances.",
            "ablation_id": "2403.02090v3.No1"
        },
        {
            "research_objective": "Examine the effects of conversation context on the understanding and interpretation of social tasks.",
            "experiment_process": "The study investigates the model's performance with different context settings using a BERT-based baseline model on the YouTube dataset. Three variations are compared: the target utterance without additional context, incorporation of only the preceding context, and a baseline using both preceding and following contexts.",
            "result_discussion": "Leveraging both preceding and following contexts yields the best results for all social tasks. It is noted that the advantage of using following context is relatively small in pronoun coreference resolution because this task typically requires resolving pronouns against people in the preceding context.",
            "ablation_id": "2403.02090v3.No2"
        },
        {
            "research_objective": "Validate the effectiveness of player permutation learning by assessing its impact on the model's performance.",
            "experiment_process": "The ablation study involves training models with and without player permutation learning, which shuffles anonymized player identities. The performances for three tasks are evaluated using a BERT-based baseline on the YouTube dataset.",
            "result_discussion": "The permutation learning approach consistently improves performance across all tasks, suggesting that it helps the model learn more generalizable representations of player interactions. This approach is applied to all comparison methods for fair comparisons.",
            "ablation_id": "2403.02090v3.No3"
        }
    ]
}