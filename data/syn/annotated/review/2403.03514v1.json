{
    "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
    "abstract": "Developing Large Language Models (LLMs) with robust long-context capabilities has been the recent research focus, resulting in the emergence of long-context LLMs  proficient in Chinese.\nHowever, the evaluation of these models remains underdeveloped due to a lack of benchmarks.\nTo address this gap, we present CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs. CLongEval is characterized by three key features: (1) Sufficient data volume, comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability, accommodating to models with context windows size from 1K to 100K; (3) High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels.\nWith CLongEval, we undertake a comprehensive assessment of 6 open-source long-context LLMs and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese.\nWe also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings.111The dataset, evaluation scripts, and model outputs are released in https://github.com/zexuanqiu/CLongEval. We will regularly update the test results for newly released long-context LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks, including machine translation Hendy et al. (2023  ###reference_b15###); Jiao et al. (2023  ###reference_b20###), fact checking Huang et al. (2023  ###reference_b17###), text style transfer Reif et al. (2022  ###reference_b39###); Li et al. (2020  ###reference_b26###) and other generation tasks Hu et al. (2023  ###reference_b16###); Li et al. (2022  ###reference_b25###).\nTo enable LLMs to support more intricate and diverse applications, an increasing number of studies focus on extending the context window these models can handle. Consequently, many long-context LLMs that support Chinese have emerged, including both commercial models OpenAI (2023  ###reference_b33###) and open-source ones Cui (2023  ###reference_b9###); Bai et al. (2023a  ###reference_b2###); ZhupuAI (2023  ###reference_b50###); InternLMTeam (2024  ###reference_b18###), of which the context lengths span from 32K to 200K. Despite these developments, the efficacy of models in long-context settings remains underexamined, primarily due to the lack of a robust evaluation benchmark.\n###figure_1### Recently, a few benchmarks have been proposed for the evaluation of English long-context LLMs An et al. (2023  ###reference_b1###); Bai et al. (2023b  ###reference_b3###). As for the Chinese domain, there exists only a bilingual benchmark Bai et al. (2023b  ###reference_b3###), wherein only 5 out of 21 test tasks are designed for Chinese. This benchmark offers only 1K instances in total, with an average token length capped at approximately 13K222The reported average length is 13,386 characters, yielding at most 13K tokens after tokenization., rendering it inadequate for a comprehensive evaluation. Therefore, there is an urgent need for a high-quality Chinese benchmark for long-context LLMs. Considering the considerable advancements in this field, the establishment of such a benchmark facilitates a thorough investigation of existing models, which might bring insights to the research community.\nIn this paper, we present CLongEval, a benchmark designed for evaluating Chinese long-context LLMs. Prior to the construction of datasets, we conduct a systematic analysis of the key capabilities requisite for handling long context,\nto ensure a thorough assessment of the model\u2019s functionality (\u00a7 2  ###reference_###). Analogous to the human problem-solving paradigm, the basic functionalities of long-context LLMs can be conceptualized as: (1) the capacity to precisely identify and acquire the key information framing in either partial or full context; and (2)\nthe competence to reason out the answer based on the given information in either an extractive or abstractive manner.\nThese key abilities establish the evaluation framework behind CLongEval, which is illustrated in Figure 1  ###reference_###.\nTo accommodate models with varying spans of context windows, we consider three subsets within CLongEval: small set (1K-16K), medium set (16K-50K), and large set (50K-100K) (\u00a7 3.1  ###reference_###).\nIn dataset construction, we select test tasks that correspond to the capabilities outlined in the evaluation framework. Moreover, we ensure that primary test tasks are highly aligned with real-life user scenarios so that the benchmark can accurately reflect models\u2019 capability in practical applications Xiong et al. (2023  ###reference_b46###) (\u00a7 3.2  ###reference_###).\nOverall, we craft 7 distinct tasks in CLongEval: 2 tasks are human-annotated, 1 task is GPT-4-annotated and 4 tasks are re-constructed from public datasets.\nWith CLongEval, we evaluate 8 long-context LLMs proficient in Chinese, including two commercial models known for their powerful long-text processing capability: Moonshot-v1-128K and GPT-4-Turbo-128K (\u00a7 4.3  ###reference_###). We highlight several key findings in the long-context setting: (1) The commercial models consistently outperform open-source models across tasks, and the performance gap is particularly evident in tasks that primarily involve straightforward information extraction. (2) Extraction with full context is the most challenging setting. GPT-4-Turbo displays a more significant decline in performance as context length increases, compared to other settings. (3) For tasks requiring an understanding of partial context, the answer\u2019s position within a long context does not consistently lead to significant performance fluctuations. More analysis and observations are elaborated in \u00a7 4.4  ###reference_### and Appendix A  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Evaluation Framework in CLongEval",
            "text": "To offer a thorough and systematic evaluation, we analyze the key capabilities necessary for the efficacy of long-context LLMs.\nGenerally speaking, the capacity for a long-context LLM in interpreting human textual instructions largely depends on its ability of information acquisition. Moreover, an indispensable ability for these models extends beyond mere information collection to encompass reasoning based on the assimilated information.\nIt refers to the capability to recognize and parse relevant information framed in extensive and complex textual input. It bottlenecks LLMs\u2019 effectiveness in synthesizing its contextualized knowledge to execute a wide array of tasks, from answering questions to carrying out complex instructions. Moreover, as the length of the input text increases, maintaining a coherent and precise grasp of the input information becomes increasingly challenging.\nIn this evaluation dimension, we conduct a two-fold classification based on the distribution of information requisite for task fulfillment: full-context and partial-context information acquisition. For each category, we introduce an array of test tasks tailored to assess the model\u2019s proficiency, including tasks that demand an accurate comprehension of the entire input (i.e. full-context), and those that rely on a correct understanding of selective snippets of the input (i.e. partial-context), respectively. Figure 1  ###reference_### illustrates the categorization of tasks that require either partial- or full-context in information acquisition.\nIt refers to the ability to perform the inferential process of synthesizing a conclusion from presented lengthy statements Pi et al. (2022  ###reference_b36###).\nIn real-world applications, most tasks require not only a precise understanding of the input text but also the capacity of reasoning based on the provided information. LLMs equipped with proficient reasoning abilities can navigate the problem-solving and decision-making procedure, both of which are crucial cognitive functions necessary for handling complex tasks Pomerol (1997  ###reference_b37###).\nIn the reasoning process, outputs can be synthesized through two ways: content extraction and abstraction. Accordingly, the evaluation dimension of reasoning incorporates these two distinct types of test tasks.\nThe abstraction tasks involve generating new content not explicitly in the source material, demanding a deeper understanding and recombination of input. In contrast, extraction tasks assess the model\u2019s ability to directly identify and extract information from the input without altering the original content. This framework enables a nuanced evaluation of LLMs\u2019 reasoning capabilities, including the capacity for generating novel insights and accurately retrieving information.\nFigure 1  ###reference_### illustrates the tasks to test the reasoning ability in either an extractive or abstractive manner.\nMin\nMax\n#Data\nMin\nMax\n#Data\nMin\nMax\n#Data\n1,693\n16,224\n294\n14,437\n50,553\n398\n49,636\n99,038\n299\n768\n15,589\n358\n14,669\n43,225\n353\n41,481\n88,731\n356\n1,022\n13,716\n300\n12,958\n48,677\n400\n42,046\n87,092\n300\n738\n13,521\n303\n13,635\n44,100\n402\n43,875\n84,475\n300\n1,069\n16,385\n550\n15,956\n51,016\n300\n51,404\n98,803\n150\n1,249\n17,830\n400\n18,006\n56,086\n400\n55,367\n95,073\n300\n1,773\n24,597\n400\n23,894\n74,004\n400\n73,984\n125,529\n300"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The CLongEval Benchmark",
            "text": "The Long Story QA task involves LLMs answering questions based on a context snippet from a long story. To excel at this task, the model should identify the relevant snippet and abstractively reason out the answer. Unlike the normative and objective nature of MultiFieldQA (Bai et al., 2023b  ###reference_b3###), the stories we choose are narrative, creative, and inherently longer, offering a valuable addition to lengthy single-document QA evaluation. Inspired by NarrativeQA (Ko\u010disk\u1ef3 et al., 2018  ###reference_b21###), this task involves annotated questions that refer to non-consecutive parts of the text. Annotators are encouraged to provide concise answers in their own words rather than copying directly from the snippet.\nWe curate 153 Chinese narrative novels from a website333https://www.wenshuoge.com  ###reference_www.wenshuoge.com###. that gathers public domain books. The collected novels cover genres including martial arts, social themes, and mysteries. 200 non-overlapping stories are extracted from the collection, and the number of questions per story is proportional to its length in tokens, resulting in more questions for longer stories. We then extract snippets from each story, evenly distributed throughout, with an average of 720 Chinese characters per snippet. The number of snippets for each story corresponds to the expected number of questions. Then 6 question-answer pairs for each snippet are generated by instructing GPT-4-Turbo following the aforementioned annotating principles. Annotators then select a specific question-answer pair that is most related to a given snippet from 6 options. In addition, it is ensured that the complexity of questions related to characters, events, and the reasons behind occurrences is maintained. The questions are manually revised to include chapter information and replace pronouns with character names, making the questions more specific. There are 995 question-answer pairs based on 200 stories (i.e., contexts), with an average question length of 18.5 Chinese characters and an average answer length of 11.0 Chinese characters. Note that snippets are used for annotation, whereas during testing, the model is still required to find answers from the entire story.\nThis task is designed to assess a model\u2019s long-term memory capability. This task utilizes inputs from multi-day conversations between a user and a companion chatbot, where the model is required to accurately respond to questions about specific details from the conversation history of a particular day.\nIt determines the ability to maintain contextual understanding, ensure meaningful interactions, and interpret user behaviors over time Zhong et al. (2023  ###reference_b49###). Moreover, this capability becomes increasingly crucial as the length of the model\u2019s input extends, presenting a greater challenge in retaining a precise memory of the input content.\nTo construct the test dataset, we utilize dialogue records from 140 days of interactions between 80 virtual users and companion chatbots, and manually annotate 1,067 QA pairs. We adopt the experimental setting in Zhong et al. (2023  ###reference_b49###) to construct the evaluation dataset.\nFor the user profiles, we manually craft profiles for 20 virtual users, including names, personalities, and topics of interest, and prompt GPT-4-Turbo to generate the rest virtual user profiles.\nLeveraging the user meta-information, we employ GPT-4-Turbo to simulate dialogues between different users and companion chatbots in 140 days. Due to the limitation of context windows, we apply the hierarchical event summary in Zhong et al. (2023  ###reference_b49###) to generate long dialogues.\nAll the generated conversation records are reviewed and deduplicated.\nGiven the dialogue records, we manually craft 1,067 probing questions and answers to evaluate the model\u2019s ability to accurately retrieve relevant memories and generate appropriate responses.\nText summarization is to distill information from a source text and present it in a condensed form.\nAs a pivotal task in natural language processing, summarization requires a full-context understanding of input and complex reasoning.\nIn CLongEval, we introduce a long story summarization task that comprises long-context input based on the story, which is more practical needs and poses more challenges in the aggregation of long-context information.\nTo obtain high-qulity long-context Chinese corpus, we utilize Cloud Translation API444https://cloud.google.com/translate  ###reference_###. to translate the BOOKSUM dataset (Kry\u015bci\u0144ski et al., 2022  ###reference_b22###) into Chinese, which covers books from various domains and includes highly abstractive, human written summaries on three levels: paragraph, chapter-, and book-level. Formally, each sample  in BOOKSUM comprises a textual input  and its corresponding summary , and  may be a paragraph, a chapter, or a whole book.\nWe choose continuous paragraphs or chapters  in expected length and concatenate them to construct long-context input  to ensure coherent semantics from the translated BOOKSUM dataset. Subsequently, we utilize GPT-4-Turbo to aggregate the corresponding summaries  of the chosen continuous paragraphs or chapters into an overall summary , which can be regarded as the appropriate and highly abstractive summary of the constructed long-context input.\nAll the generated summary  is passed to manual check and refinement to guarantee the quality.\nIn this task,  news articles are stacked in one single context, with each article containing a news index (ranging from 1 to N) and its content. The goal of this task is to assess whether the LLMs can comprehensively read all news articles in a long context and determine the category of each news from given possible category pools at once. Completing this task requires the model to carefully read and analyze all the information within the long context. This task is akin to SpaceDigest of ZeroScrolls (Shaham et al., 2023  ###reference_b40###) or PassageCount of LongBench (Bai et al., 2023b  ###reference_b3###), where LLMs analyze long contexts piece by piece. However, their requested outputs are aggregated numbers (e.g., the count of positive reviews), making it difficult to gauge the LLMs\u2019 genuine understanding of each part. The proposed stacked news labeling task, in contrast, presents a more demanding challenge that tests the ability to comprehend lengthy contexts.\nTo construct this dataset, we begin by extracting a subset from THUnews (Sun et al., 2016  ###reference_b42###), an extensive collection of around 840K Chinese news articles.The subset include 9 categories: Sports, Entertainment, Home, Real Estate, Education, Politics, Gaming, Technology, and Finance. Each category contains an equal number of news articles, with the sampled articles having an average Chinese character count of 588.1. We randomly select news articles from different categories to fill the context until the desired context length was reached. Finally, we create 1005 contexts as test samples.\nTypo Detection is aimed at extracting misspelled Chinese characters from a given input. Unlike prior works (Tseng et al., 2015  ###reference_b44###; Lv et al., 2023  ###reference_b31###) that focus on sentence-level typo recognition, our tacked typo detection aims to identify all typos present in the lengthy input, which is of practical importance. This task requires LLMs to have full-context understanding capabilities as well as distinguished information extraction abilities.\nWe utilize the collected Chinese narrative corpus same as Long Story QA to generate 1000 contexts as test samples. Each context is divided into multiple paragraphs, identified by a paragraph ID that starts from 0 and increases incrementally. We randomly select some paragraphs and choose one Chinese character as a typo candidate from each selected paragraph. A corresponding homophone is then used to replace the chosen character, creating a homophonic typo. To maintain a balanced distribution of typos, the number of typos is determined based on the data length: 10 for the small set, 20 for the medium set, and 30 for the large set. Roughly half of the paragraphs in each context contain misspelled characters.\n###table_1### LStQA\nLCvMem\nLStSum\nStNLab\nStTDet\nKpRet\nTblQry\n29.34\n41.10\n10.29\n0.59\n0\n2.86\n7.50\n35.52\n29.34\n14.29\n4.97\n0.09\n6.39\n9.75\n31.94\n47.71\n11.20\n4.31\n0\n11.18\n6.64\n49.36\n53.40\n16.37\n0.46\n0.91\n33.67\n22.60\n49.55\n58.34\n17.29\n16.46\n2.27\n21.87\n20.75\n53.82\n57.41\n17.00\n11.16\n0.91\n34.97\n17.25\n60.21\n51.76\n21.56\n25.36\n66.50\n79.70\n84.24\n16.90\n26.30\n7.74\n0\n0\n1.21\nN/A\n18.41\n22.45\n8.56\n0\n0\n0.93\nN/A\n29.59\n32.07\n8.13\n0\n0\n1.45\n4.50\n25.13\n36.84\n13.99\n0\n0\n1.64\n6.25\n51.20\n38.29\n17.38\n37.40\n9.32\n22.40\n52.76\n19.03\n18.16\n2.36\n0\n0\n0.89\n2.67\n15.62\n28.39\n8.31\n0\n0\n0.51\n0.67\nIn this synthetic key-passage retrieval task, the context comprises a JSON object serialized as a string, containing multiple key-passage pairs. Each key is a unique string of 32 randomly generated characters including both letters and numbers, while the corresponding value is a continuous passage in Chinese. The objective of this task is to retrieve the corresponding passage directly based on the given key.\nUnlike LStQA and LCvMem, KpRet focuses on the model\u2019s information extraction ability, rather than summarizing the answer from a located snippet. KpRet draws inspiration from the synthetic key-value retrieval task mentioned in (Liu et al., 2023a  ###reference_b29###), but differs in that we aim to provide semantically meaningful natural language text instead of randomly generated 128-bit UUID strings, aligning more closely with real-world scenarios of passage retrieval (Nguyen et al., 2016  ###reference_b32###). The main challenge lies in accurately retrieving and reproducing relatively long passages in their entirety.\nAll passages are sampled from three Chinese QA datasets, namely WebQA (Li et al., 2016  ###reference_b27###), Sogou QA555https://github.com/sherlcok314159/ChineseMRC-Data  ###reference_RC-Data###., and CMRC2018 (Cui et al., 2018  ###reference_b10###), ensuring no repetition among them. All the passages exhibit a relatively consistent length, with an average of 81.2 Chinese characters. To construct 200 contexts, we have generated a substantial number of key-passage pairs. For each context, we uniformly select 5 questioned keys according to the position, resulting in a total of 1000 test examples.\nIn the table querying task, a context consists of multiple tables formatted in Markdown. In table querying, the objective is to locate a specific table within the context and retrieve a value from that table based on querying conditions. Unlike the key-value data structure in KpRet, TblQry involves the model\u2019s simultaneous utilization of both row and column indices to extract a specific value from the table. Our question format follows a conditional pattern: \u201dIn Table A, when the value of Column B is C, what is the value of Column D?\u201d In this question format, LLMs need to first identify Table A among multiple tables in the context, then locate the row based on the value of Column B and retrieve the value of Column D. Moreover, Unlike KpRet which returns long passages, TblQry typically returns shorter values like numbers or names, with an average token length of 5.0. Therefore, this task primarily assesses LLMs\u2019 advanced contextual querying abilities rather than their proficiency in reproducing complex passages.\nAll the tables used in this task are sourced from WikTable (Zhong et al., 2017  ###reference_b48###), a collection of English tables. We filter out excessively long tables and retain only those with a token count not exceeding 2000 tokens. Due to resource constraints, we only translate the column headers and the conditioned column into Chinese using the Cloud Translation API for each table. This ensures that the questions are posed in Chinese, while the returned values from the tables remain in English or numerical format. In total, 180 contexts containing multiple tables are constructed. The number of questions for each context is proportional to the number of tables it possesses, and we evenly distribute the tables that need to be queried across different positions within each context. Finally, 1100 test samples are generated."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset Configuration",
            "text": "Anchored by the capabilities outlined in the evaluation framework, we create CLongEval, which contains 7,267 test samples across 7 tasks, including Long Story QA, Long Conversation Memory, Long Story Summarization, Stacked News Labeling, Stacked Typo Detection, Key-Passage Retrieval and Table Querying. Among them, Long Story QA and Long Conversation Memory are human-annotated, Long Story Summarization is GPT-4-annotated and the rest 4 tasks are re-constructed from public datasets.\nAn overview of all tasks in CLongEval and detailed statistics are provided in Table 1  ###reference_###. In this paper, we use InternLM2 InternLMTeam (2024  ###reference_b18###) tokenizer to tokenize the input and report the number of tokens as context length.\nWe notice the divergence of context lengths supported by existing long-context LLMs. To ensure a broad scope of applicability of CLongEval, we stratify the benchmark into three subsets: a small set, a medium set, and a large set.\nSpecifically, the small set primarily includes test data with lengths ranging from 1K to 16K tokens, the medium set mainly encompasses lengths from 16K to 50K tokens, and the large set primarily extends from 50K to 100K tokens."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Construction",
            "text": "For a comprehensive evaluation, 7 tasks are collected in alignment with the predefined evaluation framework. The examples for each task are provided in Appendix B  ###reference_###.\nThe Long Story QA task involves LLMs answering questions based on a context snippet from a long story. To excel at this task, the model should identify the relevant snippet and abstractively reason out the answer. Unlike the normative and objective nature of MultiFieldQA (Bai et al., 2023b  ###reference_b3###  ###reference_b3###), the stories we choose are narrative, creative, and inherently longer, offering a valuable addition to lengthy single-document QA evaluation. Inspired by NarrativeQA (Ko\u010disk\u1ef3 et al., 2018  ###reference_b21###  ###reference_b21###), this task involves annotated questions that refer to non-consecutive parts of the text. Annotators are encouraged to provide concise answers in their own words rather than copying directly from the snippet.\nWe curate 153 Chinese narrative novels from a website333https://www.wenshuoge.com  ###reference_www.wenshuoge.com###  ###reference_www.wenshuoge.com###. that gathers public domain books. The collected novels cover genres including martial arts, social themes, and mysteries. 200 non-overlapping stories are extracted from the collection, and the number of questions per story is proportional to its length in tokens, resulting in more questions for longer stories. We then extract snippets from each story, evenly distributed throughout, with an average of 720 Chinese characters per snippet. The number of snippets for each story corresponds to the expected number of questions. Then 6 question-answer pairs for each snippet are generated by instructing GPT-4-Turbo following the aforementioned annotating principles. Annotators then select a specific question-answer pair that is most related to a given snippet from 6 options. In addition, it is ensured that the complexity of questions related to characters, events, and the reasons behind occurrences is maintained. The questions are manually revised to include chapter information and replace pronouns with character names, making the questions more specific. There are 995 question-answer pairs based on 200 stories (i.e., contexts), with an average question length of 18.5 Chinese characters and an average answer length of 11.0 Chinese characters. Note that snippets are used for annotation, whereas during testing, the model is still required to find answers from the entire story.\nThis task is designed to assess a model\u2019s long-term memory capability. This task utilizes inputs from multi-day conversations between a user and a companion chatbot, where the model is required to accurately respond to questions about specific details from the conversation history of a particular day.\nIt determines the ability to maintain contextual understanding, ensure meaningful interactions, and interpret user behaviors over time Zhong et al. (2023  ###reference_b49###  ###reference_b49###). Moreover, this capability becomes increasingly crucial as the length of the model\u2019s input extends, presenting a greater challenge in retaining a precise memory of the input content.\nTo construct the test dataset, we utilize dialogue records from 140 days of interactions between 80 virtual users and companion chatbots, and manually annotate 1,067 QA pairs. We adopt the experimental setting in Zhong et al. (2023  ###reference_b49###  ###reference_b49###) to construct the evaluation dataset.\nFor the user profiles, we manually craft profiles for 20 virtual users, including names, personalities, and topics of interest, and prompt GPT-4-Turbo to generate the rest virtual user profiles.\nLeveraging the user meta-information, we employ GPT-4-Turbo to simulate dialogues between different users and companion chatbots in 140 days. Due to the limitation of context windows, we apply the hierarchical event summary in Zhong et al. (2023  ###reference_b49###  ###reference_b49###) to generate long dialogues.\nAll the generated conversation records are reviewed and deduplicated.\nGiven the dialogue records, we manually craft 1,067 probing questions and answers to evaluate the model\u2019s ability to accurately retrieve relevant memories and generate appropriate responses.\nText summarization is to distill information from a source text and present it in a condensed form.\nAs a pivotal task in natural language processing, summarization requires a full-context understanding of input and complex reasoning.\nIn CLongEval, we introduce a long story summarization task that comprises long-context input based on the story, which is more practical needs and poses more challenges in the aggregation of long-context information.\nTo obtain high-qulity long-context Chinese corpus, we utilize Cloud Translation API444https://cloud.google.com/translate  ###reference_###  ###reference_###. to translate the BOOKSUM dataset (Kry\u015bci\u0144ski et al., 2022  ###reference_b22###  ###reference_b22###) into Chinese, which covers books from various domains and includes highly abstractive, human written summaries on three levels: paragraph, chapter-, and book-level. Formally, each sample  in BOOKSUM comprises a textual input  and its corresponding summary , and  may be a paragraph, a chapter, or a whole book.\nWe choose continuous paragraphs or chapters  in expected length and concatenate them to construct long-context input  to ensure coherent semantics from the translated BOOKSUM dataset. Subsequently, we utilize GPT-4-Turbo to aggregate the corresponding summaries  of the chosen continuous paragraphs or chapters into an overall summary , which can be regarded as the appropriate and highly abstractive summary of the constructed long-context input.\nAll the generated summary  is passed to manual check and refinement to guarantee the quality.\nIn this task,  news articles are stacked in one single context, with each article containing a news index (ranging from 1 to N) and its content. The goal of this task is to assess whether the LLMs can comprehensively read all news articles in a long context and determine the category of each news from given possible category pools at once. Completing this task requires the model to carefully read and analyze all the information within the long context. This task is akin to SpaceDigest of ZeroScrolls (Shaham et al., 2023  ###reference_b40###  ###reference_b40###) or PassageCount of LongBench (Bai et al., 2023b  ###reference_b3###  ###reference_b3###), where LLMs analyze long contexts piece by piece. However, their requested outputs are aggregated numbers (e.g., the count of positive reviews), making it difficult to gauge the LLMs\u2019 genuine understanding of each part. The proposed stacked news labeling task, in contrast, presents a more demanding challenge that tests the ability to comprehend lengthy contexts.\nTo construct this dataset, we begin by extracting a subset from THUnews (Sun et al., 2016  ###reference_b42###  ###reference_b42###), an extensive collection of around 840K Chinese news articles.The subset include 9 categories: Sports, Entertainment, Home, Real Estate, Education, Politics, Gaming, Technology, and Finance. Each category contains an equal number of news articles, with the sampled articles having an average Chinese character count of 588.1. We randomly select news articles from different categories to fill the context until the desired context length was reached. Finally, we create 1005 contexts as test samples.\nTypo Detection is aimed at extracting misspelled Chinese characters from a given input. Unlike prior works (Tseng et al., 2015  ###reference_b44###  ###reference_b44###; Lv et al., 2023  ###reference_b31###  ###reference_b31###) that focus on sentence-level typo recognition, our tacked typo detection aims to identify all typos present in the lengthy input, which is of practical importance. This task requires LLMs to have full-context understanding capabilities as well as distinguished information extraction abilities.\nWe utilize the collected Chinese narrative corpus same as Long Story QA to generate 1000 contexts as test samples. Each context is divided into multiple paragraphs, identified by a paragraph ID that starts from 0 and increases incrementally. We randomly select some paragraphs and choose one Chinese character as a typo candidate from each selected paragraph. A corresponding homophone is then used to replace the chosen character, creating a homophonic typo. To maintain a balanced distribution of typos, the number of typos is determined based on the data length: 10 for the small set, 20 for the medium set, and 30 for the large set. Roughly half of the paragraphs in each context contain misspelled characters.\n###table_2### LStQA\nLCvMem\nLStSum\nStNLab\nStTDet\nKpRet\nTblQry\n29.34\n41.10\n10.29\n0.59\n0\n2.86\n7.50\n35.52\n29.34\n14.29\n4.97\n0.09\n6.39\n9.75\n31.94\n47.71\n11.20\n4.31\n0\n11.18\n6.64\n49.36\n53.40\n16.37\n0.46\n0.91\n33.67\n22.60\n49.55\n58.34\n17.29\n16.46\n2.27\n21.87\n20.75\n53.82\n57.41\n17.00\n11.16\n0.91\n34.97\n17.25\n60.21\n51.76\n21.56\n25.36\n66.50\n79.70\n84.24\n16.90\n26.30\n7.74\n0\n0\n1.21\nN/A\n18.41\n22.45\n8.56\n0\n0\n0.93\nN/A\n29.59\n32.07\n8.13\n0\n0\n1.45\n4.50\n25.13\n36.84\n13.99\n0\n0\n1.64\n6.25\n51.20\n38.29\n17.38\n37.40\n9.32\n22.40\n52.76\n19.03\n18.16\n2.36\n0\n0\n0.89\n2.67\n15.62\n28.39\n8.31\n0\n0\n0.51\n0.67\nIn this synthetic key-passage retrieval task, the context comprises a JSON object serialized as a string, containing multiple key-passage pairs. Each key is a unique string of 32 randomly generated characters including both letters and numbers, while the corresponding value is a continuous passage in Chinese. The objective of this task is to retrieve the corresponding passage directly based on the given key.\nUnlike LStQA and LCvMem, KpRet focuses on the model\u2019s information extraction ability, rather than summarizing the answer from a located snippet. KpRet draws inspiration from the synthetic key-value retrieval task mentioned in (Liu et al., 2023a  ###reference_b29###  ###reference_b29###), but differs in that we aim to provide semantically meaningful natural language text instead of randomly generated 128-bit UUID strings, aligning more closely with real-world scenarios of passage retrieval (Nguyen et al., 2016  ###reference_b32###  ###reference_b32###). The main challenge lies in accurately retrieving and reproducing relatively long passages in their entirety.\nAll passages are sampled from three Chinese QA datasets, namely WebQA (Li et al., 2016  ###reference_b27###  ###reference_b27###), Sogou QA555https://github.com/sherlcok314159/ChineseMRC-Data  ###reference_RC-Data###  ###reference_RC-Data###., and CMRC2018 (Cui et al., 2018  ###reference_b10###  ###reference_b10###), ensuring no repetition among them. All the passages exhibit a relatively consistent length, with an average of 81.2 Chinese characters. To construct 200 contexts, we have generated a substantial number of key-passage pairs. For each context, we uniformly select 5 questioned keys according to the position, resulting in a total of 1000 test examples.\nIn the table querying task, a context consists of multiple tables formatted in Markdown. In table querying, the objective is to locate a specific table within the context and retrieve a value from that table based on querying conditions. Unlike the key-value data structure in KpRet, TblQry involves the model\u2019s simultaneous utilization of both row and column indices to extract a specific value from the table. Our question format follows a conditional pattern: \u201dIn Table A, when the value of Column B is C, what is the value of Column D?\u201d In this question format, LLMs need to first identify Table A among multiple tables in the context, then locate the row based on the value of Column B and retrieve the value of Column D. Moreover, Unlike KpRet which returns long passages, TblQry typically returns shorter values like numbers or names, with an average token length of 5.0. Therefore, this task primarily assesses LLMs\u2019 advanced contextual querying abilities rather than their proficiency in reproducing complex passages.\nAll the tables used in this task are sourced from WikTable (Zhong et al., 2017  ###reference_b48###  ###reference_b48###), a collection of English tables. We filter out excessively long tables and retain only those with a token count not exceeding 2000 tokens. Due to resource constraints, we only translate the column headers and the conditioned column into Chinese using the Cloud Translation API for each table. This ensures that the questions are posed in Chinese, while the returned values from the tables remain in English or numerical format. In total, 180 contexts containing multiple tables are constructed. The number of questions for each context is proportional to the number of tables it possesses, and we evenly distribute the tables that need to be queried across different positions within each context. Finally, 1100 test samples are generated."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We study how the position of the referenced chunk in the context affects the model\u2019s performance for four tasks that only need partial context. The results are shown in Figure 2  ###reference_###. The position of the referenced chunk in the context is discretized into six intervals, with larger numbers indicating a closer position to the end. It is observed that for LStQA and LCvMem, the evaluated models show a \u201dlost in the middle\u201d phenomenon (Liu et al., 2023a  ###reference_b29###) where the models\u2019 performance decreases when the referenced chunks are in the middle of the context. For KpRet, most open-source models only show some non-zero performance when the answer is located at the end of the context; GPT-4-Turbo shows a nearly linear decline in performance as the answer\u2019s position in the context becomes deeper, while Moonshot-v1 does not exhibit significant degradation. Similarly, for TblQry, GPT-4-Turbo\u2019s performance drops as the answer\u2019s position goes deeper, eventually getting surpassed by Moonshot-v1. The performance across different positions on KpRet and TblQry does not exhibit a distinct pattern.\nWe are interested in evaluating LLMs\u2019 performance for the tasks of StNLab and StTDet, without considering the stacked long-context scenario. For the news labeling task (NLab for short), we create a test set of 4,500 samples by sampling 500 news articles from each of 9 news categories on StNLab, and each time a news article is given as input for the LLM to determine its category. For the typo detection task (TDet for short), 3,000 paragraphs from StTDet are sampled as test samples with each containing a typo, and LLMs are asked to take each paragraph as a model input to identify typos.\nTable 4  ###reference_### reveals that open-source models achieve over 80% accuracy for news labeling and at least 18% accuracy for typo detection. However, when news articles or paragraphs containing typos are stacked to form longer texts, their accuracy drops dramatically, even reaching 0 in the medium set as shown in Table 2  ###reference_###. Also, detailed performance results on StNLab in Appendix A.1  ###reference_### illustrate that GPT-4-Turbo misclassifies a significant portion of the news articles as the context length increases. In contrast, Moonshot-v1 consistently achieves high accuracy scores as the context length increases."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "8 LLMs are selected for evaluation based on whether they feature long context capability and exceptional support for the Chinese.\nCommercial Models: (1) GPT-4-Turbo-128K, the GPT-4-1106-preview model (OpenAI, 2023  ###reference_b33###) with a 128K context window from OpenAI. (2) Moonshot-v1 666https://platform.moonshot.cn  ###reference_platform.moonshot.cn###. supporting up to 200K Chinese characters, developed by Moonshot AI. We call the 32K version to run the small set and call its 128K version for both medium and large sets. Open-source Models: (3) Chinese-LLAMA2-64K (Cui, 2023  ###reference_b9###), extending context length of Chinese-LLAMA2 to 64K via YaRN Peng et al. (2023b  ###reference_b35###) . (4) Chinese-Alpaca2-64K (Cui, 2023  ###reference_b9###), the 64K context version of Chinese-Alpaca2. (5) Qwen-7B-32K (Bai et al., 2023a  ###reference_b2###), extending Qwen-7B to 32K context length via NTK-aware scaled RoPE (bloc97, 2023  ###reference_b5###). (6) ChatGLM3-6B-32K (ZhupuAI, 2023  ###reference_b50###), the 32K context version of ChatGLM3-6B. (7) InternlM2-7B-200K (InternLMTeam, 2024  ###reference_b18###), effectively supporting ultra-long contexts of up to 200K tokens using dynamic NTK extrapolation (Liu et al., 2023b  ###reference_b30###). (8) InternLM2-20B-200K (InternLMTeam, 2024  ###reference_b18###), similar to InternLM2-7B-200K but is more robust and capable of handling intricate scenarios. For InternLM2-7B/20B with the small set, we rely on its native support for a 32K context window. The values of the maximum output token limit for each task under different subsets are listed in Table 5  ###reference_###. All the experiments are run on a server with 4 NVIDIA A100 (80GB) GPUs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation Metrics",
            "text": "The evaluation is fully automatic. For LstQA and LCvMem, F1 is employed to measure the unigram overlap between the generated and reference answer after ignoring white spaces and punctuation. For LstSum, we use ROUGE-L (Lin, 2004  ###reference_b28###) to measure the n-gram overlap between the generated and reference summary. For both StNlab and StTDet, a metric called Average Accuracy is introduced. It measures the ratio of the number of segments correctly answered by the generated answer to the total number of segments in the gold reference. On StNLab, it indicates the percentage of news in the context that is correctly classified, while on StTDet, it denotes the accuracy of identifying misspelled words in the context. For KpRet, Edit Score based on Levenshtein distance is employed to measure the difference between the generated string and the gold reference string. For TblQry, Exact Match is utilized to measure whether the generated column value is identical to the gold reference.\nFor each of the 7 tasks, we first calculate the score per test sample using the aforementioned corresponding metrics and report the mean score across samples."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "###table_3### 23.18\n42.71\n7.88\n23.12\n20.85\n8.95\n30.72\n27.29\n11.06\n35.52\n44.20\n16.44\n45.09\n52.64\n16.79\n49.55\n58.64\n17.29\nTable 2  ###reference_### presents the performance on all datasets in CLongEval. We observe the following key findings from the experimental results: (i) For LstQA and LCvMem, GPT-4-Turbo does not show significant F1-score improvement compared to the top-performing open-source InternLM2-20B in the small set. However, it significantly outperforms InternLM2-20B (e.g., scoring 54.18 vs 36.84 on LCvMem). Also, Moonshot-v1 exhibits less noticeable score degradation on medium and large sets compared to open-source models on LStQA. (ii) For LStSum, both Moonshot-v1 and GPT-4-Turbo show consistent Rouge-L scores on small and medium sets. (iii) For StNlab and StTDet which require careful analysis of full-text chunks to output either labeling results or identify spelling errors, there is a substantial performance gap between open-source and closed ones, with scores of all evaluated open-source models in the medium set being zero. GPT-4-Turbo\u2019s performance drops by 51.8% when moving from the small set to the medium one on StNLab. Meanwhile, Moonshot-v1 performs well on StNLab, with only an 11.83% decrease when expanding from the small set to the large one. (iv) For KpRet and TlbQry which involve information retrieval, all open-source models experience a sharp decline in performance as the input length increases, and Moonshot-v1 shows more robust handling of longer inputs compared to GPT-4-Turbo.\n###table_4### 4.31\n80.91\n0\n18.67\n4.97\n60.09\n0.09\n22.27\n0.46\n86.71\n0.91\n34.23\n16.46\n85.87\n2.27\n56.20\n11.16\n84.04\n0.91\n56.90\n86.71\n25.36\n62.06\n79.70\nTable 3  ###reference_### presents the performance of smaller context window models Baichuan2-7B (Yang et al., 2023  ###reference_b47###), Mistral-7B (Jiang et al., 2023  ###reference_b19###) and Yi-6B 777https://github.com/01-ai/Yi  ###reference_github.com/01-ai/Yi###. on LstQA, LCvMem, and LstSum. We also examine how the input length affects the performance of InternLM2-7B by truncating the context to 4K and 8K. Notably, shorter maximum context lengths result in lower scores, highlighting the need for effective long-context modeling in our benchmark."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": "###figure_2### We study how the position of the referenced chunk in the context affects the model\u2019s performance for four tasks that only need partial context. The results are shown in Figure 2  ###reference_###  ###reference_###. The position of the referenced chunk in the context is discretized into six intervals, with larger numbers indicating a closer position to the end. It is observed that for LStQA and LCvMem, the evaluated models show a \u201dlost in the middle\u201d phenomenon (Liu et al., 2023a  ###reference_b29###  ###reference_b29###) where the models\u2019 performance decreases when the referenced chunks are in the middle of the context. For KpRet, most open-source models only show some non-zero performance when the answer is located at the end of the context; GPT-4-Turbo shows a nearly linear decline in performance as the answer\u2019s position in the context becomes deeper, while Moonshot-v1 does not exhibit significant degradation. Similarly, for TblQry, GPT-4-Turbo\u2019s performance drops as the answer\u2019s position goes deeper, eventually getting surpassed by Moonshot-v1. The performance across different positions on KpRet and TblQry does not exhibit a distinct pattern.\nWe are interested in evaluating LLMs\u2019 performance for the tasks of StNLab and StTDet, without considering the stacked long-context scenario. For the news labeling task (NLab for short), we create a test set of 4,500 samples by sampling 500 news articles from each of 9 news categories on StNLab, and each time a news article is given as input for the LLM to determine its category. For the typo detection task (TDet for short), 3,000 paragraphs from StTDet are sampled as test samples with each containing a typo, and LLMs are asked to take each paragraph as a model input to identify typos.\nTable 4  ###reference_###  ###reference_### reveals that open-source models achieve over 80% accuracy for news labeling and at least 18% accuracy for typo detection. However, when news articles or paragraphs containing typos are stacked to form longer texts, their accuracy drops dramatically, even reaching 0 in the medium set as shown in Table 2  ###reference_###  ###reference_###. Also, detailed performance results on StNLab in Appendix A.1  ###reference_###  ###reference_### illustrate that GPT-4-Turbo misclassifies a significant portion of the news articles as the context length increases. In contrast, Moonshot-v1 consistently achieves high accuracy scores as the context length increases."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Evaluation for Long-Context LLMs",
            "text": "Research work of long context modeling predominantly adopt perplexity as the evaluation metric Beltagy et al. (2020  ###reference_b4###); Sun et al. (2021  ###reference_b43###); Press et al. (2021  ###reference_b38###); Chen et al. (2023b  ###reference_b7###); Peng et al. (2023b  ###reference_b35###); bloc97 (2023  ###reference_b5###).\nSynthetics tasks, such as retrieval tasks, are used to assess and analyze the ability to model long input for LLMs Chen et al. (2023a  ###reference_b6###); Li et al. (2023  ###reference_b24###).\nHowever, as discussed in Sun et al. (2021  ###reference_b43###); Xiong et al. (2023  ###reference_b46###), the perplexity value and performance on synthetic tasks may not adequately reflect a language model\u2019s capability in addressing tasks in real-world scenarios.\nRecently an English benchmark An et al. (2023  ###reference_b1###) are proposed for the evaluation of long-context LLMs. Bai et al. (2023b  ###reference_b3###) introduces a bilingual benchmark, but the quantity of test examples for Chinese is quite limited. Besides the targeted language, CLongEval differs from them in these aspects: (1) It includes novel tasks that closely simulate real-world LLM usage scenarios, and (2) The test samples possess a wider span of context lengths."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We presented CLongEval, a benchmark for Chinese long-context LLMs, which contains 7 tasks and 7,267 examples. To the best of our knowledge, CLongEval is the first benchmark in this setting. Based on two basic capabilities for long-context LLMs, i.e., information acquisition and reasoning, we collected corresponding tasks and datasets for a comprehensive evaluation.\nWe benchmarked 8 long-context LLMs  and provided an in-depth analysis regarding each fine-grained capability."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Supplemential Analysis",
            "text": ""
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Data Samples",
            "text": "For each of the 7 tasks, we show an example of test samples starting on the next page. For Long Story QA, Long Conversation Memory, Key-Passage Retrieval, and Table Querying which require partial-context understanding, the input consists of the prompt, context, and the partial-context-specific question. For Long Story Summarization, Stacked News Labeling, and Stacked Typo Detection which require full-context understanding, the input consists of the prompt and the context.\nLong Story QA\nLong Conversation Memory\nLong Story Summarization\nStacked News Labeling\nStacked Typo Detection\nKey-Passage Retrieval\nTable Querying"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.5\">\n<tr class=\"ltx_tr\" id=\"S2.T1.5.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S2.T1.5.6.1\" rowspan=\"2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.6.1.1\">Task Name</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.5.6.2\" rowspan=\"2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.6.2.1\">Annotated</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.5.6.3\" rowspan=\"2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.6.3.1\">#Data</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S2.T1.5.6.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.6.4.1\">Small Set</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S2.T1.5.6.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.6.5.1\">Medium Set</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S2.T1.5.6.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.6.6.1\">Large Set</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.7\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.1.1\">Min</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.2.1\">Max</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.3.1\">#Data</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.4.1\">Min</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.5.1\">Max</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.6.1\">#Data</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.7.1\">Min</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.8.1\">Max</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.7.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.7.9.1\">#Data</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.5.8.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Long Story QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.5.8.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.5.8.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">995</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.4.1\">1,693</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.5.1\">16,224</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.6.1\">294</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.7.1\">14,437</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.8.1\">50,553</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.9.1\">398</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.10.1\">49,636</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.11.1\">99,038</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S2.T1.5.8.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.8.12.1\">299</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.5.9.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Long Conversation Memory</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.9.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.9.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">1,067</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.4.1\">768</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.5.1\">15,589</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.6.1\">358</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.7.1\">14,669</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.8.1\">43,225</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.9.1\">353</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.10.1\">41,481</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.11.1\">88,731</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.5.9.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.9.12.1\">356</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.1.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Long Story Summarization</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">1,000</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.4.1\">1,022</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.5.1\">13,716</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.6.1\">300</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.7.1\">12,958</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.8.1\">48,677</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.9.1\">400</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.10.1\">42,046</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.11.1\">87,092</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.1.1.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.1.1.12.1\">300</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.2.2.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Stacked News Labeling</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.2.2.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.2.2.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">1,005</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.4.1\">738</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.5.1\">13,521</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.6.1\">303</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.7.1\">13,635</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.8.1\">44,100</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.9.1\">402</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.10.1\">43,875</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.11.1\">84,475</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.2.2.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.2.2.12.1\">300</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.3.3.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Stacked Typo Detection</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">1,000</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.4.1\">1,069</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.5.1\">16,385</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.6.1\">550</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.7.1\">15,956</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.8.1\">51,016</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.9.1\">300</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.10.1\">51,404</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.11.1\">98,803</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.3.3.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.3.3.12.1\">150</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.4.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Key-Passage Retrieval</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.4.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.4.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">1,100</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.4.1\">1,249</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.5.1\">17,830</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.6.1\">400</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.7.1\">18,006</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.8.1\">56,086</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.9.1\">400</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.10.1\">55,367</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.11.1\">95,073</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S2.T1.4.4.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.4.4.12.1\">300</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S2.T1.5.5.2\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">Table Querying</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.5.5.1\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.5.5.3\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">1,100</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.4\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.4.1\">1,773</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.5\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.5.1\">24,597</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.6\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.6.1\">400</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.7\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.7.1\">23,894</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.8\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.8.1\">74,004</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.9\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.9.1\">400</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.10\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.10.1\">73,984</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.11\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.11.1\">125,529</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S2.T1.5.5.12\" style=\"padding-left:2.8pt;padding-right:2.8pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S2.T1.5.5.12.1\">300</p>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>An overview of the test tasks in CLongEval. <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.7.1\">Annotated</span> denotes whether the test samples are newly human-annotated. Min and Max refer to the minimum and maximum lengths of the examples within each subset.</figcaption>\n</figure>",
            "capture": "Table 1: An overview of the test tasks in CLongEval. Annotated denotes whether the test samples are newly human-annotated. Min and Max refer to the minimum and maximum lengths of the examples within each subset."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.1.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.1.2.1\">LStQA</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.1.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.1.3.1\">LCvMem</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.1.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.1.4.1\">LStSum</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.1.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.1.5.1\">StNLab</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.1.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.1.6.1\">StTDet</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.1.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.1.7.1\">KpRet</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.1.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.1.8.1\">TblQry</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\" id=\"S3.T2.1.2.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.2.1.1\">Small Set</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.3.1\">Zh-LLAMA2-7B-64K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.2.1\">29.34</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.3.1\">41.10</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.4.1\">10.29</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.5.1\">0.59</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.6.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.7.1\">2.86</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.8.1\">7.50</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.1\">Zh-Alpaca2-7B-64K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.2.1\">35.52</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.3.1\">29.34</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.4.1\">14.29</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.5.1\">4.97</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.6.1\">0.09</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.7.1\">6.39</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.8.1\">9.75</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.1\">Qwen-7B-32K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.2.1\">31.94</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.3.1\">47.71</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.4.1\">11.20</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.5.1\">4.31</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.6.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.7.1\">11.18</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.8.1\">6.64</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.6.1\">ChatGLM3-6B-32K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.6.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.2.1\">49.36</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.6.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.3.1\">53.40</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.6.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.4.1\">16.37</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.6.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.5.1\">0.46</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.6.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.6.1\">0.91</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.6.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.7.1\">33.67</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.6.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.8.1\">22.60</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.7.1\">InternLM2-7B-32K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.7.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.7.2.1\">49.55</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.7.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.7.3.1\">58.34</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.7.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.7.4.1\">17.29</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.7.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.7.5.1\">16.46</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.7.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.7.6.1\">2.27</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.7.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.7.7.1\">21.87</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.7.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.7.8.1\">20.75</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.8.1\">InternLM2-20B-32K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.8.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.8.2.1\">53.82</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.8.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.8.3.1\">57.41</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.8.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.8.4.1\">17.00</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.8.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.8.5.1\">11.16</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.8.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.8.6.1\">0.91</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.8.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.8.7.1\">34.97</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.8.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.8.8.1\">17.25</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.9.1\">Moonshot-v1-32K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.9.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.9.2.1\">60.21</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.9.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.9.3.1\">51.76</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.9.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.9.4.1\">21.56</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.9.5\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.9.5.1\">89.01</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.9.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.9.6.1\">25.36</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.9.7\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.9.7.1\">86.74</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.9.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.9.8.1\">66.50</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.10.1\">GPT-4-Turbo-128K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.10.2\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.10.2.1\">66.19</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.10.3\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.10.3.1\">63.42</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.10.4\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.10.4.1\">21.96</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.10.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.10.5.1\">79.70</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.10.6\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.10.6.1\">38.35</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.10.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.10.7.1\">84.24</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.10.8\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.10.8.1\">82.35</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\" id=\"S3.T2.1.11.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.11.1.1\">Medium Set</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.12.1\">Zh-LLAMA2-7B-64K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.12.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.12.2.1\">16.90</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.12.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.12.3.1\">26.30</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.12.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.12.4.1\">7.74</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.12.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.12.5.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.12.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.12.6.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.12.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.12.7.1\">1.21</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.12.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.12.8.1\">N/A</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.13.1\">Zh-Alpaca2-7B-64K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.13.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.13.2.1\">18.41</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.13.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.13.3.1\">22.45</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.13.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.13.4.1\">8.56</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.13.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.13.5.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.13.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.13.6.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.13.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.13.7.1\">0.93</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.13.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.13.8.1\">N/A</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.14.1\">InternLM2-7B-200K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.14.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.14.2.1\">29.59</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.14.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.14.3.1\">32.07</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.14.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.14.4.1\">8.13</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.14.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.14.5.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.14.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.14.6.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.14.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.14.7.1\">1.45</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.14.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.14.8.1\">4.50</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.15.1\">InternLM2-20B-200K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.15.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.15.2.1\">25.13</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.15.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.15.3.1\">36.84</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.15.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.15.4.1\">13.99</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.15.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.15.5.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.15.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.15.6.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.15.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.15.7.1\">1.64</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.15.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.15.8.1\">6.25</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.16.1\">Moonshot-v1-128K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.16.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.16.2.1\">51.20</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.16.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.16.3.1\">38.29</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.16.4\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.16.4.1\">18.81</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.16.5\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.16.5.1\">86.30</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.16.6\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.16.6.1\">11.33</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.16.7\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.16.7.1\">78.64</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.16.8\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.16.8.1\">66.50</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.17\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.17.1\">GPT-4-Turbo-128K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.17.2\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.17.2.1\">52.63</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.17.3\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.17.3.1\">54.18</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.17.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.17.4.1\">17.38</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.17.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.17.5.1\">37.40</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.17.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.17.6.1\">9.32</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.17.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.17.7.1\">22.40</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.17.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.17.8.1\">52.76</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\" id=\"S3.T2.1.18.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.18.1.1\">Large Set</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.19\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.19.1\">InternLM2-7B-200K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.19.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.19.2.1\">19.03</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.19.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.19.3.1\">18.16</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.19.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.19.4.1\">2.36</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.19.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.19.5.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.19.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.19.6.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.19.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.19.7.1\">0.89</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.19.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.19.8.1\">2.67</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.20\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.20.1\">InternLM2-20B-200K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.20.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.20.2.1\">15.62</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.20.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.20.3.1\">28.39</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.20.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.20.4.1\">8.31</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.20.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.20.5.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.20.6\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.20.6.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.20.7\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.20.7.1\">0.51</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.20.8\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.20.8.1\">0.67</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.21\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S3.T2.1.21.1\">Moonshot-v1-128K</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\" id=\"S3.T2.1.21.2\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.21.2.1\">41.52</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\" id=\"S3.T2.1.21.3\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.21.3.1\">32.59</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\" id=\"S3.T2.1.21.4\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.21.4.1\">16.38</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\" id=\"S3.T2.1.21.5\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.21.5.1\">78.48</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\" id=\"S3.T2.1.21.6\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.21.6.1\">4.33</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\" id=\"S3.T2.1.21.7\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.21.7.1\">51.50</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\" id=\"S3.T2.1.21.8\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.21.8.1\">52.00</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>The CLongEval\u00a0Leaderboard. The results are up to date as of 02/15/2024. N/A means the maximum token length of the dataset surpasses the model\u2019s context window. Zh-LLAMA2/Alpaca2 denotes Chinese-LLAMA2/Alpaca2 for short.</figcaption>\n</figure>",
            "capture": "Table 2: The CLongEval\u00a0Leaderboard. The results are up to date as of 02/15/2024. N/A means the maximum token length of the dataset surpasses the model\u2019s context window. Zh-LLAMA2/Alpaca2 denotes Chinese-LLAMA2/Alpaca2 for short."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S4.T3.1.1.2\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T3.1.1.2.1\">LStQA</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S4.T3.1.1.3\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T3.1.1.3.1\">LCvMem</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S4.T3.1.1.4\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T3.1.1.4.1\">LStSum</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1\">Baichuan2-7B-4K</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.2.2.1\">23.18</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.2.3.1\">42.71</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T3.1.2.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.2.4.1\">7.88</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.3.1\">Mistral-7B-8K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.3.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.3.2.1\">23.12</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.3.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.3.3.1\">20.85</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.3.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.3.4.1\">8.95</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.4.1\">Yi-6B-4K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.4.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.4.2.1\">30.72</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.4.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.4.3.1\">27.29</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.4.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.4.4.1\">11.06</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.5.1\">InternLM2-7B-4K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.5.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.5.2.1\">35.52</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.5.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.5.3.1\">44.20</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.5.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.5.4.1\">16.44</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.6.1\">InternLM2-7B-8K</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.6.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.6.2.1\">45.09</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.6.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.6.3.1\">52.64</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T3.1.6.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.6.4.1\">16.79</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\" id=\"S4.T3.1.7.1\">InternLM2-7B-32K</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_tt\" id=\"S4.T3.1.7.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.7.2.1\">49.55</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_tt\" id=\"S4.T3.1.7.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.7.3.1\">58.64</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_tt\" id=\"S4.T3.1.7.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T3.1.7.4.1\">17.29</p>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Peformance (%) under truncated context in the small set.</figcaption>\n</figure>",
            "capture": "Table 3: Peformance (%) under truncated context in the small set."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S4.T4.1.1.2\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.2.1\">StNlab</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S4.T4.1.1.3\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.3.1\">Nlab</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S4.T4.1.1.4\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.4.1\">StTDet</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S4.T4.1.1.5\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.1.5.1\">TDet</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.2.1\">Qwen-7B</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.2.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.2.2.1\">4.31</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.2.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.2.3.1\">80.91</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.2.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.2.4.1\">0</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S4.T4.1.2.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.2.5.1\">18.67</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.3.1\">Zh-Alpaca2-7B</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.3.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.3.2.1\">4.97</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.3.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.3.3.1\">60.09</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.3.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.3.4.1\">0.09</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.3.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.3.5.1\">22.27</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.4.1\">ChatGLM3-6B</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.4.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.4.2.1\">0.46</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.4.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.4.3.1\">86.71</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.4.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.4.4.1\">0.91</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.4.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.4.5.1\">34.23</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.5.1\">InternLM2-7B</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.5.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.5.2.1\">16.46</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.5.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.5.3.1\">85.87</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.5.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.5.4.1\">2.27</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.5.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.5.5.1\">56.20</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.6.1\">InternLM2-20B</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.6.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.6.2.1\">11.16</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.6.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.6.3.1\">84.04</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.6.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.6.4.1\">0.91</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.6.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.6.5.1\">56.90</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.7.1\">Moonshot-v1</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.7.2\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.7.2.1\">89.01</span></td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.7.3\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.7.3.1\">86.71</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.7.4\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.7.4.1\">25.36</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S4.T4.1.7.5\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.7.5.1\">62.06</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.1.8.1\">GPT-4-Turbo</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T4.1.8.2\">\n<p class=\"ltx_p ltx_align_top\" id=\"S4.T4.1.8.2.1\">79.70</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T4.1.8.3\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.8.3.1\">90.31</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T4.1.8.4\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.8.4.1\">38.22</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S4.T4.1.8.5\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S4.T4.1.8.5.1\">75.63</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Perfomance (%) comparison of <span class=\"ltx_text ltx_font_italic\" id=\"S4.T4.6.1\">StNLab</span> vs. <span class=\"ltx_text ltx_font_italic\" id=\"S4.T4.7.2\">NLab</span> and <span class=\"ltx_text ltx_font_italic\" id=\"S4.T4.8.3\">StTDet</span> vs. <span class=\"ltx_text ltx_font_italic\" id=\"S4.T4.9.4\">TDet</span>.</figcaption>\n</figure>",
            "capture": "Table 4: Perfomance (%) comparison of StNLab vs. NLab and StTDet vs. TDet."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T5.1\">\n<tr class=\"ltx_tr\" id=\"A1.T5.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.1.1.1\">Task</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T5.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.1.2.1\">Small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T5.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.1.3.1\">Medium</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T5.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.1.1.4.1\">Large</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.1\">LStQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.2\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.3\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.1.2.4\">100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.3.1\">LCvMem</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.3.2\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.3.3\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.3.4\">100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.4.1\">LStSum</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.4.2\">400</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.4.3\">400</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.4.4\">800</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.5.1\">StNLab</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.5.2\">800</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.5.3\">800</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.5.4\">1500</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.6.1\">StTDet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.6.2\">400</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.6.3\">800</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.6.4\">800</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.7.1\">KpRet</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.7.2\">400</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.7.3\">400</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T5.1.7.4\">400</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T5.1.8.1\">TblQry</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T5.1.8.2\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T5.1.8.3\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T5.1.8.4\">50</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>The values of maximum output token limits for the small, medium, and large set in the inference stage.</figcaption>\n</figure>",
            "capture": "Table 5: The values of maximum output token limits for the small, medium, and large set in the inference stage."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T6.1\">\n<tr class=\"ltx_tr\" id=\"A1.T6.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt\" id=\"A1.T6.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"A1.T6.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"A1.T6.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.1.2.1\">StNLab</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A1.T6.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.1.3.1\">NLab</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"A1.T6.1.1.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.1.4.1\">StTDet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A1.T6.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.1.5.1\">TDet</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.2.1\">FmtAcc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.2.2\">AvgAcc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.2.3\">Acc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.2.4\">Acc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.2.5\">FmtAcc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.2.6\">AvgAcc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.2.7\">Acc</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.2.8\">Acc</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.1\">Qwen-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.2\">22.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.3\">4.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.4\">2.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.5\">80.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.6\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.7\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.8\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T6.1.3.9\">18.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"A1.T6.1.4.1\">Zh-Alpaca2-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.4.2\">20.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.4.3\">4.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.4.4\">1.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.4.5\">60.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.4.6\">6.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.4.7\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.4.8\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.4.9\">22.27</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"A1.T6.1.5.1\">ChatGLM3-6B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.5.2\">0.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.5.3\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.5.4\">0.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.5.5\">86.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.5.6\">84.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.5.7\">0.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.5.8\">0.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.5.9\">34.23</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"A1.T6.1.6.1\">InternLM2-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.6.2\">29.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.6.3\">16.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.6.4\">4.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.6.5\">85.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.6.6\">46.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.6.7\">2.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.6.8\">2.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.6.9\">56.20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"A1.T6.1.7.1\">InternLM2-20B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.7.2\">18.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.7.3\">11.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.7.4\">3.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.7.5\">84.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.7.6\">64.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.7.7\">0.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.7.8\">0.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.7.9\">56.90</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"A1.T6.1.8.1\">Moonshot-v1-32K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.8.2.1\">99.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.8.3.1\">89.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.8.4.1\">89.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.8.5\">86.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.8.6\">72.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.8.7\">25.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.8.8\">25.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T6.1.8.9\">62.06</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_l ltx_border_r\" id=\"A1.T6.1.9.1\">GPT-4-Turbo-128K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.9.2\">91.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.9.3\">79.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.9.4\">73.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.9.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.9.5.1\">90.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.9.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.9.6.1\">92.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.9.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.9.7.1\">38.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.9.8\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.9.8.1\">38.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T6.1.9.9\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.1.9.9.1\">75.63</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Peformance (%) of different metrics in the small set .</figcaption>\n</figure>",
            "capture": "Table 6: Peformance (%) of different metrics in the small set ."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.03514v1_figure_1.png",
            "caption": "Figure 1: The evaluation framework of CLongEval. The seven test tasks in CLongEval are designed to comprehensively assess two important capabilities of long-context LLMs: information acquisition and reasoning."
        },
        "2": {
            "figure_path": "2403.03514v1_figure_2.png",
            "caption": "Figure 2: Effects of referenced chunk positions. The analyzed samples range from 10K to 60K in context length."
        },
        "3": {
            "figure_path": "2403.03514v1_figure_3.png",
            "caption": "Figure 3: Performance change analysis of GPT-4-Turbo on StNLab."
        },
        "4": {
            "figure_path": "2403.03514v1_figure_4.png",
            "caption": "Figure 4: Performance change analysis of Moonshot-v1 on StNLab."
        }
    },
    "references": [
        {
            "1": {
                "title": "L-eval: Instituting standardized evaluation for long context language models.",
                "author": "Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.",
                "venue": "arXiv preprint arXiv:2307.11088.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Qwen technical report.",
                "author": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2309.16609"
            }
        },
        {
            "3": {
                "title": "Longbench: A bilingual, multitask benchmark for long context understanding.",
                "author": "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023b.",
                "venue": "arXiv preprint arXiv:2308.14508.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Longformer: The long-document transformer.",
                "author": "Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.",
                "venue": "arXiv preprint arXiv:2004.05150.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.",
                "author": "bloc97. 2023.",
                "venue": null,
                "url": "https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/"
            }
        },
        {
            "6": {
                "title": "Extending context window of large language models via positional interpolation.",
                "author": "Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a.",
                "venue": "arXiv preprint arXiv:2306.15595.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Longlora: Efficient fine-tuning of long-context large language models.",
                "author": "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b.",
                "venue": "arXiv preprint arXiv:2309.12307.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Kerple: Kernelized relative positional embedding for length extrapolation.",
                "author": "Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:8386\u20138399.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Chinese-llama-alpaca.",
                "author": "Yiming Cui. 2023.",
                "venue": null,
                "url": "https://github.com/ymcui/Chinese-LLaMA-Alpaca"
            }
        },
        {
            "10": {
                "title": "A span-extraction dataset for chinese machine reading comprehension.",
                "author": "Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. 2018.",
                "venue": "arXiv preprint arXiv:1810.07366.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Flashattention-2: Faster attention with better parallelism and work partitioning.",
                "author": "Tri Dao. 2023.",
                "venue": "arXiv preprint arXiv:2307.08691.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Flashattention: Fast and memory-efficient exact attention with io-awareness.",
                "author": "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:16344\u201316359.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Longnet: Scaling transformers to 1,000,000,000 tokens.",
                "author": "Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023.",
                "venue": "arXiv preprint arXiv:2307.02486.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Lm-infinite: Simple on-the-fly length generalization for large language models.",
                "author": "Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. 2023.",
                "venue": "arXiv preprint arXiv:2308.16137.",
                "url": null
            }
        },
        {
            "15": {
                "title": "How good are gpt models at machine translation? a comprehensive evaluation.",
                "author": "Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023.",
                "venue": "arXiv preprint arXiv:2302.09210.",
                "url": null
            }
        },
        {
            "16": {
                "title": "AMERICANO: argument generation with discourse-driven decomposition and agent interaction.",
                "author": "Zhe Hu, Hou Pong Chan, and Yu Yin. 2023.",
                "venue": "CoRR, abs/2310.20352.",
                "url": "https://doi.org/10.48550/ARXIV.2310.20352"
            }
        },
        {
            "17": {
                "title": "Manitweet: A new benchmark for identifying manipulation of news on social media.",
                "author": "Kung-Hsiang Huang, Hou Pong Chan, Kathleen R. McKeown, and Heng Ji. 2023.",
                "venue": "CoRR, abs/2305.14225.",
                "url": "https://doi.org/10.48550/ARXIV.2305.14225"
            }
        },
        {
            "18": {
                "title": "Official release of internlm2 7b and 20b base and chat models.",
                "author": "InternLMTeam. 2024.",
                "venue": null,
                "url": "https://github.com/InternLM/InternLM"
            }
        },
        {
            "19": {
                "title": "Mistral 7b.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023.",
                "venue": "arXiv preprint arXiv:2310.06825.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Is chatgpt a good translator? a preliminary study.",
                "author": "Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023.",
                "venue": "arXiv preprint arXiv:2301.08745, 1(10).",
                "url": null
            }
        },
        {
            "21": {
                "title": "The narrativeqa reading comprehension challenge.",
                "author": "Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018.",
                "venue": "Transactions of the Association for Computational Linguistics, 6:317\u2013328.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Booksum: A collection of datasets for long-form narrative summarization.",
                "author": "Wojciech Kry\u015bci\u0144ski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2105.08209"
            }
        },
        {
            "23": {
                "title": "Efficient memory management for large language model serving with pagedattention.",
                "author": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.",
                "venue": "In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611\u2013626.",
                "url": null
            }
        },
        {
            "24": {
                "title": "How long can context length of open-source llms truly promise?",
                "author": "Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023.",
                "venue": "In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Text revision by on-the-fly representation optimization.",
                "author": "Jingjing Li, Zichao Li, Tao Ge, Irwin King, and Michael R Lyu. 2022.",
                "venue": "pages 10956\u201310964.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Unsupervised text generation by learning from search.",
                "author": "Jingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael Lyu, and Irwin King. 2020.",
                "venue": "Advances in Neural Information Processing Systems, 33:10820\u201310831.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Dataset and neural recurrent sequence labeling model for open-domain factoid question answering.",
                "author": "Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. 2016.",
                "venue": "arXiv preprint arXiv:1607.06275.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Rouge: A package for automatic evaluation of summaries.",
                "author": "Chin-Yew Lin. 2004.",
                "venue": "In Text summarization branches out, pages 74\u201381.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Lost in the middle: How language models use long contexts.",
                "author": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023a.",
                "venue": "arXiv preprint arXiv:2307.03172.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Scaling laws of rope-based extrapolation.",
                "author": "Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. 2023b.",
                "venue": "arXiv preprint arXiv:2310.05209.",
                "url": null
            }
        },
        {
            "31": {
                "title": "General and domain-adaptive chinese spelling check with error-consistent pretraining.",
                "author": "Qi Lv, Ziqiang Cao, Lei Geng, Chunhui Ai, Xu Yan, and Guohong Fu. 2023.",
                "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing, 22(5):1\u201318.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Ms marco: A human generated machine reading comprehension dataset.",
                "author": "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016.",
                "venue": "choice, 2640:660.",
                "url": null
            }
        },
        {
            "33": {
                "title": "New models and developer products announced at DevDay.",
                "author": "OpenAI. 2023.",
                "venue": null,
                "url": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday"
            }
        },
        {
            "34": {
                "title": "Rwkv: Reinventing rnns for the transformer era.",
                "author": "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. 2023a.",
                "venue": "arXiv preprint arXiv:2305.13048.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Yarn: Efficient context window extension of large language models.",
                "author": "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023b.",
                "venue": "arXiv preprint arXiv:2309.00071.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Logigan: Learning logical reasoning via adversarial pre-training.",
                "author": "Xinyu Pi, Wanjun Zhong, Yan Gao, Nan Duan, and Jian-Guang Lou. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:16290\u201316304.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Artificial intelligence and human decision making.",
                "author": "Jean-Charles Pomerol. 1997.",
                "venue": "European Journal of Operational Research, 99(1):3\u201325.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Train short, test long: Attention with linear biases enables input length extrapolation.",
                "author": "Ofir Press, Noah Smith, and Mike Lewis. 2021.",
                "venue": "In International Conference on Learning Representations.",
                "url": null
            }
        },
        {
            "39": {
                "title": "A recipe for arbitrary text style transfer with large language models.",
                "author": "Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 837\u2013848, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-short.94"
            }
        },
        {
            "40": {
                "title": "Zeroscrolls: A zero-shot benchmark for long text understanding.",
                "author": "Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023.",
                "venue": "arXiv preprint arXiv:2305.14196.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Roformer: Enhanced transformer with rotary position embedding.",
                "author": "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.",
                "venue": "Neurocomputing, 568:127063.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Thuctc: An efficient chinese text classifier.",
                "author": "Maosong Sun, Jingyang Li, Zhipeng Guo, Yu Zhao, Yabin Zheng, Xiance Si, and Zhiyuan Liu. 2016.",
                "venue": null,
                "url": null
            }
        },
        {
            "43": {
                "title": "Do long-range language models actually use long-range context?",
                "author": "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 807\u2013822.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Introduction to sighan 2015 bake-off for chinese spelling check.",
                "author": "Yuen-Hsien Tseng, Lung-Hao Lee, Li-Ping Chang, and Hsin-Hsi Chen. 2015.",
                "venue": "In Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing, pages 32\u201337.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Efficient streaming language models with attention sinks.",
                "author": "Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023.",
                "venue": "arXiv preprint arXiv:2309.17453.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Effective long-context scaling of foundation models.",
                "author": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. 2023.",
                "venue": "arXiv preprint arXiv:2309.16039.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Baichuan 2: Open large-scale language models.",
                "author": "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. 2023.",
                "venue": "arXiv preprint arXiv:2309.10305.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning.",
                "author": "Victor Zhong, Caiming Xiong, and Richard Socher. 2017.",
                "venue": "arXiv preprint arXiv:1709.00103.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Memorybank: Enhancing large language models with long-term memory.",
                "author": "Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. 2023.",
                "venue": "arXiv preprint arXiv:2305.10250.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Chatglm3 series: Open bilingual chat llms.",
                "author": "ZhupuAI. 2023.",
                "venue": null,
                "url": "https://github.com/THUDM/ChatGLM3"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.03514v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5",
            "5.1",
            "5.2"
        ],
        "methodology_sections": [
            "2",
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.03514v1",
        "paper_title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
        "research_background": "### Motivation\nThe motivation behind this paper stems from the remarkable progress of Large Language Models (LLMs) in various Natural Language Processing (NLP) tasks, such as machine translation, fact-checking, and text style transfer. However, there is a growing need to extend the context window that these models can handle to support more complex applications, particularly in the Chinese language domain. Existing long-context LLMs for Chinese, both commercial and open-source, are emerging, but their efficacy in handling long contexts hasn't been thoroughly examined due to a lack of robust evaluation benchmarks.\n\n### Research Problem\nThe research problem addressed in this paper is the absence of a comprehensive and high-quality benchmark for evaluating the performance of Chinese long-context LLMs. Current benchmarks either focus primarily on English or provide limited coverage for Chinese with inadequate data, which hampers a full assessment of these models' capabilities. Hence, there is an urgent need to develop a dedicated benchmark that can facilitate a thorough investigation into the efficacy and limitations of Chinese long-context LLMs.\n\n### Relevant Prior Work\n1. **Performance in NLP Tasks**: The introduction acknowledges several works demonstrating LLM capabilities in machine translation (Hendy et al., 2023; Jiao et al., 2023), fact-checking (Huang et al., 2023), and text style transfer (Reif et al., 2022; Li et al., 2020).\n\n2. **Development of Long-Context LLMs**: Numerous studies have focused on extending context windows for models supporting Chinese, leading to the creation of various long-context LLMs with context lengths ranging from 32K to 200K (OpenAI, 2023; Cui, 2023; Bai et al., 2023a; ZhupuAI, 2023; InternLMTeam, 2024).\n\n3. **Benchmarks for Long-Context LLMs**: Existing benchmarks for evaluating long-context LLMs mostly focus on English (An et al., 2023; Bai et al., 2023b). In the Chinese context, the only significant benchmark is bilingual and severely limited in scope and size, with only 5 out of 21 tasks designed for Chinese (Bai et al., 2023b).\n\n4. **Evaluation Framework**: The current benchmarks employ an evaluation framework that involves identifying key information and reasoning from given contexts, which are also the foundational abilities designed into the CLongEval benchmark framework.\n\n### Summary\nIn summary, the paper is motivated by the need to assess the capabilities of Chinese long-context LLMs more effectively. It identifies the lack of a comprehensive benchmark for this purpose as the primary research problem. To address this, the paper introduces CLongEval, a specialized benchmark designed to evaluate Chinese long-context LLMs across a range of tasks and context lengths, drawing on prior studies and existing evaluation frameworks to define the essential capabilities for long-context understanding.",
        "methodology": "The methodology section for \"CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models\" outlines a comprehensive and systematized evaluation focused on essential capabilities of long-context Large Language Models (LLMs). Here is a detailed explanation of the proposed method or model with its key components and innovations:\n\n### Proposed Method or Model:\n\n#### Key Components:\n\n1. **Information Acquisition:**\n   - **Definition:** Refers to the ability of the LLMs to recognize and parse relevant information from extensive and complex textual input.\n   - **Importance:** This capability is crucial for the models to synthesize contextualized knowledge necessary for executing a variety of tasks ranging from simple question answering to following complex instructions.\n   - **Challenges:** As the input text lengthens, maintaining a coherent and precise understanding of the information becomes increasingly challenging for LLMs.\n   \n2. **Classification Based on Contextual Information:**\n   - **Full-Context Information Acquisition:** Tasks that require an accurate comprehension of the entire input text.\n   - **Partial-Context Information Acquisition:** Tasks that rely on a correct understanding of selective snippets of the input text.\n   - **Evaluation Tasks:** A range of test tasks are introduced within these two categories to assess the model\u2019s proficiency in both full-context and partial-context scenarios. \n\n#### Innovations:\n\n1. **Reasoning Ability:**\n   - **Definition:** The capability to perform the inferential process and synthesize conclusions from lengthy textual inputs.\n   - **Application:** LLMs with proficient reasoning skills can effectively navigate problem-solving and decision-making procedures essential for handling complex tasks.\n   - **Framework:**\n     - **Content Extraction:** Tasks that assess the model\u2019s ability to directly identify and retrieve information from the input without altering the original content.\n     - **Abstraction:** Tasks that involve generating new content not explicitly included in the source material, requiring a deeper understanding and recombination of the input information.\n     - **Evaluation Tasks:** The evaluation dimension includes distinct types of test tasks for both extraction and abstraction, allowing a nuanced assessment of LLMs\u2019 reasoning capabilities.\n\n### Data Metrics:\n\n- The methodology section includes metrics that summarize the data statistics for various tasks, including minimum and maximum data points and their corresponding sample sizes. These metrics help in understanding the scope and scale of the evaluation tasks.\n\nIn essence, CLongEval provides a detailed framework to evaluate Chinese Large Language Models focusing on long-context tasks. The proposed model considers both information acquisition and reasoning capabilities in-depth, employing a structured method to classify and assess these abilities systematically through diverse, tailored test tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Setup:\n1. **Tasks and Context Positions**:\n   - **Tasks**:\n     - Long Sequence Text-based Question Answering (LStQA)\n     - Long Context-based Memory (LCvMem)\n     - Keyphrase Retrieval (KpRet)\n     - Table Querying (TblQry)\n     - News Labeling (StNLab)\n     - Typo Detection (StTDet)\n\n2. **Datasets**:\n   - For **StNLab** (News Labeling): A test set of 4,500 samples was created by sampling 500 news articles from each of 9 news categories.\n   - For **StTDet** (Typo Detection): 3,000 paragraphs containing typos were sampled.\n\n3. **Evaluation Metrics**:\n   - Performance metrics were task-specific, with a focus on accuracy for news labeling and typo detection tasks.\n\n#### Results:\n- **Task-Specific Performance without Long Context**:\n  - **News Labeling (StNLab)**:\n    - Open-source models achieved over 80% accuracy.\n  - **Typo Detection (StTDet)**:\n    - Open-source models achieved at least 18% accuracy.\n\n- **Performance with Stacked Contexts**:\n  - Accuracy dropped dramatically when news articles or paragraphs were stacked to form longer texts, occasionally reaching 0 in the medium set.\n  - For news labeling (StNLab), GPT-4-Turbo misclassified a significant portion of articles as context length increased, while Moonshot-v1 maintained high accuracy despite increasing context length."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate how the position of the referenced chunk in the context affects the model's performance for tasks that only need partial context.",
            "experiment_process": "The position of the referenced chunk is discretized into six intervals, with larger numbers indicating a closer position to the end. The study includes tasks like LStQA, LCvMem, KpRet, and TblQry. Models like LLMs, GPT-4-Turbo, and Moonshot-v1 are evaluated based on their performance at different positions. The experimental setup is shown in Figure 2.",
            "result_discussion": "For LStQA and LCvMem, models show a 'lost in the middle' phenomenon with performance decreasing when referenced chunks are in the middle. For KpRet, open-source models perform best when the answer is at the end, while GPT-4-Turbo shows a linear decline and Moonshot-v1 remains stable. Similarly, for TblQry, GPT-4-Turbo's performance drops with deeper answers, eventually getting surpassed by Moonshot-v1, and no distinct pattern is observed for KpRet and TblQry.",
            "ablation_id": "2403.03514v1.No1"
        },
        {
            "research_objective": "To assess the performance of LLMs on news labeling and typo detection without considering long-context scenarios.",
            "experiment_process": "A test set of 4,500 samples from 9 news categories is created for news labeling (NLab), and 3,000 paragraphs containing typos are sampled for typo detection (TDet). LLMs are evaluated on their ability to label news articles and detect typos. The evaluation metrics involve accuracy.",
            "result_discussion": "Open-source models achieve over 80% accuracy for news labeling and at least 18% accuracy for typo detection in the non-stacked context. However, their performance drops dramatically when news articles or paragraphs are stacked to form longer texts, with some accuracy reaching zero in the medium set. GPT-4-Turbo misclassifies significantly more as context length increases, whereas Moonshot-v1 maintains high accuracy.",
            "ablation_id": "2403.03514v1.No2"
        },
        {
            "research_objective": "To evaluate the overall performance of LLMs on various tasks in the CLongEval benchmark.",
            "experiment_process": "The evaluation involves eight LLMs, both commercial and open-source, with varying context window sizes. Tasks include LstQA, LCvMem, LStSum, StNLab, StTDet, KpRet, and TlbQry. Performance metrics such as F1-score, Rouge-L, and accuracy are reported. Experiments are conducted on a server with 4 NVIDIA A100 (80GB) GPUs.",
            "result_discussion": "GPT-4-Turbo and Moonshot-v1 exhibit strong performance on several tasks. InternLM2-20B's performance in small set scenarios is comparable to GPT-4-Turbo but falls short in larger contexts. Moonshot-v1 shows less score degradation on extended context lengths. For StNLab and StTDet, open-source models perform poorly on medium sets, while GPT-4-Turbo\u2019s performance drops significantly with increasing context length. Moonshot-v1 remains robust across expanding input lengths for retrieval tasks like KpRet and TlbQry.",
            "ablation_id": "2403.03514v1.No3"
        }
    ]
}