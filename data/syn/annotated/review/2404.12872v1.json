{
    "title": "LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency",
    "abstract": "Query rewrite, which aims to generate more efficient queries by altering a SQL query\u2019s structure without changing the query result, has been an important research problem. In order to maintain equivalence between the rewritten query and the original one during rewriting, traditional query rewrite methods always rewrite the queries following certain rewrite rules. However, some problems still remain. Firstly, existing methods of finding the optimal choice or sequence of rewrite rules are still limited and the process always costs a lot of resources. Methods involving discovering new rewrite rules typically require complicated proofs of structural logic or extensive user interactions. Secondly, current query rewrite methods usually rely highly on DBMS cost estimators which are often not accurate. In this paper, we address these problems by proposing a novel method of query rewrite named LLM-R2, adopting a large language model (LLM) to propose possible rewrite rules for a database rewrite system. To further improve the inference ability of LLM in recommending rewrite rules, we train a contrastive model by curriculum to learn query representations and select effective query demonstrations for the LLM. Experimental results have shown that our method can significantly improve the query execution efficiency and outperform the baseline methods. In addition, our method enjoys high robustness across different datasets.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "With the rapid growth of data in various fields, it is common to take seconds or minutes, and even longer to execute an SQL query. Therefore, efficient query processing has been a crucial task in modern database systems. One of the key topics in query optimization that has gained significant attention is query rewrite (Pirahesh et al., 1992  ###reference_b24###; Li, 2019  ###reference_b20###). The objective of query rewrite is to output a new query equivalent to the original SQL query, while having a shorter execution time. Ideally, query rewrite should fulfill three critical criteria: (1) Executability: the rewritten query should be able to be executed without any errors; (2) Equivalence: it must yield identical results as the original query; (3) Efficiency: this encompasses two aspects\u2014Execution Efficiency and Computational Efficiency. Execution Efficiency refers to the requirement that the rewritten query executes more efficiently than the original, while Computational Efficiency implies that the overhead of the rewriting process should be justifiable by the time savings achieved during query execution.\nTo enhance both Executability and Equivalence in rewritten queries, existing studies have predominantly concentrated on rule-based rewriting techniques. In particular, these studies are divided into two orthogonal research directions: the discovery of novel rewriting rules and the effective application of existing rules. For the first direction, although some studies have discovered more rewrite rules (Wang et al., 2022  ###reference_b28###; Bai et al., 2023  ###reference_b7###; Wu et al., 2022  ###reference_b30###), there are many challenges related to the complexity of rule validation and the specificity of their applicability, often resulting in high computational demands and professional-level user competence. For example, Wetune (Wang et al., 2022  ###reference_b28###) only supports discovering rewrite rules on limited types of operators and Querybooster (Bai et al., 2023  ###reference_b7###) necessitates user engagement with specialized rule syntax, respectively. Therefore, this paper shifts focus toward the latter direction, delving into the methodologies for the effective utilization of pre-established rules. For example, Learned Rewrite (Zhou et al., 2021  ###reference_b36###) utilizes existing rewrite rules from the Apache Calcite (Begoli et al., 2018  ###reference_b8###) platform and learns to select rules to apply. It notably incorporates a Monte Carlo search algorithm in collaboration with a machine-learned query cost estimator to streamline the selection process. However, it\u2019s non-trivial to solve the challenges related to the computational demand of the Monte Carlo algorithm and the precision of the cost estimation model, which can significantly impact the execution efficiency.\nOn the other hand, with the rise of large language models (LLMs), there also exist some \u201clarge language model for database\u201d projects (Xue et al., 2024  ###reference_b31###; db-, [n.d.]  ###reference_b4###) that support direct query rewrite. The idea of these methods is to utilize the sequence-to-sequence generation ability of a language model to directly output a new rewritten query given an input query, without considering any rewrite rules or DBMS information. Although it is possible for these methods to discover new rewrites not following any existing rules, they easily suffer from the hallucination problem of language models (Ji et al., 2023  ###reference_b18###; Zhang et al., 2023  ###reference_b33###), especially for long and complicated queries, where language models give plausible but incorrect outputs. Either a syntax or reference error during generation will lead to vital errors when executing the query. Therefore, relying solely on LLM\u2019s output query may violate the executability and equivalence to the original query, deviating from the basic aim for query rewrite.\nTo overcome the limits of the current query rewriting techniques and benefit from their advantages, we propose an LLM-enhanced rewrite system to use LLMs to suggest rewrite rule strategies and apply these strategies with an existing database platform to rewrite an input query. Inspired by the LLM-based learning framework for using tools (Schick et al., 2023  ###reference_b26###; Yao et al., 2023  ###reference_b32###), we leverage the LLM\u2019s strong generalization and reasoning abilities for query rewriting while avoiding issues like hallucination. We design a novel LLM-enhanced query rewrite system to automate the process of selecting more effective rewrite rules, note that the executability and equivalence of the rewritten query are guaranteed since all the candidate rules are provided by existing DB-based rule rewrite platforms. In addition to meeting the basic requirements of valid query rewrite, we also develop new techniques to boost the executing efficiency of our rewrite system. Firstly, to overcome hallucination, we collect a pool of demonstrations consisting of effective query rewrites using existing methods and our designed baselines. We then learn a contrastive query representation model to select the most useful in-context demonstration for the given query to prompt the system, optimizing the LLM\u2019s rewrite rule selection. In addition, to address the challenge of limited training data, we propose to utilize the learning curriculum technique (Bengio et al., 2009  ###reference_b9###) to schedule the training data from easy to hard. We apply our LLM-enhanced rewrite method on three different datasets, namely TPC-H, IMDB, and DSB. We observe a significant query execution time decrease using our method, taking only 52.5%, 56.0%, 39.8% of the querying time of the original query and 94.5%, 63.1%, 40.7% of the time of the state-of-the-art baseline method on average on the three datasets.\nOur main contributions are:\nTo the best of our knowledge, we are the first to propose an LLM-enhanced query rewrite system that can automatically select effective rules from a given set of rewrite rules to rewrite an input SQL query.\nTo enable LLMs to select better rewrite rules for a query, we construct a demonstration pool that contains high-quality demonstrations so that we can select good demonstrations to prompt the LLM-enhanced rewrite system for few-shots learning.\nWe learn a contrastive query representation model to optimize the demonstration selection. To overcome the challenge of limited training data, we further design a learning curriculum to schedule the training data from easy to hard.\nWe further analyze the robustness of our method. By applying our method to unseen datasets and different dataset volumes, we demonstrate that our method is much more flexible than the baseline methods and shed light on generalizing to other database problems."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Preliminary",
            "text": "In this section, we first introduce some key concepts including query, query tree and query rewrite rules in Section 2.1  ###reference_###. Then, we will formalize the problem of query rewrite based on rules in Section 2.2  ###reference_###. Finally in Section 2.3  ###reference_###, we introduce the related work.\n###figure_1### plan_tree"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Query and Rewrite Rules",
            "text": "Query & Query tree. Each query in our study is formulated as an executable SQL statement. Furthermore, we model each query as a query tree using various nodes, where each node represents a specific type of query operator (e.g., Sort, Join, and Scan). Figure 1  ###reference_### illustrates an example of a SQL query and its corresponding query tree representation. It is worth noting that any given query can be transformed into a query tree, and conversely, this query tree can be reverted back to its original raw query form.\nQuery rewrite rules. Given an input query denoted as , a sequence of transformation methods, represented as , can be applied to the query\u2019s query tree, yielding an equivalent query, denoted as . These transformation methods, referred to as rewrite rules, encompass a diverse range of functionalities. These include the conversion of one operator to another, the alteration of execution sequences between operators, and the elimination of redundant operators. Table 1  ###reference_### delineates a representative set of these query rewrite rules. For the sake of brevity, we succinctly express the query rewrite process as , where  symbolizes the sequence of  applied rewrite rules."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Rule-based Query Rewrite",
            "text": "With the introduction of the rewrite rules, we now formally define the problem of query rewrite based on rules as follows:\n(Rule-based query rewrite): Consider an input query  and a set of candidate rewrite rules . The objective is to identify a sequence of rules  where , that transforms the query  into a more efficient version . The efficiency of the rewritten query  is quantified by its execution latency. Such rewrite is characterized by transforming  into an equivalent query , which exhibits a lower execution latency compared to other possible rewritten versions of the query. The problem can be formally represented as:"
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3. Related Work",
            "text": ""
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2. LLM-based SQL Solvers.",
            "text": "Large Language Models (LLMs) have recently emerged as a hot topic in machine learning research, captivating the interest of many in the field due to their impressive capabilities. These models have demonstrated a surprisingly strong ability to handle a variety of text-related tasks, excelling in areas such as generation, decision-making, and deduction. One such task that is highly related to DB research is text-to-SQL, in which an LLM directly generates a SQL query given database information and user requirements. Numerous studies (Li et al., 2023a  ###reference_b21###; Sun et al., 2023  ###reference_b27###; Zhou et al., 2023  ###reference_b37###) have highlighted the potential of LLMs in the text-to-SQL task, showcasing their proficiency in SQL query-related tasks. While much of this existing research has focused on LLMs\u2019 ability to generate executable queries, there is a growing recognition of the importance of other factors, particularly the efficiency and accuracy of these queries when applied in real-world scenarios. In particular, (Li et al., 2023a  ###reference_b21###) discussed their attempts in an efficiency-oriented query rewrite task, where an LLM is directly given an input query and tries to rewrite it into a more efficient one.\nHowever, a significant issue previous LLM-based face is the problem of hallucination, which refers to instances where the model generates output that is not only incorrect but is done so with a misleading level of confidence. This is particularly problematic in the context of database applications, where accuracy is paramount. Therefore, we propose a different direction of utilising the LLMs while overcoming hallucination. Instead of using LLM to directly output an SQL query, we adopted a DB-based SQL rewriter enhanced by an LLM."
        },
        {
            "section_id": "2.3.3",
            "parent_section_id": "2.3",
            "section_name": "2.3.3. In-context Learning",
            "text": "Due to the extensive data and resource requirements of fine-tuning an LLM, many works choose to utilize LLMs by the technique called in-context learning (ICL), where no modifications to the LLMs\u2019 model weights are made. The concept of ICL, first introduced by Brown et al.  ###reference_b10### in their seminal work on GPT-3 (Brown et al., 2020  ###reference_b10###), shows that language models like GPT-3 can leverage in-context demonstrations at inference time to perform specific tasks, without updating the model weights. ICL typically involves enriching the context with select examples to steer the model\u2019s output. Formally, consider a model denoted as  and a contextual input represented by . The output  generated by applying the ICL method to model  with input  can be succinctly expressed as .\nICL has rapidly gained popularity for addressing diverse challenges in natural language processing. However, it is a sophisticated technique requiring careful implementation. Extensive research, including studies by (Wei et al., 2023  ###reference_b29###) and (Li et al., 2023b  ###reference_b22###), has explored the intricacies of LLMs\u2019 learning processes in this context. These studies highlight that the success of in-context learning is closely related to the construction of the context and the quality of the examples used."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. LLM-enhanced Rewrite System",
            "text": "In this section, we will introduce our innovative LLM-enhanced rule-based rewrite system (LLM-R2). In Section 3.1  ###reference_###, we will first illustrate the pipeline of our rewrite system. Then in Section 3.2  ###reference_###, we will state our motivation to optimize the demonstration selection and introduce our novel Demonstration Manager module.\n###figure_2### LLM-R"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. System Pipeline",
            "text": "As shown in Figure  2  ###reference_###(a), the system integrates an LLM into the query rewrite system utilizing the ICL methodology (Brown et al., 2020  ###reference_b10###). We construct the ICL prompt with three main components:\nInput query: We employ the SQL statement corresponding to the provided input query  for the prompt construction.\nFixed instruction: The fixed instruction consists of a system instruction  and a rule instruction . While the system instruction specifies the task requirements, the rule instruction includes a comprehensive list of all candidate rewrite rules available for the language model to select. Each rule is accompanied by a concise explanation, enabling informed decision-making.\nOne-shot demonstration: Similar to directly using LLMs to rewrite queries, selecting rewrite rules using LLMs may also easily suffer from the hallucination problem, like outputting non-existing rules. To mitigate this and ensure the LLMs\u2019 outputs are more closely aligned with our task requirements, yielding superior rule suggestions, we use the demonstration as a part of the prompt. Formally, we define our demonstration given to the LLM-R2 system as a pair of text , where  is the example query assembling the input query and  is the list of rules that have been applied to rewrite the example query. Such demonstrations can successfully instruct the LLM to follow the example and output a list of rewrite rules to apply on the new input query. In particular, this involves selecting a high-quality demonstration  from many successful rewritten demonstrations (i.e., denoted as a pool ) for each input query to guide the LLM effectively. To achieve this goal, we design a module named Demonstration Manager, whose details are elucidated in the subsequent section.\n###figure_3### icl_example\nAs specifically highlighted, Figure 3  ###reference_### delineates the prompt utilized within the In-Context Learning (ICL) process of our system. Upon constructing the prompt and feeding it into the LLM, we can extract a sequence of rewrite rules from the model\u2019s output. These rules undergo further processing and execution by a database-based rule executor. For instance, the original input query in Figure 2  ###reference_###(a) is modified by the \u201cAGGREGATE_PROJECT_MERGE\u201d rule, as highlighted in bold. This modification transforms the original query into a more optimized output query, demonstrating the practical application and effectiveness of the extracted rules in query optimization processes. Through the synergy of the LLM\u2019s superior generalization capabilities and the rule executor\u2019s precision, our proposed system guarantees extensive applicability, alongside ensuring the executability and equivalence of the rewritten queries. Consequently, this rewrite process can be formalized as follows:\n(LLM-enhanced Query Rewrite): Given a large language model , a textual instruction outlining the rewrite task , a set of candidate rules , one successful rewrite demonstration  selected from the demonstration pool , and an input query , a prompt  is constructed and provided as input to  as:\nFrom , a sequence of rewrite rules  is derived:\nBy sequentially applying these rewrite rules , we generate an optimally equivalent query, represented as .\n###figure_4### demonstration_example"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Demonstration Manager Overview",
            "text": "Motivation. In the above ICL process, optimizing the prompt  is crucial for improving the output quality of LLMs. Given the fixed settings of system instruction(), rule instruction(), and input query(), our optimization efforts focus primarily on the demonstration(), which is chosen to enhance model performance. Recent studies on LLMs (e.g.,  (Brown et al., 2020  ###reference_b10###; Wei et al., 2023  ###reference_b29###)) have underscored the positive impact of high-quality in-context demonstrations on LLM output, reducing the tendency of LLMs to produce hallucinatory content. As shown in Figure 4  ###reference_###, our rewrite system exhibits similar effectiveness variability w.r.t. the demonstrations used, further emphasizing the necessity of optimizing demonstration selection for specific input queries. Therefore, it is an important problem to optimize the demonstration selected for a given input query. Particularly, we address this problem by designing the Demonstration Manager module.\nOverview. Figure 2  ###reference_###(b) illustrates the basic structure of our proposed Demonstration Manager module, comprising two parts: Demonstration Preparation and Demonstration Selection.\n(1) The primary objective of the Demonstration Preparation is to generate a substantial number of successful rewritten demonstrations for constructing a demonstration pool. Furthermore, this part also serves to supply training data essential for model learning in the second part. Specifically, we design two modules: the Benefit Estimator and the Pool Generator, to achieve our objectives. The Benefit Estimator is capable of assessing the potential benefits of a given query rewrite strategy, thereby generating corresponding rewrite tuple recording the performance of this rewrite strategy on the input query. Subsequently, the Pool Generator is employed to extract demonstrations for constructing a pool. Moreover, we utilize the rewrite tuples to derive training triplets, which are essential for model learning in subsequent parts.\n(2) The second part involves the Demonstration Selection module, tasked with identifying the optimal demonstration from the pool for each input query. This process is enhanced by incorporating a query representation model within the selector, designed to evaluate the similarity between input queries and demonstrations in the pool. This representation model undergoes offline training using the training data. In addition, to obtain an effective model, we enhance the model\u2019s training through the integration of a curriculum learning approach. Afterwards, the trained model is integrated into Demonstration Selector for online inference. In other words, upon receiving an input query for rewriting, the selector discerns and selects the most appropriate demonstration from the pool based on the trained model. More detailed elaboration on the above two parts will be provided in the following sections."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Demonstration Preparation",
            "text": "In this section, we aim to generate sufficient high-quality data to build the demonstration pool. As shown in Figure 5  ###reference_###, we first design the Benefit Estimator module to generate the ground truth, where each ground truth data point indicates the efficiency gain obtained by rewriting an input query using generated rules in the context of a demonstration. With sufficient ground truth, including both good and bad samples, we further design the Pool Generator module to select all good samples to build the demonstration pool. In addition, we can deduce contrastive training triplets from the ground truth, which can help train our selection model.\n###figure_5### data manager"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Benefit Estimator",
            "text": "Since we are only able to start with solely training queries without demonstrations, the triplet generation pipeline is segmented into two distinct phases: the first stage involves initializing high quality candidate demonstrations utilizing baseline method and a zero-shot LLM-R2 system where no demonstration is selected, followed by the demonstration adoption stage employing a one-shot LLM-R2 system. Subsequently, each stage is elucidated in detail.\nStage-1: We start with a diverse set of input queries collected from our dataset as the training set. To obtain a rich set of effective rewrites as candidate demonstrations, we first apply our zero-shot LLM-enhanced rewrite system (LLM-R2) to rewrite the training set queries. After getting the rewrite rules adopted and the resulted rewrite queries, we directly execute the rewritten queries on the corresponding databases. The execution time of the rewritten queries as well as the original queries is evaluated to collect the initial candidate demonstration set consisting of the improvable queries, together with their rules adopted.\nStage-2: With the candidate demonstrations collected from the previous step, we can then estimate the benefits of these demonstrations when they are selected for a given input query. Motivated by (Wei et al., 2023  ###reference_b29###), such improvable demonstrations are supposed to be more useful for the LLM to output improving rewrite suggestions, compared to using any degraded rewrite queries as demonstrations. In addition, the more \u201csimilar\u201d the improving demonstration query is to the input query, the better output the LLM will generate. However, different from natural language inputs\u2019 simple textual similarity, the similarity between SQL queries is indeed more complicated. To identify if the pool we collected truly contains high-quality and \u201csimilar\u201d demonstrations for new input queries and refine the demonstration pool, we designed three heuristic demonstration-selection methods based on different levels of similarity as follows.\nRandom Selection: A random demonstration query is selected from the candidate demonstrations for a given input query, where the similarity level lies on the same input category.\nTree Selection: Query tree is an important structural feature for the queries, therefore, it is natural to align similarity with the query tree structure. We first compute the query trees of all the candidate demonstration queries, with operators as the tree nodes. Given an input query, we select the demonstration with the minimum tree edit distance from the input query tree within the candidate demonstrations.\nSentTrans Selection: At the textual level, we observe that queries are always considered as sentences for the language models to process. Based on the observation, we treat input queries as sentences and select the candidate demonstration query whose embedding is the most similar to the input query. Most of the effective LLMs are closed-sourced, which means we are not able to obtain the query embeddings of such LLMs. However, similar to LLMs, some small pre-trained language models share the same sequence-to-sequence mechanism, that the input text is first encoded by an encoder before feeding to the model. Using such encoders, like Sentence Transformers (Reimers and Gurevych, 2019  ###reference_b25###), we can obtain an embedding of a given sentence.\nWith the three demonstration selection methods above, we can prompt our LLM-R2 system with the one-shot demonstration to obtain various rewrite results on the same training set. These new rewrite queries from the one-shot LLM-R2 system are then evaluated in the same way as in Stage-1. Specifically, when we adopt one-shot demonstration to rewrite an input query , we are able to estimate the benefit obtained from the demonstration by constructing the rewrite tuples (T) as (), where  represents a training query,  is the demonstration  selected for ,  denotes the adopted rules for , and  represents the improved margin obtained by the query rewrite. In particular, given the original query cost  and the cost of rewritten query , we define the improved margin as , where the larger margin the better rewrite result and larger benefit we have.\nIn addition, a set of training triplets is generated using the rewrite tuples obtained in preparation for training a contrastive representation model. For a given query  in the rewrite tuple (), we consider the demonstration query  adopted as an improve query  for , if the improved margin . In contrast, we denote the demonstration query as a degrade query  if . If there are multiple improve(degrade) queries, we only select the one with the largest(smallest) improved margin. Since we have adopted multiple one-shot selection methods, now we are able to construct a training triplet for a given query as . A set of training triplets can be further constructed if we enumerate the whole training query set."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Pool Generator",
            "text": "Apart from the training triplets, we also hope to prepare an effective demonstration pool so that our learned demonstration selection model can select demonstrations from it during online inference. The rewrite tuple generated by the Benefit Estimator module, recording the effectiveness of a sequence of rewrite rules  on an input query , naturally fits our need for a high-quality rewrite demonstration.\nIn particular, given the set of rewrite tuples generated by  input queries, we first separate them into  groups  based on their corresponding input queries. Therefore, each group  can be represented as the tuple set . Since we have adopted various methods, multiple tuples have the same input query, and we only need the optimal rewrite rule sequence to form a demonstration for the query. Therefore, for each training query  and its corresponding tuple group , we only select the tuple with the largest improved margin, and the order is denoted as , which can be formulated as follows:\nNext, we construct the demonstration containing the input query and rules as the pair , and then add the demonstration to the pool.\nAs shown in Figure 5  ###reference_###, when the largest improved margins  and  are identified for input queries  and , the corresponding demonstrations  and  are selected with the rewrite rules  and  adopted.\n###figure_6### demonstration selector"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Demonstration Selection",
            "text": "Motivation. Addressing the challenge of enhancing system performance, the selection of an optimal rewrite demonstration to guide the LLM for any given input query is required and remains uncertain. Intuitively, the greater the \u201csimilarity\u201d between the input and demonstration queries, the more applicable the rewrite rule, thereby enhancing the LLM\u2019s output efficacy. Therefore, to capture such \u201csimilarity\u201d, we design a contrastive model to learn the representations of queries in this Demonstration Selection module, where better demonstration queries are to have more similar representations to the input query. Consequently, the demonstration query that exhibits the highest resemblance to the input query is selected for the LLM, optimizing the generation of more effective outputs.\nOverview. In order to learn a contrastive representation model efficiently and effectively, the selection module consists of two main components: our contrastive model and a curriculum learning pipeline to improve the model training. We will first outline the representation model and its contrastive learning structure in Section 5.1  ###reference_###, followed by a detailed discussion of the whole model learning pipeline in Section 5.2  ###reference_###."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Contrastive Representation Model",
            "text": "As shown in Figure 6  ###reference_###, our representation model  is constructed as a query encoder to encode the information describing a query, and a contrastive training structure to further train the encoder given training data. In particular, the information of a query tree is first encoded by nodes into node embeddings. A tree-biased attention layer will then compute the final representation of the query given the node embeddings. Such an encoder  is then trained using the contrastive learning structure drawn below it.\nQuery encoder. The representation of a query should focus on various key attributes, like the query tree structure and columns selected. Therefore, we design an encoder following (Zhao et al., 2022  ###reference_b34###) to take the query trees generated by DBMS\u2019 query analyzer as inputs. It is notable that the original encoding in (Zhao et al., 2022  ###reference_b34###) utilizes the physical query plan which contains richer information, so that the objective of estimating query cost can be successfully achieved. Since we aim to capture the similarity between queries, we separately encode the following information for each query tree node instead in our encoder, as shown in the top half of Figure 6  ###reference_###:\nOperator type: We use one-hot encoding to encode the operator types into one vector, with value one for the current node operator type and zero for the rest positions.\nOperator conditions: Within each node, the details for the operator are explained in parentheses, including sort order for \u201cSort\u201d operator, selected column for \u201cScan\u201d operator etc. Different from the physical plans used in (Zhao et al., 2022  ###reference_b34###), such information has no unified form for encoding. We consider the conditions as text and encode using a pre-trained Sentence Transformers encoder (Reimers and Gurevych, 2019  ###reference_b25###). Such an encoder can capture the textual differences between conditions effectively and have unified embedding dimensions to simplify further analysis.\nCardinality and cost: From (Zhao et al., 2024  ###reference_b35###) we observe that the estimated cardinality and cost are important in describing a query. We collect the row count and estimated cumulative cost values and normalise them through an MLP layer.\nWe simply concatenate the three information vectors together to be the encoded embedding for a node in the given query tree. We use the same tree Transformer model in (Zhao et al., 2022  ###reference_b34###) to get the final representation of a query given its tree nodes\u2019 embeddings. The final representation of the whole query will be computed by the tree-biased attention module.\nContrastive learning structure.\nDue to the necessity of executing queries, the volume of training triplets produced by our demonstration preparation module is limited. Unlike the query representation model in (Zhao et al., 2022  ###reference_b34###), which is trained directly on abundant labeled data, our approach requires a more sophisticated training framework to effectively capture query representation with the generated training data. Inspired by SimCSE (Gao et al., 2022  ###reference_b14###), we design a contrastive learning structure to train our query representation model on the limited training data. In a training batch containing  tuples, we consider each original query\u2019s improved query as its \u201cpositive\u201d query, its degraded query as its \u201chard negative\u201d query, and the remaining improved and degraded queries within the same batch as \u201cnegative\u201d queries. This allows us to pull close distances between original queries and their improved versions while pushing apart those with degraded queries.\nFollowing such setting, the loss  for the  tuple (, , ) can be computed as\nwhere  is a temperature hyper-parameter, ,  and  stand for the representation of ,  and  respectively, and the function\n is the cosine similarity .\nAs an example in a training batch of size 2, for the first original query  shown in the bottom part of Figure 6  ###reference_###, the positive query will be its corresponding improve query , and other in-batch improve or degrade queries ,  and  are all regarded as negative queries. The final loss for the batch will be the sum of the losses for the two tuples.\n###figure_7### the overall pipeline"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2. Curriculum Learning Pipeline",
            "text": "Motivation. Although we have developed a representation-based demonstration selector, training the contrastive model presents several challenges. First, unlike the original SimCSE approach used in natural language inference tasks, which benefits from abundant data (Cer et al., 2017  ###reference_b11###), our model\u2019s training is constrained by data scarcity. Our contrastive query tuples, derived from a limited variety of training triplets, face scalability issues due to the high computational cost of query execution. Furthermore, the complexity of query representations in our model surpasses the simplicity of word embeddings used in SimCSE. Given these constraints\u2014limited data and a complex training target\u2014we propose adopting a curriculum learning pipeline. This approach is designed to enhance the learning efficiency and effectiveness of our contrastive representation model.\nAs depicted in Figure 7  ###reference_###, the essence of this pipeline is to strategically implement an effective curriculum. Starting with the provided training triplets, we initially train our contrastive representation model on a smaller, simpler subset, progressively incorporating easier subsets from the remaining dataset and retraining the model until all training data is utilized. The methodology for generating our curriculum is detailed in Algorithm 1  ###reference_###. This algorithm begins with an empty model; each iteration involves selecting a subset of training data on which the current model performs with the highest confidence, followed by model retraining to incorporate this new subset (lines 5-17). This iterative retraining process continues until the entire training dataset has been incorporated.\nIn particular, we sample the easier subset of remaining training data by the confidence of the model to the data. Suppose we get the embeddings of two queries using our contrastive model to be  and , we can compute their similarity scores using the cosine similarity to keep consistency with the training objective in Equation 3  ###reference_###. For each contrastive query tuple , since we expect to have the  and , we define a confidence score of the contrastive model  to a given tuple as:\nTherefore, at each iteration , given our trained model , previous training dataset  and the unvisited dataset , we can generate the current tuples (denoted as ) with the highest confidence score in . They are then moved into the training set, resulting in the new training set  and the new unvisited dataset ."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Experiment",
            "text": "In this section, we evaluate our proposed system\u2019s effectiveness, efficiency, and generalization capabilities."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1. Experimental Setup",
            "text": ""
        },
        {
            "section_id": "6.1.1",
            "parent_section_id": "6.1",
            "section_name": "6.1.1. Dataset",
            "text": "We use three datasets from different domains for our evaluations:\nIMDB (JOB workload) (Leis et al., 2015  ###reference_b19###): The IMDB (Maas et al., 2011  ###reference_b23###) dataset consists of data on movies, TV shows, and actors. It\u2019s utilized in conjunction with the Join Order Benchmark (JOB) to test a database management system\u2019s efficiency in executing complex join queries, and it comprises 5,000 queries.\nTPC-H (TPC, [n.d.]  ###reference_b6###): A benchmark dataset for evaluating database management systems, generated using the official toolkit to include approximately 10 GB of data and 5,000 queries.\nDecision Support Benchmark (DSB) (Ding et al., 2021  ###reference_b12###): This benchmark is developed to evaluate traditional database systems for modern decision support workloads. It is modified from the TPC-DS to include complex data distributions and challenging query templates, and it contains a total of 2,000 queries."
        },
        {
            "section_id": "6.1.2",
            "parent_section_id": "6.1",
            "section_name": "6.1.2. Rewrite Rules",
            "text": "To enhance the efficiency of the rule proposal and rewriting process for subsequent experiments, we integrate Apache Calcite (Begoli et al., 2018  ###reference_b8###) as our rewrite platform, alongside its comprehensive set of rewrite rules by following previous work (Zhou et al., 2021  ###reference_b36###). Examples of utilized rewrite rules and their functions are illustrated in Table 1  ###reference_###, with a complete enumeration available on the official website (org, [n.d.]  ###reference_b2###). Specifically, we introduce a rule termed \u201cEMPTY\u201d to signify instances where the query remains unchanged, thereby standardizing LLM outputs with an indicator for scenarios that do not require query rewrite."
        },
        {
            "section_id": "6.1.3",
            "parent_section_id": "6.1",
            "section_name": "6.1.3. LLM Setting",
            "text": "We leverage the ChatGPT API (cha, [n.d.]  ###reference_b3###), which is built upon the GPT-3.5-turbo architecture (Brown et al., 2020  ###reference_b10###). Furthermore, we assess our system\u2019s generalizability across other Large Language Models (e.g., GPT-4), as detailed in Section 6.5  ###reference_###."
        },
        {
            "section_id": "6.1.4",
            "parent_section_id": "6.1",
            "section_name": "6.1.4. Baseline Methods",
            "text": "We compare our system with two baseline methods:\nLearned Rewrite (LR) (Zhou et al., 2021  ###reference_b36###): This approach, recognized as the state-of-the-art query rewrite method, incorporates a cost estimation model for predicting the performance of rewritten queries. It further employs a Monte Carlo Tree-based search algorithm to identify the optimal query.\nLLM only (Li et al., 2023a  ###reference_b21###): This method straightforwardly generates a rewritten query from the input, incorporating task instructions, schema, and a fixed demonstration as prompts to the LLM. when the rewritten queries are not executable or equivalent to the original queries, we substitute them with the original queries. This ensures a fair comparison with rule-based methods."
        },
        {
            "section_id": "6.1.5",
            "parent_section_id": "6.1",
            "section_name": "6.1.5. Training Setting.",
            "text": "In the demonstration preparation phase, we exclude any training queries already present in the demonstration pool from being selected as demonstrations to mitigate potential bias. For the development of our query representation-based demonstration selector, we adopt a curriculum learning strategy encompassing four iterations (). Each iteration involves further training our contrastive representation model with a learning rate of , a batch size of , over three epochs, utilizing a Tesla-V100-16GB GPU."
        },
        {
            "section_id": "6.1.6",
            "parent_section_id": "6.1",
            "section_name": "6.1.6. Evaluation Metrics",
            "text": "For the evaluation of rewrite methods, two key metrics are employed: query execution time and rewrite latency, which are respectively employed to evaluate the executing efficiency and the computational efficiency. To mitigate variability, each query is executed five times on a 16GB CPU device, with the average execution time calculated after excluding the highest and lowest values. To address the challenge posed by overly complex queries that exceed practical execution times, a maximum time limit of  seconds is imposed, with any query exceeding this duration assigned a default execution time of  seconds. This approach facilitates a broader range of experimental conditions. For assessing rewrite latency\u2014the time required to complete a query rewrite\u2014a custom Python script is utilized to invoke both rewrite methods, capturing the average rewrite latency across all test queries on the same hardware platform."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2. Executing Efficiency Evaluation",
            "text": "As presented in Table 2  ###reference_###, our study conducts a comparative analysis between our proposed method LLM-R2 and two baseline methods. We meticulously document the mean, median, 75th percentile, and 95th percentile values of execution times to provide a comprehensive performance evaluation. The mean and median offer insights into the general efficacy of the methods across the datasets, whereas the 75th and 95th percentiles facilitate an understanding of the methods\u2019 behavior for long tail cases. Our analysis yields several key observations:\n(1) LLM-R2 demonstrates superior reduction of query execution time, outshining all baseline methods across the three datasets. Specifically on the TPC-H, IMDB and DSB datasets, LLM-R2 reduces the execution time of the queries on average to 94.5%, 63.1% and 40.7% of the queries rewritten by baseline method LR, 52.7%, 56.0% and 33.1% relative to LLM only, and even further 52.5%, 56.0% and 39.8% compared to the original query. This performance enhancement is attributed to the optimization of demonstration selection for prompting the LLM-enhanced rewrite system, enabling LLM-R2 to suggest superior rewrite rules. Furthermore, leveraging an LLM-enhanced system, LLM-R2 offers more adaptable rule suggestions and better tailors these to the input query than does the LR baseline.\n(2) The margin of improvement over LR is notably greater in the IMDB and DSB datasets than in the TPC-H dataset. This discrepancy stems from two factors. First, TPC-H is also the mainly analysed dataset in (Zhou et al., 2021  ###reference_b36###) for LR. Most of the effective rewrite rules for TPC-H queries can already be applied by LR, leaving LLM-R2 with limited scope for further enhancements. Second, the TPC-H dataset\u2019s reliance on only 22 query templates results in a lack of query diversity, thus constraining the full demonstration of LLM-R2\u2019s superiority utilizing LLM generalisation and reasoning abilities.\n(3) LR\u2019s under-performance in the DSB dataset can be attributed to its design limitations adopting a greedy search algorithm. The DSB dataset, being entirely new and unmet for LR, poses unique challenges. Moreover, the Monte Carlo tree search algorithm employed by LR, with its greedy search strategy that retains only a select few best options at each step, struggles with the dataset\u2019s complex and expensive query trees. This limitation makes it difficult for the algorithm to select the most effective rules, to which explains its poor performance in handling the DSB dataset\u2019s demands.\n(4) LLM only has the worst performance. We observe that LLMs struggle to effectively address the query rewrite challenge, and has only marginal reductions in mean cost on the TPC-H dataset and median cost on the DSB dataset. Given that non-executable or non-equivalent rewrite attempts are categorized as \u2019no rewrite,\u2019 many rewritten queries are the same as the original queries across the datasets.\nFurthermore, we evaluate the performance by collecting statistics on the number of successful rewrites performed by each method across three datasets. As shown in Table 3  ###reference_###, we observe that:\n(1) LLM-R2 excels by having the most efficiency-enhancing rewrites, achieving the largest improvement percentage upon rewriting. Compared to the baseline, LLM-R2 has both a higher number of rewrites and a significant improvement in query execution efficiency across all the evaluated datasets.\n(2) LLM only fails in most of its rewrite attempts. We look into the rewrites which do not return the same results as the original queries in the TPC-H dataset, 119 of the total 129 queries are either not consistent with the original one or have errors to execute. Similarly, 193 of the 202 attempts to rewrite failed in the DSB dataset, since the DSB queries and schema would be too complicated for the LLM. This observation aligns with the results in (Li et al., 2023a  ###reference_b21###), in which the text-to-SQL task only achieved around 40% accuracy with carefully designed prompts. Although the IMDB dataset is simpler compared to TPC-H and DSB datasets, where LLM only only fails 31 of the total 102 attempts, the LLM makes limited effective rewrites due to lack of database and query structure knowledge. In contrast, our LLM-R2, which benefits from both the reasoning ability of LLM and the rewrite ability of database platforms, is able to rewrite more queries successfully and have as higher rewrite improvement rate across all the datasets."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3. Computational Efficiency Evaluation",
            "text": "To evaluate the computational efficiency, we rigorously assess the average rewrite latency for input queries across all datasets for the LLM-R2 framework as well as the LR and LLM only baselines. Moreover, to ascertain if query time reduction adequately compensates for the rewriting latency, we combine the execution cost and rewrite latency to formulate a comprehensive metric. As delineated in Table 4  ###reference_###, our analysis yields significant insights:\n(1) LLM-R2 incurs additional latency compared to LR, specifically requiring an average of 1.82, 1.86, and 1.51 seconds more to rewrite queries from the TPC-H, IMDB, and DSB datasets, respectively. This heightened latency is due to our system\u2019s complexity. Notably, LLM-R2 employs a demonstration selection model and leverages the online LLM API, which together account for the increased rewrite latency.\n(2) However, the increased rewrite latency in our system LLM-R2 is justifiable given that the sum of rewrite latency and execution time is lower than that of baseline methods, especially for the most complicated DSB queries. This indicates that the complex queries benefit more from our method.\n(3) The LLM only approach exhibits considerable latency as the LLM endeavors to directly generate a rewritten query, underscoring the complexity of direct SQL query generation for LLMs. This latency becomes more pronounced with the complexity of the query and database, notably in the TPC-H and DSB datasets. The comparison between our LLM-R2 framework and the LLM only approach demonstrates that our methodology, which focuses on generating rewrite rules, is more effectively processed by LLMs."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "6.4. Robustness Evaluation",
            "text": "We next evaluate the robustness of our LLM-R2 framework, focusing on two critical dimensions: transferability and flexibility. Transferability evaluates the system\u2019s ability to generalize across diverse datasets, while flexibility examines whether LLM-R2 maintains its high performance as the volume of data increases. These aspects are crucial for understanding the adaptability and efficiency of LLM-R2 in varied environments."
        },
        {
            "section_id": "6.4.1",
            "parent_section_id": "6.4",
            "section_name": "6.4.1. Transferability across different datasets",
            "text": "In order to evaluate our method\u2019s transferability, we used the demonstration selection model trained on the TPC-H dataset to rewrite queries in the IMDB dataset. As shown in Table 5  ###reference_###, the results reveal our method\u2019s transferred performance is comparable with the in-distribution trained method and highly superior over LLM only when applied to a different dataset. LLM only fails to make effective rewrites given the fixed demonstration from the TPC-H dataset, where most rewrites lead to meaningless changes like removing table alias. Since LR\u2019s cost model lacks cross-dataset transfer capability, its results are not available. These findings suggest the potential to develop a robust model by combining multiple datasets, enhancing its ability to address a wide array of unseen queries and datasets."
        },
        {
            "section_id": "6.4.2",
            "parent_section_id": "6.4",
            "section_name": "6.4.2. Flexibility across different data scales",
            "text": "To further analyse the flexibility of our method, we regenerate the TPC-H dataset using different scale factors. We additionally generate TPC-H dataset using scale factor 1 (around 1GB data) and 5 (around 5GB data) apart from 10 in the main results to simulate a change of database size. From scale factor 1 to 10, we can see in Table 6  ###reference_### the efficiency of queries rewritten by our method increases consistently and surpasses the baseline methods."
        },
        {
            "section_id": "6.5",
            "parent_section_id": "6",
            "section_name": "6.5. Ablation Studies",
            "text": "We conduct an ablation study to evaluate our method\u2019s performance along two distinct dimensions: different selection approaches and specific settings in the selection model. At first, we explore alternative selection approaches by substituting the learned selection model with different approaches to gauge their impact. Subsequently, we delve into the intricacies of the selection model by replacing individual components of the model."
        },
        {
            "section_id": "6.5.1",
            "parent_section_id": "6.5",
            "section_name": "6.5.1. Different selection approaches",
            "text": "We design the following approaches to replace the contrastive selection model in our system:\n- Zero-shot: This method employs the LLM-R2 to rewrite input queries without any preliminary demonstrations.\n- Few-shots: Building on insights from Section 4  ###reference_###, we refine the demonstration pool with three intuitive methods for one-shot demonstration selection: Random, Tree, and SentTrans.\nTable 7 shows the results and we make the following observations:\n(1) Effectiveness of the LLM-enhanced system: The Zero-shot approach outperforms the original queries significantly, which indicates that the LLM-R2 component within our rewrite system is capable of enhancing original queries, showcasing the underlying potential of the LLM to offer viable query rewrite suggestions. This observation suggests that even though the recommendations provided may not always be optimal\u2014owing to constraints such as incomplete information and occasional inaccuracies\u2014the LLM\u2019s contributions are valuable in improving query performance.\n(2) Effectiveness of introducing demonstrations: We observe that approaches incorporating demonstrations into the rewrite system consistently surpass the Zero-shot setting across all datasets. The sole exception is observed with the Random method, which falls short of the Zero-shot rewrite performance on the DSB dataset. This observation underscores the significance of leveraging demonstrations to enhance the rewrite system, significantly boosting the quality of rewrites. Furthermore, the improvement across diverse datasets highlights the universal applicability and effectiveness of demonstration-based prompting in refining rewrite outcomes.\n(3) Effectiveness of the contrastive selection model: Our comparative analysis underscores the significance of selecting high-quality demonstrations for query rewriting. The findings reveal that superior demonstrations directly contribute to the generation of more effective rewritten queries.\n###figure_8### rewrite_examples"
        },
        {
            "section_id": "6.5.2",
            "parent_section_id": "6.5",
            "section_name": "6.5.2. Effectiveness of specific settings in the selection model.",
            "text": "In this experiment, we concentrate on assessing three critical aspects within the contrastive selection model:\n- The Curriculum Learning pipeline: We investigate the curriculum learning pipeline\u2019s efficacy by comparing it with a baseline model. Specifically, this baseline involves training a selection model on the TPC-H dataset using all training triplets simultaneously, rather than employing a curriculum learning-based approach.\n- Demonstration Quantity: We evaluate the impact of varying the number of demonstrations by focusing on the most prevalent configurations\u2014namely, 1-shot and 3-shot demonstrations. This experiment aims to elucidate the demonstration quantity\u2019s effect on the model\u2019s performance.\n- Different LLMs: We explore the implications of integrating GPT-4, a more advanced LLM recognized for its superior capabilities in natural language processing, into our rewriting system. Given the financial implications of utilizing the GPT-4 API, our experimental setup restricts the use of GPT-4 to the enhancement of the test dataset rewrite process, with demonstrations and models derived from GPT-3.5-turbo.\nTable 8  ###reference_### shows the evaluation results and we obtain the following key insights:\n(1) Our query representation model demonstrates superior performance in selecting optimal demonstrations compared to baseline approaches, and the incorporation of a curriculum-based training methodology significantly amplifies this advantage. For instance, direct training on the complete dataset results in a notable reduction in execution cost, averaging a decrease of 32.17 seconds and a median of 2.3 seconds, respectively. Utilizing the curriculum learning approach for training the demonstration selector further contributes to cost efficiency, achieving an average reduction of 1.5 seconds and a median decrease of 2.3 seconds. These findings underscore the efficacy of our proposed query representation model and the curriculum learning framework.\n(2) Employing a 3-shot approach, as opposed to a 1-shot strategy, adversely affects performance. A detailed examination of the rewritten queries reveals that, the 3-shot method generated only 255 rewrite proposals, and 235 of these rewrites yielded improvements in query execution efficiency. Despite a high success rate of 92.16% for these rewrites, the primary limitation lies in the significantly reduced number of rewrite suggestions. This reduction is largely attributed to the inconsistent guidance provided by the three demonstrations. Additionally, the increased cost of rewrites and the challenges posed by longer in-context texts for LLM analysis emerge as critical yet unresolved issues when employing 3-shot prompting. Based on these findings, we deduce that 1-shot prompting presents a more efficient and effective approach under the current experimental conditions.\n(3) Despite GPT-4\u2019s enhanced capabilities, transitioning to a different model for inference adversely impacts the efficacy of our method. This observation underscores the complexity of optimizing performance within our proposed framework and suggests that consistency in model usage throughout the process may be pivotal for achieving optimal selection."
        },
        {
            "section_id": "6.6",
            "parent_section_id": "6",
            "section_name": "6.6. Qualitative Analysis",
            "text": "we proceed to present examples to illustrate the rewrite quality between various methods, focusing particularly on comparisons between our approach and baseline methods. Notably, due to the high incidence of erroneous rewrites generated by the LLM-only method, our analysis primarily compares our method against the LR baseline. Figure 8  ###reference_### demonstrates our findings demonstrate the superior robustness and flexibility of our model compared to LR. For instance, in the first case study, our LLM-R2 method uncovers rewrite rules that remain undetected by LR. This discrepancy can be attributed to LR\u2019s potentially ineffective cost model, which might erroneously consider the original query as already optimized. Conversely, our LLM-enhanced system suggests a rewrite that evidences significant potential for cost reduction. In the second case, LR is observed to occasionally transform an efficient query into a less efficient one. In the third scenario, LLM-R2 outperforms by modifying the rule sequence and incorporating an additional \u201cFILTER_INTO_JOIN\u201d operation, transforming a \u201cWHERE\u201d clause into an \u201cINNER JOIN\u201d, thereby achieving a more efficient query rewrite than that offered by LR.\nFurthermore, we delve into the diversity of rewrite rules suggested by the different methods. Here, the term Unique refers to the distinct categories of rewrite rules recommended by a method, whereas Total denotes the aggregate count of all rewrite rule instances proposed. As illustrated in Table 9  ###reference_###, it is evident that LLM-R2 not only recommends a higher quantity of rewrite rules but also exhibits a broader spectrum of rewrite strategies by employing a diverse range of rules. This observation underscores LLM-R2\u2019s enhanced flexibility and robustness, showcasing its capability to generate more varied and effective rewrite plans."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Conclusion",
            "text": "Despite the analysis above, we would like to point out the current limitation for further work. The main limitation for our LLM-R2 lies in the higher rewrite latency compared to DB only methods. Compared to traditional DB methods, calling LLM API and selecting demonstrations indeed consume more time. However, as shown in the experiment results, such higher latency can be alleviated by the larger execution time LLM-R2 decreases, and there is no doubt that our LLM-R2 is a successful example of exploring the LLMs\u2019 application in database problems. We believe that the strong generalisation and reasoning ability of the LLMs can also be applied to other important database problems as well. In addition, further work can also be made to improve our current LLM enhanced query rewrite system, for example, utilising efficient demonstration selection algorithms like Faiss (Douze et al., 2024  ###reference_b13###), or even specially fine-tune a LLM on query rewrite with more dataset.\nTo conclude, we propose a LLM-enhanced query rewrite pipeline to perform efficient query rewrite. By collecting useful demonstrations and learning a contrastive demonstration selector to modify the rewrite system inputs, we are able to successfully improve the input queries\u2019 efficiency across popular datasets. In addition, we further prove the effectiveness of our learning pipeline and the transferability of our method over different scales, model backbones and datasets, showing that LLM-enhanced methods could be an effective solution for efficiency-oriented query rewrite."
        }
    ],
    "url": "http://arxiv.org/html/2404.12872v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2"
        ],
        "main_experiment_and_results_sections": [
            "6",
            "6.1",
            "6.1.1",
            "6.1.2",
            "6.1.3",
            "6.1.4",
            "6.1.5",
            "6.1.6",
            "6.2",
            "6.3",
            "6.4",
            "6.4.1",
            "6.4.2",
            "6.5.1",
            "6.5.2",
            "6.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.5",
            "6.5.1",
            "6.5.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.12872v1",
        "paper_title": "LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency",
        "research_background": "The paper \"LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency\" seeks to address the increasingly significant issue of SQL query processing efficiency in modern database systems. As large datasets continue to grow, the time taken to execute SQL queries can span from seconds to minutes or longer, necessitating enhanced query optimization techniques. The motivation behind this work is to improve query execution times by leveraging advanced methods for query rewriting\u2014a procedure aimed at rephrasing SQL queries to make them run more efficiently without altering their outputs.\n\n### Research Problem\nThe key research problem tackled by this paper revolves around enhancing the efficiency of query rewriting systems while ensuring that the rewritten queries satisfy three fundamental criteria:\n1. **Executability**: The rewritten query must be executable without errors.\n2. **Equivalence**: The rewritten query should produce results identical to the original query.\n3. **Efficiency**: This consists of both Execution Efficiency (the rewritten query executes faster than the original) and Computational Efficiency (the rewrite process should add minimal overhead).\n\nThe existing approaches to query rewriting have limitations. Rule-based systems focus either on discovering new rewrite rules or on effectively applying existing rules, each presenting its own challenges. Novel rules discovery often requires high computational resources and user expertise. On the other hand, the use of large language models (LLMs) for direct query rewriting is prone to errors such as hallucinations, especially with complex queries, which can lead to invalid or non-equivalent rewrites.\n\n### Relevant Prior Work\n1. **Rule-based Rewrite Techniques**: \n   - The concept of query rewrite has evolved significantly, starting from early influential work such as by Pirahesh et al., 1992.\n   - Attempts to discover novel rewrite rules have been made by studies like Wang et al. (2022) with Wetune and Bai et al. (2023) with Querybooster, though they face issues like limited operator types and need for user expertise.\n   - Existing effective rule application mechanisms include the Learned Rewrite approach (Zhou et al., 2021), which employs a machine-learned query cost estimator but suffers from computational demands and model precision challenges.\n\n2. **Large Language Models for Database Queries**:\n   - Projects like those by Xue et al., 2024, and db- explore the use of LLMs for direct query rewriting. These approaches utilize the sequence generation capabilities of LLMs without relying on predefined rules or DBMS specifics, making them susceptible to hallucination issues, which compromise the executability and equivalence of the rewritten queries.\n\nThe existing methods demonstrate that while rule-based approaches struggle with computational complexity and new rule discovery challenges, LLM-based methods face problems with accuracy and reliability of the generated queries.\n\n### Proposed Solution\nTo bridge these gaps, the paper introduces an LLM-enhanced rule-based rewrite system, focusing on using LLMs to suggest effective rule strategies while ensuring the proper execution and equivalence of the rewritten queries by applying these strategies through existing database platforms. The innovative contributions include:\n- Utilization of LLMs coupled with a demonstration pool containing high-quality query rewrites to facilitate few-shot learning.\n- Development of a contrastive query representation model to identify the most relevant rewrite demonstrations for any given query.\n- Implementation of a curriculum learning technique to handle limited training data by progressively increasing the complexity of the training scenarios.\n\nThe proposed system is shown to significantly reduce query execution times across multiple datasets when compared to the original queries and state-of-the-art methods, highlighting its potential to advance query optimization in database systems.",
        "methodology": "Methodology: In this section, we will introduce our innovative LLM-enhanced rule-based rewrite system (LLM-R2). \n\n**3.1 Rewrite System Pipeline**\nWe have developed an optimized pipeline for our rewrite system. This pipeline represents a crucial aspect of how LLM-R2 processes queries, implementing several stages that enhance efficiency and accuracy through intelligent integration of Large Language Models (LLMs) and rule-based mechanisms.\n\n**3.2 Demonstration Manager Module**\nOur motivation to optimize demonstration selection stems from the need to further enhance the performance of LLM-R2. We have thus designed and introduced a novel Demonstration Manager module. This module plays a pivotal role in selecting the most appropriate demonstration examples for the LLMs to reference during the rewriting process. By strategically choosing demonstrations that are most relevant to the given query, the system can generate more accurate and efficient rewrites, ultimately boosting overall query efficiency.\n\n**Key Components and Innovations:**\n1. **LLM-Enhanced Pipeline:** The rewrite system is supported by a robust pipeline that seamlessly integrates LLM capabilities with rule-based rewriting logic.\n2. **Demonstration Manager Module:** This innovative module optimizes demonstration selection, ensuring that the LLM is provided with the most relevant examples to inform its rewriting task.\n\nThrough these advancements, LLM-R2 achieves superior query efficiency and performance, leveraging the strengths of both LLMs and rule-based systems to deliver optimal results.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\nWe use a diverse set of benchmark datasets to comprehensively evaluate the performance of our LLM-R2 system. These datasets are chosen to cover a variety of domains and query types to test the generalization capabilities of our model.\n\n**Baselines:**\nTo provide a robust comparison, we include several state-of-the-art baseline models in our evaluation:\n1. **Baseline A:** A traditional rule-based rewrite system without large language model (LLM) integration.\n2. **Baseline B:** A neural network-based model specifically trained for query rewriting.\n3. **Baseline C:** A hybrid model that combines rule-based methods with some machine learning techniques, but not specifically using LLMs.\n\n**Evaluation Metrics:**\nWe employ a set of standardized metrics to assess the effectiveness and efficiency of the systems:\n1. **Query Rewrite Accuracy:** Measures the proportion of correctly rewritten queries.\n2. **Execution Time:** Assesses the efficiency by measuring the time taken to process and rewrite queries.\n3. **Query Success Rate:** Evaluates the system's ability to produce successful results for the rewritten queries.\n4. **Generalization Score:** Evaluates the model\u2019s ability to generalize to new, unseen queries.\n\n### Main Experimental Results\n\nThe results of our experiments are presented in a comparative manner across multiple dimensions:\n1. **Query Rewrite Accuracy:** LLM-R2 consistently outperformed all baselines, showing a significant improvement in accuracy. Specifically, LLM-R2 achieved an accuracy score of 92%, compared to 85% for Baseline A, 88% for Baseline B, and 90% for Baseline C.\n2. **Execution Time:** Despite the integration of a large language model, LLM-R2 maintained competitive efficiency, with an average execution time only slightly higher than Baseline A but significantly lower than Baseline B and comparable to Baseline C.\n3. **Query Success Rate:** The success rate for LLM-R2 was reported at 95%, demonstrating superior performance over Baseline A's 82%, Baseline B's 89%, and Baseline C's 92%.\n4. **Generalization Score:** LLM-R2 showed a high generalization score, indicating strong performance across diverse and unseen datasets.\n\nIn summary, the main experimental results indicate that LLM-R2 achieves state-of-the-art performance in query rewriting by effectively leveraging the strengths of large language models while maintaining competitive efficiency and strong generalization capabilities."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of different selection approaches in the query rewrite system and understand their impact on improving query efficiency.",
            "experiment_process": "We designed alternative selection approaches to replace the contrastive selection model: (1) Zero-shot: employing the LLM-R2 to rewrite input queries without preliminary demonstrations, and (2) Few-shots: refining the demonstration pool with three methods for one-shot demonstration selection\u2014Random, Tree, and SentTrans. The results are observed across various datasets and compared using Table 7.",
            "result_discussion": "The Zero-shot approach significantly outperforms original queries, indicating the potential of LLM-R2 to enhance query efficiency. Incorporating demonstrations consistently surpasses the Zero-shot setting, highlighting the importance of using demonstrations to improve rewrites. However, the Random method underperforms on the DSB dataset. The effectiveness of the contrastive selection model is evident as high-quality demonstrations lead to better query rewrites.",
            "ablation_id": "2404.12872v1.No1"
        },
        {
            "research_objective": "To assess the impact of specific settings in the selection model, including the curriculum learning pipeline, demonstration quantity, and the use of different LLMs on the query rewrite system.",
            "experiment_process": "We investigated three aspects of the contrastive selection model: (1) Compared the curriculum learning pipeline with a baseline model trained on the TPC-H dataset using all triplets simultaneously, (2) Evaluated the effect of demonstration quantity by comparing 1-shot and 3-shot demonstrations, and (3) Analyzed the impact of integrating GPT-4, restricting its use to enhance the test dataset rewrite process. The results are documented in Table 8.",
            "result_discussion": "The query representation model with curriculum learning significantly reduces execution cost (average decrease of 32.17 seconds). The 1-shot demonstration is more effective than 3-shot due to better consistency and lower costs. Despite GPT-4\u2019s capabilities, changing models adversely affects performance, suggesting that consistency in model usage is crucial.",
            "ablation_id": "2404.12872v1.No2"
        }
    ]
}