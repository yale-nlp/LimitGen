{
    "title": "Case-Based Reasoning Approach for Solving Financial Question Answering (March.29, 2024)",
    "abstract": "Measuring a machine\u2019s understanding of human language often involves assessing its reasoning skills, i.e. logical process of deriving answers to questions. While recent language models have shown remarkable proficiency in text-based tasks, their efficacy in complex reasoning problems involving heterogeneous information such as text, tables, and numbers remain uncertain. Addressing this gap, FinQA ((Chen et al., 2021)) introduced a numerical reasoning dataset for financial documents and simultaneously proposed a program-generation approach . Our investigation reveals that half of the errors (48%) stem from incorrect operations being generated. To address this issue, we propose a novel approach to tackle numerical reasoning problems using case-based reasoning (CBR), an artificial intelligence paradigm that provides problem-solving guidance by offering similar cases (i.e. similar questions and corresponding logical programs). Our model retrieves relevant cases to address a given question, and then generates an answer based on the retrieved cases and contextual information. Through experiments on the FinQA dataset, we demonstrate competitive performance of our approach and additionally show that by expanding case repository, we can help solving complex multi-step programs which FinQA showed weakness of.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Reading comprehension is a key metric for assessing an individual\u2019s ability to interpret and integrate written text. Natural Language Processing (NLP), developed to process large volumes of sequential data, includes the area of Question Answering (QA), which focuses on systems that automatically answer natural language questions, effectively measuring a machine\u2019s language understanding. QA research has evolved significantly since the 1960s (Androutsopoulos et al., 1995  ###reference_b3###), particularly with deep learning\u2019s rise, marked by the creation of over 80 new benchmark datasets in the last two years (Rogers et al., 2023  ###reference_b22###).\nThe advancement of large language models has notably enhanced text-based QA tasks, even outperforming humans on some benchmarks (Liu et al., 2019  ###reference_b17###; Lan et al., 2019  ###reference_b15###; Clark et al., 2020  ###reference_b7###). This success has spurred interest in table-based QA, leading to new frameworks for tabular data pre-training. However, despite the achievements in text-based applications, the performance in table-based QA tasks has not been as impressive (Herzig et al., 2020  ###reference_b12###; Yin et al., 2020  ###reference_b27###), raising questions about their effectiveness in handling complex reasoning in multi-modal documents that include text, tables, and numerical data. The current research tends to overlook the complexities of processing different types of information found in real-world documents.\nTo bridge this gap, recent efforts have focused on creating new QA datasets that incorporate both tabular and textual data, necessitating numerical reasoning (Chen et al., 2021  ###reference_b6###; Zhao et al., 2022  ###reference_b28###; Zhu et al., 2021  ###reference_b29###). Specifically, the FinQA dataset by Chen et al. (2021) provides a benchmark for numerical reasoning in financial documents (Chen et al., 2021  ###reference_b6###). Despite utilizing pre-trained language models such as BERT and RoBERTa, fine-tuned for these datasets, baseline models have yielded underwhelming performance. This discrepancy in results, especially given these models\u2019 effectiveness in traditional text-based QA tasks, suggests a pressing need for further research into numerical reasoning QA that integrates both table and textual data. \nMoreover, when evaluating the capabilities of large language models (LLM) like GPT-3.5, it becomes apparent that LLMs also face challenges in this domain. They frequently struggle to retrieve table cells and fail to comprehensively address queries requiring complex reasoning steps, underscoring the necessity for advancements in our approach to QA tasks involving numerical reasoning.\nOur analysis indicates that a significant source of errors in current baseline model arises from incorrect operations during the reasoning steps. Specifically, the accuracy of producing the correct answer (execution accuracy) was often lower than the accuracy of generating the correct program (program accuracy), due to the model generating incorrect operations but accidentally arriving at the right answer. To accurately assess a machine\u2019s reasoning ability, program accuracy is deemed more crucial than execution accuracy. The current model utilizes a sequence-to-sequence (seq-to-seq) architecture with an LSTM decoder, which creates programs in an autoregressive manner, leading us to wonder if pre-informing the model about operations could be beneficial.\nIn response to these challenges, we propose a novel strategy that employs case-based reasoning (CBR), an artificial intelligence technique that solves new problems by referencing solutions to similar problems. Our approach involves retrieving cases that are closely related in terms of questions and programs to address specific queries, followed by producing answers leveraging these cases and contextual clues. By integrating similar questions and their operations as additional data, we aim to enhance the model\u2019s ability to solve complex reasoning tasks. Furthermore, we anticipate that providing the model with a more closely related cases will enhance its ability to generate accurate programs.\nWe plan to evaluate our approach across multiple settings: the basic (FinQA baseline model), gold case, and retrieved case. In the gold case environment, we provide cases with identical operations to the answer for the program generation model. After assessing the effectiveness of our strategy with gold cases, we proceed to test our CBR method using cases that have been actively retrieved. Our experiments will explore various retrieval models and methods of case presentation to the program generation model, aiming to demonstrate that our model can identify relevant cases and outperform the current baseline model."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "Given the document\u2019s extensive text and tables, a context retriever was initially developed to identify the most relevant contexts for answering a specific question. The goal of this model is to pinpoint the top-k relevant contexts E, given a question Q and a document D. FinQANet utilizes a dense vector spaces approach for quickly and accurately extracting pertinent contexts from large documents, shifting from traditional, less efficient vector space models like TF-IDF and BM25 (Sparck Jones, 1972  ###reference_b23###; Robertson et al., 1995  ###reference_b21###). This approach facilitates more meaningful semantic comparisons, leading to more precise matches between questions and their relevant contexts (Karpukhin et al., 2020  ###reference_b14###). \nThe context retriever leverages a pre-trained BERT model, processing combinations of the question with each context as input to the model. It then fine-tunes this classifier on the FinQA dataset. The top-k relevant contexts identified are subsequently used in the program generation phase.\n###figure_1### The program generator operates on a sequence-to-sequence (seq-to-seq) architecture, which is ideally suited for handling long sequential data through an encoder and decoder structure (Sutskever et al., 2014  ###reference_b24###). Initially, this system examines the given question Q and the contexts E retrieved earlier, with its encoder capturing essential information. Subsequently, the decoder systematically generates the final program P in an autoregressive manner. For the decoding process, FinQANet employs an LSTM model, incorporating an attention mechanism to improve the precision and clarity of its outputs (Bahdanau et al., 2014  ###reference_b4###).\nA key distinction of this program generator from standard seq-to-seq models lies in wider range of decoder outputs. This not only includes the input embeddings but also specific tokens for developing a logical program, such as various operations and step memory tokens. These step memory tokens are instrumental in outlining the program\u2019s logical sequence. An illustrative Figure  1  ###reference_### details the process of program decoding within the FinQANet\u2019s program generator.\nThe FinQA dataset utilizes a specific programming language designed for executing mathematical and tabular operations. The program sequence includes a variety of operations, such as addition, subtraction, multiplication, division, and others, tailored for table summarization and averaging. The operands for these operations are either directly extracted from the document or derived from preceding steps. For example, to perform a subtraction on the results of two division operations, it could be articulated as follows: Divide(arg1, arg2), Divide(arg1, arg2), Subtract(#0, #1).\nThe quantity of potential cases for each query is equivalent to the dataset\u2019s size. Considering the extensive pool of candidates, it\u2019s essential to identify \"gold cases\" for model training purposes. During the training phase, we leverage those gold cases, which are ideal examples that our case retriever aims to identify. Developing a mechanism for accurately retrieving these cases poses a significant challenge. To define and select gold cases, we first evaluate the similarity between the given question and candidate question.\nWe utilize a BERT encoder to generate representations of the questions (Devlin et al., 2018  ###reference_b9###). Through computing the cosine similarity between these vector representations, we can identify cases that closely match the input question in terms of their content and context. Cosine similarity measures the cosine of the angle between two vectors, indicating how closely related they are in terms of orientation in a multi-dimensional space. It is defined as:\nFollowing the question similarity assessment, we rank candidate cases based on the similarity of their logical solutions or \"programs,\" using program scores. To determine program similarity, we employ the Levenshtein (edit) distance, which quantifies the minimum number of edits (insertions, deletions, substitutions) needed to transform one sequence into another (Levenshtein et al., 1966  ###reference_b16###). This comparison is done at the word level, prioritizing the matching of operations over arguments due to their critical role in logical reasoning. The program score is calculated as follows:\nwhere  and  indicate the number of operations and arguments in the target program, respectively, and  and  denote the edit distances for operations and arguments between the candidate and target programs.\nCandidates with program scores above 0.9 are considered gold cases, while others form the negative set. From analyzing the top 100 candidates based on question similarity, approximately 5.3% of our dataset did not have appropriate positive candidates. However, we expect that our case retrieval model can still effectively learn from instances that do have positive matches. We plan to explore diverse strategies to introduce more variability into our candidate pool for training purposes, which will be elaborated upon in the methodology section. The selection of the 0.9 threshold was aimed at ensuring the high quality of candidate selection."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Overview of FinQANet",
            "text": "FinQANet is designed as a benchmark model for the FinQA challenge, built around a dual-component architecture: a context retrieval system and a program generation system. Financial reports, often exceeding 2,000 tokens, pose a significant challenge for existing QA models due to their length. To address this, FinQANet begins by pinpointing the most pertinent sections of the input document. For handling tabular data, it converts each row into a readable sentence format. For instance, a table cell with row header named \u2019risk-free interest rate\u2019 and column header named \u20192006\u2019 can be converted into \u201cthe risk-free interest rate of 2006 was 5%.\u201d\nFor a question Q and a document D that includes both text and tables, the model identifies the top k relevant contexts E. It then formulates a series of programmatic steps , where each  represents a predefined operation or one derived from the contexts. This series of steps, when executed, produces the answer A, calculated as follows:\nGiven the document\u2019s extensive text and tables, a context retriever was initially developed to identify the most relevant contexts for answering a specific question. The goal of this model is to pinpoint the top-k relevant contexts E, given a question Q and a document D. FinQANet utilizes a dense vector spaces approach for quickly and accurately extracting pertinent contexts from large documents, shifting from traditional, less efficient vector space models like TF-IDF and BM25 (Sparck Jones, 1972  ###reference_b23###  ###reference_b23###; Robertson et al., 1995  ###reference_b21###  ###reference_b21###). This approach facilitates more meaningful semantic comparisons, leading to more precise matches between questions and their relevant contexts (Karpukhin et al., 2020  ###reference_b14###  ###reference_b14###). \nThe context retriever leverages a pre-trained BERT model, processing combinations of the question with each context as input to the model. It then fine-tunes this classifier on the FinQA dataset. The top-k relevant contexts identified are subsequently used in the program generation phase.\n###figure_2### The program generator operates on a sequence-to-sequence (seq-to-seq) architecture, which is ideally suited for handling long sequential data through an encoder and decoder structure (Sutskever et al., 2014  ###reference_b24###  ###reference_b24###). Initially, this system examines the given question Q and the contexts E retrieved earlier, with its encoder capturing essential information. Subsequently, the decoder systematically generates the final program P in an autoregressive manner. For the decoding process, FinQANet employs an LSTM model, incorporating an attention mechanism to improve the precision and clarity of its outputs (Bahdanau et al., 2014  ###reference_b4###  ###reference_b4###).\nA key distinction of this program generator from standard seq-to-seq models lies in wider range of decoder outputs. This not only includes the input embeddings but also specific tokens for developing a logical program, such as various operations and step memory tokens. These step memory tokens are instrumental in outlining the program\u2019s logical sequence. An illustrative Figure  1  ###reference_###  ###reference_### details the process of program decoding within the FinQANet\u2019s program generator.\nThe FinQA dataset utilizes a specific programming language designed for executing mathematical and tabular operations. The program sequence includes a variety of operations, such as addition, subtraction, multiplication, division, and others, tailored for table summarization and averaging. The operands for these operations are either directly extracted from the document or derived from preceding steps. For example, to perform a subtraction on the results of two division operations, it could be articulated as follows: Divide(arg1, arg2), Divide(arg1, arg2), Subtract(#0, #1)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Exploring Case-Based Reasoning (CBR)",
            "text": "Our research adopts a case-based reasoning (CBR) methodology, a problem-solving approach that involves a three-step cycle: (i) identifying and retrieving relevant previous cases similar to the current problem, (ii) reusing these cases to solve the problem at hand, and (iii) revising the solution if necessary for better alignment with the problem requirements (Aamodt and Plaza, 1994  ###reference_b1###; Das et al., 2021  ###reference_b8###). Our implementation focuses on the first two steps: retrieval and reuse, which will be elaborated upon in methodology.\nThe quantity of potential cases for each query is equivalent to the dataset\u2019s size. Considering the extensive pool of candidates, it\u2019s essential to identify \"gold cases\" for model training purposes. During the training phase, we leverage those gold cases, which are ideal examples that our case retriever aims to identify. Developing a mechanism for accurately retrieving these cases poses a significant challenge. To define and select gold cases, we first evaluate the similarity between the given question and candidate question.\nWe utilize a BERT encoder to generate representations of the questions (Devlin et al., 2018  ###reference_b9###  ###reference_b9###). Through computing the cosine similarity between these vector representations, we can identify cases that closely match the input question in terms of their content and context. Cosine similarity measures the cosine of the angle between two vectors, indicating how closely related they are in terms of orientation in a multi-dimensional space. It is defined as:\nFollowing the question similarity assessment, we rank candidate cases based on the similarity of their logical solutions or \"programs,\" using program scores. To determine program similarity, we employ the Levenshtein (edit) distance, which quantifies the minimum number of edits (insertions, deletions, substitutions) needed to transform one sequence into another (Levenshtein et al., 1966  ###reference_b16###  ###reference_b16###). This comparison is done at the word level, prioritizing the matching of operations over arguments due to their critical role in logical reasoning. The program score is calculated as follows:\nwhere  and  indicate the number of operations and arguments in the target program, respectively, and  and  denote the edit distances for operations and arguments between the candidate and target programs.\nCandidates with program scores above 0.9 are considered gold cases, while others form the negative set. From analyzing the top 100 candidates based on question similarity, approximately 5.3% of our dataset did not have appropriate positive candidates. However, we expect that our case retrieval model can still effectively learn from instances that do have positive matches. We plan to explore diverse strategies to introduce more variability into our candidate pool for training purposes, which will be elaborated upon in the methodology section. The selection of the 0.9 threshold was aimed at ensuring the high quality of candidate selection."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "The research area of Question Answering (QA) encompasses a wide range of tasks, including text-based, table-based, and numerical reasoning QA. Each of these subdomains contributes unique challenges and insights, laying the groundwork for our approach to addressing financial QA tasks through Case-Based Reasoning (CBR)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Text-Based Question Answering",
            "text": "Text-based QA systems have seen significant advancements, largely due to the development of large-scale datasets such as SQuAD (Stanford Question Answering Dataset) and deep learning models like BERT and its variants. These systems primarily focus on extracting answers from textual data, where the answer to a question is a segment of text found within a provided document. Rajpurkar et al. introduced SQuAD, which catalyzed progress in this area by challenging models to predict answer spans within passages (Rajpurkar et al., 2016  ###reference_b20###). \nFollowing this, Devlin et al. presented BERT, showcasing its ability to understand the context of words in text by pre-training on a large corpus and fine-tuning on specific tasks such as SQuAD (Devlin et al., 2018  ###reference_b9###). This methodology has set a precedent for text-based QA, influencing subsequent research and development in NLP.\n###figure_3###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Table-Based Question Answering",
            "text": "Table-based QA represents a challenging segment of NLP research that demands an intricate understanding of both language and the structured nature of tables. Unlike textual data, tables organize information in a two-dimensional structure, using rows and columns to systematically categorize data. This organization facilitates direct comparison and lookup of numerical values, making tables a unique form of data representation that models must effectively interpret (Dong et al., 2022  ###reference_b11###). \nThe WikiTableQuestions dataset, introduced by Pasupat and Liang, serves as a pivotal benchmark for evaluating the performance of table-based QA systems (Pasupat and Liang, 2015  ###reference_b19###). This dataset highlights the intricate nature of table-based queries, which often require a nuanced understanding of both a table\u2019s layout and its contained information.\nIn response to the complexities associated with table-based QA, there has been significant progress in the development of transformer-based language models tailored to understand the dual nature of tables as both textual and structural entities. These models are designed to comprehend tables by jointly learning their semantic content and structural layout, building upon the foundational text understanding capabilities of traditional NLP models. Among these, TaPas, developed by Herzig et al., stands out as a BERT adaptation pre-trained specifically for table comprehension. TaPas enhances BERT\u2019s capabilities by introducing specialized embeddings to capture row and column information and incorporating additional classification layers for more precise cell and operation selection tasks (Herzig et al., 2020  ###reference_b12###). Another notable contribution is TaBERT by Yin et al., which approaches table encoding by treating tables as sequences analogous to text. This methodology enables TaBERT to produce context-aware cell representations, effectively capturing the relationships between cells and their collective relevance to a query (Yin et al., 2020  ###reference_b27###). Together, these developments underscore the growing importance of dedicated models and datasets in pushing the boundaries of table-based QA research."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "This paper is dedicated to addressing the complexities of financial question answering (QA) through a case-based reasoning (CBR) framework. Central to our methodology are two integral components: a retrieval module for identifying similar cases and a reuse module for applying these cases to solve the problem at hand. The design of our methodological approach is visually represented in Figure 2  ###reference_###, highlighting the process from case retrieval based on specific queries to their application in aiding the program generator. We have implemented distinct methodologies for each segment, as detailed in the subsequent sections.\nIn our case retrieval component, we explore both architectures to balance their benefits and drawbacks. After evaluating both methods, we will select or combine them based on their performance and suitability for our model. The initial broad filtering capability of the bi-encoder, followed by the detailed scrutiny provided by the cross-encoder, could potentially offer a synergistic approach, leveraging the strengths of both models. This dual-stage process is anticipated to optimize case retrieval for enhancing the program generation phase, especially during testing, by preliminarily narrowing down relevant cases with the bi-encoder and then applying the cross-encoder for finer selection."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Retrieving Cases",
            "text": "The number of candidate cases for each question corresponds to the size of the dataset. Given the vast number of potential cases, our objective is to filter out only the most relevant cases that will significantly aid the program generation process. To achieve this, we have devised a retrieval model capable of selecting cases with a high degree of relevance to the posed question. Throughout the training phase, we target \u2019gold cases\u2019\u2014cases that are predefined as highly relevant based on their operational similarity to the query, as mentioned in the preliminary section, with a program score above 0.9 serving as the benchmark for relevance. These gold cases form our positive set, with the remainder being classified as negative. To discriminate between these sets, we employ a classifier trained for this specific purpose, utilizing either Bi-encoders or Cross-encoders for sequence comparison, which are detailed further below.\nIn our case retrieval component, we explore both architectures to balance their benefits and drawbacks. After evaluating both methods, we will select or combine them based on their performance and suitability for our model. The initial broad filtering capability of the bi-encoder, followed by the detailed scrutiny provided by the cross-encoder, could potentially offer a synergistic approach, leveraging the strengths of both models. This dual-stage process is anticipated to optimize case retrieval for enhancing the program generation phase, especially during testing, by preliminarily narrowing down relevant cases with the bi-encoder and then applying the cross-encoder for finer selection."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Bi-Encoder Architecture",
            "text": "The bi-encoder architecture operates by independently encoding the query and potential cases into dense vector representations, as depicted in Figure 3  ###reference_###. This approach utilizes separate instances of pre-trained models such as BERT or RoBERTa for encoding, ensuring that each text input, whether a query or a case, is transformed into a high-dimensional space where semantic similarities can be quantitatively assessed. Renowned for its efficiency, this approach has demonstrated commendable performance in QA tasks (Mazar\u00e9 et al., 2018  ###reference_b18###; Dinan et al., 2018  ###reference_b10###; Karpukhin et al., 2020  ###reference_b14###). \nThe strength of the bi-encoder lies in its efficiency; by pre-computing and caching the embeddings of the potential cases, the system can rapidly compare these embeddings with those of incoming queries. This characteristic makes it especially useful for rapid retrieval in extensive datasets. However, it\u2019s important to note that while bi-encoders excel in speed, they may not capture the nuanced interplay between queries and cases as effectively as cross-encoders due to their separate encoding processes (Humeau et al., 2019  ###reference_b13###).\n###figure_4###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Cross-Encoder Architecture",
            "text": "In contrast to the bi-encoder, the cross-encoder architecture simultaneously encodes the query and each potential case by concatenating them into a single input sequence for processing by models like BERT or RoBERTa. The integrated encoding process allows the model to perform self-attention over the entire input, enabling it to capture subtle semantic relationships and contextual nuances between the query and the case (Wolf et al., 2019  ###reference_b26###; Vig and Ramea, 2019  ###reference_b25###). \nWhile cross-encoders demonstrate superior accuracy and a deeper understanding of the query-case relationship due to their comprehensive encoding strategy, they are computationally more intensive (Humeau et al., 2019  ###reference_b13###). Encoding each query-case pair individually results in slower retrieval times compared to bi-encoders. Nonetheless, the enhanced accuracy and contextual sensitivity of cross-encoders make them invaluable for tasks where precision is paramount, and the complexity of the financial domain often necessitates such an approach.\n###figure_5### In our case retrieval component, we explore both architectures to balance their benefits and drawbacks. After evaluating both methods, we will select or combine them based on their performance and suitability for our model. The initial broad filtering capability of the bi-encoder, followed by the detailed scrutiny provided by the cross-encoder, could potentially offer a synergistic approach, leveraging the strengths of both models. This dual-stage process is anticipated to optimize case retrieval for enhancing the program generation phase, especially during testing, by preliminarily narrowing down relevant cases with the bi-encoder and then applying the cross-encoder for finer selection."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Reusing Retrieved Cases in Program Generation",
            "text": "After successfully retrieving relevant cases through our case retrieval module, the next pivotal step in our methodology involves effectively incorporating these cases into the program generation process. To achieve this, we draw inspiration from the program generator model used in the FinQA baseline but introduce novel adaptations by employing two distinct methods to integrate the retrieved cases: the Concatenation approach and the Separate Encoder approach. Both methods are grounded in well-established practices within the field of NLP and have shown promise in enhancing model performance by providing additional relevant context."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Concatenation Approach",
            "text": "The Concatenation approach is straightforward yet effective; it involves appending the retrieved cases directly to the retrieved contexts using a special separation token. This aggregated input is then processed through a pre-trained encoder, such as BERT or RoBERTa, to produce a comprehensive embedding that captures both the contextual nuances of the query and the supportive information from the cases. Following the encoding, an LSTM model takes over to sequentially generate the program tokens. This method leverages the powerful contextual embedding capabilities of pre-trained models to enhance the relevance and accuracy of the generated programs by ensuring that the additional case information is seamlessly integrated into the input sequence. The simplicity of this approach, combined with the proven effectiveness of encoder models like BERT and RoBERTa in handling concatenated inputs (Wolf et al., 2019  ###reference_b26###), makes it a viable strategy for enriching the program generation process."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Separate Encoder Approach",
            "text": "In contrast, the Separate Encoder approach utilizes an additional pre-trained encoder, such as BERT or RoBERTa, dedicated exclusively to encoding the retrieved cases. This method allows for a more isolated analysis of the cases, ensuring that their content is thoroughly understood and represented before being considered in the program generation phase. The LSTM model, responsible for producing the program tokens, incorporates a separate attention mechanism directed towards the encoder output for the cases. This targeted attention enables the program generator to dynamically weigh the relevance of the case information at each step of the token generation process, potentially leading to more nuanced and accurate program outputs. By maintaining a distinct processing stream for the cases, this approach aims to maximize the utility of the retrieved information, ensuring that each case contributes meaningfully to the final program generated. \nEmploying these approaches within the context of financial QA tasks, especially when integrated with a CBR framework, signifies a novel adjustment designed to capitalize on insights from specific cases to refine the process of generating programs. By exploring different techniques for case retrieval and program generation, our objective is to identify and apply the optimal approach for integrating detailed case information into the program creation process. This will, in turn, enhance the model\u2019s capability in addressing complex financial questions."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "This section outlines the experimental framework designed to evaluate the effectiveness of our case-based reasoning (CBR) system in the area of financial question answering (QA). The primary goal of this paper is to enhance the quality of program generation in financial QA tasks. As depicted in Table 1  ###reference_###, our experiments demonstrate a significant improvement in the program generator\u2019s performance when it is fed with gold cases. We hypothesize that the program generator will produce better outcomes as the case retriever\u2019s efficiency improves and supplies higher quality cases to the generator.\nWe explored two separate architectures: the Bi-encoder and the Cross-encoder, using BERT-base, RoBERTa-base, and RoBERTa-large as pre-trained models for both architectures. Initial findings suggest that the Cross-encoder marginally surpasses the Bi-encoder in performance, with larger models demonstrating improved results. This emphasizes the significance of both model size and architectural design in attaining superior precision in retrieval tasks.\nExperiments were conducted using various input types, including questions only, programs only, and a combination of both. The findings reveal that combining questions and programs as inputs significantly improves retrieval performance. This combination approach allows for a more nuanced understanding and representation of cases, leading to more precise retrievals.\nIt is crucial to optimize the training set to train our case retriever model, as training on 6,250 question-candidate pairs for each of the 6,251 questions would be excessively time-consuming. We experimented with different strategies for selecting positive and negative training sets, initially focusing on question similarity and later incorporating sampling methods to introduce variability and balance.\nFine-tuning hyperparameters such as learning rates, optimizers, batch sizes, and the ratio of positive to negative candidates was necessary for enhancing model performance. This process involved iterative testing and adjustments to identify the optimal settings for our specific task.\nIn line with the methodology section, we investigate both Concatenation and Separate Encoder methods for incorporating retrieved cases into the program generation process. The initial findings presented in Table 1  ###reference_### indicate that the Concatenation approach outperforms the Separate Encoder method significantly. This suggests that the model is more effective when using a unified embedding that combines the query\u2019s contextual details with supportive case information. To further understand their effectiveness, we will evaluate each strategy using various pre-trained encoder models, including RoBERTa-base and RoBERTa-large. Our goal through this examination is to identify the architecture and model pairing that best facilitates the creation of precise and logically coherent programs.\nThe choice of input cases plays a complex role in how well the model works. We plan to experiment with different input configurations to understand their impact comprehensively. This includes deciding between using gold cases versus retrieved (noisy) cases in training phase, which might introduce variability. Additionally, we will test whether incorporating both the question and program from cases, or just the program, affects the program generator\u2019s effectiveness. The number of cases used as input and the choice between including only operators or the entire program in the input will also be examined. These experiments will shed light on the optimal way to leverage case information for improving program generation.\nThrough these experimental investigations, we aim to refine our understanding of how best to harness the potential of CBR in enhancing the capabilities of program generators for financial QA tasks. By meticulously examining the effects of architectural choices, encoder models, and input case configurations, we anticipate identifying strategies that significantly improve the generation of logical and accurate programs."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Dataset Overview",
            "text": "For our experiments, we utilize the FinQA dataset, which consists of 8,281 pairs of financial questions and answers, crafted by financial experts. The dataset is divided into training (6,251 pairs), validation (883 pairs), and testing (1,147 pairs) sets, in a 75%/ 10%/ 15% split, respectively. Analysis shows that 23.42% of questions can be answered using text only, 62.43% require information from tables, and 14.15% need both text and table data for answers. In terms of reasoning complexity, 59.10% of the programs involve a single reasoning step, 32.71% require two steps, and 8.19% necessitate three or more steps. \nIn the preliminary section, we\u2019ve defined \u2019gold cases\u2019 as those with a program score higher than 0.9. When analyzing the top 100 candidates from the dataset based on question similarity, we find that 5.3% of the training data lacks gold cases for its questions, indicating some questions do not have a matching candidate case with similar questions and programs. 20.6% of the training set contains fewer than 10 gold cases, while the remaining 74.0% have more than 10 gold cases. When considering the entire training set (6,250 pairs), only 0.8% of questions have no gold cases."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Experiment on Case Retriever",
            "text": "The case retriever is crucial for finding the most relevant, or \u2019gold\u2019, cases to support the program generator. We use precision as the key metric to evaluate the case retriever, which assesses how accurately it identifies relevant cases from the dataset in its top-k retrieved cases. Precision is the fraction of retrieved gold cases among the top-k retrieved cases, serving as a direct measure of the retriever\u2019s performance.\nWe explored two separate architectures: the Bi-encoder and the Cross-encoder, using BERT-base, RoBERTa-base, and RoBERTa-large as pre-trained models for both architectures. Initial findings suggest that the Cross-encoder marginally surpasses the Bi-encoder in performance, with larger models demonstrating improved results. This emphasizes the significance of both model size and architectural design in attaining superior precision in retrieval tasks.\nExperiments were conducted using various input types, including questions only, programs only, and a combination of both. The findings reveal that combining questions and programs as inputs significantly improves retrieval performance. This combination approach allows for a more nuanced understanding and representation of cases, leading to more precise retrievals.\nIt is crucial to optimize the training set to train our case retriever model, as training on 6,250 question-candidate pairs for each of the 6,251 questions would be excessively time-consuming. We experimented with different strategies for selecting positive and negative training sets, initially focusing on question similarity and later incorporating sampling methods to introduce variability and balance.\nFine-tuning hyperparameters such as learning rates, optimizers, batch sizes, and the ratio of positive to negative candidates was necessary for enhancing model performance. This process involved iterative testing and adjustments to identify the optimal settings for our specific task."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Influential Factors on Case Retriever Performance",
            "text": "The performance of the case retriever is subject to various factors:\nWe explored two separate architectures: the Bi-encoder and the Cross-encoder, using BERT-base, RoBERTa-base, and RoBERTa-large as pre-trained models for both architectures. Initial findings suggest that the Cross-encoder marginally surpasses the Bi-encoder in performance, with larger models demonstrating improved results. This emphasizes the significance of both model size and architectural design in attaining superior precision in retrieval tasks.\nExperiments were conducted using various input types, including questions only, programs only, and a combination of both. The findings reveal that combining questions and programs as inputs significantly improves retrieval performance. This combination approach allows for a more nuanced understanding and representation of cases, leading to more precise retrievals.\nIt is crucial to optimize the training set to train our case retriever model, as training on 6,250 question-candidate pairs for each of the 6,251 questions would be excessively time-consuming. We experimented with different strategies for selecting positive and negative training sets, initially focusing on question similarity and later incorporating sampling methods to introduce variability and balance.\nFine-tuning hyperparameters such as learning rates, optimizers, batch sizes, and the ratio of positive to negative candidates was necessary for enhancing model performance. This process involved iterative testing and adjustments to identify the optimal settings for our specific task."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Dual-Stage Architecture Exploration",
            "text": "Further experimentation will focus on the dual-stage architecture integrating both Bi-encoder and Cross-encoder approaches, as discussed in the methodology section. The idea is to refine the number of cases the bi-encoder retrieves, allowing the cross-encoder to focus on a reduced set of candidates during testing. This approach is expected to fine-tune the dual-stage process, leveraging the strengths of both architectures to optimize performance during test time."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Experiment on Program Generator",
            "text": "The main function of the program generator is to create logical steps and corresponding programs that efficiently address financial questions. Through the use of CBR, we anticipate that it will utilize the retrieved cases to enhance the accuracy and pertinence of the answers produced. We aim for the program generator to exhibit improved reasoning skills and to produce precise, logical programs that closely mirror solutions provided by human experts.\nOriginal FinQA research utilizes two main metrics for evaluation: program accuracy and execution accuracy. Program accuracy assesses whether two symbolic programs are mathematically equivalent, focusing on the logical structure of the generated program compared to a gold standard. Execution accuracy measures the correctness of the final answers produced by executing the generated programs. While execution accuracy can sometimes overestimate performance due to coincidentally correct answers, program accuracy might underreport effectiveness because of its inability to recognize multiple valid solutions to the same problem. \nTo address these issues and specifically gauge the impact of case-based reasoning (CBR) on logical step generation, we introduce Operator accuracy as an additional metric. Operator accuracy examines the precision in generating correct operators within programs, given the case input, providing insight into the CBR\u2019s contribution to enhancing logical reasoning in program generation.\nIn line with the methodology section, we investigate both Concatenation and Separate Encoder methods for incorporating retrieved cases into the program generation process. The initial findings presented in Table 1  ###reference_###  ###reference_### indicate that the Concatenation approach outperforms the Separate Encoder method significantly. This suggests that the model is more effective when using a unified embedding that combines the query\u2019s contextual details with supportive case information. To further understand their effectiveness, we will evaluate each strategy using various pre-trained encoder models, including RoBERTa-base and RoBERTa-large. Our goal through this examination is to identify the architecture and model pairing that best facilitates the creation of precise and logically coherent programs.\nThe choice of input cases plays a complex role in how well the model works. We plan to experiment with different input configurations to understand their impact comprehensively. This includes deciding between using gold cases versus retrieved (noisy) cases in training phase, which might introduce variability. Additionally, we will test whether incorporating both the question and program from cases, or just the program, affects the program generator\u2019s effectiveness. The number of cases used as input and the choice between including only operators or the entire program in the input will also be examined. These experiments will shed light on the optimal way to leverage case information for improving program generation.\nThrough these experimental investigations, we aim to refine our understanding of how best to harness the potential of CBR in enhancing the capabilities of program generators for financial QA tasks. By meticulously examining the effects of architectural choices, encoder models, and input case configurations, we anticipate identifying strategies that significantly improve the generation of logical and accurate programs."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Influential Factors on Program Generator Performance",
            "text": "Several key factors impact the accuracy of the program generator:\nIn line with the methodology section, we investigate both Concatenation and Separate Encoder methods for incorporating retrieved cases into the program generation process. The initial findings presented in Table 1  ###reference_###  ###reference_###  ###reference_### indicate that the Concatenation approach outperforms the Separate Encoder method significantly. This suggests that the model is more effective when using a unified embedding that combines the query\u2019s contextual details with supportive case information. To further understand their effectiveness, we will evaluate each strategy using various pre-trained encoder models, including RoBERTa-base and RoBERTa-large. Our goal through this examination is to identify the architecture and model pairing that best facilitates the creation of precise and logically coherent programs.\nThe choice of input cases plays a complex role in how well the model works. We plan to experiment with different input configurations to understand their impact comprehensively. This includes deciding between using gold cases versus retrieved (noisy) cases in training phase, which might introduce variability. Additionally, we will test whether incorporating both the question and program from cases, or just the program, affects the program generator\u2019s effectiveness. The number of cases used as input and the choice between including only operators or the entire program in the input will also be examined. These experiments will shed light on the optimal way to leverage case information for improving program generation.\nThrough these experimental investigations, we aim to refine our understanding of how best to harness the potential of CBR in enhancing the capabilities of program generators for financial QA tasks. By meticulously examining the effects of architectural choices, encoder models, and input case configurations, we anticipate identifying strategies that significantly improve the generation of logical and accurate programs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our research has primarily focused on augmenting the program generator with additional, contextually similar cases to address the challenge of generating incorrect operations, a notable issue in processing complex, real-world documents containing text, tables, and numerical data.\nThe significant contribution of our study lies in providing the program generator with extra information through similar cases. This methodology is based on the premise that a deeper understanding of multi-modal information can substantially benefit not just the financial domain but also various fields where data is presented in mixed formats. The initial results, particularly the program generator\u2019s improved performance with gold cases, underscore the potential of our approach to refine the model\u2019s reasoning capabilities and align its outputs more closely with expert-level solutions.\nHowever, our experiments also highlight a critical challenge: enhancing the case retriever\u2019s performance. The effectiveness of our proposed method is directly tied to the quality of cases retrieved. As such, refining the case retriever not only stands as our main challenge but also as the primary area for future exploration. The forthcoming stages of this research will aim to quantify the impact of the case retriever\u2019s performance on the overall efficacy of the program generator. By focusing on this aspect, we expect to gain valuable insights into optimizing the case retrieval process, thereby maximizing the potential benefits of our CBR approach.\nIn conclusion, this thesis proposes a novel method of leveraging similar cases to improve the program generator\u2019s accuracy in financial QA tasks, with the potential to extend these benefits across various domains dealing with multi-modal data. The dependency of the program generator\u2019s success on the case retriever\u2019s performance highlights an essential area for further research. As we continue to refine our approach, we aim to not only address the current limitations but also to pave the way for advancements in machine understanding of complex documents, thereby contributing to the broader field of artificial intelligence."
        }
    ],
    "url": "http://arxiv.org/html/2405.13044v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.2.1",
            "5.2.2",
            "5.3",
            "5.3.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2",
            "5",
            "5.2",
            "5.2.1",
            "5.2.2",
            "5.3",
            "5.3.1"
        ]
    },
    "research_context": {
        "paper_id": "2405.13044v1",
        "paper_title": "Case-Based Reasoning Approach for Solving Financial Question Answering (March.29, 2024)",
        "research_background": "### Paper Motivation\n\nThe paper is motivated by the current limitations in financial question answering (QA) systems, specifically those dealing with numerical reasoning tasks that incorporate both text and tabular data. Despite significant progress in text-based QA through large language models, table-based QA tasks have not seen similar success. This gap is mainly due to the complexities involved in processing and integrating multiple types of information (text, tables, and numerical data) in real-world documents. The problem is further exacerbated by the models\u2019 tendency to produce incorrect operations during reasoning steps, leading to a notable discrepancy between program accuracy and execution accuracy.\n\n### Research Problem\n\nThe primary research problem addressed in the paper is to enhance the effectiveness of QA systems in numerical reasoning tasks involving multi-modal data, with a particular focus on financial documents. The current baseline models, even when using advanced pre-trained language models, fall short in accurately handling complex reasoning steps required for such tasks. The paper aims to bridge this gap by proposing a novel case-based reasoning (CBR) approach, which involves using solutions from similar past problems to improve the model\u2019s ability to generate correct operations and, consequently, correct answers.\n\n### Relevant Prior Work\n\n1. **Historical Context of QA**: The QA research has evolved substantially since the 1960s (Androutsopoulos et al., 1995).\n\n2. **Advancements with Deep Learning**: Recent years have seen a rise in large language models that significantly improve text-based QA tasks (Liu et al., 2019; Lan et al., 2019; Clark et al., 2020).\n\n3. **Challenges in Table-Based QA**: Despite successes in text-based applications, table-based QA tasks have underperformed (Herzig et al., 2020; Yin et al., 2020).\n\n4. **Development of New Datasets**: Efforts such as the creation of the FinQA dataset by Chen et al. (2021) emphasize numerical reasoning in financial documents, highlighting problems not addressed by current models.\n\n5. **Challenges with Large Language Models**: Even advanced models like GPT-3.5 face difficulties in handling tasks requiring complex reasoning steps, failing to retrieve relevant table cells effectively.\n\n6. **Evaluation Metrics**: The paper distinguishes between program accuracy and execution accuracy, emphasizing the importance of the former for assessing a machine\u2019s reasoning ability.\n\nBy addressing these prior discoveries and limitations, the paper proposes a novel CBR-based approach to improve the integration and processing of multi-modal data in financial QA, focusing on enhancing the coherence between generated programs and accurate answers.",
        "methodology": "The proposed method in the paper 'Case-Based Reasoning Approach for Solving Financial Question Answering' employs a structured case-based reasoning (CBR) framework, focusing on solving financial question answering tasks through a methodical process involving two integral components: a retrieval module and a reuse module. **Methodology Outline:** 1. **Case Retrieval Component:** - **Architectures Employed:** The paper investigates two different architectures within the case retrieval component: a bi-encoder and a cross-encoder. - **Bi-Encoder:** Utilized for broad filtering capabilities, the bi-encoder swiftly narrows down the pool of relevant cases. - **Cross-Encoder:** Following the bi-encoder, the cross-encoder performs a detailed analysis for finer selection of cases. - **Performance Evaluation:** The final model chosen will be based on their respective performances and suitability for the specific requirements of the financial QA tasks. 2. **Reuse Component:** - **Application to Problem Solving:** Once relevant cases are identified through the retrieval component, they are applied in a systematic manner to solve the target financial questions. - **Aiding Program Generator:** This application process assists the program generator in creating accurate and contextually appropriate responses based on the retrieved cases. **Innovations and Benefits:** - **Synergistic Architecture Use:** The innovative aspect of this methodology lies in the potential combination of bi-encoder and cross-encoder architectures to leverage their collective strengths for optimal case retrieval. - **Enhanced Program Generation:** By using a two-stage retrieval process, the model aims to improve the efficiency and accuracy of the program generation phase, particularly during testing. In summary, the paper presents a novel approach employing a dual-stage case retrieval mechanism within a CBR framework to improve financial question answering, balancing broad filtering and fine selection of relevant cases for effective problem solving.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n1. **Objective:**\n   - To evaluate the effectiveness of a case-based reasoning (CBR) system in the financial question answering (QA) domain, with the primary goal of improving program generation quality.\n\n2. **Architectures Explored:**\n   - **Bi-encoder and Cross-encoder:** \n     - Models: BERT-base, RoBERTa-base, and RoBERTa-large.\n     - Key Observation: The Cross-encoder generally outperforms the Bi-encoder, and performance improves with the larger models.\n\n3. **Input Types:**\n   - Variations:\n     - Questions only.\n     - Programs only.\n     - Combination of both questions and programs.\n   - Key Finding: Combining both questions and programs as inputs significantly enhances retrieval performance, offering a nuanced understanding and representation of cases.\n\n4. **Training Set Optimization:**\n   - Handled the practical challenge of training on 6,250 question-candidate pairs per question by experimenting with strategies for selecting positive and negative training sets, focusing initially on question similarity and incorporating sampling methods for variability.\n\n5. **Hyperparameter Tuning:**\n   - Extensively fine-tuned learning rates, optimizers, batch sizes, and the ratio of positive to negative candidates through iterative testing to enhance model performance.\n\n6. **Encoding Methods:**\n   - Compared **Concatenation** and **Separate Encoder** methods for processing retrieved cases in the program generation phase.\n   - Key Finding: The Concatenation method outperformed the Separate Encoder method, suggesting that a unified embedding combining the query\u2019s contextual information with supporting case details is more effective.\n\n7. **Input Case Variability:**\n   - Planned Experiments:\n     - Use of gold cases vs. retrieved (noisy) cases in the training phase.\n     - Effects of including both question and program from cases or just the program.\n     - Impact of the number of input cases.\n     - Choice between using only operators or the entire program in the input.\n\n**Main Experimental Results:**\n\n- **Performance Metrics:**\n  - The study focuses on enhancing precision in retrieval tasks and improving program generation performance through various architecture and model pairings.\n  - Indicators of success: Better program generator outcomes correlate with the increased efficiency and quality of the case retriever.\n\n- **Key Insights:**\n  - Larger models and the Cross-encoder architecture show superior performance.\n  - The combination of questions and programs as inputs to the retriever leads to significantly improved results.\n  - The Concatenation approach in encoding retrieved cases yields better program generation quality compared to Separate Encoder method.\n\nThrough thorough experimental investigations, significant strategies were identified, pointing towards optimal architectural choices, encoder models, and input configurations that enhance the capabilities of program generators in financial QA tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "This study aims to enhance the program generation quality in financial question answering (QA) by employing a case-based reasoning (CBR) framework.",
            "experiment_process": "The experiment evaluates the impact of providing gold cases to the program generator. It involves experimenting with Bi-encoder and Cross-encoder architectures using BERT-base, RoBERTa-base, and RoBERTa-large models. Various input types such as questions only, programs only, and a combination of both are tested. The performance is assessed based on metrics like program accuracy, execution accuracy, and operator accuracy. Two approaches for incorporating retrieved cases into the program generation are analyzed: Concatenation and Separate Encoder.",
            "result_discussion": "The findings reveal that providing gold cases significantly improves the program generator's performance. Initial experiments indicate that the Cross-encoder slightly outperforms the Bi-encoder, and larger models yield better results. Combining questions and programs as inputs enhances retrieval performance, and the Concatenation approach shows better results than the Separate Encoder method. These outcomes underscore the importance of architectural choices and input configurations in optimizing the performance of program generators.",
            "ablation_id": "2405.13044v1.No1"
        },
        {
            "research_objective": "To determine the effectiveness of different factors that impact the case retriever's performance in financial QA tasks.",
            "experiment_process": "The performance of the case retriever is evaluated using precision as the key metric. Two architectures, Bi-encoder and Cross-encoder, are explored with models like BERT-base, RoBERTa-base, and RoBERTa-large. Experiments include various input types such as questions only, programs only, and a combination of both. Strategies for selecting positive and negative training sets are tested, alongside fine-tuning hyperparameters like learning rates, optimizers, and batch sizes.",
            "result_discussion": "Initial findings show that the Cross-encoder marginally surpasses the Bi-encoder, with larger models demonstrating better results. Combining questions and programs as inputs significantly improves retrieval performance by providing a nuanced understanding of cases. The experiments help identify optimal settings for training the case retriever model effectively.",
            "ablation_id": "2405.13044v1.No2"
        },
        {
            "research_objective": "To explore the influence of using retrieved (noisy) cases versus gold cases on the program generator's effectiveness in financial QA.",
            "experiment_process": "The study tests various input configurations, deciding between using gold cases and retrieved (noisy) cases in the training phase. It also examines the impact of incorporating both the question and program from cases or just the program. Different numbers of cases used as input and the inclusion of only operators or entire programs are analyzed.",
            "result_discussion": "The experimental results demonstrate that using gold cases generally yields better performance compared to noisy cases. Incorporating both questions and programs from cases enhances the program generator\u2019s effectiveness. These experiments provide insights into the optimal ways to leverage case information for improving program generation.",
            "ablation_id": "2405.13044v1.No3"
        }
    ]
}