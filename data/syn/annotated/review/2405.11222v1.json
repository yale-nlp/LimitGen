{
    "title": "ISDS-NLP at SemEval-2024 Task 10: Transformer based neural networks for emotion recognition in conversations",
    "abstract": "This paper outlines the approach of the ISDS-NLP team in the SemEval 2024 Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF). For Subtask 1 we obtained a weighted F1 score of 0.43 and placed 12 in the leaderboard. We investigate two distinct approaches: Masked Language Modeling (MLM) and Causal Language Modeling (CLM). For MLM, we employ pre-trained BERT-like models in a multilingual setting, fine-tuning them with a classifier to predict emotions. Experiments with varying input lengths, classifier architectures, and fine-tuning strategies demonstrate the effectiveness of this approach. Additionally, we utilize Mistral 7B Instruct V0.2, a state-of-the-art model, applying zero-shot and few-shot prompting techniques. Our findings indicate that while Mistral shows promise, MLMs currently outperform them in sentence-level emotion classification.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Task 10 from SemEval 2024 competition Kumar et al. (2024  ###reference_b9###) addresses the complex challenge of identifying the emotions within dialogues (English and Hindi). This task comprises two primary objectives: firstly, assigning an emotion label to each utterance within a dialogue, and secondly, discerning the trigger utterance or utterances responsible for an emotion-flip within the dialogue Kumar et al. (2022  ###reference_b11###). Emotions play a crucial role in human interaction and one can understand more from a text if one knows the underlying sentiment of the writer. In contexts where disagreements may arise, such as customer service platforms, virtual assistant chats or forums, identifying trigger utterances for emotion flips can help mediate conflicts and prevent escalation. A chatbot dealing with an angry customer would benefit from knowing how to speak in order to generate empathetic responses. If it knows that the chatbot\u2019s current sentence can trigger an emotion flip from neutral to anger, the chatbot should refine it, or if the emotion flip is from anger to joy, the chatbot should be more confident in such a response in the future.\nBoth types of models we tried for Subtask 1 were based on transformers. The first one used BERT-like models and we achieved the best accuracy with them, while the second one is a state of the art causal model (Mistral, Jiang et al. (2023  ###reference_b6###)) that was tested in zero-shot and few-shot settings with poorer results.\nAlthough in the first task our system worked well, placing 12th in the leaderboard, the other 2 tasks were much harder and we placed 14th on the second subtask. We believed that with a better strategy to prevent overfitting (like under or over-sampling), our system would have improved. Our code is open source and available to use on GitHub  ###reference_2024-task-10###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "The competition had 3 subtasks explained in Figure 1  ###reference_### and we participated in all of them with the best results on subtask 1 where we placed 12th with an F1 score of 0.43.\n###figure_1###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset",
            "text": "The dataset contains English and Hindi code-mixed conversations for Subtask 1 and 2 and English only conversations for Subtask 3 (Table 1  ###reference_###). The dataset is quite small, except for the training dataset for Subtask 2 and 3. If we were to combined them for Subtask 1, our F1 score would reach 0.97, but it wasn\u2019t allowed. This fact shows that with more data our model would do really well. The dataset is based on MELD, a known emotion recognition dataset, which was then augmented with triggers for the emotion-flip task.\nThere were 8 distinct emotions to predict: neutral, anger, surprise, fear, joy, sadness, disgust, and contempt. By far the most predominant emotion is neutral, followed by joy and anger (Figure 2  ###reference_###). If we look at Subtask 2, most often the emotion flips are from neutral to joy or anger (Figure 3  ###reference_###).\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "System overview",
            "text": "We tried two approaches, both of them based on transformer architecture: Masked Language Modelling and Casual Modelling. We chose these two architectures because of their recent successes in NLP."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Masked Language Modelling",
            "text": "We used pre-trained BERT-like models in a multilingual setting so that it can tokenise Hindi sentences. These pre-trained models will give us the features from sentences and then we pass them through a classifier which will do the prediction for each task Figure 4  ###reference_###.\n###figure_4###"
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Input",
            "text": "Analysis of dialogue sentences reveals a predominantly short length, with a sharp decline in frequency after 30 tokens (see Figure 5  ###reference_###). To optimize performance, various maximum sequence lengths were tested, with 55 tokens yielding the best results (Figure 6  ###reference_###). Data preprocessing, (such as lemmatization, removing punctuation or stopwords) didn\u2019t help the model learn better so we kept the input as is. Probably this is because punctuation and stopwords contain useful information that the models is able to learn.\n###figure_5### ###figure_6###"
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Output",
            "text": "Selecting the optimal hidden state layer is crucial for leveraging the pre-trained model\u2019s results. Our experiments demonstrated that using the final layer\u2019s output yielded the strongest performance, with accuracy declining in earlier layers. For MLM-type models, the [CLS] token encodes the features, which is what we pass to our classification layer.\nAmong various classifiers tested (Table 2  ###reference_###), fully connected layers excelled, likely due to their ability to model complex, non-linear relationships. The top-performing model employed a fully connected layer with 0.5 dropout and a Softmax activation function."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Fine-tuning",
            "text": "The large pre-trained language models we employed offer a robust foundation for understanding language in general. Through fine-tuning, we adapt them to the nuances of our emotion recognition task. Inspired by the strategy presented in Sun et al. (2020  ###reference_b12###), we initially train only the classifier with a larger learning rate (5e-5) and a warm-up period of 10,000 steps over \u2019k\u2019 epochs (we tried a range of \u2019k\u2019 from 1 to 10). Subsequently, we fine-tune both the classifier and the transformer\u2019s final layer using a smaller learning rate (2e-5). Our goal in freezing the transformer weights at first, and then training them with a reduced learning rate, is to minimize the risk of overfitting."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Causal Modelling",
            "text": "Given the success of generative models we also tried Mistral 7B Instruct V0.2 which is believed to be state of the art in its category of models Jiang et al. (2023  ###reference_b6###). These type of LLMs have had success in a large number of NLP tasks, but seem to still lag Masked Language Models in sentence classification."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Prompting",
            "text": "In Causal Modelling, how you prompt the model significantly influences its performance. We tested different prompting strategies in both zero-shot and few-shot settings:\nZero-Shot Learning: Here, we provide the model with a single example and ask it to predict the emotion without any additional references. For zero-shot learning the best prompting technique was: \"[INS] Given the following sentence: {sentence}. ### Predict which emotion is expressed. Chose one of the following options: neutral, anger, surprise, fear, joy, sadness, disgust, and contempt. Answer in one word only. ### Answer: [\\INS]\"\nFew-Shot Learning: In this setting, we give the model several examples \u2013 one for each emotion \u2013 along with their corresponding labels. This leverages the model\u2019s in-context learning ability, potentially boosting its performance for unseen samples. For few-shot learning the best prompting technique was: \"[INS] This is an example of a sad sentence: {sentence} {repeat for every emotion}. ### Predict the emotion of the following sentence: sentence. Chose one of the following options: neutral, anger, surprise, fear, joy, sadness, disgust, and contempt. Answer in one word only. ### Answer: [\\INS]\""
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental setup",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data Split Strategy",
            "text": "We employed a classic data split approach:\nInitial Development: We combined the training and development sets and shuffled the data. Subsequently, we used 70% for training, 10% for validation, and the remaining 20% as a held-out test set.\nCompetition Test Set Release: Upon the competition\u2019s test set release, we directly evaluated our models using the platform. To maximize training data, we trained on the combined training set with a 20% validation split.\nFinal Model: Once we selected our best model, we re-trained it on the entire dataset without validation. This re-training didn\u2019t yield significant improvements"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Subtask 1",
            "text": "We\u2019ll focus on Subtask 1, where we achieved strong results. The key hyperparameters used:\nBatch Size: A batch size of 64 provided the best balance. Smaller sizes hurt performance, while larger sizes exceeded our memory constraints.\nFine-Tuning: We trained for 4 epochs with frozen model weights, followed by 3 epochs with only the last layer unfrozen (as detailed in section 3.1.3  ###reference_.SSS3###).\nClassifier: Our classifier used 128 neurons, 0.5 dropout, and a softmax activation.\nOptimization: We used cross-entropy loss, the AdamW optimizer, and experimented with different learning rates (see section 3.1.3  ###reference_.SSS3###).\nEvaluation: We measured performance using the MulticlassF1Score with 8 classes and \u2019macro\u2019 averaging."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "Our top-performing model (Table 3  ###reference_###) was a fine-tuned FacebookAI/xlm-roberta-large Conneau et al. (2019  ###reference_b2###). This highlights the superiority of fine-tuned Masked Language Models (MLMs) over Mistral for sentence classification tasks. The results suggest that smaller Causal models remain less effective than fine-tuned MLMs in this domain. We also see that few-shot Mistral is worse than zero-shot, probably because too much data in the prompt confuses the model.\nIn terms of number of epochs, our best model was overfitting when finetuned for too many epochs (Table 4  ###reference_###) and we finally trained for 4 + 3 epochs."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Error analysis",
            "text": "Our confusion matrix (Figure 7  ###reference_###) reveals that the model overpredicts the \u2019neutral\u2019 emotion, likely due to its prevalence in the training data. This created a bias, leading the model to misclassify instances of other emotions as \u2019neutral\u2019. While we attempted to mitigate this with class weights in the loss function, it proves insufficient. In the future, we should explore more robust techniques like oversampling or undersampling to address the class imbalance.\n###figure_7### As seen in the emotion accuracy chart (Figure 8  ###reference_###), the model performs best on the dominant \u2019neutral\u2019 class, along with well-represented emotions like \u2019joy\u2019 and \u2019sadness\u2019. Conversely, the model struggles to predict the \u2019disgust\u2019 emotion, which aligns with its under-representation in the training data. This suggests a direct correlation between dataset frequency and model proficiency for each emotion.\n###figure_8###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Overall, our system achieved encouraging results in Subtask 1, despite exhibiting some overfitting for dominant labels. While performance on the emotion-flip detection tasks (Subtasks 2 and 3) highlights areas for improvement, we still placed in the first half of the leaderboard. Looking ahead, we plan to investigate hybrid transformer-LSTM architectures for a more nuanced understanding of emotion-flip triggers. Additionally, enriching the data by incorporating a broader conversational context through multi-turn analysis could enhance our model\u2019s capabilities. Not least, even though we tried Mistral, there are newer causal models like Mixtral Jiang et al. (2024  ###reference_b7###) and Solar Kim et al. (2023  ###reference_b8###) which could perform better at this type of task."
        }
    ],
    "url": "http://arxiv.org/html/2405.11222v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2",
            "3.2.1"
        ],
        "main_experiment_and_results_sections": [
            "2.1",
            "4",
            "4.1",
            "4.2",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1.1",
            "3.1.2",
            "3.1.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.11222v1",
        "paper_title": "ISDS-NLP at SemEval-2024 Task 10: Transformer based neural networks for emotion recognition in conversations",
        "research_background": "### Introduction\nTask 10 from the SemEval 2024 competition (Kumar et al., 2024) addresses the intricate challenge of identifying emotions within dialogues in both English and Hindi. This task contains two primary objectives: \n1. Assigning an emotion label to each utterance within a dialogue.\n2. Identifying the trigger utterance or utterances responsible for an emotion-flip within a dialogue (Kumar et al., 2022).\n\nEmotions heavily influence human interactions, and possessing insight into the underlying sentiments can greatly enhance understanding of textual content. For example, in conflict-prone environments such as customer service platforms, virtual assistant chats, or forums, pinpointing trigger utterances for emotion flips can aid in mediating conflicts and preventing escalation. A chatbot interacting with an irate customer can benefit from adjustments in conversation to elicit more empathetic responses. Recognizing a potential emotion flip from neutral to anger enables the chatbot to refine its responses to avoid escalation, while a transition from anger to joy can boost the chatbot's confidence in using specific reassuring responses.\n\nBoth types of models we attempted for Subtask 1 utilized transformers. Our first approach employed BERT-like models which achieved the best accuracy, whereas the second approach used a state-of-the-art causal model (Mistral, Jiang et al., 2023) tested in zero-shot and few-shot settings but yielded poorer results.\n\nWhile our system performed relatively well in the first task, ranking 12th on the leaderboard, the other two subtasks proved more challenging, leading to a 14th place finish in the second subtask. We believe that implementing a more effective strategy to prevent overfitting, such as under-sampling or over-sampling, could have improved our system's performance. Our code is open-source and can be accessed on GitHub.\n\n### Motivation\nEmotions are vital in human communication, and understanding the sentiment behind a text can significantly improve comprehension. In practical contexts, such as customer service and virtual assistant interactions, effectively identifying emotions and their triggers can help de-escalate conflicts and enhance user satisfaction. A proactive approach to detecting and managing emotions can transform chatbot interactions, ensuring they are more empathetic and responsive.\n\n### Research Problem\nThe primary research problem this paper addresses is the identification and labeling of emotions in dialogue utterances, as well as recognizing the specific utterances that trigger emotion flips. This task is complex due to the nuances of human emotions and the varying contexts in which they are expressed.\n\n### Relevant Prior Work\n- Kumar et al. (2022) introduced the framework for identifying emotions in dialogues and detecting triggers for emotion flips, emphasizing the importance of this capability in understanding sentiment dynamics in conversations.\n- BERT-like models, known for their effectiveness in various NLP tasks, were utilized to achieve the best performance in classifying emotion labels.\n- The causal model (Mistral, Jiang et al., 2023) used in zero-shot and few-shot settings demonstrated the challenges in applying such models to this specific task, highlighting areas for potential improvement.",
        "methodology": "The proposed method for emotion recognition in conversations involves utilizing two transformer-based neural network architectures: Masked Language Modelling and Casual Modelling. \n\n1. **Masked Language Modelling (MLM)**:\n   - This approach involves predicting masked out tokens within a sequence. By doing this, the model learns to understand the context surrounding the masked words, which is beneficial for capturing nuances in conversational data.\n\n2. **Casual Modelling**:\n   - This method focuses on predicting the next token in a sequence, similar to auto-regressive models. It enables the model to generate coherent text by understanding the flow and direction of conversations, which is critical for emotion recognition.\n\nBoth approaches leverage the strengths of transformer architectures, which have proven to be highly successful in recent natural language processing (NLP) tasks. These transformers enable the models to handle long-range dependencies in text efficiently, making them suitable for the complex task of emotion recognition in conversations.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Dataset:** The main dataset for the experiment consists of English and Hindi code-mixed conversations for Subtask 1 and 2, and English-only conversations for Subtask 3. This dataset is primarily based on the MELD (Multimodal EmotionLines Dataset), a well-known emotion recognition dataset, augmented with emotion-flip triggers for the designated tasks. Specific data characteristics include multiple distinct emotions: neutral, anger, surprise, fear, joy, sadness, disgust, and contempt, with neutral being the most dominant emotion followed by joy and anger.\n\n**Baselines:** The baseline models were based on transformer-based neural networks tailored for emotion recognition in conversations. Although details of other baseline methods were not provided, transformer models are typically utilized for their strong performance in NLP tasks.\n\n**Evaluation Metrics:** The primary evaluation metric reported is the F1 score, which is a measure of a model's accuracy considering both precision and recall.\n\n**Main Experimental Results:** The results indicated that with the provided datasets, the model's F1 score could achieve 0.97 if data for Subtask 2 and 3 were combined for Subtask 1, signaling a strong performance capability with larger datasets. This demonstrates that the model would significantly benefit from increased data availability, further aligning with the hypothesis that data richness directly enhances model efficacy in emotion recognition.\n\nBy focusing on these points, the main experiment highlights the effectiveness of transformer-based models when applied to conversational emotion recognition, reinforcing the need for ample data to fully leverage their potential."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to investigate the impact of various input lengths on the performance of emotion recognition models in dialogue sentences.",
            "experiment_process": "Analysis of dialogue sentences revealed a predominantly short length, with a sharp decline in frequency after 30 tokens. Various maximum sequence lengths were tested, with 55 tokens yielding the best results. Data preprocessing such as lemmatization, and removing punctuation or stopwords did not help the model learn better, so the input was kept as is.",
            "result_discussion": "The optimal performance was achieved with a maximum sequence length of 55 tokens. The findings suggest that punctuation and stopwords contain useful information that the model is able to learn.",
            "ablation_id": "2405.11222v1.No1"
        },
        {
            "research_objective": "To determine the impact of different hidden state layers and classifier architectures on the performance of emotion recognition models.",
            "experiment_process": "Experiments were conducted to identify the optimal hidden state layer of pre-trained models for emotion recognition. The final layer's output was found to yield the strongest performance, with accuracy declining in earlier layers. Various classifiers were tested, including fully connected layers, which excelled due to their ability to model complex, non-linear relationships. The top-performing model employed a fully connected layer with 0.5 dropout and a Softmax activation function.",
            "result_discussion": "Using the final layer's output of pre-trained models provided the strongest performance. A fully connected layer with 0.5 dropout and Softmax activation function was the best-performing classifier, likely due to its ability to capture complex, non-linear relationships.",
            "ablation_id": "2405.11222v1.No2"
        },
        {
            "research_objective": "To determine the effectiveness of a staged fine-tuning strategy on large pre-trained language models for emotion recognition in conversations.",
            "experiment_process": "The fine-tuning involved initially training only the classifier with a larger learning rate (5e-5) and a warm-up period of 10,000 steps over 'k' epochs (ranging from 1 to 10). Subsequently, both the classifier and the transformer's final layer were fine-tuned using a smaller learning rate (2e-5). The initial freezing of transformer weights aimed to minimize the risk of overfitting.",
            "result_discussion": "The staged fine-tuning strategy helped in adapting the large pre-trained language models to the nuances of the emotion recognition task while minimizing overfitting. This was achieved by initially training at a higher learning rate for the classifier and subsequently fine-tuning at a lower rate for both the classifier and the transformer\u2019s final layer.",
            "ablation_id": "2405.11222v1.No3"
        }
    ]
}