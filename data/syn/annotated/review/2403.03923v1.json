{
    "title": "Did Translation Models Get More Robust Without Anyone Even Noticing?",
    "abstract": "Neural machine translation (MT) models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to \u201cnoisy\u201d inputs, such as spelling errors, abbreviations, and other formatting issues.\nIn this paper, we revisit this insight in light of recent multilingual MT models and large language models (LLMs) applied to machine translation.\nSomewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data.\nThis is notable because, even though LLMs have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness.\nNext, we show that similar trends hold for social media translation experiments \u2013 LLMs are more robust to social media text.\nWe include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise.\nAltogether, we show that robustness to many types of noise has increased.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "For years, the conventional wisdom has been that neural machine translation (MT) models are highly sensitive to source-side artificial and natural noise at inference time (Belinkov and Bisk, 2018  ###reference_b7###).\nThis insight has motivated many works that seek to make MT models more robust to noise through either specialized training (Ebrahimi et al., 2018  ###reference_b9###; Karpukhin et al., 2019  ###reference_b15###; Park et al., 2020  ###reference_b24###; Vaibhav et al., 2019  ###reference_b44###) or bespoke architectures (Rust et al., 2022  ###reference_b34###; Salesky et al., 2021  ###reference_b35###).\nHowever, MT is increasingly being performed in a different paradigm than when these analyses and architectures were created.\nPreviously, models were mostly trained from scratch on task-specific data, often for a single language pair.\nNowadays, strong results often depend on large pretrained encoder-decoder models (NLLB Team et al., 2022  ###reference_b21###), instruction-tuned large language models (LLMs) like TowerLLM (Alves et al., 2024  ###reference_b3###), or opaque proprietary systems like ChatGPT.111https://chat.openai.com/  ###reference_chat.openai.com/###\nThese huge models may make existing robustness techniques more expensive (specialized training takes longer with larger models) or impossible (specialized architectures cannot be grafted onto an existing pretrained system).\nSo the question is, how necessary are these robustness techniques for LLMs?\nAre they still needed to mitigate the brittleness of subword-level transformers, or have larger models and training sets made today\u2019s models sufficiently robust on their own?\n###figure_1### In this work, we investigate these questions through experiments on social media text and synthetically noised corpora.\nThese experiments have complementary roles: social media text contains diverse noise phenomena, but isolating their effect is not straightforward because the errors are not labeled.\nOn the other hand, synthetic errors may differ in major ways from \u201cnaturally occurring\u201d noise, but they are interpretable and controllable, offering a way to measure noise in vitro.\nBy evaluating on a broad spectrum of error types, we can paint a more vivid picture of what kinds of noise, and at what quantities, cause problems for MT systems.\nWe make the following contributions:222Our code is available at https://github.com/deep-spin/robust-mt  ###reference_###.\nWe show (\u00a73  ###reference_###) that large pretrained models are much more robust to synthetic source-side errors than conventional single-pair NMT models (see Figure 1  ###reference_###), even when their performance is similar on clean data. These results hold across several language pairs and varieties of noise, even though the large models lack architectural features that obviously encourage robustness to character noise.\nWe introduce (\u00a73.1  ###reference_###) a novel technique for measuring the robustness of MT models by learning a regression to predict the quality decline as a function of how noisy the source is.\nWe show (\u00a74.1  ###reference_###) that models that are robust to synthetic errors perform better at translating social media text.\nWe investigate the relationship between synthetic robustness and performance on \u201creal-world\u201d noise.\nWe conduct (\u00a74.2  ###reference_###) reference-free MT experiments on MultiLexNorm (van der Goot et al., 2021  ###reference_b11###), a lexical normalization dataset that appears to have never before been used for MT.\nWe show that LLMs are more robust than conventional models to this type of noise.\nWe show (\u00a75  ###reference_###) that source correction pipelines can be an effective approach to mitigate the impact of synthetic noise without substantially worsening performance on clean data, although they are significantly less effective with stronger models, suggesting that the benefits of source correction and model robustness are not complementary. Source correction is less effective on social media data, likely because there are not enough errors to outweigh the risk of error propagation."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "In recent years, mainstream MT techniques have been based on the transformer (Vaswani et al., 2017  ###reference_b45###), which uses multi-headed self-attention to mix information across time steps.\nIn the original work, transformers used an encoder-decoder paradigm similar to recurrent MT models (Bahdanau et al., 2014  ###reference_b5###).\nThese models pair an encoder over the source with a decoder, an autoregressive language model that predicts target tokens one at a time.\nThese tokens usually come from a learned subword vocabulary (Kudo, 2018  ###reference_b17###; Sennrich et al., 2016  ###reference_b37###).\nInitially, transformer MT models were trained from scratch for a single language pair on parallel data from sources such as the OPUS parallel corpus collection (Tiedemann, 2012  ###reference_b40###).\nAlthough single language pair models often perform well, they struggle in the absence of large quantities of data, making it difficult to achieve good results on low resource language pairs.\nThis problem can be mitigated somewhat through multilingual training with systems like M2M-100 (Fan et al., 2021  ###reference_b10###) and NLLB-200 (NLLB Team et al., 2022  ###reference_b21###).\nLow resource language pairs often benefit from training data in other languages.\nOne challenge is language imbalance \u2013 the subword vocabulary and training procedure need to be designed to allow strong performance across covered language pairs in spite of this imbalance.\nIn parallel to these MT-centric developments, transformers have increasingly been used in a transfer learning set-up in which a large model is pretrained on some generic objective for which massive data is available.\nSuch a model can then be finetuned on a particular downstream task or set of tasks.\nWhen the pretraining objective is language modeling (Radford et al., 2018  ###reference_b27###), this makes it straightforward to use the model for text generation tasks such as MT.\nRecent generation-oriented models are often decoder-only: to use such a model for MT, the source is simply treated as part of the decoder\u2019s context.\nSuch models have shown some success for MT (Hendy et al., 2023  ###reference_b12###).\nIn recent times, the paradigm has shifted from traditional finetuning to instruction tuning (Sanh et al., 2022  ###reference_b36###; Wei et al., 2022  ###reference_b46###), in which the finetuning data is accompanied by a prompt containing an instruction.\nThis has been shown to give models the ability to generalize to related tasks and has proven to be effective for MT (Alves et al., 2023  ###reference_b2###, 2024  ###reference_b3###).\nA common technique to increase robustness is to train MT models on examples with added source errors.\nGiven that high-quality corpora containing authentic errors are rare,\nthe added noise is generally synthetic (Karpukhin et al., 2019  ###reference_b15###), although it can be tuned to resemble natural errors (Martucci et al., 2021  ###reference_b18###; Vaibhav et al., 2019  ###reference_b44###).\nWhether training on synthetic noise is actually helpful for becoming robust to natural errors is an open question, with various works coming to contradictory conclusions (Belinkov and Bisk, 2018  ###reference_b7###; Vaibhav et al., 2019  ###reference_b44###).\nAnother hazard is that training on noise can reduce performance on clean data (Khayrallah and Koehn, 2018  ###reference_b16###), so the quantity of noise needs to be chosen carefully.\nAs an alternative to specialized training techniques, robustness can be achieved with architectures other than the ubiquitous subword-level transformer.\nModeling at the character or byte level (Sutskever et al., 2011  ###reference_b39###; Xue et al., 2022  ###reference_b47###) means that perturbations make only small changes to the sequence of tokens that the model is exposed to, whereas these same perturbations can cause a subword-level model to be exposed to completely different subword types.\nThis may make character- and byte-level models more robust, although the evidence is mixed (Mielke et al., 2021  ###reference_b20###).\nThese models are also much slower than subword-level models because of longer sequence lengths.\nAs an alternative, MT models can be trained on representations that are invariant to character shuffles (Belinkov and Bisk, 2018  ###reference_b7###) or on visual representations of text (Salesky et al., 2021  ###reference_b35###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Architectures for MT",
            "text": "In recent years, mainstream MT techniques have been based on the transformer (Vaswani et al., 2017  ###reference_b45###  ###reference_b45###), which uses multi-headed self-attention to mix information across time steps.\nIn the original work, transformers used an encoder-decoder paradigm similar to recurrent MT models (Bahdanau et al., 2014  ###reference_b5###  ###reference_b5###).\nThese models pair an encoder over the source with a decoder, an autoregressive language model that predicts target tokens one at a time.\nThese tokens usually come from a learned subword vocabulary (Kudo, 2018  ###reference_b17###  ###reference_b17###; Sennrich et al., 2016  ###reference_b37###  ###reference_b37###).\nInitially, transformer MT models were trained from scratch for a single language pair on parallel data from sources such as the OPUS parallel corpus collection (Tiedemann, 2012  ###reference_b40###  ###reference_b40###).\nAlthough single language pair models often perform well, they struggle in the absence of large quantities of data, making it difficult to achieve good results on low resource language pairs.\nThis problem can be mitigated somewhat through multilingual training with systems like M2M-100 (Fan et al., 2021  ###reference_b10###  ###reference_b10###) and NLLB-200 (NLLB Team et al., 2022  ###reference_b21###  ###reference_b21###).\nLow resource language pairs often benefit from training data in other languages.\nOne challenge is language imbalance \u2013 the subword vocabulary and training procedure need to be designed to allow strong performance across covered language pairs in spite of this imbalance.\nIn parallel to these MT-centric developments, transformers have increasingly been used in a transfer learning set-up in which a large model is pretrained on some generic objective for which massive data is available.\nSuch a model can then be finetuned on a particular downstream task or set of tasks.\nWhen the pretraining objective is language modeling (Radford et al., 2018  ###reference_b27###  ###reference_b27###), this makes it straightforward to use the model for text generation tasks such as MT.\nRecent generation-oriented models are often decoder-only: to use such a model for MT, the source is simply treated as part of the decoder\u2019s context.\nSuch models have shown some success for MT (Hendy et al., 2023  ###reference_b12###  ###reference_b12###).\nIn recent times, the paradigm has shifted from traditional finetuning to instruction tuning (Sanh et al., 2022  ###reference_b36###  ###reference_b36###; Wei et al., 2022  ###reference_b46###  ###reference_b46###), in which the finetuning data is accompanied by a prompt containing an instruction.\nThis has been shown to give models the ability to generalize to related tasks and has proven to be effective for MT (Alves et al., 2023  ###reference_b2###  ###reference_b2###, 2024  ###reference_b3###  ###reference_b3###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robustness to Character Noise",
            "text": "Character perturbations have been shown to have a large negative impact on the performance of MT models (Belinkov and Bisk, 2018  ###reference_b7###).\nConsequently, a number of techniques have been proposed to mitigate their impact.\nA common technique to increase robustness is to train MT models on examples with added source errors.\nGiven that high-quality corpora containing authentic errors are rare,\nthe added noise is generally synthetic (Karpukhin et al., 2019  ###reference_b15###  ###reference_b15###), although it can be tuned to resemble natural errors (Martucci et al., 2021  ###reference_b18###  ###reference_b18###; Vaibhav et al., 2019  ###reference_b44###  ###reference_b44###).\nWhether training on synthetic noise is actually helpful for becoming robust to natural errors is an open question, with various works coming to contradictory conclusions (Belinkov and Bisk, 2018  ###reference_b7###  ###reference_b7###; Vaibhav et al., 2019  ###reference_b44###  ###reference_b44###).\nAnother hazard is that training on noise can reduce performance on clean data (Khayrallah and Koehn, 2018  ###reference_b16###  ###reference_b16###), so the quantity of noise needs to be chosen carefully.\nAs an alternative to specialized training techniques, robustness can be achieved with architectures other than the ubiquitous subword-level transformer.\nModeling at the character or byte level (Sutskever et al., 2011  ###reference_b39###  ###reference_b39###; Xue et al., 2022  ###reference_b47###  ###reference_b47###) means that perturbations make only small changes to the sequence of tokens that the model is exposed to, whereas these same perturbations can cause a subword-level model to be exposed to completely different subword types.\nThis may make character- and byte-level models more robust, although the evidence is mixed (Mielke et al., 2021  ###reference_b20###  ###reference_b20###).\nThese models are also much slower than subword-level models because of longer sequence lengths.\nAs an alternative, MT models can be trained on representations that are invariant to character shuffles (Belinkov and Bisk, 2018  ###reference_b7###  ###reference_b7###) or on visual representations of text (Salesky et al., 2021  ###reference_b35###  ###reference_b35###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Robustness to Synthetic Noise",
            "text": "In our first experiments, we evaluate how various models perform in the presence of token-level synthetic errors.\nAlthough synthetic errors differ from \u201cnaturally occurring\u201d noise, they are adjustable and function as a stress test for MT systems.\nWe use four types of synthetic perturbations, each of which is a plausible error based on the mechanics of typing.\nFor each noise type, we corrupt 10% to 100% of whitespace-delimited tokens in the FLORES-200 devtest data (NLLB Team et al., 2022  ###reference_b21###).\nWe use the following noise types:\nswap: flip two adjacent characters.\nchardupe: duplicate a character.\nchardrop: delete a character.\nkey: replace a character with an adjacent one on a keyboard. We use the QWERTZ layout for German, AZERTY for French, QWERTY for English and Portuguese, and South Korean Dubeolsik for Korean.\nFor Korean, we used hangul-jamo333https://github.com/jonghwanhyeon/hangul-jamo  ###reference_mo### to decompose hangul characters into jamo, which represent individual keystrokes, before applying perturbations.\nWe use models that differ in their scope (bi- or multilingual), architecture (encoder-decoder or decoder-only), and size (74M-7B parameters).\nOPUS: We use transformer encoder-decoder models trained from scratch on a single language pair and released as part of OPUS-MT (Tiedemann and Thottingal, 2020  ###reference_b42###). Model and vocabulary sizes are listed in Table 2  ###reference_###.\nNLLB (NLLB Team et al., 2022  ###reference_b21###), like OPUS, is an encoder-decoder transformer trained on parallel text.\nHowever, NLLB is a many-to-many system trained on data in 202 languages.\nWe use the 3.3 billion parameter version.\nTower: We use the 7 billion parameter version of TowerInstruct444https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1  ###reference_ct-7B-v0.1### (Alves et al., 2024  ###reference_b3###), a recently-released instruction-tuned LLM derived from Llama 2 (Touvron et al., 2023  ###reference_b43###).\nIt covers translation between 10 languages.\nGPT-3.5:555Specifically, we use gpt-3.5-turbo-1106. the architecture and training data of GPT-3.5 are unknown, making it difficult to draw scientific conclusions from its results. We include it because it has shown success at MT (Hendy et al., 2023  ###reference_b12###) and the related GPT-4 has been shown to be able to correct some character perturbations (Cao et al., 2023  ###reference_b8###).\nFor NLLB, Tower, and the models with a listed HF Path in Table 2  ###reference_###, we use public checkpoints from the Hugging Face transformers library.666https://github.com/huggingface/transformers  ###reference_s###\nFor the pten OPUS model, we use a script777https://github.com/huggingface/transformers/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py  ###reference_s/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py### to convert its Marian checkpoint888https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/por-eng  ###reference_allenge/tree/master/models/por-eng### to transformers format.\nWe directly use checkpoints from the Tatoeba Challenge (Tiedemann, 2020  ###reference_b41###) and perform inference with Marian (Junczys-Dowmunt et al., 2018  ###reference_b14###) for enko999https://object.pouta.csc.fi/Tatoeba-MT-models/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip  ###reference_els/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip### and koen101010https://object.pouta.csc.fi/Tatoeba-MT-models/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip  ###reference_els/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip### because the conversion script failed for them.\nWe generate translations using beam search (Reddy et al., 1977  ###reference_b29###) with a beam size of 5 for all models except GPT-3.5.\nFor GPT-3.5, we sample with temperature 0.\nWe use prompts for Tower111111\u2018\u2018Translate the following text from [source language] to [target language].nSource:[source text]n[target language]:\u2019\u2019 and GPT-3.5.121212\u2018\u2018Translate this sentence from [source language] to [target language].nSource:[source text]nTarget:\u2019\u2019\nOur base metric for scoring the translation performance on a corpus is COMET (Rei et al., 2020  ###reference_b31###).131313Specifically, we use COMET-22 (Rei et al., 2022a  ###reference_b30###).\nCOMET computes a normalized score for a hypothesis , conditioned on the source  and a reference .\nWhen we compute scores for translations from noisy data, we provide the COMET model the clean source, not the noisy version that was actually used to generate hypotheses.\nWe measure the trajectory of performance as the amount of noise is increased, as depicted in Figure 1  ###reference_###.\nTo represent this trajectory as a single number, for each configuration we fit a linear regression to predict how much COMET declines relative to the clean performance141414There is no need to learn an intercept term because the decline is relative to the model\u2019s clean performance. as a function of the proportion of noised tokens.\nWe report the learned slope, which we call COMET-slope.\nThe higher (closer to zero) the COMET-slope is, the more robust the model is.\nThis metric can also be interpreted as the number of COMET points that would be lost if every token were corrupted.\nswap\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-73.61\n-65.80\n-71.02\n-66.00\n-74.02\n-50.02\n-73.35\n-64.01\n\nNLLB\n-22.45\n-18.20\n-21.81\n-20.75\n-19.65\n-22.89\n-21.03\n-20.34\n\nTower\n-19.42\n-28.54\n-18.70\n-27.64\n-18.48\n-26.16\n-17.39\n-28.79\n\nGPT-3.5\n-3.89\n-4.36\n-4.46\n-5.85\n-4.79\n-20.89\n-3.76\n-6.78\nchardrop\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-69.40\n-54.17\n-64.33\n-50.72\n-71.37\n-44.14\n-70.13\n-53.15\n\nNLLB\n-22.13\n-16.51\n-21.56\n-17.10\n-18.33\n-20.81\n-20.89\n-18.52\n\nTower\n-18.42\n-19.34\n-18.64\n-17.93\n-15.19\n-24.62\n-17.89\n-20.80\n\nGPT-3.5\n-6.59\n-6.55\n-7.32\n-5.68\n-6.72\n-17.81\n-6.63\n-7.09\nchardupe\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-56.35\n-35.17\n-47.42\n-33.00\n-65.89\n-31.11\n-58.98\n-35.65\n\nNLLB\n-3.73\n-4.02\n-3.60\n-5.36\n-2.82\n-4.19\n-3.76\n-4.27\n\nTower\n-3.21\n-4.13\n-2.88\n-4.39\n-3.70\n-7.66\n-2.79\n-3.76\n\nGPT-3.5\n-1.14\n-1.36\n-1.32\n-1.42\n-1.42\n-5.64\n-0.98\n-1.44\nkey\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-74.13\n-64.46\n-73.35\n-66.05\n-76.68\n-38.50\n-76.20\n-66.02\n\nNLLB\n-26.20\n-20.17\n-25.07\n-21.80\n-23.69\n-26.41\n-24.25\n-23.54\n\nTower\n-28.64\n-29.49\n-27.13\n-31.89\n-25.89\n-23.80\n-26.33\n-36.55\n\nGPT-3.5\n-8.19\n-9.17\n-8.17\n-8.63\n-8.91\n-16.31\n-7.78\n-10.27\nThe results in Table 1  ###reference_### show that on clean data, Tower and GPT-3.5 are the two strongest systems, with at least one of them recording the highest COMET score for all eight language pairs.\nThe gap between the strongest system and the much smaller OPUS models is at most  COMET points for all language pairs except ende.\nHowever, Table 3  ###reference_### shows that the differences become much larger on noisy data.\nFor all language pairs and noise types, OPUS suffers far more from perturbations than any of the other models do.\nOn the other end of the spectrum, GPT-3.5 is almost always more robust than other models, sometimes strikingly so.\nNLLB and Tower are between these two extremes \u2013 they are far more robust than OPUS, but only come close to GPT-3.5 for chardupe noise.\nFor swap and chardrop noise, NLLB is more robust than Tower when translating to English, while the reverse is true when translating from English.\nThis trend is less consistent for chardupe noise.\nFor key noise, NLLB is more robust than Tower for every pair except koen.\nAll models decline as the noise level is increased, but they do not decline in the same way.\nIn Figure 2  ###reference_### we show how the fluency of the model\u2019s predictions, measured by the perplexity of GPT-2 (Radford et al., 2019  ###reference_b28###), is related to the quality of those predictions in terms of COMET.\nFor fren swaps, it is clear that NLLB and GPT-3.5 continue producing fluent English text even as the noise level increases.\n###figure_2### From these experiments, it is tempting to conclude that robustness depends largely on model size (OPUS is  times smaller than any other system) or on multilinguality (all systems except OPUS are multilingual).\nHowever, Figure 3  ###reference_### tells a different story.\nWe reran swap noise experiments with three extra models: the 600M and 1.3 billion parameter versions of NLLB (the former of which is distilled from the 54B version of NLLB) and the 1.2 billion parameter version of M2M (Fan et al., 2021  ###reference_b10###).\nDespite NLLB-1.3B and M2M-1.2B being multilingual models of similar sizes, they do not respond the same to noise: NLLB-1.3B follows a similar curve to NLLB-3.3B, while M2M-1.2B suffers nearly as much as OPUS.\nNLLB-600M is somewhere between these extremes.\nFurther work is needed to determine what factors influence this kind of robustness.\nIntroducing perturbations affects not only translation quality but also runtime.\nPerturbations create character sequences that are less similar to the data that tokenizers are trained on, which leads to more pieces being used to encode the sentence.\nThis is true even for chardrop noise, which increases the length of the tokenized sequence even as it shortens the detokenized sequence.\nIn Table 4  ###reference_###, we compare tokenizers by their fertility \u2014 the average number of subword pieces per whitespace word \u2014 on clean and key data.\nWhile OPUS tokenizers generally have very low fertility on clean data, it increases more than the other tokenizers, suggesting the tokenizer itself is less robust to character perturbations.\nIt is also notable that Tower and GPT-3.5 have high fertility even on clean Korean text.\nWhile this is a symptom of tokenizer unfairness in large models (Petrov et al., 2023  ###reference_b25###), it can also be a sign of tokenizer robustness: the higher the fertility, the closer the model is to byte-level tokenization.\nThis results in noisy token sequences that are much closer to the clean sequences for Tower and GPT-3.5, as can be seen in terms of F1 in Table 5  ###reference_###.\nThe same trend does not hold for the other languages."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experiments",
            "text": "In all of our synthetic experiments, we adopt a simple set-up: for each translation corpus, we introduce a particular type of perturbation into some percentage of the source-side tokens.\nWe then compare performance translating this perturbed corpus to the performance on clean data.\nA model\u2019s robustness can be characterized by the steepness of its decline as the noise level is increased: a flatter slope indicates that the model handles noise better.\nWe use four types of synthetic perturbations, each of which is a plausible error based on the mechanics of typing.\nFor each noise type, we corrupt 10% to 100% of whitespace-delimited tokens in the FLORES-200 devtest data (NLLB Team et al., 2022  ###reference_b21###  ###reference_b21###).\nWe use the following noise types:\nswap: flip two adjacent characters.\nchardupe: duplicate a character.\nchardrop: delete a character.\nkey: replace a character with an adjacent one on a keyboard. We use the QWERTZ layout for German, AZERTY for French, QWERTY for English and Portuguese, and South Korean Dubeolsik for Korean.\nFor Korean, we used hangul-jamo333https://github.com/jonghwanhyeon/hangul-jamo  ###reference_mo###  ###reference_mo### to decompose hangul characters into jamo, which represent individual keystrokes, before applying perturbations.\nWe use models that differ in their scope (bi- or multilingual), architecture (encoder-decoder or decoder-only), and size (74M-7B parameters).\nOPUS: We use transformer encoder-decoder models trained from scratch on a single language pair and released as part of OPUS-MT (Tiedemann and Thottingal, 2020  ###reference_b42###  ###reference_b42###). Model and vocabulary sizes are listed in Table 2  ###reference_###  ###reference_###.\nNLLB (NLLB Team et al., 2022  ###reference_b21###  ###reference_b21###), like OPUS, is an encoder-decoder transformer trained on parallel text.\nHowever, NLLB is a many-to-many system trained on data in 202 languages.\nWe use the 3.3 billion parameter version.\nTower: We use the 7 billion parameter version of TowerInstruct444https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1  ###reference_ct-7B-v0.1###  ###reference_ct-7B-v0.1### (Alves et al., 2024  ###reference_b3###  ###reference_b3###), a recently-released instruction-tuned LLM derived from Llama 2 (Touvron et al., 2023  ###reference_b43###  ###reference_b43###).\nIt covers translation between 10 languages.\nGPT-3.5:555Specifically, we use gpt-3.5-turbo-1106. the architecture and training data of GPT-3.5 are unknown, making it difficult to draw scientific conclusions from its results. We include it because it has shown success at MT (Hendy et al., 2023  ###reference_b12###  ###reference_b12###) and the related GPT-4 has been shown to be able to correct some character perturbations (Cao et al., 2023  ###reference_b8###  ###reference_b8###).\nFor NLLB, Tower, and the models with a listed HF Path in Table 2  ###reference_###  ###reference_###, we use public checkpoints from the Hugging Face transformers library.666https://github.com/huggingface/transformers  ###reference_s###  ###reference_s###\nFor the pten OPUS model, we use a script777https://github.com/huggingface/transformers/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py  ###reference_s/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py###  ###reference_s/blob/main/src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py### to convert its Marian checkpoint888https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/por-eng  ###reference_allenge/tree/master/models/por-eng###  ###reference_allenge/tree/master/models/por-eng### to transformers format.\nWe directly use checkpoints from the Tatoeba Challenge (Tiedemann, 2020  ###reference_b41###  ###reference_b41###) and perform inference with Marian (Junczys-Dowmunt et al., 2018  ###reference_b14###  ###reference_b14###) for enko999https://object.pouta.csc.fi/Tatoeba-MT-models/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip  ###reference_els/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip###  ###reference_els/eng-kor/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip### and koen101010https://object.pouta.csc.fi/Tatoeba-MT-models/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip  ###reference_els/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip###  ###reference_els/kor-eng/opusTCv20210807-sepvoc_transformer-big_2022-07-28.zip### because the conversion script failed for them.\nWe generate translations using beam search (Reddy et al., 1977  ###reference_b29###  ###reference_b29###) with a beam size of 5 for all models except GPT-3.5.\nFor GPT-3.5, we sample with temperature 0.\nWe use prompts for Tower111111\u2018\u2018Translate the following text from [source language] to [target language].nSource:[source text]n[target language]:\u2019\u2019 and GPT-3.5.121212\u2018\u2018Translate this sentence from [source language] to [target language].nSource:[source text]nTarget:\u2019\u2019\nOur base metric for scoring the translation performance on a corpus is COMET (Rei et al., 2020  ###reference_b31###  ###reference_b31###).131313Specifically, we use COMET-22 (Rei et al., 2022a  ###reference_b30###  ###reference_b30###).\nCOMET computes a normalized score for a hypothesis , conditioned on the source  and a reference .\nWhen we compute scores for translations from noisy data, we provide the COMET model the clean source, not the noisy version that was actually used to generate hypotheses.\nWe measure the trajectory of performance as the amount of noise is increased, as depicted in Figure 1  ###reference_###  ###reference_###.\nTo represent this trajectory as a single number, for each configuration we fit a linear regression to predict how much COMET declines relative to the clean performance141414There is no need to learn an intercept term because the decline is relative to the model\u2019s clean performance. as a function of the proportion of noised tokens.\nWe report the learned slope, which we call COMET-slope.\nThe higher (closer to zero) the COMET-slope is, the more robust the model is.\nThis metric can also be interpreted as the number of COMET points that would be lost if every token were corrupted.\nswap\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-73.61\n-65.80\n-71.02\n-66.00\n-74.02\n-50.02\n-73.35\n-64.01\n\nNLLB\n-22.45\n-18.20\n-21.81\n-20.75\n-19.65\n-22.89\n-21.03\n-20.34\n\nTower\n-19.42\n-28.54\n-18.70\n-27.64\n-18.48\n-26.16\n-17.39\n-28.79\n\nGPT-3.5\n-3.89\n-4.36\n-4.46\n-5.85\n-4.79\n-20.89\n-3.76\n-6.78\nchardrop\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-69.40\n-54.17\n-64.33\n-50.72\n-71.37\n-44.14\n-70.13\n-53.15\n\nNLLB\n-22.13\n-16.51\n-21.56\n-17.10\n-18.33\n-20.81\n-20.89\n-18.52\n\nTower\n-18.42\n-19.34\n-18.64\n-17.93\n-15.19\n-24.62\n-17.89\n-20.80\n\nGPT-3.5\n-6.59\n-6.55\n-7.32\n-5.68\n-6.72\n-17.81\n-6.63\n-7.09\nchardupe\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-56.35\n-35.17\n-47.42\n-33.00\n-65.89\n-31.11\n-58.98\n-35.65\n\nNLLB\n-3.73\n-4.02\n-3.60\n-5.36\n-2.82\n-4.19\n-3.76\n-4.27\n\nTower\n-3.21\n-4.13\n-2.88\n-4.39\n-3.70\n-7.66\n-2.79\n-3.76\n\nGPT-3.5\n-1.14\n-1.36\n-1.32\n-1.42\n-1.42\n-5.64\n-0.98\n-1.44\nkey\n\n\n\n\nModel\nende\ndeen\nenfr\nfren\nenko\nkoen\nenpt\npten\n\n\n\nOPUS\n-74.13\n-64.46\n-73.35\n-66.05\n-76.68\n-38.50\n-76.20\n-66.02\n\nNLLB\n-26.20\n-20.17\n-25.07\n-21.80\n-23.69\n-26.41\n-24.25\n-23.54\n\nTower\n-28.64\n-29.49\n-27.13\n-31.89\n-25.89\n-23.80\n-26.33\n-36.55\n\nGPT-3.5\n-8.19\n-9.17\n-8.17\n-8.63\n-8.91\n-16.31\n-7.78\n-10.27\nThe results in Table 1  ###reference_###  ###reference_### show that on clean data, Tower and GPT-3.5 are the two strongest systems, with at least one of them recording the highest COMET score for all eight language pairs.\nThe gap between the strongest system and the much smaller OPUS models is at most  COMET points for all language pairs except ende.\nHowever, Table 3  ###reference_###  ###reference_### shows that the differences become much larger on noisy data.\nFor all language pairs and noise types, OPUS suffers far more from perturbations than any of the other models do.\nOn the other end of the spectrum, GPT-3.5 is almost always more robust than other models, sometimes strikingly so.\nNLLB and Tower are between these two extremes \u2013 they are far more robust than OPUS, but only come close to GPT-3.5 for chardupe noise.\nFor swap and chardrop noise, NLLB is more robust than Tower when translating to English, while the reverse is true when translating from English.\nThis trend is less consistent for chardupe noise.\nFor key noise, NLLB is more robust than Tower for every pair except koen."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Analysis",
            "text": "###figure_3### All models decline as the noise level is increased, but they do not decline in the same way.\nIn Figure 2  ###reference_###  ###reference_### we show how the fluency of the model\u2019s predictions, measured by the perplexity of GPT-2 (Radford et al., 2019  ###reference_b28###  ###reference_b28###), is related to the quality of those predictions in terms of COMET.\nFor fren swaps, it is clear that NLLB and GPT-3.5 continue producing fluent English text even as the noise level increases.\n###figure_4### From these experiments, it is tempting to conclude that robustness depends largely on model size (OPUS is  times smaller than any other system) or on multilinguality (all systems except OPUS are multilingual).\nHowever, Figure 3  ###reference_###  ###reference_### tells a different story.\nWe reran swap noise experiments with three extra models: the 600M and 1.3 billion parameter versions of NLLB (the former of which is distilled from the 54B version of NLLB) and the 1.2 billion parameter version of M2M (Fan et al., 2021  ###reference_b10###  ###reference_b10###).\nDespite NLLB-1.3B and M2M-1.2B being multilingual models of similar sizes, they do not respond the same to noise: NLLB-1.3B follows a similar curve to NLLB-3.3B, while M2M-1.2B suffers nearly as much as OPUS.\nNLLB-600M is somewhere between these extremes.\nFurther work is needed to determine what factors influence this kind of robustness.\nIntroducing perturbations affects not only translation quality but also runtime.\nPerturbations create character sequences that are less similar to the data that tokenizers are trained on, which leads to more pieces being used to encode the sentence.\nThis is true even for chardrop noise, which increases the length of the tokenized sequence even as it shortens the detokenized sequence.\nIn Table 4  ###reference_###  ###reference_###, we compare tokenizers by their fertility \u2014 the average number of subword pieces per whitespace word \u2014 on clean and key data.\nWhile OPUS tokenizers generally have very low fertility on clean data, it increases more than the other tokenizers, suggesting the tokenizer itself is less robust to character perturbations.\nIt is also notable that Tower and GPT-3.5 have high fertility even on clean Korean text.\nWhile this is a symptom of tokenizer unfairness in large models (Petrov et al., 2023  ###reference_b25###  ###reference_b25###), it can also be a sign of tokenizer robustness: the higher the fertility, the closer the model is to byte-level tokenization.\nThis results in noisy token sequences that are much closer to the clean sequences for Tower and GPT-3.5, as can be seen in terms of F1 in Table 5  ###reference_###  ###reference_###.\nThe same trend does not hold for the other languages."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Robustness to Social Media Text",
            "text": "The previous experiments show that large translation models and LLMs are more robust to synthetic character perturbations than conventional MT models.\nBut is this result applicable to \u201cauthentically noisy\u201d domains such as social media text?\nThe nature of \u201cnoise\u201d here is different than in the synthetic task: social media text does not necessarily contain many errors (Rello and Baeza-Yates, 2012  ###reference_b33###), but the domain is very different from FLORES.\nThese factors make it difficult to isolate the effect of noise from the general domain adaptation problem.\nIn an ideal world, we would have a translation corpus in which each example is a triple consisting of an original noisy source sequence, a manually annotated cleaned source sequence, and a gold standard translation.\nThis would allow translations of clean and noisy versions of the same source to be compared on some reference-based metric, isolating the effect of the errors.\nUnfortunately, to our knowledge no such corpus exists, so instead we perform two complementary investigations.\nFirst, we evaluate our models on MTNT (Michel and Neubig, 2018  ###reference_b19###), a noisy social media translation corpus.\nAlthough this is a useful test of our models\u2019 capabilities, the noise in the corpus is not labeled and there is no clean version of the same data to compare to.\nThis motivates our second experiment, in which we translate data from MultiLexNorm (van der Goot et al., 2021  ###reference_b11###), a lexical normalization benchmark.\nTogether, these two experiments allow us to see both which models succeed and how badly they fail.\nMichel and Neubig (2018  ###reference_b19###) estimate that  percent of English tokens in MTNT enfr are misspelled.\nThis is much less noise than we used in our synthetic experiments, but is higher than in formal corpora.\nWe finetuned OPUS and NLLB on the MTNT train sets for enfr,\nusing early stopping with patience 3 and validating by loss every 100 steps.\nOther hyperparameters are in Table 7  ###reference_###.\nResults are shown in Table 8  ###reference_###.\nGPT-3.5 performs best.\nAmong other models, Tower is generally strongest, although NLLB roughly equals it after finetuning.\nOPUS performs worst.\nThe MultiLexNorm dataset covers 13 languages, but many of the corpora are tiny or are in languages that Tower does not cover, so we only use English, German, and Spanish151515For experiments involving Spanish, we use Helsinki-NLP/opus-mt-tc-big-en-es (234.8M parameters, 55k vocabulary) and Helsinki-NLP/opus-mt-es-en (77.9M parameters, 65k vocabulary) from transformers. as sources.\nIn experiments with English sources, we translate to German and Spanish; otherwise, we translate to English.\nStatistics are presented in Table 9  ###reference_###.\nAs MultiLexNorm lacks reference translations, we use three reference-free evaluation techniques.\nFirst, we use faux-BLEU (Anastasopoulos, 2019  ###reference_b4###), which computes  (Papineni et al., 2002  ###reference_b23###), where  is the hypothesis computed from the noisy source,  is the hypothesis computed from the clean source, and  is treated as a pseudoreference.161616Specifically, we use spBLEU (NLLB Team et al., 2022  ###reference_b21###).\nBy analogy we also compute faux-COMET.\nThese faux-metrics measure the similarity between  and , with faux-BLEU being a lexical metric that captures surface-level features, while faux-COMET is more semantic.\nIn addition, we use a new metric that we dub QE.\nGiven , , noisy and clean source sequences  and , and a reference-free quality estimation metric QE, .\nWe use COMETKiwi (Rei et al., 2022b  ###reference_b32###) to compute QE.\nA QE close to zero means that a model produces similar-quality outputs for both inputs, indicating robustness, whereas a large positive value indicates that translation quality suffers on noisy data.\nTable 6  ###reference_### shows the performance of all models with both noisy and gold-standard cleaned versions of the corpora.\nIn terms of QE, GPT-3.5 performs best for all language pairs.\nIt also has the best faux-COMET for all except ende (where Tower passes it).\nBy faux-BLEU it is outperformed by Tower for ende and NLLB for esen.\nThere is a contrast between the performance of NLLB and Tower.\nWhile Tower has the better QE for all language pairs and the better faux-COMET for all except esen, NLLB outperforms it by faux-BLEU for all except ende.\nThis suggests that NLLB preserves lexical structure, while Tower preserves \u201cdeeper\u201d features.\n###figure_5###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "MultiLexNorm Experiments",
            "text": "While MTNT is an established benchmark and useful sanity check, it is not controllable like our synthetic experiments; we cannot isolate the effect of noise because there is no non-noisy version of the corpus.\nTherefore we pivot to evaluate models on translating MultiLexNorm (van der Goot et al., 2021  ###reference_b11###), a lexical normalization dataset that pairs social media text primarily from Twitter with manually cleaned versions of the same.\nSwitching from MTNT to MultiLexNorm comes with a trade-off: in order to gain clean sources, we lose references.\nThe MultiLexNorm dataset covers 13 languages, but many of the corpora are tiny or are in languages that Tower does not cover, so we only use English, German, and Spanish151515For experiments involving Spanish, we use Helsinki-NLP/opus-mt-tc-big-en-es (234.8M parameters, 55k vocabulary) and Helsinki-NLP/opus-mt-es-en (77.9M parameters, 65k vocabulary) from transformers. as sources.\nIn experiments with English sources, we translate to German and Spanish; otherwise, we translate to English.\nStatistics are presented in Table 9  ###reference_###  ###reference_###.\nAs MultiLexNorm lacks reference translations, we use three reference-free evaluation techniques.\nFirst, we use faux-BLEU (Anastasopoulos, 2019  ###reference_b4###  ###reference_b4###), which computes  (Papineni et al., 2002  ###reference_b23###  ###reference_b23###), where  is the hypothesis computed from the noisy source,  is the hypothesis computed from the clean source, and  is treated as a pseudoreference.161616Specifically, we use spBLEU (NLLB Team et al., 2022  ###reference_b21###  ###reference_b21###).\nBy analogy we also compute faux-COMET.\nThese faux-metrics measure the similarity between  and , with faux-BLEU being a lexical metric that captures surface-level features, while faux-COMET is more semantic.\nIn addition, we use a new metric that we dub QE.\nGiven , , noisy and clean source sequences  and , and a reference-free quality estimation metric QE, .\nWe use COMETKiwi (Rei et al., 2022b  ###reference_b32###  ###reference_b32###) to compute QE.\nA QE close to zero means that a model produces similar-quality outputs for both inputs, indicating robustness, whereas a large positive value indicates that translation quality suffers on noisy data.\nTable 6  ###reference_###  ###reference_### shows the performance of all models with both noisy and gold-standard cleaned versions of the corpora.\nIn terms of QE, GPT-3.5 performs best for all language pairs.\nIt also has the best faux-COMET for all except ende (where Tower passes it).\nBy faux-BLEU it is outperformed by Tower for ende and NLLB for esen.\nThere is a contrast between the performance of NLLB and Tower.\nWhile Tower has the better QE for all language pairs and the better faux-COMET for all except esen, NLLB outperforms it by faux-BLEU for all except ende.\nThis suggests that NLLB preserves lexical structure, while Tower preserves \u201cdeeper\u201d features.\n###figure_6###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Source Correction",
            "text": "So far we have shown that OPUS is less robust to synthetic noise than larger models and performs worse on social media text.\nNext we handle noise with source correction pipelines.\nPipelines cannot be learned end-to-end and introduce a risk of error propagation, but they are modular and interpretable, and can be used with closed-source models.\nThe effect of correcting synthetic enfr errors is shown in Figure 4  ###reference_###.\nFor OPUS models, source correction is often beneficial, with gains of more than 40 COMET points for mT5 over the baseline when at least 50% of tokens are noised.\nSource correction brings smaller performance gains for NLLB and Tower.\nSource correction has a slight negative effect on GPT-3.5, indicating that extremely robust models have little to gain from source correction.\nHowever, Table 12  ###reference_### shows that correcting clean inputs is nearly harmless.\nmT5 never loses more than  COMET points.\nEven the weaker JamSpell never causes a degradation of more than half a COMET point, suggesting that it could be useful because of its fast runtime.\nSocial media data is a challenge for both of our source correctors because they were trained on other domains.\nUsing either corrected source leads to a decline of at least one COMET point compared to the raw source.\nAlthough an oracle delivers some gains, it still cannot equal the performance of finetuning when it is available.\nThese results seem to confirm that MTNT is not particularly noisy in terms of spelling errors (Karpukhin et al., 2019  ###reference_b15###; Michel and Neubig, 2018  ###reference_b19###).\nBased on the results in Table 13  ###reference_###, it would be easy to conclude that source correction does not work for MTNT.\nHowever, Figure 5  ###reference_### shows that some sentences do benefit from correction, and others are not harmed.\nIn these terms, mT5 appears to be a higher-risk corrector than JamSpell, improving translation quality for  of examples (versus  for JamSpell) while making quality worse for  of examples (versus  for JamSpell).\nThis, in addition to the oracle performance, suggests that further research could make source correction a useful tool in this domain."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Correction Models",
            "text": "We use two architectures for source correction:\nJamSpell (Ozinov, 2019  ###reference_b22###) is a non-neural spell-checker with trigram context. We use the default English model from the package.171717https://github.com/bakwc/JamSpell  ###reference_github.com/bakwc/JamSpell###\nWe finetune mT5-based (Xue et al., 2021  ###reference_b48###) correctors on the Prob+Word dataset from NeuSpell181818https://github.com/neuspell/neuspell  ###reference_### (Jayanthi et al., 2020  ###reference_b13###). We use the mT5-Large (1.2B parameter) version of the architecture with the hyperparameter ranges in Table 10  ###reference_###.\nWe select the checkpoint with the best chrF (Popovi\u0107, 2015  ###reference_b26###) when decoding a noised version of the FLORES English dev set.\nWe decode with a beam size of 5.\nThese two models are minimalistic and maximalistic approaches to correction.\nJamSpell is fast (more than 10k tokens per second on a laptop CPU) but uses only trigram context.\nOn the other hand, mT5 is a large encoder-decoder that leverages a long context.\nIts subword vocabulary allows it to make open-vocabulary corrections, but it is much slower than JamSpell and is prone to hallucination."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Correcting Synthetic Errors",
            "text": "The effect of correcting synthetic enfr errors is shown in Figure 4  ###reference_###  ###reference_###.\nFor OPUS models, source correction is often beneficial, with gains of more than 40 COMET points for mT5 over the baseline when at least 50% of tokens are noised.\nSource correction brings smaller performance gains for NLLB and Tower.\nSource correction has a slight negative effect on GPT-3.5, indicating that extremely robust models have little to gain from source correction.\nHowever, Table 12  ###reference_###  ###reference_### shows that correcting clean inputs is nearly harmless.\nmT5 never loses more than  COMET points.\nEven the weaker JamSpell never causes a degradation of more than half a COMET point, suggesting that it could be useful because of its fast runtime."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Correcting MTNT",
            "text": "###figure_7### ###figure_8### Social media data is a challenge for both of our source correctors because they were trained on other domains.\nUsing either corrected source leads to a decline of at least one COMET point compared to the raw source.\nAlthough an oracle delivers some gains, it still cannot equal the performance of finetuning when it is available.\nThese results seem to confirm that MTNT is not particularly noisy in terms of spelling errors (Karpukhin et al., 2019  ###reference_b15###  ###reference_b15###; Michel and Neubig, 2018  ###reference_b19###  ###reference_b19###).\nBased on the results in Table 13  ###reference_###  ###reference_###, it would be easy to conclude that source correction does not work for MTNT.\nHowever, Figure 5  ###reference_###  ###reference_### shows that some sentences do benefit from correction, and others are not harmed.\nIn these terms, mT5 appears to be a higher-risk corrector than JamSpell, improving translation quality for  of examples (versus  for JamSpell) while making quality worse for  of examples (versus  for JamSpell).\nThis, in addition to the oracle performance, suggests that further research could make source correction a useful tool in this domain."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We presented several experiments testing the robustness of MT systems to synthetic and natural noise.\nOn synthetic noise, we showed that large multilingual MT models and LLMs are far more robust than older techniques.\nThe experiments on social media translation showed that the same larger models also worked better on natural noise.\nWe added further support for this conclusion through reference-free translation experiments with a novel evaluation metric based on quality estimation.\nFinally, we exhibited circumstances in which pipeline-based source correction techniques can improve performance on noisy text, both synthetic and natural."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T1.8.8.9\">Model</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1\">ende</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.2.2.2\">deen</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.3.3.3\">enfr</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.4.4.4\">fren</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.5.5.5\">enko</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.6.6.6\">koen</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.7.7.7\">enpt</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.8.8.8\">pten</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.8.9.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.8.9.1.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.8.9.1.1.1\">OPUS</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.8.9.1.2\">84.61</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.8.9.1.3\">88.24</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.8.9.1.4\">87.74</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.8.9.1.5\">89.21</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.8.9.1.6\">86.60</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.8.9.1.7\">86.35</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.8.9.1.8\">88.98</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T1.8.9.1.9\">88.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.8.10.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.8.10.2.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.8.10.2.1.1\">NLLB</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.10.2.2\">87.82</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.10.2.3\">89.31</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.10.2.4\">88.11</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.10.2.5\">89.25</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.10.2.6\">88.51</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.10.2.7\">87.59</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.10.2.8\">89.23</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.10.2.9\">89.54</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.8.11.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T1.8.11.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.8.11.3.1.1\">Tower</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.11.3.2\">88.21</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.11.3.3\">89.62</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.11.3.4\">88.79</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.11.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.11.3.5.1\">89.63</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.11.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.11.3.6.1\">89.33</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.11.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.11.3.7.1\">88.39</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.11.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.11.3.8.1\">89.83</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T1.8.11.3.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.11.3.9.1\">89.98</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.8.12.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T1.8.12.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T1.8.12.4.1.1\">GPT-3.5</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.8.12.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.12.4.2.1\">88.52</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.8.12.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.12.4.3.1\">89.64</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.8.12.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.12.4.4.1\">88.83</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.8.12.4.5\">89.45</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.8.12.4.6\">89.04</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.8.12.4.7\">87.98</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.8.12.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.12.4.8.1\">89.83</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T1.8.12.4.9\">89.81</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Baseline COMET scores for FLORES with no added noise.</figcaption>\n</figure>",
            "capture": "Table 1: Baseline COMET scores for FLORES with no added noise."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.8.9.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"S3.T2.8.9.1.1\">LP</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"S3.T2.8.9.1.2\">HF Path</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S3.T2.8.9.1.3\">Vocab</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S3.T2.8.9.1.4\">Params</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.1\">ende</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.2.1\">opus-mt-en-de</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T2.1.1.3\">58.1k</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T2.1.1.4\">74.4M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.2.2.1\">deen</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.2.2.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.2.2.2.1\">opus-mt-de-en</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.2.2.3\">58.1k</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.2.2.4\">74.4M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.3.1\">enfr</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.3.3.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.3.3.2.1\">opus-mt-tc-big-en-fr</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.3.3.3\">53.0k</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.3.3.4\">232.7M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.4.4.1\">fren</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.4.4.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.4.4.2.1\">opus-mt-tc-big-fr-en</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.4.4.3\">53.0k</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.4.4.4\">232.7M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.5.5.1\">enko</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.5.5.2\">-</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.5.5.3\">32.0k*</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.5.5.4\">209.2M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.6.6.1\">koen</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.6.6.2\">-</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.6.6.3\">32.0k*</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.6.6.4\">209.2M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.7.7.1\">enpt</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.7.7.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.7.7.2.1\">opus-mt-tc-big-en-pt</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.7.7.3\">54.7k</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.7.7.4\">234.5M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.8.8.1\">pten</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.8.8.2\">-</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.8.8.3\">60.0k</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T2.8.8.4\">75.4M</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T2.13.1\">OPUS</span> models. Each path is preceded by <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.14.2\">Helsinki-NLP/</span>. *enko use separate 32k source and target vocabularies. All others use shared vocabularies.</figcaption>\n</figure>",
            "capture": "Table 2: OPUS models. Each path is preceded by Helsinki-NLP/. *enko use separate 32k source and target vocabularies. All others use shared vocabularies."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell\">\n<p class=\"ltx_p ltx_align_center\" id=\"S3.T3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.8.8\" style=\"font-size:90%;\">swap\n<br class=\"ltx_break\"/>\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.8.8.8\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S3.T3.8.8.8.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T3.8.8.8.8.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.8.9.1\">Model</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.1.1.1.1.1\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.1.1.1.1.1.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.1.1.1.1.1.2\">de</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.2.2.2.2.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.2.2.2.2.2.1\">de</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.2.2.2.2.2.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.3.3.3.3.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.3.3.3.3.3.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.3.3.3.3.3.2\">fr</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.4.4.4.4.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.4.4.4.4.4.1\">fr</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.4.4.4.4.4.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.5.5.5.5.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.5.5.5.5.5.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.5.5.5.5.5.2\">ko</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.6.6.6.6.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.6.6.6.6.6.1\">ko</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.6.6.6.6.6.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.7.7.7.7.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.7.7.7.7.7.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.7.7.7.7.7.2\">pt</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.8.8.8.8.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.8.8.1\">pt</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.8.8.2\">en</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S3.T3.8.8.8.9.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.8.8.8.9.1.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.8.8.8.9.1.1.1\">OPUS</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.8.8.8.9.1.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.9.1.2.1\">-73.61</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.8.8.8.9.1.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.9.1.3.1\">-65.80</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.8.8.8.9.1.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.9.1.4.1\">-71.02</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.8.8.8.9.1.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.9.1.5.1\">-66.00</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.8.8.8.9.1.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.9.1.6.1\">-74.02</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.8.8.8.9.1.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.9.1.7.1\">-50.02</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.8.8.8.9.1.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.9.1.8.1\">-73.35</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.8.8.8.9.1.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.9.1.9.1\">-64.01</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.8.8.8.10.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.8.8.8.10.2.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.8.8.8.10.2.1.1\">NLLB</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.10.2.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.10.2.2.1\">-22.45</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.10.2.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.10.2.3.1\">-18.20</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.10.2.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.10.2.4.1\">-21.81</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.10.2.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.10.2.5.1\">-20.75</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.10.2.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.10.2.6.1\">-19.65</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.10.2.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.10.2.7.1\">-22.89</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.10.2.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.10.2.8.1\">-21.03</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.10.2.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.10.2.9.1\">-20.34</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.8.8.8.11.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.8.8.8.11.3.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.8.8.8.11.3.1.1\">Tower</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.11.3.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.11.3.2.1\">-19.42</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.11.3.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.11.3.3.1\">-28.54</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.11.3.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.11.3.4.1\">-18.70</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.11.3.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.11.3.5.1\">-27.64</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.11.3.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.11.3.6.1\">-18.48</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.11.3.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.11.3.7.1\">-26.16</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.11.3.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.11.3.8.1\">-17.39</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.8.8.8.11.3.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.8.11.3.9.1\">-28.79</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.8.8.8.12.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.8.8.8.12.4.1.1\">GPT-3.5</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.2\"><span class=\"ltx_text\" id=\"S3.T3.8.8.8.12.4.2.1\">-3.89</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.3\"><span class=\"ltx_text\" id=\"S3.T3.8.8.8.12.4.3.1\">-4.36</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.4\"><span class=\"ltx_text\" id=\"S3.T3.8.8.8.12.4.4.1\">-4.46</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.5\"><span class=\"ltx_text\" id=\"S3.T3.8.8.8.12.4.5.1\">-5.85</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.6\"><span class=\"ltx_text\" id=\"S3.T3.8.8.8.12.4.6.1\">-4.79</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.7\"><span class=\"ltx_text\" id=\"S3.T3.8.8.8.12.4.7.1\">-20.89</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.8\"><span class=\"ltx_text\" id=\"S3.T3.8.8.8.12.4.8.1\">-3.76</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.8.8.8.12.4.9\"><span class=\"ltx_text\" id=\"S3.T3.8.8.8.12.4.9.1\">-6.78</span></span></span>\n</span>\n</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.8.8.9\"></span></span></p>\n</div>\n<div class=\"ltx_flex_cell\">\n<p class=\"ltx_p ltx_align_center\" id=\"S3.T3.16\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.16.8\" style=\"font-size:90%;\">chardrop\n<br class=\"ltx_break\"/>\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.16.8.8\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S3.T3.16.8.8.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T3.16.8.8.8.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.8.9.1\">Model</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.9.1.1.1.1\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.9.1.1.1.1.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.9.1.1.1.1.2\">de</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.10.2.2.2.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.10.2.2.2.2.1\">de</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.10.2.2.2.2.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.11.3.3.3.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.11.3.3.3.3.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.11.3.3.3.3.2\">fr</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.12.4.4.4.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.12.4.4.4.4.1\">fr</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.12.4.4.4.4.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.13.5.5.5.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.13.5.5.5.5.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.13.5.5.5.5.2\">ko</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.14.6.6.6.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.14.6.6.6.6.1\">ko</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.14.6.6.6.6.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.15.7.7.7.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.15.7.7.7.7.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.15.7.7.7.7.2\">pt</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.16.8.8.8.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.8.8.1\">pt</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.8.8.2\">en</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S3.T3.16.8.8.9.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.16.8.8.9.1.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.16.8.8.9.1.1.1\">OPUS</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.16.8.8.9.1.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.9.1.2.1\">-69.40</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.16.8.8.9.1.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.9.1.3.1\">-54.17</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.16.8.8.9.1.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.9.1.4.1\">-64.33</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.16.8.8.9.1.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.9.1.5.1\">-50.72</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.16.8.8.9.1.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.9.1.6.1\">-71.37</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.16.8.8.9.1.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.9.1.7.1\">-44.14</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.16.8.8.9.1.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.9.1.8.1\">-70.13</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.16.8.8.9.1.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.9.1.9.1\">-53.15</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.16.8.8.10.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.16.8.8.10.2.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.16.8.8.10.2.1.1\">NLLB</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.10.2.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.10.2.2.1\">-22.13</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.10.2.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.10.2.3.1\">-16.51</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.10.2.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.10.2.4.1\">-21.56</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.10.2.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.10.2.5.1\">-17.10</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.10.2.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.10.2.6.1\">-18.33</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.10.2.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.10.2.7.1\">-20.81</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.10.2.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.10.2.8.1\">-20.89</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.10.2.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.10.2.9.1\">-18.52</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.16.8.8.11.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.16.8.8.11.3.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.16.8.8.11.3.1.1\">Tower</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.11.3.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.11.3.2.1\">-18.42</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.11.3.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.11.3.3.1\">-19.34</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.11.3.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.11.3.4.1\">-18.64</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.11.3.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.11.3.5.1\">-17.93</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.11.3.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.11.3.6.1\">-15.19</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.11.3.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.11.3.7.1\">-24.62</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.11.3.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.11.3.8.1\">-17.89</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.16.8.8.11.3.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.8.11.3.9.1\">-20.80</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.16.8.8.12.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.16.8.8.12.4.1.1\">GPT-3.5</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.2\"><span class=\"ltx_text\" id=\"S3.T3.16.8.8.12.4.2.1\">-6.59</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.3\"><span class=\"ltx_text\" id=\"S3.T3.16.8.8.12.4.3.1\">-6.55</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.4\"><span class=\"ltx_text\" id=\"S3.T3.16.8.8.12.4.4.1\">-7.32</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.5\"><span class=\"ltx_text\" id=\"S3.T3.16.8.8.12.4.5.1\">-5.68</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.6\"><span class=\"ltx_text\" id=\"S3.T3.16.8.8.12.4.6.1\">-6.72</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.7\"><span class=\"ltx_text\" id=\"S3.T3.16.8.8.12.4.7.1\">-17.81</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.8\"><span class=\"ltx_text\" id=\"S3.T3.16.8.8.12.4.8.1\">-6.63</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.16.8.8.12.4.9\"><span class=\"ltx_text\" id=\"S3.T3.16.8.8.12.4.9.1\">-7.09</span></span></span>\n</span>\n</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.16.8.9\">\n</span></span></p>\n</div>\n<div class=\"ltx_flex_cell\">\n<p class=\"ltx_p ltx_align_center\" id=\"S3.T3.24\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.24.8\" style=\"font-size:90%;\">chardupe\n<br class=\"ltx_break\"/>\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.24.8.8\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S3.T3.24.8.8.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T3.24.8.8.8.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.8.9.1\">Model</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.17.1.1.1.1\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.17.1.1.1.1.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.17.1.1.1.1.2\">de</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.18.2.2.2.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.18.2.2.2.2.1\">de</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.18.2.2.2.2.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.19.3.3.3.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.19.3.3.3.3.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.19.3.3.3.3.2\">fr</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.20.4.4.4.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.20.4.4.4.4.1\">fr</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.20.4.4.4.4.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.21.5.5.5.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.21.5.5.5.5.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.21.5.5.5.5.2\">ko</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.22.6.6.6.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.22.6.6.6.6.1\">ko</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.22.6.6.6.6.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.23.7.7.7.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.23.7.7.7.7.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.23.7.7.7.7.2\">pt</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.24.8.8.8.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.8.8.1\">pt</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.8.8.2\">en</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S3.T3.24.8.8.9.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.24.8.8.9.1.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.24.8.8.9.1.1.1\">OPUS</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.24.8.8.9.1.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.9.1.2.1\">-56.35</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.24.8.8.9.1.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.9.1.3.1\">-35.17</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.24.8.8.9.1.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.9.1.4.1\">-47.42</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.24.8.8.9.1.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.9.1.5.1\">-33.00</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.24.8.8.9.1.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.9.1.6.1\">-65.89</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.24.8.8.9.1.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.9.1.7.1\">-31.11</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.24.8.8.9.1.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.9.1.8.1\">-58.98</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.24.8.8.9.1.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.9.1.9.1\">-35.65</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.24.8.8.10.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.24.8.8.10.2.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.24.8.8.10.2.1.1\">NLLB</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.10.2.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.10.2.2.1\">-3.73</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.10.2.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.10.2.3.1\">-4.02</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.10.2.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.10.2.4.1\">-3.60</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.10.2.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.10.2.5.1\">-5.36</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.10.2.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.10.2.6.1\">-2.82</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.10.2.7\"><span class=\"ltx_text\" id=\"S3.T3.24.8.8.10.2.7.1\">-4.19</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.10.2.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.10.2.8.1\">-3.76</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.10.2.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.10.2.9.1\">-4.27</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.24.8.8.11.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.24.8.8.11.3.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.24.8.8.11.3.1.1\">Tower</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.11.3.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.11.3.2.1\">-3.21</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.11.3.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.11.3.3.1\">-4.13</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.11.3.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.11.3.4.1\">-2.88</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.11.3.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.11.3.5.1\">-4.39</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.11.3.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.11.3.6.1\">-3.70</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.11.3.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.11.3.7.1\">-7.66</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.11.3.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.11.3.8.1\">-2.79</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.24.8.8.11.3.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.11.3.9.1\">-3.76</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.24.8.8.12.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.24.8.8.12.4.1.1\">GPT-3.5</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.2\"><span class=\"ltx_text\" id=\"S3.T3.24.8.8.12.4.2.1\">-1.14</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.3\"><span class=\"ltx_text\" id=\"S3.T3.24.8.8.12.4.3.1\">-1.36</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.4\"><span class=\"ltx_text\" id=\"S3.T3.24.8.8.12.4.4.1\">-1.32</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.5\"><span class=\"ltx_text\" id=\"S3.T3.24.8.8.12.4.5.1\">-1.42</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.6\"><span class=\"ltx_text\" id=\"S3.T3.24.8.8.12.4.6.1\">-1.42</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.8.12.4.7.1\">-5.64</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.8\"><span class=\"ltx_text\" id=\"S3.T3.24.8.8.12.4.8.1\">-0.98</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.24.8.8.12.4.9\"><span class=\"ltx_text\" id=\"S3.T3.24.8.8.12.4.9.1\">-1.44</span></span></span>\n</span>\n</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.24.8.9\"></span></span></p>\n</div>\n<div class=\"ltx_flex_cell\">\n<p class=\"ltx_p ltx_align_center\" id=\"S3.T3.32\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.32.8\" style=\"font-size:90%;\">key\n<br class=\"ltx_break\"/>\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.32.8.8\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S3.T3.32.8.8.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T3.32.8.8.8.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.8.9.1\">Model</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.25.1.1.1.1\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.25.1.1.1.1.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.25.1.1.1.1.2\">de</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.26.2.2.2.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.26.2.2.2.2.1\">de</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.26.2.2.2.2.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.27.3.3.3.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.27.3.3.3.3.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.27.3.3.3.3.2\">fr</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.28.4.4.4.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.28.4.4.4.4.1\">fr</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.28.4.4.4.4.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.29.5.5.5.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.29.5.5.5.5.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.29.5.5.5.5.2\">ko</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.30.6.6.6.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.30.6.6.6.6.1\">ko</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.30.6.6.6.6.2\">en</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.31.7.7.7.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.31.7.7.7.7.1\">en</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.31.7.7.7.7.2\">pt</span></span>\n<span class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.32.8.8.8.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.8.8.1\">pt</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.8.8.2\">en</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S3.T3.32.8.8.9.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.32.8.8.9.1.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.32.8.8.9.1.1.1\">OPUS</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.32.8.8.9.1.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.9.1.2.1\">-74.13</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.32.8.8.9.1.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.9.1.3.1\">-64.46</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.32.8.8.9.1.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.9.1.4.1\">-73.35</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.32.8.8.9.1.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.9.1.5.1\">-66.05</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.32.8.8.9.1.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.9.1.6.1\">-76.68</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.32.8.8.9.1.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.9.1.7.1\">-38.50</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.32.8.8.9.1.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.9.1.8.1\">-76.20</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.32.8.8.9.1.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.9.1.9.1\">-66.02</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.32.8.8.10.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.32.8.8.10.2.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.32.8.8.10.2.1.1\">NLLB</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.10.2.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.10.2.2.1\">-26.20</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.10.2.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.10.2.3.1\">-20.17</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.10.2.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.10.2.4.1\">-25.07</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.10.2.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.10.2.5.1\">-21.80</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.10.2.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.10.2.6.1\">-23.69</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.10.2.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.10.2.7.1\">-26.41</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.10.2.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.10.2.8.1\">-24.25</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.10.2.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.10.2.9.1\">-23.54</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.32.8.8.11.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.32.8.8.11.3.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.32.8.8.11.3.1.1\">Tower</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.11.3.2\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.11.3.2.1\">-28.64</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.11.3.3\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.11.3.3.1\">-29.49</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.11.3.4\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.11.3.4.1\">-27.13</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.11.3.5\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.11.3.5.1\">-31.89</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.11.3.6\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.11.3.6.1\">-25.89</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.11.3.7\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.11.3.7.1\">-23.80</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.11.3.8\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.11.3.8.1\">-26.33</span></span>\n<span class=\"ltx_td ltx_align_right\" id=\"S3.T3.32.8.8.11.3.9\"><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.8.11.3.9.1\">-36.55</span></span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.32.8.8.12.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.1\"><span class=\"ltx_text ltx_font_medium ltx_font_smallcaps\" id=\"S3.T3.32.8.8.12.4.1.1\">GPT-3.5</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.2\"><span class=\"ltx_text\" id=\"S3.T3.32.8.8.12.4.2.1\">-8.19</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.3\"><span class=\"ltx_text\" id=\"S3.T3.32.8.8.12.4.3.1\">-9.17</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.4\"><span class=\"ltx_text\" id=\"S3.T3.32.8.8.12.4.4.1\">-8.17</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.5\"><span class=\"ltx_text\" id=\"S3.T3.32.8.8.12.4.5.1\">-8.63</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.6\"><span class=\"ltx_text\" id=\"S3.T3.32.8.8.12.4.6.1\">-8.91</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.7\"><span class=\"ltx_text\" id=\"S3.T3.32.8.8.12.4.7.1\">-16.31</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.8\"><span class=\"ltx_text\" id=\"S3.T3.32.8.8.12.4.8.1\">-7.78</span></span>\n<span class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.32.8.8.12.4.9\"><span class=\"ltx_text\" id=\"S3.T3.32.8.8.12.4.9.1\">-10.27</span></span></span>\n</span>\n</span><span class=\"ltx_text ltx_font_medium\" id=\"S3.T3.32.8.9\"></span></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>COMET-slope on the FLORES devtest set.</figcaption>\n</figure>",
            "capture": "Table 3: COMET-slope on the FLORES devtest set."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T4.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T4.3.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T4.3.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T4.3.1.1.2\">English</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T4.3.1.1.3\">Portuguese</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T4.3.1.1.4\">Korean</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.3.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T4.3.2.2.1\"></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S3.T4.3.2.2.2\">clean</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S3.T4.3.2.2.3\">key</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S3.T4.3.2.2.4\">clean</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S3.T4.3.2.2.5\">key</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S3.T4.3.2.2.6\">clean</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S3.T4.3.2.2.7\">key</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T4.3.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.3.3.1.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T4.3.3.1.1.1\">OPUS</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.3.3.1.2\">1.25</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.3.3.1.3\">2.71</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.3.3.1.4\">1.29</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.3.3.1.5\">2.62</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.3.3.1.6\">1.75</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.3.3.1.7\">2.93</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.3.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.3.4.2.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T4.3.4.2.1.1\">NLLB</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.4.2.2\">1.40</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.4.2.3\">2.20</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.4.2.4\">1.53</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.4.2.5\">2.21</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.4.2.6\">2.03</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.4.2.7\">3.02</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.3.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.3.5.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T4.3.5.3.1.1\">Tower</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.5.3.2\">1.42</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.5.3.3\">2.47</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.5.3.4\">1.88</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.5.3.5\">2.55</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.5.3.6\">6.37</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.3.5.3.7\">7.44</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.3.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T4.3.6.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T4.3.6.4.1.1\">GPT-3.5</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T4.3.6.4.2\">1.24</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T4.3.6.4.3\">2.23</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T4.3.6.4.4\">1.71</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T4.3.6.4.5\">2.34</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T4.3.6.4.6\">4.17</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T4.3.6.4.7\">5.12</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Tokenizer fertility with clean and key-perturbed data. For English, we used the enfr <span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T4.5.1\">OPUS</span> model.</figcaption>\n</figure>",
            "capture": "Table 4: Tokenizer fertility with clean and key-perturbed data. For English, we used the enfr OPUS model."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T5.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T5.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T5.1.1.1.2\"><span class=\"ltx_text\" id=\"S3.T5.1.1.1.2.1\" style=\"font-size:90%;\">swap</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T5.1.1.1.3\"><span class=\"ltx_text\" id=\"S3.T5.1.1.1.3.1\" style=\"font-size:90%;\">chardrop</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T5.1.1.1.4\"><span class=\"ltx_text\" id=\"S3.T5.1.1.1.4.1\" style=\"font-size:90%;\">chardupe</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T5.1.1.1.5\"><span class=\"ltx_text\" id=\"S3.T5.1.1.1.5.1\" style=\"font-size:90%;\">key</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T5.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T5.1.2.1.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T5.1.2.1.1.1\" style=\"font-size:90%;\">OPUS</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T5.1.2.1.2\"><span class=\"ltx_text\" id=\"S3.T5.1.2.1.2.1\" style=\"font-size:90%;\">21.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T5.1.2.1.3\"><span class=\"ltx_text\" id=\"S3.T5.1.2.1.3.1\" style=\"font-size:90%;\">27.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T5.1.2.1.4\"><span class=\"ltx_text\" id=\"S3.T5.1.2.1.4.1\" style=\"font-size:90%;\">36.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T5.1.2.1.5\"><span class=\"ltx_text\" id=\"S3.T5.1.2.1.5.1\" style=\"font-size:90%;\">33.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T5.1.3.2.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T5.1.3.2.1.1\" style=\"font-size:90%;\">NLLB</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T5.1.3.2.2\"><span class=\"ltx_text\" id=\"S3.T5.1.3.2.2.1\" style=\"font-size:90%;\">27.6</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T5.1.3.2.3\"><span class=\"ltx_text\" id=\"S3.T5.1.3.2.3.1\" style=\"font-size:90%;\">35.1</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T5.1.3.2.4\"><span class=\"ltx_text\" id=\"S3.T5.1.3.2.4.1\" style=\"font-size:90%;\">45.7</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T5.1.3.2.5\"><span class=\"ltx_text\" id=\"S3.T5.1.3.2.5.1\" style=\"font-size:90%;\">42.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T5.1.4.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T5.1.4.3.1.1\" style=\"font-size:90%;\">Tower</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T5.1.4.3.2\"><span class=\"ltx_text\" id=\"S3.T5.1.4.3.2.1\" style=\"font-size:90%;\">50.0</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T5.1.4.3.3\"><span class=\"ltx_text\" id=\"S3.T5.1.4.3.3.1\" style=\"font-size:90%;\">62.4</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T5.1.4.3.4\"><span class=\"ltx_text\" id=\"S3.T5.1.4.3.4.1\" style=\"font-size:90%;\">74.8</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T5.1.4.3.5\"><span class=\"ltx_text\" id=\"S3.T5.1.4.3.5.1\" style=\"font-size:90%;\">71.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T5.1.5.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.T5.1.5.4.1.1\" style=\"font-size:90%;\">GPT-3.5</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T5.1.5.4.2\"><span class=\"ltx_text\" id=\"S3.T5.1.5.4.2.1\" style=\"font-size:90%;\">39.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T5.1.5.4.3\"><span class=\"ltx_text\" id=\"S3.T5.1.5.4.3.1\" style=\"font-size:90%;\">52.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T5.1.5.4.4\"><span class=\"ltx_text\" id=\"S3.T5.1.5.4.4.1\" style=\"font-size:90%;\">65.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T5.1.5.4.5\"><span class=\"ltx_text\" id=\"S3.T5.1.5.4.5.1\" style=\"font-size:90%;\">62.8</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>F1 between clean Korean token sequences and their 100% noisy counterparts.</figcaption>\n</figure>",
            "capture": "Table 5: F1 between clean Korean token sequences and their 100% noisy counterparts."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.4.4\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T6.4.4.5\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T6.1.1.1\">\n<span class=\"ltx_text\" id=\"S4.T6.1.1.1.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S4.T6.1.1.1.2\" style=\"font-size:90%;\">de</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T6.2.2.2\">\n<span class=\"ltx_text\" id=\"S4.T6.2.2.2.1\" style=\"font-size:90%;\">de</span><span class=\"ltx_text\" id=\"S4.T6.2.2.2.2\" style=\"font-size:90%;\">en</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T6.3.3.3\">\n<span class=\"ltx_text\" id=\"S4.T6.3.3.3.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S4.T6.3.3.3.2\" style=\"font-size:90%;\">es</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T6.4.4.4\">\n<span class=\"ltx_text\" id=\"S4.T6.4.4.4.1\" style=\"font-size:90%;\">es</span><span class=\"ltx_text\" id=\"S4.T6.4.4.4.2\" style=\"font-size:90%;\">en</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T6.8.8.5\"><span class=\"ltx_text\" id=\"S4.T6.8.8.5.1\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.6\"><span class=\"ltx_text\" id=\"S4.T6.8.8.6.1\" style=\"font-size:90%;\">FB</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.7\"><span class=\"ltx_text\" id=\"S4.T6.8.8.7.1\" style=\"font-size:90%;\">FC</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\" id=\"S4.T6.5.5.1\">\n<span class=\"ltx_text\" id=\"S4.T6.5.5.1.1\" style=\"font-size:90%;\">QE</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.8\"><span class=\"ltx_text\" id=\"S4.T6.8.8.8.1\" style=\"font-size:90%;\">FB</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.9\"><span class=\"ltx_text\" id=\"S4.T6.8.8.9.1\" style=\"font-size:90%;\">FC</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\" id=\"S4.T6.6.6.2\">\n<span class=\"ltx_text\" id=\"S4.T6.6.6.2.1\" style=\"font-size:90%;\">QE</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.10\"><span class=\"ltx_text\" id=\"S4.T6.8.8.10.1\" style=\"font-size:90%;\">FB</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.11\"><span class=\"ltx_text\" id=\"S4.T6.8.8.11.1\" style=\"font-size:90%;\">FC</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\" id=\"S4.T6.7.7.3\">\n<span class=\"ltx_text\" id=\"S4.T6.7.7.3.1\" style=\"font-size:90%;\">QE</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.12\"><span class=\"ltx_text\" id=\"S4.T6.8.8.12.1\" style=\"font-size:90%;\">FB</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.13\"><span class=\"ltx_text\" id=\"S4.T6.8.8.13.1\" style=\"font-size:90%;\">FC</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" id=\"S4.T6.8.8.4\">\n<span class=\"ltx_text\" id=\"S4.T6.8.8.4.1\" style=\"font-size:90%;\">QE</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.8.9.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T6.8.9.1.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.8.9.1.1.1\" style=\"font-size:90%;\">OPUS</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.2\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.2.1\" style=\"font-size:90%;\">82.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.3\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.3.1\" style=\"font-size:90%;\">81.81</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T6.8.9.1.4\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.4.1\" style=\"font-size:90%;\">6.54</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.5\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.5.1\" style=\"font-size:90%;\">75.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.6\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.6.1\" style=\"font-size:90%;\">88.44</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T6.8.9.1.7\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.7.1\" style=\"font-size:90%;\">4.04</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.8\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.8.1\" style=\"font-size:90%;\">85.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.9\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.9.1\" style=\"font-size:90%;\">84.23</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T6.8.9.1.10\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.10.1\" style=\"font-size:90%;\">4.51</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.11\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.11.1\" style=\"font-size:90%;\">86.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.12\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.12.1\" style=\"font-size:90%;\">88.28</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.8.9.1.13\"><span class=\"ltx_text\" id=\"S4.T6.8.9.1.13.1\" style=\"font-size:90%;\">2.80</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.10.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T6.8.10.2.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.8.10.2.1.1\" style=\"font-size:90%;\">NLLB</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.2\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.2.1\" style=\"font-size:90%;\">86.1</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.3\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.3.1\" style=\"font-size:90%;\">86.86</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T6.8.10.2.4\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.4.1\" style=\"font-size:90%;\">2.73</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.5\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.5.1\" style=\"font-size:90%;\">75.8</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.6\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.6.1\" style=\"font-size:90%;\">87.54</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T6.8.10.2.7\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.7.1\" style=\"font-size:90%;\">2.70</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.8\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.8.1\" style=\"font-size:90%;\">88.2</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.9\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.9.1\" style=\"font-size:90%;\">88.45</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T6.8.10.2.10\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.10.1\" style=\"font-size:90%;\">2.16</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.10.2.11.1\" style=\"font-size:90%;\">88.0</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.12\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.12.1\" style=\"font-size:90%;\">89.11</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.10.2.13\"><span class=\"ltx_text\" id=\"S4.T6.8.10.2.13.1\" style=\"font-size:90%;\">2.39</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.11.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T6.8.11.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.8.11.3.1.1\" style=\"font-size:90%;\">Tower</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.11.3.2.1\" style=\"font-size:90%;\">87.3</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.11.3.3.1\" style=\"font-size:90%;\">88.65</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T6.8.11.3.4\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.4.1\" style=\"font-size:90%;\">1.74</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.5\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.5.1\" style=\"font-size:90%;\">74.6</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.6\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.6.1\" style=\"font-size:90%;\">89.85</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T6.8.11.3.7\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.7.1\" style=\"font-size:90%;\">2.18</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.8\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.8.1\" style=\"font-size:90%;\">87.9</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.9\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.9.1\" style=\"font-size:90%;\">89.25</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T6.8.11.3.10\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.10.1\" style=\"font-size:90%;\">1.50</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.11\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.11.1\" style=\"font-size:90%;\">85.5</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.12\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.12.1\" style=\"font-size:90%;\">88.96</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.8.11.3.13\"><span class=\"ltx_text\" id=\"S4.T6.8.11.3.13.1\" style=\"font-size:90%;\">1.98</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.8.12.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T6.8.12.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T6.8.12.4.1.1\" style=\"font-size:90%;\">GPT-3.5</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.2\"><span class=\"ltx_text\" id=\"S4.T6.8.12.4.2.1\" style=\"font-size:90%;\">87.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.3\"><span class=\"ltx_text\" id=\"S4.T6.8.12.4.3.1\" style=\"font-size:90%;\">88.45</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T6.8.12.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.4.1\" style=\"font-size:90%;\">1.15</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.5.1\" style=\"font-size:90%;\">83.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.6.1\" style=\"font-size:90%;\">91.86</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T6.8.12.4.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.7.1\" style=\"font-size:90%;\">1.32</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.8.1\" style=\"font-size:90%;\">89.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.9.1\" style=\"font-size:90%;\">90.14</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T6.8.12.4.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.10.1\" style=\"font-size:90%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.11\"><span class=\"ltx_text\" id=\"S4.T6.8.12.4.11.1\" style=\"font-size:90%;\">87.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.12\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.12.1\" style=\"font-size:90%;\">91.23</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.8.12.4.13\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.8.12.4.13.1\" style=\"font-size:90%;\">0.91</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>MultiLexNorm results. FB is faux-BLEU and FC is faux-COMET.</figcaption>\n</figure>",
            "capture": "Table 6: MultiLexNorm results. FB is faux-BLEU and FC is faux-COMET."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T7.4\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T7.4.5.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T7.4.5.1.1\"></th>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T7.4.5.1.2\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T7.4.5.1.2.1\">OPUS</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T7.4.5.1.3\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T7.4.5.1.3.1\">NLLB</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T7.2.2.3\">Initial learning rate</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.2.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T7.4.4.3\">Batch size</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T7.3.3.1\"></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T7.4.4.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.4.6.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T7.4.6.2.1\">Batch type</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T7.4.6.2.2\">Tokens</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T7.4.6.2.3\">Sequences</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.4.7.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T7.4.7.3.1\">#A6000 GPUs</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T7.4.7.3.2\">1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T7.4.7.3.3\">4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.4.8.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T7.4.8.4.1\">Max training</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T7.4.8.4.2\">20k steps</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T7.4.8.4.3\">3 epochs</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Hyperparameters for finetuning MTNT.</figcaption>\n</figure>",
            "capture": "Table 7: Hyperparameters for finetuning MTNT."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T8.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T8.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T8.2.2.3\">Method</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1\">enfr</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.2.2.2\">fren</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T8.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T8.2.3.1.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.2.3.1.1.1\">OPUS</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T8.2.3.1.2\">77.51</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T8.2.3.1.3\">80.07</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.2.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T8.2.4.2.1\">+finetune</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T8.2.4.2.2\">79.37</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T8.2.4.2.3\">81.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T8.2.5.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.2.5.3.1.1\">NLLB</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T8.2.5.3.2\">79.20</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T8.2.5.3.3\">80.70</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T8.2.6.4.1\">+finetune</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T8.2.6.4.2\">80.85</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T8.2.6.4.3\">82.60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T8.2.7.5.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T8.2.7.5.1.1\">Tower</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T8.2.7.5.2\">80.65</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T8.2.7.5.3\">82.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T8.2.8.6.1\">ChatGPT</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T8.2.8.6.2\">81.33</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T8.2.8.6.3\">84.72</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>COMET on the MTNT test set.</figcaption>\n</figure>",
            "capture": "Table 8: COMET on the MTNT test set."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T9.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T9.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T9.1.1.1.1.1\" style=\"font-size:90%;\">Language</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T9.1.1.1.2\"><span class=\"ltx_text\" id=\"S4.T9.1.1.1.2.1\" style=\"font-size:90%;\">Sent.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T9.1.1.1.3\"><span class=\"ltx_text\" id=\"S4.T9.1.1.1.3.1\" style=\"font-size:90%;\">%Noisy</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T9.1.1.1.4\"><span class=\"ltx_text\" id=\"S4.T9.1.1.1.4.1\" style=\"font-size:90%;\">Reference</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T9.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.2.1.1\"><span class=\"ltx_text\" id=\"S4.T9.1.2.1.1.1\" style=\"font-size:90%;\">English</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T9.1.2.1.2\"><span class=\"ltx_text\" id=\"S4.T9.1.2.1.2.1\" style=\"font-size:90%;\">1967</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T9.1.2.1.3\"><span class=\"ltx_text\" id=\"S4.T9.1.2.1.3.1\" style=\"font-size:90%;\">6.9</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.2.1.4\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T9.1.2.1.4.1.1\" style=\"font-size:90%;\">(</span>Baldwin et\u00a0al.<span class=\"ltx_text\" id=\"S4.T9.1.2.1.4.2.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03923v1#bib.bib6\" title=\"\">2015</a><span class=\"ltx_text\" id=\"S4.T9.1.2.1.4.3.3\" style=\"font-size:90%;\">)</span></cite></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.3.2.1\"><span class=\"ltx_text\" id=\"S4.T9.1.3.2.1.1\" style=\"font-size:90%;\">German</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T9.1.3.2.2\"><span class=\"ltx_text\" id=\"S4.T9.1.3.2.2.1\" style=\"font-size:90%;\">583</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T9.1.3.2.3\"><span class=\"ltx_text\" id=\"S4.T9.1.3.2.3.1\" style=\"font-size:90%;\">8.9</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.3.2.4\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T9.1.3.2.4.1.1\" style=\"font-size:90%;\">(</span>Sidarenka et\u00a0al.<span class=\"ltx_text\" id=\"S4.T9.1.3.2.4.2.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03923v1#bib.bib38\" title=\"\">2013</a><span class=\"ltx_text\" id=\"S4.T9.1.3.2.4.3.3\" style=\"font-size:90%;\">)</span></cite></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T9.1.4.3.1\"><span class=\"ltx_text\" id=\"S4.T9.1.4.3.1.1\" style=\"font-size:90%;\">Spanish</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T9.1.4.3.2\"><span class=\"ltx_text\" id=\"S4.T9.1.4.3.2.1\" style=\"font-size:90%;\">531</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T9.1.4.3.3\"><span class=\"ltx_text\" id=\"S4.T9.1.4.3.3.1\" style=\"font-size:90%;\">7.7</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T9.1.4.3.4\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S4.T9.1.4.3.4.1.1\" style=\"font-size:90%;\">(</span>Alegria et\u00a0al.<span class=\"ltx_text\" id=\"S4.T9.1.4.3.4.2.2.1.1\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.03923v1#bib.bib1\" title=\"\">2013</a><span class=\"ltx_text\" id=\"S4.T9.1.4.3.4.3.3\" style=\"font-size:90%;\">)</span></cite></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Statistics of selected MultiLexNorm corpora.</figcaption>\n</figure>",
            "capture": "Table 9: Statistics of selected MultiLexNorm corpora."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T10\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T10.5\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T10.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T10.1.1.2\"><span class=\"ltx_text\" id=\"S5.T10.1.1.2.1\" style=\"font-size:90%;\">Initial learning rate</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S5.T10.1.1.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.2.2.2\"><span class=\"ltx_text\" id=\"S5.T10.2.2.2.1\" style=\"font-size:90%;\">Batch size</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T10.2.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.3.3.2\"><span class=\"ltx_text\" id=\"S5.T10.3.3.2.1\" style=\"font-size:90%;\">Max train steps</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T10.3.3.1\">\n<span class=\"ltx_text\" id=\"S5.T10.3.3.1.1\" style=\"font-size:90%;\">k</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T10.4.4.2\"><span class=\"ltx_text\" id=\"S5.T10.4.4.2.1\" style=\"font-size:90%;\">Validation steps</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T10.4.4.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T10.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T10.5.5.2\"><span class=\"ltx_text\" id=\"S5.T10.5.5.2.1\" style=\"font-size:90%;\">Patience</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T10.5.5.1\"></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Hyperparameters for mT5 correction models.</figcaption>\n</figure>",
            "capture": "Table 10: Hyperparameters for mT5 correction models."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T11\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T11.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T11.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T11.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T11.1.1.1.2\"><span class=\"ltx_text\" id=\"S5.T11.1.1.1.2.1\" style=\"font-size:90%;\">swap</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T11.1.1.1.3\"><span class=\"ltx_text\" id=\"S5.T11.1.1.1.3.1\" style=\"font-size:90%;\">chardrop</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T11.1.1.1.4\"><span class=\"ltx_text\" id=\"S5.T11.1.1.1.4.1\" style=\"font-size:90%;\">chardupe</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T11.1.1.1.5\"><span class=\"ltx_text\" id=\"S5.T11.1.1.1.5.1\" style=\"font-size:90%;\">key</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T11.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T11.1.2.1.1\"><span class=\"ltx_text\" id=\"S5.T11.1.2.1.1.1\" style=\"font-size:90%;\">None</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T11.1.2.1.2\"><span class=\"ltx_text\" id=\"S5.T11.1.2.1.2.1\" style=\"font-size:90%;\">40.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T11.1.2.1.3\"><span class=\"ltx_text\" id=\"S5.T11.1.2.1.3.1\" style=\"font-size:90%;\">47.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T11.1.2.1.4\"><span class=\"ltx_text\" id=\"S5.T11.1.2.1.4.1\" style=\"font-size:90%;\">68.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T11.1.2.1.5\"><span class=\"ltx_text\" id=\"S5.T11.1.2.1.5.1\" style=\"font-size:90%;\">45.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T11.1.3.2.1\"><span class=\"ltx_text\" id=\"S5.T11.1.3.2.1.1\" style=\"font-size:90%;\">JamSpell</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T11.1.3.2.2\"><span class=\"ltx_text\" id=\"S5.T11.1.3.2.2.1\" style=\"font-size:90%;\">75.5</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T11.1.3.2.3\"><span class=\"ltx_text\" id=\"S5.T11.1.3.2.3.1\" style=\"font-size:90%;\">63.5</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T11.1.3.2.4\"><span class=\"ltx_text\" id=\"S5.T11.1.3.2.4.1\" style=\"font-size:90%;\">91.7</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T11.1.3.2.5\"><span class=\"ltx_text\" id=\"S5.T11.1.3.2.5.1\" style=\"font-size:90%;\">78.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T11.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T11.1.4.3.1\"><span class=\"ltx_text\" id=\"S5.T11.1.4.3.1.1\" style=\"font-size:90%;\">mT5-Large</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T11.1.4.3.2\"><span class=\"ltx_text\" id=\"S5.T11.1.4.3.2.1\" style=\"font-size:90%;\">85.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T11.1.4.3.3\"><span class=\"ltx_text\" id=\"S5.T11.1.4.3.3.1\" style=\"font-size:90%;\">83.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T11.1.4.3.4\"><span class=\"ltx_text\" id=\"S5.T11.1.4.3.4.1\" style=\"font-size:90%;\">95.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T11.1.4.3.5\"><span class=\"ltx_text\" id=\"S5.T11.1.4.3.5.1\" style=\"font-size:90%;\">87.9</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>Performance by chrF of source correctors on errors in FLORES at the 100% noise level.</figcaption>\n</figure>",
            "capture": "Table 11: Performance by chrF of source correctors on errors in FLORES at the 100% noise level."
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T12\">\n<div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_flex_size_2 ltx_align_middle\" id=\"S5.T12.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T12.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T12.4.4.5\"><span class=\"ltx_text\" id=\"S5.T12.4.4.5.1\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T12.1.1.1\">\n<span class=\"ltx_text\" id=\"S5.T12.1.1.1.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S5.T12.1.1.1.2\" style=\"font-size:90%;\">de</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T12.2.2.2\">\n<span class=\"ltx_text\" id=\"S5.T12.2.2.2.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S5.T12.2.2.2.2\" style=\"font-size:90%;\">fr</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T12.3.3.3\">\n<span class=\"ltx_text\" id=\"S5.T12.3.3.3.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S5.T12.3.3.3.2\" style=\"font-size:90%;\">ko</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T12.4.4.4\">\n<span class=\"ltx_text\" id=\"S5.T12.4.4.4.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S5.T12.4.4.4.2\" style=\"font-size:90%;\">pt</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T12.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T12.4.5.1.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T12.4.5.1.1.1\" style=\"font-size:90%;\">OPUS</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T12.4.5.1.2\"><span class=\"ltx_text\" id=\"S5.T12.4.5.1.2.1\" style=\"font-size:90%;\">-0.44</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T12.4.5.1.3\"><span class=\"ltx_text\" id=\"S5.T12.4.5.1.3.1\" style=\"font-size:90%;\">-0.45</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T12.4.5.1.4\"><span class=\"ltx_text\" id=\"S5.T12.4.5.1.4.1\" style=\"font-size:90%;\">-0.30</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T12.4.5.1.5\"><span class=\"ltx_text\" id=\"S5.T12.4.5.1.5.1\" style=\"font-size:90%;\">-0.38</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.4.6.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T12.4.6.2.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T12.4.6.2.1.1\" style=\"font-size:90%;\">NLLB</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.4.6.2.2\"><span class=\"ltx_text\" id=\"S5.T12.4.6.2.2.1\" style=\"font-size:90%;\">-0.33</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.4.6.2.3\"><span class=\"ltx_text\" id=\"S5.T12.4.6.2.3.1\" style=\"font-size:90%;\">-0.39</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.4.6.2.4\"><span class=\"ltx_text\" id=\"S5.T12.4.6.2.4.1\" style=\"font-size:90%;\">-0.16</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.4.6.2.5\"><span class=\"ltx_text\" id=\"S5.T12.4.6.2.5.1\" style=\"font-size:90%;\">-0.37</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.4.7.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T12.4.7.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T12.4.7.3.1.1\" style=\"font-size:90%;\">Tower</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.4.7.3.2\"><span class=\"ltx_text\" id=\"S5.T12.4.7.3.2.1\" style=\"font-size:90%;\">-0.28</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.4.7.3.3\"><span class=\"ltx_text\" id=\"S5.T12.4.7.3.3.1\" style=\"font-size:90%;\">-0.30</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.4.7.3.4\"><span class=\"ltx_text\" id=\"S5.T12.4.7.3.4.1\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.4.7.3.5\"><span class=\"ltx_text\" id=\"S5.T12.4.7.3.5.1\" style=\"font-size:90%;\">-0.26</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.4.8.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T12.4.8.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T12.4.8.4.1.1\" style=\"font-size:90%;\">GPT-3.5</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T12.4.8.4.2\"><span class=\"ltx_text\" id=\"S5.T12.4.8.4.2.1\" style=\"font-size:90%;\">-0.30</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T12.4.8.4.3\"><span class=\"ltx_text\" id=\"S5.T12.4.8.4.3.1\" style=\"font-size:90%;\">0.32</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T12.4.8.4.4\"><span class=\"ltx_text\" id=\"S5.T12.4.8.4.4.1\" style=\"font-size:90%;\">-0.10</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T12.4.8.4.5\"><span class=\"ltx_text\" id=\"S5.T12.4.8.4.5.1\" style=\"font-size:90%;\">-0.32</span></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_flex_size_2 ltx_align_middle\" id=\"S5.T12.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T12.8.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T12.8.4.5\"><span class=\"ltx_text\" id=\"S5.T12.8.4.5.1\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T12.5.1.1\">\n<span class=\"ltx_text\" id=\"S5.T12.5.1.1.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S5.T12.5.1.1.2\" style=\"font-size:90%;\">de</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T12.6.2.2\">\n<span class=\"ltx_text\" id=\"S5.T12.6.2.2.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S5.T12.6.2.2.2\" style=\"font-size:90%;\">fr</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T12.7.3.3\">\n<span class=\"ltx_text\" id=\"S5.T12.7.3.3.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S5.T12.7.3.3.2\" style=\"font-size:90%;\">ko</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T12.8.4.4\">\n<span class=\"ltx_text\" id=\"S5.T12.8.4.4.1\" style=\"font-size:90%;\">en</span><span class=\"ltx_text\" id=\"S5.T12.8.4.4.2\" style=\"font-size:90%;\">pt</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T12.8.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T12.8.5.1.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T12.8.5.1.1.1\" style=\"font-size:90%;\">OPUS</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T12.8.5.1.2\"><span class=\"ltx_text\" id=\"S5.T12.8.5.1.2.1\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T12.8.5.1.3\"><span class=\"ltx_text\" id=\"S5.T12.8.5.1.3.1\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T12.8.5.1.4\"><span class=\"ltx_text\" id=\"S5.T12.8.5.1.4.1\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T12.8.5.1.5\"><span class=\"ltx_text\" id=\"S5.T12.8.5.1.5.1\" style=\"font-size:90%;\">-0.01</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.8.6.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T12.8.6.2.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T12.8.6.2.1.1\" style=\"font-size:90%;\">NLLB</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.8.6.2.2\"><span class=\"ltx_text\" id=\"S5.T12.8.6.2.2.1\" style=\"font-size:90%;\">-0.04</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.8.6.2.3\"><span class=\"ltx_text\" id=\"S5.T12.8.6.2.3.1\" style=\"font-size:90%;\">-0.03</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.8.6.2.4\"><span class=\"ltx_text\" id=\"S5.T12.8.6.2.4.1\" style=\"font-size:90%;\">-0.03</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.8.6.2.5\"><span class=\"ltx_text\" id=\"S5.T12.8.6.2.5.1\" style=\"font-size:90%;\">0.03</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.8.7.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T12.8.7.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T12.8.7.3.1.1\" style=\"font-size:90%;\">Tower</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.8.7.3.2\"><span class=\"ltx_text\" id=\"S5.T12.8.7.3.2.1\" style=\"font-size:90%;\">-0.04</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.8.7.3.3\"><span class=\"ltx_text\" id=\"S5.T12.8.7.3.3.1\" style=\"font-size:90%;\">-0.08</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.8.7.3.4\"><span class=\"ltx_text\" id=\"S5.T12.8.7.3.4.1\" style=\"font-size:90%;\">-0.03</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T12.8.7.3.5\"><span class=\"ltx_text\" id=\"S5.T12.8.7.3.5.1\" style=\"font-size:90%;\">-0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T12.8.8.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T12.8.8.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T12.8.8.4.1.1\" style=\"font-size:90%;\">GPT-3.5</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T12.8.8.4.2\"><span class=\"ltx_text\" id=\"S5.T12.8.8.4.2.1\" style=\"font-size:90%;\">-0.11</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T12.8.8.4.3\"><span class=\"ltx_text\" id=\"S5.T12.8.8.4.3.1\" style=\"font-size:90%;\">-0.08</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T12.8.8.4.4\"><span class=\"ltx_text\" id=\"S5.T12.8.8.4.4.1\" style=\"font-size:90%;\">0.07</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T12.8.8.4.5\"><span class=\"ltx_text\" id=\"S5.T12.8.8.4.5.1\" style=\"font-size:90%;\">-0.04</span></td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>The number of lost COMET points when preprocessing clean FLORES sources with JamSpell (top) or mT5-large (bottom).</figcaption>\n</figure>",
            "capture": "Table 12: The number of lost COMET points when preprocessing clean FLORES sources with JamSpell (top) or mT5-large (bottom)."
        },
        "13": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T13\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T13.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T13.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T13.1.1.1.1\"><span class=\"ltx_text\" id=\"S5.T13.1.1.1.1.1\" style=\"font-size:90%;\">Method</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S5.T13.1.1.1.2\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T13.1.1.1.2.1\" style=\"font-size:90%;\">OPUS</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S5.T13.1.1.1.3\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T13.1.1.1.3.1\" style=\"font-size:90%;\">NLLB</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S5.T13.1.1.1.4\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T13.1.1.1.4.1\" style=\"font-size:90%;\">Tower</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S5.T13.1.1.1.5\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T13.1.1.1.5.1\" style=\"font-size:90%;\">GPT-3.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T13.1.2.2.1\"><span class=\"ltx_text\" id=\"S5.T13.1.2.2.1.1\" style=\"font-size:90%;\">Base</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T13.1.2.2.2\"><span class=\"ltx_text\" id=\"S5.T13.1.2.2.2.1\" style=\"font-size:90%;\">77.51</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T13.1.2.2.3\"><span class=\"ltx_text\" id=\"S5.T13.1.2.2.3.1\" style=\"font-size:90%;\">79.20</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T13.1.2.2.4\"><span class=\"ltx_text\" id=\"S5.T13.1.2.2.4.1\" style=\"font-size:90%;\">80.65</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T13.1.2.2.5\"><span class=\"ltx_text\" id=\"S5.T13.1.2.2.5.1\" style=\"font-size:90%;\">81.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T13.1.3.3.1\"><span class=\"ltx_text\" id=\"S5.T13.1.3.3.1.1\" style=\"font-size:90%;\">Finetuned</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T13.1.3.3.2\"><span class=\"ltx_text\" id=\"S5.T13.1.3.3.2.1\" style=\"font-size:90%;\">79.37</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T13.1.3.3.3\"><span class=\"ltx_text\" id=\"S5.T13.1.3.3.3.1\" style=\"font-size:90%;\">80.85</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T13.1.3.3.4\"><span class=\"ltx_text\" id=\"S5.T13.1.3.3.4.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T13.1.3.3.5\"><span class=\"ltx_text\" id=\"S5.T13.1.3.3.5.1\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T13.1.4.4.1\"><span class=\"ltx_text\" id=\"S5.T13.1.4.4.1.1\" style=\"font-size:90%;\">JamSpell</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T13.1.4.4.2\"><span class=\"ltx_text\" id=\"S5.T13.1.4.4.2.1\" style=\"font-size:90%;\">76.42</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T13.1.4.4.3\"><span class=\"ltx_text\" id=\"S5.T13.1.4.4.3.1\" style=\"font-size:90%;\">78.09</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T13.1.4.4.4\"><span class=\"ltx_text\" id=\"S5.T13.1.4.4.4.1\" style=\"font-size:90%;\">79.17</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T13.1.4.4.5\"><span class=\"ltx_text\" id=\"S5.T13.1.4.4.5.1\" style=\"font-size:90%;\">79.68</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T13.1.5.5.1\"><span class=\"ltx_text\" id=\"S5.T13.1.5.5.1.1\" style=\"font-size:90%;\">mT5</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T13.1.5.5.2\"><span class=\"ltx_text\" id=\"S5.T13.1.5.5.2.1\" style=\"font-size:90%;\">76.41</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T13.1.5.5.3\"><span class=\"ltx_text\" id=\"S5.T13.1.5.5.3.1\" style=\"font-size:90%;\">77.88</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T13.1.5.5.4\"><span class=\"ltx_text\" id=\"S5.T13.1.5.5.4.1\" style=\"font-size:90%;\">79.34</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T13.1.5.5.5\"><span class=\"ltx_text\" id=\"S5.T13.1.5.5.5.1\" style=\"font-size:90%;\">80.16</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T13.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T13.1.6.6.1\"><span class=\"ltx_text\" id=\"S5.T13.1.6.6.1.1\" style=\"font-size:90%;\">Oracle</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T13.1.6.6.2\"><span class=\"ltx_text\" id=\"S5.T13.1.6.6.2.1\" style=\"font-size:90%;\">78.65</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T13.1.6.6.3\"><span class=\"ltx_text\" id=\"S5.T13.1.6.6.3.1\" style=\"font-size:90%;\">80.11</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T13.1.6.6.4\"><span class=\"ltx_text\" id=\"S5.T13.1.6.6.4.1\" style=\"font-size:90%;\">81.59</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T13.1.6.6.5\"><span class=\"ltx_text\" id=\"S5.T13.1.6.6.5.1\" style=\"font-size:90%;\">82.16</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span>COMET on MTNT with source correction. Models with correction use the base MT model, not the finetuned version. The oracle selects the best-scoring hypothesis between the baseline and the two correctors.</figcaption>\n</figure>",
            "capture": "Table 13: COMET on MTNT with source correction. Models with correction use the base MT model, not the finetuned version. The oracle selects the best-scoring hypothesis between the baseline and the two correctors."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.03923v1_figure_1.png",
            "caption": "Figure 1: COMET-22 score for English-French on the FLORES-200 devtest set as an increasing proportion of source tokens are noised by randomly swapping a pair of characters."
        },
        "2": {
            "figure_path": "2403.03923v1_figure_2.png",
            "caption": "Figure 2: COMET score plotted against the GPT-2 perplexity of predicted translations on en\u2192\u2192\\rightarrow\u2192fr with varying levels of swap noise for various models."
        },
        "3": {
            "figure_path": "2403.03923v1_figure_3.png",
            "caption": "Figure 3: COMET on en\u2192\u2192\\rightarrow\u2192fr swaps with models of various sizes. 0 noise is clean text, while 1.0 indicates that every token has been perturbed."
        },
        "4": {
            "figure_path": "2403.03923v1_figure_4.png",
            "caption": "Figure 4: \u0394\u0394\\Deltaroman_\u0394COMET compared to the same model without correction for en\u2192\u2192\\rightarrow\u2192fr."
        },
        "5": {
            "figure_path": "2403.03923v1_figure_5.png",
            "caption": "Figure 5: Sentence-level COMET per example for MTNT en\u2192\u2192\\rightarrow\u2192fr. Position on the x-axis indicates COMET without correction, whereas performance on the y-axis indicates performance if the source is preprocessed with the jamspell (left) or mt5 (right).\nExamples above the dashed line benefit from correction, while examples below it are harmed. Translation predictions come from Tower."
        },
        "6": {
            "figure_path": "2403.03923v1_figure_6.png",
            "caption": "Figure 5: Sentence-level COMET per example for MTNT en\u2192\u2192\\rightarrow\u2192fr. Position on the x-axis indicates COMET without correction, whereas performance on the y-axis indicates performance if the source is preprocessed with the jamspell (left) or mt5 (right).\nExamples above the dashed line benefit from correction, while examples below it are harmed. Translation predictions come from Tower."
        }
    },
    "references": [
        {
            "1": {
                "title": "Introducci\u00f3n a la tarea compartida Tweet-Norm 2013: Normalizaci\u00f3n l\u00e9xica de tuits en Espa\u00f1ol.",
                "author": "I\u00f1aki Alegria, Nora Aranberri, V\u00edctor Fresno, Pablo Gamallo, Lluis Padr\u00f3, I\u00f1aki San Vicente, Jordi Turmo, and Arkaitz Zubiaga. 2013.",
                "venue": "In Tweet-Norm@ SEPLN, pages 1\u20139.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Steering large language models for machine translation with finetuning and in-context learning.",
                "author": "Duarte Alves, Nuno Guerreiro, Jo\u00e3o Alves, Jos\u00e9 Pombal, Ricardo Rei, Jos\u00e9 de Souza, Pierre Colombo, and Andre Martins. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11127\u201311148, Singapore. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.findings-emnlp.744"
            }
        },
        {
            "3": {
                "title": "Tower: An open multilingual large language model for translation-related tasks.",
                "author": "Duarte M. Alves, Jos\u00e9 Pombal, Nuno M. Guerreiro, Pedro H. Martins, Jo\u00e3o Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, Jos\u00e9 G. C. de Souza, and Andr\u00e9 F. T. Martins. 2024.",
                "venue": null,
                "url": "http://arxiv.org/abs/2402.17733"
            }
        },
        {
            "4": {
                "title": "An analysis of source-side grammatical errors in NMT.",
                "author": "Antonios Anastasopoulos. 2019.",
                "venue": "In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 213\u2013223, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W19-4822"
            }
        },
        {
            "5": {
                "title": "Neural machine translation by jointly learning to align and translate.",
                "author": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.",
                "venue": "arXiv preprint arXiv:1409.0473.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition.",
                "author": "Timothy Baldwin, Marie Catherine de Marneffe, Bo Han, Young-Bum Kim, Alan Ritter, and Wei Xu. 2015.",
                "venue": "In Proceedings of the Workshop on Noisy User-generated Text, pages 126\u2013135, Beijing, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W15-4319"
            }
        },
        {
            "7": {
                "title": "Synthetic and natural noise both break neural machine translation.",
                "author": "Yonatan Belinkov and Yonatan Bisk. 2018.",
                "venue": "In International Conference on Learning Representations.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Unnatural error correction: GPT-4 can almost perfectly handle unnatural scrambled text.",
                "author": "Qi Cao, Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8898\u20138913, Singapore. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.emnlp-main.550"
            }
        },
        {
            "9": {
                "title": "On adversarial examples for character-level neural machine translation.",
                "author": "Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018.",
                "venue": "In Proceedings of the 27th International Conference on Computational Linguistics, pages 653\u2013663, Santa Fe, New Mexico, USA. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/C18-1055"
            }
        },
        {
            "10": {
                "title": "Beyond english-centric multilingual machine translation.",
                "author": "Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021.",
                "venue": "The Journal of Machine Learning Research, 22(1):4839\u20134886.",
                "url": null
            }
        },
        {
            "11": {
                "title": "MultiLexNorm: A shared task on multilingual lexical normalization.",
                "author": "Rob van der Goot, Alan Ramponi, Arkaitz Zubiaga, Barbara Plank, Benjamin Muller, I\u00f1aki San Vicente Roncal, Nikola Ljube\u0161i\u0107, \u00d6zlem \u00c7etino\u011flu, Rahmad Mahendra, Talha \u00c7olako\u011flu, Timothy Baldwin, Tommaso Caselli, and Wladimir Sidorenko. 2021.",
                "venue": "In Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021), pages 493\u2013509, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.wnut-1.55"
            }
        },
        {
            "12": {
                "title": "How good are gpt models at machine translation? a comprehensive evaluation.",
                "author": "Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2302.09210"
            }
        },
        {
            "13": {
                "title": "NeuSpell: A neural spelling correction toolkit.",
                "author": "Sai Muralidhar Jayanthi, Danish Pruthi, and Graham Neubig. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 158\u2013164, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-demos.21"
            }
        },
        {
            "14": {
                "title": "Marian: Fast neural machine translation in C++.",
                "author": "Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, Andr\u00e9 F. T. Martins, and Alexandra Birch. 2018.",
                "venue": "In Proceedings of ACL 2018, System Demonstrations, pages 116\u2013121, Melbourne, Australia. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P18-4020"
            }
        },
        {
            "15": {
                "title": "Training on synthetic noise improves robustness to natural noise in machine translation.",
                "author": "Vladimir Karpukhin, Omer Levy, Jacob Eisenstein, and Marjan Ghazvininejad. 2019.",
                "venue": "In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 42\u201347, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-5506"
            }
        },
        {
            "16": {
                "title": "On the impact of various types of noise on neural machine translation.",
                "author": "Huda Khayrallah and Philipp Koehn. 2018.",
                "venue": "In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 74\u201383, Melbourne, Australia. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W18-2709"
            }
        },
        {
            "17": {
                "title": "Subword regularization: Improving neural network translation models with multiple subword candidates.",
                "author": "Taku Kudo. 2018.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66\u201375, Melbourne, Australia. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P18-1007"
            }
        },
        {
            "18": {
                "title": "Lexical modeling of asr errors for robust speech translation.",
                "author": "Giuseppe Martucci, Mauro Cettolo, Matteo Negri, and Marco Turchi. 2021.",
                "venue": "In Proceedings of Interspeech 2021, pages 2282\u20132286. ISCA.",
                "url": null
            }
        },
        {
            "19": {
                "title": "MTNT: A testbed for machine translation of noisy text.",
                "author": "Paul Michel and Graham Neubig. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543\u2013553, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D18-1050"
            }
        },
        {
            "20": {
                "title": "Between words and characters: a brief history of open-vocabulary modeling and tokenization in nlp.",
                "author": "Sabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gall\u00e9, Arun Raja, Chenglei Si, Wilson Y Lee, Beno\u00eet Sagot, et al. 2021.",
                "venue": "arXiv preprint arXiv:2112.10508.",
                "url": null
            }
        },
        {
            "21": {
                "title": "No language left behind: Scaling human-centered machine translation.",
                "author": "NLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2207.04672"
            }
        },
        {
            "22": {
                "title": "Jamspell.",
                "author": "Filipp Ozinov. 2019.",
                "venue": null,
                "url": "https://github.com/bakwc/JamSpell"
            }
        },
        {
            "23": {
                "title": "Bleu: a method for automatic evaluation of machine translation.",
                "author": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.",
                "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.3115/1073083.1073135"
            }
        },
        {
            "24": {
                "title": "Adversarial subword regularization for robust neural machine translation.",
                "author": "Jungsoo Park, Mujeen Sung, Jinhyuk Lee, and Jaewoo Kang. 2020.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1945\u20131953, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.findings-emnlp.175"
            }
        },
        {
            "25": {
                "title": "Language model tokenizers introduce unfairness between languages.",
                "author": "Aleksandar Petrov, Emanuele La Malfa, Philip Torr, and Adel Bibi. 2023.",
                "venue": "Advances in Neural Information Processing Systems, 36.",
                "url": null
            }
        },
        {
            "26": {
                "title": "chrF: character n-gram F-score for automatic MT evaluation.",
                "author": "Maja Popovi\u0107. 2015.",
                "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392\u2013395, Lisbon, Portugal. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W15-3049"
            }
        },
        {
            "27": {
                "title": "Improving language understanding by generative pre-training.",
                "author": "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.",
                "venue": "OpenAI blog.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.",
                "venue": "OpenAI blog, 1(8):9.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Speech understanding systems: A summary of results of the five-year research effort.",
                "author": "D Raj Reddy et al. 1977.",
                "venue": "Department of Computer Science. Camegie-Mell University, Pittsburgh, PA, 17.",
                "url": null
            }
        },
        {
            "30": {
                "title": "COMET-22: Unbabel-IST 2022 submission for the metrics shared task.",
                "author": "Ricardo Rei, Jos\u00e9 G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr\u00e9 F. T. Martins. 2022a.",
                "venue": "In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578\u2013585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2022.wmt-1.52"
            }
        },
        {
            "31": {
                "title": "COMET: A neural framework for MT evaluation.",
                "author": "Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-main.213"
            }
        },
        {
            "32": {
                "title": "CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task.",
                "author": "Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, Jos\u00e9 G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and Andr\u00e9 F. T. Martins. 2022b.",
                "venue": "In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634\u2013645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2022.wmt-1.60"
            }
        },
        {
            "33": {
                "title": "Social media is not that bad! the lexical quality of social media.",
                "author": "Luz Rello and Ricardo Baeza-Yates. 2012.",
                "venue": "In Proceedings of the International AAAI Conference on Web and Social Media, volume 6, pages 559\u2013562.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Language modelling with pixels.",
                "author": "Phillip Rust, Jonas F Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond Elliott. 2022.",
                "venue": "arXiv preprint arXiv:2207.06991.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Robust open-vocabulary translation from visual text representations.",
                "author": "Elizabeth Salesky, David Etter, and Matt Post. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7235\u20137252, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.576"
            }
        },
        {
            "36": {
                "title": "Multitask prompted training enables zero-shot task generalization.",
                "author": "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=9Vrb9D0WI4"
            }
        },
        {
            "37": {
                "title": "Neural machine translation of rare words with subword units.",
                "author": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.",
                "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P16-1162"
            }
        },
        {
            "38": {
                "title": "Rule-based normalization of German Twitter messages.",
                "author": "Uladzimir Sidarenka, Tatjana Scheffler, and Manfred Stede. 2013.",
                "venue": "In Proc. of the GSCL Workshop Verarbeitung und Annotation von Sprachdaten aus Genres internetbasierter Kommunikation.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Generating text with recurrent neural networks.",
                "author": "Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011.",
                "venue": "In Proceedings of the 28th international conference on machine learning (ICML-11), pages 1017\u20131024.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Parallel data, tools and interfaces in OPUS.",
                "author": "J\u00f6rg Tiedemann. 2012.",
                "venue": "In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 2214\u20132218, Istanbul, Turkey. European Language Resources Association (ELRA).",
                "url": "http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf"
            }
        },
        {
            "41": {
                "title": "The tatoeba translation challenge \u2013 realistic data sets for low resource and multilingual MT.",
                "author": "J\u00f6rg Tiedemann. 2020.",
                "venue": "In Proceedings of the Fifth Conference on Machine Translation, pages 1174\u20131182, Online. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2020.wmt-1.139"
            }
        },
        {
            "42": {
                "title": "OPUS-MT \u2013 building open translation services for the world.",
                "author": "J\u00f6rg Tiedemann and Santhosh Thottingal. 2020.",
                "venue": "In Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, pages 479\u2013480, Lisboa, Portugal. European Association for Machine Translation.",
                "url": "https://aclanthology.org/2020.eamt-1.61"
            }
        },
        {
            "43": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.",
                "venue": "arXiv preprint arXiv:2307.09288.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Improving robustness of machine translation with synthetic noise.",
                "author": "Vaibhav Vaibhav, Sumeet Singh, Craig Stewart, and Graham Neubig. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1916\u20131920, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1190"
            }
        },
        {
            "45": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "Advances in neural information processing systems, 30.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Finetuned language models are zero-shot learners.",
                "author": "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=gEZrGCozdqR"
            }
        },
        {
            "47": {
                "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models.",
                "author": "Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022.",
                "venue": "Transactions of the Association for Computational Linguistics, 10:291\u2013306.",
                "url": "https://doi.org/10.1162/tacl_a_00461"
            }
        },
        {
            "48": {
                "title": "mT5: A massively multilingual pre-trained text-to-text transformer.",
                "author": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.41"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.03923v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "5",
            "5.1"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "5",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.03923v1",
        "paper_title": "Did Translation Models Get More Robust Without Anyone Even Noticing?",
        "research_background": "### Motivation\n\nThe paper investigates the robustness of neural machine translation (MT) models, particularly in the context of modern large language models (LLMs) and pretrained encoder-decoder systems. Traditionally, MT models have been known to be sensitive to source-side noise, both artificial and natural. This sensitivity has driven research toward enhancing their robustness through specialized training techniques or bespoke architectures. However, the current paradigm in MT\u2014dominated by large pretrained models and instruction-tuned LLMs\u2014raises questions about the continuing necessity and effectiveness of these robustness techniques.\n\n### Research Problem\n\nThe central problem the paper addresses is whether the robustness techniques developed for older, single-pair MT models are still necessary or effective for modern LLMs and pretrained models. Specifically, it investigates if these large and sophisticated models have inherently become more robust to noise, thus reducing or eliminating the need for specialized robustness improvements.\n\n### Relevant Prior Work\n\n1. **Sensitivity of MT Models to Noise**: The foundational insight that MT models are sensitive to source-side noise was highlighted by Belinkov and Bisk (2018).\n\n2. **Specialized Training and Architectures**: Various techniques and architectures have been proposed to increase the robustness of MT models:\n   - Specialized training approaches (Ebrahimi et al., 2018; Karpukhin et al., 2019; Park et al., 2020; Vaibhav et al., 2019).\n   - Bespoke architectures designed to handle noise more effectively (Rust et al., 2022; Salesky et al., 2021).\n\n3. **Modern MT Paradigm**: The landscape has shifted towards using large pretrained models and LLMs such as:\n   - NLLB Team (2022) which showed strong results using large pretrained encoder-decoder models.\n   - Instruction-tuned LLMs like TowerLLM (Alves et al., 2024).\n   - Opaque proprietary systems like ChatGPT.\n\n### Contributions \n\n1. **Robustness of Large Pretrained Models**: The study demonstrates that large pretrained models are significantly more robust to synthetic source-side errors than conventional single-pair NMT models across multiple language pairs and varieties of noise, even without specialized robustness features.\n\n2. **Robustness Prediction Technique**: The paper introduces a novel technique for measuring MT model robustness by learning to predict the quality decline as a function of source noise levels.\n\n3. **Real-world Robustness**: It shows that models robust to synthetic errors also perform better on social media text, validating their real-world applicability.\n\n4. **Reference-free MT Experiments**: Conducts experiments on MultiLexNorm, a lexical normalization dataset not previously used for MT, demonstrating that LLMs are more robust to this type of noise compared to conventional models.\n\n5. **Source Correction Pipelines**: Evaluates source correction pipelines as a method to mitigate synthetic noise impact, noting their limited effectiveness with stronger models and on social media data.",
        "methodology": "### Proposed Method:\nIn this study, the authors propose the use of source correction pipelines to handle noise in translation tasks. Their methodology involves a comprehensive evaluation of different translation models on their robustness to synthetic noise and performance on social media text, followed by the application of source correction pipelines. Here are the key components and innovations:\n\n1. **Assessment of Translation Model Robustness:**\n   - The study begins by assessing the robustness of various translation models, specifically OPUS, larger models like mT5, NLLB, and GPT-3.5, against synthetic noise. OPUS faces more difficulty dealing with synthetic noise and social media text compared to larger models.\n\n2. **Introduction of Source Correction Pipelines:**\n   - Source correction pipelines are employed to handle noisy input text before translation. Though not end-to-end learnable and prone to error propagation, these pipelines are noted for their modularity, interpretability, and compatibility with closed-source models.\n\n5. **Challenges with Social Media Data:**\n   - The source correctors, trained on non-social media domains, struggle with social media text, causing at least a one COMET point decline compared to raw sources.\n\n6. **Fine-Tuning vs Source Correction:**\n   - Although an oracle corrector can make some gains, it still falls short of the performance achieved through fine-tuning. This is especially pertinent considering MTNT does not exhibit significant spelling noise, based on prior research.\n\n7. **Case-by-Case Effectiveness:**\n   - Detailed analysis reveals that some sentences benefit from correction while others do not, presenting source correction as a higher-risk approach, particularly with mT5 compared to JamSpell. The inconsistent impact stresses the need for further research to refine source correction\u2019s efficacy.\n\n### Innovations:\n- Leveraging modular and interpretable source correction pipelines to pre-process noisy input for translation models.\n- Identification of the limitations of source correction in certain domains (social media) and its relative ineffectiveness compared to fine-tuning in those cases.\n- Providing a nuanced understanding of model performance under distinct noise conditions, including the resilience of larger models versus the more noise-sensitive OPUS models.\n\n### Conclusion:\nThe methodology demonstrates that source correction pipelines can ameliorate the performance of various translation models on noisy inputs, with variable success across different models and text domains. Key insights include the potential for substantial gains in specific conditions, the minimal impact on clean inputs, and the challenges posed by social media text. Further research is encouraged to enhance and tailor source correction techniques for broader applicability and robustness.",
        "main_experiment_and_results": "### Main Experiment Setup:\n\n#### Datasets:\n- **FLORES-200 devtest data**: Selected to subject models to token-level synthetic errors, with noise types reflecting plausible typing errors.\n\n#### Noise Types:\n1. **swap**: Flip two adjacent characters.\n2. **chardupe**: Duplicate a character.\n3. **chardrop**: Delete a character.\n4. **key**: Replace a character with an adjacent one on a keyboard (different layouts used based on language).\n\n#### Models Evaluated:\n1. **OPUS**: Transformer encoder-decoder models trained on single language pairs (model size: 74M-7B parameters).\n2. **NLLB** (3.3 billion parameters): Multilingual encoder-decoder transformer trained on data for 202 languages.\n3. **TowerInstruct**: 7 billion parameter instruction-tuned LLM derived from Llama 2, translating between 10 languages.\n4. **GPT-3.5**: Model architecture and training data are unknown, included due to proven MT capabilities.\n\n#### Evaluation Metrics:\n- **COMET (Rei et al., 2020)**: Normalized score for translation performance, provided clean source for noisy data evaluations.\n- **COMET-slope**: Measures the stability of translation quality under increasing noise, indicating robustness (closer to zero meaning more robust).\n\n#### Experimental Procedure:\n- Generate translations using beam search for all models except GPT-3.5, which used temperature 0 sampling.\n- Linear regression to determine COMET-slope showing performance decline relative to noise proportion.\n\n### Main Experimental Results:\n\n- **Clean Data**:\n  - Tower and GPT-3.5 are the strongest, with the highest COMET scores across eight language pairs.\n  - The gap between the strongest models and OPUS is significant, with OPUS generally showing lower performance.\n\n- **Noisy Data**:\n  - OPUS suffers significantly more from perturbations compared to other models.\n  - GPT-3.5 consistently shows the highest robustness across noise types, especially standing out with minimal performance degradation.\n  - NLLB and Tower show intermediate robustness:\n    - They are more robust than OPUS but generally less so than GPT-3.5.\n\n- **Fluency Analysis**:\n  - NLLB and GPT-3.5 maintain fluent output even as noise levels increase, indicating their robustness.\n\nThe study concludes that while model size and multilinguality contribute to robustness, other factors are also at play, as similar-sized multilingual models (e.g., NLLB-1.3B and M2M-1.2B) do not exhibit the same noise response patterns. Further investigation is required to identify the determinants of robustness in translation models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate how various translation models perform in the presence of synthetic token-level errors, to determine their robustness against such noise.",
            "experiment_process": "The experiment involved four types of synthetic perturbations (swap, chardupe, chardrop, key) applied to 10% to 100% of whitespace-delimited tokens in the FLORES-200 devtest data. Models tested include OPUS, NLLB, Tower, and GPT-3.5, differing in their scope (bi- or multilingual), architecture (encoder-decoder or decoder-only), and size (74M-7B parameters). Translations were generated using beam search (GPT-3.5 was sampled with temperature 0), with scoring done using the COMET metric. The robustness of the models was quantified by the COMET-slope, representing the decline of performance relative to clean data as the noise level increased.",
            "result_discussion": "Results showed that on clean data, Tower and GPT-3.5 were the strongest systems. Differences became much larger on noisy data, where OPUS suffered far more from perturbations than any other models. GPT-3.5 was almost always more robust, followed by NLLB and Tower, which were more robust than OPUS but less so compared to GPT-3.5. The robustness did not solely depend on model size or multilinguality, as seen when comparing NLLB versions and M2M. Noise also affected runtime due to varying tokenizer robustness, with larger models closer to byte-level tokenization showing less impact from noise.",
            "ablation_id": "2403.03923v1.No1"
        },
        {
            "research_objective": "Assess the robustness of translation models to authentic social media text, which inherently has different kinds of 'noise' compared to synthetic errors.",
            "experiment_process": "Two complementary investigations were conducted: First, models were evaluated on the MTNT corpus, a noisy social media translation benchmark. Second, translations of data from the MultiLexNorm dataset, a lexical normalization benchmark, were compared with manually cleaned versions. In the absence of reference translations in MultiLexNorm, three reference-free evaluation metrics (faux-BLEU, faux-COMET, and QE) were used to assess robustness.",
            "result_discussion": "GPT-3.5 was the best performer in both MTNT and MultiLexNorm experiments. Among other models, Tower generally performed well, although NLLB nearly equaled it after finetuning on MTNT. Faux-metrics indicated that GPT-3.5 had the best performance for most language pairs, while Tower and NLLB exhibited different strengths: Tower preserved deeper features, and NLLB maintained lexical structure better.",
            "ablation_id": "2403.03923v1.No2"
        },
        {
            "research_objective": "Determine the effectiveness of source correction techniques in mitigating noise in both synthetic and social media text translation scenarios.",
            "experiment_process": "Synthetic errors were corrected using source correction pipelines and the performance of different models (OPUS, NLLB, Tower, GPT-3.5) compared using COMET scores. Social media text from MTNT was also corrected and evaluated to see if source correction improved translation quality. The primary correction methods were mT5 and JamSpell, and their impact on translation quality was observed.",
            "result_discussion": "Source correction significantly benefited OPUS models, with mT5 yielding over 40 COMET points improvement for heavily noised inputs (synthetic). Smaller gains were observed for NLLB and Tower, with a slight negative impact on GPT-3.5. In social media domain (MTNT), source correction\u2019s benefit was less clear, with performance gains seen in some sentences but overall mixed results, suggesting further research is necessary to optimize source correction for real-world noisy texts.",
            "ablation_id": "2403.03923v1.No3"
        }
    ]
}