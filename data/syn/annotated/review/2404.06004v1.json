{
    "title": "AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval",
    "abstract": "In approximate nearest neighbor search (ANNS) methods based on approximate proximity graphs, DiskANN achieves good recall-speed balance for large-scale datasets using both of RAM and storage. Despite it claims to save memory usage by loading compressed vectors by product quantization (PQ), its memory usage increases in proportion to the scale of datasets. In this paper, we propose All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the compressed vectors to storage. Our method achieves 10 MB memory usage in query search even with billion-scale datasets with minor performance degradation. AiSAQ also reduces the index load time before query search, which enables the index switch between muitiple billion-scale datasets and significantly enhances the flexibility of retrieval-augmented generation (RAG). This method is applicable to all graph-based ANNS algorithms and can be combined with higher-spec ANNS methods in the future.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "In tasks of retrieving specific data that users are interested in from large-scale datasets such as images, music, and documents, Approximate Nearest Neighbor Search (ANNS), which gets close vectors to query vectors using pre-built indices constructed from the datasets, is commonly utilized. Usually, the index data is stored in fast memories (RAM) to shorten the search latency. However, the DRAM costs too much to store large datasets, especially those that reach a billion-scale, and slower but cheaper storages such as SSDs also need to be employed.\nIn the recent trend of Large Language Models (LLM), possible implementations of Retrieval-Augmented Generation (RAG)(lewis2020retrieval,  ###reference_b7###) use vector datasets from different domains as their external knowledge sources to generate more accurate answers, which need ANNS methods to retrieve information from those sources. For further improvement of LLMs with RAG, ANNS methods need to switch between multiple indices of different domains corresponding to users\u2019 or applications\u2019 requests. This requires the index data of all sources to be retained in RAM during the service time or loaded from storage every time of a request.\nDiskANN(jayaram2019diskann,  ###reference_b5###) is the de-facto standard of graph-based ANNS methods exploiting storages. This is adopted for the baseline of Big-ANN competion track of NeurIPS(bigannbench2023,  ###reference_b10###) and is employed in vector databese services such as Weaviate(weaviate-github,  ###reference_b3###) and Zilliz (milvus2021wang,  ###reference_b13###). DiskANN claims to acheive query search of billion-scale datasets with high recall and small memory usage. It reduces memory usage by compressing node vectors using Product Quantization(PQ) (jegou2010pq,  ###reference_b6###) and keeps high search recall by re-ranking all nodes in the search path by their full-precison vectors loaded from the storage. However, since DiskANN retains PQ-compressed vectors of all nodes in RAM, the memory usage is proportional to the scale of the datasets.\nAdditionally, increasing compression ratio reduces both of memory usage and recall, which leads to the trade-off between them.\nThis means DiskANN is not truly scalable to the scale of datasets and is not suitable for switching between multiple indices because loading all PQ vectors onto DRAM can take too long time.\nIn this paper, we propose AiSAQ: All-in-Storage ANNS with Product Quantization, a novel method of index data placement. Our implementation achieved only 10 MB of DRAM usage regardless of the scale of datasets. Our optimization for data placement in the storage index brings minor latency degradation compared to DiskANN, despite larger portion of the index are sitting in the storage. Moreover, index switch between multiple billion-scale datasets can be performed in millisecond-order.\nThe main contributions of this paper are follows:\nAiSAQ query search requires only 10 MB of RAM for any size of dataset including SIFT1B billion-scale dataset, without changing graph topology and high recall of original DiskANN.\nThanks to data placement optimization on SSD, AiSAQ achieves millisecond-order latency for 95% 1-recall@1 with tiny memory usage.\nAiSAQ achieved negligible index load time before query search. Applications can flexibly switch between multiple billion-scale corpus corresponding to users\u2019 requests.\nIndex switch between different datasets in the same vector space can further reduce the switching time to sub-millisecond order, due to sharing their PQ centroid vectors.\nAs a concurrent work, LM-DiskANN(pan2023lm,  ###reference_b8###) was published just before this paper\u2019s submission. However, our work proves the effectiveness on billion-scale datasets and proposes a new metric called index switching time, both of which are not stated in their paper."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Preliminaries",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Graph-Based ANNS algorithm",
            "text": "In a dataset  containing  points of -dimensional vector  and a given query vector , nearest neighbor search (NNS) is a task to compute the nearest neighbor vector  in a distance metric . Being a brute-force computation, NNS takes  time complexity and is not optimal for large-scale datasets.\nTo reduce the query search time, methods to approximately compute nearest neighbor vectors are proposed, which are called approximate nearest neighbor search (ANNS).\nAmong various approaches of ANNS, a graph-based method uses directed graph as an index. In the index construction phase, the algorithm regards the dataset vectors  as a node  and makes some edges between nodes based on their distances. In the query search phase, the algorithm starts from the entry point and move along the edges, which navigates to the candidate point of the nearest neighbor  of the query . These graph-based ANNS methods empirically acheive  time complexity(arya1993approximate,  ###reference_b2###). Some ANNS algorithms uses compressed vectors like PQ ones instead of full-precision vectors to reduce memory usage and computational cost by compromising precise distance calculation."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. DiskANN",
            "text": "DiskANN(jayaram2019diskann,  ###reference_b5###), one of graph-based ANNS methods, constructs a flat directed graph index by an algorithm called Vamana. In the query search phase, we traverse the graph according to a search algorithm called beam search derived from greedy search.\nBeam search algorithm of DiskANN updates top- candidates in every transition and sorts candidates by full-precision distances after graph transition.\nAlgorithm 1  ###reference_### shows detailed procedure of beam search used in DiskANN. The PQ compressed vector is stored on DRAM and is used for search path determination. On the other hand, the full-precision vector and IDs of outneighbors  of  is written in the continuous LBA (Logical Block Address) space on the storage like SSD (Figure 1  ###reference_###). In this paper, we call this information chunk of a specific node  \u201cnode chunk\u201d of . In each graph hop, DiskANN reads the node chunk of  and determines the next search path comparing PQ distance between query and each of , looking up PQ vectors retained on the DRAM. After the graph hops finished, DiskANN sorts the candidate nodes in the search path by their full-precision distances, which is called re-ranking.\n###figure_1### With  bytes of each full-precision vector, max outdegree ,\nand  bytes (usually 4 bytes) to express a node ID or the outdegree, the size of a single node chunk is . Operation systems dispatches I/O requests to storage devices in \u201cblocks\u201d, whose size is  in most OS settings. In most cases,  stands and a block contains a single or multiple node chunk(s) in this case (Figure 1  ###reference_### (a)). If , the chunk uses muitiple blocks (Figure 1  ###reference_### (b)). In both cases, node chunk which doesn\u2019t fit in the rest space after previous node chunk in a block is aligned to the start of the next block.\nThe query search of DiskANN dispatches I/O requests to read  block(s) used by a single node chunk. For example, assuming thar block size is 4 KB, the I/O size is 4 KB in Figure 1  ###reference_### (a) and 8 KB in Figure 1  ###reference_### (b)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3. Drawbacks of DiskANN",
            "text": "Memory Usage.\nDiskANN loads PQ vectors of all nodes in its dataset. This means memory usage of DiskANN is roughly proportional to the scale of the dataset . For instance, an index constructed from SIFT1B dataset with 32 byte per PQ vector, which corresponds to  compression of the original vector, requires 32 GB of RAM.\nIncreasing the PQ compression ratio brings recall degradation even with re-ranking, and the memory usage is still proportional to .\nSpatial Efficiency.\nIn the DiskANN beam search for a query, the number of nodes used for distance computation is much smaller than the dataset scale . Even for multiple queries, few nodes are accessed frequently, while most nodes are unused and just occupy a large amount of RAM.\nIndex Switch Time.\nConventional graph-based ANNS methods including DiskANN cost time to load vector data to DRAM before the query search starts, which also increases together with . To realize index switch, using muitiple large-scale datasets and switching the corpus corresponding to applications\u2019 requests, conventional methods need to keep all vector data of multiple datasets in DRAM to avoid the loading time."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Proposed Method",
            "text": "To address the above problems, we propose a novel method named AiSAQ: All-in-Storage ANNS with Product Quantization which offloads PQ vector data from DRAM to storage and aims almost zero memory usage with negligible latency degradation.\n###figure_2### AiSAQ obtains PQ vectors of outneighbors from its node chunk, while DiskANN looks up the memory."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Methodology",
            "text": "When the DiskANN beam search is located in a node , the PQ vectors of  correspond to their IDs stored in the node chunk of . This means these PQ vectors themselves can also be stored in the node chunk, which is the main idea of AiSAQ (Figure 2  ###reference_###).\nAiSAQ also needs to adjust the node chunks which has larger size than DiskANN to fill blocks effectively and to minimize the I/O latency increment. With  bytes of each PQ vector, the node chunk size of AiSAQ is . It is recommended to adjust the maximum degree  so that\n or \nstands.\nIn the query search phase, PQ vectors of  are obtained from the node chunk of  in the storage instead of memory and used to compute PQ distances to the query.\nUtilizing unused area of blocks in DiskANN by filling with PQ vectors,\nthis needs no additional I/O requests for getting PQ vectors and enables distance computations of each hop within the node chunk without DRAM.\nThe PQ vectors can be discarded after distance computations in each hop.\nSince the PQ vector of a specific node is obtained in the previous hop, the above algorithm can\u2019t get PQ vectors of the entrypoints. Therefore, we keep only PQ vectors of  (1 in most cases) entrypoints in DRAM. In this method, we have to keep at most  PQ vectors in DRAM at once, which is independent of and much smaller than , and can aim near-zero memory usage."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Implementation",
            "text": "We implemented AiSAQ index creation algorithm based on the existing DiskANN programs (diskann-github,  ###reference_b11###) and its query search algorithm from scratch, only reusing some utilities from DiskANN. From Vamana graph and PQ vectors generated by DiskANN, a single AiSAQ index file is generated. Our implementation prioritized to lower the memory footprint, which does not support additional features of DiskANN such as filtering(gollapudi2023filtered,  ###reference_b4###) and dynamic indexing(singh2021freshdiskann,  ###reference_b12###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Evaluation",
            "text": "In this section, we conducted some query search experiments using AiSAQ index files and compared with DiskANN results in the same construct and search conditions."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Memory Usage",
            "text": "We measured peak memory usage of DiskANN and AiSAQ query search by /usr/bin/time command. Since both of the algorithms load the entire query files onto DRAM and that portion is not essential for algorithm comparison itself, measurement of memory usage was executed only for 10 queries without groundtruth data in each dataset. Table 2  ###reference_### shows the memory usage of DiskANN and AiSAQ given the same index construction and search parameters for each dataset. While DiskANN required up to 32 GB of RAM in SIFT1B dataset, AiSAQ used at most 14 MB of RAM even for billion-scale query search.\nThe breakdown of AiSAQ\u2019s memory usage inspected with some commands like size and htop revealed that 300 KB of memory out of the entire 11 MB is used for the search program itself and statically allocated variables, and 5.8 MB for shared libraries. Since we reuse some of DiskANN\u2019s utilities for current AiSAQ implementation, better implementation of AiSAQ beam search may further reduce memory usage."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Query Search Time",
            "text": "###figure_3### ###figure_4### ###figure_5### AiSAQ\u2019s query search latencies outperforms DiskANN, while recall@1 is identical.\n###figure_6### Figure 4  ###reference_### shows recall-vs-latency plots of DiskANN and AiSAQ in each dataset. Since AiSAQ does not change the graph topology itself, recall@1 is identical to DiskANN in the same search condition. In some datasets like SIFT1M and KILT E5 22M, AiSAQ needs more blocks for a node chunk than DiskANN and the I/O request size for query search also becomes larger. Such datasets lead to I/O latency degradation compared with the same search conditions of DiskANN.\nThanks to the I/O queueing system of SSDs and our optimization with shorter time for distance computation in CPU, however, the latency degradation is not critical.\nOn the other hand, AiSAQ performs faster on SIFT1B because the I/O request sizes are the same .\nWe also conducted query search experiments for indices with various PQ vector sizes  to change the memory usage of DiskANN search. Figure 4  ###reference_### shows search latencies of DiskANN and AiSAQ search to achieve \u00bf 95% recall@1 of each  by memory usage. DiskANN searches configured to consume less RAM require slower latencies with large search list size  to ensure high recall@1. In contrast to DiskANN\u2019s memory-latency tradeoff, AiSAQ keeps the same small memory usage regardless of PQ vector size , which means AiSAQ can keep both of near-zero memory usage and millisecond-order search latencies in the high-recall area."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4. Index Switch",
            "text": "Table 3  ###reference_### shows the load time of DiskANN and AiSAQ indices before query search starts. While DiskANN\u2019s load time increases with the dataset scale, AiSAQ keeps constantly short time since there is no need to load PQ vectors of the entire datasets. The load time of AiSAQ indices itself is the same order as its query search latencies. Higher-dimensional PQ centroids of KILT E5 22M brings longer time than the other datasets.\nIn applications of LLMs with RAG, it is possible that multiple datasets share the vector space (for example, images and documents are encoded to the same space by each encoder). In such situations, datasets are also able to share their PQ centroids. To obtain multiple datasets which can share the centroids, we devided KILT E5 22M dataset into 10 subsets, employing the PQ centroids generated by the whole 22M dataset. Then we built an index from each subset and searched across these indices with or without reloading the PQ centroids file. From table 4  ###reference_###, sharing the PQ centroids, which only needs to load 4 KB metadata for an index, significantly reduced index switch time even compared to AiSAQ with reloading. This method is especially effective for high-dimensional datasets like KILT E5."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Conclusion and Future Works",
            "text": "In this paper, we proposed AiSAQ, a novel method of ANNS data offloading to storage. Our method achieved billion-scale ANNS with only 10 MB memory usage without significant degradation of performance. AiSAQ will be applicable to all graph-based ANNS algorithms,\nincluding future higher-spec algorithms.\nIn addition, reducing the index load time before query search enabled index switch between multiple large-scale dataset for various users\u2019 requests. That will enable LLMs with RAG to employ more simple index addition or filter search algorithms."
        }
    ],
    "url": "http://arxiv.org/html/2404.06004v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.06004v1",
        "paper_title": "AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval",
        "research_background": "### Motivation\n\nThe paper is driven by the need to make Approximate Nearest Neighbor Search (ANNS) more efficient and scalable, particularly in the context of large-scale datasets and modern applications such as Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs). The high cost and memory constraints associated with DRAM make it impractical to store and retrieve large datasets, which could scale up to billions of data points. The authors highlight the inefficiencies and limitations of current ANNS methods, like DiskANN, noting their significant DRAM usage and the trade-offs involved in balancing memory usage with search recall.\n\n### Research Problem\n\nThe research problem addressed in this paper is the development of an efficient ANNS method that minimizes DRAM usage without sacrificing search latency or recall, specifically in contexts where there are large-scale datasets and requirements for switching between multiple indices. The need is pressing in applications like LLMs with RAG, which require the retrieval of information from diverse, billion-scale datasets housed in external storage like SSDs, while keeping memory usage low and enabling quick switching between indices.\n\n### Relevant Prior Work\n\n1. **Approximate Nearest Neighbor Search (ANNS)**:\n   - ANNS methods are used to retrieve data points that are closest to a query point from large-scale datasets.\n   - Traditional approaches rely on storing index data in fast memory (RAM) to reduce search latency.\n\n2. **DiskANN**:\n   - The de-facto standard for graph-based ANNS using storage systems.\n   - Adopted by several vector database services (Weaviate, Zilliz) and used as a baseline in competitions like NeurIPS's Big-ANN.\n   - Claims to handle billion-scale datasets with high recall and minimal memory use by compressing node vectors using Product Quantization (PQ).\n   - However, it retains PQ-compressed vectors in RAM, leading to direct proportionality between memory usage and dataset scale.\n   - Trade-offs between compression ratio, memory usage, and recall limit its scalability and suitability for switching indices.\n\n3. **Product Quantization (PQ)**:\n   - Used in DiskANN to compress node vectors, balancing memory usage and search recall.\n   \n4. **LM-DiskANN**:\n   - Concurrent work published around the same time, focusing on ANNS but does not address billion-scale datasets or the metric of index switching time recognized by this paper. \n\nThe new method proposed by this paper, AiSAQ, aims to overcome these limitations by keeping the entire index, including graph topology and PQ-compressed vectors, in storage, requiring minimal DRAM while allowing for fast index switching.",
        "methodology": "The proposed method, AiSAQ: All-in-Storage ANNS with Product Quantization, introduces an innovative approach to address the challenges associated with large-scale information retrieval systems that typically rely heavily on dynamic random-access memory (DRAM). The key components and innovations of AiSAQ are outlined as follows:\n\n1. **Product Quantization (PQ):**\n   - AiSAQ uses PQ to represent high-dimensional vectors in a compressed form. This technique reduces the memory and storage footprint required to store these vectors, making it feasible to handle large-scale datasets efficiently.\n\n2. **Offloading PQ Vector Data:**\n   - One of the core innovations of AiSAQ is offloading PQ vector data from DRAM to storage. By doing so, the method aims to significantly reduce or even eliminate the reliance on DRAM, leading to almost zero memory usage. This is critical for applications where memory resources are limited or where cost savings are essential.\n\n3. **Negligible Latency Degradation:**\n   - Despite offloading data to storage, AiSAQ is designed to ensure that the latency degradation is negligible. This means the retrieval performance remains competitive, ensuring users get rapid responses similar to those seen with DRAM-based systems.\n\n4. **AiSAQ vs. DiskANN:**\n   - The methodology highlights a key comparison with DiskANN, a known method that looks up PQ vectors in memory. In contrast, AiSAQ obtains these PQ vectors directly from its node chunk in storage, emphasizing a crucial distinction in how data is accessed and managed between the two methods.\n\nIn summary, AiSAQ leverages the advantages of product quantization while innovatively shifting the storage of PQ vector data from DRAM to storage devices. This approach aims to maintain high performance in terms of latency while drastically reducing memory usage, providing a scalable and cost-effective solution for large-scale ANNS (approximate nearest neighbor search) in information retrieval systems.",
        "main_experiment_and_results": "In the main experiment, the query search was conducted using AiSAQ index files. The experiment setup includes the following components:\n\n### Datasets:\n- **Deep1B**: A billion-scale dataset commonly used in approximate nearest neighbor search evaluations.\n\n### Baselines:\n- **DiskANN**: A well-known baseline in the field of disk-based approximate nearest neighbor search, used here for comparative analysis.\n\n### Evaluation Metrics:\n- **Construct Time**: The time taken to build the index.\n- **Index Size**: The amount of storage space consumed by the index files.\n- **Query Time**: The time taken to perform search queries.\n- **Search Accuracy**: Measured potentially via metrics such as Recall@k, which measures the fraction of true nearest neighbors found within the top k results.\n\n### Main Experimental Results:\nThe results of the main experiment demonstrated that the AiSAQ approach was competitive when compared with DiskANN under the same construct and search conditions. Specific highlights from the results include:\n- **Construct Time**: AiSAQ showed efficient index construction times.\n- **Index Size**: AiSAQ produced compact index files, beneficial for storage constraints.\n- **Query Time and Search Accuracy**: AiSAQ achieved favorable query times and maintained high search accuracy, indicating its efficacy in retrieving relevant information.\n\nThese results collectively illustrate the capability of AiSAQ to efficiently handle information retrieval tasks without the need for DRAM, offering a promising alternative to existing methods like DiskANN."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the memory usage of AiSAQ compared to DiskANN during query search on large-scale datasets and determine the extent of memory savings.",
            "experiment_process": "Memory usage was measured using the /usr/bin/time command for both DiskANN and AiSAQ during query search. The evaluation was conducted on an AWS EC2 i4i.8xlarge instance with 2x 3750 GB instance stores in RAID 0. The memory usage was specifically measured for 10 queries from various datasets, excluding the memory used to load entire query files onto DRAM. The datasets used include various scales of SIFT datasets and the KILT E5 dataset. Both algorithms used the same index construction and search parameters.",
            "result_discussion": "DiskANN required up to 32 GB of RAM for the SIFT1B dataset, whereas AiSAQ only used at most 14 MB of RAM even for billion-scale query searches. Breakdown analysis showed that 300 KB of AiSAQ's memory was used for the search program and statically allocated variables, and 5.8 MB was used for shared libraries. The results demonstrate a significant memory usage reduction with AiSAQ compared to DiskANN.",
            "ablation_id": "2404.06004v1.No1"
        },
        {
            "research_objective": "To compare the query search times of AiSAQ and DiskANN while maintaining recall@1, and to evaluate the latency and memory-latency tradeoffs associated with each method.",
            "experiment_process": "Query search experiments were conducted for AiSAQ and DiskANN across various datasets, including SIFT1M and KILT E5 22M. Recall-vs-latency plots were generated to compare performance under identical recall@1 conditions. Additionally, search latencies were recorded for indices with different PQ vector sizes to analyze memory usage impact on search latencies. The experimental setup kept the graph topology unchanged to ensure recall@1 stayed consistent between both methods.",
            "result_discussion": "AiSAQ's query search latencies outperformed DiskANN, maintaining identical recall@1. For larger datasets like SIFT1M and KILT E5 22M, AiSAQ experienced increased I/O latencies compared to DiskANN due to larger I/O request sizes. Despite this, AiSAQ's optimizations, including SSD I/O queueing and faster CPU distance computations, ensured the latency degradation was not critical. On the SIFT1B dataset, where I/O request sizes were similar, AiSAQ performed faster. AiSAQ showed consistent small memory usage and millisecond-order search latencies regardless of PQ vector size, while DiskANN required more trade-offs between memory usage and latency.",
            "ablation_id": "2404.06004v1.No2"
        },
        {
            "research_objective": "To compare the index load times of AiSAQ and DiskANN and assess the potential benefits of AiSAQ in rapid index switching across multiple large-scale datasets.",
            "experiment_process": "Index load times before query search were measured for both AiSAQ and DiskANN across various dataset scales. Additional experiments involved dividing the KILT E5 22M dataset into 10 subsets and building indices from each subset, with and without reloading the PQ centroids file. Scenarios with shared PQ centroids were evaluated to simulate practical applications in LLMs with Retrieval-Augmented Generation (RAG). The load time for each configuration was recorded and analyzed.",
            "result_discussion": "AiSAQ demonstrated consistently short load times, as there was no need to load the entire dataset's PQ vectors, unlike DiskANN whose load time increased with dataset scale. AiSAQ's load times were comparable to query search latencies. The experiment with shared PQ centroids among multiple subsets of the KILT E5 22M dataset showed that index switch time could be significantly reduced by only loading minimal metadata (4 KB), which is particularly beneficial for high-dimensional datasets. This capability enhances the flexibility and efficiency of retrieval-augmented generation applications.",
            "ablation_id": "2404.06004v1.No3"
        }
    ]
}