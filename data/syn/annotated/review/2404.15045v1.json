{
    "title": "Multi-Head Mixture-of-Experts",
    "abstract": "Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in training and inference costs. However, it exhibits two issues: (1) Low expert activation, where only a small subset of experts are activated for optimization, leading to suboptimal performance and limiting its effectiveness in learning a larger number of experts in complex tasks. (2) Lack of fine-grained analytical capabilities for multiple semantic concepts within individual tokens.\nIn this paper, we propose Multi-Head Mixture-of-Experts (MH-MoE). MH-MoE employs a multi-head mechanism to split each input token into multiple sub-tokens. Then these sub-tokens are assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form.\nThe above operations enables MH-MoE to collectively attend to information from various representation spaces within different experts to deepen context understanding while significantly enhancing expert activation.\nIt\u2019s worth noting that our MH-MoE is straightforward to implement and decouples from other SMoE frameworks, making it easy to integrate with these frameworks for enhanced performance.\nExtensive experimental results across three tasks: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks, demonstrate the effectiveness of MH-MoE. Our code are available at https://github.com/yushuiwx/MH-MoE .",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large capacity models, such as Large Language Models (LLMs) (Zhao et al., 2023  ###reference_b38###; Pham et al., 2023  ###reference_b25###; Chung et al., 2022  ###reference_b4###; OpenAI, 2023  ###reference_b22###) and Large Multi-modal Models (LMMs) (Wang et al., 2022  ###reference_b35###; Peng et al., 2023  ###reference_b24###), have demonstrated their efficacy across various domains and tasks.\n(a)\n \n\n\n       \nSMoE\nMH-MoE\n\n       \nActivation: 8.33%\nActivation: 90.71%\n \n\n\n(b)\n \n\n\n       \nRGB Input\nSub-tokens Assign\n###figure_1### ###figure_2### To further enhance performance, a reliable approach involves scaling up these models by augmenting the parameter count (Fedus et al., 2022  ###reference_b10###). But for most of these densely-activated large-capacity models (referred to as Dense models), which utilize all their parameters to process all inputs, the extremely large size of these models significantly reduces inference speed, further limiting their practicality.\n###figure_3### A promising alternative, facilitating model scalability while mitigating the burdensome computational costs, resides in Sparse Mixtures of Experts (SMoE) (Shazeer et al., 2017b  ###reference_b30###; Du et al., 2021  ###reference_b9###; Chi et al., 2022  ###reference_b3###; Clark et al., 2022  ###reference_b5###). In contrast to Dense model, SMoE contains parallel feed-forward neural networks (referred to as experts) within each building block, and strategically activates distinct experts for specific input tokens via a router, thereby yielding noteworthy efficiency enhancements.\nGShard (Lepikhin et al., 2020  ###reference_b18###) scales a Dense model from 2B to 600B parameters with lower training costs than a 100B Dense model. And recently, Mixtral 87B (Jiang et al., 2024  ###reference_b13###), a SMoE model containing 8 experts (7B parameter in total) is shown to outperform or matches LLaMA-2 70B (Touvron et al., 2023  ###reference_b32###) and GPT-3.5.\nDespite its success, SMoE has some drawbacks:\n(1) Low experts activation, which means that only a small subset of experts are activated during optimization and inference, e.g., 8.33% activation ratio shown in Figure 1  ###reference_### (a), while the majority of them are not used at all (see the dark area). As a result, SMoE fails to utilize the full expressive power of these experts, especially when the number of experts is large, which significantly limits effectiveness and scalability of SMoE.\n(2) Absence of fine-grained analytical capabilities. The current tokenization patterns impose limitations on the model\u2019s capacity to grasp multiple semantic interpretations linked to individual tokens. In the context of visual data, dividing images into patches for tokenization may either neglect finer image details when using larger patches or escalate computational requirements when employing smaller ones. For language data, the tokenization of false cognates across different languages or polysemous words within a single language results in them being represented by the same tokens, despite carrying distinct meanings. This can subsequently lead to confusion within the models.\nTo tackle the above issues, we propose Multi-Head Mixture-of-Experts (MH-MoE). The workflow of MH-MoE is illustrated in Figure 2  ###reference_###. By employing a multi-head mechanism to split each input token into multiple sub-tokens and distribute them to different experts, MH-MoE achieves denser expert activation without an increase in computational and parameter complexity. Specifically, as shown in Figure 2  ###reference_###, when provided with a single input token, MH-MoE activates four experts by splitting it into four sub-tokens, whereas SMoE only activates one expert.\nFurthermore, the allocation of sub-tokens to distinct experts enables the model to simultaneously focus on information from various representation spaces within different experts, ensuring a more granular understanding for subtle differences in both vision and language patterns. See in Figure 2  ###reference_###, sub-tokens assigned to Experts 3 and 2 capture a detailed understanding of each character\u2019s actions within an image patch, while those assigned to Experts 1 and 4 explicitly model the semantics of the false cognate \u2018camera\u2019.\nAfter expert processing, sub-tokens are seamlessly reintegrated into the original token form, thereby circumventing any additional computational burden in subsequent non-parallel layers, e.g., attention layer, while also integrating semantic information captured from multiple experts\nMH-MoE maintains following strengths:\n(1) Higher experts activation & better scalability. MH-MoE can alleviate lower expert activation problem and significantly enhance the usage of larger experts by enabling optimization of almost all of experts, e.g., achieving 90.71% activation in Figure 1  ###reference_### (a), allowing for more efficient scaling of model capacity.\n(2) Finer-grained understanding ability. Multi-head mechanism adopted in MH-MoE assign sub-tokens to different experts, enabling to jointly attend to information from different representation spaces at different experts, and finally achieving better finer-grained understanding ability. For example, refer to the bright area in Figure 1  ###reference_### (b), where sub-tokens are distributed among a more diverse set of experts, facilitating the capture of semantically-rich information.\n(3) Seamless integration. The implementation of MH-MoE is remarkably straightforward and decoupled from other SMoE optimization methods (e.g., GShard (Lepikhin et al., 2020  ###reference_b18###)), making it very easy to integrate them together to achieve better performance.\nWe evaluate the proposed MH-MoE on three model pre-training and fine-tuning setting: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling. Extensive experimental among these three tasks demonstrate the effectiveness of MH-MoE."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Sparse Mixture of Experts. Sparse Mixture-of-Experts (SMoE) (Shazeer et al., 2017b  ###reference_b30###; Du et al., 2021  ###reference_b9###; Chi et al., 2022  ###reference_b3###; Clark et al., 2022  ###reference_b5###) enhances model capacity while maintaining a constant computational demand, thus achieving better performance than densely-activated models on various tasks (Lepikhin et al., 2021  ###reference_b19###; Kumatani et al., 2021  ###reference_b16###; Zhao et al., 2023  ###reference_b38###; Pham et al., 2023  ###reference_b25###) and being emerged as a pivotal advancement in the field of deep learning.\nDifferent from densely-activated models, each MoE layer consists of  independent Feed-Forward Networks (FFN)  as the experts, along with a gating function  to model a probability distribution indicating the weights over these experts\u2019 outputs.\nFor the hidden representation  of each input token, the gating value of routing  to expert  is denoted as:\nwhere  denotes the trainable embedding of the -th expert and . Then, the corresponding  experts, according to the top- gated values, are activated and the output  of the MoE layer is\nwhere  denote activated experts set and .\n###figure_4### ###figure_5### (a) SMoE\n(b) MH-MoE\nRouting Mechanism in SMoE. As described above, the most commonly used routing mechanism involves selecting the top- experts from  experts, where  (Shazeer et al., 2017a  ###reference_b29###), e.g.,  = 2 and  = 2048 in GShard (Lepikhin et al., 2020  ###reference_b18###). Such a routing mechanism allows the combination of\ndata parallelism and expert parallelism. Yang et al. (2021  ###reference_b37###) and Lepikhin et al. (2020  ###reference_b18###) suggest that larger values of  often contribute to better model performance. However, with the increase in the value of , training models with conventional top- routing implementation becomes much less efficient (Lepikhin et al., 2020  ###reference_b18###).\nIn this paper, we introduce MH-MoE, a simple but efficient manner to make denser expert activation without an increase in computational complexity."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "The full architecture of MH-MoE can be seen in Figure 3  ###reference_###, MH-MoE addresses low experts activation and confusion over ambiguity of tokens issues by applying a multi-head mechanism to split each token into sub-tokens and route them to various experts to achieve denser expert activation as well as deeper understanding."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Multi-Head Mixture-of-Experts",
            "text": "Concretely, we denote a sequence of inputs tokens by , where  is the number of tokens and  represents the length of token dimension. In MH-MoE, each parallel layer contains a set of  experts, each presented as {},  denotes the number of heads in multi-head mechanism, which is decoupled from the head in the multi-head self-attention layer. For clarity, we describe the operation of a single MH-MoE layer here only.\nFirst,  is projected by a multi-head layer with parameter matrices ,\nwhere . After that, every token in  is split into  sub-tokens along the token dimensions, and these sub-tokens are arranged in parallel according to the original token sequence, forming a new feature space  as:\nwhere function  denotes the token splitting operation: , and each sub-token is presented as , meaning it is the the  sub-token split from the  token.\nThen all these sub-tokens are fed into the gating function . The gating value of routing a certain sub-token  into the  expert is computed as\nwhere  is the learnable embedding of the  expert.\nIn this paper, we mainly focus on top- routing, i.e., only the experts with the largest top- routing score is activated.\n denote the set of activated experts and . Then  is processed by these activated experts as following,\nAfter that, all obtained  are rearranged in the original order of sub-tokens and integrated together as\nwhere .\nAfter that,  is transformed back the into original token form by a token merging operation : :\nwhere . Finally,  is projected by a merge layer with parameter matrices  to effective integration of multiple features  capturing detailed information from different expert representation spaces. The operation is presented as following:\nThen we get the final output  of the single MH-MoE layer.\nWe name the token splitting (Eq. 4  ###reference_###) and token merging (Eq. 8  ###reference_###) operations together as the Token-Splitting-Mergin (TSM) operation. By implementing the aforementioned operations, we have effectively increased the average volume of data routed to a specific expert by a factor of , as demonstrated in Eq. 4  ###reference_###. Consequently, this achievement has resulted in denser expert activation. Furthermore, the allocation of sub-tokens to distinct experts within MH-MoE enables us to collectively capture semantic information from diverse feature spaces across these experts, thereby enhancing the model\u2019s ability to achieve a finer-grained understanding.\nThe operations mentioned above ensure that the shapes of the input and output in the MH-MoE layer remain unchanged. Consequently, no additional computational cost is introduced in the subsequent block. Specifically, we introduce a hyperparameter  to scale the inner dimensions of each expert, aiming to balance the parameters introduced by the multi-head layer and merge layer, aligning the model\u2019s parameters and computational complexity with the original SMoE.\nAs the Pytorch-like style pseudocode of MH-MoE shown in Appendix E  ###reference_###, MH-MoE is characterized by its overall simplicity of implementation, necessitating minimal modifications to the SMoE implementation. Additionally, it is decoupled from other SMoE optimization strategies (Lepikhin et al., 2020  ###reference_b18###; Chi et al., 2022  ###reference_b3###), thereby facilitating its convenient integration with other optimized SMoE frameworks to enhance performance."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Training Objectives",
            "text": "The training objective of MH-MoE involves the simultaneous minimization of both the loss associated with the target task and an auxiliary load balancing loss.\nLoad balancing loss. As described in Section 2  ###reference_###, there is usually an expert load imbalance problem (Xie et al., 2023  ###reference_b36###; Lepikhin et al., 2020  ###reference_b18###). So, following (Lepikhin et al., 2020  ###reference_b18###; Fedus et al., 2022  ###reference_b10###), given the sub-token set  (depicted in Eq. 4  ###reference_###) and the frequency  of how many sub-tokens are routed to the  expert, we compute the load balancing loss  via:\nwhere  denotes the number of experts,  is the number of sub-tokens contained in .  is the gating function depicted in Eq. 5  ###reference_###, denoting the gating value of routing a certain sub-token  into the  expert.\n###figure_6### ###figure_7### ###figure_8### (a)\n(b)\n(c)\nTask specific loss. The term  is dependent on the particular task that MH-MoE is designed to learn. For instance, during pre-training, we utilize the language modeling loss (Radford et al., 2018  ###reference_b26###), whereas the model predicts the next word in a sequence.\nSo, the overall training objective is to minimize:\nwhere  is a coefficient for load balancing."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Perplexity Evaluation",
            "text": "We examined the validation perplexity curves for all pre-trained models and pre-training tasks under two expert settings (8 experts and 32 experts). The perplexity trends are depicted in Figure 4  ###reference_###, with the final perplexity values listed in Table 1  ###reference_###.\nWe can observe that as training progresses: 1) the perplexity of our MH-MoE remained lower in comparison to the compared baselines, indicating more effective learning; 2) MH-MoE achieved the lowest perplexity across three distinct experimental setups; 3) an increase in the number of experts led to a corresponding decrease in the perplexity of MH-MoE, suggesting that the model benefits from enhanced representation learning capabilities as more experts are incorporated.\nThese results collectively demonstrate the superiority of MH-MoE in terms of learning efficiency and language representation across multiple pre-training paradigms."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Downstream Evaluation",
            "text": "For each pre-training task, we conduct corresponding downstream evaluation to validate the efficacy of MH-MoE.\nEnglish-focused Language Modeling.\nWe evaluate our models on a total of 9 different zero-shot benchmarks to assess their abilities across various natural language tasks like common sense reasoning, general language understanding and knowledge understanding using the LLM Evaluation Harness (Gao et al., 2023  ###reference_b11###).\nAs shown in Table 2  ###reference_###, comparing X-MoE with the Dense model, X-MoE show notable improvement, indicating that SMoE models (e.g., X-MoE) benefit from the large model capacity. Overall, for all benchmarks, our MH-MoE obtains the best performance, achieving an average performance gain of 1.1 for 8 experts setting and 1.5 for 32 experts setting compared to X-MoE, demonstrating the effectiveness of our proposed multi-head mechanism on modeling English-focused language.\nMulti-lingual Language Modeling.\nWe evaluate our multi-lingual language models on the cross-lingual natural language inference (XNLI) corpus (Conneau et al., 2018  ###reference_b7###), which is the extension of the multi-genre NLI (MultiNLI) corpus to 14 languages. We follow the LLM Evaluation Harness pipeline and use the zero-shot setting to evaluate the multi-lingual ability.\nTable 4  ###reference_### presents the zero-shot evaluation results on XNLI task. Similarly, X-MoE benefit from the large model capacity and show notable improvement compared with Dense model. Overall, MH-MoE obtains the best performance, surpassing X-MoE by an average performance gain of 0.6 for 8 experts setting and 0.8 for 32 experts setting. Comparing MH-MoE with the X-MoE, it shows that MH-MoE models provide consistent gains on downstream tasks, demonstrating the effectiveness of our proposed multi-head mechanism on modeling cross-lingual natural language.\nMasked Multi-modal Modeling. We evaluate on the widely used vision-language understanding and generation benchmarks, including visual question answering (Goyal et al., 2017  ###reference_b12###), visual reasoning (Suhr et al., 2019  ###reference_b31###) and image captioning (Lin et al., 2014  ###reference_b20###). We report vqa-score on VQAv2, accuracy for NLVR2. For COCO image captioning, we report BLEU@4 (B@4), METEOR (M), CIDEr (C), and SPICE (S).\nTable 4  ###reference_### presents the evaluation results. For VQA task, MH-MoE outperforms both Dense and X-MoE by a large margin, e.g., 4.24 and 1.69 points gain on test-dev split, respectively. For visual reasoning task, MH-MoE beats all these two baselines on both dev (1.5 points gain than X-MoE) and test-P split (1.7 points gain than X-MoE). For image captioning task, MH-MoE surpasses X-MoE by 4.2%, 10.2%, 9.4% in terms of B@4, M and S, respectively.\nAbove results show that X-MoE exhibits enhanced visual information comprehension, which also validates the effectiveness of our proposed multi-head mechanism in capturing diverse semantic and detailed information within visual data."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablation Studies",
            "text": "This section presents experimental analysis to demonstrate the functionality of MH-MoE. In all comparative experiments, we ensure parameter equality across models by adjusting the internal dimensions of the experts.\nNumber of Heads . We conduct experiments by adjusting the number of heads ( = , , , , and ) in MH-MoE. As shown in Table 7  ###reference_###, we find that across all settings of , our model consistently outperforms the X-MoE, demonstrating the effectiveness of MH-MoE. Besides, as the value of  increases, we observe an initial improvement followed by a decline in our model\u2019s performance. This leads us to hypothesize that when  the enhancement in performance benefits from the multi-head mechanism by activating a greater number of experts, thereby enhancing the model\u2019s effectiveness and capturing a wider range of fine-grained token information. However, as  continues to increase beyond 6, the excessive subdivision of tokens may inadvertently impair their original semantic content, resulting in a decrease in model performance.\nEffect of MH-MoE Components. As shown in Figure 3  ###reference_### (b), the multi-head mechanism utilized in our MH-MoE primarily incorporates two components: the Multilayer Perceptron (MLP) layers, including the multi-head layer (Eq. 3  ###reference_###) and merge layer (Eq. 9  ###reference_###), and the Token-Splitting-Merging (TSM) operation (Eq. 4  ###reference_### and Eq. 8  ###reference_###). We conduct a detailed analysis of the effectiveness of each component within our model, as well as the necessity of their integration.\nHarness\n            \n\n\nXNLI\n \n\n\n\n\u2006\nX-MoE\nMH-MoE \nMH-MoE\n###figure_9### The results are presented in Table 7  ###reference_###. A comparative analysis between Dense v.s. Dense, as well as X-MoE v.s. X-MoE, reveals that introduction of the MLP layer does not enhance the model\u2019s performance. Similarly, when comparing MH-MoE with MH-MoE, it becomes evident that the inclusion of only the MLP, in the absence of the TS, also does not yield any improvement in the model\u2019s effectiveness. The parameter quantities of the models being compared pairwise are equal.\nAn intriguing observation is made when comparing MH-MoE with MH-MoE. Introducing Token-Splitting-Merging (TSM) alone, without MLP, results in a slight increase in model performance. In contrast, a significant enhancement in model performance is only achieved when both MLP and TS are incorporated simultaneously. We hypothesize that introduction of TS, without the integration of MLP, activates more experts, but the segmentation and merging of the model appears overly straightforward and abrupt in its execution. This limitation hinders the model\u2019s ability to meaningfully segment tokens into sub-tokens and effectively merge the diverse information gathered from different expert spaces.\nNumber of MLP layers. We explore the impact of varying the number of layers ( = 0, 1, 2, 3) in MLP on MH-MoE performance. For configurations exceeding a single layer, ReLU activation functions were incorporated between MLP layers to ensure the non-linearity of transformations. The parameter quantities of the models being compared are equal.\nUpon analyzing the results in Table 7  ###reference_###, we observe that increasing the number of MLP layers beyond one had a negligible impact on the model\u2019s performance. This indicates that a single-layer MLP is sufficient for accomplishing token segmentation and fusion.\n###figure_10### ###figure_11### (a) Upstream\n\u2006\n(b) Downstream"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experts Activation Analysis",
            "text": "Experts Activation. We visualize the activation of each\nexpert varies across parallel expert layers for X-MoE and MH-MoE at Figure 5  ###reference_###. It can be observed that: 1) X-MoE demonstrate a more skewed distribution, wherein a significant portion of experts remain inactivated all the time. 2) Our MH-MoE achieves a denser expert activation compared to X-MoE, effectively mitigating the issue of low expert utilization. 3) As the number of heads  increases, the expert activation frequency in MH-MoE also rises.\nScalability.\nWe explore the scalability for both X-MoE and MH-MoE by scaling up the number of experts from 8 to 256 (about 7B parameters).\n###figure_12### For upstream performance, as shown in Figure 6  ###reference_### (a), with the increase of experts, our MH-MoE could bring more gains. It is because MH-MoE could mitigate the low expert activation problem effectively. With this ability, the superiority of the large-scale SMoE model will be better exerted, thereby achieving the improvement of the upper bound of SMoE with more experts. Detailed validation perplexity curves for these scaling up experiments can be found in Figure 9  ###reference_### at Appendix F  ###reference_###.\nFor downstream performance shown in Figure 6  ###reference_### (b), for X-MoE, expert number = 64 is the upper bound, meaning that continuing to increase the number of experts will not bring any gain. Our MH-MoE not only has a performance advantage over the X-MoE with the same number of experts, but also improves the upper bound from 64 to 256, which demonstrates the effectiveness of the scalability of our MH-MoE on downstream tasks."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Fine-grained understanding Analysis",
            "text": "In Section 4  ###reference_###, our model excels in multiple upstream and downstream tasks, demonstrating superior fine-grained modeling capabilities, both for languages and images. In this section, we delve into a more granular analysis to validate how the multi-head mechanism aids MH-MoE in capturing diverse and intricate semantic information that is often challenging to comprehend, e.g., polysemous and false cognates words (denoted as PF tokens) in languages, and semantically-rich areas in images. Note that for languages data, we utilized the GPT-4 API (OpenAI, 2023  ###reference_b22###) to extract polysemous words and false cognates from the XNLI (Conneau et al., 2018  ###reference_b7###) corpus, and the corresponding prompt can be found in Table 12  ###reference_###.\n###figure_13### 100k\n200k\n250k\nRGB\n100k\n200k\n250k\nRGB\nExperts Assign within Token.\nFor languages data, we compute and compare the divergence levels (i.e., the number of different experts these sub-tokens are routed to) of sub-tokens split from PF tokens and Non-PF tokens. We conduct on MH-MoE with 8 heads (=8) and represent the divergence of each token by calculating the mean divergence across the model\u2019s various layers. The results, presented in Figure 7  ###reference_###, clearly demonstrate that the distribution of divergence for PF tokens is significantly skewed towards the right when compared to that of Non-PF tokens. This indicates that, in the MH-MoE\u2019s inference process, PF tokens route their sub-tokens to a greater number of different experts, thereby capturing diverse semantic information in contrast to Non-PF tokens for a better polysemous and false cognates word modeling.\nFor image data, we analyzed how the divergence levels of different patches evolve during the training process, as illustrated in Figure 8  ###reference_###. Interestingly, we observe that as the training steps increase, the divergence levels gradually increase in high-frequency texture regions (or regions with rich semantics), while the divergence levels in low-frequency texture regions gradually decrease. This indicates that during the training process, MH-MoE tends to route tokens from areas with complex textures to a greater variety of experts, thereby enhancing the finer-grained understanding of the semantics in that region. For more visualization examples, please refer to the Figure 10  ###reference_### at Appendix G  ###reference_###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Complexity & Parameter Analysis.",
            "text": "We present a analysis of Complexity & Parameter for X-MoE and MH-MoE in Appendix D  ###reference_###, to validate that for all experiments setting, the computational and parameter cost of our MH-MoE are both lower than SMoE. Besides, a detailed parameter count for all experiments and comparable models can be seen in Table 11  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we study how we can to achieve a denser experts activation without introducing additional cost, while improving the fine-grained understanding ability. With the proposed Multi-Head Mixture-of-Experts, we can easily implement the aforementioned functionality. Furthermore, the simplicity of MH-MoE allows it to integrate with other SMoE frameworks to enhance performance easily. Extensive empirical results across three tasks demonstrate the effectiveness of MH-MoE."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Broader Impact",
            "text": "In previous NLP pipelines, the dimension of word tokens has been conventionally maintained constant during both training and inference stages. We are the first to attempt token segmentation outside of the multi-head attention module, aiming to enhance the model\u2019s capabilities in several respects, including a more nuanced and multifaceted understanding of the token content as well as fostering a sparser network architecture. We believe this to be a counterintuitive yet worthwhile exploration in the field."
        }
    ],
    "url": "http://arxiv.org/html/2404.15045v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.15045v1",
        "paper_title": "Multi-Head Mixture-of-Experts",
        "research_background": "**Motivation:**\n\nThe paper is motivated by the need to enhance the performance and scalability of large capacity models such as Large Language Models (LLMs) and Large Multi-modal Models (LMMs). While increasing the parameter count in these models can improve performance, it also significantly reduces inference speed, making them impractical for many applications. Sparse Mixtures of Experts (SMoE) have been proposed as an alternative to achieve scalability while mitigating computational costs. However, SMoE faces issues like low expert activation and the absence of fine-grained analytical capabilities, which limit its effectiveness and scalability.\n\n**Research Problem:**\n\nThe research problem addressed in the paper is two-fold: \n1. Improving the expert activation in SMoE models to better utilize their full expressive power, especially when dealing with a large number of experts.\n2. Enhancing the fine-grained analytical capabilities of the model to better interpret and process multiple semantic interpretations linked to individual tokens, in both visual and language data.\n\n**Relevant Prior Work:**\n\n1. **Dense Models and Scalability:**\n   - Works such as those by Fedus et al., 2022, highlighted the benefits of scaling up model parameters to boost performance, while recognizing the issues with inference speed.\n   \n2. **Sparse Mixtures of Experts (SMoE):**\n   - Notable works include Shazeer et al., 2017b, Du et al., 2021, Chi et al., 2022, and Clark et al., 2022, which introduced the idea of using distinct experts for specific input tokens to enhance efficiency.\n   - GShard (Lepikhin et al., 2020) demonstrated scaling Dense models with lower costs.\n   - Mixtral 87B (Jiang et al., 2024) was shown to outperform or match models like LLaMA-2 70B.\n\n3. **Issues with SMoE:**\n   - The problem of low expert activation was identified, with models activating only a small percentage of experts, leading to underutilization.\n   - The limitations in fine-grained analytical capabilities due to tokenization constraints were also highlighted, impacting the ability to grasp multiple semantics in both visual and language tasks.\n\nThe proposed solution, Multi-Head Mixture-of-Experts (MH-MoE), aims to address these issues by using a multi-head mechanism to split input tokens into sub-tokens and distribute them to different experts, thereby achieving denser expert activation and better fine-grained understanding without additional computational complexity. This approach is evaluated in three model settings, demonstrating its effectiveness across various tasks.",
        "methodology": "**Multi-Head Mixture-of-Experts (MH-MoE)**\n\n**Methodology:**\n\nMH-MoE addresses the issues of low expert activation and confusion over token ambiguity by introducing a multi-head mechanism. This mechanism splits each token into sub-tokens and routes them to various experts, resulting in denser expert activation and a deeper understanding of the tokens.\n\n**Key Components and Innovations:**\n1. **Multi-Head Mechanism:** MH-MoE leverages a multi-head mechanism to handle tokens more effectively. By splitting each token into sub-tokens, it ensures that these sub-tokens can be processed more precisely.\n2. **Routing to Experts:** The sub-tokens are routed to various experts. This routing not only enhances the activation of different experts but also allows for specialized processing of token components.\n3. **Denser Expert Activation:** By routing sub-tokens to multiple experts, MH-MoE ensures that more experts are activated. This leads to a more thorough and comprehensive processing of the input data.\n4. **Deeper Understanding:** The multi-head mechanism combined with expert routing allows for a deeper understanding of the tokens, reducing ambiguity and improving the model's performance.\n\nThis method aims to tackle the challenges of low activation levels and ambiguity in token interpretation by employing a structured approach to expert utilization and token processing.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Baselines:**\n\n1. **Dense:** A Transformer decoder without using sparsely-activated parallel modules (i.e., SMoE layer).\n2. **X-MoE:** An implementation based on Chi et al. (2022), which serves as a foundation for MH-MoE.\n\n**Ensuring Fairness:**\n- All models, including the baseline models and MH-MoE, are pre-trained using the same training data.\n- The parameter count of MH-MoE is consistent with or smaller than that of X-MoE.\n\n**Pre-training Data:**\n1. **English-Focused Experiments:**\n   - Pre-trained on the RedPajama dataset, an open-source dataset including sources like Common Crawl, C4, Wikipedia, and additional curated data.\n   - Utilizes GPT tasks to predict the next word in a sequence.\n   \n2. **Multilingual Representation:**\n   - Pre-trained on multilingual Wikipedia, following the approach described in XLM.\n   - Utilizes GPT tasks for training.\n   \n3. **Multimodal Domain:**\n   - Pre-trained on masked multi-modality modeling tasks using both monomodal and multimodal data.\n   - Includes M images, GB documents, and M image-text pairs.\n\n**Model Architecture and Hyperparameters:**\n- **Backbone Architecture:**\n  - Uses X-MoE from Chi et al. (2022) as the backbone for MH-MoE.\n  - For English-focused and multilingual language modeling, the models use a Transformer decoder (L = 12, H = 768, A = 12) with the GPT-4 vocabulary.\n  - For masked multi-modal modeling, follows the same Transformer encoder architecture as BEiT v3.\n- **Training:**\n  - Pre-training for all tasks occurs on NVIDIA DGX-2 Stations.\n  - Head number for all tasks is set to 4.\n\n### Main Experimental Results\n\nWhile the specific numerical results and comparisons of performance are not provided in the excerpt, the main experiment results should primarily demonstrate the effectiveness of MH-MoE compared to both Dense and X-MoE models across the three different pre-training domains (English-focused, multilingual, and multimodal). These results would validate the hypothesis that MH-MoE, by incorporating the multi-head mixture-of-experts approach, achieves superior performance in task-specific benchmarks while maintaining or reducing computational complexity."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To identify the impact of the number of heads on the performance of MH-MoE and understand how different head settings affect model effectiveness and fine-grained token information processing.",
            "experiment_process": "We conducted experiments with the number of heads set to 2, 4, 6, 8, and 10 in MH-MoE. Throughout the experiments, parameter equality was ensured by adjusting the internal dimensions of the experts. Performance comparisons were made against X-MoE across all settings.",
            "result_discussion": "The results demonstrated that MH-MoE consistently outperforms X-MoE across all settings. Performance improved initially as the number of heads increased, with the optimal performance observed at 6 heads. However, further increases in the number of heads led to a decline in model performance, suggesting that excessive subdivision of tokens may impair original semantic content.",
            "ablation_id": "2404.15045v1.No1"
        },
        {
            "research_objective": "To evaluate the individual contribution and necessity of Multilayer Perceptron (MLP) layers and Token-Splitting-Merging (TSM) components in the multi-head mechanism of MH-MoE.",
            "experiment_process": "We analyzed the effectiveness of each component within MH-MoE by comparing different configurations: Dense vs. Dense, X-MoE vs. X-MoE, MH-MoE with MLP only, and MH-MoE with TSM only. The parameter quantities were kept equal across comparative models. Performance analysis was based on specific metrics presented in Table 7.",
            "result_discussion": "The results revealed that the introduction of the MLP layer alone did not enhance model performance. Similarly, MH-MoE with only MLP, without TS, did not show improvement. However, introducing TSM alone resulted in a slight performance increase. The most significant enhancement was observed when both MLP and TS were incorporated, allowing more effective segmentation and merging of token information.",
            "ablation_id": "2404.15045v1.No2"
        },
        {
            "research_objective": "To determine the optimal number of Multilayer Perceptron (MLP) layers required for token segmentation and fusion in MH-MoE.",
            "experiment_process": "We explored the impact of varying the number of MLP layers (0, 1, 2, and 3) on MH-MoE performance. In configurations with more than one layer, ReLU activation functions were incorporated between MLP layers to ensure non-linearity. Parameter equality was maintained across all model comparisons.",
            "result_discussion": "The analysis indicated that increasing the number of MLP layers beyond one had minimal impact on model performance. This suggests that a single-layer MLP is sufficient to achieve effective token segmentation and fusion.",
            "ablation_id": "2404.15045v1.No3"
        }
    ]
}