{
    "title": "Data-Augmentation-Based Dialectal Adaptation for LLMs",
    "abstract": "This report presents GmuNLP\u2019s participation to the DIALECT-COPA shared task at VarDial 2024 Chifu et al. (2024), which focuses on evaluating the commonsense reasoning capabilities of large language models (LLMs) on South Slavic micro-dialects. The task aims to assess how well LLMs can handle non-standard dialectal varieties, as their performance on standard languages is already well-established. We propose an approach that combines the strengths of different types of language models and leverages data augmentation techniques to improve task performance on three South Slavic dialects: Chakavian, Cherkano, and Torlak. We conduct experiments using a language-family-focused encoder-based model (BERTi\u0107) and a domain-agnostic multilingual model (AYA-101). Our results demonstrate that the proposed data augmentation techniques lead to substantial performance gains across all three test datasets in the open-source model category. This work highlights the practical utility of data augmentation and the potential of LLMs in handling non-standard dialectal varieties, contributing to the broader goal of advancing natural language understanding in low-resource and dialectal settings.111Code and data are publicly available: https://github.com/ffaisal93/dialect_copa",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent advancements in large language models (LLMs) have led to remarkable performance on a wide range of natural language understanding tasks, particularly in standard languages. However, the effectiveness of these models on non-standard dialectal varieties remains an open question Faisal et al. (2024  ###reference_b8###). The DIALECT-COPA shared task, introduced by Ljube\u0161i\u0107 et al. (2024  ###reference_b15###), aims to bridge this gap by evaluating the commonsense reasoning capabilities of LLMs on South Slavic dialects.\nCommonsense reasoning, as originally proposed by Gordon et al. (2011  ###reference_b10###), requires models to make plausible inferences based on everyday knowledge and understanding of the world. Extending this task to dialects poses unique challenges, as models must capture the nuances and variations specific to these language varieties. The DIALECT-COPA shared task provides a platform to explore the adaptability and generalization capabilities of LLMs in this context.\nIn this GmuNLP submission, we explore the potential of data augmentation techniques in enhancing the performance of language models on dialectal commonsense reasoning tasks. Our approach harnesses the power of state-of-the-art LLMs to generate synthetic training data, which we combine with the provided training dataset. By employing a diverse set of language models, we aim to quantify the performance gains achievable through data augmentation. Specifically, we utilize three categories of language models to maximize dialectal task performance: (1) smaller language models that are well-suited for low-resource settings and can be easily customized, (2) mid-size language models that strike a balance between task-specific performance and language understanding capabilities, and (3) closed-source language models that generate high-quality synthetic task data to further enhance the performance of the other two categories of language models.\nWe achieved the highest scores across all three test datasets in the open-source model category. In addition, our solution performed on par with the GPT-4 zero-shot iterative prompting approach employed by one of the teams, demonstrating the competitiveness of the proposed approach against state-of-the-art closed-source models. Furthermore, we achieved substantial performance improvements for the small-scale, language-family-focused model BERTi\u0107 by combining it with our data augmentation strategy, showcasing the effectiveness of our approach in boosting the performance of language models tailored for low-resource settings.\nThe remainder of this paper is organized as follows: Section 2  ###reference_### provides an overview of the DIALECT-COPA shared task and dataset, Section 3  ###reference_### describes our methodology and experimental setup, Section 4  ###reference_### presents our results and analysis, and Section 5  ###reference_### concludes the paper and discusses future directions."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The DIALECT-COPA shared task",
            "text": "In the DIALECT-COPA shared task, a premise sentence is provided along with a question that can be either a cause or an effect. The objective is to build a classifier that selects the most plausible response from two candidate answer choices based on the given premise and question. To illustrate, consider the following training example in English, where the task is to identify the most plausible cause:\n{\"premise\": \"My body cast a shadow over the grass.\",\n\n\n\"choice1\": \"The sun was rising.\",\n\n\n\"choice2\": \"The grass was cut.\",\n\n\n\"question\": \"cause\", \"label\": 0, \"idx\": 0}\n\n\nThe DIALECT-COPA dataset consists of such cause-effect examples across 8 languages and dialects, challenging models to perform commonsense reasoning in non-standard language varieties.\n###table_1### code\nlanguage\nen\nEnglish\nsl\nSlovenian\nsl-cer\nCerkno\nhr\nCroatian\nhr-ckm\nChakavian\nsr\nSerbian\nsr-trans\nSerbian (transliterated)\nsr-tor\nTorlak\nsr-tor-trans\nTorlak (transliterated)\nmk\nMacedonian\nmk-trans\nMacedonian (transliterated)\nThe DIALECT-COPA dataset encompasses training and validation data in 7 languages, including English, 6 moderately resourced South Slavic languages, and two related micro-dialects. The test dataset features these two micro-dialects along with an additional previously unseen dialect. The three dialects in the test set are as follows:\nThe Cerkno dialect of Slovenian, spoken in the Slovenian Littoral region, specifically in the town of Idrija.\nThe Chakavian dialect of Croatian from the northern Adriatic, particularly from the town of \u017dminj.\nThe Torlak dialect, spoken in southeastern Serbia, northeastern North Macedonia, and northwestern Bulgaria, with the specific test instances coming from the town of Lebane.\nCerkno and Torlak dialects are present in all three dataset splits (training, validation, and test) whereas, the Chakavian dialect is intentionally held out from the training and validation splits and is exclusively encountered during the test phase. Each dialect in the test dataset comprises 500 instances. Table 1  ###reference_### presents the detailed statistics of the DIALECT-COPA dataset, providing an overview of the distribution of instances across languages and dialects."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Phases",
            "text": "In this section, we report different phases of our experiments. We step by step perform experiments to choose appropriate base models followed by data augmentation, combination and task-specific model tuning.\n###table_2### 4 shot (2 cause, 2 effect)\n4 shot (2 cause, 2 effect)\n4 shot (2 cause, 2 effect)\nFT (eval. lang)\nFT (eval. lang)\nFT (eval. lang)\nFT (eval. lang)\n###table_3### Original DIALECT-COPA training data\nTransliterated (Cyrillic  Latin) training data\nProviding grammar rules and few-shot Croatian-Chakavian examples to generate synthetic parallel hr-ckm-train examples given the hr-train examples\nAdditional synthetic English training data generated by GPT-4 Whitehouse et al. (2023  ###reference_b19###)\nReverse-augmentation on [lang]-train, [lang]-trans, and [lang]-claude data\nMachine translation of en-gpt4 source data to other languages using the NLLB-6B model\nIn the preliminary phase of our experiments, we conduct a series of trials to identify base language models that demonstrate strong performance on language understanding tasks in a multilingual context. To achieve this, we fine-tune widely-used encoder-based models, such as BERT (Devlin et al., 2019  ###reference_b7###), mBERT, and XLM-R (Conneau et al., 2020  ###reference_b6###), on the English and Croatian subsets of the DIALECT-COPA training dataset. Additionally, we explore the potential of more recently open-sourced large language models (LLMs) of varying sizes, such as LLaMA-2 (Touvron et al., 2023  ###reference_b18###), Aya-101 \u00dcst\u00fcn et al. (2024  ###reference_b21###) and MaLA-500 (Lin et al., 2024  ###reference_b13###), to gauge their effectiveness on the task.\nOur key observations from this preliminary phase are as follows:\nBERT, mBERT, and XLM-R exhibit comparable performance on the Croatian subset, achieving an accuracy of around 55%(+/-) after 3 epochs of in-language fine-tuning. However, the monolingual English BERT model surpasses the multilingual models on the English subset when fine-tuned for the same number of epochs.\nBERTi\u0107 (Ljube\u0161i\u0107 and Lauc, 2021  ###reference_b14###), a transformer-based model pre-trained on Bosnian, Croatian, Montenegrin, and Serbian languages, aligns well with the target languages of the DIALECT-COPA test set. Fine-tuning BERTi\u0107 on the Croatian subset yields a notable performance improvement of approximately 12 percent (i.e. 7 percentage points) compared to the aforementioned multilingual models.\nEmploying 4-shot prompting with the LLaMA-2 7B parameter model results in better performance on the English subset. However, for the Croatian subset, LLaMA-2 generates random inferences. This finding aligns with expectations, as LLaMA-2 is primarily an English-centric model and not inherently multilingual. In an effort to address the multilingual limitations of LLaMA-2, Lin et al. (2024  ###reference_b13###) proposed MaLA-500, a multilingual adaptation of the model that underwent fine-tuning using a causal language modeling objective. However, after this adaptation, MaLA-500 produces random-level inferences on the English subset.\nAya-101, a 13B parameter mt5-xxl-based model Xue et al. (2021  ###reference_b20###) instruction-tuned in 101 languages. It shows superior performance both in English and Croatian.\nBased on these preliminary findings, we select the two best-performing models, Aya-101 and BERTi\u0107 , for further experimentation in the subsequent phases of our study.\nWe report our preliminary experimental findings in Table 2  ###reference_###. The results of our preliminary experiments are summarized in Table 2  ###reference_###.\n###table_4### All original dialect-copa training data mixed together\n[en, hr, mk, sl, sl-cer, sr, sr-tor]-train\n\n[sr, sr-tor]-trans\nCombining all original, transliterated as well as reverse-augmented and synthetic training data (only latin script ones)\n[en, hr, sl, sl-cer]-train\n\n[sr, sr-tor, mk]-trans\n\n[en, hr, mk-trans, sl, sl-cer, sr-trans, sr-tor-trans]-reverse\n\nen-gpt4, hr-ckm-claude\n\n[hr, sl, mk-trans, sr-trans]-nllb\nCombining all original, transliterated as well as reverse-augmented and synthetic training data (Both latin and cyrillic script)\nall available training data\nSelective otrsl setting with upsampled data count by repetition for mk, hr and hr-ckm\nhr-train, mk-trans, hr-ckm-claude\n\n[hr-train, mk-trans, hr-ckm-claude]-reverse\n\n[hr, mk-trans]-nllb\nSelective otrsl setting with upsampled data count by repetition for hr and hr-ckm\nhr-train, hr-ckm-claude\n\n[hr-train, hr-ckm-claude]-reverse\n\nhr-nllb\nSame as previous but for sl and sl-cer\n[sl, sl-cer]-train\n\n[sl, sl-cer]-reverse\n\nsl-nllb\nSame as previous but for sr and sr-tor\n[sr, sr-tor]-trans\n\n[sr-trans, sr-tor-trans]-reverse\n\nsr-nllb-trans\nSame as previous but we include both transliterated as well as Cyrillic script data\n[sr, sr-tor]-train, [sr, sr-tor]-trans\n\n[sr, sr-trans, sr-tor, sr-tor-trans]-reverse\n\nsr-nllb, sr-nllb-trans\nCross-lingual mix and match using all data from otrsl setting\n[en, hr, sl, sl-cer]-train\n\n[sr, sr-tor, mk]-trans\n\n[en, hr, mk-trans, sl, sl-cer, sr-trans, sr-tor-trans]-reverse\n\nen-gpt4, hr-ckm-claude\n\n[hr, sl, mk-trans, sr-trans]-nllb\nCross-lingual mix and match using all data from  setting\nhr-train, mk-trans, hr-ckm-claude\n\n[hr-train, mk-trans, hr-ckm-claude]-reverse\n\n[hr, mk-trans]-nllb\nCross-lingual mix and match using all data from  setting\nhr-train, hr-ckm-claude\n\n[hr-train, hr-ckm-claude]-reverse\n\nhr-nllb\nCross-lingual mix and match using all data from otrslc setting except English\nall available training data except English\nTo address the limited size of the DIALECT-COPA training dataset, which consists of only 400 instances per language, we employ various data augmentation techniques to expand the available training data. This step is crucial in mitigating the data scarcity bottleneck and improving the models\u2019 ability to generalize across diverse dialectal variations. By augmenting the training data, we aim to provide a more representative dataset for task-specific fine-tuning and instruction tuning of our selected language models. The data augmentation approaches we explore include:\nThe test dataset primarily contains instances written using the Latin script. Hence, we transliterate the Macedonian (mk) dataset from Cyrillic to Latin script to maintain consistency with the already available Serbian, and Torlak transliterated datasets.\nFor each instance in the training data, we swap the premise and the correct answer choice, effectively transforming cause examples into effect examples and vice versa, thereby doubling the number of training instances. For example consider the following premise and two \u2018effect\u2019 choices:\npremise: I poured water on my sleeping\nfriend.\n\n\nchoice1: My friend awoke. \u2713\n\n\nchoice2: My friend snored.\nNow our proposed reverse-augmentation method will transform the above example in a \u2018casue-specific\u2019 question as follows:\npremise: My friend awoke.\n\n\nchoice1: I poured water on my sleeping\nfriend.\u2713\n\n\nchoice2: My friend snored.\nWe utilize a publicly available English COPA-style synthetic dataset generated by GPT-4 Achiam et al. (2023  ###reference_b4###), as introduced by Whitehouse et al. (2023  ###reference_b19###). To expand the coverage of this synthetic data to other languages, we translate the English examples using the NLLB-6B machine translation model Team et al. (2022  ###reference_b17###) to all the four DIALECT-COPA standard languages: Croatian, Macedonian, Serbian and Slovenian.\nThe DIALECT-COPA dataset does not provide any training or validation data for the Chakavian dialect. To overcome this limitation, we compile a set of Croatian to Chakavian conversion rules and corresponding examples from online language community forums (uni,  ###reference_b2###). In addition to these rules, we also gather a few Croatian to Chakavian lyrics translations lyr  ###reference_b3###. We then prompt the Claude-3 language model (ant,  ###reference_b1###) with these rules and examples, instructing it to translate the Croatian sentences from the DIALECT-COPA training set into their Chakavian equivalents. Through this process, we create a synthetic Chakavian training set in the style of DIALECT-COPA, which we refer to as [lang]-claude. Here is an example with ground truth Croatian to Chakavian translation (correctly translated words are bolded):\n\u2192\n\nCroatian (source): Djevojka je prona\u0161la kukca u \u017eitaricama. Izgubila je apetit.\n\n\u2192\n\nChakavian (gold-translation): Mlada je na\u0161la neko blago va \u017eitaricah. Je zgubila tiek.\n\n\u2192\n\nChakavian (claude-translation): Divojka je na\u0161la buba u \u017eitarican. Zgubila je tiek.\nWe observe that only a small number of words, specifically three in this instance, are correctly translated from Croatian to Chakavian. Despite the limited accuracy of the translation, this synthetic translated dataset enables us to train and evaluate models on the Chakavian dialect, despite the absence of original training data for this specific dialect. The detailed report on the dialect conversion rules and the Claude-3 prompt template used for generating the synthetic Chakavian dataset can be found in Appendix A  ###reference_###.\nTable 3  ###reference_### provides a comprehensive overview of the data augmentation techniques employed and the languages covered by each approach.\nFollowing the data augmentation process, we create various data combinations by merging the augmented data with the original training datasets. Table 4  ###reference_### provides a comprehensive overview of the various training data combination settings we employ, along with their respective descriptions and the specific data sources included in each combination. These combinations are designed to investigate the impact of different data characteristics on the performance of our models. For instance, the otrsl setting combines all original, transliterated, reverse-augmented, and synthetic data while excluding any data written in the Cyrillic script. The rationale behind this combination is to assess whether our Latin-only DIALECT-COPA test set benefits from the absence of script variations in the training data. Additionally, we introduce a language-agnostic data combination denoted as , in which we perform cross-lingual modifications by ensuring that the premise, choice1, and choice2 for each example are presented in different languages. This combination allows us to evaluate the models\u2019 ability to handle language-agnostic reasoning.\nEncoder-based models can be fine-tuned using any of the data settings created in the previous steps. However, to perform few-shot prompting or instruction tuning with generative language models (LLMs), we need to design prompt-based instructions. During our preliminary experiments, we observed that using 4-shot same-class prompting (i.e., providing 4 cause examples for a cause-based question) yields slightly better results compared to combining 2 cause and 2 effect examples in the prompt. Specifically, this approach led to a 4.9% improvement on the English validation set. So we opted for 4-shot same-class prompting to perform inference.\nThe following prompt template is used for inference and instruction tuning of the Aya-101 model:\nInstruction: Given the premise, {premise}, What is the correct {question} {\u2018before\u2019/\u2018after\u2019} this?\n\n\nA: {choice1}\n\n\nB: {choice2}\n\n\nCorrect {question}: {correct_answer}\nBy designing the prompt in this manner, we provide the model with a clear instruction, the premise, and the two answer choices. The model is then expected to select the correct answer based on the given question type (cause or effect). This template is employed both during inference and instruction tuning of the Aya-101 model to ensure consistency and optimize performance on the DIALECT-COPA dataset.\nWe employ two distinct approaches for task-specific tuning of our selected models. The first approach, known as full model fine-tuning, involves updating all the weights of the model during the training process. We apply this method to the BERTi\u0107 model, fine-tuning it for 5-10 epochs on the DIALECT-COPA dataset. However, for mid-size models like Aya-101, full fine-tuning may be unnecessarily computationally expensive, especially considering the limited amount of training data available.\nTo address this concern, we use LoRA (Low-Rank Adaptation) adapter tuning (Hu et al., 2022  ###reference_b12###) which is a more parameter-efficient tuning approach. LoRA introduces a small number of trainable parameters in the form of low-rank matrices, which are inserted between the layers of the pre-trained model. Note that this draws from a long history of efficient adaption using dedicated units Houlsby et al. (2019  ###reference_b11###); Pfeiffer et al. (2020  ###reference_b16###); Faisal and Anastasopoulos (2022  ###reference_b9###). During training, only these newly introduced parameters are updated, while the original model weights remain frozen. This approach significantly reduces the number of trainable parameters, making it more suitable for fine-tuning on smaller datasets. By employing LoRA adapter tuning, we can effectively adapt the Aya-101 model to the DIALECT-COPA dataset without the need for full model fine-tuning, thereby striking a balance between performance and computational efficiency."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": "In this section, we present and discuss the results of our experiments on the DIALECT-COPA dataset.\n###table_5### System Description\n10-shot with first 10 test instances (without answer)\nRAG implementation; Chakavian and Cerkno lexical dictionary; Reasoning instruction and self referral grading task\n0-shot iterative prompt\n4-shot prompting\nLORA adapter tuning on  4-shot prompting\nLORA adapter tuning on  4-shot prompting\nLORA adapter tuning on  4-shot prompting\nLORA adapter tuning on standard variety of target dialect\nFine-tuning XLM-RoBERTa base for multiple choice QA task\n###table_6### Table 6  ###reference_### presents a comparison of the best-performing submissions from different teams on the DIALECT-COPA test set. We categorize the submissions into two groups: Category 1 includes teams that utilize closed-source model weights, while Category 2 consists of teams that rely on open-source model weights. Our submissions belong to the latter category. We observe that the closed-source GPT-4 model achieves the best overall performance. Team JSI employs GPT-4 with a 10-shot prompting approach, where they provide the first 10 test instances without revealing the answers. Interestingly, even the 0-shot prompting using GPT-4 (by team UNRI) outperforms all submissions in Category 2 using open-source models. Among the Category 2 submissions, GMUNLP (our submission) achieves the highest performance on all varieties. The base Aya-101 model with 4-shot prompting yields the best average score across all languages. However, LoRA adapter tuning on different data combinations results in language-specific best scores.\nTable 7  ###reference_### presents the results of our selected 10 system submissions. We observe that the best performance achieved by the BERTi\u0107 model on the otrsl setting is 62%, which is approximately 17% lower compared to the otrcl-tuned Aya-101 model. When comparing language-specific results, we find that the Torlak (sr-tor) dialect is the easiest to predict for both the Aya-101 and BERTi\u2019c models, while the Cerkno dialect (sl-cer) proves to be the most challenging to learn.\nInterestingly, upsampling the Cerkno dialect-related data (-tuned) does not yield the best score for the Cerkno test-set. Instead, upsampling the Chakavian dialect-related data using the  setting leads to better scores on the Cerkno test set. This observation holds true for both the Aya-101 and BERTi\u0107 base models, indicating that leveraging data from more closely related languages does not always provide the most significant benefit. We believe this phenomenon warrants further investigation to gain a deeper understanding of the complex interplay between language-relatedness and task-specific model performance."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Test Set Insights",
            "text": "Table 6  ###reference_###  ###reference_### presents a comparison of the best-performing submissions from different teams on the DIALECT-COPA test set. We categorize the submissions into two groups: Category 1 includes teams that utilize closed-source model weights, while Category 2 consists of teams that rely on open-source model weights. Our submissions belong to the latter category. We observe that the closed-source GPT-4 model achieves the best overall performance. Team JSI employs GPT-4 with a 10-shot prompting approach, where they provide the first 10 test instances without revealing the answers. Interestingly, even the 0-shot prompting using GPT-4 (by team UNRI) outperforms all submissions in Category 2 using open-source models. Among the Category 2 submissions, GMUNLP (our submission) achieves the highest performance on all varieties. The base Aya-101 model with 4-shot prompting yields the best average score across all languages. However, LoRA adapter tuning on different data combinations results in language-specific best scores.\nTable 7  ###reference_###  ###reference_### presents the results of our selected 10 system submissions. We observe that the best performance achieved by the BERTi\u0107 model on the otrsl setting is 62%, which is approximately 17% lower compared to the otrcl-tuned Aya-101 model. When comparing language-specific results, we find that the Torlak (sr-tor) dialect is the easiest to predict for both the Aya-101 and BERTi\u2019c models, while the Cerkno dialect (sl-cer) proves to be the most challenging to learn.\nInterestingly, upsampling the Cerkno dialect-related data (-tuned) does not yield the best score for the Cerkno test-set. Instead, upsampling the Chakavian dialect-related data using the  setting leads to better scores on the Cerkno test set. This observation holds true for both the Aya-101 and BERTi\u0107 base models, indicating that leveraging data from more closely related languages does not always provide the most significant benefit. We believe this phenomenon warrants further investigation to gain a deeper understanding of the complex interplay between language-relatedness and task-specific model performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we explored the impact of data augmentation techniques on fine-tuning multilingual language models for improving common sense reasoning in dialectal variations. Our experiments encompassed a range of language models, from smaller to mid-sized architectures, to investigate their adaptability to dialectal nuances. The observed variations in performance and the upper limits achieved by different models reflect the diverse ways in which language models handle and adapt to dialectal variations. The insights gained from this work may contribute to the development of more robust and adaptable language models that can handle the challenges posed by dialectal variations. Future work can explore advanced data augmentation techniques, investigate the impact of domain-specific knowledge integration, and develop novel architectures tailored to the unique characteristics of dialects."
        }
    ],
    "url": "http://arxiv.org/html/2404.08092v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.08092v1",
        "paper_title": "Data-Augmentation-Based Dialectal Adaptation for LLMs",
        "research_background": "### Data-Augmentation-Based Dialectal Adaptation for LLMs\n\n#### Introduction\nThe paper is motivated by the significant progress that has been made in large language models (LLMs), which excel in standard language contexts but often lack effectiveness when applied to non-standard dialectal varieties. The focal research problem is the performance gap of LLMs in handling dialectal variations, which remains largely unexplored, as highlighted by the studies of Faisal et al. (2024).\n\nTo address this issue, the paper aligns with the goals of the DIALECT-COPA shared task, established by Ljube\u0161i\u0107 et al. (2024), that evaluates LLMs on their capability to perform commonsense reasoning in South Slavic dialects. Building on Gordon et al. (2011)\u2019s definition of commonsense reasoning\u2014requiring a model to draw plausible inferences from everyday knowledge\u2014the paper points out the unique challenges presented by dialectal varieties which involve specific nuances and variations.\n\nTo bridge this gap, the GmuNLP team employs data augmentation techniques aimed at improving language models for dialectal commonsense reasoning tasks. Their approach involves generating synthetic training data using state-of-the-art LLMs and integrating it with the existing dataset. They utilize a mix of smaller, mid-size, and closed-source language models to maximize performance gains. This strategy not only enhanced performance across all three test datasets in the open-source model category but also showed competitive results against a team that used GPT-4's zero-shot iterative prompting approach.\n\nFurther, their approach led to significant performance improvements for the small-scale, low-resource-focused language model BERTi\u0107, validating the efficacy of their data augmentation strategy in bolstering model performance in low-resource settings.\n\n#### Relevant Prior Work\n1. **Performance of LLMs in Non-standard Languages:** Faisal et al. (2024) focussed on the gap in performance of LLMs when shifted from standard languages to dialectal varieties.\n2. **DIALECT-COPA Shared Task** by Ljube\u0161i\u0107 et al. (2024) is crucial work aimed at evaluating LLMs on South Slavic dialects.\n3. **Commonsense Reasoning:** Gordon et al. (2011) laid the foundation for commonsense reasoning by defining it as the ability to make everyday inferences. This foundational concept is paramount to understanding what is expected of LLMs beyond just language processing.\n\nThe paper builds on these studies by specifically targeting the adaptation of LLMs through data augmentation\u2014a method not extensively explored in the dialectal context.",
        "methodology": "## Methodology: Data-Augmentation-Based Dialectal Adaptation for LLMs\n\n### Preliminary Phase:\nThe initial phase involved identifying effective base models for dialectal adaptation tasks within a multilingual context. The models selected for fine-tuning included BERT, mBERT, and XLM-R on the English and Croatian subsets of the DIALECT-COPA training dataset. Additionally, more recently open-sourced LLMs like LLaMA-2, Aya-101, and MaLA-500 were also evaluated.\n\nKey observations included:\n- BERT, mBERT, and XLM-R showed similar performance (about 55% accuracy) on Croatian after 3 epochs of in-language fine-tuning. The English BERT model outperformed multilingual ones on the English subset.\n- BERTi\u0107, pre-trained on Slavonic languages, showed a 12% (7 percentage points) performance improvement when fine-tuned on Croatian.\n- LLaMA-2 performed well on English with 4-shot prompting but poorly on Croatian\u2014a testimony to its English-centric design.\n- Aya-101, an mt5-xxl-based model trained in 101 languages, displayed superior performance for both English and Croatian.\n\nBased on these findings, BERTi\u0107 and Aya-101 were chosen for further experimentation.\n\n### Data Augmentation:\nTo address the small size of the DIALECT-COPA training dataset, the following data augmentation techniques were employed:\n\n1. **Transliteration:**\n   - Transliteration of the Macedonian (mk) dataset from Cyrillic to Latin script to maintain consistency with Serbian and Torlak transliterated datasets.\n\n2. **Reverse-Augmentation:**\n   - For each training instance, swapping the premise and the correct answer choice to create cause-specific and effect-specific questions, effectively doubling the number of training instances.\n\n3. **Synthetic Data Generation:**\n   - Using GPT-4 to generate synthetic English COPA-style datasets, which were then translated to Croatian, Serbian, Macedonian, and Slovenian using the NLLB-6B machine translation model.\n\n4. **Chakavian Dialect Adaptation:**\n   - Rules and examples for Croatian to Chakavian conversion were compiled from online sources, and synthetic Chakavian training sets were generated using the Claude-3 language model.\n\n### Task-Specific Tuning:\nTwo approaches were taken for model tuning:\n\n1. **Full Model Fine-Tuning:**\n   - Applied to BERTi\u0107 for 5-10 epochs on DIALECT-COPA dataset, updating all model weights during training.\n\n2. **LoRA (Low-Rank Adaptation) Adapter Tuning:**\n   - Used for mid-size models like Aya-101 to introduce trainable low-rank matrices within the pre-trained model layers. This method updated only the new parameters while keeping the original weights frozen, allowing efficient fine-tuning on smaller datasets.\n\n### Prompt Design:\nFor using prompt-based instructions with generative language models, 4-shot same-class prompting (four cause or four effect examples) was found to yield a 4.9% improvement on the English validation set. The following prompt template was used for the Aya-101 model:\n\n```\nInstruction: Given the premise, {premise}, What is the correct {question} {\u2018before\u2019/\u2018after\u2019} this?\n\nA: {choice1}\nB: {choice2}\n\nCorrect {question}: {correct_answer}\n```\n\nThis template provided clear instructions, the premise, and two answer choices, streamlining both inference and instruction tuning processes.\n\nIn conclusion, the methodology combined various augmentation techniques with assessment and tuning of both encoder and generative models to adapt to dialectal variations effectively, fostering better performance in language-understanding tasks.",
        "main_experiment_and_results": "### Datasets\nThe experiments were conducted on the DIALECT-COPA dataset.\n\n### Baselines\nThe main baselines evaluated were:\n1. **Zero-shot prompting with GPT-4 (team UNRI)**\n2. **10-shot prompting with GPT-4 (team JSI)**\n3. **4-shot prompting using the base Aya-101 model**\n\n### Model Implementations\n- **RAG implementation:** Included Chakavian and Cerkno lexical dictionary, reasoning instruction, and self-referral grading task.\n- **Zero-shot iterative prompting** \n- **4-shot prompting**\n- **Fine-tuning XLM-RoBERTa base:** Aimed at the multiple-choice QA task.\n\n### Evaluation Metrics\nPerformance was evaluated using:\n- Accuracy on different dialect test sets in the DIALECT-COPA dataset.\n- Best average score across languages.\n- Language-specific best scores.\n\n### Main Results\n- **Performance Comparison:**\n  - The closed-source GPT-4 model achieves the best performance overall.\n  - Even 0-shot prompting using GPT-4 outperforms all open-source model submissions.\n  - Among open-source models, GMUNLP (our submission) achieves the highest performance across all varieties.\n\n- **Key Findings:**\n  - The base Aya-101 model with 4-shot prompting yields the best average score across all languages.\n  - The BERTi\u0107 model achieves the highest performance on the otrsl setting with 62%, but this is still 17% lower than the otrcl-tuned Aya-101 model.\n  - The Torlak dialect (sr-tor) is the easiest to predict for both Aya-101 and BERTi\u0107 models, whereas the Cerkno dialect (sl-cer) is the most challenging.\n\nThese results indicate that leveraging data from closely related languages does not always guarantee better performance, suggesting that the relationship between language relatedness and task-specific model performance may be more complex than initially assumed. Further investigation is warranted to understand this interplay better."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Identify base language models that demonstrate strong performance on language understanding tasks in a multilingual context.",
            "experiment_process": "In the preliminary phase, we fine-tune widely-used encoder-based models, such as BERT, mBERT, and XLM-R, on the English and Croatian subsets of the DIALECT-COPA training dataset. Additionally, we explore the potential of open-sourced LLMs like LLaMA-2, Aya-101, and MaLA-500. We assess performance after 3 epochs of in-language fine-tuning and use the best-performing models for further experimentation.",
            "result_discussion": "BERT, mBERT, and XLM-R achieved around 55% accuracy on the Croatian subset. Monolingual BERT outperformed on the English subset. BERTi\u0107 showed a notable performance improvement of 12% on the Croatian subset. LLaMA-2 performed well on English but generated random inferences on Croatian. Aya-101 showed superior performance in both English and Croatian, leading to its selection for further study along with BERTi\u0107.",
            "ablation_id": "2404.08092v1.No1"
        },
        {
            "research_objective": "Create synthetic training data and assess its impact on model performance for handling non-standard dialectal varieties.",
            "experiment_process": "To address data scarcity, various data augmentation techniques were used, including transliterating Macedonian from Cyrillic to Latin script, reverse-augmenting cause-effect examples, generating synthetic data with GPT-4, and creating a synthetic Chakavian dataset using Claude-3 translation based on compiled rules. These augmented datasets were then combined in various settings to assess their impact on the models, which were fine-tuned using LoRA adapter tuning for computational efficiency.",
            "result_discussion": "Combining datasets from multiple languages boosted performance on the Croatian subset, and increasing data quantity via data augmentation improved performance for most languages and dialects. Discarding Cyrillic script instances boosted performance for certain languages while hurting others. Both BERTi\u0107 and Aya-101 models showed better performance with these augmentations, with language-specific tuning yielding the best results for respective dialects.",
            "ablation_id": "2404.08092v1.No2"
        },
        {
            "research_objective": "Evaluate the performance of different models and settings on the DIALECT-COPA test set.",
            "experiment_process": "Models were tested on the DIALECT-COPA test set using various settings. The Aya-101 model was used with 4-shot prompting and LoRA adapter tuning on different data combinations. The performances of these models were compared against those of other competing submissions.",
            "result_discussion": "Closed-source GPT-4 models achieved the best overall performance on the test set. Among open-source models, GMUNLP\u2019s submission (Aya-101) achieved the highest performance. BERTi\u0107's best performance on one setting was 62%, while the Aya-101 model achieved 17% higher accuracy. Torlak dialect predictions were easier, while Cerkno dialect was the most challenging. Leveraging data from closely related languages didn\u2019t always yield the highest performance, indicating complex interplay between language-relatedness and model performance.",
            "ablation_id": "2404.08092v1.No3"
        }
    ]
}