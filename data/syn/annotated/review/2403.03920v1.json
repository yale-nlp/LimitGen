{
    "title": "Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts",
    "abstract": "This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts. We integrate Richard Elmore\u2019s Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement. Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development. We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructional dynamics. This paper emphasizes the importance of aligning AI/ML technologies with pedagogical goals to realize their full potential in educational settings, advocating for a balanced approach that considers ethical considerations, data quality, and the integration of human expertise.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Instructional improvement is an iterative process where educators harness data-driven insights to refine the teaching practices and improve student learning. To meet the shifts in post-pandemic learning needs and the demand of artificial intelligence (AI) advancement on workforce development, the education system seeks new instructional and learning strategies that are personalized, effective, safe, and scalable (Cardona et al., 2023  ###reference_b9###). Throughout the years, richer and more complex educational data have been generated by the advancement of instructional practices, providing vast potential for analyses but at the same time posing challenges to the approaches that process such data. Conventional quantitative methods are limited by the capacity of calculation and the efficiency of models, hence preventing efforts to improve teaching and learning outcomes. AI/ML approaches are able to effectively process the existing and forthcoming complex data with scalability and precision (Berland et al., 2014  ###reference_b6###), presenting an unprecedented opportunity to promote the research and instructional practices in education. These characteristics of new data and methods provide timely and actionable insights into the dynamics of the instructional environment. Furthermore, in recent years, this trend has been accelerated by the rapid adoption of generative AI tools, such as ChatGPT and Bard, which synergizes the capabilities of both text analysis and generation. A new field of research has emerged, in which researchers integrate the cutting-edge AI/ML techniques with educational domain knowledge of curriculum, teaching, and learning and to explore crucial questions for instructional improvement.\nTo ground the discussion of this paper into a clearly defined domain, we need to first provide general backgrounds and definitions of related terms. Artificial Intelligence, or AI, is about automation and generative capabilities. AI systems leverage hardware, algorithms, and data to generate \u201cintelligence,\u201d enabling tasks such as decision-making, pattern discovery, and the execution of various actions (Ruiz and Fusco, 2023  ###reference_b52###). Whereas on the other hand, Machine Learning (ML) is a range of approaches to develop algorithms that can identify rules and patterns inside structured or unstructured data including text, audio, video, etc. On the basis of ML, Natural Language Processing (NLP) specifically addresses the interpretation of linguistic data and helps computers manipulate such data to discover patterns (Chowdhury, 2003  ###reference_b12###).\nIn this paper, we will review and discuss how AI/ML methods provide us with innovative solutions to analyze the textual data in education, as well as summarize the promises and pitfalls of these new methodological advancements. To guide our review of this emerging new field, we use the Instructional Core Framework that depicts the dynamic interactions among students, educators, and educational content to create learning opportunities and classroom environments (City et al., 2009  ###reference_b13###; Gillies, 2015  ###reference_b23###; Hennessy et al., 2023  ###reference_b27###). Textual artifacts pertaining to each component and their interactions offer valuable insights into instructional practices. Specifically, we identify three main areas of adaptation of AI/ML analysis of textual data pertaining to three major components of instruction: teacher, student, and content. Each of these three components utilize and generate textual data at the same time. Teachers design curricula and lesson plans as textual data. At the same time, the instruction inside classrooms can also be transcribed to serve as data for analysis on instruction quality. On the other hand, teachers rely on the feedback from student assignments to provide tailored course content. Students submit their assignments as potential textual data for analysis while taking teachers\u2019 instruction as inputs. Content itself serves as a major textual data input, but also intakes new data like feedback from both of the other two components. For each component, we offer example studies and conduct a nuanced examination of the complexities, potentials, and limitations inherent in AI/ML-powered textual analysis of instructional artifacts. In the end, we remain cautiously optimistic about the value of this line of research in advancing education and identify several major directions for future research."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Instructional Core Framework",
            "text": "We use the Instructional Core framework to facilitate the categorization of textual analysis in educational research, analyze the use cases, and anticipate future directions. We also use this framework to contextualize the findings and assist the audience to understand the broader implications of the results within the existing body of knowledge. The instructional Core framework challenges prevailing paradigms, advocating for a departure from teacher-centric instructional approaches towards a more dynamic interplay of pivotal constituents (City et al., 2009  ###reference_b13###). Teachers, students, and content converge as the central components in these intricate pedagogical interactions. The instructional core functions as a guide through the multifaceted terrain of instruction, offering an analytic lens into the classroom dynamics that are conducive to the enhancement of both teaching and learning.\n###figure_1### The instructional core includes three interdependent components, with dynamic interactions between one another. These components include teachers\u2019 knowledge and skill, students\u2019 engagement in learning, and academically challenging content. The components in the instructional core are decisive of educational performances and achievements as the desired outcomes that improve the quality of teaching and learning. To change performance, either the components themselves or the relationship between teacher and the student in the presence of content need to be changed. It is indicated that basically only three ways of increasing learning and performance exist: increasing the knowledge and skill of teachers, affecting content, and altering the relationship between the components (Elmore, 2008  ###reference_b21###). These approaches aim to make the instructional cores more practical and implement them effectively.\nInstructional improvement can focus on any of the three components in the instructional core, including the quality and dynamics of the interactions between them. For example, efforts for improvement in these three components of instructional core can involve providing feedback on teachers\u2019 knowledge and practices (Bain and Swan, 2011  ###reference_b5###), academic assessment based on group and individual student practice opportunities (Doabler et al., 2019  ###reference_b20###), and generating high-quality and coherent curriculum and materials as instructional content (Harris et al., 2015  ###reference_b26###). These efforts aim to enhance the effectiveness of teaching and learning processes, contributing to instructional improvement in educational settings.\nBesides the individual components, the connections between instructional core components are also key characteristics integrated within the framework, linking teachers, students, and content in a holistic manner. These interdependent relationships provide additional ways to improve educational practices as well, leading to another important aspect in the instructional core framework which is that once you change one component, you have to change them all (City et al., 2009  ###reference_b13###). As indicated in the paradigm, all three components are correlated, meaning that disentangling one from the other two may cause imbalance and consequently fail to create desired outcomes of improving instructional practices. If we aim to enhance the content, we should assess whether teachers are equipped to handle the more advanced curriculum and also consider whether students can effectively engage with it (Elmore, 2010  ###reference_b22###). This demonstrates the interplay between content and teachers, as well as between content and students. Merely incorporating high-level content into the curriculum without taking into account the acceptance of both students and teachers would not be prudent. For instance, to foster student engagement, it is crucial to furnish students with a suitable curriculum while ensuring that teachers are capable of managing varying levels of student achievement and maintaining classroom control. The perspectives may differ, but they all contribute to a comprehensive evaluation of all three components and their interrelationships.\nTo achieve effective instructional improvement, it is crucial to thoroughly gather, organize, and analyze relevant data pertaining to the instructional core. As mentioned, the volume of educational data generated by innovative instructional activities in recent years poses new challenges. With the help of AI/ML, this integrated process enables educators and administrators to gain valuable insights and make informed decisions based on evidence. There are different forms of textual artifacts of classroom instruction generated, such as transcripts of instructional discourses, students\u2019 written homework, chat history between tutors and students, textbooks and different types of Open Educational Resources. These first-hand educational materials and data contain nuanced details and patterns of classroom dynamics which are often absent from traditional, structured, administrative data produced by conventional methods (Abrahamson and S\u00e1nchez-Garc\u00eda, 2016  ###reference_b2###; Scribner and Donaldson, 2001  ###reference_b55###). NLP and other methods can analyze them timely. Then, the integration of education domain knowledge to guide these methods applications will be the key. We envision this type of method will gain momentum and importance moving forward. The rest of this paper will demonstrate how such integration would benefit educational practices from the perspectives of three instructional cores, introducing the AI/ML methods adopted and how the combination with education domain knowledge is implemented.\nTo show how the integration of AI/ML in the instructional core holds immense transformative potential for education in the future, revolutionizing teaching practices and enhancing student learning experiences (Cardona et al., 2023  ###reference_b9###), here we provide an overview of the case studies and beyond. By providing automated feedback on teacher discourse, AI/ML becomes a powerful tool for professional development and instructional change. For students, it facilitates autograding and coursework evaluation, offering valuable feedback and access to intelligent tutoring systems. Additionally, AI/ML enables large-scale data analysis, providing unique insights and assisting in the generation of tasks, lesson plans, and other course materials. This integration not only streamlines administrative tasks but also reshapes the dynamics within the educational landscape. In terms of teacher-content dynamics, AI/ML offers tailored tasks suited to varying cognitive demands while providing teachers with manageable plans. It bridges the gap between teachers and students by diminishing pre-existing perceptions of student achievement levels. This transformation eliminates the divide between teachers\u2019 assumptions, research-based expectations, and students\u2019 actual capabilities, fostering effective conversations and facilitating a more accurate understanding of students\u2019 potentials. Ultimately, the integration of AI/ML in the instructional core has the power to propel education towards unprecedented possibilities, fostering deeper connections and personalized learning experiences."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Textual Analysis Aims to Support Teachers\u2019 Learning",
            "text": "In recent years, the landscape of professional development for educators has undergone a transformative shift, leveraging the capabilities of AI and machine learning to enhance instructional practices. Traditional feedback mechanisms, often sporadic and evaluative in nature, are evolving as technology becomes an integral part of this process (Demszky et al., 2023  ###reference_b18###; Jensen et al., 2020  ###reference_b30###). As part of practical demand, research has just started to demonstrate the efficacy of AI-driven tools in refining teacher discourse and fostering a more inclusive classroom environment. Under this context, NLP offers a powerful approach to measure teaching practices through the analysis of textual data like classroom transcripts by identifying linguistic nuances, communication styles, the frequency and quality of teacher-student engagement (Tosey and Mathison, 2010  ###reference_b62###). This provides educators with valuable insights into their strengths and areas for improvement. In this section, we summarize a few exemplary studies."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Advancing Classroom Discourse: Adopting Natural Language Process (NLP) in Authentic Teacher Questions",
            "text": "Classroom discourse in educational settings manifests through diverse forms, ranging from teacher-directed lectures and procedural communication to open-ended discussions fostering collaborative exchanges between students and teachers (Alexander, 2008  ###reference_b4###; Juzwik et al., 2015  ###reference_b32###). By creating dialogic space and opening up classroom discourse, teachers are able to enhance students\u2019 active learning, particularly in language classrooms, as high-quality classroom discourse is characterized by open and authentic questions along with formative feedback whereby student contributions are probed and elaborated on (Hardman, 2016  ###reference_b25###; Soter et al., 2008  ###reference_b57###).\nThe abundance of authentic questions can promote substantive conversation and are related to student engagement and achievement growth in a broader sense. Kelly et al. (2018  ###reference_b35###) incorporated AI to address the insufficiency of scaling the measure of question authenticity, which previously relied on human observations or coding. The study adopted automatic speech recognition, NLP, and ML to train models to detect authentic questions. The methodology for this study underwent iterative refinement, drawing insights from two primary data sources. Firstly, a comprehensive archival database with text transcripts from 451 observations across 112 classrooms was utilized (Partnership data). Additionally, a new set of 132 high-quality audio recordings from 27 classrooms was collected (Class 5.0 data), adhering to technical constraints aimed at anticipating large-scale automated data collection and analysis. These diverse data sources laid the foundation for the investigation conducted by the research team.\nThe research adopted different methods for different data collections (Kelly et al., 2018  ###reference_b35###). The first one is a semi-automated approach on the Partnership data, relying on human coders to deliver a wide range of different tasks including segmenting the raw classroom audio into teacher utterances, identifying which utterances are questions, distinguishing instructional questions from non-instructional questions, and providing approximate transcriptions of the instructional questions. The next step includes syntactic and discourse parsing (Manning et al., 2014  ###reference_b42###; Surdeanu et al., 2015  ###reference_b58###) which was used to compute sentence (a teacher utterance) and multi-sentence discourse features. The study team measured various properties of language to arrive at a set of 244 features at each level of structure (word, sentence, discourse), including question stems (e.g., \u201cwhat\u201d), word order, and referential chains, etc. The second one, applied on the Class 5.0 data, was a fully automated approach with the ultimate goal of providing a measure of authenticity from a recording of classroom audio without any human involvement.\nWith different approaches and abundant datasets collected, a sufficiently high correlation between the computer- and human-coded authenticity suggests a promising avenue towards more automated measurement. It highlights how NLP techniques can be used for such analysis purposes, not only offering a more efficient and scalable approach to measuring question authenticity but also holding the potential for improving teacher professional development and classroom instruction. However, the authors also recognized the challenges that may occur when implementing in real-world cases, which include issues like noise, dialect diversity, data imbalance, and many more. It is undeniable that authenticity in itself is a complex construct, yet still, the inclusion of automated approaches empowered by NLP can offer some promising solutions to identify authentic questions. By leveraging NLP, innovative approaches can provide high-quality feedback to teachers derived from transcripts of classroom interactions and detect instructional factors which are well-aligned with commonly used observation protocols (Danielson, 2013  ###reference_b16###). This unlocks the potential to enhance existing classroom observation systems through collecting far more data on teaching with a lower cost, higher speed, and the detection of multifaceted classroom practices (Liu and Cohen, 2021  ###reference_b37###).\nAccountable talk moves are effective probes in mathematics classes with amplified student achievement (Michaels et al., 2008  ###reference_b45###). By utilizing advances in automatic speech recognition and NLP, automated approaches can help identify such patterns to direct classroom discourse into a more engaging and efficient way (Jacobs et al., 2022  ###reference_b29###). The pilot study showcases a promising trend of increased talk moves, emphasizing the potential of platforms like the TalkMoves application to democratize access to high-quality feedback for educators. Building on these advancements, the prowess of ML in swiftly and accurately classifying classroom discourse activities is disclosed, offering automated insights that align remarkably well with human-coded evaluations (Wang et al., 2014  ###reference_b71###). This convergence underscores the capacity of NLP to streamline feedback processes and provide invaluable support for instructional improvement by utilizing textual data generated through the instructional practices.\nThe instructional processes in classrooms consist of a diverse set of activities, one of which is \u201clesson study\u201d where teachers systematically examine their practice and improve their instructional practices (Makinae, 2019  ###reference_b41###; Saito, 2012  ###reference_b54###). \u201cLesson study\u201d recognizes the complexity of cognitive demands inside classrooms and provides ways for teachers to understand such complexity. With the assistance of AI/ML, the activities can be operated in a consistent and fluent manner. Appropriated trained and fine-tuned NLP models would efficiently analyze huge amounts of data generated from inside the classrooms and provide accurate feedback for teachers. With guidance from these analyses and feedback teachers can now develop their own learning and understanding, helping both teachers and students achieve high-level tasks in classrooms which consequently predicts higher performance and cognitive skills. By having the high-level tasks and the right instructional practice, students are capable of achieving very competitive outcomes (Newmann et al., 2001  ###reference_b47###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Generative AI for Teacher Coaching",
            "text": "Teachers\u2019 actions and inactions in the classroom become critical in determining the quality of the learning experiences for students and their well-being. Professional development serves as a dynamic and evolving mechanism through which educators engage in continuous learning and refinement of their skills. Internationally, classroom observation is widely recognized as a key tool for teachers\u2019 professional development and evaluation (Martinez et al., 2016  ###reference_b44###). In the loop of such evaluation, the feedback is conventionally provided by school administrators, peer teachers, and instructional coaches. There are also pre-determined protocols and assessments that include various rubrics, facilitating the evaluation process. However, due to the variation in expertise, resources, and human subjectivity under different contexts, the assessments of such kinds inherently have limitations in their validity and applicability (Kelly et al., 2020  ###reference_b34###). The heterogeneity of evaluation processes and feedback indicates disparities in teacher development opportunities and practices, which in turn significantly impact student educational outcomes.\nPrior efforts have shown the promising characteristics of adopting machine learning and natural language processing in assessing teachers\u2019 instructional effectiveness such as detecting authentic questions (Kelly et al., 2018  ###reference_b35###) and accountable talk moves (Jacobs et al., 2022  ###reference_b29###; Suresh et al., 2022  ###reference_b59###). These works mostly include detection and recognition of key elements that have positive impacts on student learning, thus selected to annotate the areas for improvement. Such tasks can be easier to get automated to provide fast and accurate feedback on instructional practices, but are still subject to human interpretation for substantial works of improvement. To provide high-level, actionable feedback similar to the ones proposed by human evaluators and coaches during professional development sessions, generative AI, like GPT-3.5, has huge potential in providing relevant feedback that aligns with conventional coaching. These tools have already demonstrated capabilities in generating texts in education settings, and are widely adopted by teachers and school professionals, playing multiple roles in pedagogical activities (Jeon and Lee, 2023  ###reference_b31###).\nIn the selected case study for this section, researchers investigated whether ChatGPT can help teachers and education professionals by providing effective feedback on a high level, like generating classroom observation rubrics and helpful pedagogical suggestions (Wang and Demszky, 2023  ###reference_b68###). The study was conducted three-fold, assigning three separate tasks to ChatGPT. The first one is to score a transcript segment for items derived from classroom observation instruments, according to criteria of the Classroom Assessment Scoring System (CLASS) and Mathematical Quality Instruction (MQI) rubrics. The second one is to identify highlights and missed opportunities for CLASS and MQI items, elaborating on these elements that occurred in the process of instructional practices. The third one is to provide open-ended, actionable suggestions to the teacher for eliciting more student mathematical reasoning in the classroom. These three stages of task analysis are similar to what evaluation and coaching would look like with human to human interactions, but are on a more efficient basis because of the automated process.\nThe data used in this study was extracted from the National Center for Teacher Effectiveness (NCTE) Transcript dataset (Demszky and Hill, 2023  ###reference_b17###) in this work, the largest publicly available dataset of U.S. classroom transcripts linked with classroom observation scores. The dataset consists of four years (2010-2013) of classroom transcripts from 4th and 5th grade elementary mathematics observations. At the time of data collection, experts assessed and rated the transcripts based on two instruments, both CLASS and MQI. CLASS instrument segmented instructions into 15-minute sections, whereas MQI corresponds with 7.5-minute sections. All randomly selected segments are provided to ChatGPT in combination with zero-shot prompting. The authors measured zero-shot performance firstly because segments of the transcripts are long, causing annotated sections to exceed the input limit. Additionally, zero-shot prompting closely aligns with teachers\u2019 day-to-day interactions with ChatGPT, utilizing the generative AI as it is given without specific fine-tuning. The study recruited human participants to examine the validity of ChatGPT\u2019s performance on all three tasks, evaluating the outputs alongside human ratings. The results indicated that ChatGPT is able to provide relevant responses, yet failed to give novel and insightful feedback. This can be attributed to the scarcity of data, leading to insufficient information around examples of teacher coaching. The incorporation of generative AI presents promising prospects for attaining entirely automated teacher coaching instructions and feedback, contingent upon the availability of ample training data and illustrative examples. While it is acknowledged that zero-shot performances fall short of constituting a flawless solution, it demonstrates both the potential and the areas for improvement.\nIn sum, this study employs a zero-shot performance evaluation approach, mimicking how teachers might interact with generative AI in their day-to-day activities without specific fine-tuning. This approach aligns with practical implementations and offers insights into the model\u2019s adaptability. However, the effectiveness of generative AI, as demonstrated in the study, is contingent upon the availability of ample training data and illustrative examples, the lack of which points out challenges in replicating the novelty and insightfulness of human-generated feedback. Despite illustrating ChatGPT\u2019s performance of zero-shot prompting, the study still calls for development and fine-tuning required for AI models in the context of teacher coaching and professional development."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Textual Analysis for Student Support and Assessment",
            "text": "AI/ML technologies including NLP present diverse avenues for enhancing students\u2019 coursework. This transformative potential is evident in three major applications: (a) generating personalized and adaptive items through fine-tuned models tailored to specific subject areas; (b) analyzing and auto-grading coursework; and (c) providing students with feedback, including intelligent tutoring. Just as AI/ML contributes to the democratization of high-quality feedback for educators in professional development, its applications in supporting student learning hold the promise of more personalized, adaptive, and efficient learning experiences for students.\nIntegrating AI/ML in adaptive learning emerges as an approach to customize instruction to learners\u2019 background, experiences, and prior knowledge (Alam, 2023  ###reference_b3###; Peng et al., 2019  ###reference_b49###; Walkington, 2013  ###reference_b66###). It possesses the capability to recommend optimal content from an ample amount of available resources, provide guidance on well-structured long-term curricula, and facilitate the connection of suitable learners through suggestions and precise performance evaluations, among other functionalities (Maghsudi et al., 2021  ###reference_b40###). The range of customization spans across different subject areas, as well as various means of pedagogical designs (Ruiz-Rojas et al., 2023  ###reference_b53###).\nFurthermore, teachers have used AI/ML-powered tools to grade assignments, provide efficient and timely feedback to students, and predict student learning trajectory and outcome (Chen, 2018  ###reference_b11###; Wilson et al., 2022  ###reference_b74###). This automation of assessment processes offers benefits such as rapid evaluation, consistency in grading, and the ability to handle large volumes of student work. This type of technology takes input from student submissions through online platforms or through transcribed responses to conduct computer-based algorithms like text mining in order to find the similarities between student responses and the assessment requirements determined by teachers in grading criteria (Kakkonen and Sutinen, 2004  ###reference_b33###). By targeting the implementation of AI/ML methods to free-text questions for testing the comprehensive ability of students, teachers can be alleviated from heavy workload with the assistance of automated grading systems which focus on the critical words and sentences, analyze the logical semantic relationship of the context and predict the interpretable grades (Wang et al., 2018  ###reference_b70###). Even without the grading rubrics and building NLP models by only taking human graded answers into consideration, some currently available auto-grading technology can still handle the assessment and achieve a good inter-rater agreement with expert grading (Yang et al., 2017  ###reference_b75###).\nThe third application, intelligent tutoring, is a dynamic combination of the previous two. By leveraging student log data and utilizing sentence\u2010level semantic representations of student responses to open\u2010ended questions, AI/ML can provide a collaborative filtering\u2010based approach to both predict student scores as well as recommend appropriate feedback messages for teachers to send to their students (Botelho et al., 2023  ###reference_b8###). Intelligent Tutoring Systems (ITSs) have been central to deliver adaptive guidance and instruction, evaluate learners, define and update the learner\u2019s model, and classify or cluster learners (Corbett et al., 1997  ###reference_b15###; Mousavinasab et al., 2021  ###reference_b46###; Nwana, 1990  ###reference_b48###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. ChatGPT for Automated Scoring and Assessment",
            "text": "Generative AI like ChatGPT utilizes LLMs to generate responses in accordance with prompts and inputs. By further fine-tuning the models, especially when such approaches are provided and integrated into the service, LLMs can demonstrate higher levels of performance specifically designed to tackle different tasks. For example, with sufficient training data, a 2-billion parameter model named MathGLM can provide multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (Yang et al., 2023  ###reference_b76###). Beyond this, generative AI are adopted under other contexts including automatically scoring student-written constructed responses using example assessment tasks in science education.\nOthers investigate the application of fine-tuned ChatGPT (GPT-3.5) for this specific task (Latif and Zhai, 2023  ###reference_b36###). While GPT-3.5 has demonstrated proficiency in natural language processing, its direct use for scoring student responses is limited by the contextual variation in language. The study engaged in a fine-tuning process, training GPT-3.5 on specific assessment tasks using a dataset that included student responses and expert scores. The research encompassed six tasks, each characterized differently, to comprehensively assess the model\u2019s performance. A comparative analysis was conducted, comparing GPT-3.5 with BERT\u2014 which stands for Bidirectional Encoder Representations from Transformers, another pre-trained natural language processing model that has significantly advanced language understanding tasks by capturing bidirectional context representations (Devlin et al., 2019  ###reference_b19###). The research aimed to gauge the effectiveness of GPT-3.5 in enhancing automatic scoring accuracy. Additionally, the study explored text data augmentation as a creative strategy to leverage GPT-3.5-turbo\u2019s capabilities for enhancing automated evaluation precision. It is demonstrated that the effectiveness of this approach in producing a more extensive and diverse training set for scoring mechanisms, leading to improved performance in evaluating student text responses (Cochran et al., 2023  ###reference_b14###). Furthermore, the study adopted a benchmark PandaLM\u2014 which is a tool designed for optimizing domain-specific training and refining large language models\u2019 instructions (Wang et al., 2023  ###reference_b69###)\u2014 to facilitate a comprehensive evaluation of model performance across various tasks. The results indicate that fine-tuned GPT-3.5 outperformed BERT in scoring accuracy across all six tasks. GPT-3.5 demonstrated a remarkable average increase of 9.1% in accuracy for the multi-label and multi-class tasks, affirming its effectiveness in domain-specific automatic scoring. The findings suggest that domain-specific fine-tuning enhances the performance of language models for educational assessment tasks, providing a valuable tool for educators and researchers. The fine-tuned models have been released for public use and community engagement, which further supports its practical implications.\nDespite great promise, potential limitations may include the need for extensive fine-tuning and a broader generalizability study of model performance and utility across various educational contexts, like online/in-person instruction or class sizes, and subjects."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Textual Analysis for Content Analysis and Development",
            "text": "Data science technologies and artificial intelligence can provide unprecedented opportunities for educational research. The capabilities of analyzing and generating text data can be incorporated into the content aspect of Elmore\u2019s Instructional Core Framework. Specifically, natural language processing can be applied to massive amounts of text data and by analyzing them, can provide distinct insights which previously were neglected. Additionally, generative AI enables teachers to pay more attention to working with students, as these specifically trained language models can relieve them from many routine tasks like lesson planning.\nCultivating the power of AI/ML with text data is a promising approach to analyze student essays, forum discussions, and other educational text data to gain insights into learning processes, identify common misconceptions, and improve learning outcomes. Textbooks are widely utilized as learning and instructional technologies (Torney-Purta et al., 2001  ###reference_b61###), and are swiftly evolving along with the trends of technological development. Furthermore, high-quality textbooks and instructional materials in general can have significantly positive effects on student learning and outcomes (Read, 2015  ###reference_b50###). Large language models (LLMs)\u2014a type of language model that can both understand and generate texts\u2014is an integrated approach to both analyze the previous resources and provide new materials based on insight gained from the analyses. Drawing content from 15 U.S. history textbooks, researchers examined the language using several methods to identify the prevalent topics of textbooks, the actors discussed (gender, ethnicity) and their characterization (as passive or active via lexicons), and the contexts they are related to (Lucy et al., 2020  ###reference_b39###). Besides running analyses on previous textbooks and educational resources, LLMs and generative AI show promise in other applications, including in-time evaluation on online educational resources, which are accumulating swiftly and providing new insights."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Leveraging Technology and Expertise to Evaluate Online Resources for Effective Mathematics Instruction",
            "text": "As the online landscape of educational resources burgeons, schools and educators are increasingly turning to these materials alongside traditional textbooks. Open educational resources (OERs), with their undeniable promise of cost-effectiveness and accessibility, present a prospect for overcoming the educational gap and fostering educational justice by democratizing the access to high-quality materials (Richter and McPherson, 2012  ###reference_b51###). However, a crucial question looms: how can we ensure the pedagogical efficacy and curricular alignment of these readily available resources, particularly in the critical developmental stage of mathematical learning at a given age?\nAn interdisciplinary group of researchers, funded by National Science Foundation, proposes a novel and rigorous approach to addressing this critical gap by integrating domain knowledge in mathematics instruction with the latest technologies of AI/ML analysis of text data."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1. Open Education Resources as Data",
            "text": "The study begins with an extensive data collection phase, amassing over 300,000 math lesson plans from a variety of OER platforms, including widely known sites like BetterLesson, IllustrativeMathematics, and Achieve the Core. The research team pays special attention to the representativeness of the sampled lesson plans to guarantee that the collection exemplifies the breadth of resources available for educators. The gathered lesson materials are methodically organized and stored using advanced data management solutions, enhancing efficiency in data handling and allowing for the integration of various types of data, such as download numbers and comments from the OER websites. The team also conducts regular exploratory data analysis to examine the distribution of key measures within the lesson materials. This process is crucial for making sense of the data and identifying any potential errors. This robust data collection and analysis formed a solid foundation for the subsequent phases of the study, where the quality of these lesson materials are scrutinized and enhanced through human-centered evaluations and AL/ML algorithms."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2. Classification methods of high quality lesson plans: human-centered data science approach",
            "text": "Central to the study is a human-centered method for assessing the quality of lesson plans, an approach instrumental in ensuring the reliability and validity of both the measures and the algorithms employed. This approach initiates with the development of a theory/conceptual framework for high-quality instruction through comprehensive literature review and the application of the Delphi method, a mixed-method technique combining surveys and an expert panel to achieve consensus on key aspects of instructional quality and their measurement. Utilizing domain expertise from educators and researchers, the study formulates rigorous quality measures and coding rubrics, laying the groundwork for human coding. A team of experienced teachers and instructional coaches then manually codes 1,000 selected lesson plans, evaluating them against the established criteria for curriculum alignment and pedagogical efficacy. This step provides a nuanced understanding of what constitutes quality in K-12 mathematical instruction, ensuring the machine learning (ML) algorithms, trained on these evaluations, are based on a well-defined framework and expert-endorsed training dataset.\nThe research team combines machine learning algorithms with human expertise to assess lesson plans in a comprehensive study to enhance the quality of educational resources. These algorithms are specifically developed to identify essential quality markers in lesson plans, such as high cognitive levels and the presence of deep, thought-provoking questions. To augment the insights from human coding, the research team develops ML algorithms to identify patterns and markers of quality in lesson plans, offering a scalable solution for assessing the broad array of available lesson materials. Concurrently, 30 math educators are employed to rate 2,000 machine-coded lesson plans using the same rubrics developed for this study. The feedback and ratings from these teachers are instrumental in refining the algorithms.\nMoreover, the research team employs multi-task classification based on two powerful pre-trained language models BERT (Devlin et al., 2019  ###reference_b19###) and RoBERTa (Liu et al., 2019  ###reference_b38###), trained on the data from these 2,000 coded lesson plans. This approach enables the algorithm to rate various aspects of lesson plan quality, providing a comprehensive assessment. By integrating indicators like cognitive complexity and question depth, the system becomes adept at recognizing high-quality educational content. Researchers typically use several evaluation methods to validate the quality ratings generated by the classification. These include cross-validation, where a subset of the data is used to test the algorithm, and comparing algorithm ratings with human experts. This rigorous validation process ensures that the machine learning assessments are both accurate and aligned with the practical standards set by experienced educators. Through this blend of AI and human judgment, the study aims to offer a robust tool for educators, guiding them toward high-quality, pedagogically sound resources in the evolving digital education landscape. Therefore, by fine-tuning AI\u2019s assessment capabilities through teachers\u2019 practical understanding of quality measures drawn from classroom experience, the study ensures that algorithms are not only theoretically sound but also practically relevant and effective. Integrating human expertise and automation presents a powerful tool for educators, enabling them to navigate and select high-quality OERs, thereby addressing the critical need for pedagogical efficacy and curricular alignment in the digital age of education."
        },
        {
            "section_id": "5.1.3",
            "parent_section_id": "5.1",
            "section_name": "5.1.3. Future direction: fine-tune LLM to generate lesson materials that are consistent with these quality measures",
            "text": "The future direction of the research project involves fine-tuning LLMs to generate lesson materials that adhere to established quality measures. This initiative marks a pivotal shift from evaluating existing educational resources to actively creating new ones using cutting-edge AI technology. The primary goal of fine-tuning LLMs is to align the generated lesson materials with the high-quality benchmarks set in the study\u2019s earlier phases. Furthermore, insights from teacher evaluations of existing lesson plans will inform the iterative fine-tuning process. The continuous refinement will also involve pilot testing of the AI-generated materials in real classroom settings to gather empirical data on their effectiveness and to make context-specific adjustments. A key challenge will be bridging the gap between AI capabilities and educational expertise. Addressing this, the research emphasizes interdisciplinary collaboration, uniting AI researchers, data scientists, and educational experts to ensure that the LLMs are trained and fine-tuned in a manner that accurately reflects the pedagogical principles and practices. The team will also continuously explore customization and adaptability of LLMs, aiming to create tailored lesson materials that cater to diverse learning styles, student needs, and curriculum requirements. Ethical considerations, particularly regarding the use of data and the potential impacts on educational equity, will remain a priority. Moreover, practical aspects like the scalability of the AI-generated materials and their integration into educational infrastructures will be key focal points. The proposed future direction with LLMs represents a forward-looking move to educational content creation, leveraging the latest advancements in AI to produce high-quality, adaptable, and effective educational resources.\nTo conduct supervised learning qualitative and quantitative methodologies to offer a nuanced and comprehensive assessment of large-scale lesson plan datasets. By orchestrating a symphony of cutting-edge natural language processing (NLP) techniques with established human coding methodologies, the project will delve deep into the world of online lesson plans, specifically focusing on those readily available under Creative Commons licenses and tailored to the specific needs of middle grades mathematics education. This ambitious undertaking is driven by three core research aims: forging a conceptual framework, validating measures of quality, and illuminating the teacher\u2019s lens.\nA cornerstone of the study is the collaborative construction of a robust conceptual framework defining the key dimensions of high-quality middle-grades math lesson plans. This framework will be meticulously crafted by convening an expert panel of leading mathematics education researchers and seasoned practitioners. This collaborative effort will foster a shared understanding of what constitutes effective instruction in this crucial developmental stage, laying the groundwork for subsequent evaluation endeavors.\nThe study will then move towards developing and rigorously validating reliable measures of these key quality dimensions. This complex process will leverage both the analytical prowess of state-of-the-art computational tools, including machine learning algorithms, and the meticulous insights gleaned from human coding practices. This dual approach will ensure the accuracy and comprehensiveness of the assessment process, enabling the project to effectively evaluate a vast collection of online lesson plans.\nTo gain a deeper understanding of the human element within this equation, the study will embark on an exploratory mixed-methods investigation. This research phase will shed light on the cognitive processes employed by teachers as they navigate, interpret, and ultimately select lesson plans for their students. By unveiling the critical link between lesson plan quality and its tangible impact on student learning outcomes, this innovative approach will provide invaluable insights for both researchers and practitioners alike.\nThe transformative potential of this project for the field of mathematics education research is undeniable. By harnessing the combined power of advanced technology and human expertise, the study will move beyond traditional assessment methods and delve into the multifaceted nature of lesson plan quality. This comprehensive framework will empower educators to confidently navigate the labyrinth of online materials, armed with reliable data-driven insights to inform their decisions for optimal student learning. In doing so, the project paves the way for a future where evidence-based practices reign supreme, propelling mathematics education to new heights of effectiveness and student success."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Conclusion and Future Directions",
            "text": "This paper summarizes this emerging field of adopting AI/ML powered textual analysis for instructional improvement. We utilize the instructional core framework, to guide the review and discussion of existing studies pertaining to three components of teacher, student, and content, providing one to two case studies to illustrate the scenarios under which AI/ML can be implemented. AI, particularly Large Language Models and Generative AI, holds the potential to expedite the translation of research into EdTech products and redefine research methodologies. We highlight the transformative capacity of these technologies in enhancing instructional efficiency and effectiveness. By weaving together learning and instructional theories and latest technologies, we envision that AI-powered textual analysis and generation will make a significant contribution to K-12 research and practices. This paper is written to inspire dialogue, foster innovation, and chart a path toward an era of unparalleled educational excellence. Specifically, we envision the following trends and applications.\nThe mutually reinforcing relationship between AI/ML methodologies and the field of education is vital for their deep integration. This relationship is characterized by a bidirectional flow of insights. AI/ML advancements introduce revolutionary shifts in various educational aspects, such as reshaping teaching and learning experiences and enhancing educational tools. Conversely, educational domain knowledge is essential to ensure the relevance, usefulness, and effectiveness of AI/ML models in educational contexts. For instance, a significant research direction in AI for education is its adaptive role in supporting a diverse student body. AI can tailor personalized learning paths to align with individual students\u2019 unique needs, preferences, learning histories, and styles. By analyzing student performance, AI systems can swiftly identify areas of struggle and strength, enabling curriculum adjustments. This individualized approach enhances education accessibility and efficacy (Mansouri et al., 2023  ###reference_b43###). Importantly, human expertise in curriculum design is crucial for the success of AI applications. For effective learning trajectory recommendations, machine learning models depend on domain knowledge-derived knowledge graphs for accurate predictions (Shaik et al., 2022  ###reference_b56###). Additionally, AI tools help reduce teachers\u2019 administrative burdens, freeing them to focus more on instruction. For example, various higher education institutions have adopted commercial AI-based software like Gradescope and Canvas for grading and attendance monitoring. However, adopting these tools without proper human oversight and modification can introduce unnoticed biases and potentially harm student-teacher rapport. This challenge requires researchers and practitioners aiming to advance AI in education to revise the field\u2019s ontology through novel methodologies and establish a robust feedback loop between educational domain knowledge and AI/ML models. The effectiveness of such feedback loops is essential for continuous improvement and refinement during development and implementation.\nFurthermore, as AI integration in education progresses, addressing ethical considerations becomes increasingly important. Incorporating human elements and domain knowledge into AI/ML algorithms is a promising approach to ensure fairness, accountability, transparency, and explainability. Additionally, involving educational stakeholders in the research and development process is essential for ensuring the inclusivity and cultural responsiveness of AI-driven tools and their applications. An effective AI/ML tool should aim to bridge educational gaps and tackle some of the most pressing challenges in education. The synergy between AI\u2019s proficiency in pattern recognition and automation and human insight in context-specific understanding has the potential to foster an adaptive, inclusive, and equitable educational ecosystem, which can empower all learners and educators by providing high-quality, personalized educational resources without bias across socio-economic backgrounds, highlighting the technology\u2019s role in promoting educational equity.\nGenerative AI focuses on generating human-like content based on patterns and context, as well as training data. While traditional NLP primarily involves tasks such as text classification, sentiment analysis, and information retrieval, where the emphasis is on extracting specific information or insights from text. For generative AI, it is capable of completing both creative and non-creative tasks with good accuracy (Gozalo-Brizuela and Garrido-Merchan, 2023  ###reference_b24###). Due to the nature of its construction, generative AI is highly flexible in providing responses and finishing tasks. It can offer relevant and consistent answers without specific training, enabling possibilities for wide range usage and scalability. Beyond this, there are also multiple specifically trained models like Galactica that can store, combine and reason about scientific knowledge, trained on a large scientific corpus of papers, reference material, knowledge bases and many other sources (Taylor et al., 2022  ###reference_b60###). However, despite its abilities to address various tasks and problems in different fields, from education to medical and management (Zhang et al., 2023  ###reference_b77###), generative AI like ChatGPT can have limitations and failures including reasoning, factual errors, math, coding, and bias (Borji, 2023  ###reference_b7###). Another aspect that can be both a strength and a limitation is that generative AI requires substantial computational resources, especially for large-scale models, making it difficult to implement under certain conditions. On the other hand, for traditional NLP, it is easier to tackle specific tasks, but may struggle with understanding nuanced context and generating contextually rich responses. When focusing on specific patterns or features, traditional NLP can provide more interpretable, streamlined outputs with clearer insights into how decisions are made, making it easier to understand model behavior. Additionally, traditional NLP usually does not require intensive computational resources, making it more adaptable for condense works.\nAdding on to that, there are still multiple challenges in the field that, once properly addressed, can significantly benefit this area of work. First of all, the further development of AI/ML in the field of education is still in urgent need of high-quality datasets. These datasets are large-scale, annotated datasets that are labeled based on domain knowledge of high quality teaching and learning. By feeding such data to the current AI/ML applications, NLP, and LLMs, researchers and technologists can fine-tune the existing models to make them more suitable for education-specific issues. Furthermore, if linked to school contexts, student backgrounds, and learning outcomes, AI/ML can be even more advanced toward consistently and accurately assisting the instructional performance as indicated by Elmore (2008  ###reference_b21###)\u2019s instructional core framework. As demonstrated in case studies, in many research projects, human experts and the process of human coding are still vital protocols that need to be completed to ensure quality responses that align with the intent of the studies. Thus, reinforcement learning and human feedback are essential to inform the process. This can also integrate teachers and educators as researchers who can actively participate in the research process, providing rigorous interactions between research and practice. Another important aspect is ethical artificial intelligence, aiming to protect privacy, increase data representativeness, address algorithm bias, establish evaluation criterion, and regulate data use. Internally, current language modeling primarily emphasizes effectiveness on standard benchmarks and efficiency, with less emphasis on reliability and practical effectiveness, and with deficiencies in constant temporal updates (Zhuo et al., 2023  ###reference_b79###). Externally, without responsibly constructed training data and properly protected privacy data, LLMs can easily leak private information in generation (Carlini et al., 2021  ###reference_b10###). During deployment, it\u2019s observed that Language Models (LLMs) may be manipulated to produce malicious content or decisions by unethical users (Weidinger et al., 2021  ###reference_b73###), indicating that even internally ethical language models can be used unethically by producers and users.\nHuman-centered learning within the sphere of AI in education emphasizes the utilization of AI to amplify human creativity. This approach acknowledges the essential value of human experience in both the educational processes and AI-tool development, positioning AI as a catalyst for fostering creativity and developing skills. Diverting from the previous trends that focus on technology improvement as the driver of changes in the field of education, this trend emphasizes technology\u2019s role in enabling human potential through encouraging critical thinking, nuanced problem-solving, and collaboration as well as fostering imagination, curiosity, and innovative thinking.\nTo realize the vision of AI in education, promoting AI literacy alongside AI integration is essential (Wang and Lester, 2023  ###reference_b67###; Velander et al., 2023  ###reference_b63###). Several key areas require attention in future research. Future studies should investigate how AI can transcend the learning experience in traditional classroom settings during school hours. This involves examining AI\u2019s role in facilitating human cognition and behavioral development through adaptive algorithms and flexible accessibility. The appropriate integration of AI-based tools into current educational frameworks is required to maximize AI\u2019s capabilities in enhancing the learning process. Determining the optimal way to incorporate AI-based methodologies into existing curricula is an area that needs exploration, particularly in facilitating human critical thinking, problem-solving skills, and emotional intelligence through human-AI collaboration.\nIn addition to integration, developing AI literacy among students and educators is vital. AI literacy is imperative to prepare students for the demands of the future labor market. There is a need for well-designed AI curricula that equip students with the necessary skills to enter the AI-augmented labor force. These curricula should not only explain how AI functions but also address the ethical considerations of its implications and applications. Moreover, educators play a pivotal role in human-centered learning, both in delivering curricula and safeguarding students\u2019 learning experiences. As such, teachers and administrators need to be prepared for teaching in the digital age. Research on the design of professional development programs is indispensable. These programs should focus on integrating AI tools into improving teaching practices, innovating pedagogical strategies, and understanding the ethical use of AI in various educational settings.\nFor future research in human-centered learning, understanding the human experience in learning about AI, as well as learning with AI, is crucial for cultivating human potential and creativity. This understanding is key to fostering a more dynamic and innovative educational landscape.\nLarge language models sometimes suffer the issue of emergent abilities, a phenomenon that unpredictably emerges. Researchers consider an ability to be emergent if it is not present in smaller models but is present in larger models (Wei et al., 2022  ###reference_b72###). The existence of emergent abilities may hinder the scaling of language models which in turn restrict the development of the field. Additionally, we are running out of the resources to train AI/ML models. It is also suggested that high-quality language data is likely exhausted before 2026, and low-quality language and image data could be exhausted by 2060 (Villalobos et al., 2022b  ###reference_b65###). Without sufficient training data, all applications related to artificial intelligence and natural language processing will be facing a severe bottleneck that restrains further advancement. Additionally, with the accelerating and significant increase of parameter magnitudes in ML models, the limitation of hardware capabilities would pose the risk of discontinuities in processing the growing model sizes (Villalobos et al., 2022a  ###reference_b64###). To address the issue, researchers have proposed ways to reach efficient computation over LLMs, aiming to identify the optimal model size as well as number of tokens. Current models may suffer underperformance, not utilizing the computing budget to the fullest. By evaluating the balance between model sizes and training resources, LLMs can benefit from effective computational allocation (Hoffmann et al., 2022  ###reference_b28###).\nIn envisioning the future development of AI/ML methods, especially LLMs, it is crucial to establish a concrete foundation for technological capacities. This should be considered from both the hardware and software perspectives. Lying the groundwork for future development would provide us with more potential as well as more freedom to explore the application and integration of AI/ML in education."
        }
    ],
    "appendix": [],
    "tables": {},
    "image_paths": {
        "1": {
            "figure_path": "2403.03920v1_figure_1.png",
            "caption": "Figure 1. Instructional Core Framework by Elmore (2008), with examples of educational artifacts."
        }
    },
    "references": [
        {
            "1": {
                "title": "Learning Is Moving in New Ways: The Ecological Dynamics of Mathematics Education.",
                "author": "Dor Abrahamson and Ra\u00fal S\u00e1nchez-Garc\u00eda. 2016.",
                "venue": "Journal of the Learning Sciences 25, 2 (April 2016), 203\u2013239.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Harnessing the Power of AI to Create Intelligent Tutoring Systems for Enhanced Classroom Experience and Improved Learning Outcomes. In Intelligent Communication Technologies and Virtual Mobile Networks (Lecture Notes on Data Engineering and Communications Technologies), G. Rajakumar, Ke-Lin Du, and \u00c1lvaro Rocha (Eds.). Springer Nature, Singapore, 571\u2013591.",
                "author": "Ashraf Alam. 2023.",
                "venue": "https://doi.org/10.1007/978-981-99-1767-9_42",
                "url": null
            }
        },
        {
            "3": {
                "title": "Culture, dialogue and learning: Notes on an emerging pedagogy.",
                "author": "Robin Alexander. 2008.",
                "venue": "Exploring talk in school 2008 (2008), 91\u2013114.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Technology enhanced feedback tools as a knowledge management mechanism for supporting professional growth and school reform.",
                "author": "A. Bain and G. Swan. 2011.",
                "venue": "Educational Technology Research and Development 59 (2011), 673\u2013685.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Educational Data Mining and Learning Analytics: Applications to Constructionist Research.",
                "author": "Matthew Berland, Ryan Baker, and Paulo Blikstein. 2014.",
                "venue": "Technology, Knowledge and Learning 19 (July 2014).",
                "url": null
            }
        },
        {
            "6": {
                "title": "A Categorical Archive of ChatGPT Failures.",
                "author": "Ali Borji. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "7": {
                "title": "Leveraging ChatGPT to Democratize and Decolonize Global Surgery: Large Language Models for Small Healthcare Budgets.",
                "author": "Fabio Botelho, Jean Marie Tshimula, and Dan Poenaru. 2023.",
                "venue": "World Journal of Surgery 47, 11 (Nov. 2023), 2626\u20132627.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Artificial Intelligence and the Future of Teaching and Learning: Insights and Recommendations.",
                "author": "Miguel A. Cardona, Roberto J. Rodr\u00edguez, and Kristina Ishmael. 2023.",
                "venue": "(2023).",
                "url": null
            }
        },
        {
            "9": {
                "title": "Extracting Training Data from Large Language Models. 2633\u20132650.",
                "author": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.",
                "venue": "https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting",
                "url": null
            }
        },
        {
            "10": {
                "title": "Predicting Student Performance Using Data from an Auto-Grading System.",
                "author": "Huanyi Chen. 2018.",
                "venue": "Master\u2019s thesis. University of Waterloo.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Natural Language Processing.",
                "author": "Gobinda G. Chowdhury. 2003.",
                "venue": "Annual Review of Information Science and Technology (ARIST) 37 (2003), 51\u201389.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Instructional rounds in education. Vol. 30.",
                "author": "Elizabeth A. City, Richard F. Elmore, Sarah E. Fiarman, and Lee Teitel. 2009.",
                "venue": "Cambridge, MA: Harvard Education Press.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Improving Automated Evaluation of Student Text Responses Using GPT-3.5 for Text Data Augmentation. In Artificial Intelligence in Education (Lecture Notes in Computer Science), Ning Wang, Genaro Rebolledo-Mendez, Noboru Matsuda, Olga C. Santos, and Vania Dimitrova (Eds.). Springer Nature Switzerland, Cham, 217\u2013228.",
                "author": "Keith Cochran, Clayton Cohn, Jean Francois Rouet, and Peter Hastings. 2023.",
                "venue": "https://doi.org/10.1007/978-3-031-36272-9_18",
                "url": null
            }
        },
        {
            "14": {
                "title": "Chapter 37 - Intelligent Tutoring Systems.",
                "author": "Albert T. Corbett, Kenneth R. Koedinger, and John R. Anderson. 1997.",
                "venue": "In Handbook of Human-Computer Interaction (Second Edition), Marting G. Helander, Thomas K. Landauer, and Prasad V. Prabhu (Eds.). North-Holland, Amsterdam, 849\u2013874.",
                "url": null
            }
        },
        {
            "15": {
                "title": "EVALUATION INSTRUMENT.",
                "author": "Charlotte Danielson. 2013.",
                "venue": "(2013).",
                "url": null
            }
        },
        {
            "16": {
                "title": "The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts.",
                "author": "Dorottya Demszky and Heather Hill. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "17": {
                "title": "Can Automated Feedback Improve Teachers\u2019 Uptake of Student Ideas? Evidence From a Randomized Controlled Trial in a Large-Scale Online Course.",
                "author": "Dorottya Demszky, Jing Liu, Heather C. Hill, Dan Jurafsky, and Chris Piech. 2023.",
                "venue": "Educational Evaluation and Policy Analysis (May 2023), 01623737231169270.",
                "url": null
            }
        },
        {
            "18": {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "19": {
                "title": "Do Components of Explicit Instruction Explain the Differential Effectiveness of a Core Mathematics Program for Kindergarten Students With Mathematics Difficulties? A Mediated Moderation Analysis.",
                "author": "Christian T. Doabler, Mike Stoolmiller, Patrick C. Kennedy, Nancy J. Nelson, Ben Clarke, Brian Gearin, Hank Fien, Keith Smolkowski, and Scott K. Baker. 2019.",
                "venue": "Assessment for Effective Intervention 44, 3 (June 2019), 197\u2013211.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Improving the instructional core.",
                "author": "Richard Elmore. 2008.",
                "venue": "Draft manuscript (2008).",
                "url": null
            }
        },
        {
            "21": {
                "title": "Leading the instructional core.",
                "author": "Richard Elmore. 2010.",
                "venue": "Conversation, 11 (3) (2010), 1\u201312.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Enhancing Classroom-based Talk: Blending practice, research and theory.",
                "author": "Robyn M. Gillies. 2015.",
                "venue": "Routledge.",
                "url": null
            }
        },
        {
            "23": {
                "title": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models.",
                "author": "Roberto Gozalo-Brizuela and Eduardo C. Garrido-Merchan. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "24": {
                "title": "Opening-up Classroom Discourse to Promote and Enhance Active, Collaborative and Cognitively-Engaging Student Learning Experiences.",
                "author": "J. Hardman. 2016.",
                "venue": "(2016).",
                "url": null
            }
        },
        {
            "25": {
                "title": "Impact of project-based curriculum materials on student learning in science: Results of a randomized controlled trial.",
                "author": "Christopher J. Harris, William R. Penuel, Cynthia M. D\u2019Angelo, Angela Haydel DeBarger, Lawrence P. Gallagher, Cathleen A. Kennedy, Britte Haugen Cheng, and Joseph S. Krajcik. 2015.",
                "venue": "Journal of Research in Science Teaching 52, 10 (2015), 1362\u20131385.",
                "url": null
            }
        },
        {
            "26": {
                "title": "An analysis of the forms of teacher-student dialogue that are most productive for learning.",
                "author": "Sara Hennessy, Elisa Calcagni, Alvin Leung, and Neil Mercer. 2023.",
                "venue": "Language and Education 37, 2 (March 2023), 186\u2013211.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Training Compute-Optimal Large Language Models.",
                "author": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "28": {
                "title": "Promoting rich discussions in mathematics classrooms: Using personalized, automated feedback to support reflection and instructional change.",
                "author": "Jennifer Jacobs, Karla Scornavacco, Charis Harty, Abhijit Suresh, Vivian Lai, and Tamara Sumner. 2022.",
                "venue": "Teaching and Teacher Education 112 (2022), 103631.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Toward Automated Feedback on Teacher Discourse to Enhance Teacher Learning. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201313.",
                "author": "Emily Jensen, Meghan Dale, Patrick J. Donnelly, Cathlyn Stone, Sean Kelly, Amanda Godley, and Sidney K. D\u2019Mello. 2020.",
                "venue": "https://doi.org/10.1145/3313831.3376418",
                "url": null
            }
        },
        {
            "30": {
                "title": "Large language models in education: A focus on the complementary relationship between human teachers and ChatGPT.",
                "author": "Jaeho Jeon and Seongyong Lee. 2023.",
                "venue": "Education and Information Technologies 28, 12 (Dec. 2023), 15873\u201315892.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Inspiring dialogue: Talking to learn in the English classroom.",
                "author": "Mary M. Juzwik, Carlin Borsheim-Black, Samantha Caughlan, and Anne Heintz. 2015.",
                "venue": "Teachers College Press.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Automatic assessment of the content of essays based on course materials. In ITRE 2004. 2nd International Conference Information Technology: Research and Education. IEEE, London, England, UK, 126\u2013130.",
                "author": "T. Kakkonen and E. Sutinen. 2004.",
                "venue": "https://doi.org/10.1109/ITRE.2004.1393660",
                "url": null
            }
        },
        {
            "33": {
                "title": "Using global observation protocols to inform research on teaching effectiveness and school improvement: Strengths and emerging limitations.",
                "author": "Sean Kelly, Robert Bringe, Esteban Aucejo, and Jane Cooley Fruehwirth. 2020.",
                "venue": "Education Policy Analysis Archives 28 (April 2020), 62\u201362.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Automatically Measuring Question Authenticity in Real-World Classrooms.",
                "author": "Sean Kelly, Andrew M. Olney, Patrick Donnelly, Martin Nystrand, and Sidney K. D\u2019Mello. 2018.",
                "venue": "Educational Researcher 47, 7 (Oct. 2018), 451\u2013464.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Fine-tuning ChatGPT for Automatic Scoring.",
                "author": "Ehsan Latif and Xiaoming Zhai. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "36": {
                "title": "Measuring Teaching Practices at Scale: A Novel Application of Text-as-Data Methods.",
                "author": "Jing Liu and Julie Cohen. 2021.",
                "venue": "Educational Evaluation and Policy Analysis 43, 4 (Dec. 2021), 587\u2013614.",
                "url": null
            }
        },
        {
            "37": {
                "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "38": {
                "title": "Content Analysis of Textbooks via Natural Language Processing: Findings on Gender, Race, and Ethnicity in Texas U.S. History Textbooks.",
                "author": "Li Lucy, Dorottya Demszky, Patricia Bromley, and Dan Jurafsky. 2020.",
                "venue": "AERA Open 6, 3 (July 2020), 233285842094031.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Personalized Education in the Artificial Intelligence Era: What to Expect Next.",
                "author": "Setareh Maghsudi, Andrew Lan, Jie Xu, and Mihaela van der Schaar. 2021.",
                "venue": "IEEE Signal Processing Magazine 38, 3 (May 2021), 37\u201350.",
                "url": null
            }
        },
        {
            "40": {
                "title": "The Origin and Development of Lesson Study in Japan.",
                "author": "Naomichi Makinae. 2019.",
                "venue": "In Theory and Practice of Lesson Study in Mathematics: An International Perspective, Rongjin Huang, Akihiko Takahashi, and Jo\u00e3o Pedro da Ponte (Eds.). Springer International Publishing, Cham, 169\u2013181.",
                "url": null
            }
        },
        {
            "41": {
                "title": "The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Kalina Bontcheva and Jingbo Zhu (Eds.). Association for Computational Linguistics, Baltimore, Maryland, 55\u201360.",
                "author": "Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014.",
                "venue": "https://doi.org/10.3115/v1/P14-5010",
                "url": null
            }
        },
        {
            "42": {
                "title": "Full Personalized Learning Path Recommendation: A Literature Review. In International Conference on Advanced Intelligent Systems and Informatics. Springer, 185\u2013195.",
                "author": "Nesrine Mansouri, Makram Soui, and Mourad Abed. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "43": {
                "title": "Classroom observation for evaluating and improving teaching: An international perspective.",
                "author": "Felipe Martinez, Sandy Taut, and Kevin Schaaf. 2016.",
                "venue": "Studies in Educational Evaluation 49 (2016), 15\u201329.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Deliberative Discourse Idealized and Realized: Accountable Talk in the Classroom and in Civic Life.",
                "author": "Sarah Michaels, Catherine O\u2019Connor, and Lauren B. Resnick. 2008.",
                "venue": "Studies in Philosophy and Education 27, 4 (July 2008), 283\u2013297.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Intelligent tutoring systems: a systematic review of characteristics, applications, and evaluation methods.",
                "author": "Elham Mousavinasab, Nahid Zarifsanaiey, Sharareh R. Niakan Kalhori, Mahnaz Rakhshan, Leila Keikha, and Marjan Ghazi Saeedi. 2021.",
                "venue": "Interactive Learning Environments 29, 1 (Jan. 2021), 142\u2013163.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Authentic Intellectual Work and Standardized Tests: Conflict or Coexistence? Improving Chicago\u2019s Schools.",
                "author": "Fred M. Newmann, Anthony S. Bryk, and Jenny K. Nagaoka. 2001.",
                "venue": "(2001).",
                "url": null
            }
        },
        {
            "47": {
                "title": "Intelligent tutoring systems: an overview.",
                "author": "Hyacinth S. Nwana. 1990.",
                "venue": "Artificial Intelligence Review 4, 4 (Dec. 1990), 251\u2013277.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Personalized adaptive learning: an emerging pedagogical approach enabled by a smart learning environment.",
                "author": "Hongchao Peng, Shanshan Ma, and Jonathan Michael Spector. 2019.",
                "venue": "Smart Learning Environments 6, 1 (Sept. 2019), 9.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Where Have All the Textbooks Gone?: Toward Sustainable Provision of Teaching and Learning Materials in Sub-Saharan Africa.",
                "author": "Tony Read. 2015.",
                "venue": "World Bank Publications.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Open educational resources: education for the world?",
                "author": "Thomas Richter and Maggie McPherson. 2012.",
                "venue": "Distance Education 33, 2 (Aug. 2012), 201\u2013219.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Glossary of Artificial Intelligence Terms for EducatorsEducator CIRCLS Blog.",
                "author": "Pati Ruiz and Judi Fusco. 2023.",
                "venue": "Retrieved from Glossary of Artificial Intelligence Terms for Educators\u2013CIRCLS (2023).",
                "url": null
            }
        },
        {
            "52": {
                "title": "Empowering Education with Generative Artificial Intelligence Tools: Approach with an Instructional Design Matrix.",
                "author": "Lena Ivannova Ruiz-Rojas, Patricia Acosta-Vargas, Javier De-Moreta-Llovet, and Mario Gonzalez-Rodriguez. 2023.",
                "venue": "Sustainability 15, 15 (Jan. 2023), 11524.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Key issues of lesson study in Japan and the United States: a literature review.",
                "author": "Eisuke Saito. 2012.",
                "venue": "Professional Development in Education 38, 5 (Nov. 2012), 777\u2013789.",
                "url": null
            }
        },
        {
            "54": {
                "title": "The Dynamics of Group Learning in a Cohort: From Nonlearning to Transformative Learning.",
                "author": "Jay Paredes Scribner and Joe F. Donaldson. 2001.",
                "venue": "Educational Administration Quarterly 37, 5 (Dec. 2001), 605\u2013636.",
                "url": null
            }
        },
        {
            "55": {
                "title": "A review of the trends and challenges in adopting natural language processing methods for education feedback analysis.",
                "author": "Thanveer Shaik, Xiaohui Tao, Yan Li, Christopher Dann, Jacquie McDonald, Petrea Redmond, and Linda Galligan. 2022.",
                "venue": "IEEE Access 10 (2022), 56720\u201356739.",
                "url": null
            }
        },
        {
            "56": {
                "title": "What the Discourse Tells Us: Talk and Indicators of High-Level Comprehension.",
                "author": "A. Soter, I. Wilkinson, P. K. Murphy, L. Rudge, Kristin Reninger, and Margaret E. Edwards. 2008.",
                "venue": "International Journal of Educational Research 47 (2008), 372\u2013391.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Two Practical Rhetorical Structure Theory Parsers. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, Matt Gerber, Catherine Havasi, and Finley Lacatusu (Eds.). Association for Computational Linguistics, Denver, Colorado, 1\u20135.",
                "author": "Mihai Surdeanu, Tom Hicks, and Marco Antonio Valenzuela-Esc\u00e1rcega. 2015.",
                "venue": "https://doi.org/10.3115/v1/N15-3001",
                "url": null
            }
        },
        {
            "58": {
                "title": "The TalkMoves Dataset: K-12 Mathematics Lesson Transcripts Annotated for Teacher and Student Discursive Moves.",
                "author": "Abhijit Suresh, Jennifer Jacobs, Charis Harty, Margaret Perkoff, James H. Martin, and Tamara Sumner. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "59": {
                "title": "Galactica: A Large Language Model for Science.",
                "author": "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "60": {
                "title": "Citizenship and Education in Twenty-Eight Countries: Civic Knowledge and Engagement at Age Fourteen.",
                "author": "Judith Torney-Purta, Rainer Lehmann, Hans Oswald, and Wolfram Schulz. 2001.",
                "venue": "Technical Report. IEA Secretariat, Herengracht 487, 1017 BT, Amsterdam, The Netherlands.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Neuro\u2010linguistic programming as an innovation in education and teaching.",
                "author": "Paul Tosey and Jane Mathison. 2010.",
                "venue": "Innovations in Education and Teaching International 47, 3 (Aug. 2010), 317\u2013326.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Artificial Intelligence in K-12 Education: eliciting and reflecting on Swedish teachers\u2019 understanding of AI and its implications for teaching & learning.",
                "author": "Johanna Velander, Mohammed Ahmed Taiye, Nuno Otero, and Marcelo Milrad. 2023.",
                "venue": "Education and Information Technologies (2023), 1\u201321.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Machine Learning Model Sizes and the Parameter Gap.",
                "author": "Pablo Villalobos, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, Anson Ho, and Marius Hobbhahn. 2022a.",
                "venue": "",
                "url": null
            }
        },
        {
            "64": {
                "title": "Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning.",
                "author": "Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022b.",
                "venue": "",
                "url": null
            }
        },
        {
            "65": {
                "title": "Using adaptive learning technologies to personalize instruction to student interests: The impact of relevant contexts on performance and learning outcomes.",
                "author": "Candace A. Walkington. 2013.",
                "venue": "Journal of Educational Psychology 105, 4 (2013), 932\u2013945.",
                "url": null
            }
        },
        {
            "66": {
                "title": "K-12 Education in the Age of AI: A Call to Action for K-12 AI Literacy.",
                "author": "Ning Wang and James Lester. 2023.",
                "venue": "International journal of artificial intelligence in education 33, 2 (2023), 228\u2013232.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction.",
                "author": "Rose E. Wang and Dorottya Demszky. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "68": {
                "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization.",
                "author": "Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "69": {
                "title": "Intelligent Auto-grading System. In 2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS). 430\u2013435.",
                "author": "Zining Wang, Jianli Liu, and Ruihai Dong. 2018.",
                "venue": "https://doi.org/10.1109/CCIS.2018.8691244",
                "url": null
            }
        },
        {
            "70": {
                "title": "Automatic classification of activities in classroom discourse.",
                "author": "Zuowei Wang, Xingyu Pan, Kevin F. Miller, and Kai S. Cortina. 2014.",
                "venue": "Computers & Education 78 (Sept. 2014), 115\u2013123.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Emergent Abilities of Large Language Models.",
                "author": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "72": {
                "title": "Ethical and social risks of harm from Language Models.",
                "author": "Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "73": {
                "title": "Classification of open-ended responses to a research-based assessment using natural language processing.",
                "author": "Joseph Wilson, Benjamin Pollard, John M. Aiken, Marcos D. Caballero, and H. J. Lewandowski. 2022.",
                "venue": "Physical Review Physics Education Research 18, 1 (June 2022), 010141.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Can Short Answers to Open Response Questions Be Auto-Graded Without a Grading Rubric?",
                "author": "Xi Yang, Lishan Zhang, and Shengquan Yu. 2017.",
                "venue": "In Artificial Intelligence in Education, Elisabeth Andr\u00e9, Ryan Baker, Xiangen Hu, Ma. Mercedes T. Rodrigo, and Benedict Du Boulay (Eds.). Vol. 10331. Springer International Publishing, Cham, 594\u2013597.",
                "url": null
            }
        },
        {
            "75": {
                "title": "GPT Can Solve Mathematical Problems Without a Calculator.",
                "author": "Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "76": {
                "title": "One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era.",
                "author": "Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, Gyeong-Moon Park, Sung-Ho Bae, Lik-Hang Lee, Pan Hui, In So Kweon, and Choong Seon Hong. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "77": {
                "title": "A Memory-Augmented Neural Model for Automated Grading. 189\u2013192.",
                "author": "Siyuan Zhao, Yaqiong Zhang, Xiaolu Xiong, Anthony Botelho, and Neil Heffernan. 2017.",
                "venue": "https://doi.org/10.1145/3051457.3053982",
                "url": null
            }
        },
        {
            "78": {
                "title": "Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity.",
                "author": "Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023.",
                "venue": "arXiv preprint arXiv:2301.12867 (2023), 12\u20132.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.03920v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.1.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "5.1",
            "5.1.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "4.1",
            "4.2",
            "5.1.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.03920v1",
        "paper_title": "Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts",
        "research_background": "### Motivation\n\nThe paper is motivated by the need to improve instructional practices and student learning outcomes using data-driven approaches. Post-pandemic learning demands and advancements in AI necessitate personalized, effective, safe, and scalable educational strategies (Cardona et al., 2023). The complexity and volume of educational data have grown significantly, posing challenges to conventional quantitative methods in processing such data effectively. AI and machine learning (ML) approaches offer scalability, precision, and the ability to provide actionable insights, thus presenting a significant opportunity to enhance educational research and practices.\n\n### Research Problem\n\nThe main research problem addressed in the paper is the need to identify and leverage AI/ML methods for analyzing textual data in education. Specifically, the paper aims to explore innovative solutions for processing and deriving insights from various educational textual artifacts, including curricula, lesson plans, classroom transcriptions, and student assignments. The goal is to understand how AI/ML can be applied to these data types to improve instructional quality while acknowledging and addressing the complexities and limitations of these methodologies.\n\n### Relevant Prior Work\n\n1. **Advancements in Educational Data and Instructional Practices:**\n   - The paper references how instructional practices have generated richer and more complex data over the years (Cardona et al., 2023).\n   \n2. **Limitations of Conventional Quantitative Methods:**\n   - Conventional quantitative methods are noted to be limited by calculation capacity and model efficiency, consequently hindering improvements in teaching and learning outcomes (Berland et al., 2014).\n\n3. **Role of AI and ML:**\n   - AI and ML have been recognized for their scalability and precision in processing complex data (Berland et al., 2014).\n   - The paper highlights the rapid adoption of generative AI tools like ChatGPT and Bard, which enhance both text analysis and generation capabilities.\n\n4. **Definitions and Applications of AI/ML and NLP:**\n   - AI entails automation and generative capabilities, enabling complex tasks through hardware, algorithms, and data (Ruiz and Fusco, 2023).\n   - ML involves developing algorithms to identify patterns in structured and unstructured data, including text (Chowdhury, 2003).\n   - NLP specifically focuses on the interpretation of linguistic data, aiding in the discovery of patterns within textual data (Chowdhury, 2003).\n\n5. **Instructional Core Framework:**\n   - The Instructional Core Framework, which defines the interactions among students, educators, and educational content, serves as a foundation for exploring AI/ML applications in text analysis (City et al., 2009; Gillies, 2015; Hennessy et al., 2023).\n\nThe paper integrates these foundational concepts and prior research to assess the potential and limitations of AI/ML-powered textual analysis for instructional improvement, ultimately aiming to guide future research in this promising field.",
        "methodology": "### Methodology Section\n\n**Proposed Method or Model:**\nThe methodology seeks to enhance instructional quality through a hybrid approach blending computer-assisted textual analysis with AI-driven insights from educational artifacts. The key components and innovative facets are outlined below:\n\n1. **AI and Machine Learning in Professional Development:**\n   - The approach leverages recent advancements in AI and machine learning to transform professional development for educators.\n   - Emphasis is placed on overcoming the limitations of traditional feedback mechanisms, which are described as sporadic and evaluative.\n\n2. **Natural Language Processing (NLP):**\n   - NLP is employed to analyze textual data such as classroom transcripts.\n   - The analysis focuses on identifying linguistic nuances, communication styles, and the frequency and quality of teacher-student engagement.\n\n3. **Objective Measurement of Teaching Practices:**\n   - NLP provides a structured and objective way of measuring teaching practices.\n   - Specific metrics include linguistic nuances and communication styles that can offer deep insights into teacher discourse.\n\n4. **Generating Insights for Educators:**\n   - The model aims to provide educators with actionable insights specific to their instructional methods.\n   - It identifies strengths and areas for improvement to foster a more inclusive classroom environment.\n\n**Innovative Components:**\n- The integration of AI and machine learning into traditional professional development frameworks.\n- Application of NLP to detailed classroom interactions, moving beyond basic quantifiable metrics.\n- The focus on nuanced linguistic and engagement aspects of teacher-student interactions, which are often overlooked in conventional reviews.\n\n**Contextual Studies:**\n- The methodology builds on recent exemplary studies demonstrating the efficacy of AI-driven tools in refining teacher discourse.\n- The incorporation of detailed textual analysis offers a powerful supplementary tool to enhance instructional quality.\n\n**Summarized Insights:**\n- The ongoing research highlights the effectiveness of NLP and AI in generating comprehensive, actionable feedback from educational artifacts.\n- The ultimate goal is to provide a more robust framework for educators to continually improve and adapt their teaching strategies.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n**Datasets:** \nThe main datasets consist of classroom transcripts, which include a diverse range of educational artifacts. These datasets likely encompass various teaching sessions, covering a breadth of subjects and teaching styles, aiming to reflect a comprehensive view of instructional practices.\n\n**Baselines:**\nTo evaluate the effectiveness of the computer-assisted textual analysis, traditional feedback mechanisms serve as the baseline. These mechanisms include sporadic evaluative feedback typically given to educators based on direct observations or manual analysis of their teaching practices.\n\n**Evaluation Metrics:**\nThe evaluation focuses on identifying the following key aspects in the classroom transcripts:\n1. Linguistic nuances.\n2. Communication styles.\n3. Frequency and quality of teacher-student engagement.\n\nThese metrics are used to measure the depth and quality of insights generated by the AI-driven tools compared to the traditional methods.\n\n**Main Experimental Results:**\nThe AI-driven tools demonstrated significant efficacy in refining teacher discourse. By leveraging NLP to analyze the classroom transcripts, the results showcased a more detailed and nuanced understanding of teaching practices. Specifically, the AI approach provided more valuable insights into the teachers' strengths and areas requiring improvement, which traditional feedback mechanisms failed to capture comprehensively. These insights included subtle linguistic features and the dynamics of teacher-student interactions that led to fostering a more inclusive classroom environment.\n\nOverall, the main experiment underscores the potential of advanced computational tools in transforming professional development for educators by providing granular and actionable feedback."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the use of Natural Language Processing (NLP) and Machine Learning (ML) techniques for detecting and enhancing the authenticity of teacher questions in classroom discourse.",
            "experiment_process": "The study combined diverse datasets: an archival database with text transcripts from 451 observations across 112 classrooms (Partnership data) and 132 high-quality audio recordings from 27 classrooms (Class 5.0 data). The Partnership data utilized a semi-automated approach involving human coders for segmenting raw audio, identifying questions, distinguishing instructional questions, and providing transcriptions. Syntactic and discourse parsing methods were employed to extract 244 linguistic features. The Class 5.0 data followed a fully automated approach aiming to calculate authenticity from classroom audio recordings without human involvement. Correlations between computer- and human-coded ratings were measured to gauge the effectiveness of the NLP models.",
            "result_discussion": "The study found a high correlation between computer- and human-coded authenticity ratings, indicating a promising potential for NLP approaches in automating the measurement of question authenticity in classrooms. Challenges noted include real-world implementation issues such as noise, dialect diversity, and data imbalance. The study highlighted the NLP techniques' potential to streamline feedback processes and enhance classroom instructional practices by providing valuable, scalable feedback based on classroom interactions.",
            "ablation_id": "2403.03920v1.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of generative AI, specifically ChatGPT, in providing automated teacher coaching and generating actionable instructional feedback that aligns with human evaluations.",
            "experiment_process": "The researchers conducted a case study using ChatGPT for three tasks: scoring transcript segments based on CLASS and MQI rubrics, identifying highlights and missed opportunities in instructional practices, and providing open-ended suggestions for improving student mathematical reasoning. The data was extracted from the National Center for Teacher Effectiveness Transcript dataset, consisting of 4th and 5th grade elementary mathematics observations. Human evaluators assessed the validity of ChatGPT's performance in these tasks by comparing its outputs to human ratings using zero-shot prompting.",
            "result_discussion": "The study found that ChatGPT provided relevant responses but lacked novelty and insight in its feedback, attributed to data scarcity. The performance of generative AI showed potential but emphasized the need for extensive training data and fine-tuning to achieve high-quality, automated teacher coaching. The study underscored that zero-shot performances offer insights into adaptation but fall short in replicating human-like novelty and depth in feedback, indicating areas for future development.",
            "ablation_id": "2403.03920v1.No2"
        },
        {
            "research_objective": "To develop and evaluate an automated grading model for essay writing and open-ended assignments within the context of MOOCs, addressing the need for scalable grading solutions.",
            "experiment_process": "The study utilized the Kaggle Automated Student Assessment Prize (Kaggle ASAP) dataset for training and evaluating a memory networks-powered automated grading model, consisting of layers for Input Representation, Memory Addressing, Memory Reading, and Output. The model's performance was compared with the Enhanced AI Scoring Engine (EASE) across multiple essay sets. The focus was on developing reliable assignment representations through vectorization and memory storage of pertinent samples.",
            "result_discussion": "The memory-augmented neural model demonstrated state-of-the-art performance, particularly excelling in 7 out of 8 essay sets. Key performance factors included reliable representation and memory component. The study's limitations were noted, including testing solely on the Kaggle ASAP dataset, and suggested further exploration with diverse datasets and improved representation mechanisms. The findings highlight the model's potential applicability across various subjects and assignment formats.",
            "ablation_id": "2403.03920v1.No3"
        },
        {
            "research_objective": "To investigate the application of fine-tuned generative AI (ChatGPT) for automatically scoring student-written responses and compare its performance with other models like BERT.",
            "experiment_process": "The study engaged in a fine-tuning process, training GPT-3.5 on a dataset of student responses and expert scores across six different assessment tasks. A comparative analysis was conducted with BERT, assessing each model's accuracy in multi-label and multi-class scoring tasks. The research also employed text data augmentation and a benchmarking tool (PandaLM) to evaluate model performance comprehensively.",
            "result_discussion": "Fine-tuned GPT-3.5 outperformed BERT in scoring accuracy across all tasks, with an average increase of 9.1% in accuracy for multi-label and multi-class tasks. This indicates the effectiveness of fine-tuning in enhancing language models for educational assessments. However, the study acknowledged limitations, such as the need for extensive fine-tuning and generalizability across different educational contexts, suggesting potential areas for future research.",
            "ablation_id": "2403.03920v1.No4"
        }
    ]
}