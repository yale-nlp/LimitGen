{
    "title": "AQuA - Combining Experts\u2019 and Non-Experts\u2019 Views To Assess Deliberation Quality in Online Discussions Using LLMs",
    "abstract": "Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts\u2019 annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score. We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training. The analysis of experts\u2019 vs. non-experts\u2019 annotations confirms theoretical findings in the social science literature.\n\n\n\nKeywords:\u2009deliberative quality, adapter models, quality score",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "In the evolving landscape of democratic discourse, the concept of deliberation stands as a cornerstone, embodying the exchange of ideas, critical discussion, and consensus-building among citizens (Dryzek, 2002  ###reference_b13###). Central to the efficacy of these deliberations is their quality, a multifaceted construct traditionally gauged by dimensions such as rationality, civility, reciprocity, and constructiveness (Friess and Eilders, 2015  ###reference_b18###). More recent research has explored various indicators of deliberative quality in online discussions (Steenbergen et al., 2003  ###reference_b39###; Friess and Eilders, 2015  ###reference_b18###; Scudder, 2022  ###reference_b37###). However, most of these approaches require manual annotation of discussion data from trained coders and serve to analyze the discussion in retrospect.\nAs the digital age drives an increasing volume of public conversations onto online platforms, the demand to assess their quality through the previously mentioned dimensions in an automated, scalable manner is growing (Diakopoulos, 2015  ###reference_b12###; Beauchamp, 2020  ###reference_b4###).\nPrevious efforts have demonstrated the potential of using natural language processing (NLP) and machine learning algorithms to automatically identify features of deliberation such as argumentative structure, emotional tone, and engagement patterns (Lawrence and Reed, 2020  ###reference_b27###; Acheampong et al., 2020  ###reference_b1###; Shin and Rask, 2021  ###reference_b38###). The interest in automating such assessments, with projects like the one implemented by Falk and Lapesa (2023a  ###reference_b15###) in their examination of argument and deliberative quality with adapter models (Houlsby et al., 2019  ###reference_b25###), is growing.\nMotivated by this research, this study introduces AQuA, an index to measure the deliberative quality of individual comments in online discussions with a single score. While there is an ongoing debate on the usefulness of aggregating multiple indices of deliberation (B\u00e4chtiger et al., 2022  ###reference_b7###), we argue that for some tasks a single value, composed of several theoretically based criteria is favorable. Our approach combines predictions on various dimensions of deliberation with insights gained from both expert and non-expert evaluations, resulting in a single deliberative quality score. We make use of data that has been annotated from both trained experts and crowd annotators, representing the non-experts\u2019 view. We calculate correlation coefficients between the annotated deliberative quality criteria and the perceived deliberativeness of the comments to attribute importance to each individual criterion.\nWe train 20 adapter models on aspects of deliberation to form the basis for a single deliberation score.\nTo combine the automated predictions in a meaningful way, we calculate the correlation coefficients between experts\u2019 and non-experts\u2019 assessments of deliberative quality.\nWe define a single normalized score using the correlations as weights, hereby, creating an interpretable and explainable measure for deliberative quality.\nFinally, we show in experiments that our score can automatically assess the deliberative quality of discussion comments.\nOur method consists of two components: (1) the utilization\nof adapters trained on discrete facets of deliberation,\nand (2) the integration of correlations between\nannotations from experts and non-experts to establish a\nnormalized score for deliberative quality. In developing this index, we extensively test and evaluate its effectiveness across diverse datasets, demonstrating its utility in real-world applications. By doing so, we aim to contribute to the burgeoning field of computational social science, offering scholars, policymakers, and practitioners a tool to monitor and analyze public dialogues. Our trained adapter weights and the code for calculating AQuA scores are available under https://github.com/mabehrendt/AQuA  ###reference_###.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "Before explaining our approach in detail, we give an overview on the previous work to quantify aspects of deliberation in online discussions and the adapter approach to efficiently train language models for downstream tasks."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Adapters",
            "text": "Adapters, as introduced by Rebuffi et al. (2017  ###reference_b36###) are an efficient approach to customize pre-trained language models like RoBERTa (Liu et al., 2019  ###reference_b28###) for specific tasks. This method involves the integration of additional bottleneck layers into the model for each distinct task, which adds new weights while leaving the original pre-trained weights unaltered.\nThe concept of adapter layers was first applied to NLP by Houlsby et al. (2019  ###reference_b25###), who adapted the Transformer architecture (Vaswani et al., 2017  ###reference_b41###) to include these layers. The design of the adapter involves compressing the input\u2019s dimensionality to a significantly smaller size, applying a non-linear function, and incorporating a skip-connection to circumvent the bottleneck, with task-specific layer normalization parameters also being adjustable.\nThe strategic insertion of adapter layers has been a focus of research, with Houlsby et al. (2019  ###reference_b25###) positioning them subsequent to both the multi-head attention and feed-forward layers within the Transformer architecture. Pfeiffer et al. (2021  ###reference_b33###) found in an extensive search on architectural parameters, that placing only one adapter after the feed forward layer in the Transformer works best throughout all their experiments. We also apply this architecture for our models.\nThe introduction of AdapterHub by Pfeiffer et al. (2020  ###reference_b34###) and the adapters library by Poth et al. (2023  ###reference_b35###) further facilitated the sharing and reuse of pre-trained adapters within the community.\nSubsequent studies, such as those by Mendonca et al. (2022  ###reference_b29###), explored the training of individual adapters for dialogue quality estimation, and the use of AdapterFusion (Pfeiffer et al., 2021  ###reference_b33###) to merge features from different adapters. Falk and Lapesa (2023a  ###reference_b15###) trained 20 adapters on features for argument and deliberative quality to examine their dependencies. In our work, we follow a similar path to train adapters to evaluate specific aspects of deliberative quality and subsequently combine them using correlation coefficients between experts\u2019 and non-experts\u2019 annotations, to create a single deliberative quality metric."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   AQuA: An Additive Score for Deliberative Quality",
            "text": "With AQuA we propose a metric for assessing the quality of individual comments in online discussions. Our approach combines predictions on various dimensions of deliberation with insights gained from both experts\u2019 and non-experts\u2019 evaluations, resulting in a single deliberative quality score. Our methodology consists of two components: (1) the utilization of adapters trained on discrete facets of deliberation, and (2) the integration of correlations between experts\u2019 and non-experts\u2019 annotations to establish a normalized score for deliberative quality. We therefore harness annotations of the same data, once labeled by trained experts for a variety of deliberative qualities, such as the degree of justification, and once labeled by non-experts on their personal assessment of the deliberativeness of a comment. We calculate correlation coefficients between each individual deliberative criterion (experts\u2019 labels) and the binary indicator for deliberativeness (non-experts\u2019 labels).\nThe idea of our approach is to aggregate individual scores calculated by adapters in a meaningful way to obtain a single score for each comment, in which some aspects contribute more to the perceived deliberativeness than others. For this reason we call our approach AQuA, an \u201cAdditive deliberative Quality score with Adapters\u201d."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Datasets",
            "text": "Our analysis is based on three datasets:\nThe KODIE dataset, comprising 13,587 comments that were collected and annotated as part of a scientific study that explored the impact of news organizations\u2019 interactive moderation on the deliberative quality of users\u2019 political discussions (Heinbach et al., 2022  ###reference_b24###).\nThe comments were posted on the Facebook pages of four German national and regional news outlets with high outreach and diverse audiences. These news outlets delivered data that included all published and deleted/hidden posts and comments on their Facebook pages for a period of 12 weeks per news outlet.\nThe #meinfernsehen2021 (German for my television) dataset (Gerlach and Eilders, 2022  ###reference_b21###) is the result of a large scale citizen participation on the future of public television in Germany. Overall, 1,714 comments from the participation process have been manually coded as part of a quantitative content analysis to examine the discussion quality.\nThe CrowdAnno project Wilms et al. (2023  ###reference_b42###) collected a non-expert representation of deliberative quality via crowd annotations for a subset of, i.a., both the KODIE and #meinfernsehen datasets.\nThe annotations from two different perspectives are explained in the following."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1.   KODIE & #meinfernsehen - the Experts\u2019 View",
            "text": "The KODIE annotation framework (Heinbach et al., 2022  ###reference_b24###), assigns 23 score-based deliberative and further labels on other aspects to each comment.\nThese annotations were conducted by trained coders with a scientific background, focusing on deliberative criteria such as fact claims, relevance to the discussion topic, and respectful engagement with other users. The deliberative criteria can each be assigned to one of the three main dimensions of deliberation (B\u00e4chtiger et al., 2009  ###reference_b3###; Esau et al., 2021  ###reference_b14###; Graham, 2010  ###reference_b23###; Coe et al., 2014  ###reference_b10###; Papacharissi, 2004  ###reference_b32###):\nRationality, measured by indicators such as reasoning, solution proposals, and provision of additional knowledge.\nReciprocity, measured as mutual references between users within a discussion.\nCivility, measured as the presence of a respectful interaction with others and the absence of insults, pejorative speech, and other markers of disrespect.\nThe following coding scheme was used: all categories were coded on a four-point scale from \u201cclearly not present\u201d to \u201cclearly present\u201d.\nIntercoder reliability was tested on a subset of 130 comments and exceeded the critical threshold of Krippendorff\u2019s  of .67 for all categories (\u00d8 = .83). The #meinfernsehen data is annotated with the same scheme as KODIE. For #meinfernsehen intercoder reliability was tested on 159 comments, exceeding the critical threshold of Krippendorff\u2019s  of .67 for 20 out of 21 categories (\u00d8 = .74).\nWe selected 19 out of the 23 deliberative quality criteria to train adapters, since some annotated aspects, e.g., threat of violence were not found in the data. In addition to the deliberative quality criteria, we included storytelling, which is considered a type II deliberation criterion, according to B\u00e4chtiger et al. (2009  ###reference_b2###), since the description of personal experience when suggesting a solution contributes to the perceived quality of a comment (Falk and Lapesa, 2023b  ###reference_b16###). The 20 deliberative aspects that we use are listed in Table 1  ###reference_###. After filtering out data points with missing annotations and coding errors, we were left with a total of 13,069 comments to train our adapter models. In the following we will write\nfor the -th score () of the -th comment ()."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2.   CrowdAnno - the Non-Experts\u2019 View",
            "text": "In the CrowdAnno project, Wilms et al. (2023  ###reference_b42###) gathered data on non-experts\u2019 perception of uncivil, deliberative, and fact-claiming communication within German online comments through crowd annotation. The dataset includes 13,677 comments from different news media comment sections and online citizen participation projects, annotated by 681 crowdworkers. For AQuA, we used a subset of 1,742 comments that are identical to the KODIE and #meinfernsehen data. Crowd workers were tasked with evaluating, whether a comment is perceived as enriching and value-adding to the discussion or not, i.e., marking if it contains enriching communication, which could serve as a proxy for deliberative quality. The final score is aggregated from evaluations by 9 different crowd annotators via majority vote. To minimize annotator bias, the crowd workers were sampled to reflect various sociodemographic and educational backgrounds. We will write\nfor the binary deliberativeness label of the -th comment."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Training the Adapters",
            "text": "To automatically predict the various deliberation criteria, we use pre-trained language models, such as BERT (Devlin et al., 2019  ###reference_b11###). We follow the adapter approach: adapters are extra weights , that are plugged into pre-trained language models and then learned for a specific task . The adapted language model for the -th deliberation criterion is written as , where  is some text input. Note that while learning these extra weights, we do not alter the pre-trained model weights.\nMore precisely, we used the adapter architecture proposed by Pfeiffer et al. (2021  ###reference_b33###), which is shown in Figure 2  ###reference_###. We trained 20 individual adapters to predict scores  for individual indicators for deliberative quality in user comments for the KODIE dataset. For training we perform a 65% (train), 15% (val), 20% (test) split on our dataset, resulting in 8,495 training data points, 1,960 for validation and 2,614 for testing.\nEach of the 20 adapters for AQuA is trained with a multi-label classification objective, minimizing the cross entropy loss. We train each adapter for 10 epochs and save the model with the best macro F1 score."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Calculating the Weights",
            "text": "Assigning an importance to the individual quality dimensions for the overall quality measurement is not a simple task. Our intuition for weighting the deliberative criteria is to include the perception of people who potentially read and write these comments.\nFor that reason we linked the scientific theory of deliberation to the view of non-scientists by combining the datasets described in detail in Section 3.1  ###reference_###.\nMore precisely, we obtain the weight for each deliberative criterion  by calculating the correlation coefficient,\nbetween the scientific label  (with mean ) for each of the  aspects of deliberation and the perception of crowd workers on the comments deliberativeness  (with mean ) for all  comments. Note that  is a value from the interval between  and ."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Building the AQuA Score",
            "text": "We build an overall quality score  for each comment as the weighted sum of the weights  and the predicted score  for each of the  quality adapters:\nThe highest and lowest possible scores depend on the number  of criteria and on the range of the predictions . Since the labels from KODIE are from the set , the predictions are also from this set. The highest possible score can be reached by setting all positively weighted criteria to their maximum value (i.e, 3) and all negatively weighted criteria to their minimum value (i.e, 0),\nwhere  if  and zero otherwise. Similarly, the smallest possible score is\nTo get a more intuitive range of values, we scale  to an interval between 0 and 5:\nwhich is the definition of our proposed AQuA score. Figure 1  ###reference_### graphically illustrates, how the AQuA score is calculated for a given input comment."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5.   Applying the Score to English Comments",
            "text": "To apply our method to English datasets, we used the wmt19-en-de-model111https://huggingface.co/facebook/wmt19-en-de  ###reference_e### (Ng et al., 2019  ###reference_b30###), to automatically translate all comments in the examined dataset from English to German. Another alternative would be to train adapter models on English data. Since the KODIE dataset consists of German Facebook comments on political issues, discussing German politicians as well, we decided not to translate these comments to train adapter models, but to translate English comments and use the pre-trained German models for evaluation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Analysis and Experiments",
            "text": "After defining the AQuA score in the previous sections, we briefly discuss the choice of our base model and then analyze the weights that we calculated for the individual adapter predictions. Finally, we conduct several experiments to show that our model can successfully predict deliberative quality in user comments.\nAn interesting observation is that the lowest ranked comments in the dataset are much shorter than the high ranked ones. To study whether comment length alone is the most important factor that causes our model to predict a large score, we take a closer look at the distribution of scores depending on the length of the comment. Figure 3  ###reference_### displays the AQuA score (y-axis) in comparison to the comment length (x-axis, word count). While it is true that short comments get the lowest scores, which is probably due to the fact that they do not have much content, the visual analysis reveals also that medium length comments get the highest scores. This rules out that comment length is the most relevant factor for our score."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Choice of the Base Model",
            "text": "The correlation coefficients are one important part that affect the composition of AQuA. The other part are the predictions of each of the 20 trained adapters. The adapter weights can be trained with different base architectures.\nTo determine which base model performs best, we examine the performance of different models, namely German BERT Base cased (Chan et al., 2020  ###reference_b8###) and multilingual BERT (Devlin et al., 2019  ###reference_b11###) in the cased and uncased variants, on the KODIE test split. The training procedure is the same as described in Section 3.2  ###reference_###. The results are shown in Table 2  ###reference_###. As the datasets are highly imbalanced, and some deliberative qualities do not occur often in the training data, we report the weighted averaged F1 score, i.e., a global weighted average F1 score for each class.\nThe trained adapter weights with the multilingual BERT model as base model outperform the German BERT model on 15 out of the 20 tasks. In direct comparison, the cased variant of Multilingual BERT performs slightly better than the uncased one. Based on these results we take the multilingual BERT Base cased model222https://huggingface.co/bert-base-multilingual-cased  ###reference_ual-cased### as our base model for calculating the AQuA score."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Insights from the Correlations",
            "text": "The calculated correlation coefficients serve as weights in AQuA to give more importance to some deliberative aspects than others. Besides their values determining the importance for each criterion, the sign of the correlation coefficient reveals if an aspect is positively or negatively associated with comment quality. In the following, we discuss the coefficients and examine whether findings from previous deliberative research are consistent with our results. The coefficients with large absolute values are marked bold in Table 1  ###reference_###.\nFor an overview of the data distribution, Table 3  ###reference_### lists the absolute frequencies of each label for each deliberative quality criteria in the subset of the KODIE and #meinfernsehen datasets that have been annotated using the CrowdAnno framework. These points were used to calculate the correlation coefficients. Note that these are not the frequencies in the dataset used for training the adapters. However, the small subset reflects the class imbalance that is present in the data, indicating that some categories such as vulgar language, insults and even storytelling do not occur often.\nIt is striking that nearly all indicators for rationality are strongly positively correlated with non-experts\u2019 perceived deliberative quality of comments. Using well-reasoned arguments that are relevant to the topic has been found to be an important aspect in distinguishing between comments of high and low deliberative quality (Diakopoulos, 2015  ###reference_b12###; Kolhatkar et al., 2020  ###reference_b26###). Unfounded expressions of opinion, on the other hand, are perceived as non-constructive, i.e., negative, in user comments. Our results support that finding, as opinion is highly negatively correlated with the perceived deliberative quality.\nOf all the indicators of reciprocity, referring to personal characteristics of others has the greatest positive impact on the overall score. This is surprising as deliberative literature primarily highlights engaging with others\u2019 positions, not their personal traits, as a quality indicator (e.g., Ziegele et al., 2020  ###reference_b44###).\nWithin the civility criteria, sarcasm stands out with a rather high negative correlation coefficient. Sarcasm, as well as doubting, criticism, and insults have been identified as one form of expressing disrespect towards other participants (Bender et al., 2011  ###reference_b5###).\nThe large correlation weight for sarcasm is a stable finding, since it is more frequent in the KODIE data, in contrast to insults.\nWhile not being a central aspect of deliberation, storytelling in form of personal anecdotes can foster empathy and mutual understanding between participants and resolve differences (Black, 2008  ###reference_b6###). Thus, it is reasonable that storytelling plays an important role in the weighting of AQuA, as well."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Evaluating the Score",
            "text": "Having trained the AQuA score using the KODIE, #meinfernsehen and CrowdAnno datasets, we next show that the learned adapter weights and correlations transfer to other datasets as well and give scores that are qualitatively and also quantitatively convincing.\n###table_1### ###figure_2### An interesting observation is that the lowest ranked comments in the dataset are much shorter than the high ranked ones. To study whether comment length alone is the most important factor that causes our model to predict a large score, we take a closer look at the distribution of scores depending on the length of the comment. Figure 3  ###reference_###  ###reference_### displays the AQuA score (y-axis) in comparison to the comment length (x-axis, word count). While it is true that short comments get the lowest scores, which is probably due to the fact that they do not have much content, the visual analysis reveals also that medium length comments get the highest scores. This rules out that comment length is the most relevant factor for our score."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1.   SFU Opinion and Comments Corpus",
            "text": "We predict AQuA scores on comments of the SFU opinion and comment corpus (SOCC) (Kolhatkar et al., 2020  ###reference_b26###). The dataset includes 1,121 comments on news articles that have been annotated for constructiveness (binary annotations) and toxicity (four point scale from not toxic to very toxic). According to Kolhatkar et al. (2020  ###reference_b26###), constructive comments are required \u201cto create a civil dialogue through remarks that are relevant to the article and not intended to merely provoke an emotional response\u201d.\nWe calculate AQuA scores and use them to predict the binary constructive label for each comment in the SOCC. Choosing a threshold of 2.3, i.e., inferring , if  2.3, we get an F1 score of 81.73. Note that the threshold is a hyperparameter and a value of 2.3 was chosen, because with performed best on the data. As the dataset also comprises labels for toxic comments, we use the individual adapter predictions for screaming, vulgar, insults, sarcasm, and discrimination to predict the level of toxicity for each comment. Both the SOCC labels  as well as our predictions  are numbers from 0 to 3, therefore we simply use the individual predictions of each adapter as an indicator for the toxicity level and calculate the weighted average F1 score.\nWith 829 comments labeled as not toxic at all (label 0), 172 with label 1, 35 with label 2 and only 7 comments that are marked as clearly toxic (label 3), the distribution is very similar to the one we see in the datasets we used for AQuA. Table 4  ###reference_### shows that we reach good F1 scores for adapters that align with toxicity."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2.   Europolis",
            "text": "For a qualitative analysis of the AQuA score, we apply it to the Europolis dataset (Gerber et al., 2018  ###reference_b20###).\nEuropolis includes transcribed speech contributions of a deliberative poll on migration and climate change,\nannotated for interactivity, respect, storytelling, justification and common good.\nWe calculate AQuA scores for each contribution in the dataset and report the top 3 highest and lowest ranked comments in Table 5  ###reference_###.\nFor interpretability, we list both the predicted labels of the individual adapters and the original Europolis labels (in both cases only for values greater than 0).\nWhile both differ, the AQuA labels approximately match the original Europolis labels. The top 3 comments are all rated highly with positive deliberative aspects such as storytelling, justification and additional knowledge, while the lowest comments exhibit negative deliberative aspects such as\nsarcasm and references to other participants. Overall, all of the the lowest scored comments are questions to clarify certain aspects in the discussion, whereas the higher scored comments consist\nof sophisticated opinions.\nWhen comparing the AQuA predictions to the original Europolis labels, we find that the AQuA score seems consistent with the original labels, while enhancing the prediction since the AQuA score consists of 20 deliberative aspects instead of the 5. This demonstrates the value of AQuA as a unified score that can be applied to any dataset based on the chosen deliberative aspects.\nAn interesting observation is that the lowest ranked comments in the dataset are much shorter than the high ranked ones. To study whether comment length alone is the most important factor that causes our model to predict a large score, we take a closer look at the distribution of scores depending on the length of the comment. Figure 3  ###reference_###  ###reference_###  ###reference_### displays the AQuA score (y-axis) in comparison to the comment length (x-axis, word count). While it is true that short comments get the lowest scores, which is probably due to the fact that they do not have much content, the visual analysis reveals also that medium length comments get the highest scores. This rules out that comment length is the most relevant factor for our score."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "In this work we introduce AQuA, an approach for an automated deliberative quality score based on large language models and adapters. The score combines annotations of experts and the view of non-experts on real online discussion comments.\nWe show that the trained adapters are capable of predicting individual scores for different aspects of deliberative quality and that the overall score aggregates these predictions in a meaningful way. The correlation coefficients between experts\u2019 and non-experts\u2019 annotations reveal the most important positive and negative deliberative aspects, which allows us to confirm theoretical and empirical findings in deliberation literature into AQuA.\nFurthermore, we evaluate our score (trained on KODIE and CrowdAnno) on two further datasets (SOCC and Europolis) to show that the predictions of the learned adapters transfer well to unseen datasets. First, we show that the adapter predictions that build the AQuA score are useful for classifying constructive and toxic comments on the SOCC dataset. Then we perform a qualitative analysis of the AQuA score by manual assessing the top 3 and bottom 3 scored comments in the Europolis dataset and show that comments with well formed opinions receive large scores, while comments providing little value to the discussion receive lower scores.\nOverall, we show that AQuA can be used successfully to automatically assess deliberative quality while aligning with theoretical and empirical background in deliberation literature."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.02761v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.1",
            "4.3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.02761v3",
        "paper_title": "AQuA - Combining Experts\u2019 and Non-Experts\u2019 Views To Assess Deliberation Quality in Online Discussions Using LLMs",
        "research_background": "**Motivation:**\nThis paper is driven by the increasing need to assess the quality of public discussions taking place on online platforms in an automated and scalable manner. Traditional methods of evaluating deliberative quality, which include factors like rationality and civility, rely on manual annotations and retrospective analyses. With the rise of digital communications, there's an urgent need for more efficient methods that leverage natural language processing (NLP) and machine learning to automate this process.\n\n**Research Problem:**\nThe primary research problem addressed by this study is to develop an index that can automatically measure the deliberative quality of individual comments in online discussions through a single score. Existing approaches require manual annotations and retrospective analysis, making them less scalable for the large volumes of online discussions occurring today. Furthermore, there is an ongoing debate about the usefulness of aggregating multiple indices of deliberation into a single score, which this study aims to address by proposing a balanced and theoretically-grounded approach.\n\n**Relevant Prior Work:**\n1. **Core Concepts of Deliberation:**\n   - Deliberation encompasses the exchange of ideas, critical discussion, and consensus-building among citizens (Dryzek, 2002).\n   - Quality dimensions like rationality, civility, reciprocity, and constructiveness are crucial for evaluating deliberative discussions (Friess and Eilders, 2015).\n\n2. **Traditional and Manual Approaches:**\n   - Prior studies have largely relied on manual annotation to gauge deliberation quality (Steenbergen et al., 2003; Friess and Eilders, 2015; Scudder, 2022).\n\n3. **Automated Methods:**\n   - Initial efforts to automate the assessment of deliberative quality have used NLP and machine learning to identify features such as argumentative structure, emotional tone, and engagement patterns (Lawrence and Reed, 2020; Acheampong et al., 2020; Shin and Rask, 2021).\n   - Projects by Falk and Lapesa (2023a) in examining argument and deliberative quality with adapter models (Houlsby et al., 2019) have demonstrated significant interest and progress in this area.\n\n4. **Crowdsourced Annotations:**\n   - Previous research has explored combining expert and non-expert (crowd) annotations to create balanced and comprehensive evaluation metrics for deliberative quality.\n\nBy developing and testing the AQuA index, this study introduces a novel method for combining expert and non-expert evaluations to create an interpretable and explainable measure of deliberative quality. This research contributes to computational social science and provides a practical tool for scholars, policymakers, and practitioners for monitoring and analyzing public dialogues.",
        "methodology": "### AQuA - Combining Experts\u2019 and Non-Experts\u2019 Views To Assess Deliberation Quality in Online Discussions Using LLMs\n\n**Methodology:**\nIn AQuA, we propose a metric aimed at evaluating the quality of individual comments within online discussions. This metric synergizes predictive elements based on various deliberation dimensions with evaluations from both experts and non-experts, culminating in a unified deliberative quality score. The methodology is structured into two primary components:\n\n1. **Utilization of Adapters Trained on Deliberative Facets:**\n   - We employ specialized adapters trained to evaluate discrete facets of deliberation. These facets may include variables such as the degree of justification provided within the comment, among others.\n\n2. **Integration of Experts\u2019 and Non-Experts\u2019 Annotations:**\n   - Both experts and non-experts annotate the same data. Experts provide labels based on specific deliberative qualities, whereas non-experts rate the comments based on their personal perception of deliberativeness.\n   - We compute correlation coefficients between each deliberative criterion (provided by experts) and a binary indicator representing deliberativeness (provided by non-experts). These correlations help to normalize and combine the different criteria into a unified score.\n\n**Key Innovation:**\n- **Aggregative Scoring System:** The core innovation of AQuA lies in its method of aggregating the individual scores derived from the adapters. Instead of treating all deliberative facets equally, we assign varying weights to different aspects based on their contribution to the perceived deliberativeness. This weighted aggregation results in a single, additive deliberative quality score for each comment.\n\nBy integrating expert and non-expert insights and normalizing the deliberative criteria accordingly, our approach aims to provide a nuanced and comprehensive assessment of comment quality in online discussions. This makes AQuA a robust tool for evaluating deliberation, leveraging both specialized understanding and general perception.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n\n1. **Model and Dataset**:\n    - The experiment centers around assessing deliberative quality in online discussions using a defined AQuA score.\n    - The base model used for predictions is briefly discussed, but specific details about the choice and architecture of the model are not explicitly provided.\n    - The dataset consists of user comments with varying lengths and presumably addresses the quality of deliberation within these comments.\n\n2. **Baselines**:\n    - The text no specific baselines are mentioned, focusing instead on the analysis of the model's predictions and characteristics of the dataset.\n\n3. **Evaluation Metrics**:\n    - The primary metric evaluated is the AQuA score itself, which aims to quantify the deliberative quality of user comments.\n\n#### Main Experimental Results\n\n- An interesting observation highlighted is the relationship between comment length and deliberative quality scores:\n    - **Short Comments**: These comments tend to receive the lowest AQuA scores, likely due to having less content that can be evaluated for deliberative quality.\n    - **Medium Length Comments**: Contrary to focusing on only short comments, the analysis reveals that medium-length comments receive the highest AQuA scores, suggesting a more nuanced relationship between comment length and quality.\n\nIn summary, the main experiment demonstrates that the AQuA model can effectively predict deliberation quality, with medium-length comments generally scoring higher than very short ones, indicating that deliberation quality involves more than just comment length."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the best base model for training adapter weights in predicting deliberative quality using the AQuA score.",
            "experiment_process": "The study evaluates different base models, including German BERT Base cased and multilingual BERT (cased and uncased variants), on the KODIE test split. The training procedure follows the same steps as described in Section 3.2. The performance is measured using the weighted averaged F1 score due to the highly imbalanced dataset.",
            "result_discussion": "The multilingual BERT model with cased variants outperforms the German BERT model in 15 out of the 20 tasks. As a result, the multilingual BERT Base cased model is chosen for calculating the AQuA score.",
            "ablation_id": "2404.02761v3.No1"
        },
        {
            "research_objective": "To verify the consistency and relevance of the calculated correlation coefficients used as weights in the AQuA score and compare them to findings in previous deliberative research.",
            "experiment_process": "The coefficients are calculated for each deliberative quality criterion using the subset of the KODIE and #meinfernsehen datasets annotated through the CrowdAnno framework. The study discusses the values and signs of the coefficients, listing absolute frequencies of each label to provide an overview of data distribution.",
            "result_discussion": "The results show that well-reasoned arguments and relevant content strongly positively correlate with higher deliberative quality, supporting earlier research. Conversely, unfounded opinions are negatively correlated. Surprising findings include that references to personal characteristics have a positive impact, while sarcasm has a high negative correlation. These insights help in understanding the weight given to various deliberative aspects in the AQuA score.",
            "ablation_id": "2404.02761v3.No2"
        },
        {
            "research_objective": "To evaluate the transferability and effectiveness of the AQuA score on different datasets, ensuring its qualitative and quantitative reliability.",
            "experiment_process": "The AQuA score is applied to the Europolis dataset, involving contributions annotated for interactivity, respect, storytelling, justification, and common good. The study calculates the AQuA scores and ranks contributions, comparing predicted labels from individual adapters and original Europolis labels.",
            "result_discussion": "The AQuA labels align well with original Europolis labels. Top-ranked comments exhibit positive deliberative aspects like storytelling and justification, while the lowest-ranked contain sarcasm and clarification questions. The AQuA score, representing 20 deliberative aspects, enhances predictions compared to the original 5 aspects, demonstrating its robustness and applicability across datasets.",
            "ablation_id": "2404.02761v3.No3"
        }
    ]
}