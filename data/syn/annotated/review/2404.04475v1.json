{
    "title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
    "abstract": "LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation.\nHowever, these auto-annotators can introduce complex biases that are hard to remove.\nEven simple, known confounders such as preference for longer outputs remains in existing automated evaluation metrics.\nWe propose a simple regression analysis approach for controlling biases in auto-evaluations.\nAs a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for chat LLMs that uses LLMs to estimate response quality.\nDespite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs.\nWe introduce a length-controlled AlpacaEval that aims to answer the counterfactual question: \"What would the preference be if the model\u2019s and baseline\u2019s output had the same length?\"\nTo achieve this, we first fit a generalized linear model to predict the biased output of interest (auto-annotator preferences) based on the mediators we want to control for (length difference) and other relevant features.\nWe then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths.\nLength-controlling not only improves the robustness of the metric to manipulations in model verbosity, we also find that it increases the Spearman correlation with LMSYS\u2019 Chatbot Arena from 0.94 to 0.98.\nWe release the code and resulting leaderboard.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Developing and improving NLP systems requires reliable, low-cost evaluations that can quantify progress. In closed-ended tasks, such as multiple-choice QA, such evaluations are straightforward to implement and trust (Novikova et al., 2017  ###reference_b14###; Yeh et al., 2021  ###reference_b27###). However, such evaluations cannot be applied to extremely open-ended settings such as instruction following for language models.\nEven neural reference-based evaluation metrics such as BERTscore (Zhang* et al., 2020  ###reference_b28###) face challenges in those settings due to the difficulty of collecting a diverse set of references that can cover the space of valid outputs.\nRecently, there has been a push toward reference-free evaluation methods that leverage high-performance LLMs, e.g.\nAlpacaEval (Li et al., 2023  ###reference_b11###), MTBench (Zheng et al., 2023  ###reference_b29###), and WildBench (Lin et al., 2024  ###reference_b12###).\nWhile these approaches show a high correlation with human annotators,\nthey often do so by exploiting spurious correlations such as the length of the output, the presence of lists, or various position biases Li et al. (2023  ###reference_b11###); Zheng et al. (2023  ###reference_b29###); Koo et al. (2023  ###reference_b9###); Wang et al. (2023  ###reference_b25###); Wu & Aji (2023  ###reference_b26###).\nCreating a way to debias automated evaluation metrics would be highly valuable \u2013 it would address the major drawback of LLM-based reference-free evaluations, and enable low-cost, accurate evaluations for developing NLP systems in many open-ended settings.\nOur work focuses on this challenge of taking an existing automated evaluation metric (e.g. AlpacaEval) and a suspected spurious correlate (e.g. length) and producing a debiased metric.\nWe propose a simple, interpretable debiasing strategy for automated evaluation metrics based on basic, regression-based adjustments for observational causal inference.\nWe view spurious correlates \u2013 such as the length of the response \u2013 as undesirable mediators VanderWeele (2015  ###reference_b23###) in a causal graph and use regression-based causal inference Hern\u00e1n & Robins (2010  ###reference_b8###) techniques to provide simple adjustments to automated evaluations that control for any suspected spurious correlation.\nApplying this approach to the popular AlpacaEval benchmark, we show that controlling for length has significant positive effects on automated evaluation.\nWe find that it is more correlated on average with LMSYS\u2019 Chatbot Arena (Zheng et al., 2023  ###reference_b29###) than both (length-uncontrolled) AlpacaEval and MT-bench, and that it is significantly more robust to gaming the evaluation by increasing the verbosity of the output.\nOur contributions are the following:\nWe propose a simple regression-based debiasing approach for automated evaluation that satisfies several desirable properties for an automatic evaluation metric.\nWe apply the approach to AlpacaEval, producing AlpacaEval-LC that is more robust to length-based spurious correlates.\nWe show that AlpacaEval-LC correlates better with the human evaluations of model rankings (Chatbot Arena) fig. 1  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Problem Setting",
            "text": "Our work relates to both the classic literature on reference-free evaluations, as well as more recent developments in automated and human evaluation of chatbots. We describe some of these relevant works below, with some additional exposition on the details of AlpacaEval, which we study more closely in our debiasing experiments.\nReference-free evaluation metrics have a long history, including classic methods (Louis & Nenkova, 2013  ###reference_b13###) and more recent neural supervised learning methods (Kryscinski et al., 2020  ###reference_b10###; Sinha et al., 2020  ###reference_b20###; Goyal & Durrett, 2020  ###reference_b7###).\nWhile this latter class of algorithms has become sufficiently accurate that they match inter-annotator agreement rates, other works have shown that such measurements are heavily confounded by spurious correlations such as perplexity and length (Durmus et al., 2022  ###reference_b4###). Recently, there has been a push to leverage LLMs as a zero-shot, reference-free evaluation measure (Zheng et al., 2023  ###reference_b29###; Dubois et al., 2023  ###reference_b2###; Li et al., 2023  ###reference_b11###; Lin et al., 2024  ###reference_b12###).\nIn the chatbot setting, two well-known such metrics are AlpacaEval and MT-bench, both of which query an LM (based upon GPT4) to attempt to assess the quality of a weaker LM\u2019s output.\nAlpacaEval is an LLM-based automated evaluation metric \u2013 it operates on a fixed set of 805 instructions chosen to be representative of user interactions on the Alpaca web demo.\nFor each instruction, both a baseline model  (currently GPT-4 turbo) and the evaluated model  produce responses.\nA GPT-4 turbo-based evaluator then compares the responses head-to-head and outputs the probability of preferring the evaluated model.\nA win rate is then computed as the expected probability that the auto-evaluator prefers the evaluated model\u2019s output on the 805 instructions.\nThis win rate serves as a performance measure of the evaluated LM chatbot.\nOriginally, AlpacaEval was designed as a development metric for the Alpaca chatbot (Taori et al., 2023  ###reference_b21###) and AlpacaFarm simulator (Dubois et al., 2023  ###reference_b2###).\nThe metric was designed to control for certain biases, such as the order in which the model and baseline were presented, by randomizing their sequence.\nHowever, other factors like length and style effects were not controlled for, as the authors found that humans had similar biases on the analyzed data.\nSubsequent use of AlpacaEval as a leaderboard revealed that these uncontrolled biases could be significantly gamed by AI systems in ways that human biases couldn\u2019t.\nImportant to our work is that AlpacaEval has several interpretable properties.\nAs a win rate, its values are in , it\nhas a symmetry to baseline swaps, i.e, , i.e., and comparing a baseline to itself is \nAny posthoc correction to AlpacaEval should maintain these properties, alongside the usual desiderata of being low-cost, accurate, and robust.\nThe automated approach to pairwise evaluation in AlpacaEval can be viewed as a low-cost approximation to the Chatbot Arena Zheng et al. (2023  ###reference_b29###), which aims to build real-world human evaluations through live interactions.\nThe approach in the chatbot arena is that users are presented with a pair of anonymized language models, and they can send instructions to both language models simultaneously. The user receives responses from both LMs and rates the response that is higher quality.\nAt the end, the head-to-head comparisons are converted to an Elo rating Elo & Sloan (1978  ###reference_b5###) which serves as the model score.\nAs a reminder, the difference of Elo ratings between two players can be converted to a win rate, and vis versa.\nThis approach has many desirable properties \u2013 it is driven by real users and the dynamic nature of the instructions makes it hard to saturate this benchmark.\nHowever, this metric cannot be used for model development due to the cost of running many live human evaluations.\nIn the remainder of this work, we will treat the Chatbot Arena as a silver standard that we wish to approximate.\nAlthough Chatbot Arena likely still contains biases (e.g. internet users may focus on surface features rather than \u201chard to measure\u201d capabilities such as factuality), it represents the largest and most ecologically valid human evaluation process today.\nWe define the problem of pairwise evaluation of a language model in the following way.\nGiven an instruction  sampled from a distribution , a baseline model generates a response  and the evaluated model generates a response .\nA human annotator then produces a preference  indicating the model with the better response.\nAn automated surrogate such as AlpacaEval is a (potentially randomized) predictor  that aims to approximate the corresponding human label .\nAlpacaEval\u2019s current win rate is , i.e., the expected predicted preference of human annotators for the model response over the baseline\u2019s response."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Length-Controlled AlpacaEval",
            "text": "A major challenge in building automated evaluations  is the spurious correlations problem. Specifically, consider a simple example in which there is a spurious correlate  (e.g. length) such that heavily relying upon  can be predictive of the human label .\nThe confounder  is initially predictive of  but becomes less predictive as model builders explicitly begin to optimize against the metric.\nAdopting this causal view, we ask our motivating question\nWhat would the AlpacaEval metric be, if the outputs of all models had the same length as those of the baseline?\nOur goal in this section will be to operationalize this into a simple regression-based estimator.\nTo be precise, we hypothesize that automated evaluation measures  such as AlpacaEval return their quality estimates through a combination of direct effects that measure the quality of the model response and indirect effects that are mediated by spurious variables such as the length of outputs. The goal of controlling for the spurious correlates is thus equivalent to controlling these indirect effects. See fig. 2  ###reference_### for a visual representation.\n###figure_2### This abstraction leads us to a simple approach for bias correction, inspired by methods for estimating Controlled Direct Effect (VanderWeele, 2010  ###reference_b24###), for which simple Generalized Linear Models (GLMs) can provide reasonable estimates.\nOur approach will be to estimate the contribution of 3 different components to the AlpacaEval quality judgment:\nModel identity Whether an output comes from the baseline model  or the evaluated model  should impact the probability that an output wins the pairwise comparison.\nLength of output\nThe length of output is known to affect both human and model judgments of output quality (Dubois et al., 2023  ###reference_b2###; Singhal et al., 2023  ###reference_b19###), and so we expect this to also affect the win probability.\nInstruction difficulty Models do not perform uniformly over instructions: the preference of humans will generally depend on the instruction.\nFor example, the baseline might be much better for coding tasks than any other tasks.\nFor every instruction we thus want to model the difficulty of that task for the baseline.\nNote that the (baseline) \u201cinstruction difficulty\u201d is not caused by \u201cmodel\u201d but conditioning on it can enhance the precision of estimates in regression analysis by reducing unexplained variability Pearl (2009  ###reference_b17###).\nWe can obtain length corrected AlpacaEval score in two steps:\n\n\n1 first, we can fit a model to these three attributes, and\n\n2 then we zero out the \u201clength of output\u201d term to obtain counterfactual estimates of AlpacaEval win rate.\nMotivated by the previous discussion, we will model the AlpacaEval predictions  with a logistic regression that has 3 terms: model, length, and instruction.\nWe will first present the overall regression formula, explain the details of the featurization, and then describe some naturally appealing properties of our featurization.\nThe model and instruction terms are straightforward \u2013 they can be viewed as the log-linear contribution of the model () and each instruction\u2019s difficulty () on the baseline win rate.\nThe length term is linear in a normalized length feature, where the normalizer standardizes the length to have unit variance and transforms this via a tanh, as differences in lengths should have strong diminishing returns on the log odds.\nImportantly, this formula fulfills the identity property, i.e.,  , and symmetry property, i.e,  of the original win rate.\nIdentity holds as the length term is zero due to having no difference in length, while the other two terms are zero as the coefficients are identical.\nFor symmetry note that , and it is clear that swapping  and  flips the sign of the model and instruction terms. For the length term, the same is true as flipping  and  negates the length difference, and tanh is an odd function.\nMore generally any additive term that is antisymmetric and centered around 0 would satisfy the desired properties.\nUsing the model from eq. 1  ###reference_### we can answer the counterfactual question of what the automatic evaluation  might be if the length of the evaluated model matched that of the base model, i.e., .\nIn this case, the second, length term becomes zero and we obtain the length corrected win rate estimate as\nIn other words, we simply remove the length term from the regression and compute the implied win rate.\nTraining of the regression is simple and uses off-the-shelf libraries for fitting generalized linear models.\nSince our GLM uses a logit link function, we fit the model in Eq 1  ###reference_### using the cross-entropy loss .\nIn AlpacaEval\u2019s leaderboard, we use a constant baseline , so without loss of generality we can drop , which can be absorbed into the corresponding parameters for .\nIn total, for a leaderboard with  models and  instructions, our GLM contains  parameters to be estimated from  examples (, ,  for each model,  for each instruction).\nThis will be overdetermined when  and  are both large, as in the case of AlpacaEval.\nHowever, to ensure our procedure is robust even for small  and , we use 5-fold cross-validation with  regularization on weights to avoid potential overfitting.\nThe one complexity of our regression is that the instruction difficulty term  is shared across models, and so we estimate this separately by first fitting a joint regression across all models with the  term fixed to one and using the estimated  from this regression.\nFor the remaining regression coefficients, we simply fit , , and  on the AlpacaEval predictions for each model separately, re-using the already estimated  as these do not depend on the model being evaluated.\nFitting models separately is important as it implies that previously computed metrics won\u2019t change when adding a new model to the leaderboard.\nFinally, we have added an additional weak regularization on  to prevent an adversary from performing attacks that intentionally truncate sequences that a model performs poorly on.\nIn this case, the poor performance of the model would be perfectly correlated with the short length, and the model builder would be able to exploit the length corrections to boost the performance of the model.\nAdding a regularization term makes it so that any model performance issues would be explained by the model terms first, and then any residual effects would be captured by the length effects, as intended.\nThe regularization is weak enough that we empirically found it to not affect non-adversarial models."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "We apply our approach to AlpacaEval, as this benchmark has known length confounders, contains a large set of pre-computed LLM-based pairwise comparisons, and is widely used by the research community.\nWe evaluate our approach on several measures of interest:\nDecreasing length gameability: We call a metric length gameable if simply prompting the model to be more or less verbose significantly affects the metric outcome.\nIdeally, length gameability is low for two reasons.\nFirst, we would like evaluations that prioritize the content rather than the style of the answer.\nSecond, the benchmark should not be too dependent on the prompting strategy as users usually think of evaluations of models rather than the entire system, which includes the prompt.\nCorrelation with chatbot arena: If our gameability and robustness metrics represent better capturing human preference, we should improve our correlation with chatbot arena.\nWe measure Spearman rather than Pearson correlation as probabilities are log-linearly correlated with ELO ratings, rather than linearly.\nRobustness and interpretability: Our corrected metric should be robust to simple adversarial attacks such as truncation, and be interpretable to users as a win-rate.\nWe show that AlpacaEval-LC fulfills all these goals.\nWe release the code  ###reference_### for all experiments.\n###figure_3### Figure 4  ###reference_### shows leaderboard changes due to our length control approach.\nWe see that proprietary models, which often generate shorter responses, perform much better on AlpacaEval-LC, and the biggest rank losses are in open-source models that have gone through the RLHF process Ouyang et al. (2022  ###reference_b15###).\nGiven that AlpacaEval is a potential optimization target for open-source language models, these results are consistent with the hypothesis that existing open models had exploited the length bias of AlpacaEval.\n###figure_4### One potential issue with simple bias corrections is that they may be gamed through white-box adversarial attacks, e.g., postprocessing the outputs of models\nto make them look better on AlpacaEval-LC.\nOne example of such an attack is to truncate all outputs to a few characters, besides those that are much better and around the same length as the baseline.\nA naive GLM fitted on such outputs should naturally predict very high win rates in the counterfactual world where outputs have the same length as the baseline.\nIndeed, when doing such post-processing to GPT-4 outputs, win rates increase from  (AlpacaEval 2.0) to  (AlpacaEval-LC, no regularization).\nTo mitigate such adversarial attacks, our approach includes a regularization term on .\nThis decreases the gamed win rate to  (AlpacaEval-LC with regularization) while having an imperceptible impact on standard models.\nFigure 5  ###reference_### shows that LC win rates can be interpreted similarly to raw win rates.\nIn particular, the baseline always has a win rate of  and\n.\nThis seems very natural but wouldn\u2019t hold for most length-correction methods, such as normalizing by length.\nMore interestingly, a nice property of our GLM is that once we fit the weights for one baseline, we can predict the win rate between any pair of models on the leaderboard.\nAs a result, we can predict the leaderboard for any other baseline as seen in fig. 5  ###reference_###.\nAnother common way to control some covariates is through stratification.\nOne potential metric, dubbed length-balanced (LB) win rate, would thus be to compute the average win rate stratified on examples where the model outputs are (1) longer and (2) shorter than the baseline Duong (2024  ###reference_b3###).\nLB satisfies many of the desiderata of length control but has one main downside: robustness.\nIn particular, stratification relies upon having enough samples within each stratum, otherwise the estimates may rapidly become unstable. This can increase variance, e.g., if one model is naturally longer than another, but can also introduce adversarial vulnerabilities.\nThe first and last rows in fig. 6  ###reference_### show that length-balanced win rates improve both the length gameability (measured by the normalized standard deviation of win rate across concise/standard/verbose prompts) and the Chatbot arena correlation.\nHowever, this approach is strictly dominated by our length-controlled method \u2013 in arena correlation, gameability, and adversarial win rate gains from truncating bad GPT-4 outputs as discussed in section 4.3  ###reference_###.\nAnother option Galambosi (2024  ###reference_b6###); Teortaxes (2024  ###reference_b22###) is to directly normalize the win rate by a function of the length of the model\u2019s and baseline\u2019s output.\nWe have tried several variations on normalization (e.g. directly dividing by lengths, logistic function of lengths, etc).\nIn our experiments, the function that performed best was dividing the raw win rate by a temperature-scaled logistic function of the average difference of lengths.\nWe call this metric length-normalized (LN) win rate.\nFigure 6  ###reference_### shows that this simple LN win rate performs surprisingly well on many of the metrics.\nWe chose to present and implement the length-controlled (LC) win rate, as it is more principled (as an estimate of the direct effect), interpretable (as a win rate), and performs slightly better on all quantitative metrics except adversarial gameability."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "AlpacaEval-LC increases correlation with Chatbot Arena to 0.98",
            "text": "Our prior experiments demonstrate that length control reduces the high sensitivity to length in AlpacaEval.\nHowever, our goal is not simply to make metrics that are less sensitive to length, but to produce metrics that are overall more representative of human judgments.\nFigure 1  ###reference_### shows that controlling for length increased the Spearman correlation with Chat Arena from 0.94 to 0.98.\nOf existing benchmarks, this difference is significant enough to make the length-corrected version of AlpacaEval the metric with the highest correlation with Chat Arena which we are aware of.\nCorrelations are computed on every benchmark that evaluates at least 25 models from the Chatbot Arena.\nAlpacaEval and AlpacaEval-LC have 38 such models, MT bench has 34.\nThe bootstrap p-value comparing the correlation with AlapcaEval\u2019s correlation is 0.07 and 0.06 compared to MT-bench.\nFigure 4  ###reference_###  ###reference_### shows leaderboard changes due to our length control approach.\nWe see that proprietary models, which often generate shorter responses, perform much better on AlpacaEval-LC, and the biggest rank losses are in open-source models that have gone through the RLHF process Ouyang et al. (2022  ###reference_b15###  ###reference_b15###).\nGiven that AlpacaEval is a potential optimization target for open-source language models, these results are consistent with the hypothesis that existing open models had exploited the length bias of AlpacaEval.\n###figure_5###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "AlpacaEval-LC is interpretable and robust",
            "text": "###figure_6### One potential issue with simple bias corrections is that they may be gamed through white-box adversarial attacks, e.g., postprocessing the outputs of models\nto make them look better on AlpacaEval-LC.\nOne example of such an attack is to truncate all outputs to a few characters, besides those that are much better and around the same length as the baseline.\nA naive GLM fitted on such outputs should naturally predict very high win rates in the counterfactual world where outputs have the same length as the baseline.\nIndeed, when doing such post-processing to GPT-4 outputs, win rates increase from  (AlpacaEval 2.0) to  (AlpacaEval-LC, no regularization).\nTo mitigate such adversarial attacks, our approach includes a regularization term on .\nThis decreases the gamed win rate to  (AlpacaEval-LC with regularization) while having an imperceptible impact on standard models.\nFigure 5  ###reference_###  ###reference_### shows that LC win rates can be interpreted similarly to raw win rates.\nIn particular, the baseline always has a win rate of  and\n.\nThis seems very natural but wouldn\u2019t hold for most length-correction methods, such as normalizing by length.\nMore interestingly, a nice property of our GLM is that once we fit the weights for one baseline, we can predict the win rate between any pair of models on the leaderboard.\nAs a result, we can predict the leaderboard for any other baseline as seen in fig. 5  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Comparisons to baselines for length control",
            "text": "###figure_7### Let\u2019s now briefly discuss two other potential family length-correction methods that have been proposed in the community Duong (2024  ###reference_b3###); Galambosi (2024  ###reference_b6###); Teortaxes (2024  ###reference_b22###).\nAnother common way to control some covariates is through stratification.\nOne potential metric, dubbed length-balanced (LB) win rate, would thus be to compute the average win rate stratified on examples where the model outputs are (1) longer and (2) shorter than the baseline Duong (2024  ###reference_b3###  ###reference_b3###).\nLB satisfies many of the desiderata of length control but has one main downside: robustness.\nIn particular, stratification relies upon having enough samples within each stratum, otherwise the estimates may rapidly become unstable. This can increase variance, e.g., if one model is naturally longer than another, but can also introduce adversarial vulnerabilities.\nThe first and last rows in fig. 6  ###reference_###  ###reference_### show that length-balanced win rates improve both the length gameability (measured by the normalized standard deviation of win rate across concise/standard/verbose prompts) and the Chatbot arena correlation.\nHowever, this approach is strictly dominated by our length-controlled method \u2013 in arena correlation, gameability, and adversarial win rate gains from truncating bad GPT-4 outputs as discussed in section 4.3  ###reference_###  ###reference_###.\nAnother option Galambosi (2024  ###reference_b6###  ###reference_b6###); Teortaxes (2024  ###reference_b22###  ###reference_b22###) is to directly normalize the win rate by a function of the length of the model\u2019s and baseline\u2019s output.\nWe have tried several variations on normalization (e.g. directly dividing by lengths, logistic function of lengths, etc).\nIn our experiments, the function that performed best was dividing the raw win rate by a temperature-scaled logistic function of the average difference of lengths.\nWe call this metric length-normalized (LN) win rate.\nFigure 6  ###reference_###  ###reference_### shows that this simple LN win rate performs surprisingly well on many of the metrics.\nWe chose to present and implement the length-controlled (LC) win rate, as it is more principled (as an estimate of the direct effect), interpretable (as a win rate), and performs slightly better on all quantitative metrics except adversarial gameability."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "###figure_8### Length is a well-known bias of automated evaluators of chatbot LLMs but several others have been noted, including a bias of models towards their own outputs Zheng et al. (2023  ###reference_b29###), or presence of lists Dubois et al. (2023  ###reference_b2###). While we focus on a more detailed study of length biases here, we note that the same approaches can be applied to other biases by representing them as additional features in the logistic regression.\nAdditionally, our preliminary explorations of self-annotator biases shows that the effect exists but is often smaller than general model differences. fig. 7  ###reference_### shows that the ranking of considered models does not change when using different annotators.\nIn particular, Claude 3 Opus prefers GPT4 Preview, and Mistral Large prefers the former two than itself.\nOur work is closely related to the recent work that aims to debias (implicit or explicit) reward models used to finetune LLMs with RLHF (Singhal et al. (2023  ###reference_b19###)).\nFor example Shen et al. (2023  ###reference_b18###); Chen et al. (2024  ###reference_b1###) try to train a reward model that is uncorrelated to length by making it predict the length at the same time as the reward and disentangle the two.\nPark et al. (2024  ###reference_b16###) extends this intuition to the case of implicit reward models.\nThis type of debiasing would not work out-of-the-box in typical auto-evaluation settings, e.g. AlpacaEval, which uses closed source LLM as judges rather than training a reward model.\nOur post-hoc debiasing could however be used in the RLHF setting, and we encourage future work to look into that.\nWe propose a simple method for mitigating the length bias of\nLLM-based automatic evaluations, specifically, AlpacaEval.\nThe procedure consists of fitting a generalized linear model to predict the auto-evaluators preferences, conditioned on the length of the models\u2019 output.\nWe then get the length-controlled preference by predicting what the auto-evaluator would have preferred if the model\u2019s output and the baseline\u2019s output had the same length.\nWe show that the resulting length-controlled AlpacaEval, has higher correlations with humans, has much less length bias, and is robust (hard to game)."
        }
    ],
    "url": "http://arxiv.org/html/2404.04475v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.04475v1",
        "paper_title": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
        "research_background": "### Paper's Motivation\nThe paper is motivated by the need for reliable, low-cost evaluation methods in NLP systems, especially in open-ended tasks such as instruction following for language models. Despite the usefulness of recent reference-free evaluation methods leveraging large language models (LLMs), these methods suffer from spurious correlations, particularly related to output length, which undermine their reliability. Addressing these biases would greatly improve the accuracy and utility of automated evaluations in various NLP applications.\n\n### Research Problem\nThe primary research problem tackled in this paper is the development of a debiased automated evaluation metric. Specifically, the authors focus on mitigating the effect of spurious correlates, such as response length, in automated evaluators like AlpacaEval. The goal is to create a more robust and accurate evaluation method that aligns better with human judgments and is less susceptible to manipulative tactics (e.g., artificially increasing output verbosity).\n\n### Relevant Prior Work\n1. **NLP System Evaluations**: Traditional approaches for straightforward tasks like multiple-choice QA (Novikova et al., 2017; Yeh et al., 2021) are easy to implement but not suitable for open-ended settings.\n   \n2. **Neural Reference-Based Evaluation Metrics**: Metrics such as BERTscore (Zhang* et al., 2020) face challenges due to the difficulty in collecting diverse references that cover valid outputs comprehensively.\n\n3. **Reference-Free Evaluation Methods**: Recent proposals such as AlpacaEval (Li et al., 2023), MTBench (Zheng et al., 2023), and WildBench (Lin et al., 2024) use high-performance LLMs and have shown high correlation with human annotators. However, these methods also exploit spurious correlations like output length, list presence, or positional biases (Li et al., 2023; Zheng et al., 2023; Koo et al., 2023; Wang et al., 2023; Wu & Aji, 2023).\n\n4. **Debiasing Techniques**: The proposed approach builds on principles from observational causal inference, such as viewing spurious correlates as undesirable mediators in causal graphs (VanderWeele, 2015) and regression-based causal inference techniques (Hern\u00e1n & Robins, 2010).\n\nBy focusing on these areas, the authors aim to advance methods that can offer low-cost and accurate automated evaluations, crucial for the development and improvement of NLP systems in open-ended settings.",
        "methodology": "The proposed methodology for Length-Controlled AlpacaEval aims to address the issue of spurious correlations in automated evaluation systems such as AlpacaEval. Here's a detailed breakdown of the method:\n\n### Key Components:\n1. **Causal View Approach**:\n    - **Spurious Correlates**: Identifies spurious correlates that could unduly affect evaluation metrics.\n    - **Controlled Direct Effect**: Draws inspiration from methods used to estimate controlled direct effects to control for these spurious correlates.\n\n2. **Regression-Based Estimator**:\n    - **Hypothesis**: Automated evaluation measures (e.g., AlpacaEval) return quality estimates through a combination of direct effects (quality of model response) and indirect effects (mediated by various factors).\n    - **Goal**: Operationalize the hypothesis into a simple regression-based model to control and correct the effects of spurious variables.\n\n3. **Components of the Model**:\n    - **Model Identity**: Whether the output is from the baseline model or the evaluated model affects the probability of winning the pairwise comparison.\n    - **Instruction Difficulty**: Models perform differently based on task instructions; hence, the difficulty of each instruction is modeled for the baseline.\n  \n### Methodology:\n1. **Regression Model**:\n    - **Logistic Regression**: The model for AlpacaEval predictions includes terms for model identity and instruction difficulty.\n    - **Normalization**: Standardization to capture the diminishing returns of differences on log odds.\n    - **Equation**: The logistic regression fulfills identity and symmetry properties, ensuring consistency in evaluation metrics across different scenarios.\n\n2. **Implementation Steps**:\n    - **Step 1**: Fit a logistic regression model incorporating the attributes (model and instruction).\n    - **Step 2**: Obtain counterfactual estimates of AlpacaEval win rate.\n    - **Training and Regularization**: \n        - Use standard libraries for fitting generalized linear models with cross-entropy loss.\n        - Employ 5-fold cross-validation with L2 regularization to prevent overfitting.\n        - Perform separate estimation of instruction difficulty across all models to ensure shared components do not affect individual model assessments.\n        - Weak regularization to prevent adversarial exploitation.\n\n### Innovations:\n- **Counterfactual Estimates**: The methodology provides corrected win rate estimates that reflect quality independent of various factors.\n- **Robust Training**: Use of cross-validation and L2 regularization ensures the model remains robust even with small datasets.\n- **Modular Component Fitting**: Separately fitting components ensures stability in evaluations even as new models are added to the leaderboard.\n- **Antisymmetric Terms**: Ensuring terms are antisymmetric and centered around zero to maintain identity and symmetry properties.\n- **Guard Against Adversarial Behavior**: Regularization prevents exploitation of the model by intentionally manipulating certain aspects.\n\nIn summary, the length-controlled AlpacaEval method ensures that evaluations are unbiased, providing more accurate quality assessments, and maintains robustness and stability across changing datasets and models.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Setup:**\nThe main experiment focuses on applying a length control approach to AlpacaEval, a popular benchmark containing known length confounders and a large set of pre-computed LLM-based pairwise comparisons. The evaluation centers around three primary measures:\n\n1. **Decreasing Length Gameability:** This measure determines if simply prompting the model to be more or less verbose significantly affects the metric outcome, aiming for low gameability to prioritize content over style and lessen dependence on prompting strategies.\n\n2. **Correlation with Chatbot Arena:** Improvement in these metrics should correlate with human preferences better. The correlation is measured using Spearman correlation because probabilities are log-linearly related to ELO ratings.\n\n3. **Robustness and Interpretability:** The corrected metric should be resilient to simple adversarial attacks like truncation and maintain interpretability to users as a win-rate.\n\n**Datasets:**\nThe primary dataset used for evaluation consists of pre-existing pairwise comparisons in the AlpacaEval benchmark.\n\n**Baselines:**\nSeveral baselines are considered for comparison:\n- The original AlpacaEval.\n- Alternative length-control methods like length-balanced (LB) win rate and length-normalized (LN) win rate.\n\n**Evaluation Metrics:**\nMetrics used for evaluation include:\n- Length gameability, measured by win rate variance across prompts.\n- Correlation with Chatbot Arena preferences (using Spearman correlation).\n- Resilience to adversarial attacks and interpretability of win-rates.\n\n**Results:**\n\n1. **Improved Leaderboard:** The application of the length control method shows proprietary models, which generate shorter responses, performing better. This suggests that open-source models previously exploited length biases in AlpacaEval.\n\n2. **Interpretability and Predictability:** The LC method maintains natural interpretability as win rates and allows for robust predictions across different baselines on the leaderboard, unlike other length-correction techniques that can be unstable or adversarially vulnerable.\n\nIn conclusion, the main experiment demonstrates that the AlpacaEval-LC method effectively reduces length gameability, improves correlation with human preferences, and maintains robustness and interpretability, making it a compelling choice over other length-control methodologies."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To reduce the length bias in the AlpacaEval automated evaluation metric and make it more representative of human judgments.",
            "experiment_process": "The research proposes a regression-based estimator to adjust for length bias. The approach involves building a logistic regression model with three main terms: model identity, length of output, and instruction difficulty. The regression model is then used to generate counterfactual win rates by zeroing out the 'length of output' term. The approach utilizes generalized linear models (GLMs) with regularization to prevent overfitting and adversarial attacks. The model parameters are estimated from the AlpacaEval dataset using off-the-shelf libraries and cross-validation.",
            "result_discussion": "The length-controlled version of AlpacaEval significantly reduced length gameability, as evidenced by a lower fluctuation in win rates upon varying verbosity prompts. Additionally, it increased the Spearman correlation with LMSYS' Chatbot Arena from 0.94 to 0.98, making it the most correlated metric with human preferences among evaluated benchmarks. Furthermore, the method proved robust against adversarial attacks, such as intentional truncation of outputs to manipulate win rates.",
            "ablation_id": "2404.04475v1.No1"
        },
        {
            "research_objective": "To compare the effectiveness of length-controlled (LC) win rate with other length-correction methods like length-balanced (LB) win rate and length-normalized (LN) win rate.",
            "experiment_process": "Two alternative length-correction methods were discussed. The length-balanced (LB) win rate involves stratifying examples based on whether the model outputs are longer or shorter than the baseline. The length-normalized (LN) win rate involves normalizing the win rate by a function of the output length difference, using a temperature-scaled logistic function. Both methods' performance was compared to the LC win rate on metrics like length gameability and correlation with the Chatbot Arena. The study conducted a series of experiments to validate the performance, robustness, and interpretability of these methods.",
            "result_discussion": "The length-controlled (LC) win rate outperformed the length-balanced (LB) win rate and the length-normalized (LN) win rate in terms of correlation with the Chatbot Arena, gameability, and robustness. Specifically, while LB improved gameability and correlation, it struggled with robustness due to required sample sizes within each stratum. The LN method performed well on multiple metrics but was less interpretable compared to the LC win rate. Ultimately, the LC win rate was selected for its balanced performance and interpretability.",
            "ablation_id": "2404.04475v1.No2"
        }
    ]
}