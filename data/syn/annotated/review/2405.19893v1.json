{
    "title": "Similarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi\u2013layered Thoughts",
    "abstract": "In recent years, large language models (LLMs) have made remarkable achievements in various domains. However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge-intensive tasks, where retrieval-augmented generation (RAG) can be of help. Nevertheless, existing retrieval-augmented models typically use similarity as a bridge between queries and documents and follow a retrieve-then-read procedure.\nIn this work, we argue that similarity is not always the \u201cpanacea\u201d and totally relying on similarity would sometimes degrade the performance of retrieval-augmented generation.\nTo this end, we propose MetRag, a Multi\u2013layEred Thoughts enhanced Retrieval-Augmented Generation framework.\nTo begin with, beyond existing similarity-oriented thought, we embrace a small-scale utility model that draws supervision from an LLM for utility-oriented thought and further come up with a \u201csmarter\u201d model by comprehensively combining the similarity- and utility-oriented thoughts.\nFurthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task-adaptive summarizer to endow retrieval-augmented generation with compactness-oriented thought.\nFinally, with multi-layered thoughts from the precedent stages, an LLM is called for knowledge-augmented generation.\nExtensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, large language models (LLMs) such as ChatGPT OpenAI (2023a  ###reference_b15###), GPT4 OpenAI (2023b  ###reference_b16###), Llama Touvron et al. (2023  ###reference_b22###) have made remarkable achievements in a variety of tasks due to their marvelous capability in language comprehension and generation Zhou et al. (2023  ###reference_b26###).\nHowever, the untimeliness and cost of knowledge updates Kasai et al. (2022  ###reference_b12###) together with hallucinations issues of LLMs Rawte et al. (2023  ###reference_b19###) have curtailed their applications in knowledge-intensive tasks to a large extent Mallen et al. (2023  ###reference_b14###), where retrieval-augmented generation (RAG) approaches that prepending documents to the query without updating the underlying language models would come in handy Ram et al. (2023  ###reference_b18###); Asai et al. (2023a  ###reference_b1###).\nNevertheless, existing retrieval-augmented generation approaches are typically similarity-based Jiang et al. (2023  ###reference_b10###), i.e., they retrieve documents from external corpus based on similarity. Then, the retrieved documents are prepended as context for LLMs at once or independently for generation augmentation Shi et al. (2023  ###reference_b21###). In total, these approaches have been found to outperform purely parametric\nLLMs Jiang et al. (2023  ###reference_b10###), especially in some knowledge-intensive generation tasks Kwiatkowski et al. (2019  ###reference_b13###).\nIn this work, we argue that similarity is not always the \u201cpanacea\u201d for retrieval-augmented generation and totally relying on similarity would sometimes degrade the performance.\nAs is shown in the upper part of Figure 1  ###reference_te1###, when a user types in the query \u201cTell me about author George RR Martin\u201d, a similarity-driven retrieval system would rank the documents in a given corpus according to similarity metrics, i.e., the semantic relevance or TF-IDF based metric Robertson and Zaragoza (2009  ###reference_b20###). Even though the retrieved documents barely provide useful information, e.g., \u201cGeorge RR Martin is an author\u201d, it would rank higher due to high similarity score and the document that states the publications \u201cThe Song of Ice and Fire\u201d of George RR Martin with higher information gain would rank lower due to inadequate low similarity score. Besides, given the fact that the retrieved documents are often more than one, using them in isolation due to the context limitation of LLMs Shi et al. (2023  ###reference_b21###) or simply aggregating the Top- document without considering the relationships between them makes it difficult to capture the commonalities and characteristics among them and even confuse LLMs due to excessive text length thus incurring information loss and probably performance degradation Mallen et al. (2023  ###reference_b14###).\n###figure_1### Given the above limitations in current approaches, beyond similarity we aim to endow retrieval-augmented generation with multi-layered thoughts (i.e., utility- and compactness-oriented thoughts) for performance boosting.\nHowever, the solution is quite non-trivial, which needs to tackle the following essential challenges:\n(C1) To train a model that is capable of perceiving utility-oriented thoughts rather than solely similarity, external labeled data is required.\nHowever, it is hard to obtain external labeled data for guiding the learning process with explicit supervision. Though LLMs can serve as data annotators and come up with high-quality corpus for model training and have demonstrated their tremendous capability in many circumstances, the innate uncontrollable and instability characteristics would sometimes deteriorate the performance.\n(C2) With high-quality retrieved documents, to reduce the burden that dozens of documents impose on LLMs and better capture the commonalities and characteristics between retrieved documents, document summarization is plausible. However, simple summarization cannot guarantee that the most important information w.r.t. the input query can be retained, hence there is need to train a summarization model that aligns with the task itself thus possessing compactness-oriented thoughts.\nTo this end, we propose MetRag.\nIn particular, with an LLM serves as supervision on document utility w.r.t. the input query, we come up with a utility model that aligns itself with the LLM\u2019s feedback such that beyond similarity it is also equipped with utility-oriented thoughts.\nConsidering that an LLM is strong most of time but sometimes would go out of control, yet dense retrievers trained on labelled corpus remain stable with relevance guarantee, though sometimes useless, we further combine similarity- and utility-oriented thoughts and takes the output of a similarity model and a utility model into consideration (C1), as depicted in Figure 1  ###reference_te1###.\nFurthermore, to endow the summarization model with compactness-oriented thoughts, we first distill the summary ability from a strong teacher model (e.g., GPT4). Afterwards with multiple generated summaries, a subsequent reward model is utilized to further constrain the summarization model to align with the end task (C2).\nWith the multi-layered thoughts derived from the precedent stages, an LLM is called for knowledge-augmented generation.\nAt last, we evaluate the proposed MetRag on multiple knowledge-extensive tasks, extensive experiments and analysis have demonstrated the superiority of the proposed method."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": ""
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Task-Oriented Summarization",
            "text": "Large language models (LLMs) Zhao et al. (2023  ###reference_b25###); OpenAI (2023a  ###reference_b15###, b  ###reference_b16###) have made remarkable achievements in a variety of domains such as question answering, and summarization. However, calling large-scale commercial LLMs are of high cost and may induce data leakage issue, hence many approaches have been devoted to distilling the ability of large-scale LLMs (e.g., ChatGPT OpenAI (2023a  ###reference_b15###)) to small-scale LLMs (e.g., Llama2 Touvron et al. (2023  ###reference_b22###)) and enhance their capabilities for downstream tasks, e.g., generate high-quality summary through distillation Jung et al. (2023  ###reference_b11###). However, for knowledge-intensive tasks, simple summarization is far from optimized because it cannot ensure that the most important information relevant to the input query is retained, hence it is necessary to generate the summary associated with downstream tasks. Though RECOMP Xu et al. (2023  ###reference_b24###) has made its step to train the summarization model for enhancing performance in tasks such as question answer and language modeling, it purely designs intricate samples and performs distillation without further aligning strategy for performance boosting, which downgrades its performance in applications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The proposed approach",
            "text": "###figure_2###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "In this work, we propose MetRag, as shown in Figure 2  ###reference_###. MetRag endows retrieval-augmented generation with multi-layered thoughts by firstly embracing LLM\u2019s supervision for utility-oriented thoughts and combining similarity and utility of documents for performance boosting (I, detailed in Section 3.2  ###reference_###) and further pursuing compactness-oriented thoughts via a task-adaptive summarizer (II, detailed in Section 3.3  ###reference_###), finally incorporating the derived multi-layered thoughts for answer generation (III, detailed in Section 3.4  ###reference_###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "A Tale of Two \u201cModels\u201d",
            "text": ""
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Similarity Model as an Off-the-shelf Retriever",
            "text": "Given an input query , an off-the-shelf dense retriever  is incorporated to map the input to a low-dimensional embedding  such that we can efficiently retrieve relevant documents w.r.t. query  from a given corpus  via a predefined similarity metric  (which could be cosine similarity and so on), where the similarity score between the query and document can be computed as follows:\nWith the derived similarity scores, the Top- documents  that have the highest similarity scores w.r.t. input query  are retrieved for enhancing follow-up tasks."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 LLM\u2019s Supervision Empowered Utility Model",
            "text": "In this work, we argue that similarity is not always the \u201cpanacea\u201d for information retrieval and totally relying on similarity would sometimes degrade the performance of retrieval-augmented generation approaches. As is illustrated in Figure 1  ###reference_te1###, document with higher information gain to the input query would rank lower due to inadequate low similarity score. Inspired by the great success of LLMs in a variety of tasks, we aim to incorporate an LLM for supervision on document utility (In this paper, we define the utility of one document w.r.t. a question by its usefulness in assisting an LLM to answer this question, which is modelled by the normalization of the probability of generating correct answers with a specific LLM.) w.r.t. the input query to endow the retriever  with utility-oriented thoughts.\nTo start with, given the Top- documents set  () that a retriever  considers most similar to the input query , the approximated similarity likelihood of each document  can be formalized as follows:\nwhere  is the temperature hyperparameter that controls the smoothness of probability distribution, with a higher  produces a softer probability distribution while a lower  results in a \"harder\" max operation and  denotes the documents pool for model training.\nTo endow the retriever  with LLM\u2019s insights on retrieval utility, we further incorporate an LLM as the supervision signal on document utility of  w.r.t. the input query  such that beyond similarity the trained utility model can take the utility a document provides into consideration, which we can formalize as follows:\nFinally, we break down the barriers of both sides and push the similarity distribution toward LLM-supervision enhanced utility distribution by minimizing the KL-divergence of these two distributions as follows:\nwhere  is a set of queries. Due to the reason that the LLM is served as the external supervision, during training the parameter of the LLM is frozen and we only update the parameters of retriever  and finally come up with the utility model , thus the overall optimization process remains lightweight. Besides, we further add an empty string  to  in the training process to enable the utility model with the ability to judge whether introducing a document for a query can promote the utility or not thus achieving selective retrieval."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Reunion of Similarity- and Utility- oriented Thoughts",
            "text": "Considering that an LLM would sometimes go out of control and produces unreliable supervision signals which degrade the model performance sometimes yet dense retrievers trained on accurately labelled corpus remain stable with relevance guarantee, though sometimes useless, we further combine similarity- and utility-oriented thoughts and take the output of a similarity model into consideration via an integration strategy . Therefore, the final score between document  and the input query  can be defined as follows:\nwhere  and  find value of the -largest similarity score and -largest utility score among retrieved documents through similarity and utility, respectively. Finally, only documents with  are permitted to proceed to the subsequent stages. With the derived final score, we can easily obtain the current documents set  w.r.t. input query  for follow-up tasks."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Pursuit of Compactness-oriented Thoughts",
            "text": "Given the fact that the retrieved documents are often more than one, using them in isolation due to the context limitation of LLMs or simply aggregating the Top- document without considering the relationships between them makes it difficult to capture the commonalities and characteristics among them and even confuse LLMs thus incurring information loss and probably performance degradation, where text summarization can be of help.\nHowever, simple summarization cannot ensure that the most important information relevant to the input query is retained, therefore it is necessary to train a summarization model that aligns with the task itself. Hence, we propose the Task-adaptive Summarizer, which not only reduces the computational costs in end tasks but also relieves the burden of LLMs to identify relevant information in a large chunk of retrieved documents to obtain compactness-oriented thoughts."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Distilling from Strong Teacher Models",
            "text": "To initiate the summarization process, we design instructions via randomly sampled queries and retrieved documents.\nTo start with, given query , we harness the expertise of a sophisticated teacher model , e.g., GPT4, to extract summarization proficiencies that facilitate the creation of a preliminary summary  and compile an initial corpus  consisting of Instruction-Summary pairs\n, where  and  are defined as follows:\nwhere  is the summarization instruction, Template is the summarization prompt template (detailed in Section A.1  ###reference_###) and  is the -th document retrieved for query  from former stage.\nNext, we apply Lora tuning to meticulously refine an open-source student model, e.g., Llama2, which results in an initial summarizer model  tailored for the end task."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Alignment via End-task Feedback",
            "text": "To ensure the faithfulness of the summarizer to the end task, inspired by the principles of the DPO Rafailov et al. (2023  ###reference_b17###), we incorporate the LLM\u2019s performance on the end task as reward for the summarizer.\nWith model , given query  we adeptly produce concise summaries of the retrieved documents as  and generate prompt  to obtain end task response  and the label  indicating whether the response is correct or not, which accords with the following distribution:\nFormally, we define the training corpus of this aligning process as  consisting of triplets  for each query .\nWith a reward model  parameterized by  to estimate , the binary classification loss can be defined as follows:\nwhere  is the sigmoid function.\nFurthermore, we follow the concept of DPO that eschews the need to explicitly estimate the reward model by solving  as a function of language model policy , which is formalized as follows:"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Knowledge-augmented Generation",
            "text": "With the input query  and the external knowledge  derived from the former stages, we can directly call an LLM (can be fine-tuned in a supervised manner using question answering datasets), where its knowledge-augmented generation of answer  can be formalized as follows:\nwhere  is the probability of the answer  given the query  and the external knowledge , and  denotes the argument of the maximum, i.e., the answer  for which  is maximized."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We split all train, validation and test sets of datasets following Izacard et al. (2023  ###reference_b9###); Asai et al. (2023b  ###reference_b2###).\nOur training data includes randomly sampled instruction-following input-output pairs. We randomly sample 50k pairs from training sets of NQ, TriviaQA and HotpotQA to train our model in different stages with Llama2 as base LLM.\nAll experiments are conducted using 4 NVIDIA A100 GPUs.\nWe train the utility model for 5 epochs with a learning rate of 1e-5, a batch size of 16 for each device, a warm-up ratio of 0.2, the passage window size of 50 and the temperature parameter  set to 0.05.\nFor task-adaptive summarizer and generation model, we utilize open-source Llama Factory 666https://github.com/hiyouga/LLaMA-Factory and adopt Lora tuning for 1 epoch with a learning rate of 5e-5, a batch size of 4 and a cosine learning rate scheduler.\nFor NQ, TriviaQA and HotpotQA, we use 2018 English Wikipedia as the external retrieval source while for PopQA, since the 2018 Wikipedia sometimes lacks articles about entities that have been added to Wikipedia recently, we use December 2020 preprocessed Wikipedia corpus as the external retrieval source. We preprocess the external corpus following Izacard et al. (2023  ###reference_b9###) 777https://github.com/facebookresearch/atlas.\nAs for retriever , we incorporate BGE owing to its superior performance in a variety of benchmark leaderboards and retrieve up to 5 documents for each input query for testing, where  is defined as cosine similarity.\nWe examine the effectiveness of each component in MetRag by preparing the following variants:\ni) MetRag w/o Comb, which removes the combination of similarity model and utility model and degrades to the original similarity based retriever;\nii) MetRag w/o AS, which removes the task-adaptive summarization for information integration.\nWe plot the performance comparison in Figure 3  ###reference_###, from which we can observe that the overall performance would drop a lot when either component is discarded, thus the effectiveness of our dedicate design is verified.\nIn particular, we find that the MetRag w/o Comb performs worst among all variants, showcasing that the augmented information, which serves as the cornerstone in retrieval-augmented generation, deserves more attention.\n###figure_3### ###figure_4### ###figure_5### Since the utility model plays a key role in bring in the supervision of LLMs into passage selection process, we take a closer look at how the passage window size  888Here, since the empty string  is added by default, we only count the passages involved in training. would influence the final performance by directly incorporating the utility model trained under different settings to the final task. Due to the space limit, we average the metrics in four datasets and present the result in Figure 4  ###reference_### and find that the model performance improves as the window size grows, which demonstrates that the growing passage window size endows more LLMs\u2019 powers for distinguishing passage significance among diverse inputs to the utility model thus improving performance on downstream tasks. However, due to the computational burden (the model training time grows linearly with the passage window size) that a large passage window size imposes during training, there is need to balance the trade-off between performance and cost.\nWhen optimizing the utility model, we add an empty string  in the training process to leverage the knowledge of LLMs to achieve selective retrieval. Here, we conduct some case studies to examine this mechanism.\nAs we can see from Table 3  ###reference_###, there are example queries that the utility model deems no retrieval and in total there are 20.9% queries which the utility model ranks the empty string  higher than other documents. As we can see from Table 3  ###reference_###, many of the listed queries are commonsense knowledge that has been memorized in LLMs\u2019 parameters and we can easily call an LLM for the required answer instead of retrieval, which illustrates that in retrieval-augmented generation, despite the further knowledge external corpus introduces, the inherent knowledge of LLMs when deploying retrieval-based techniques is worth investigation and we believe that our work has provided a straightforward and plausible solution.\nHowever, this design still has certain limitations, i.e., the LLM used for end task needs to be the same or stronger than the one utilized during utility model training, otherwise this mechanism may not work.\n[]\\VerbContentQueriesD\nIs Cartagena or Chess more popular around the world?\nIn what country is Chalhuacocha?\nWhat was Ian Fleming\u2019s first Bond book?\nWhen was catch me if you can made?\nCathay is a poetic name for which country?\nWho sings sugar sugar you are my candy girl?\nWho used to present I\u2019m a celebrity now?\nWhat county is Icknield Walk First School located in?\nWhere does the cell spend most of its time in the cell cycle?\nWhat country lies to north of the Republic of Chad?\nAre Vintage Life and InStyle both US-based magazines?\nWho is the first president to be impeached?\nAre Andrew Stevens and Charles Burnett both American?\nWhich Genre of Television did Charles Quinton Murphy act for?\nHow was President Kennedy assassinated?\nWhich animal is the carrier of the h1n1 virus?\nWhat part of the body produces insulin?\nTo further analyse the impact of LLMs on utility modeling, beside Llama2 we also add Baichuan and Llama2 as the base LLMs for comparison by directly incorporating the utility model trained under different settings to the final task and present the metrics in four datasets in Table 2  ###reference_###, from which we have two main conclusions: 1) Usually, a large-scale LLM can outperforms an LLM of smaller-scale, e.g., Llama2 vs Llama2. Furthermore, we also find an interesting phenomenon that sometimes (e.g., PopQA dataset) the score of Llama2 is higher than that of Llama2, which is consistent of the findings reported in Self-RAGAsai et al. (2023b  ###reference_b2###). 2) Baichuan underperforms the Llama series in our experiments, one possible reason is that it places greater emphasis on its application in specific languages such as Chinese, as is stated in its original paperBaichuan (2023  ###reference_b4###).\nTo start with, we introduce a case study to illustrate the virtues of our task-adaptive summarizer in Table 4  ###reference_###, as we can see from the table, since the original information is quite long and with a lot of distracting information, an LLM tend to get lost in a large chunk of words. However, with our task-adaptive summarizer, the most relevant information for this query is extracted thus an LLM can easily answer the question given the extracted knowledge.\n[]\\VerbContentQueD\nWhat is Walter de la Pole\u2019s occupation?\n[]\\VerbContentOriD\nWalter de la Pole: Family Walter was the son and heir of the MP\nSir Edmund de la Pole and his second wife.\nWalter de la Pole Sir Walter de la Pole (November 1371 \u2013 1434), of\nDernford in Sawston, Cambridgeshire, was an English politician.\nRichard de la Pole Richard de la Pole (1480 \u2013 24 February 1525)\nwas a pretender to the English crown. Commonly nicknamed \"White\nRose\", he was the last Yorkist claimant to actively and openly seek\nthe crown of England. He lived in exile\u2026(4203 characters)\n[]\\VerbContentSumD\nWalter de la Pole was an English politician. (44 characters)\nIn sum, the virtues of task-adaptive summarizer are two-fold.\nOn the one hand, by using a small-scale LLM (i.e., a 13B Llama model) as the summarizer it could speed up the inference for downstream tasks.\nThe summary ratio across the four datasets are 8.25%, 7.35%, 6.77% and 7.32%\nrespectively, which might increase a small amount of inference cost by adding the summary stage, nevertheness, since the inference cost of LLMs is linearly correlated with token number, with the knowledge derived from the summarizer, it can potentially decrease the inference cost for high-cost commercial LLMs such as GPT4.\nOn the other hand, it further enhances the performance by extracting the most relevant information for a given task so as to alleviate distracting information, which improves the EM and F1 metrics across the four datasets on average by 1.4% and 1.7% respectively, which further justifies our model design."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We split all train, validation and test sets of datasets following Izacard et al. (2023  ###reference_b9###  ###reference_b9###); Asai et al. (2023b  ###reference_b2###  ###reference_b2###).\nOur training data includes randomly sampled instruction-following input-output pairs. We randomly sample 50k pairs from training sets of NQ, TriviaQA and HotpotQA to train our model in different stages with Llama2 as base LLM.\nAll experiments are conducted using 4 NVIDIA A100 GPUs.\nWe train the utility model for 5 epochs with a learning rate of 1e-5, a batch size of 16 for each device, a warm-up ratio of 0.2, the passage window size of 50 and the temperature parameter  set to 0.05.\nFor task-adaptive summarizer and generation model, we utilize open-source Llama Factory 666https://github.com/hiyouga/LLaMA-Factory and adopt Lora tuning for 1 epoch with a learning rate of 5e-5, a batch size of 4 and a cosine learning rate scheduler.\nFor NQ, TriviaQA and HotpotQA, we use 2018 English Wikipedia as the external retrieval source while for PopQA, since the 2018 Wikipedia sometimes lacks articles about entities that have been added to Wikipedia recently, we use December 2020 preprocessed Wikipedia corpus as the external retrieval source. We preprocess the external corpus following Izacard et al. (2023  ###reference_b9###  ###reference_b9###) 777https://github.com/facebookresearch/atlas.\nAs for retriever , we incorporate BGE owing to its superior performance in a variety of benchmark leaderboards and retrieve up to 5 documents for each input query for testing, where  is defined as cosine similarity."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Tasks and Datasets",
            "text": "We evaluate our proposed MetRag and multiple baselines on a variety of knowledge-intensive public datasets (including general Open-Domain QA: NQ, TriviaQA-unfiltered222As the test set is not publicly available, we follow the split setting of Guu et al. (2020  ###reference_b7###) and use 11,313 queries for testing., HotpotQA and entity-centric QA datasets: PopQA333We test on the long-tail subset that includes 1399 queries with less than 100 Wikipedia page views.) and evaluate the performance via metrics EM (following Mallen et al. (2023  ###reference_b14###), we evaluate the performance based on whether the gold answers are included in the model generations rather than strictly exact matching) and F1.\nAll experiments are conducted in a zero-shot manner, where we provide instructions and retrieved information about tasks without few-shot demonstrations Wei et al. (2022  ###reference_b23###)."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Baselines",
            "text": "Baselines without retrievals.\nWe evaluate strong publicly available pre-trained LLMs,\nChatGLM2 Du et al. (2022  ###reference_b5###),\nLlama2 (Touvron et al., 2023  ###reference_b22###), Baichuan (Baichuan, 2023  ###reference_b4###) and Qwen (Bai et al., 2023  ###reference_b3###); and models trained and reinforced using private data, ChatGPT444We use GPT-3.5-turbo-16K in our experiment. (OpenAI, 2023a  ###reference_b15###).\nBaselines with retrievals.\nWe evaluate models augmented with retrieval only at test time or during training. The first category includes standard RAG baselines, where an LM (e.g., Llama2) generates output given the query prepended with the top retrieved documents. The latter category includes approaches that are trained with retrieved passages, including Self-RAG Asai et al. (2023b  ###reference_b2###), RECOMP Xu et al. (2023  ###reference_b24###)555We report results reported in the original paper.."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Settings",
            "text": "We split all train, validation and test sets of datasets following Izacard et al. (2023  ###reference_b9###  ###reference_b9###  ###reference_b9###); Asai et al. (2023b  ###reference_b2###  ###reference_b2###  ###reference_b2###).\nOur training data includes randomly sampled instruction-following input-output pairs. We randomly sample 50k pairs from training sets of NQ, TriviaQA and HotpotQA to train our model in different stages with Llama2 as base LLM.\nAll experiments are conducted using 4 NVIDIA A100 GPUs.\nWe train the utility model for 5 epochs with a learning rate of 1e-5, a batch size of 16 for each device, a warm-up ratio of 0.2, the passage window size of 50 and the temperature parameter  set to 0.05.\nFor task-adaptive summarizer and generation model, we utilize open-source Llama Factory 666https://github.com/hiyouga/LLaMA-Factory and adopt Lora tuning for 1 epoch with a learning rate of 5e-5, a batch size of 4 and a cosine learning rate scheduler.\nFor NQ, TriviaQA and HotpotQA, we use 2018 English Wikipedia as the external retrieval source while for PopQA, since the 2018 Wikipedia sometimes lacks articles about entities that have been added to Wikipedia recently, we use December 2020 preprocessed Wikipedia corpus as the external retrieval source. We preprocess the external corpus following Izacard et al. (2023  ###reference_b9###  ###reference_b9###  ###reference_b9###) 777https://github.com/facebookresearch/atlas.\nAs for retriever , we incorporate BGE owing to its superior performance in a variety of benchmark leaderboards and retrieve up to 5 documents for each input query for testing, where  is defined as cosine similarity."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results and Analysis",
            "text": "We examine the effectiveness of each component in MetRag by preparing the following variants:\ni) MetRag w/o Comb, which removes the combination of similarity model and utility model and degrades to the original similarity based retriever;\nii) MetRag w/o AS, which removes the task-adaptive summarization for information integration.\nWe plot the performance comparison in Figure 3  ###reference_###  ###reference_###, from which we can observe that the overall performance would drop a lot when either component is discarded, thus the effectiveness of our dedicate design is verified.\nIn particular, we find that the MetRag w/o Comb performs worst among all variants, showcasing that the augmented information, which serves as the cornerstone in retrieval-augmented generation, deserves more attention.\n###figure_6### ###figure_7### ###figure_8### Since the utility model plays a key role in bring in the supervision of LLMs into passage selection process, we take a closer look at how the passage window size  888Here, since the empty string  is added by default, we only count the passages involved in training. would influence the final performance by directly incorporating the utility model trained under different settings to the final task. Due to the space limit, we average the metrics in four datasets and present the result in Figure 4  ###reference_###  ###reference_### and find that the model performance improves as the window size grows, which demonstrates that the growing passage window size endows more LLMs\u2019 powers for distinguishing passage significance among diverse inputs to the utility model thus improving performance on downstream tasks. However, due to the computational burden (the model training time grows linearly with the passage window size) that a large passage window size imposes during training, there is need to balance the trade-off between performance and cost.\nWhen optimizing the utility model, we add an empty string  in the training process to leverage the knowledge of LLMs to achieve selective retrieval. Here, we conduct some case studies to examine this mechanism.\nAs we can see from Table 3  ###reference_###  ###reference_###, there are example queries that the utility model deems no retrieval and in total there are 20.9% queries which the utility model ranks the empty string  higher than other documents. As we can see from Table 3  ###reference_###  ###reference_###, many of the listed queries are commonsense knowledge that has been memorized in LLMs\u2019 parameters and we can easily call an LLM for the required answer instead of retrieval, which illustrates that in retrieval-augmented generation, despite the further knowledge external corpus introduces, the inherent knowledge of LLMs when deploying retrieval-based techniques is worth investigation and we believe that our work has provided a straightforward and plausible solution.\nHowever, this design still has certain limitations, i.e., the LLM used for end task needs to be the same or stronger than the one utilized during utility model training, otherwise this mechanism may not work.\n[]\\VerbContentQueriesD\nIs Cartagena or Chess more popular around the world?\nIn what country is Chalhuacocha?\nWhat was Ian Fleming\u2019s first Bond book?\nWhen was catch me if you can made?\nCathay is a poetic name for which country?\nWho sings sugar sugar you are my candy girl?\nWho used to present I\u2019m a celebrity now?\nWhat county is Icknield Walk First School located in?\nWhere does the cell spend most of its time in the cell cycle?\nWhat country lies to north of the Republic of Chad?\nAre Vintage Life and InStyle both US-based magazines?\nWho is the first president to be impeached?\nAre Andrew Stevens and Charles Burnett both American?\nWhich Genre of Television did Charles Quinton Murphy act for?\nHow was President Kennedy assassinated?\nWhich animal is the carrier of the h1n1 virus?\nWhat part of the body produces insulin?\nTo further analyse the impact of LLMs on utility modeling, beside Llama2 we also add Baichuan and Llama2 as the base LLMs for comparison by directly incorporating the utility model trained under different settings to the final task and present the metrics in four datasets in Table 2  ###reference_###  ###reference_###, from which we have two main conclusions: 1) Usually, a large-scale LLM can outperforms an LLM of smaller-scale, e.g., Llama2 vs Llama2. Furthermore, we also find an interesting phenomenon that sometimes (e.g., PopQA dataset) the score of Llama2 is higher than that of Llama2, which is consistent of the findings reported in Self-RAGAsai et al. (2023b  ###reference_b2###  ###reference_b2###). 2) Baichuan underperforms the Llama series in our experiments, one possible reason is that it places greater emphasis on its application in specific languages such as Chinese, as is stated in its original paperBaichuan (2023  ###reference_b4###  ###reference_b4###).\nTo start with, we introduce a case study to illustrate the virtues of our task-adaptive summarizer in Table 4  ###reference_###  ###reference_###, as we can see from the table, since the original information is quite long and with a lot of distracting information, an LLM tend to get lost in a large chunk of words. However, with our task-adaptive summarizer, the most relevant information for this query is extracted thus an LLM can easily answer the question given the extracted knowledge.\n[]\\VerbContentQueD\nWhat is Walter de la Pole\u2019s occupation?\n[]\\VerbContentOriD\nWalter de la Pole: Family Walter was the son and heir of the MP\nSir Edmund de la Pole and his second wife.\nWalter de la Pole Sir Walter de la Pole (November 1371 \u2013 1434), of\nDernford in Sawston, Cambridgeshire, was an English politician.\nRichard de la Pole Richard de la Pole (1480 \u2013 24 February 1525)\nwas a pretender to the English crown. Commonly nicknamed \"White\nRose\", he was the last Yorkist claimant to actively and openly seek\nthe crown of England. He lived in exile\u2026(4203 characters)\n[]\\VerbContentSumD\nWalter de la Pole was an English politician. (44 characters)\nIn sum, the virtues of task-adaptive summarizer are two-fold.\nOn the one hand, by using a small-scale LLM (i.e., a 13B Llama model) as the summarizer it could speed up the inference for downstream tasks.\nThe summary ratio across the four datasets are 8.25%, 7.35%, 6.77% and 7.32%\nrespectively, which might increase a small amount of inference cost by adding the summary stage, nevertheness, since the inference cost of LLMs is linearly correlated with token number, with the knowledge derived from the summarizer, it can potentially decrease the inference cost for high-cost commercial LLMs such as GPT4.\nOn the other hand, it further enhances the performance by extracting the most relevant information for a given task so as to alleviate distracting information, which improves the EM and F1 metrics across the four datasets on average by 1.4% and 1.7% respectively, which further justifies our model design."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Main Results",
            "text": "From the empirical results across multiple knowledge-intensive datasets, the major findings can be summarized as follows:\nRetrieval largely improves performance.\nWhen compared against approaches without retrieval, the retrieval-augmented approaches demonstrated the superiority in EM metric even when compared to the strong baseline ChatGPT, showing the tremendous power and potential of retrieval-augmented generation. Besides, with retrieval-augmentation, even small-scale LLMs (e.g., 7B LLMs) can achieve comparable performance w.r.t. large-scale LMs (e.g., 13B LLMs) in terms of EM metric, showing their power in pursuit of true knowledge.\nLong-tail queries benefit more from retrieval.\nAs depicted in Table 1  ###reference_###, retrieval-augmented approaches achieve most performance gain in PopQA dataset with long-tail queries while the large-scale competitive LLM ChatGPT performs worst in this dataset due to the untimely updated knowledge, which illustrates that performance degradation due to knowledge updating issues can be alleviated to a great extent by retrieval.\nSupervised Fine-tuning improves instruction following.\nWith regard to F1 metric, we find that approaches trained with retrieved passages perform better, showcasing their ability in instruction following for abstracting concise answers (which is the case of ChatGPT with remarkable instruction following ability for zero-shot tasks). However, for approaches without supervised fine-tuning, there exists a seesaw effect between EM and F1 with one focuses on answer accuracy while other focuses on the balance between exactness and conciseness (which is stated in input prompts).\nThe way of incorporating external information matters!\nDifferent from approaches that directly incorporate the retrieved passages for answer generation, our proposed MetRag endows multi-layered thoughts to \u201ctake the essence and discard the dross\u201d so that the most useful information of retrieved passages can be abstracted and distraction information can be dropped for end tasks. Experimental results in four different datasets illustrate and verify the rationale of our proposed MetRag."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Analysis",
            "text": "We examine the effectiveness of each component in MetRag by preparing the following variants:\ni) MetRag w/o Comb, which removes the combination of similarity model and utility model and degrades to the original similarity based retriever;\nii) MetRag w/o AS, which removes the task-adaptive summarization for information integration.\nWe plot the performance comparison in Figure 3  ###reference_###  ###reference_###  ###reference_###, from which we can observe that the overall performance would drop a lot when either component is discarded, thus the effectiveness of our dedicate design is verified.\nIn particular, we find that the MetRag w/o Comb performs worst among all variants, showcasing that the augmented information, which serves as the cornerstone in retrieval-augmented generation, deserves more attention.\n###figure_9### ###figure_10### ###figure_11### Since the utility model plays a key role in bring in the supervision of LLMs into passage selection process, we take a closer look at how the passage window size  888Here, since the empty string  is added by default, we only count the passages involved in training. would influence the final performance by directly incorporating the utility model trained under different settings to the final task. Due to the space limit, we average the metrics in four datasets and present the result in Figure 4  ###reference_###  ###reference_###  ###reference_### and find that the model performance improves as the window size grows, which demonstrates that the growing passage window size endows more LLMs\u2019 powers for distinguishing passage significance among diverse inputs to the utility model thus improving performance on downstream tasks. However, due to the computational burden (the model training time grows linearly with the passage window size) that a large passage window size imposes during training, there is need to balance the trade-off between performance and cost.\nWhen optimizing the utility model, we add an empty string  in the training process to leverage the knowledge of LLMs to achieve selective retrieval. Here, we conduct some case studies to examine this mechanism.\nAs we can see from Table 3  ###reference_###  ###reference_###  ###reference_###, there are example queries that the utility model deems no retrieval and in total there are 20.9% queries which the utility model ranks the empty string  higher than other documents. As we can see from Table 3  ###reference_###  ###reference_###  ###reference_###, many of the listed queries are commonsense knowledge that has been memorized in LLMs\u2019 parameters and we can easily call an LLM for the required answer instead of retrieval, which illustrates that in retrieval-augmented generation, despite the further knowledge external corpus introduces, the inherent knowledge of LLMs when deploying retrieval-based techniques is worth investigation and we believe that our work has provided a straightforward and plausible solution.\nHowever, this design still has certain limitations, i.e., the LLM used for end task needs to be the same or stronger than the one utilized during utility model training, otherwise this mechanism may not work.\n[]\\VerbContentQueriesD\nIs Cartagena or Chess more popular around the world?\nIn what country is Chalhuacocha?\nWhat was Ian Fleming\u2019s first Bond book?\nWhen was catch me if you can made?\nCathay is a poetic name for which country?\nWho sings sugar sugar you are my candy girl?\nWho used to present I\u2019m a celebrity now?\nWhat county is Icknield Walk First School located in?\nWhere does the cell spend most of its time in the cell cycle?\nWhat country lies to north of the Republic of Chad?\nAre Vintage Life and InStyle both US-based magazines?\nWho is the first president to be impeached?\nAre Andrew Stevens and Charles Burnett both American?\nWhich Genre of Television did Charles Quinton Murphy act for?\nHow was President Kennedy assassinated?\nWhich animal is the carrier of the h1n1 virus?\nWhat part of the body produces insulin?\nTo further analyse the impact of LLMs on utility modeling, beside Llama2 we also add Baichuan and Llama2 as the base LLMs for comparison by directly incorporating the utility model trained under different settings to the final task and present the metrics in four datasets in Table 2  ###reference_###  ###reference_###  ###reference_###, from which we have two main conclusions: 1) Usually, a large-scale LLM can outperforms an LLM of smaller-scale, e.g., Llama2 vs Llama2. Furthermore, we also find an interesting phenomenon that sometimes (e.g., PopQA dataset) the score of Llama2 is higher than that of Llama2, which is consistent of the findings reported in Self-RAGAsai et al. (2023b  ###reference_b2###  ###reference_b2###  ###reference_b2###). 2) Baichuan underperforms the Llama series in our experiments, one possible reason is that it places greater emphasis on its application in specific languages such as Chinese, as is stated in its original paperBaichuan (2023  ###reference_b4###  ###reference_b4###  ###reference_b4###).\nTo start with, we introduce a case study to illustrate the virtues of our task-adaptive summarizer in Table 4  ###reference_###  ###reference_###  ###reference_###, as we can see from the table, since the original information is quite long and with a lot of distracting information, an LLM tend to get lost in a large chunk of words. However, with our task-adaptive summarizer, the most relevant information for this query is extracted thus an LLM can easily answer the question given the extracted knowledge.\n[]\\VerbContentQueD\nWhat is Walter de la Pole\u2019s occupation?\n[]\\VerbContentOriD\nWalter de la Pole: Family Walter was the son and heir of the MP\nSir Edmund de la Pole and his second wife.\nWalter de la Pole Sir Walter de la Pole (November 1371 \u2013 1434), of\nDernford in Sawston, Cambridgeshire, was an English politician.\nRichard de la Pole Richard de la Pole (1480 \u2013 24 February 1525)\nwas a pretender to the English crown. Commonly nicknamed \"White\nRose\", he was the last Yorkist claimant to actively and openly seek\nthe crown of England. He lived in exile\u2026(4203 characters)\n[]\\VerbContentSumD\nWalter de la Pole was an English politician. (44 characters)\nIn sum, the virtues of task-adaptive summarizer are two-fold.\nOn the one hand, by using a small-scale LLM (i.e., a 13B Llama model) as the summarizer it could speed up the inference for downstream tasks.\nThe summary ratio across the four datasets are 8.25%, 7.35%, 6.77% and 7.32%\nrespectively, which might increase a small amount of inference cost by adding the summary stage, nevertheness, since the inference cost of LLMs is linearly correlated with token number, with the knowledge derived from the summarizer, it can potentially decrease the inference cost for high-cost commercial LLMs such as GPT4.\nOn the other hand, it further enhances the performance by extracting the most relevant information for a given task so as to alleviate distracting information, which improves the EM and F1 metrics across the four datasets on average by 1.4% and 1.7% respectively, which further justifies our model design."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we propose MetRag, a multi\u2013layered thoughts enhanced retrieval-augmented generation framework, which first embraces LLM\u2019s supervision to obtain utility-oriented thoughts and combines the similarity and utility of documents for performance boosting, and further pursuits compactness-oriented thoughts via a task-adaptive summarizer.\nFinally, with multi-layered thoughts from the precedent stages, an LLM is called for knowledge-augmented generation.\nExtensive experiments on knowledge-intensive tasks have demonstrated the superiority of the proposed MetRag."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "One of the limitation of our work is that the effectiveness of utility model is highly hinges on a strong LLM\u2019s supervision, although we have find that an LLM like LLama 7B or 13B is enough for training a satisfactory utility model. In sum, our work opens up a fresh perspective to reconsider retrieval-augmented generation, but more complex situation that require reading a large amount of material to\nanswer (e.g., legal or medical documents) is still unresolved. Hence, extending our framework in the super-long contexts is one of the future work."
        }
    ],
    "url": "http://arxiv.org/html/2405.19893v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.2.1",
            "4.2.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.19893v1",
        "paper_title": "Similarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi\u2013layered Thoughts",
        "research_background": "### Paper's Motivation\n\nThe paper is motivated by the limitations of large language models (LLMs) in knowledge-intensive tasks due to issues such as the untimeliness and cost of knowledge updates, as well as hallucination problems. Although retrieval-augmented generation (RAG) approaches have shown promise by leveraging external documents to supplement the LLMs without updating the underlying models, these methods predominantly rely on similarity-based retrieval. The authors argue that relying solely on similarity can degrade performance, particularly when the retrieved documents have high similarity scores but low information utility for the task at hand. They aim to enhance RAG approaches by addressing these limitations and introducing multi-layered thoughts that consider factors beyond mere similarity.\n\n### Research Problem\n\nThe primary research problem addressed in this paper is how to improve the performance of retrieval-augmented generation by moving beyond purely similarity-based approaches. The challenges that need to be addressed include:\n1. Training a model that can perceive utility-oriented thoughts in addition to similarity.\n2. Reducing the burden on LLMs imposed by multiple retrieved documents and capturing commonalities and characteristics between these documents to avoid information loss and performance degradation.\n\n### Relevant Prior Work\n\nThe paper builds upon and differentiates itself from previous work in several key areas:\n- **LLMs and Knowledge-Intensive Tasks**: Previous work highlighted the remarkable achievements of LLMs like ChatGPT and GPT4 in various tasks, but also pointed out their limitations in timely knowledge updates and hallucinations (Zhou et al., 2023; Kasai et al., 2022; Rawte et al., 2023).\n- **Retrieval-Augmented Generation (RAG)**: Existing RAG approaches enhance LLMs by prepending retrieved documents to the query. However, these are typically similarity-based and do not consider the utility or compactness of the information, leading to issues when the most useful documents are not the most similar ones (Jiang et al., 2023; Ram et al., 2023; Asai et al., 2023).\n- **Document Similarity and Summarization**: The limitations of similarity-based retrieval and simple summarization in capturing the most critical information regarding a query have been previously discussed (Robertson and Zaragoza, 2009; Shi et al., 2023; Mallen et al., 2023).\n- **Combining Similarity with Utility and Compactness**: The authors propose combining similarity-oriented thoughts with utility-oriented and compactness-oriented thoughts to better align with the task requirements. This brings in concepts from both dense retrievers and LLM-based feedback (Kwiatkowski et al., 2019).\n\nBy addressing these limitations and leveraging both similarity and utility-oriented retrieval, combined with advanced summarization techniques, the authors propose MetRag, a novel approach to retrieval-augmented generation.",
        "methodology": "I'm sorry, but without the actual methodology text or a description of the proposed method or model, I can't provide a description of the proposed method or model. Could you provide the specific details or text from the methodology section?",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n- Natural Questions (NQ)\n- TriviaQA\n- HotpotQA\n- PopQA\n\n**Training Data:**\n- 50,000 instruction-following input-output pairs sampled from the training sets of NQ, TriviaQA, and HotpotQA.\n\n**Model:**\n- Base Large Language Model (LLM): Llama2\n\n**External Retrieval Source:**\n- 2018 English Wikipedia for NQ, TriviaQA, and HotpotQA\n- December 2020 preprocessed Wikipedia corpus for PopQA\n\n**Training Setup:**\n\n1. **Utility Model Training:**\n   - Epochs: 5 \n   - Learning Rate: 1e-5 \n   - Batch Size: 16 per device\n   - Warm-up Ratio: 0.2 \n   - Passage Window Size: 50 \n   - Temperature Parameter: 0.05\n\n2. **Task-Adaptive Summarizer and Generation Model:**\n   - Using open-source Llama Factory, with Lora tuning \n   - Epochs: 1 \n   - Learning Rate: 5e-5\n   - Batch Size: 4 \n   - Learning Rate Scheduler: Cosine\n\n**Retriever:**\n- BGE for retrieving up to 5 documents per input query, using cosine similarity.\n\n**Infrastructure:**\n- All experiments were conducted on 4 NVIDIA A100 GPUs.\n\n### Evaluation Metrics\n\n- Exact Match (EM) and F1 scores across the four datasets.\n\n### Main Experimental Results\n\nOverall, the results demonstrate the importance of each advanced component in the MetRag model and highlight the trade-offs between performance and computational cost."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the individual contribution of combining similarity and utility models, and the task-adaptive summarization component in MetRag's performance.",
            "experiment_process": "The researchers prepared two variants of MetRag for testing: (i) MetRag w/o Comb, which removes the combination of similarity model and utility model, and (ii) MetRag w/o AS, which removes the task-adaptive summarization for information integration. Experiments were conducted using 4 NVIDIA A100 GPUs. They evaluated the performance via metrics EM and F1 on NQ, TriviaQA, HotpotQA, and PopQA datasets.",
            "result_discussion": "The results showed a significant drop in overall performance when either component was removed, verifying the effectiveness of the dedicated design. MetRag w/o Comb performed the worst, highlighting the importance of the augmented information derived from combining similarity and utility models.",
            "ablation_id": "2405.19893v1.No1"
        },
        {
            "research_objective": "To assess the impact of the passage window size in the utility model on the final performance of MetRag.",
            "experiment_process": "The utility model was trained with different passage window sizes. The metrics from the four datasets (NQ, TriviaQA, HotpotQA, and PopQA) were averaged to assess performance. The experiments included evaluating the balance between increasing passage window size and the associated computational costs.",
            "result_discussion": "Performance improved as the window size grew, showing that a larger passage window size helps the utility model better distinguish passage significance among diverse inputs. However, there is a need to balance performance improvements with the increased computational burden.",
            "ablation_id": "2405.19893v1.No2"
        },
        {
            "research_objective": "To examine the efficacy of leveraging the inherent knowledge of LLMs during the selective retrieval process in MetRag.",
            "experiment_process": "An empty string was added during the utility model training process to leverage LLM knowledge for selective retrieval. Example queries were examined where the utility model ranked this empty string higher than documents, indicating no need for retrieval. Metrics and case studies from four datasets (NQ, TriviaQA, HotpotQA, and PopQA) were analyzed.",
            "result_discussion": "20.9% of the queries indicated that the utility model preferred the LLM's inherent knowledge over retrieved documents. This approach showcased a straightforward solution; however, the LLM utilized for the end task needs to be as strong or stronger than the one used during utility model training.",
            "ablation_id": "2405.19893v1.No3"
        },
        {
            "research_objective": "To compare the impact of different base LLMs on utility modeling in MetRag.",
            "experiment_process": "The utility model was trained using different base LLMs (Llama2, Baichuan) and incorporated directly into the final task. Performance was evaluated across four datasets (NQ, TriviaQA, HotpotQA, and PopQA) to analyze any differences.",
            "result_discussion": "Larger LLMs typically outperformed smaller ones, with some anomalies such as Llama2 scoring higher than Llama2 on PopQA, which was consistent with findings from Self-RAG. Baichuan underperformed compared to the Llama series possibly due to its emphasis on specific languages, like Chinese.",
            "ablation_id": "2405.19893v1.No4"
        },
        {
            "research_objective": "To illustrate the advantages of the task-adaptive summarizer in MetRag.",
            "experiment_process": "A case study was conducted to highlight the effectiveness of the task-adaptive summarizer. The summarizer's impact on inference speed and performance was analyzed across four datasets, with the summarizer using a small-scale LLM (13B Llama model).",
            "result_discussion": "The task-adaptive summarizer improved inference speed and reduced the inference cost. Furthermore, it enhanced performance by extracting the most relevant information, thus improving EM and F1 metrics on average by 1.4% and 1.7% respectively, justifying the model design.",
            "ablation_id": "2405.19893v1.No5"
        }
    ]
}