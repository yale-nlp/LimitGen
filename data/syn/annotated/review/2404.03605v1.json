{
    "title": "Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization",
    "abstract": "We consider the problem of accurate quantization for language models, where both the weights and activations are quantized to 4 bits per parameter with uniform quantization, the lowest bitwidth format natively supported by existing GPU hardware. In this context, the key challenge is activation quantization: it is known that language models contain outlier channels whose values on average are orders of magnitude higher than than other channels, which prevents accurate low-bitwidth quantization with known techniques. We systematically study this phenomena and find that these outlier channels emerge early in training, and that they occur more frequently in layers with residual streams. We then propose a simple strategy which regularizes a layer\u2019s inputs via quantization-aware training (QAT) and its outputs via activation kurtosis regularization. We show that regularizing both the inputs and outputs is crucial for preventing a model\u2019s \u201cmigrating\u201d the difficulty in input quantization to the weights, which makes post-training quantization (PTQ) of weights more difficult. When combined with weight PTQ, we show that our approach can obtain a W4A4 model with integer quantization that performs competitively to the standard-precision W16A16 baseline.111Code is available at https://github.com/aninrusimha/qat-pretrain",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLM) have been shown to contain outlier channels, i.e., feature dimensions whose values are orders of magnitude higher than the others. These outlier channels are known to be crucial for strong model performance (Kovaleva et al., 2021  ###reference_b19###; Puccetti et al., 2022  ###reference_b23###), but pose significant challenges from a model compression perspective, for instance via post-training quantization (PTQ) (Dettmers et al., 2022  ###reference_b8###; Xiao et al., 2023  ###reference_b31###; Wei et al., 2022  ###reference_b28###). Concretely, to enable the use of low-bitwidth integer matrix multiplications\u2014which can lead to significant speed-ups\u2014both the activations and the weights need to be quantized. However the presence of high outlier values in the model activations results in high quantization errors, and thus overall poor PTQ accuracy (see, e.g., Xiao et al. (2023  ###reference_b31###)).\nTo mitigate the effect of outlier channels for activation quantization at the per-tensor level, existing works have explored various approaches, including keeping some of the computations in higher precision (Dettmers et al., 2022  ###reference_b8###; Ashkboos et al., 2023  ###reference_b1###; Zhao et al., 2023  ###reference_b35###), or \u201cmigrating\u201d the difficulty of quantizing outlier channels to other parts of the model (Xiao et al., 2023  ###reference_b31###; Wei et al., 2023  ###reference_b29###; Liu et al., 2023  ###reference_b22###). While the above strategies have been effective for achieving INT8 activation quantization, INT4 quantization with PTQ methods remains an open challenge, with current methods still facing nontrivial degradations in perplexity (Wu et al., 2023  ###reference_b30###; Shao et al., 2023  ###reference_b24###; Yuan et al., 2023  ###reference_b33###).\n###figure_1### In this work, we perform an empirical study of outlier channel phenomena from a pretraining perspective. We find that dimensions with outlier channels emerge relatively early in training (see fig. 1  ###reference_###(a), top), suggesting that their mitigation requires early intervention. These outlier channels are particularly prevalent in the output projection layer of the first layer, as well as the query-key-value projection layers of the other layers. Next, we explore a simple strategy that regularizes a layer\u2019s input and output. On the input side, we show that a quantization-aware training (QAT) approach which learns the clipping values for each activation layer (Choi et al., 2018  ###reference_b6###; Bhalgat et al., 2020  ###reference_b2###) is effective at controlling the number of outlier channels, in addition to mitigating the effect of outliers through clipping (see fig. 1  ###reference_###(b), top).\nHowever, while this approach can train a W16A4 model that has similar perplexity to a W16A16 model, post-training weight quantization to W4A4 results in nontrivial perplexity degradations, due to the model\u2019s weights now becoming more difficult to quantize (see fig. 1  ###reference_###(b), bottom). We thus additionally regularize the kurtosis of a layer\u2019s output, which discourages the creation of outliers wholesale. Specifically, this discourages the layer\u2019s weights having pathologically large rows (fig. 1  ###reference_###(c), bottom).\nPutting all these elements together, we show that we can train a language model at moderate scale (1 billion parameter models trained on 20 billion tokens) whose W4A4 perplexity is competitive to the standard-precision W16A16 baseline."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Uniform Quantization & Quantized Matmuls",
            "text": "We focus on uniform quantization, where the quantized values are evenly spaced between an interval range.\nFormally, for a given matrix  that we wish to quantize to  bits, let  and  be the pre-defined (or learned) clipping values. The quantization function  is then given by,\nwhere  is the scale factor and  is the (optional) zero-point offset.\nThis function, which can be generalized to different granularities of  (e.g., rows, columns or subgroups) transforms the entries of  into integers between .\nThe quantized matrix  can be utilized in two different ways.\nFirst, the value can be dequantized to its original precision via  before multiplication. This method is typically used by pure weight quantization schemes, which multiply in the precision the model was trained in. Weight-only quantization can reduce a model\u2019s memory footprint, and insofar as LLM inference is often memory bound, it can also enable faster inference by reducing the amount of time spent on memory operations during the forward pass (Lin et al., 2023  ###reference_b21###; Frantar & Alistarh, 2024  ###reference_b11###). However, the fact that the actual matmul is done in high precision is a fundamental limitation of weight-only quantization.\nSecond, the quantized values can be directly used for the matrix multiplication.\nLet ,  be the quantized versions of ,  with the respective scaling factors  and offsets .\nWe can approximate  with\nwhere we can make use of low-precision matmuls for .\nIn cases where the rows of  and columns of  are quantized separately with the corresponding scaling vectors  and offset vectors , we can still make use of integer matmuls since  is given by\nwhere  is a vector of 1s and  is the outer product.222If the offset vectors are not integers we can expand the expression and still use integer matmuls for . For the cross terms we can use the identity , and thus we can still make use of integer matmuls for most of the FLOPs.\n Note, however, lower-precision matmuls cannot straightfowardly be used if the  is quantized at the column level.\nThis second strategy which makes use of lower-precision matmuls can significantly improve inference latency and energy efficiency on supported hardware. For example, INT4 tensor core matmuls can be up to four times faster than FP16 tensor core matmuls on the NVIDIA Ampere architecture,333https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/  ###reference_mpere-architecture-in-depth/### while from a hardware-efficiency perspective, dedicated hardware for integer operations require much less area and energy usage than their floating-point counterparts (Jouppi et al., 2021  ###reference_b16###; van Baalen et al., 2023  ###reference_b27###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Challenges in LLM Quantization",
            "text": "In LLMs, the majority of FLOPs are spent on dense matmuls of the form  where  are the input activations (for  input tokens) and  are the model weights. For the Transformer architecture in particular this corresponds to the key, query, value projection layers, as well as the FFN layers. Given the sheer number of FLOPs in LLMs, inference efficiency can be improved significantly through lower-precision matmuls.\nWhile there has been much work on post-training weight-only quantization for pretrained LLMs (Frantar et al., 2022  ###reference_b12###; Dettmers & Zettlemoyer, 2023  ###reference_b7###; Lin et al., 2023  ###reference_b21###; Kim et al., 2023  ###reference_b18###; Dettmers et al., 2023  ###reference_b9###; Chee et al., 2023  ###reference_b4###; Lee et al., 2023  ###reference_b20###; Egiazarian et al., 2024  ###reference_b10###, inter alia), PTQ for activations remains difficult due to the presence of outlier channels in LLMs trained with standard precision (Dettmers et al., 2022  ###reference_b8###; Xiao et al., 2023  ###reference_b31###). Informally, outlier channels are a set of input channels (i.e., columns of ) whose values are many orders of magnitudes higher than the others, and have been shown to be crucial for performance (Kovaleva et al., 2021  ###reference_b19###). If one were just interested in quantizing  independently, outlier channels could be managed by quantizing each column of  separately such that the scaling factor associated with an outlier channel is commensurate. However, as outlined in the previous section this would not enable the use of lower-precision matmuls, which requires  to be quantized by (at most) rows; unfortunately row-level (i.e., per-token) quantization results in significant performance degradations (Xiao et al., 2023  ###reference_b31###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Motivating Study: Outlier Channels in Language Models",
            "text": "We first conduct a preliminary analysis to study the emergence of outlier channels during pretraining, with both our own and open-source models.\nFor our own pretrained models, we use the standard \u201cpre-LayerNorm\u201d Transformer architecture (Xiong et al., 2020  ###reference_b32###), where given layer \u2019s input  we obtain the next layer  via,\nHere  are learnable matrices, and the bias vectors are omitted for brevity. Our study focuses on the following activations that have been previously found to contain outlier channels: QKV Input (), Attn Proj Input (), MLP Input (), MLP Proj Input (). We train 1 billion parameter (24-layer model with 1920 dimensions) on 50 billion tokens from the SlimPajama dataset (Soboleva et al., 2023  ###reference_b26###). We periodically collect activation statistics for all layers by running model checkpoints on (the same) 500K tokens from the C4 dataset.\nFirst, we attempt to measure the prevalence of outlier channels aggregated by layer type and depth. For the purposes of this analysis, we name a channel an outlier if the average absolute value of the channel is over six times the average absolute value of all the input activations. This definition of an outlier channel is somewhat arbitrary, but similar definitions in the literature based on the other metrics (Kovaleva et al., 2021  ###reference_b19###) generate similar results; we use this definition as opposed to definitions on the absolute values (Dettmers et al., 2022  ###reference_b8###) to enable comparison across different layers. The results of this analysis are in fig. 2  ###reference_###.\nOur results generally follow what has been established in the literature: while outliers are distributed throughout depth, the layers which tend to have the most outlier channels in their input are those whose inputs are the residual stream of the network. Interestingly, we find that outlier channels emerge early in training, and rapidly become numerous. The proportion of outlier channels within a layer then decreases gradually and eventually plateaus.\n###figure_4### ###figure_5### We next perform a more granular analysis, where we analyze the average absolute value of channels over the training of a 1B model with 50B tokens. This is shown in fig. 3  ###reference_###.\nWithin channels, we observe that the development of outliers occurs early on during training.\nIn most cases outliers primarily occur in layers that take as input the residual stream, although there is still significant variation in the average magnitude of channels in the input to other layers.\nWe take a closer examination of the development of some the largest individual outlier channels for a particular layer in fig. 4  ###reference_###. Channel 600, which is not an outlier channel, has channel values that are distributed roughly as a Gaussian with a mean of zero. The outlier channels, in comparison, have mean values that are significantly different from zero. This initial examination suggests that outlier channels are not scaled differently than non outlier channels, but have a shifted distribution. This potentially indicates why scaling and shifting methods, like OmniQuant (Shao et al., 2023  ###reference_b24###), outperform scaling-only methods such as SmoothQuant (Xiao et al., 2023  ###reference_b31###).\nTo validate the generality of our observations, we perform our analysis on two publicly available 7B models with public checkpoints,\nPythia (Biderman et al., 2023  ###reference_b3###) and OLMo (Groeneveld et al., 2024  ###reference_b13###).\nIn fig. 5  ###reference_### we can see the development of activation outliers early on in the training of both models, although the outliers in OLMo take longer to develop. Furthermore, we confirm a pattern found across the literature, that the primary place where outliers develop is not between layers in a given attention or MLP block but in the residual stream between blocks. That is, the types of layers that do or do not develop outliers are the same in both our model and the pretrained models (e.g., QKV Input activations have outlier channels, while MLP Proj Input activations do not)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Mitigating Outlier Channels with Activation Regularization",
            "text": "Based on insights from the previous section, we propose a simple regularization strategy for quantizing the activations of the linear layers, where we use QAT on the input activations and simultaneously penalize the kurtosis of the layer\u2019s outputs."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Input Activations: QAT with Learned Clip Values",
            "text": "As evident from \u00a72.1  ###reference_###, the clip values  and  play a key role in uniform quantization. Following PACT (Choi et al., 2018  ###reference_b6###) and LSQ (Bhalgat et al., 2020  ###reference_b2###), we treat these quantization parameters as learnable parameters and optimize them with gradient descent. Concretely, during the forward pass we run the quantization/dequantization step, as shown in algorithm 1  ###reference_###. For the backward pass, we use a straight-through estimator to obtain , ,  from  (the gradients with respect to the quantized/dequantized layer). This is shown in algorithm 2  ###reference_###. We will show in our experiments that quantizing during training is crucial for 4-bit quantization; just clamping the activations without quantization leads to poor performance."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Output Activations: Kurtosis Regularization",
            "text": "In our initial experiments we found that QAT on a layer\u2019s input is sufficient to train a W16A4 model that matches the performance of a W16A16. However, since we do not perform QAT for the weights, efficient deployment requires post-training weight quantization to 4 bits. While existing work has shown that weight-only PTQ to 4 bits (i.e., W16A16  W4A16) can be done almost losslessly (Frantar et al., 2022  ###reference_b12###; Shao et al., 2023  ###reference_b24###), we observed this to not be the case with QAT models, with W16A4  W4A4 resulting in nontrivial perplexity degradations. This is due to the fact that a model can essentially \u201cmigrate\u201d the outlier channels to the corresponding rows of the weight matrix, which makes per-column weight PTQ more difficult (as shown in fig. 1  ###reference_###(b), bottom).\nOne approach to mitigating these outlier weights would be to directly regularize the weights via QAT or some other approach (e.g., -norm regularization). However, we found these direct regularization approaches to result in much worse performance and/or unstable training. We thus adopt a more indirect regularization strategy, exploiting the fact that high input channel weights typically lead to a layer\u2019s outputs having outliers, i.e., the output distribution is heavy-tailed (see fig. 1  ###reference_###). Our approach thus regularizes the output distribution\u2019s kurtosis. which measures how heavy-tailed a distribution is. An estimate of the kurtosis of a set of values  is given by,\n\nwhere  and  are respectively the empirical mean and standard deviation of , and  is a small term for numerical stability. We multiply the sum of the kurtosis estimates for each token with hyperparameter , and add the result to the cross-entropy loss. While prior work has shown the benefits of regularizing the kurtosis of a layer\u2019s activation distribution to be close to that of a uniform distribution (Chmiel et al., 2020  ###reference_b5###), regularizing the output distribution\u2019s kurtosis to make it less heavy-tailed has not been explored before to our knowledge."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Post-training Weight Quantization",
            "text": "After training the model to W16A4 with activation regularization on both the inputs/outputs, we experiment with two methods for quantizating the weights to 4 bits. The simplest baseline we use is round-to-nearest (RTN) quantization, which for our purposes implies per-token (for activations)444While there are more sophisticated activiation quantization approaches (Yuan et al., 2023  ###reference_b33###; Chee et al., 2023  ###reference_b4###), these typically have additional overhead (for low-precision matmuls) and are thus not as fast as simple RTN integer quantization. or per-output-channel (for weights) uniform min-max quantization. While the underperformance of RTN weight quantization versus more sophisticated quantization strategies that use calibration data is widely known, we deliberately include this simple data-agnostic baseline to show that activation regularization results in weights that are also easier to quantize (i.e., less perplexity degradation with RTN). Our second approach applies GPTQ (Frantar et al., 2022  ###reference_b12###), which uses a small amount of calibration data to quantize the weights, and is still near the state-of-the-art for 4-bit weight quantization."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Empirical Study",
            "text": "###table_1### In order to isolate the contributions of each component of our method, we compare against several baselines, on top of the standard-precision baseline. The activation clamping baseline uses static, per-layer clipping values to clamp the input activations. To advantage this approach as much as possible, we use an \u201coracle\u201d clipping values obtained from QAT to decide the per-layer clipping values, which was found to be more effective than grid-searching on the clipping values. In activation clamping the activations are not quantized during training, and thus this baseline isolates the effect of QAT. The kurtosis regularization baseline applies kurtosis regularization just on the outputs, without QAT. The QAT-only baseline just applies QAT in the input activations.\nAll hyperparameters were tuned for our 1B W16A16 baseline and kept constant throughout experiments, except for weight decay where we selected between  for all methods. We use a batch size of 1M tokens, learning rate of 1.5e-4, cosine learning decay, and FP16 precision. For QAT we initialize our clipping values to  for clipping value initializations, unless the layer\u2019s input is bounded. We use the same learning rate but no momentum or weight decay for clip values. For kurtosis we use 1e-5 as the regularization strength.\nWe evaluate the perplexity of each model on the C4 and PTB datasets.\nWe test models in three different weight quantization categories: 16 bits, 4 bits, and 3 bits. The 4-bit and 3-bit experiments test with both RTN and GPTQ.\nFor activations, we test in native precision (16 bits for non-QAT models, and 4 bits for the QAT models) as well as in 4 bits. For GPTQ we use a small amount of C4 data for calibration.\nOur method shows that QAT from scratch is effective for training a model with 4-bit activations. However, given that most available pretrained models are not trained with 4-bit activations, it would be ideal if we could take a 16-bit activation model and then finetune it with QAT to 4 bits.\nTo test for whether this is possible, we performed an extensive hyperparameter search for QAT finetuning on the pretrained 300M baseline model, where we finetune with QAT for 1B tokens.\nEven with extensive hyperparameter tuning, QAT finetuning resulted in a W4A4 model with a 16% degradation in perplexity over the W16A16 baseline. Upon further investigation, we found that while our QAT-pretrained models were able to learn to clip outliers without hurting performance, the QAT finetuning models struggled to do so. Finetuning the model longer than 1 billion tokens did not improve results.\nWe also tried applying OmniQuant (Shao et al., 2023  ###reference_b24###), a state-of-the-art weight-and-activation method for PTQ, to go from W16A16 to W4A4. We found this approach to not perform well, with a significant degradation in perplexity with the 1B model (74.99 on C4 and 107.29 on PTB). Our degradation is larger than what has been reported for pretrained models in the original paper, which could potentially be due to our use of a smaller model (which are typically harder to quantize). Given that the outlier channels seem to emerge early in training (\u00a73  ###reference_###), these negative results highlight the importance of early-training interventions for achieving 4-bit activation models.\nOur use of kurtosis regularization on the output activations to mitigate the effect of \u201cquantization difficulty migration\u201d from the activations to the weights is admittedly indirect. We also experimented with more direct methods for controlling the outliers in the weights: regularizing the kurtosis of the weights instead (at the tensor-level or at the column-level); and regularizing the weight\u2019s  norm. Despite an extensive hyperparameter search, these methods led to unstable training, and we were unable to get these models to converge (unless the regularization-strength hyperparameter was so low that there was effectively no regularization). QAT on the weights also proved unsuccesful, with QAT-weight models underperforming baselines by a significant margin.\nOur QAT approach requires modifying the forward and backward passes, which adds nontrivial overhead with an unoptimized, torch.compile-only implementation. This is mainly due to the reduction step in the clip val gradient in the backward pass. We thus implemented our own CUDA kernels that perform a blockwise reduction followed by atomic additions to enable faster throughput. The throughput of our custom kernels on a single H100 node (with eight GPUs) is shown in\ntable 4  ###reference_###. We find that while there is still some reduction in throughput, it is closer to the baseline setting than the torch.compile implementation. Given that the numbers in table 4  ###reference_### are from a single node, we anticipate that the actual throughput differences would be even smaller when taking into account the necessary overheads of distributed training."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experimental Setup",
            "text": "We use the Megatron-LM (Shoeybi et al., 2020  ###reference_b25###) codebase and train on the SlimPajama dataset (Soboleva et al., 2023  ###reference_b26###). While the trajectory analyses in \u00a73  ###reference_### were done for 50B tokens, due to limited compute we train for 20B tokens for these experiments.\nIn order to isolate the contributions of each component of our method, we compare against several baselines, on top of the standard-precision baseline. The activation clamping baseline uses static, per-layer clipping values to clamp the input activations. To advantage this approach as much as possible, we use an \u201coracle\u201d clipping values obtained from QAT to decide the per-layer clipping values, which was found to be more effective than grid-searching on the clipping values. In activation clamping the activations are not quantized during training, and thus this baseline isolates the effect of QAT. The kurtosis regularization baseline applies kurtosis regularization just on the outputs, without QAT. The QAT-only baseline just applies QAT in the input activations.\nAll hyperparameters were tuned for our 1B W16A16 baseline and kept constant throughout experiments, except for weight decay where we selected between  for all methods. We use a batch size of 1M tokens, learning rate of 1.5e-4, cosine learning decay, and FP16 precision. For QAT we initialize our clipping values to  for clipping value initializations, unless the layer\u2019s input is bounded. We use the same learning rate but no momentum or weight decay for clip values. For kurtosis we use 1e-5 as the regularization strength.\nWe evaluate the perplexity of each model on the C4 and PTB datasets.\nWe test models in three different weight quantization categories: 16 bits, 4 bits, and 3 bits. The 4-bit and 3-bit experiments test with both RTN and GPTQ.\nFor activations, we test in native precision (16 bits for non-QAT models, and 4 bits for the QAT models) as well as in 4 bits. For GPTQ we use a small amount of C4 data for calibration."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "We report the results of our 1B experiments on the C4 and PTB dataset in table 1  ###reference_###.\nWe observe that our approach can learn a W4A4 model that has respectable performance compared to the W16A16 baseline. We also observe that the gap between the QAT model with and without kurtosis expands as weights are quantized more and more.\nAt full precision, the gap is less than 1%. At 4 bits, this expands to between 3% and 4%, and at 3 bits this gap widens to 21%.\nAll non-QAT method have catastrophic performance degradations with 4-bit activations. Activation clamping is the only method that achieves less than two orders of magnitude increase in perplexity.\nIn table 2  ###reference_### we perform experiments on downstream tasks for select models to validate our usage of perplexity as a proxy for downstream performance. We observe that models with similar perplexity exhibit similar downstream performance.\nWe also perform a suite of experiments at the 300M scale, where we just experiment with the QAT baselines. This is shown in  table 3  ###reference_###. We largely observe the same trends, with one exception: the gap between the QAT and QAT+Kurtosis Regularization model is smaller than at the 1B scale.\n###table_2###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Analysis",
            "text": "Our method shows that QAT from scratch is effective for training a model with 4-bit activations. However, given that most available pretrained models are not trained with 4-bit activations, it would be ideal if we could take a 16-bit activation model and then finetune it with QAT to 4 bits.\nTo test for whether this is possible, we performed an extensive hyperparameter search for QAT finetuning on the pretrained 300M baseline model, where we finetune with QAT for 1B tokens.\nEven with extensive hyperparameter tuning, QAT finetuning resulted in a W4A4 model with a 16% degradation in perplexity over the W16A16 baseline. Upon further investigation, we found that while our QAT-pretrained models were able to learn to clip outliers without hurting performance, the QAT finetuning models struggled to do so. Finetuning the model longer than 1 billion tokens did not improve results.\nWe also tried applying OmniQuant (Shao et al., 2023  ###reference_b24###  ###reference_b24###), a state-of-the-art weight-and-activation method for PTQ, to go from W16A16 to W4A4. We found this approach to not perform well, with a significant degradation in perplexity with the 1B model (74.99 on C4 and 107.29 on PTB). Our degradation is larger than what has been reported for pretrained models in the original paper, which could potentially be due to our use of a smaller model (which are typically harder to quantize). Given that the outlier channels seem to emerge early in training (\u00a73  ###reference_###  ###reference_###), these negative results highlight the importance of early-training interventions for achieving 4-bit activation models.\nOur use of kurtosis regularization on the output activations to mitigate the effect of \u201cquantization difficulty migration\u201d from the activations to the weights is admittedly indirect. We also experimented with more direct methods for controlling the outliers in the weights: regularizing the kurtosis of the weights instead (at the tensor-level or at the column-level); and regularizing the weight\u2019s  norm. Despite an extensive hyperparameter search, these methods led to unstable training, and we were unable to get these models to converge (unless the regularization-strength hyperparameter was so low that there was effectively no regularization). QAT on the weights also proved unsuccesful, with QAT-weight models underperforming baselines by a significant margin.\nOur QAT approach requires modifying the forward and backward passes, which adds nontrivial overhead with an unoptimized, torch.compile-only implementation. This is mainly due to the reduction step in the clip val gradient in the backward pass. We thus implemented our own CUDA kernels that perform a blockwise reduction followed by atomic additions to enable faster throughput. The throughput of our custom kernels on a single H100 node (with eight GPUs) is shown in\ntable 4  ###reference_###  ###reference_###. We find that while there is still some reduction in throughput, it is closer to the baseline setting than the torch.compile implementation. Given that the numbers in table 4  ###reference_###  ###reference_### are from a single node, we anticipate that the actual throughput differences would be even smaller when taking into account the necessary overheads of distributed training."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations & Discussion",
            "text": "There are several limitations to our study. While we experiment with language modeling at moderate scale, we were unable to perform experiments on larger models (and train for longer) due to limited compute resources. However, we note that while the 300M parameter models did not benefit as much from the kurtosis intervention on top of QAT, at 1B there was quite a large benefit; this gives us optimism for the utility of our methods at larger scale.\nOur study targets integer quantization to 4 bits to enable the use of INT4 matmuls, which is supported by the Ampere architecture GPUs. The more recent GPU architectures (Hopper, Blackwell) unfortunately do not natively support INT4 matmuls, which limit the applicability of our approach on these GPUs. However, the latest Blackwell architecture supports FP4 computations,555https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/  ###reference_echnologies/blackwell-architecture/### and it is possible that QAT may improve FP4-training and moreover enable even lower-precision quantization.\nFinally, our study focuses on quantizing only the activations of inputs to linear layers, since linear matmuls consumes the majority of FLOPs during LLM inference (on moderate-length sequences). Future work could consider applying QAT to quantize the activations involved in the attention computations, which could be extremely useful in long-context settings."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We study outlier channels in language models from a pretraining perspective. We show that these channels emerge early in pretraining, and are moreover particularly numerous in activations with residual streams. Based on these findings, we propose a simple strategy for mitigating the effect of these outlier channels through activation regularization. We regularize the input activations with QAT plus learned clip values, and we further regularize the output activations via the kurtosis. Our approach is able to learn a W4A4 language model at reasonable scale (1 billion parameters trained on 20B tokens) that is competitive with the standard-precision W16A16 baseline."
        }
    ],
    "url": "http://arxiv.org/html/2404.03605v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.03605v1",
        "paper_title": "Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization",
        "research_background": "**Motivation:**\n\nThe paper addresses the challenge of quantizing activations in large language models (LLMs). Specifically, it tackles the issue of outlier channels, which are feature dimensions with exceedingly high values that are crucial for performance but cause significant quantization errors. This problem becomes particularly pronounced when aiming for low-bitwidth quantization (e.g., INT4) which can lead to substantial reductions in speed and efficiency if not managed properly.\n\n**Research Problem:**\n\nThe core research problem is mitigating the impact of outlier channels on activation quantization to improve the post-training quantization (PTQ) accuracy for LLMs. Existing methods have successfully achieved INT8 quantization but extending these techniques to INT4 quantization remains challenging, often resulting in notable drops in model performance.\n\n**Relevant Prior Work:**\n\n1. **Impact and Origin of Outlier Channels:**\n   - Kovaleva et al. (2021) and Puccetti et al. (2022): Highlighted the importance of outlier channels for model performance.\n   \n2. **Quantization Challenges and Strategies:**\n   - Dettmers et al. (2022), Xiao et al. (2023), Wei et al. (2022): Discussed the high quantization errors resulting from outlier channels and the need for effective quantization techniques.\n   - Dettmers et al. (2022), Ashkboos et al. (2023), Zhao et al. (2023): Proposed approaches such as maintaining some computations at higher precision.\n   - Xiao et al. (2023), Wei et al. (2023), Liu et al. (2023): Suggested strategies for \"migrating\" the difficulty of quantizing outlier channels to other parts of the model.\n\n3. **Quantization-Aware Training:**\n   - Choi et al. (2018), Bhalgat et al. (2020): Introduced methods for QAT that adjusts clipping values for each activation layer to control and mitigate the effects of outlier channels.\n\n4. **Recent Challenges in INT4 Quantization:**\n   - Wu et al. (2023), Shao et al. (2023), Yuan et al. (2023): Documented the ongoing issues and significant performance degradation in perplexity when quantizing to INT4 using current methods.\n\n**Summary of Findings:**\n\n- **Identification and Early Intervention:** The study found that outlier channels emerge early in training, indicating the necessity for early intervention strategies.\n- **Prevalence in Specific Layers:** Outlier channels were predominantly found in the output projection layer of the first layer and the query-key-value projection layers of subsequent layers.\n- **Effectiveness of QAT:** Employing a QAT approach to learn clipping values effectively reduces outlier channels' number and mitigates their impact.\n- **Additional Regularization:** Regularizing the kurtosis of a layer's output to prevent the emergence of outliers by discouraging pathologically large weight rows.\n\n**Conclusion:**\n\nIntegrating these elements, the study proposes a method that allows the training of a language model at a moderate scale (1 billion parameters trained on 20 billion tokens) with W4A4 quantization, achieving perplexity competitive to the W16A16 standard-precision baseline. This presents a significant advancement in successfully applying low-bitwidth quantization to LLMs.",
        "methodology": "In the methodology section of the paper titled \"Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization,\" the authors propose a method centered on a straightforward regularization strategy aimed at the quantization of activations within the linear layers of language models. The key aspects of their proposed approach include:\n\n1. **Quantization-Aware Training (QAT)**: This technique is used on the input activations of the model's linear layers. QAT is a common method for training neural networks with awareness of quantization effects, which helps to preserve model accuracy post-quantization.\n\n2. **Penalization of Kurtosis**: Simultaneously, the method involves penalizing the kurtosis of the output of these layers. Kurtosis is a statistical measure that describes the distribution's \"tailedness\" or the presence of outliers in the data. By penalizing higher kurtosis, the authors aim to mitigate the impact of outlier channels, which can adversely affect the quantization process.\n\nBy integrating these two components\u2014QAT on input activations and kurtosis penalization on layer outputs\u2014the proposed strategy seeks to enhance the robustness and performance of language models when their activations are quantized. This dual focus on regularization and precision aims to address the challenges posed by outliers in channel activations, ultimately leading to more efficient and effective quantization.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\n### Experiment Setup:\n- **Datasets**: The models were evaluated on two datasets \u2013 the C4 and PTB datasets.\n- **Baselines**:\n  - **Standard-Precision Baseline**: Original models without quantization.\n  - **Activation Clamping Baseline**: Uses static per-layer clipping values to clamp input activations, with \u201coracle\u201d clipping values obtained from QAT.\n  - **Kurtosis Regularization Baseline**: Applies kurtosis regularization only on outputs without QAT.\n  - **QAT-Only Baseline**: Applies Quantization-Aware Training (QAT) on input activations alone.\n- **Hyperparameters**: \n  - Tuned primarily for the 1B W16A16 baseline and kept consistent across experiments, except for weight decay.\n  - Batch Size: 1M tokens\n  - Learning Rate: 1.5e-4\n  - Learning Rate Decay: Cosine\n  - Precision: FP16\n  - Clipping Values: Initialized to a set range for QAT, with specific considerations if the layer input is bounded.\n  - Regularization Strength for Kurtosis: 1e-5\n  \n- **Quantization Categories**:\n  - **Weight Quantization**: Models were tested with 16 bits, 4 bits, and 3 bits.\n  - **Activation Quantization**: Evaluated in both native precision (16 bits for non-QAT models and 4 bits for QAT models) and in 4 bits.\n\n### Main Experimental Results:\n- **Effectiveness of Different Methods**:\n  - **QAT from Scratch**: Effective for training models with 4-bit activations.\n  - **QAT Finetuning**:\n    - Extensive hyperparameter tuning was conducted for QAT finetuning on a pretrained 300M model, finetuning with QAT for 1B tokens.\n    - QAT finetuning resulted in a W4A4 model exhibiting a 16% increase in perplexity over the W16A16 baseline.\n    - Pretrained QAT models managed to clip outliers without losing performance, while QAT finetuning struggled, and extending finetuning beyond 1 billion tokens did not yield improvements.\n  - **OmniQuant Application**:\n    - Applying OmniQuant to transition from W16A16 to W4A4 did not perform well, with significant degradation in perplexity (74.99 on C4 and 107.29 on PTB).\n    - The larger degradation compared to previously reported results might be due to the use of a smaller model.\n\n- **Importance of Early Training Interventions**:\n  - Negative results highlighted the critical role of early-training interventions for achieving 4-bit activation models.\n\n- **Performance Considerations**:\n  - The custom CUDA kernels designed for the reduction step in the clip value gradient led to better throughput compared to the torch.compile-only implementation.\n  - Throughput on a single H100 node was closer to the baseline setting, and the expectation was that distributed training would further narrow throughput differences due to necessary overheads."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Assessing the impact of quantizing activations during training using learnable clip values and determining if it's crucial for effective 4-bit quantization.",
            "experiment_process": "The study follows the PACT and LSQ methodologies, treating the quantization parameters as learnable. The forward pass involves running the quantization/dequantization step, and the backward pass uses a straight-through estimator to calculate gradients with respect to the quantized/dequantized layer. The method is compared against simple activation clamping without quantization during training.",
            "result_discussion": "The results highlight that quantizing activations during training is crucial for 4-bit quantization. Merely clamping activations without quantization leads to significantly poor performance.",
            "ablation_id": "2404.03605v1.No1"
        },
        {
            "research_objective": "Evaluating the effectiveness of output activation kurtosis regularization to prevent outlier channels from migrating the difficulty to weights, affecting post-training weight quantization (PTQ).",
            "experiment_process": "This experiment observes the effect of kurtosis regularization on the outputs of W16A4 models, which are then quantized to W4A4 using PTQ. The kurtosis of a layer's output distribution is regularized based on the empirical mean and standard deviation of activations, adding this to the cross-entropy loss. Direct methods like weight kurtosis regularization and norm regularization were also tested but resulted in worse performance.",
            "result_discussion": "QAT on a layer's input suffices for matching W16A16 performance with W16A4, but PTQ to W4A4 without output regularization leads to significant degradation. Regularizing the output distribution\u2019s kurtosis shows potential in mitigating this issue, unlike direct regularization methods which lead to unstable training or poor performance.",
            "ablation_id": "2404.03605v1.No2"
        },
        {
            "research_objective": "Examining the efficacy of different weight quantization approaches after QAT to mitigate perplexity degradation in 4-bit quantized language models.",
            "experiment_process": "The methods tested include per-token RTN quantization and GPTQ, which use a small amount of calibration data. These are compared to each other and analyzed for perplexity degradation. Activation models trained with and without QAT are also compared for compatibility with these quantization techniques.",
            "result_discussion": "GPTQ emerges as superior, closely approaching the state-of-the-art for 4-bit weight quantization. The RTN baseline highlights the improvement in quantizability of weights when using activation regularization, albeit with lesser sophistication and additional overheads for low-precision matmuls.",
            "ablation_id": "2404.03605v1.No3"
        },
        {
            "research_objective": "Isolating the contributions of different components in the proposed quantization-aware training (QAT) method with kurtosis regularization to evaluate their individual and combined impacts on performance.",
            "experiment_process": "The study examines various baselines including activation clamping, output-only kurtosis regularization, and input-only QAT. The W16A16 baseline model's hyperparameters, such as batch size, learning rate, cosine learning decay, and precision, are fine-tuned and kept constant. Models are tested for perplexity on the C4 and PTB datasets across different weight quantization categories (16, 4, and 3 bits) using two approaches: RTN and GPTQ. Activation precision is also varied between 16 bits and 4 bits for evaluation.",
            "result_discussion": "The results demonstrate QAT's effectiveness in training a 4-bit activation model from scratch, more than direct QAT finetuning methods. While QAT models show resilience in clipping outliers, finetuned models struggle, especially without kurtosis regularization. The importance of early-training interventions for achieving 4-bit precision is emphasized. Non-QAT methods suffer significant performance degradation in 4-bit activations, with QAT consistently outperforming, especially as weights are quantized more aggressively.",
            "ablation_id": "2404.03605v1.No4"
        }
    ]
}