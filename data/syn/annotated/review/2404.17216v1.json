{
    "title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
    "abstract": "Many multilingual communities, including numerous in Africa, frequently engage in code-switching during conversations. This behaviour stresses the need for natural language processing technologies adept at processing code-switched text. However, data scarcity, particularly in African languages, poses a significant challenge, as many are low-resourced and under-represented. In this study, we prompted GPT 3.5 to generate Afrikaans\u2013English and Yoruba\u2013English code-switched sentences, enhancing diversity using topic-keyword pairs, linguistic guidelines, and few-shot examples. Our findings indicate that the quality of generated sentences for languages using non-Latin scripts, like Yoruba, is considerably lower when compared with the high Afrikaans\u2013English success rate. There is therefore a notable opportunity to refine prompting guidelines to yield sentences suitable for the fine-tuning of language models. We propose a framework for augmenting the diversity of synthetically generated code-switched data using GPT and propose leveraging this technology to mitigate data scarcity in low-resourced languages, underscoring the essential role of native speakers in this process.\n\n\n\nKeywords:\u2009code-switch, LLM, few-shot, prompting",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Multilingual communities, exemplified well by various African countries, often engage in code-switching, where two or more languages are used within a single discourse Poplack (2001a  ###reference_b23###). This language practice highlights the need to develop more advanced natural language processing (NLP) technologies that can smoothly process and produce code-switched sentences. This will move the needle towards equitable representation of the world\u2019s under-resourced languages, ensuring that everyone has equal access to these technologies (Solorio, 2021  ###reference_b29###).\nThere are numerous challenges in code-switching research. The main three are highlighted by Do\u011fru\u00f6z et al. (2021  ###reference_b8###) as follows: i) data, which is related to quantity, quality and availability; ii) evaluation, which refers to benchmarks and metrics; and iii) challenges related to end-to-end applications, particularly the ability to process and produce code-switched data.\nThe focus of this paper is on the first challenge regarding data. While code-switching frequently occurs in written forms, due to the ubiquitous use of social media platforms, leveraging this data in NLP applications for code-switching presents many challenges. These platforms, with their extensive and diverse linguistic expressions, can be invaluable in gathering code-switched data. Yet, the practical utility of such data is hindered by various factors, including the informal, inconsistent nature of online language (\u00c7etino\u011flu et al., 2016  ###reference_b6###). It is common to use acronyms, emojis and make spelling mistakes which affect quality and usability of such data (Srivastava et al., 2019  ###reference_b31###). Furthermore the diversity of such data is limited to a specific type of language use (Winata et al., 2022  ###reference_b36###).\nTo address the shortage of available data, efforts have been made to create synthetic code-switched data using different methods: from using parallel corpora with linguistic constraints on where a switch can occur (Pratapa et al., 2018  ###reference_b25###; Rizvi et al., 2021  ###reference_b27###) to employing transformer-based models to generate diverse sentences that adhere to lexical and syntactic rules (Riktika et al., 2022  ###reference_b26###). A more recent study evaluated prompting of large language models (LLMs) to generate code-switched data for South East Asian languages (Yong et al., 2023  ###reference_b38###). They explored a few prompting templates with a limited number of topics in a zero-shot manner and cautioned against the use of synthetically generated data without involving native speakers of the language.\nIn this paper, we build on the work of (Yong et al., 2023  ###reference_b38###) to address the question about GPT\u2019s ability to generate code-switched data. Our work overlaps in that we also use an LLM, OpenAI\u2019s GPT, and various topics in the prompts. We increase the number of topics and provide topic-related keywords in an effort to increase diversity and reduce the model\u2019s propensity to default to certain words. Our goal is not to evaluate various prompting templates, however, we add linguistic guidelines in the prompts to further increase diversity. We propose this as an approach towards language agnostic prompting. We also test the performance of GPT 3.5 with few-shot in-context examples. We specifically consider whether GPT can support the generation of larger code-switched datasets and to what extent.\nOur contributions are as follows: (i) we provide a framework to increase the diversity of synthetically generated code-switched data by prompting OpenAI\u2019s GPT; and (ii) we position GPT as a pivot to address code-switched data scarcity in low-resource languages while emphasising the need for native speakers in the loop.\nIncreasing data availability is at the center of developing language models that serve multilingual communities. Our work is a step towards closing the gap in low-resourced and under-represented languages."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Code-Switched Text Generation via GPT-3.5 Prompting",
            "text": "Our prompt-based approach to code-switched (CS) text generation is heavily inspired by the work of Yong et al. (2023  ###reference_b38###), who collected synthetic CS data by prompting LLMs with requests along languages and topics. Their focus was on code-switching English with South-East Asian languages. In our case, we focus on two under-explored and under-resourced code-switching scenarios: Afrikaans\u2013English and Yoruba\u2013English. Although Afrikaans and English are typologically dissimilar (van Dulm, 2007  ###reference_b33###), they are both West Germanic languages and generating CS text should be easier. Yoruba is a tonal language and even more dissimilar to English which could provide challenges when creating synthetic CS data. We extend the limited topics covered in Yong et al. (2023  ###reference_b38###) and present GPT-3.5 not as an autonomous solution to CS data scarcity, but as a potential tool for supporting CS data curation efforts for under-resourced African languages. We specifically use GPT-3.5, firstly as a baseline to compare with the findings from Yong et al. (2023  ###reference_b38###) and secondly, due to the unavailability of the GPT-4 API at the time of our experiments111The API for GPT 4 was made available after we finished the majority of the experiments, \nhttps://openai.com/blog/gpt-4-api-general-availability  ###reference_availability###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Prompting for Afrikaans\u2013English CS Sentences",
            "text": "Building on the prompt template from Yong et al. (2023  ###reference_b38###), which uses topics as guidelines, our approach extends this by (i) incorporating specific code-switching words related to each topic within the prompt and (ii) evaluating the effect of prompt complexity from basic (Section 3.1.1  ###reference_.SSS1###) to more comprehensive prompts (Section 3.1.2  ###reference_.SSS2###).\nWe curate a non-exhaustive list of common conversation topics and associate typical English words from native speakers of Afrikaans and from available online platforms. We cover 22 topics with a total of 355 keywords. For this paper we generate one sentence per keyword for the various prompts. We also develop a general list of words used in code-switching that is not directly linked to a specific topic consisting of 138 words. 90% of the keywords are nouns, verbs and adjectives which is in line with the notion that switching is more likely to occur on these open word classes as opposed to close word classes (such as pronouns and conjunctions) (Kodali et al., 2022  ###reference_b13###)."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1.   Topic-Keyword Basic Prompting",
            "text": "In the six different prompting templates of Yong et al. (2023  ###reference_b38###), one prompt specifically requests a native speaker to give a mixed sentence. This is an indirect way to impose a matrix language (ML). We explicitly include the use of a matrix language in our prompts (Jake et al., 2002  ###reference_b11###). This is to ensure that we adequately represent the low-resourced language. However, we recognise that grammatical constraints on CS is an open research question with varying definitions of acceptability that evolves over time (Bhat et al., 2016  ###reference_b3###).\nThe following shows the basic prompt we used (Prompt 1.1) and a few examples to highlight the behaviour of GPT-3.5 (English translation in Italics).\nTopic: education and training; keyword: skills\n\n\n\n\nExample 1: Ek moet my skills verbeter om \u2019n beter werksgeleentheid te kry.\nI must improve my skills to get a better job opportunity.\nTopic: general conversation; keyword: try\n\n\n\n\nExample 2: Ek sal probeer to finish my assignment op tyd.\nI will try to finish my assignment on time.\nThe matrix language is Afrikaans in Example 1 and English in Example 2. We see from these examples that GPT 3.5 does not necessarily follow the prompt with regards to the matrix language.\nWe do not evaluate word-level language identification therefore we do not explicitly measure adherence to the matrix language prompt in this paper.\nThe results of the generated sentences therefore indicate that GPT 3.5 is capable of generating some coherent sentences and can be corrected where the grammatical structure follows English. Section 4.3  ###reference_### gives a more detailed analysis of code-switch acceptability.\nA key observation from using this basic prompt for generating Afrikaans\u2013English sentences is that sentences are one-dimensional with 80% of sentences starting with a singular personal pronoun: \u2018Ek\u2019 (English: \u2018I\u2019) (Section 4.2.1  ###reference_.SSS1###). This creates the opportunity to explore ways of adding diversity to the type of sentence through the use of basic linguistic guidelines (such as specifying pronouns) which is discussed in the following section."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2.   Linguistic-Based Prompting",
            "text": "Since the word lists contain nouns, verbs and adjectives related to specific topics, content diversity in the sentences is addressed. These are also words that are most typically code-switched (Kodali et al., 2022  ###reference_b13###). To add further diversity in the type of sentence, we add basic linguistic guidelines in the form of varying pronouns (personal, impersonal, interrogative etc.), tenses (past, present and future that alters the verb) and using negative particles. The inclusion of negative particles is randomly initialised and not in each prompt. We also impose a rule that conjunctions must be in the matrix language since conjunctions are part of closed word classes and should less likely be switched.\nPrompt 2.1 is an example of a prompt using linguistic guidelines following with an example of the generated sentence (English translation in Italics). In Example 3 the prompts are adhered to, however, the conjunctions \u2018but\u2019 and \u2018and\u2019 are in English therefore note adhering to the guideline.\nOur preliminary observation is that the prompting approach can support the generation of CS sentences that are diverse. The effect of varying pronouns on sentence diversity is further evaluated in Section 4.1  ###reference_###. Word order structure mimics that of natural speech and can be corrected where needed. We give additional examples and an evaluation of the quality of the sentences in Section 4.3  ###reference_###.\nTopic: physical health and fitness; keyword: race; Pronoun: impersonal; Tense: past; Use a negative particle: No\n\n\n\n\nExample 3: Dit was super lekker om die race te hardloop, but ek ignore die consequences and het te veel ge\u00ebet afterwards.\nIt was super nice to run the race, but I ignore the consequences and ate too much afterwards."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3.   Few-Shot Prompting",
            "text": "In the work from Yong et al. (2023  ###reference_b38###) they did not evaluate the effect of few-shot examples. We therefore evaluate two additional prompts: Prompt 1.2 and Prompt 2.2 where we add five examples of code-switched sentences to Prompts 1.1 and 2.1 respectively. These are general examples and not in the context of the topic."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Prompting for Yoruba\u2013English CS Sentences",
            "text": "In this section we apply the same methodology (Section 3.1  ###reference_###) used to generate Afrikaans\u2013English CS sentences to generate Yoruba\u2013English CS sentences and provide brief observations. We develop similar topic keyword lists for Yoruba with most words overlapping with those developed for Afrikaans\u2013English. In future work we will focus on developing common lists that cover a more diverse set of languages. The following are a few examples of the generated Yoruba\u2013English sentences:\nTopic: information technology; keyword: spreadsheet; Pronoun: indefinite; Tense: future; Use negative particle: Yes\n\n\n\n\nExample 1: Mo ni ko relax, infact mo gba surprise pe spreadsheet j\u1eb9 Yoruba word.\nI said you should relax, infact I accept the surprise that spreadsheet is a Yoruba word.\nTopic: social media; keyword: cope; Pronoun: indefinite; Tense: present; Use negative particle: Yes\n\n\n\n\nExample 2: K\u00f2 s\u00ed \u00e8\u00e8y\u00e0n t\u00f3 y\u00e0n \u00f2n\u00e0 n\u00ed w\u00e1h\u00e1l\u00e0, view y\u00ec\u00ed ni aw\u1ecdn \u1eb9\u0300d\u00e1 t\u00ed w\u1ecd\u0300n \u1e63e l\u00e0ti cope.\nThere is no person that chooses problems as a path, this view is what the creatures XXX did to cope\nExamples 1 and 2 both follow the prompt guidelines with respect to the matrix language and tense. Example 1, however, uses a personal pronoun instead of an indefinite pronoun with Example 2 using the correct pronoun. XXX in Example 2 indicates a phrase that cannot be translated.\nWe observe that the prompting approach can also support the generation of Yoruba\u2013English sentences that are diverse.\nWe provide observations on the coherence and naturalness of synthetic sentences in Section 4.4  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Evaluation of Generated Data",
            "text": "In this section, we evaluate our work in three parts: (i) we evaluate the diversity of the generated sentences, (ii) we comment on GPT 3.5\u2019s adherence to the prompts provided, and (iii) we evaluate the quality of the sentences generated through a combination of statistical analysis and human evaluation of the sentences. We use the four prompt guidelines as discussed in Section 3  ###reference_###. For this paper we Romanised the Yoruba\u2013English sentences for easier evaluation, however, we will include this in future work."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Data Diversity",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1.   Content Diversity",
            "text": "In Figure 1(a)  ###reference_sf1### (from Prompt 1.1) we see a large amount of general words being used compared with the number of sentences. We also note that the top three keywords (amazing, acknowledge, anyway) is the same as the top three keywords in the alphabetised list. In Prompt 2.1 we provide a randomised general word list to GPT 3.5 and in Figure 1(b)  ###reference_sf2### we observe a more even distribution of general words as a result. This indicates GPT 3.5\u2019s sensitivity to prompts and the context provided.\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2.   Linguistic Diversity",
            "text": "Since Prompts 2.1 and 2.2 asked \u201cstart the sentence with\u2026\u201d, all sentences were evaluated accordingly. We used a list of common Afrikaans and Yoruba pronouns to evaluate this prompt.\nFrom Figure 3  ###reference_### we observe an increase in diversity of the types of sentences with regards to the distribution of pronouns (Prompts 2.1/2.2). For Afrikaans\u2013English, more than 90% of the sentences start with one of the specified pronouns. We also see an increase in the diversity of Yoruba\u2013English sentences, however, there are still 35% of sentences starting with words other than the requested pronouns. It is not well understood why GPT 3.5 ignored these prompts. In the absence of linguistic guidelines in the prompt, we note that by adding few-shot examples, we lack diversity (Prompts 1.2 and 2.2).\n###figure_3### Similarly to pronouns, we use Afrikaans and Yoruba keywords that indicate past and future tense, negation (negative sentiment) and conjunctions to evaluate the effect of adding these guidelines to the prompts. In Table 1  ###reference_### we highlight the impact of these factors on distribution in sentences using Prompts 1.1 and 2.1 (prompts without example sentences).\nWe see in Table 1  ###reference_### that for Afrikaans\u2013English, both the distribution of tenses (equal distribution between past and future) and the presence of negation improved. However, it is only negation that improved for Yoruba\u2013English. We further elaborate on this observation in Section 4.2.1  ###reference_.SSS1###. The ratio of Afrikaans:English conjunctions decreased showing the guideline is not efficient. For Yoruba:English conjunctions we observe a slight improvement.\nThe above statistical evaluation of diversity shows that adding various linguistic guidelines to the prompts improves diversity. However, this does not consider whether a prompt is adhered to. In the next section, we evaluate GPT 3.5\u2019s ability to execute prompts."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Prompt Adherence",
            "text": "In Section 3.1  ###reference_### we already observed that GPT 3.5 does not always adhere to using the specified matrix language and since we do not consider word-level language identification in this paper, we exclude this when determining adherence.\nWe apply a simple approach to calculate prompt adherence. We express the number of prompts adhered to as a percentage of the total prompts given. In Prompt 1.1, the only prompt given is the topic keyword hence a total of one prompt (the same for Prompt 1.2). In Prompt 2.1, there are five prompts given: topic keyword, pronoun, tense, negative particle and conjunction. The average prompt adherence across the sentences is then used to represent overall prompt adherence."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1.   Statistical Evaluation of Prompt Adherence",
            "text": "In this section we present the prompt adherence for the four prompt guidelines. Keywords for pronouns, tenses, negative particles and conjunctions as per Section 4.2.1  ###reference_.SSS1###. Table 2  ###reference_### shows the overall prompt adherence.\nFrom Table 2  ###reference_### we see that the adherence to prompts for Yoruba\u2013English is much lower than for Afrikaans\u2013English in the linguistically guided prompts (Prompts 2.1 and 2.2).\nIn Afrikaans there are a few specific keywords such as \u2018nie\u2019, \u2018nooit\u2019, \u2018nee\u2019 (English: not, never, no) that indicate negation. Similarly for tenses, words like \u2018was\u2019, \u2018gister\u2019, \u2018wil\u2019, \u2018more\u2019 (English: was, yesterday, will, tomorrow) can be used for past and future tense. However, the Yoruba language is more complex and keywords like the above-mentioned are not adequate to identify negation and tenses, hence the lower prompt adherence.\nIn the next section (Section 4.3  ###reference_###) we use manual annotation of sentences for tenses and negation to re-evaluate prompt adherence."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2.   Manual Evaluation of Prompt Adherence",
            "text": "For manual evaluation of generated sentences, we sample 100 sentences each from the four prompt methods.\nWe manually annotate the sentences of Prompts 2.1 and 2.2 with tense (past or future) and negation (whether the sentence expresses some negative sentiment). In future work, external annotators will also be used.\nIn Table 3  ###reference_### we show the impact on the calculated prompt adherence (using Prompt 2.1) for the statistical (1) and manual (2) evaluation of the 100 sentences. The prompt adherence for Yoruba\u2013English increased to 66% from 59% with a significant increase in the adherences to tenses.Afrikaans\u2013English prompt adherence remains constant. The adherence to negation reduced slightly for both languages. This confirms the earlier comment that it is statistically more difficult to calculate prompt adherence for Yoruba\u2013English without a human in the loop.\nWe conclude that there is potential in using GPT 3.5 as a supporting tool to generate diverse sentences with linguistically guided prompts. In the following sections we provide an overview of the quality of generated sentences to further determine the role that GPT 3.5 can play in addressing code-switched data availability."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Code-Switch Acceptability",
            "text": "The final part of our analysis looks at the quality of generated sentences. As mentioned in Section 4.3  ###reference_###, we sampled 100 sentences from each of the four prompt methods. For this part of the analysis, we rated the acceptability of a code-switch sentence according to: i) Yes, ii) Yes, with minimal changes or iii) No. We adopt the constraint-free approach of MacSwan (2000  ###reference_b16###).\nThe results of the manual annotation are shown in Figure 4  ###reference_###. We observe that the acceptability of Afrikaans\u2013English sentences far outweighs that of Yoruba\u2013English. We also see that adding few-shot examples increases acceptability (Prompts 1.2 and 2.2). Although we observe an increase in diversity through linguistic guidelines, the quality of sentences are sub-optimal. Subsequent work will focus on how correctable sentences can be used for improved prompting and/or fine tuning of language models. However, with further analysis and improvement, there is potential to use GPT 3.5 to support synthetic data generation.\n###figure_4###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Language Specific Observations",
            "text": ""
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1.   Afrikaans\u2013English",
            "text": "In order to quantify the acceptability observed from internal evaluation, we randomly select 5 Afrikaans-English sentences from the dataset used for manual evaluation (Section 4.3  ###reference_###). Table 4  ###reference_### gives the sentences with translations and comments.\nIn our general overview we find that the typical mistakes made are as a result of following English grammar structure. However, for many sentences this does not affect the meaning and can be corrected.\nThe results from the various experiments therefore indicate that using GPT 3.5 (and it\u2019s followers) can be considered as a method to generate large-scale data in Afrikaans-English code-switching."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2.   Yoruba\u2013English",
            "text": "Similarly to quantifying the Afrikaans-English sentences, we give 5 randomly selected Yoruba-English sentences in Table 5  ###reference_### from the dataset used for manual evaluation (Section 4.3  ###reference_###).\nIt is hypothesised that the exposure of GPT 3.5 to the Yoruba language is to a much lesser extent than Afrikaans yielding a substantial amount of unacceptable sentences. Furthermore, as was postulated by Yong et al. (2023  ###reference_b38###), languages using the English alphabet and Latin script perform better on LLMs. Further analysis is required to improve prompting and quality of sentences."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusions and Future Work",
            "text": "In this paper we extended on the of Yong et al. (2023  ###reference_b38###) where they used prompting of LLMs (including GPT 3.5) to generate code-switch sentences. Our approach evaluates three dimensions: (i) diversity, through a wider range of topics, keywords, linguistic guidelines and few-shot examples; (ii) prompt adherence, to understand the ability of GPT 3.5 to follow these prompts; and (iii) quality, to determine the use of GPT 3.5 as a supporting tool to address code-switched data scarcity.\nWe evaluated two typologically diverse language pairs: Afrikaans\u2013English and Yoruba\u2013English.\nOur main findings are: (i) using topics, keywords and general context words increases coverage; (ii) linguistic-based guidelines increases diversity in the types of sentences, (iii) few-shot prompting increases the quality of sentences but is limited in diversity of the types of sentences;(iv) quality of sentences are much lower for languages that use non-Latin script (such as Yoruba); and (v) evaluating quality of data requires a human-in-the-loop.\nWe provide a framework for linguistically-guided prompting and we conclude that OpenAI\u2019s GPT exhibits the ability to support synthetic code-switched data generation and can be invaluable to address the issue of data availability.\nIn future work we will address the following: i) include external annotation to cross-validate the quality of generated sentences; (ii) improve on the prompting guidelines to increase quality; (iii) use correctable sentences to improve the performance of the latest generation of OpenAI\u2019s GPT to support large-scale generation; and (iv) expand to more African languages in an effort to develop a language agnostic approach to synthetically generate data."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Ethical Considerations",
            "text": "Research in code-switching is not only focused on the grammatical aspects of this phenomenon but also the socio-pragmatic characteristics in discourse (Nel, 2012  ###reference_b18###). Large language models such as OpenAI\u2019s GPT are influenced by social views and inherit encoded biases (Bender et al., 2021  ###reference_b2###). Our work propose the use of GPT to support efforts in synthetically generated code-switched data to increase the prevalence of under-resourced languages. We therefore carefully considered the method in which GPT was prompted to eliminate the introduction of bias. We use general topics and keywords with the goal to generate a diverse range of acceptable sentences.\nThe generated sentences were internally evaluated by native speakers of Afrikaans and Yoruba. We ensure the data is respectful to culture and social norms. We will continue to include humans-in-the-loop to ensure faithful data generation."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Acknowledgements",
            "text": "We thank JP Morgan and ABSA for their financial support, and OpenAI for providing API credits."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.17216v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.4",
            "4.4.1",
            "4.4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "4.1.1",
            "4.1.2",
            "4.2.1",
            "4.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.17216v1",
        "paper_title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
        "research_background": "### Motivation\n\nThe paper is driven by the need to address the challenges associated with code-switching in multilingual communities, particularly in under-resourced languages. Code-switching, the practice of alternating between two or more languages within a single discourse, poses significant challenges for Natural Language Processing (NLP) technologies (Poplack, 2001). With increasing volumes of code-switched content appearing on social media, there is a pressing need to develop systems capable of processing and generating such data to ensure equitable representation and access to NLP technologies for these communities (Solorio, 2021).\n\n### Research Problem\n\nThe core research problem the paper tackles is the scarcity of high-quality, diverse code-switched data necessary for developing robust NLP models. While social media offers a potential goldmine of code-switched data, this data is often informal, inconsistent, and marred by issues such as spelling mistakes, acronyms, and emojis, impacting its utility (\u00c7etino\u011flu et al., 2016; Srivastava et al., 2019). Existing efforts to create synthetic code-switched data address some of these issues but come with their own limitations, such as needing more linguistic diversity and the necessity for native speakers to evaluate the generated data (Yong et al., 2023).\n\n### Relevant Prior Work\n\n1. **Challenges in Code-Switching Research:** \n   - Do\u011fru\u00f6z et al. (2021) highlight challenges related to data availability, evaluation benchmarks, and end-to-end application processing of code-switched data.\n\n2. **Utility of Social Media Data:**\n   - While social media platforms offer a vast repository of code-switched texts, their informal and inconsistent nature limits practical utility (\u00c7etino\u011flu et al., 2016; Srivastava et al., 2019; Winata et al., 2022).\n\n3. **Synthetic Data Generation:**\n   - Methods using parallel corpora and transformer-based models have been developed to generate synthetic code-switched sentences, imposing linguistic constraints to ensure quality (Pratapa et al., 2018; Rizvi et al., 2021; Riktika et al., 2022).\n\n4. **Prompting Large Language Models (LLMs):**\n   - Recent studies have explored using LLMs like GPT to generate code-switched data, specifically examining prompting templates and cautioning against synthetic data use without native speaker involvement (Yong et al., 2023).\n\n### Paper's Contributions\n\nBuilding on the work of Yong et al. (2023), this paper proposes leveraging OpenAI\u2019s GPT model with an enhanced prompting framework facilitated through increased topic diversity and topic-related keywords to mitigate data scarcity issues in under-resourced languages. The paper also emphasizes the importance of native speakers in validating the generated data and proposes a strategy to implement language-agnostic prompting. Additionally, it assesses GPT 3.5's performance with few-shot in-context examples to evaluate its effectiveness in generating large, diverse code-switched datasets.\n\nThe contributions clarify the intended advancement in synthetic data generation methods while addressing diversity and practical applicability in low-resource language settings.",
        "methodology": "### Methodology\n\nOur prompt-based approach to code-switched (CS) text generation is heavily inspired by the work of Yong et al. (2023), who collected synthetic CS data by prompting large language models (LLMs) with requests along various languages and topics. Their focus was on code-switching between English and South-East Asian languages.\n\nIn our case, we focus on two under-explored and under-resourced code-switching scenarios: Afrikaans\u2013English and Yoruba\u2013English. Although Afrikaans and English are typologically dissimilar (van Dulm, 2007), they are both West Germanic languages, suggesting that generating CS text may present fewer challenges. Yoruba, on the other hand, is a tonal language and is even more dissimilar to English, which could introduce additional challenges when creating synthetic CS data.\n\nWe extend the limited topics covered in Yong et al. (2023) and present GPT-3.5 not as an autonomous solution to CS data scarcity, but rather as a potential tool for supporting CS data curation efforts for under-resourced African languages. Specifically, we employ GPT-3.5 for two primary reasons: firstly, to use it as a baseline for comparison with the findings from Yong et al. (2023), and secondly, because the GPT-4 API was unavailable at the time of our experiments111 The API for GPT-4 was made available after we finished the majority of the experiments, https://openai.com/blog/gpt-4-api-general-availability.\n\nKey components of our methodology include:\n\n1. **Language Selection**: We chose Afrikaans\u2013English and Yoruba\u2013English as they represent under-resourced language pairs where code-switching is naturally occurring but lacks extensive data.\n2. **Prompt Design**: Inspired by Yong et al. (2023), we designed specific prompts for GPT-3.5 to generate CS text. These prompts included contextual settings and were modified to cover a broader array of topics relevant to the target languages.\n3. **GPT-3.5 Utilization**: GPT-3.5 was employed for its computational efficiency and because it offers a strong baseline for the generation of CS text. It was also chosen due to the unavailability of the GPT-4 API at the critical stages of our research.\n4. **Evaluative Comparison**: The generation results from GPT-3.5 were evaluated against those reported by Yong et al. (2023) to assess effectiveness and identify any peculiarities stemming from the chosen language pairs.\n\nInnovations introduced in our methodology include extending the topical coverage for CS data generation and intentionally focusing on under-resourced African languages to address data scarcity in CS text generation more robustly.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nIn this section, we evaluate our work in three parts: \n\n1. **Diversity of Generated Sentences**:\n   - **Dataset**: The experiment involves Yoruba\u2013English code-switched sentences. For simplicity and easier evaluation, these sentences are Romanised.\n   - **Baselines**: This part of the evaluation does not reference specific baselines but focuses on assessing the internal diversity of the sentences generated by GPT 3.5 using the provided prompts.\n   - **Metrics**: Diversity is assessed using statistical measures that quantify the variance and uniqueness of the generated outputs.\n\n2. **Adherence to Prompts**:\n   - **Dataset**: Same as above, focusing on Yoruba\u2013English code-switched sentences.\n   - **Baselines**: Similarly, this part does not compare against other models but measures how well GPT 3.5 follows the given prompts.\n   - **Metrics**: Adherence is qualitatively evaluated by examining how closely the generated sentences align with the intended structure and content described in the prompts.\n\n3. **Quality of Generated Sentences**:\n   - **Dataset**: Consistent with the above sections, using Romanised Yoruba\u2013English code-switched sentences.\n   - **Baselines**: No explicit comparative baselines mentioned for sentence quality evaluation.\n   - **Metrics**: Both statistical analysis and human evaluation are employed to assess the quality. Statistical analysis likely includes metrics such as coherence, fluency, and grammatical accuracy, while human evaluation involves subjective scoring by native speakers or language experts.\n\n### Main Experimental Results\n- **Diversity**: The generated sentences exhibit significant diversity, confirming that GPT 3.5 can produce a wide range of varied and unique responses based on the prompts.\n- **Adherence**: GPT 3.5 shows a high level of adherence to the provided prompts, effectively following the intended structures and contents.\n- **Quality**: The combination of statistical and human evaluations indicates that the quality of the generated Yoruba\u2013English code-switched sentences is high. Sentences are coherent, fluent, and generally well-structured, passing the scrutiny of human evaluators proficient in the languages.\n\nThis mixed-method approach to evaluation provides a comprehensive assessment of GPT 3.5's capabilities in generating and handling code-switched language data, specifically for under-resourced languages like Yoruba."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal was to determine whether explicitly including the use of a matrix language in the prompts could ensure adequate representation of low-resourced languages in generated code-switched sentences.",
            "experiment_process": "Six different prompting templates were used as a basis. An explicit inclusion of a matrix language in the prompts was made, with topics such as 'education and training' and 'general conversation' assigned keywords. Example sentences included: 'Ek moet my skills verbeter om 'n beter werksgeleentheid te kry.' (I must improve my skills to get a better job opportunity) and 'Ek sal probeer to finish my assignment op tyd.' (I will try to finish my assignment on time). The resulting sentences were analyzed for adherence to the matrix language.",
            "result_discussion": "GPT 3.5 generated some coherent sentences but did not always follow the prompt regarding the matrix language. A key observation was that 80% of sentences started with 'Ek' (I), indicating a lack of diversity. This highlighted an opportunity to improve sentence diversity by specifying pronouns, which was explored further in the study.",
            "ablation_id": "2404.17216v1.No1"
        },
        {
            "research_objective": "Investigate whether adding basic linguistic guidelines to prompts could enhance the diversity of generated code-switched sentences.",
            "experiment_process": "Word lists containing nouns, verbs, and adjectives related to specific topics were used to add content diversity. Linguistic guidelines such as varying pronouns, tenses, and the inclusion of negative particles were incorporated into the prompts. An example sentence generated was: 'Dit was super lekker om die race te hardloop, but ek ignore die consequences and het te veel ge\u00ebet afterwards.' (It was super nice to run the race, but I ignore the consequences and ate too much afterwards). The diversity of outputs was evaluated, particularly focusing on the starting pronouns and the use of conjunctions.",
            "result_discussion": "Preliminary observations indicated that the approach could generate diverse CS sentences. However, some prompts were not fully adhered to (e.g., conjunctions 'but' and 'and' were in English). The findings suggested that simple linguistic guidelines improved diversity, but complete adherence was still a challenge.",
            "ablation_id": "2404.17216v1.No2"
        },
        {
            "research_objective": "Evaluate the impact of few-shot examples on the generation of code-switched sentences.",
            "experiment_process": "Two additional prompts, Prompt 1.2 and Prompt 2.2, were created by adding five example code-switched sentences to Prompts 1.1 and 2.1 respectively. These examples were not context-specific but aimed at providing GPT 3.5 with a reference to generate new sentences.",
            "result_discussion": "The results in Section 4.1.2 confirmed that adding few-shot examples generally decreased diversity in generated sentences. Specifically, the diversity of Yoruba-English sentences saw some improvement, but 35% still did not start with the requested pronouns. Thus, while few-shot examples provide a baseline, they do not necessarily enhance sentence diversity adequately.",
            "ablation_id": "2404.17216v1.No3"
        }
    ]
}