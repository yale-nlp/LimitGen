{
    "title": "Training LLMs to Better Self-Debug and Explain Code",
    "abstract": "In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks.\nPrior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose a training framework that significantly improves self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks. RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show iterative refinement ability, and can keep refining code continuously.\nLastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Code generation has become a crucial research task to automatically generate source code based on natural language description [1  ###reference_b1###, 2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###]. Although the recent Large Language Models (LLMs) have shown impressive capability in code generation, generating the correct code for a complex problem in single attempt is still challenging [5  ###reference_b5###, 6  ###reference_b6###, 7  ###reference_b7###, 8  ###reference_b8###, 9  ###reference_b9###, 10  ###reference_b10###, 11  ###reference_b11###, 12  ###reference_b12###]. This is expected because even for human developers, completing a hard programming problem might need multiple rounds of trial-and-error debugging. Self-debugging capability that allows LLMs to retrospect the incorrect code and make changes to resolve the implementation errors is becoming increasingly more important besides their code generation ability.\nExisting works [13  ###reference_b13###, 14  ###reference_b14###] investigate off-the-shelf LLMs in the scale of Codex (code-davinci-002) [1  ###reference_b1###], GPT-3.5 and GPT-4, and show that these LLMs are able to self-debug the wrong code they generated via prompting methods in a pipeline of code generation and self-refinement as shown in Figure 1  ###reference_###. The user first queries the LLM for a solution for the given programming task and the initial solution from the LLM is verified by executing them against the given unit tests. If the solution passes all the unit tests, it is considered correct. Otherwise, the user collects the unit test feedback and forms a new query to ask the LLM for a refinement. Such a process can iterate until the LLM generates a correct solution or reaches the maximum number of iterations.\nThere are different prompt designs when asking for refinement [13  ###reference_b13###]. Compared with directly asking for a refined solution (referred to as \u201cCode Refinement\u201d in the feedback block), asking LLMs to provide an explanation of the wrong solution and then refine it in a chain-of-thought manner (referred to as \u201cCode Explanation and Refinement\u201d in the feedback block) helps it to better understand the unit test feedback and increases the success rate of providing refined solutions (details in Appendix A.1  ###reference_###).\n###figure_1### However, how to improve LLMs\u2019 self-debugging capability remains under-explored, especially given the fact that open-sourced LLMs such as StarCoder [11  ###reference_b11###] and CodeLlama [12  ###reference_b12###] have limited self-refinement performance. For example, the StarCoder-15B model is only able to refine 4.43% wrong solutions for problems from the MBPP benchmark [3  ###reference_b3###], in contrast, GPT-3.5-Turbo can refine 28.90% under the same setting (details in Appendix A.1  ###reference_###). Such limited self-refinement ability motivates the need to better train LLMs to take feedback to explain and self-refine the wrong code. Although important, an essential challenge of training LLMs to explain and refine wrong code is the lack of training data, especially high-quality code explanation data. Previous work has explored Imitation learning from Language Feedback (ILF) [15  ###reference_b15###], which trains LLMs with human-annotated explanation, yet, such an approach is not scalable and the LLMs also do not obtain the ability to explain code.\nIn this work, we propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanation and refinement trajectories and filtering via execution verification.\nSupervised fine-tuning (SFT) on the high-quality demonstration data boosts LLMs\u2019 wrong code explanation and code refinement ability with a significant margin compared to existing prompting approaches. Furthermore, we propose reinforcement learning (RL) with a novel reward design that considers code explanation semantics and unit test passing rate, enabling LLMs to generate more useful code explanations and correct refinements. To sum up, this work makes the following contributions:\nWe propose a scalable framework consisting of automated data collection, data verification, supervised fine-tuning, and reinforcement learning with new reward designs, to improve LLMs\u2019 self-debugging ability and generate more helpful code explanations and correct refinements.\nWe perform supervised fine-tuning on three LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B) with the collected code refinement and wrong code explanation data. Our extensive experiments demonstrate that it significantly improves their wrong code explanation and code refinement ability, with the pass@1 improved by up to 15.92% and the pass@10 improved by up to 9.30% on four benchmarks.\nWe train the fine-tuned LLMs using a reinforcement learning algorithm with a novel reward design considering unit test execution and wrong code explanation semantics, and further improve the pass@1 by up to 3.54% and pass@10 by up to 2.55% over SFT on four benchmarks.\nWe conduct comprehensive ablation and analytical experiments as well as a human evaluation to show the generalizability and effectiveness of our approach."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "In this section, we present our\nproposed automated pipeline, which includes the collection of high-quality code explanation and refinement data, and the training methods.\nFigure 2  ###reference_### shows the overview of our approach.\nWe first collect a code explanation and refinement dataset by querying from pretrained or instruct models and verifying its responses with execution feedback to filter and obtain high-quality explanation and refinement data (steps 1 and 2 in Figure 2  ###reference_###, Section 2.1  ###reference_###). Then the high-quality dataset is used for supervised fine-tuning (step 3 in Figure 2  ###reference_###, Section 2.2  ###reference_###), which significantly improves the model\u2019s performance in explaining the bug and refining the code. Reinforcement learning with execution feedback is used to further guide the model to generate higher quality response and boost the model performance (step 4 in Figure 2  ###reference_###, Section 2.3  ###reference_###).\n###figure_2###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Data collection and verification",
            "text": "We use MBPP [3  ###reference_b3###] (only use the 374 problems in the training set during training), APPS [4  ###reference_b4###] (only use the 5,000 problems in the training set) and CodeContests [2  ###reference_b2###] as our base training datasets, which contain programming problems and solutions collected from various platforms. While they are helpful for training LLMs for code generation, they neither contain enough wrong solutions nor the explanation and refinement of them. To collect more wrong solutions, we prompt the pre-trained LLMs (i.e., StarCoder and CodeLlama) accordingly with 3-shot examples to sample 20 solutions (temperature set to 1.0) per problem from MBPP\u2019s training set, APPS\u2019s training set, and CodeContests. We then run these generations against test cases to select the wrong solutions that fail any test cases.\nTable 1  ###reference_### shows the number of correct (passing all the unit tests) and wrong (failing any unit test) solutions sampled for each dataset. For each wrong solution, we need an explanation of the wrong code and a correct refinement to build the code explanation and refinement dataset. We prompt pre-trained or instruction-LLMs with the problem description, wrong solution, and execution feedback (either error message or failed test case) to ask for an explanation and refinement. In this paper, we experimented with GPT-3.5-Turbo and CodeLlama-34B. We take GPT-3.5-Turbo as the example LLM in this section, and an example with it is shown in Appendix A.2  ###reference_###. We study the generalization of this data collection with different LLMs in Section 4.3  ###reference_###.\nAs LLMs may provide wrong explanations or refinements, we cannot blindly take them as training data. Thus, we verify the refinements by running them against the test cases again, and only those passing all the test cases are considered correct refinements. For explanation, we consider the explanations along with the correct refinements as correct. Overall, for example with GPT-3.5-Turbo, we get 13,735 correct explanations and refinements: 2,203 for MBPP, 6,419 for APPS, and 5,113 for CodeContests. This verification step is crucial to guarantee the quality of the automatically collected code explanation and refinement dataset."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Supervised fine-tuning",
            "text": "We form the fine-tuning data in an instruction-following format similar to StarChat [16  ###reference_b16###], where the user input is enclosed by <|user|> and <|end|>, while LLM\u2019s answer is enclosed by <|assistant|> and <|end|> in the chat history. Moreover, to alleviate the limited amount of data, we augment the fine-tuning data by using two different instructions: providing\nthe task description, the initial wrong code, and execution feedback, asking for (1) a refinement directly, or (2) an explanation of the wrong code and then a refinement in a chain-of-thought manner. Examples are given in Appendix A.2  ###reference_###\nDuring supervised fine-tuning, although we include the wrong solutions as LLM\u2019s initial answer in the chat history, we do not calculate the loss for this part since we do not want the LLM to intentionally generate those wrong solutions. They are just provided as context for code explanation and refinement if the LLM indeed makes mistakes in real use cases."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Reinforcement learning",
            "text": "Reinforcement learning is widely used to further improve the quality of LLM\u2019s generated outputs [17  ###reference_b17###, 18  ###reference_b18###, 19  ###reference_b19###, 20  ###reference_b20###]. Through the RL framework, the LLM is optimized by using an algorithm to update the weights using both success and failure trajectories and maximize the rewards of its outputs. To train the fine-tuned LLMs to generate better code explanations and more correct code refinements, we design the rewards considering both parts."
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Refinement score",
            "text": "To train LLM to refine code, the correctness of the refinement is the main goal, which can be measured by its code similarity to the ground truth, as well as the execution result. We use CodeBLEU score as metrics for code similarity and unit test passing rate as metrics for execution results.\nGiven a wrong solution , the set of correct and wrong (failed) refinements are notated by  and . For any refinement , we calculate its CodeBLEU score and the unit test passing rate as follows:\nis the average CodeBLEU score between a given refinement and all the correct refinements.  is the fraction of the number of passed unit test cases () when running the refined code , over the total number of unit test cases () provided for this problem in the dataset.\nIn Figure 3  ###reference_###, the x-axis is the scores of certain metrics, and the y-axis is the number of training data with a certain score (same for other figures in Figure 3  ###reference_###). Thus, Figure 3  ###reference_### (a) shows the frequency distribution of each score of , with blue bars referring to training data with correct refinements, and orange bars referring to that with wrong refinements. The distribution of  is shown in Figure 3  ###reference_### (b) where the correct refinements definitely pass all the test cases and can be separated from the wrong ones.\n###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Explanation score",
            "text": "In our dataset, there are wrong code explanations along with code refinement, whose quality may not be perfectly reflected by the code quality. A correct code explanation may also be followed by incorrect refinement, thus, it is necessary to consider the explanation in the reward. The code explanations followed by correct refinements are treated as ground truth, notated by . We calculate the average sentiment similarity [21  ###reference_b21###, 22  ###reference_b22###] between the explanation embedding  and corresponding embeddings in ground truth as\nThe distribution of  is shown in Figure 3  ###reference_### (c)."
        },
        {
            "section_id": "2.3.3",
            "parent_section_id": "2.3",
            "section_name": "2.3.3 Reward design",
            "text": "Given a pair of explanation and refinement, i.e., , the reward of the generated code refinement and the code explanation is designed as:\nThis code reward  is the average of CodeBLEU score and the unit test passing rate, and since both  and  are scored in the range of [0, 1], this equation makes the reward of code refinement in the range of [-5, 5]. Figure 3  ###reference_### (d) shows the distribution of the code refinement reward on the training dataset, which mitigates the overlap issue between correct and wrong outputs with CodeBLEU score alone as illustrated in Figure 3  ###reference_### (a). It also makes the reward distribution continuous, addressing the discreteness problem of only using unit test passing rate.\nFor the design of explanation reward , We observe from Figure 3  ###reference_### (c) that the explanation sentiment similarities of the training data mostly lie in the range of [0.4, 1.0], thus, we project the range of [0.4, 1.0] to [-5, 5] and treat 0.7 as the borderline (projected to 0 correspondingly) of good or bad explanations. Figure 3  ###reference_### (e) shows the distribution of the wrong code explanation reward on the training dataset. The distribution shows that there could be a good or correct code explanation followed by a wrong code refinement, where assigning a high or low reward to the entire output is not reasonable. This leads to our PPO [23  ###reference_b23###] algorithm with code refinement and explanation rewards considered separately. Due to space limit, the PPO algorithm is shown in Appendix A.3  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental setup",
            "text": "For supervised fine-tuning, we fine-tune three LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B) using the correct initial solutions and correct refinements collected from the MBPP training set, APPS training set, and CodeContests. The model is fine-tuned for two epochs, using a batch size of 128. The optimizer is AdamW [24  ###reference_b24###] with learning rate set to . The learning rate is adjusted using a warmup of 500 steps and then decayed following a cosine scheduler.\nWe further train supervised fine-tuned LLMs with reinforcement learning using the PPO algorithm. The reinforcement learning training data is all the initial solutions and collected refinement on the MBPP and APPS training set. The learning rate is , and the batch size is set to 64. We implement reinforcement learning training based on the TRL [25  ###reference_b25###] library. Both the supervised fine-tuning and reinforcement learning are conducted on 8 NVIDIA A100 GPUs, each with 40GB of memory."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Result",
            "text": "To evaluate the effectiveness of our training approach, we study the following research questions (RQs) regarding code generation and code refinement capability, iterative refinement ability, approach generalizability, and the quality of the generated code explanations."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "RQ1: Code generation and refinement capability",
            "text": "We evaluate the models trained with our approach for their code explanation and refinement ability using four benchmarks: MBPP [3  ###reference_b3###], HumanEval [1  ###reference_b1###], MBPP+ [26  ###reference_b26###], and HumanEval+ [26  ###reference_b26###]. We use pass@k [1  ###reference_b1###] and success refinement rate as the evaluation metric. For the generation of the initial solutions, the models sample 100 solutions per task in the benchmarks (temperature set to 0.8), which are run against the provided test cases. For every incorrect solution that fails any test case, we let the models sample one refinement (and one explanation)."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Pass@k",
            "text": "Table 2  ###reference_### shows the pass@k result on four benchmarks separately. In general, fine-tuning LLMs with our collected code explanation and refinement data significantly improves the pass@1 and pass@10 of all three backbones. For both StarCoder-15B and CodeLlama-13B, RL achieves the best pass@1 and pass@10 (bolded) on all four benchmarks. For CodeLlama-7B, RL achieves the best performance under seven settings except that SFT gets the highest pass@1 on the MBPP benchmark.\nFor simpler comparison, Table 3  ###reference_### shows the overall pass@k on MBPP and HumanEval, as well as the improvement brought by SFT and RL. The improvement by SFT is compared to prompting, and the improvement by RL is compared to SFT.\nOn the overall of MBPP and HumanEval, SFT improves StarCoder-15B by a 15.34% higher pass@1 and a 6.58% higher pass@10 after direct refinement. When asking for code explanation and refinement in the chain of thought, the SFT StarCoder-15B obtains a 15.92% and 6.30% improvement on pass@1 and pass@10. RL training improves SFT by another 2.43% pass@1 and 2.55% pass@10 when directly generating refinement, and improves by another 1.87% pass@1 and 1.85% pass@10 when generating code explanation and refinement. Similar improvement by SFT and RL can also be observed on CodeLlama-7B and 13B backbones.\nOn the overall of MBPP+ and HumanEval+ benchmarks, where each problem has 35 and 80 more rigorous test cases [26  ###reference_b26###], we observe larger improvements from RL training on CodeLlama backbones. CodeLlama-7B achieves a 1.79% higher pass@1 and a 2.60% higher pass@10 on the refined solutions with code explanation. CodeLlama-13B achieves a 3.54% higher pass@1 and a 1.29% higher pass@10 on the directly generated refinements. This shows that RL training enables LLMs to generate or refine more robust solutions that can pass rigorous test cases. More experiments and examples of case studies can be found in Appendix A.4.1  ###reference_.SSS1###,  A.5.1  ###reference_.SSS1###,  A.5.2  ###reference_.SSS2###,  A.5.3  ###reference_.SSS3###."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Success refinement rate",
            "text": "Table 4  ###reference_### lists the success refinement rate generated by each backbone using different approaches. The result is averaged over four benchmarks. For StarCoder-15B, prompting is only able to correctly refine 6.41 \u2013 6.90% of the wrong initial solutions. After being fine-tuned with the collected code explanation and refinement data, the supervised fine-tuned StarCoder-15B refines 16.27 \u2013 16.56% of the wrong solutions with a significant improvement of 9.37 \u2013 10.15%. RL training further increases the refinement rate by 1.03 \u2013 1.23% over SFT.\nThe improvement on CodeLlama-7B and 13B backbones is consistent with that on StarCoder-15B, where RL training eventually achieves the highest success refinement rate with a considerable boost of 1.81 \u2013 3.62%."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "RQ2: Iterative refinement ability",
            "text": "LLMs can self-debug iteratively until they find the correct solutions. Figure 4  ###reference_### shows the overall pass@k of CodeLlama-7B on four benchmarks after at most three rounds of refinements. To simplify the figure, we plot the higher pass@k from \"Refine\" and \"Expl. + Refine\" in each round of refinement of each LLM. More results of iterative refinement are in Appendix A.4.2  ###reference_.SSS2###.\n###figure_8### Both SFT and RL consistently outperform prompting in all three rounds of refinement. Even with three rounds of refinements, the prompting approach cannot achieve the same pass@k as SFT (e.g., 47.53% vs. 56.75% in Figure 4  ###reference_### (a)) after the first round of refinement. These results show that LLMs trained with our pipeline have iterative refinement ability and can achieve increasingly higher pass@k with more iterations."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "RQ3: Generalizability of approach",
            "text": "To show the generalizability of our framework, i.e., especially that the data collection is independent of GPT-3.5-Turbo, we use CodeLlama-34B as an alternative to GPT-3.5-Turbo for data collection. Since CodeLlama-34B is a pre-trained model, we use few-shot examples in the prompt to make it generate the desired wrong code explanation and refinement. The rest remains the same.\nTable 5  ###reference_### shows the pass@k results of CodeLlama-7B trained with data collected from CodeLlama-34B and Table 6  ###reference_### shows the overall for simpler comparison. SFT obtains smaller (around 1%-3% less compared to training with GPT-3.5-Turbo\u2019s data) but still notable improvements on the overall pass@1 and pass@10. Furthermore, we observe the same finding that RL training improves more of the performance on MBPP+ and HumanEval+ by 5.28% of pass@1 and 2.24% of pass10. This shows the generalizability of our pipeline and also suggests that collecting data from a stronger LLM benefits the training more in our pipeline. More results are shown in Appendix A.4.3  ###reference_.SSS3###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "RQ4: Quality of generated explanation",
            "text": "We evaluate if the explanation for wrong code is useful for developers to understand their bugs. We randomly sample 50 problems with initial wrong solutions from the MBPP and HumanEval benchmarks and query different LLMs to generate explanations for the wrong code. We score each code explanation on a scale of 1 \u2013 5 considering their correctness and helpfulness, with 1 being totally incorrect or misleading, and 5 being not only correct but also providing a detailed hint of how to fix the wrong code.\nWe perform scoring using both GPT-4 and human developers as judges. When using GPT-4 as a rater, we follow previous work [27  ###reference_b27###] to prompt it to score each explanation, and the result is shown in Table 7  ###reference_###. Both SFT and RL improve the quality of explanation over the prompting approach. Overall, GPT-4 gives higher scores to the LLMs trained with our approach than the prompting approach. Compared with GPT-3.5-Turbo explanations, the gap is significantly reduced after the training.\nDue to the effort required by human rating, we only let developers rate the StarCoder models\u2019 and GPT-3.5-Turbo\u2019s generations and each explanation is rated as the average of scores given by two developers. The human rating is consistent with GPT-4 indicating that SFT improves the explanation quality over prompting and RL training takes code explanation semantics into consideration of rewards, thus bringing continuous improvement to the explanation generation. Rubrics and examples of human rating are shown in Appendix A.6  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitation",
            "text": "One potential limitation of our study is the reliance on specific large language models (LLMs) from which we collect code explanation and refinement data. Our automated framework is designed to be independent of any specific LLM, and for this study, we use GPT-3.5 and CodeLlama-34B to collect training data, and both bring significant improvement through SFT and RL. However, for future work, it would be interesting to explore the use of other LLMs, including smaller models or a mix of diverse LLMs, to gather explanation and refinement data.\nAdditionally, our current experiments only use two types of prompts for enabling LLMs to self-debug: one that directly asks for refinement and another that first asks for an explanation of the wrong code followed by refinement. While these prompt designs have shown effectiveness, there might be better prompt strategies for self-debugging that we have not explored due to resource constraints. Exploring a broader range of prompt designs could potentially enhance the performance of our framework. Nonetheless, our proposed training framework is flexible and should be generalizable to different types of data and prompts, paving the way for future innovations in this area."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": ""
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Self-Debugging",
            "text": "Existing techniques have studied the possibility of using LLMs to refine their generations. Yet, most techniques are prompting LLMs with execution results [13  ###reference_b13###, 14  ###reference_b14###, 42  ###reference_b42###, 43  ###reference_b43###, 44  ###reference_b44###, 45  ###reference_b45###, 46  ###reference_b46###, 47  ###reference_b47###] for the refinement. Such prompting approaches bring limited improvement to smaller open-sourced LLMs compared to GPT-3.5. Other techniques train LLMs to self-debug. ILF [48  ###reference_b48###] uses human-annotated feedback information and thus is unscalable, CYCLE [49  ###reference_b49###] and Self-Edit [50  ###reference_b50###] use SFT to fine-tune LLM to generate the refinement only based on the unit test execution feedback.\nThis work has three differences compared with others that train LLMs: (1) we train LLMs to generate code explanation followed by refinement, which provides additional information to users, (2) we do not require human-annotated training data but propose a scalable pipeline to automatically collect and verify data from another LLM, (3) we design novel reward functions in the RL training stage, considering both the code and explanation quality, which beings extra improvement."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This work shows the necessity of training open-sourced LLMs to self-debug and proposes a scalable framework consisting of automated data collection, data verification, supervised fine-tuning, and reinforcement learning with new reward designs, to improve LLMs\u2019 self-debugging ability.\nThe data collection is independent of LLMs. Our experiments using either GPT-3.5-Turbo or CodeLlama-34B achieve great improvements. The data verification ensures the high quality of the collected code explanation and refinement data. SFT on our collected data brings a significant boost to LLMs\u2019 self-debugging ability, resulting in up to 15.92% higher pass@1, 9.30% higher pass@10, and 10.15% higher success refinement rate. RL training using our novel reward design further brings up to 3.54% higher pass@1, 2.55% higher pass@10, and 3.62% higher success refinement rate. Comprehensive analytical experiments validate the generalizability of our approach and illustrate the iterative refinement ability of the trained LLMs. The human evaluation indicates that the explanations generated by the LLMs trained with our approach are of higher quality and can effectively serve developers for debugging purposes."
        }
    ],
    "url": "http://arxiv.org/html/2405.18649v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6",
            "6.1",
            "6.2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.3.1",
            "2.3.2",
            "2.3.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.18649v1",
        "paper_title": "Training LLMs to Better Self-Debug and Explain Code",
        "research_background": "The motivation behind this paper stems from the ongoing challenge in code generation tasks: generating accurate code for complex problems often requires multiple attempts and inherently involves debugging. While recent Large Language Models (LLMs) have shown impressive capabilities, their ability to self-debug\u2014to analyze and correct erroneous code they generate\u2014remains limited. This limitation highlights the necessity for enhanced self-debugging capabilities, particularly in open-sourced models like StarCoder and CodeLlama, which have shown suboptimal performance in this regard.\n\nThe core research problem addressed in this paper is improving LLM's self-debugging capability to not only generate correct code but also effectively explain and refine incorrect solutions. Existing approaches have shown some effectiveness in prompting LLMs to refine code solutions by providing explanations, yet these methods fall short due to a lack of high-quality training data, particularly for code explanations.\n\nTo bridge this gap, the authors propose a novel automated pipeline for collecting high-quality data on code explanation and refinement. This pipeline utilizes execution verification to filter the generated data, ensuring its quality. Further, they bolster this data by employing supervised fine-tuning (SFT) on three specific LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B), showing significant improvements in their ability to explain and refine incorrect code solutions.\n\nRelevant prior work includes:\n1. Studies that have investigated the code generation capabilities of LLMs (e.g., Codex, GPT-3.5, GPT-4) and their ability to self-debug through prompting methods.\n2. Research on different prompt designs, showing that requesting explanations and refinements in a chain-of-thought manner enhances the LLM's ability to provide improved code solutions.\n3. Imitation learning from Language Feedback (ILF), which utilizes human-annotated explanations to train LLMs, but faces scalability issues and does not inherently teach the models to generate explanations autonomously.\n\nIn summary, this paper aims to provide a scalable and effective framework to enhance LLMs' self-debugging abilities by leveraging an automated data collection and filtering pipeline, coupled with supervised fine-tuning and reinforcement learning with novel reward designs. This approach addresses the critical need for high-quality training data and more effective training methodologies, leading to significant advancements in LLMs' capability to explain and refine their own code.",
        "methodology": "**Training LLMs to Better Self-Debug and Explain Code**\n\n**Methodology:** In this section, we present our proposed automated pipeline, which includes the collection of high-quality code explanation and refinement data, and the training methods.\n\n1. **Data Collection:** We first collect a code explanation and refinement dataset by querying from pretrained or instruct models and verifying its responses with execution feedback to filter and obtain high-quality explanation and refinement data.\n\n2. **Supervised Fine-Tuning:** Then the high-quality dataset is used for supervised fine-tuning, which significantly improves the model\u2019s performance in explaining the bug and refining the code.\n\n3. **Reinforcement Learning:** Reinforcement learning with execution feedback is used to further guide the model to generate higher quality response and boost the model performance.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Fine-Tuning Details:**\n- **Model:**\n  - StarCoder-15B\n  - CodeLlama-7B\n  - CodeLlama-13B\n\n- **Datasets:**\n  - MBPP training set\n  - APPS training set\n  - CodeContests\n\n- **Training:**\n  - **Supervised Fine-Tuning:**\n    - Epochs: 2\n    - Batch size: 128\n    - Optimizer: AdamW\n    - Learning rate schedule: Warmup of 500 steps, followed by a cosine decay\n  - **Reinforcement Learning (PPO Algorithm):**\n    - Learning rate: \n    - Batch size: 64\n    - Implemented using the TRL library\n\n- **Hardware:**\n  - 8 NVIDIA A100 GPUs, each with 40GB of memory\n\n**Main Experimental Results:**\nThe results of the main experiment should focus on performance improvements observed in the specified models after undergoing the described training processes. This typically includes measurements of accuracy, robustness, efficiency, and other relevant metrics on validation or test sets from the MBPP, APPS, and CodeContests datasets. However, since these specific results (numerical values or comparative performance) are not provided in the experiment setup description, a general summary might include observations on the enhancement in code debugging, explanation capabilities, and other tasks relevant to Large Language Models (LLMs). If more details (such as exact improvement metrics or comparative baseline results) were provided, they would be highlighted here."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the effect of supervised fine-tuning (SFT) and reinforcement learning (RL) on LLMs' code explanation and refinement abilities.",
            "experiment_process": "The experiment involves fine-tuning three LLMs (StarCoder-15B, CodeLlama-7B, and CodeLlama-13B) on a dataset collected through an automated pipeline for code explanation and refinement. The LLMs were evaluated using the pass@k metric across four benchmarks. Both direct refinement and chain-of-thought refinement involving code explanations were considered. RL was further applied with a novel reward design based on code explanation semantics and test passing rates.",
            "result_discussion": "SFT improved pass@1 by up to 15.92% and pass@10 by 9.30%. RL contributed additional improvements, with up to 3.54% increase in pass@1 and 2.55% in pass@10. The results indicate that fine-tuning significantly boosts code explanation and refinement capabilities, and RL enhances the ability to generate robust solutions.",
            "ablation_id": "2405.18649v1.No1"
        },
        {
            "research_objective": "Evaluate the success refinement rate of LLMs after fine-tuning and RL training.",
            "experiment_process": "The success refinement rate was averaged over four benchmarks, with comparison between prompting, supervised fine-tuning (SFT), and reinforcement learning (RL) approaches. The performance of StarCoder-15B, CodeLlama-7B, and CodeLlama-13B was assessed.",
            "result_discussion": "Prompting showed a success refinement rate of 6.41-6.90%. Post SFT, StarCoder-15B improved to 16.27-16.56%. RL provided an additional improvement of 1.03-1.23%. CodeLlama-7B and 13B showed similar trends, with RL achieving the highest success refinement rates with boosts of 1.81-3.62%.",
            "ablation_id": "2405.18649v1.No2"
        },
        {
            "research_objective": "Assess the iterative refinement ability of LLMs.",
            "experiment_process": "The pass@k metric was tracked over three rounds of refinements for CodeLlama-7B on four benchmarks, comparing prompting, SFT, and RL approaches. Higher pass@k scores from direct refinement and chain-of-thought refinements were plotted.",
            "result_discussion": "Both SFT and RL outperformed prompting consistently across all three rounds. Even after three rounds, prompting could not achieve the pass@k scores achieved by SFT after one round, indicating the iterative refinement capability of SFT and RL trained models.",
            "ablation_id": "2405.18649v1.No3"
        },
        {
            "research_objective": "Test the generalizability of the data collection and training framework.",
            "experiment_process": "CodeLlama-34B was used instead of GPT-3.5-Turbo to collect data for fine-tuning CodeLlama-7B. The performance was evaluated using pass@k metrics on MBPP and HumanEval benchmarks, comparing results obtained using data from different models.",
            "result_discussion": "SFT with data from CodeLlama-34B showed smaller improvements compared to GPT-3.5-Turbo data (around 1%-3% less), but still notable gains were observed. RL training continued to show enhanced performance, particularly on rigorous MBPP+ and HumanEval+ benchmarks, indicating the framework's generalizability.",
            "ablation_id": "2405.18649v1.No4"
        },
        {
            "research_objective": "Examine the quality of generated code explanations from trained LLMs.",
            "experiment_process": "Code explanations were generated for 50 problems with initial wrong solutions from MBPP and HumanEval benchmarks. The explanations were rated on a scale of 1-5 by both GPT-4 and human developers, considering correctness and helpfulness.",
            "result_discussion": "Both SFT and RL approaches improved the quality of explanations over prompting, as confirmed by higher scores from GPT-4 and human developers. This increase in quality shows that SFT and RL enhance the usefulness and accuracy of the generated explanations.",
            "ablation_id": "2405.18649v1.No5"
        }
    ]
}