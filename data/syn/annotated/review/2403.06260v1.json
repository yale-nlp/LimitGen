{
    "title": "SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations",
    "abstract": "There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based speech models to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data. This work presents a cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for content-related tasks. The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed speech and original speech. Commonly used\ndata augmentation techniques for content-related tasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of fine-tuning ( 5 hrs) on a single GPU for automatic speech recognition, phoneme recognition, and query-by-example tasks, with relative improvements of 1.09%, 3.58%, and 12.65%, respectively. SCORE provides competitive results with the recently proposed SSFT method SPIN, using only 1/3 of the processed speech compared to SPIN.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Self-supervised learning (SSL) based pre-trained speech models such as HuBERT [1  ###reference_b1###], WavLM [2  ###reference_b2###] are becoming popular for their state-of-the-art performance on almost all speech applications. These models extract latent features that capture underlying factors of speech, such as acoustic-phonetic information, speaker information, semantic information, and more [3  ###reference_b3###]. These pre-trained representations are then fine-tuned for downstream application with labelled data. However, pre-trained SSL speech models may not be ideal for downstream tasks that do not align with the pre-trained objective (for example, handling overlapping speech [2  ###reference_b2###]). One way to overcome this issue is to introduce a pre-training objective that relates to the downstream task, such as training with overlapping speech in WavLM [2  ###reference_b2###]. However, this approach requires substantial amount of compute cost as the model is pre-trained from scratch. Another alternative falls within the realm of unsupervised or self-supervised fine-tuning (SSFT)[4  ###reference_b4###]. SSFT is applied on top of pre-trained models to learn task-specific representations. Then the SSL models are fine-tuned with labelled data on the downstream tasks for robust performance. For example, ContentVec [5  ###reference_b5###] employs content preserving strategies (by\ndisentangling speakers) on top of pre-trained HuBERT model to learn content-specific representations. However, ContentVec is not very cost-effective as it requires 19 hrs on 36 GPUs on top of the pre-trained HuBERT [1  ###reference_b1###] model. Another recent SSFT approach for content-related downstream task is speaker-invariant clustering (SPIN) [4  ###reference_b4###], which requires a compute cost less than 1% of ContentVec. SPIN employs speaker invariant clustering to improve content representations. The term SSFT was proposed in [4  ###reference_b4###] to distinguish fine-tuning methods using only audio [5  ###reference_b5###, 6  ###reference_b6###] from supervised fine-tuning using labelled data [7  ###reference_b7###].\n###figure_1### In this work, a simple and cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning is proposed to preserve content. Correspondence training [8  ###reference_b8###] is the task of learning similar representations from two different instances of the same spoken content. This technique has been successfully applied to extract high quality acoustic word embeddings (AWEs), where an auto-encoder takes input as a spoken word and the target output as the same word spoken by a different speaker [8  ###reference_b8###, 9  ###reference_b9###]. This technique ensures that the encoder learns only content and forget other unnecessary information such as speaker, duration, prosody, etc. Taking inspiration from this, for SCORE fine-tuning, a perturbed speech is generated from the original speech in such a way that the spoken content is preserved. Perturbed speech utterances are generated through the application of commonly employed data augmentation techniques in automatic speech recognition, such as speed perturbation [10  ###reference_b10###] and pitch shifting. After obtaining perturbed speech, the objective is to learn similar speech representations from both the original speech and perturbed speech, making the representations pitch and duration invariant. Shifting pitch (fundamental frequency) alters the speaker information while keeping the content same. To match the representations from perturbed and original speech, soft-DTW [11  ###reference_b11###, 12  ###reference_b12###] is used as a loss function. Soft-DTW is a popular loss function for time-series data, and it has also been successfully used for the multi-pitch estimation task in music information retrieval [13  ###reference_b13###].\nThe proposed method is tested on three content-related downstream tasks on SUPERB benchmark [14  ###reference_b14###]: automatic speech recognition (ASR), phoneme recognition (PR), and query-by-example spoken term discovery (QbE). The results are compared against the performance of vanilla models (HuBERT and WavLM) and recently proposed content-preserving SSFT methods such as ContentVec and SPIN for SUPERB benchmark .\nThe main contributions of this work are as follows:\nA novel cost-effective self-supervised fine-tuning method named SCORE is proposed to improve the content representations.\nWith just less than 5 hours of SCORE fine-tuning on a single V100 GPU, SCORE fine-tuned models outperform vanilla HuBERT and WavLM on the SUPERB benchmark for content-related tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Fig. 1  ###reference_### demonstrates the proposed SCORE fine-tuning method. SCORE involves two instances of the pre-trained model, one with frozen parameters () and the other with learnable top layers (), both having same initial model weights. Top layers are chosen for fine-tuning, as they encode phonetic content for most of the SSL models [15  ###reference_b15###, 16  ###reference_b16###]. More implementation details are described in Sec. 3  ###reference_###. The input to the SCORE is a pair of perturbed speech and original speech, randomly fed to either  or , as shown in Fig. 1  ###reference_###. Randomizing the input ensures that the model  does not exclusively focus on the characteristics of perturbed speech, and found to be crucial to observe the benefits of the proposed method. To obtain the perturbed speech, data augmentations used in ASR [10  ###reference_b10###] are employed, such as speech perturbation and pitch shift. Torchaudio [17  ###reference_b17###] is used for these perturbations, with SpeedPerturbation and PitchShift functions under torchaudio.transforms111https://pytorch.org/audio/stable/transforms.html  ###reference_s.html###.\nThe obtained representations from both the models  and  are projected to a lower dimension with linear feedforward layers and L2-normalized. Obtained sequences from both models ( and ) are different in lengths due to the perturbations. Therefore, a dynamic time warping based differentiable loss function soft-Dynamic Time Warping (soft-DTW) 222https://github.com/Maghoumi/pytorch-softdtw-cuda  ###reference_w-cuda###[11  ###reference_b11###, 18  ###reference_b18###, 19  ###reference_b19###] is used to match sequences of unequal lengths (Eq. 1  ###reference_###). This learning framework ensures that the model learns the speed and pitch invariant representations for the same spoken content.\nThe soft-DTW replaces the \u201cmin\u201d operation in the DTW with \u201csoft-min\u201d operation. Soft-DTW computes the\nsoft-minimum of all alignment cost. The soft-DTW for two sequences  and\n is defined [19  ###reference_b19###] as follows:\nwhere  is the set of all possible paths. The  is the soft-min operator with a smoothing\nfactor  and  is the distance function.\nThe soft-min operator  is defined as:\nIn this work,  is used as a loss function for SCORE fine-tuning as described in Eq. 3  ###reference_###.\nIn all the experiments, we use a smoothing factor  of 0.1. However, to address potential negative values in  loss, a normalized version described in Eq. 4  ###reference_### is employed. This normalization guarantees a minimum loss value of zero for identical sequences, i.e., , and ensures  for any pair of sequences. This approach guarantees a consistently positive loss [20  ###reference_b20###, 21  ###reference_b21###]. Further, the loss from Eq. 4  ###reference_### is normalized by dividing it with the total sequence length . Algorithm 1  ###reference_### describes the entire SCORE fine-tuning method.\nThe reported numbers are from their respective papers and SUPERB benchmark leaderboard [14  ###reference_b14###] as of 13/09/2023 (https://superbbenchmark.org/leaderboard  ###reference_###).\nOur results when we run the SUPERB [14  ###reference_b14###] baseline scripts for HuBERT and WavLM for fair comparison."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Experiments are conducted on two SSL speech models: HuBERT and WavLM (BASE versions). These SSL models are fine-tuned with the SCORE method. After the SCORE fine-tuning, obtained models are used for supervised training for the content-related downstream tasks on the SUPERB benchmark.\nSimilar to SPIN [4  ###reference_b4###], the top 2 layers (11th and 12th) of the SSL models are fine-tuned as it is cost-effective (M trainable parameters) and most of the SSL models encode phonetic content in top layers [15  ###reference_b15###, 16  ###reference_b16###]. In this study, Wav2vec2 [7  ###reference_b7###] is omitted due to the fact that the linguistic content is less well represented\nin the final few layers [16  ###reference_b16###], which is crucial for content-related tasks. Fine-tuning the entire model, from bottom layers to top layers, would result in increased computational expenses, contradicting the study\u2019s intended objectives. Furthermore, there is a concern that when the entire model is fine-tuned, the fine-tuning objective could potentially lead to a collapse of the original representations [22  ###reference_b22###] learned during pre-training.\nThe details about the data, SCORE fine-tuning, and evaluation on the SUPERB benchmark are described as following:\nData: In line with prior research [4  ###reference_b4###] and to ensure a fair comparison, experiments are performed on LibriSpeech\u2019s [23  ###reference_b23###] train-clean-100 hours of data for SCORE fine-tuning. Consistent with earlier discoveries [4  ###reference_b4###], training more layers or additional data does not enhance results.\nSCORE Fine-tuning Details:\nThe representations obtained from the final Transformer layer (12th) of the models  and  are sequences of 768-dimensional vectors. These vectors are projected into 256-dimensional vectors with linear projection layers and then L2-normalized. The SCORE fine-tuning method is trained for 3.6k updates ( 1 epoch with effective batch size of 8). The model converged in just one epoch, and additional training did not yield any improvements. AdamW [24  ###reference_b24###] optimizer is used with a learning rate of  with 1k warm-up updates. One epoch roughly takes  5 hours on V100 GPU. More details are available at GitHub333https://github.com/Trikaldarshi/SCORE_Finetuning  ###reference_tuning###.\nSUPERB Benchmark: S3PRL toolkit 444https://github.com/s3prl/s3prl  ###reference_github.com/s3prl/s3prl### is used for all the SUPERB benchmark tasks. For ASR and PR, features from all the layers are aggregated with learnable weights. These aggregated features are then fed to the prediction head for each downstream task and fine-tuned with labelled data. For ASR, the prediction head consists of 2-layer 1024-unit Bi-LSTM network with CTC loss on characters [14  ###reference_b14###]. The ASR model is evaluated without any external language model. For PR, the prediction head is a frame-wise linear\ntransformation with CTC loss. More details can be found at SUPERB benchmark [14  ###reference_b14###]. Adam optimizer is used for both ASR and PR with learning rate of  and , respectively. We conducted experiments for each ASR and PR model five times and have provided the results, including the means and standard deviations, for both the vanilla models (HuBERT and WavLM) and their SCORE fine-tuned versions. For QbE, conventional supervised phoneme posteriorgram are replaced with SSL representations [14  ###reference_b14###]. For QbE, no training is required, and the evaluation is performed by running DTW on all layers separately and obtain a score for each query-document pair. For the evaluation on test set, the best layer is selected based on performance on dev set from QUESST 2014 [25  ###reference_b25###] data. In our case, we found that 12th layer provides best results for QbE for both HuBERT + SCORE and WavLM + SCORE."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Discussions",
            "text": "Table 1  ###reference_### shows the processed speech during training in \u201cpre-training\u201d stage and in \u201cSSFT stage\u201d. Processed speech is defined as \u201ctraining steps \u00d7 effective batch duration\u201d to quantify machine-independent training costs [4  ###reference_b4###]. HuBERT + SCORE improves the HuBERT model on all three tasks with relative improvement of 1.09%, 3.58%, and 12.65% for ASR, PR, and QbE, respectively. WavLM + SCORE improves the WavLM model on ASR, PR and QbE with relative improvement of 0.32%, 2.68% and 0.76%, respectively. The results are also compared with a stronger baseline ContentVec [5  ###reference_b5###], which uses 76K hours of processed speech compared to the SCORE which uses only 100 hrs in SSFT stage. ContentVec provides better results in ASR and PR when compared with SPIN and SCORE at the compute cost of 76K hrs.\nHowever, both HuBERT + SCORE and WavLM + SCORE outperform ContentVec on QbE task.\nLike SPIN, the goal of this work is to strike a balance between improving the downstream task and the additional training (i.e. SSFT) required. SCORE only needs  0.5 % of processed speech when compared with ContentVec in SSFT stage. SCORE provides competitive results with the SPIN models. WavLM + SCORE outperforms WavLM + SPIN in QbE task. Performance of HuBERT + SCORE is close to HuBERT + SPIN on ASR.\nAmong all the SSFT method, SCORE uses the least amount of processed speech ( 100 hrs) in SSFT stage."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Works",
            "text": "A simple and cost-effective SSFT method named SCORE is proposed to improve content representations of the pre-trained SSL speech models. For both the HuBERT and WavLM models, their respective SCORE fine-tuned models outperformed the original models on the SUPERB benchmark for ASR, PR, and QbE. Compared to other existing approaches of SSFT, SCORE requires the least amount of processed speech (less than 0.5% of processed speech compared to ContentVec). SCORE provides competitive results with SPIN using 1/3 of the processed speech used by SPIN. While we observed relatively fewer improvements in ASR compared to PR and QbE, we speculate that a stronger data augmentation technique directly applicable on speech waveforms could provide better gains. We consider this research direction for our future work."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S2.T1.14\" style=\"width:379.8pt;height:140.3pt;vertical-align:-13.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-142.1pt,47.4pt) scale(0.572031903205032,0.572031903205032) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.14.14\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.3.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.3.3.3.4\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.3.3.3.4.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S2.T1.3.3.3.5\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.3.3.3.5.1\">\n<tr class=\"ltx_tr\" id=\"S2.T1.3.3.3.5.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.3.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.3.3.3.5.1.1.1.1\">Training</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.3.3.3.5.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.3.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.3.3.3.5.1.2.1.1\">Processed Speech (hours)</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S2.T1.1.1.1.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.1.1.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.1.1.1.1.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.1.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.1.1.2.1.1\">ASR</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.1.1.1.1.1\">(WER) </span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.2.2.2.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S2.T1.2.2.2.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.2.2.2.2.1.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.2.2.2.2.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.2.2.2.2.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.2.2.2.2.1.1.2.1.1\">PR</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.2.2.2.2.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.2.2.2.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.2.2.2.2.1.1.1.1.1\">(PER) </span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.3.3.3.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S2.T1.3.3.3.3.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.3.3.3.3.1.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.3.3.3.3.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.3.3.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.3.3.3.3.1.1.2.1.1\">QbE</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.3.3.3.3.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.3.3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.3.3.3.3.1.1.1.1.1\">(MTWV) </span></span></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.14.14.15.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.14.14.15.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.14.14.15.1.1.1\">Pre-training</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.14.14.15.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.14.14.15.1.2.1\">SSFT</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.14.14.16.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.14.14.16.2.1\">HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib1\" title=\"\">1</a>]</cite><sup class=\"ltx_sup\" id=\"S2.T1.14.14.16.2.1.1\">\u2729</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.14.14.16.2.2\">506K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.14.14.16.2.3\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.14.14.16.2.4\">6.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.14.14.16.2.5\">5.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.14.14.16.2.6\">7.36</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.14.14.17.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.14.14.17.3.1\">WavLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib2\" title=\"\">2</a>]</cite><sup class=\"ltx_sup\" id=\"S2.T1.14.14.17.3.1.1\">\u2729</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.14.14.17.3.2\">1439K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.14.14.17.3.3\">0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.14.14.17.3.4\">6.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.14.14.17.3.5\">4.84</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.14.14.17.3.6\">8.70</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.4.4.1\">ContentVec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib5\" title=\"\">5</a>]</cite><sup class=\"ltx_sup\" id=\"S2.T1.4.4.4.1.1\">\u2729</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.4.4.2\">506K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.4.4.3\">76K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.4.4.4\">5.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.4.4.5\">4.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.4.4.6\">5.90</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.5.1\">HuBERT + SPIN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib4\" title=\"\">4</a>]</cite><sup class=\"ltx_sup\" id=\"S2.T1.5.5.5.1.1\">\u2729</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.5.2\">506K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.5.3\">356</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.5.4\">6.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.5.5\">4.39</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.5.6\">9.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.6.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.6.6.6.1\">WavLM + SPIN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib4\" title=\"\">4</a>]</cite><sup class=\"ltx_sup\" id=\"S2.T1.6.6.6.1.1\">\u2729</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.6.6.6.2\">1439K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.6.6.6.3\">356</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.6.6.6.4\">5.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.6.6.6.5\">4.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.6.6.6.6\">8.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.8.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.8.8.8.3\">HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib1\" title=\"\">1</a>]</cite><sup class=\"ltx_sup\" id=\"S2.T1.8.8.8.3.1\">\u2735</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.8.8.8.4\">506K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.8.8.8.5\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.7.7.7.1\">6.42  0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.8.8.8.2\">5.02  0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.8.8.8.6\">7.19</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.10.10.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.10.10.10.3\">WavLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib2\" title=\"\">2</a>]</cite><sup class=\"ltx_sup\" id=\"S2.T1.10.10.10.3.1\">\u2735</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.10.10.10.4\">1439K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.10.10.10.5\">0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.9.9.9.1\">6.17  0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.10.10.10.2\">4.85  0.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.10.10.10.6\">9.15</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.12.12.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.12.12.12.3\">HuBERT + SCORE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.12.12.12.4\">506K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.12.12.12.5\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.11.11.11.1\">6.35  0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.12.12.12.2\">4.84  0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.12.12.12.6\">8.10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.14.14.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T1.14.14.14.3\">WavLM + SCORE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T1.14.14.14.4\">1439K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T1.14.14.14.5\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T1.13.13.13.1\">6.15  0.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T1.14.14.14.2\">4.72  0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S2.T1.14.14.14.6\">9.22</td>\n</tr>\n</tbody>\n</table>\n<ul class=\"ltx_itemize\" id=\"S2.I1\">\n<li class=\"ltx_item\" id=\"S2.I1.ix1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2729</span>\n<div class=\"ltx_para\" id=\"S2.I1.ix1.p1\">\n<p class=\"ltx_p\" id=\"S2.I1.ix1.p1.1\"><span class=\"ltx_text\" id=\"S2.I1.ix1.p1.1.1\" style=\"font-size:90%;\">The reported numbers are from their respective papers and SUPERB benchmark leaderboard </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.I1.ix1.p1.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib14\" title=\"\">14  ###reference_b14###</a><span class=\"ltx_text\" id=\"S2.I1.ix1.p1.1.3.2\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" id=\"S2.I1.ix1.p1.1.4\" style=\"font-size:90%;\"> as of 13/09/2023 (</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://superbbenchmark.org/leaderboard\" style=\"font-size:90%;\" title=\"\">https://superbbenchmark.org/leaderboard  ###reference_###</a><span class=\"ltx_text\" id=\"S2.I1.ix1.p1.1.5\" style=\"font-size:90%;\">).</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S2.I1.ix2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2735</span>\n<div class=\"ltx_para\" id=\"S2.I1.ix2.p1\">\n<p class=\"ltx_p\" id=\"S2.I1.ix2.p1.1\"><span class=\"ltx_text\" id=\"S2.I1.ix2.p1.1.1\" style=\"font-size:90%;\">Our results when we run the SUPERB </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.I1.ix2.p1.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.06260v1#bib.bib14\" title=\"\">14  ###reference_b14###</a><span class=\"ltx_text\" id=\"S2.I1.ix2.p1.1.3.2\" style=\"font-size:90%;\">]</span></cite><span class=\"ltx_text\" id=\"S2.I1.ix2.p1.1.4\" style=\"font-size:90%;\"> baseline scripts for HuBERT and WavLM for fair comparison.</span></p>\n</div>\n</li>\n</ul>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.18.1.1\">Table 1</span>: </span>Results of the proposed SCORE fine-tuning of HuBERT and WavLM models along with baseline methods on SUPERB benchmark. The baseline methods include the BASE version of HuBERT and WavLM models, along with SSFT based ContentVec and SPIN models. The downstream tasks include ASR, PR, and QbE, which are evaluated on word error rate (WER in %), phoneme error rate (PER in %), and maximum term weighted value (MTWV in %), respectively. </figcaption>\n</figure>",
            "capture": "Table 1: Results of the proposed SCORE fine-tuning of HuBERT and WavLM models along with baseline methods on SUPERB benchmark. The baseline methods include the BASE version of HuBERT and WavLM models, along with SSFT based ContentVec and SPIN models. The downstream tasks include ASR, PR, and QbE, which are evaluated on word error rate (WER in %), phoneme error rate (PER in %), and maximum term weighted value (MTWV in %), respectively. "
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.2\" style=\"width:303.5pt;height:165.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(69.2pt,-37.7pt) scale(1.83742341035203,1.83742341035203) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.1.1.2.1\">Layer 11</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.1.1.1.3.1\">Layer 12</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.2.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.1.2.1.1\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.1.2.1.2\">67.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.1.2.1.3\">64.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.1.3.2.1\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.1.3.2.2\">52.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.1.3.2.3\">49.16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.1.4.3.1\">HuBERT + SCORE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.1.4.3.2\">66.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.1.4.3.3\">62.61</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.2.1.5.4.1\">WavLM + SCORE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.2.1.5.4.2\">52.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.2.1.5.4.3\">48.30</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.1.1\">Table 2</span>: </span>Layerwise SID accuracy (in %) on SUPERB benchmark for original and SCORE fine-tuned SSL models.</figcaption>\n</figure>",
            "capture": "Table 2: Layerwise SID accuracy (in %) on SUPERB benchmark for original and SCORE fine-tuned SSL models."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.06260v1_figure_1.png",
            "caption": "Fig. 1: SCORE fine-tuning method. SCORE takes a pair of original speech and perturbed speech as input. It then matches the output sequence X\ud835\udc4bXitalic_X from the learnable model M\u03b8subscript\ud835\udc40\ud835\udf03M_{\\theta}italic_M start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT against the output sequence X\u2032superscript\ud835\udc4b\u2032X^{\\prime}italic_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT from the frozen model M\u03d5subscript\ud835\udc40italic-\u03d5M_{\\phi}italic_M start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT using soft-DTW loss."
        }
    },
    "references": [
        {
            "1": {
                "title": "\u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d",
                "author": "W. Hsu, B. Bolte, Y. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed,",
                "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc., vol. 29, pp. 3451\u20133460, oct 2021.",
                "url": null
            }
        },
        {
            "2": {
                "title": "\u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d",
                "author": "S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei,",
                "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, July 2022.",
                "url": null
            }
        },
        {
            "3": {
                "title": "\u201cSelf-supervised speech representation learning: A review,\u201d",
                "author": "A. Mohamed, H. Lee, L. Borgholt, J.D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe,",
                "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 16, pp. 1179\u20131210, 2022.",
                "url": null
            }
        },
        {
            "4": {
                "title": "\u201cSelf-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering,\u201d",
                "author": "H. Chang, A. H. Liu, and J. Glass,",
                "venue": "in Proc. INTERSPEECH 2023, 2023, pp. 2983\u20132987.",
                "url": null
            }
        },
        {
            "5": {
                "title": "\u201cContentVec: An improved self-supervised speech representation by disentangling speakers,\u201d",
                "author": "K. Qian, Y. Zhang, H. Gao, J. Ni, C. Lai, D. Cox, M. Hasegawa-Johnson, and S. Chang,",
                "venue": "in Proceedings of the 39th International Conference on Machine Learning, Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, Eds. 17\u201323 Jul 2022, vol. 162 of Proceedings of Machine Learning Research, pp. 18003\u201318017, PMLR.",
                "url": null
            }
        },
        {
            "6": {
                "title": "\u201cImproving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation,\u201d",
                "author": "K. P. Huang, Y. Fu, Y. Zhang, and H. Lee,",
                "venue": "in Proc. Interspeech 2022, 2022, pp. 2193\u20132197.",
                "url": null
            }
        },
        {
            "7": {
                "title": "\u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d",
                "author": "A. Baevski, Y. Zhou, A. Mohamed, and M. Auli,",
                "venue": "in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, Eds. 2020, vol. 33, pp. 12449\u201312460, Curran Associates, Inc.",
                "url": null
            }
        },
        {
            "8": {
                "title": "\u201cTruly unsupervised acoustic word embeddings using weak top-down constraints in encoder-decoder models,\u201d",
                "author": "H. Kamper,",
                "venue": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6535\u20133539, 2019.",
                "url": null
            }
        },
        {
            "9": {
                "title": "\u201cDeriving translational acoustic sub-word embeddings,\u201d",
                "author": "A. Meghanani and T. Hain,",
                "venue": "in Proc. of ASRU, 2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "\u201cAudio augmentation for speech recognition,\u201d",
                "author": "T. Ko, V. Peddinti, Povey D, and S. Khudanpur,",
                "venue": "in Proc. Interspeech 2015, 2015, pp. 3586\u20133589.",
                "url": null
            }
        },
        {
            "11": {
                "title": "\u201cSoft-DTW: a differentiable loss function for time-series,\u201d",
                "author": "M Cuturi and M Blondel,",
                "venue": "in Proceedings of the 34th International Conference on Machine Learning, Doina Precup and Yee Whye Teh, Eds. 06\u201311 Aug 2017, vol. 70 of Proceedings of Machine Learning Research, pp. 894\u2013903, PMLR.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Deep Recurrent Networks for Gesture Recognition and Synthesis,",
                "author": "M. Maghoumi,",
                "venue": "Ph.D. thesis, University of Central Florida Orlando, Florida, 2020.",
                "url": null
            }
        },
        {
            "13": {
                "title": "\u201cSoft dynamic time warping for multi-pitch estimation and beyond,\u201d",
                "author": "M. Krause, C. Wei\u00df, and M. M\u00fcller,",
                "venue": "in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1\u20135.",
                "url": null
            }
        },
        {
            "14": {
                "title": "\u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d",
                "author": "S. Yang, P. Chi, Y. Chuang, C. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G. Lin, T. Huang, W. Tseng, K. Lee, D. Liu, Z. Huang, S. Dong, S. Li, S. Watanabe, A. Mohamed, and H. Lee,",
                "venue": "in Proc. Interspeech 2021, 2021, pp. 1194\u20131198.",
                "url": null
            }
        },
        {
            "15": {
                "title": "\u201cDistilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert,\u201d",
                "author": "H. Chang, S. Yang, and H. Lee,",
                "venue": "in Prof. of ICASSP 2022), 2022, pp. 7087\u20137091.",
                "url": null
            }
        },
        {
            "16": {
                "title": "\u201cLayer-wise analysis of a self-supervised speech representation model,\u201d",
                "author": "A. Pasad, J. Chou, and K. Livescu,",
                "venue": "in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021, pp. 914\u2013921.",
                "url": null
            }
        },
        {
            "17": {
                "title": "\u201cTorchaudio: Building blocks for audio and speech processing,\u201d",
                "author": "Y. Yang, M. Hira, Z. Ni, A. Astafurov, C. Chen, C. Puhrsch, D. Pollack, D. Genzel, D. Greenberg, E. Z. Yang, J. Lian, J. Hwang, J. Chen, P. Goldsborough, S. Narenthiran, S. Watanabe, S. Chintala, and V. Quenneville-B\u00e9lair,",
                "venue": "in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6982\u20136986.",
                "url": null
            }
        },
        {
            "18": {
                "title": "\u201cDeepnag: Deep non-adversarial gesture generation,\u201d",
                "author": "M. Maghoumi, E. M. Taranta, and J. LaViola,",
                "venue": "in 26th International Conference on Intelligent User Interfaces, 2021, pp. 213\u2013223.",
                "url": null
            }
        },
        {
            "19": {
                "title": "\u201cMachine learning for time series \u2013 notes from lectures at ensai,\u201d 2021.",
                "author": "R. Tavenard,",
                "venue": null,
                "url": null
            }
        },
        {
            "20": {
                "title": "\u201cDifferentiable divergences between time series,\u201d",
                "author": "M. Blondel, A. Mensch, and J. Vert,",
                "venue": "in Proc. of AIStat. 13\u201315 Apr 2021, vol. 130 of Proceedings of Machine Learning Research, pp. 3853\u20133861, PMLR.",
                "url": null
            }
        },
        {
            "21": {
                "title": "\u201cTslearn, a machine learning toolkit for time series data,\u201d",
                "author": "R. Tavenard, J. Faouzi, G. Vandewiele, F. Divo, G. Androz, C. Holtz, M. Payne, R. Yurchak, M. Ru\u00dfwurm, K. Kolar, and E. Woods,",
                "venue": "Journal of Machine Learning Research, vol. 21, no. 118, pp. 1\u20136, 2020.",
                "url": null
            }
        },
        {
            "22": {
                "title": "\u201cBetter fine-tuning by reducing representational collapse,\u201d",
                "author": "A. Aghajanyan, A. Shrivastava, A. Gupta, N. Goyal, L. Zettlemoyer, and S. Gupta,",
                "venue": "CoRR, vol. abs/2008.03156, 2020.",
                "url": null
            }
        },
        {
            "23": {
                "title": "\u201cLibrispeech: An asr corpus based on public domain audio books,\u201d",
                "author": "V. Panayotov, G. Chen, D. Povey, and S. Khudanpur,",
                "venue": "in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 5206\u20135210.",
                "url": null
            }
        },
        {
            "24": {
                "title": "\u201cDecoupled weight decay regularization,\u201d",
                "author": "I. Loshchilov and F. Hutter,",
                "venue": "in International Conference on Learning Representations, 2019.",
                "url": null
            }
        },
        {
            "25": {
                "title": "\u201cQuesst2014: Evaluating query-by-example speech search in a zero-resource setting with real-life queries,\u201d",
                "author": "X. Anguera, L. Rodriguez-Fuentes, A. Buzo, F. Metze, I. Sz\u00f6ke, and M. Penagarikano,",
                "venue": "in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 5833\u20135837.",
                "url": null
            }
        },
        {
            "26": {
                "title": "\u201cVoxceleb: Large-scale speaker verification in the wild,\u201d",
                "author": "A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman,",
                "venue": "Computer Speech & Language, vol. 60, pp. 101027, 2020.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.06260v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4.1"
        ]
    },
    "research_context": {
        "paper_id": "2403.06260v1",
        "paper_title": "SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations",
        "research_background": "Based on the provided sections, we can derive the following details about the paper:\n\n### Motivation:\nThe motivation behind this paper is to enhance the effectiveness of self-supervised learning (SSL) based pre-trained speech models in downstream speech-related tasks that may not align well with the original pre-training objectives. Specifically, the authors recognize the limitations of pre-trained SSL models such as HuBERT and WavLM in handling tasks like overlapping speech. The current solutions either involve introducing new pre-training objectives, which are compute-intensive, or employing costly self-supervised fine-tuning (SSFT) methods.\n\n### Research Problem:\nThe primary research problem addressed in this paper is how to develop a more cost-effective self-supervised fine-tuning method that can improve content representations in SSL pre-trained speech models. The proposed solution needs to be efficient in terms of computational resources while effectively preserving content information, making it suitable for content-related downstream tasks such as automatic speech recognition (ASR), phoneme recognition (PR), and query-by-example spoken term discovery (QbE).\n\n### Relevant Prior Work:\n1. **Pre-trained SSL Models**:\n   - **HuBERT [1]** and **WavLM [2]**: These models have pre-training objectives designed to capture various speech factors like acoustic-phonetic and speaker information. They achieve state-of-the-art performance in numerous speech applications.\n\n2. **ContentVec [5]**:\n   - This model employs content-preserving strategies by disentangling speakers from the HuBERT pre-trained model to learn content-specific representations. However, it requires significant computational resources (19 hours on 36 GPUs).\n\n3. **SPIN (Speaker-Invariant Clustering) [4]**:\n   - A more compute-efficient SSFT method designed to improve content representations using speaker-invariant clustering. It requires less than 1% of the compute cost of ContentVec.\n\n4. **Correspondence Training [8,9]**:\n   - This method has been successful in learning high-quality acoustic word embeddings (AWEs) by using an auto-encoder to match representations from spoken content instances by different speakers, thereby focusing on content-specific information.\n\n5. **Loss Functions**:\n   - **Soft-DTW [11,12]**: Used here as a loss function to match representations between original and perturbed speech. It is popular for handling time-series data and effective in tasks like multi-pitch estimation in music information retrieval [13].\n\nThis work aims to leverage the strengths of correspondence training to introduce a novel and cost-effective SSFT method named \"SCORE\" which improves content representations by learning similar representations from both original and perturbed speech using data augmentation techniques like speed perturbation and pitch shifting. The resultant models are tested and shown to outperform baseline models on content-related tasks within the SUPERB benchmark while requiring significantly less computational cost.",
        "methodology": "**SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations**\n\n**Methodology:**\n\nThe proposed SCORE fine-tuning method involves two instances of a pre-trained model: one with frozen parameters (\\( \\theta_f \\)) and the other with learnable top layers (\\( \\theta_l \\)). Both models start with the same initial weights. The top layers are chosen for fine-tuning as they are known to encode phonetic content in most self-supervised learning (SSL) models [15, 16].\n\n**Input Processing:**\n- The inputs to the SCORE method are a pair of perturbed and original speech signals, which are randomly fed into either the \\( \\theta_f \\) or \\( \\theta_l \\) model. This randomness ensures that \\( \\theta_l \\) does not exclusively focus on the characteristics of perturbations.\n- Data augmentations, commonly used in Automatic Speech Recognition (ASR) such as speech perturbation and pitch shift [10], are used to create the perturbed speech. This is achieved using Torchaudio's SpeedPerturbation and PitchShift functions [17].\n\n**Representation Extraction and Dimensionality Reduction:**\n- Representations obtained from both models (\\( \\theta_f \\) and \\( \\theta_l \\)) are projected to a lower dimension using linear feedforward layers and L2-normalized.\n- The sequences from the two models differ in length due to the perturbations. To address this, a dynamic time warping based differentiable loss function, soft-Dynamic Time Warping (soft-DTW), is utilized to match sequences of unequal lengths.\n\n**Soft-Dynamic Time Warping (soft-DTW):**\n- The loss function is designed to compute the soft-minimum alignment cost between two sequences \\( \\mathbf{a} \\) and \\( \\mathbf{b} \\).\n- Soft-DTW replaces the \u201cmin\u201d operation in DTW with a \u201csoft-min\u201d operation and is defined with a smoothing factor \\( \\gamma \\) and an appropriate distance function \\( D \\).\n- For this work, the smoothing factor \\( \\gamma \\) is set to 0.1.\n\n**Loss Function Normalization:**\n- To handle potential negative values in soft-DTW loss, a normalized version is employed to ensure the minimum loss value is zero for identical sequences and non-negative for any pair.\n- The loss is further normalized by dividing it by the total sequence length \\( L \\).\n\n**Overview of Algorithm 1:**\n- Although the detailed steps are not provided here, Algorithm 1 describes the entire SCORE fine-tuning method.\n  \n**Benchmarking:**\n- Results are reported based on the SUPERB benchmark leaderboard as of 13/09/2023.\n- The baseline scripts for HuBERT and WavLM were run for fair comparison.\n\nOverall, SCORE aims to fine-tune SSL models to achieve speed and pitch invariant representations by leveraging self-supervised correspondence learning using a soft-DTW loss function.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n**Models:** Two self-supervised learning (SSL) speech models, HuBERT and WavLM (BASE versions), are used as the base models. These models are fine-tuned using the SCORE methodology.\n\n**Datasets:** The experiments utilize the train-clean-100 hours subset of the LibriSpeech dataset for SCORE fine-tuning, ensuring consistent and fair comparisons with previous research.\n\n**Fine-tuning Strategy:** \n- Only the top 2 layers (11th and 12th) of the SSL models are fine-tuned due to their high phonetic content encoding and cost-effectiveness.\n- Full model fine-tuning is avoided to reduce computational expenses and prevent potential collapse of pre-trained representations.\n\n**SCORE Fine-tuning Implementation:**\n- The representations from the final (12th) Transformer layer are projected into 256-dimensional vectors and L2-normalized.\n- Fine-tuning is conducted for 3.6k updates (1 effective epoch with a batch size of 8), using the AdamW optimizer with a learning rate and 1k warm-up updates.\n- The fine-tuning process converges within one epoch, with no improvements noted following additional training, and an epoch takes roughly 5 hours on a V100 GPU.\n\n**Evaluation Metrics and Baselines:**\n**SUPERB Benchmark:** The S3PRL toolkit is employed for all the SUPERB benchmark tasks, which include ASR (Automatic Speech Recognition), PR (Phoneme Recognition), and QbE (Query by Example).\n\n- **ASR:**\n  - Features aggregated from all layers with learnable weights and passed to a 2-layer 1024-unit Bi-LSTM network with CTC loss on characters.\n  - Evaluated without any external language model.\n\n- **PR:**\n  - Features aggregated similarly to ASR, passed to a frame-wise linear transformation with CTC loss.\n\n- **QbE:**\n  - Replaces conventional supervised phoneme posteriorgram with SSL representations.\n  - Evaluated using Dynamic Time Warping (DTW) on all layers separately, selecting the best layer's score on the dev set from QUESST 2014.\n\n**Baselines:**\n- Vanilla HuBERT and WavLM models (i.e., without SCORE fine-tuning).\n\n#### Main Experimental Results\n**Results:**\n- HuBERT and WavLM, after SCORE fine-tuning, outperform their vanilla versions on respective downstream tasks on the SUPERB benchmark.\n- For QbE, the 12th layer provides the best results for both HuBERT + SCORE and WavLM + SCORE.\n\nThe detailed performance metrics (means and standard deviations) for ASR and PR models, conducted five times for each variation, illustrate the effectiveness of SCORE fine-tuning in enhancing content-related tasks. The results showcase that fine-tuning with SCORE yields improved performance in both HuBERT and WavLM models compared to their vanilla counterparts."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to investigate the effectiveness of the SCORE fine-tuning method for SSL-based speech models in enhancing task-specific representations for content-related tasks.",
            "experiment_process": "Experiments involve fine-tuning two SSL speech models, HuBERT and WavLM (BASE versions), using the SCORE method. The top 2 layers (11th and 12th) of these models are fine-tuned. Data from LibriSpeech\u2019s train-clean-100 is used, maintaining a focus on cost-effectiveness. SCORE fine-tuning details include using final Transformer layer representations, projecting them to 256-dimensional vectors, and L2-normalizing. Training uses AdamW optimizer with a specific learning rate, converging in one epoch on a V100 GPU. Downstream tasks from the SUPERB benchmark, specifically ASR and PR, are used for evaluation with detailed configurations for prediction heads and aggregation of features across layers. For QbE, evaluations are conducted using DTW on all layers, selecting the best performing layer from QUESST 2014 data.",
            "result_discussion": "The fine-tuned models using SCORE method achieved notable improvements: relative gains of 1.09% for ASR, 3.58% for PR, and 12.65% for QbE compared to the vanilla HuBERT model. Additionally, SCORE provided competitive results with the SPIN method while requiring significantly less processed speech (only 1/3 of SPIN's usage).",
            "ablation_id": "2403.06260v1.No1"
        },
        {
            "research_objective": "To determine the impact of the SCORE fine-tuning method on speaker identification (SID) accuracy by assessing any potential degradation in speaker information due to data augmentation techniques like pitch shift.",
            "experiment_process": "Experiments for the SID task from the SUPERB benchmark are conducted using the VoxCeleb1 dataset with configurations consistent with the SUPERB setup. Only the fine-tuned layers (11th and 12th) of HuBERT and WavLM models (both vanilla and SCORE fine-tuned) are tested. Pitch shift is employed as a data augmentation technique to alter speaker information, and SID accuracy is evaluated accordingly.",
            "result_discussion": "The results indicate a drop in SID accuracy for both HuBERT and WavLM models (in both layers) after fine-tuning with the SCORE method. This suggests that the representations from SCORE fine-tuned models are more speaker-invariant compared to the original models, which is advantageous for content-related tasks.",
            "ablation_id": "2403.06260v1.No2"
        }
    ]
}