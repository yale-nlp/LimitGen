{
    "title": "Rethinking ChatGPT\u2019s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs\u2019 Prompting",
    "abstract": "Over the last decade, a wide range of training and deployment strategies for Large Language Models (LLMs) have emerged. Among these, the prompting paradigms of Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in Artificial Intelligence (AI). This paper aims to emphasize the significance of utilizing free-form 111\u201cFree-form\u201d describes a stream of meaningful symbols, created by humans or through auto-regressive methods. modalities (forms of\ninput and output) and verbal free-form contexts as user-directed channels (methods for transforming modalities) for downstream deployment.\nSpecifically, we analyze the structure of modalities within both two types of LLMs and six task-specific channels during deployment. From the perspective of users, our analysis introduces and applies the analytical metrics of task customizability, transparency, and complexity to gauge their usability, highlighting the superior nature of AR-LLMs\u2019 prompting paradigms.\nMoreover, we examine the stimulation of diverse cognitive behaviors in LLMs through the adoption of free-form text and verbal contexts, mirroring human linguistic expressions of such behaviors.\nWe then detail four common cognitive behaviors to underscore how AR-LLMs\u2019 prompting successfully imitate human-like behaviors using this free-form modality and channel.\nLastly, the potential for improving LLM deployment, both as autonomous agents and within multi-agent systems, is identified via cognitive behavior concepts and principles.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "ChatGPT has emerged as the most popular AI application, with a vast user base. The success of GPT models can be attributed to the scaling of transformer-based neural networks and the extensive pre-training data, as explored in previous studies (Radford et al., 2019  ###reference_b25###; Brown et al., 2020  ###reference_b3###).\nThe scope of this paper is directed towards Large Language Models (LLMs) that are sufficiently large to acquire world knowledge, commonsense, and the linguistic capabilities required to attain high performance on benchmarks such as GLUE (Wang et al., 2019  ###reference_b34###).\nAlthough LLMs are commonly perceived as general-purpose language intelligence models, the practice often diverges from employing a singular, all-encompassing model for every task. Instead, the deployment frequently entails developing a suite of specialized models tailored to specific tasks. This specialization is facilitated through the introduction of task-specific channels, modifying the model\u2019s structure or its pre-trained parameters to better suit the nuances of individual tasks. This highlights a departure from the ideal of a universal, one-size-fits-all model, while the broad capabilities of LLMs suggest they could serve as jack-of-all-trades in language processing.\nThis trend towards creating task-specific models may stem from the tradition of evaluating linguistic intelligence through a variety of distinct tasks and benchmarks (Wang et al., 2019  ###reference_b34###), with researchers striving to excel in these tasks independently to set new benchmarks. In this paper, we delve into the mechanisms behind prevalent deployment paradigms including AR-LLMs\u2019 prompting, which underpins ChatGPT\u2019s operation, and highlight several critical observations:\n1) Models tailored with optimized task-specific channels often suffer from issues related to task customizability, transparency, and user-level complexity during deployment, affecting their overall usability;\n2) Anticipated to mimic human-like intelligence, they often exhibit slow thinking through shortcuts (Kahneman, 2011  ###reference_b12###);\n3) They frequently fall short in showcasing advanced cognitive behaviors, which we contend are vital for convincing users of the models\u2019 intelligence. Conversely, AR-LLMs\u2019 prompting paradigms introduce a more natural, human-like channel (verbal free-form context) for representing a wide array of real-life tasks and employ form-form output modalities to showcase cognitive behaviors in complex scenarios.\nSpecifically, in this paper, we commence by examining the foundational principles of language modeling, revisiting the notable split in language modeling approaches that emerged in the late 2010s: auto-encoding LMs (AE-LMs) exemplified by BERT (Jin et al., 2020  ###reference_b10###) and auto-regressive LMs (AR-LMs) exemplified by the GPT series (Radford et al., 2018  ###reference_b24###; Brown et al., 2020  ###reference_b3###). Rather than delve into an extensive array of deployment paradigms, we introduce and discuss the concepts of modalities and channels to investigate how LLMs are deployed (\u00a72  ###reference_###).\nUpon evaluating different deployment paradigms for LLMs, it becomes clear that aside from the AR-LLMs\u2019 prompting approach, other paradigms struggle to demonstrate advanced human-like cognitive behaviors. This shortfall is attributed to the constraints within modalities and channels, coupled with a tendency towards superficial learning, i.e., slow thinking (\u00a73  ###reference_### and \u00a74.1  ###reference_###).\nIn contrast, via specified context in the free-from text, the AR-LLMs\u2019 prompting strategy imitate human-like cognitive behaviors, such as reasoning, planning, and feedback learning, which are elucidated in Table 2  ###reference_### (\u00a74  ###reference_###).\nFinally, we explore how understanding cognitive behaviors can help overcome the tuning and deployment obstacles encountered by LLMs functioning as autonomous entities and within multi-agent frameworks (\u00a75  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Deploying Large Language Models",
            "text": "This section elucidates the dual objectives underlying language models, which both aim to model the joint probability distribution of text sequences through self-supervised learning techniques and generate text that is relevant to the given context.\nAfter this introduction, we present a novel framework that facilitate the characterization of various deployment paradigms through two types of data modalities, which support language comprehension, coupled with six unique channels for processing these modalities.\nThe goal of language modeling is to estimate the joint probability distribution of sequences of text (Bengio et al., 2003  ###reference_b2###). This involves developing two distinct yet relaxed formulations for constructing LLMs that leverage self-supervised learning from vast quantities of unlabeled text data. The self-supervised approach enables the training of LLMs on extensive text corpora, a practice that has been thoroughly investigated in various studies (Liu et al., 2019  ###reference_b18###; Wei et al., 2022a  ###reference_b39###). This paper focuses on how the intrinsic design of language models impacts their usability and potential to express cognitive behaviors.\nTypically, language modeling is approached by predicting the subsequent token in a sequence based on the preceding tokens. This prediction is quantified as the product of conditional probabilities for each subsequent token, considering its previous tokens, in accordance with the chain rule (Bengio et al., 2003  ###reference_b2###).\nHere,  serves as a marker for the beginning of text.\nIn the context of auto-encoding language modeling, noise is intentionally introduced to an input sequence . The primary aim is to optimize\nwhere  represents the altered, noise-added version of the input sequence.\nThe approach of masking specific tokens in the text at random, known as token-level masked language modeling (Devlin et al., 2019  ###reference_b4###), is a widely adopted strategy. This involves substituting original tokens with a special token, such as \u201c[MASK]\u201d, and training the model to predict these original tokens based on the context of the surrounding, unmasked tokens. The discrepancy between the original and reconstructed sequences is quantified through a reconstruction loss:\nThis denoising methodology also includes other variants such as span-level masked language modeling (Joshi et al., 2020  ###reference_b11###), text infilling (Lewis et al., 2020  ###reference_b14###), among others."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "The Fundamental Dichotomy in Language Modeling",
            "text": "The goal of language modeling is to estimate the joint probability distribution of sequences of text (Bengio et al., 2003  ###reference_b2###  ###reference_b2###). This involves developing two distinct yet relaxed formulations for constructing LLMs that leverage self-supervised learning from vast quantities of unlabeled text data. The self-supervised approach enables the training of LLMs on extensive text corpora, a practice that has been thoroughly investigated in various studies (Liu et al., 2019  ###reference_b18###  ###reference_b18###; Wei et al., 2022a  ###reference_b39###  ###reference_b39###). This paper focuses on how the intrinsic design of language models impacts their usability and potential to express cognitive behaviors.\nTypically, language modeling is approached by predicting the subsequent token in a sequence based on the preceding tokens. This prediction is quantified as the product of conditional probabilities for each subsequent token, considering its previous tokens, in accordance with the chain rule (Bengio et al., 2003  ###reference_b2###  ###reference_b2###).\nHere,  serves as a marker for the beginning of text.\nIn the context of auto-encoding language modeling, noise is intentionally introduced to an input sequence . The primary aim is to optimize\nwhere  represents the altered, noise-added version of the input sequence.\nThe approach of masking specific tokens in the text at random, known as token-level masked language modeling (Devlin et al., 2019  ###reference_b4###  ###reference_b4###), is a widely adopted strategy. This involves substituting original tokens with a special token, such as \u201c[MASK]\u201d, and training the model to predict these original tokens based on the context of the surrounding, unmasked tokens. The discrepancy between the original and reconstructed sequences is quantified through a reconstruction loss:\nThis denoising methodology also includes other variants such as span-level masked language modeling (Joshi et al., 2020  ###reference_b11###  ###reference_b11###), text infilling (Lewis et al., 2020  ###reference_b14###  ###reference_b14###), among others."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Exploring the Modalities within Large Language Models",
            "text": "This section delves into the concept of \u201cmodalities\u201d within LLMs, a term often implicitly associated with research on multimodal systems to describe diverse, human-like channels of communication, such as text, speech, gestures, and visual inputs (Bartneck et al., 2020  ###reference_b1###). Here, \u201cmodalities\u201d specifically refer to the various forms of input and output data utilized in LLM deployment.\nIn the operation of both AR-LLMs and AE-LLMs, we identify three primary modalities: a unique textual modality for both the input and output in AR-LLMs (unrestricted text), a distinct textual modality for AE-LLMs (masked text or contextualized n-grams), and a shared modality of intermediate dense representations applicable to both models:\n1) Intermediate Dense Representations: Fundamentally, LLMs convert each word (or subword) in a sequence into dense vector embeddings. These embeddings are generated through a series of mathematical operations, such as the self-attention mechanism, at every layer of the neural network, and are represented as  for every position  within the sequence and for every layer  in the model. Here,  ranges from  to , with  indicating the total number of elements in the sequence, and  spans from  to , where  represents the complete count of layers within the model.\n2) Textual Modalities: AE-LLMs feature an input modality of masked text, with the output modality being contextualized n-grams designed to reconstruct the masked sections. Conversely, due to their auto-regressive design, AR-LLMs are capable of encoding any text as context and generating free-form text outputs, thereby employing unrestricted text for both input and output. These modalities are inherently linked to their respective language modeling strategies."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Task-specific Channels for Deployment",
            "text": "To tailor the core capabilities of LLMs for specific downstream tasks, both input and intermediate modalities can be altered directly (for instance, by appending prefixes or incorporating verbal context) or indirectly through the use of parametric modules such as neural networks, including adapters and output layers as described subsequently. It\u2019s worth noting that direct modifications, such as prefixes, can also be achieved using parametric modules. These parametric modules undergo optimization via task-specific supervised learning. In this context, we describe the means for modality transformation aimed at specific tasks as task-specific channels.\nFor clarify, modalities are the types of data or the form in which data is processed, while channels are the pathways or methods through which these data modalities are adapted or transformed for specific tasks. Task-specific channels encompass:\n1) Adapter: Adapters are compact neural networks that can be embedded between an LLM\u2019s layers. A well-known approach, adapter tuning (Houlsby et al., 2019  ###reference_b8###), involves optimizing the adapter\u2019s parameters while leaving the original LLM parameters intact. These adapters are designed to adjust the intermediate layer representations to better align with task-specific needs.\n2) LLMs Themselves: An alternative strategy involves modifying the LLM directly to produce task-specific representations by fine-tuning the model\u2019s weights across all or selected layers. This method of fine-tuning is prevalent for AE-LLMs (Jin et al., 2020  ###reference_b10###) and has also been applied to AR-LLMs in early use of GPT-like models (Radford et al., 2018  ###reference_b24###).\n3) Output Layers: Once task-specific representations are produced by either adapters or the LLM directly, the function of the output layers is to translate these representations into a designated output space. These layers typically consist of one or several linear layers. For example, linear functions are frequently used for tasks involving classification, while tasks that involve extractive question answering often necessitate the use of two linear functions to determine the beginning and concluding positions of the answer within a text passage.\n4) Activation Prefixes: Within the scope of deploying LLMs via task-specific supervised learning, where training neural networks is common, prefix tuning (Li and Liang, 2021  ###reference_b15###) presents an innovative method that employs prefixes to directly modify intermediate representations. These prefixes are essentially embeddings that are added at various layers, with dimensions identical to those of token embeddings, functioning as virtual tokens. Introducing these prefixes at earlier stages in the model allows for the infusion of task-specific information into more advanced layers, thereby improving the model\u2019s alignment with the desired task objectives.\nBeyond the four channels previously outlined, verbal channels offer a unique approach for articulating the task context in which LLMs can identify and execute the intended tasks. These channels include:\n5) Verbal Free-form Context:\nIn this approach, a context is articulated using free-form text, such as task instructions and few-shot demonstrations, which can activate complex cognitive functions. By merely incorporating task instructions within the context, AR-LLMs are enabled to undertake a multitude of tasks through zero-shot prompts. Another widely adopted method is few-shot prompting (Radford et al., 2019  ###reference_b25###; Brown et al., 2020  ###reference_b3###), which involves learning from a limited number of examples for in-context learning without the need for gradient updates, showcasing a human-like efficiency in acquiring new tasks. This method is particularly effective in eliciting cognitive behaviors akin to those observed with few-shot demonstrations, with further details discussed in Section 4  ###reference_###.\nIt\u2019s important to recognize that, in contrast to channels that are easily differentiated by input-side modalities (such as task-specific examples), this channel (e.g., task instructions) can intertwine with model inputs, e.g., task-specific examples. This allows for the seamless integration of the models\u2019 world knowledge into tasks, for instance, \u201csummarize deep learning technology\u201d.\n6) Contextual Text Patterns:\nGiven their training on a denoising language model objective, AE-LLMs excel in completing texts by filling in missing words, a trait that can be leveraged for downstream tasks. Task-specific patterns, in this regard, serve as a mechanism to alter given task-specific examples. Typically, this involves appending the examples with a cloze-style phrase or sentence (text with missing words) tailored to the task, allowing the model to predict the intended task outcomes based on the placeholders filled within the text. Pattern Exploitation Training (PET) (Schick and Sch\u00fctze, 2021  ###reference_b28###) involves the creative design of task-specific patterns and the fine-tuning of LLMs to these patterns. Conversely, auto-prompt methods (Shin et al., 2020  ###reference_b29###) seek to optimize task-specific patterns to better fit the models, enhancing their ability to interpret and respond to the given tasks effectively."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Evaluation of Modalities and Channels",
            "text": "Essentially, any task can be articulated in human languages, such as English, using free-form context. This adaptability is a testament to the evolution of human language over thousands of years, which has been refined to describe a vast array of everyday and complex scientific problems. Typically, in a zero-shot learning context, the channel consists solely of task instructions within the prompts, capable of encompassing a wide range of tasks. For instance, Wang et al., Wang et al. (2022  ###reference_b35###) have converted standard NLP datasets designed for optimized channels into instruction-based formats for 76 different tasks.\nMoreover, free-form task instructions allow for nuanced control mechanisms, including explicit directives (such as specifying output formats or initiating reasoning processes) and subtle cues (such as inducing cognitive behaviors through few-shot examples). These aspects will be further explored in Section 4  ###reference_### and summarized in Table 2  ###reference_###.\nIn contrast, since other channels are set during the optimization process for specific tasks, they lack the flexibility for user-directed modifications. Channels that require adjustments, such as fine-tuning the LLM, adapter tuning, or prefix tuning, rely on supervised learning methods for configuration. Although prompting in AE-LLMs could, in theory, facilitate task adjustments at inference time without prior task-specific fine-tuning\u2014akin to AR-LLMs\u2019 prompting approach\u2014it often requires task-specific optimization to achieve effective channel performance. For example, techniques like Pattern Exploitation Training (PET) (Schick and Sch\u00fctze, 2021  ###reference_b28###) utilize mathematical optimization to adapt models to specific patterns, whereas Auto-prompt (Shin et al., 2020  ###reference_b29###) optimizes text patterns for language models. The question of whether this need for optimization arises from the inherent complexities of auto-encoding language models invites further research.\nThe focus here is on the understandability of the channels themselves to lay users, rather than their functional effectiveness, as this greatly influences the user experience. For example, the objective of an output layer is clear \u2014 transforming LLM representations into a specific output format. However, the process involving dense representations through matrix multiplication is not intuitively understandable to the non-specialist. Moreover, text patterns refined through AE-LLMs\u2019 Auto-prompting often lack the straightforwardness found in manually created prompts.\nThis analysis evaluates the conceptual load required to deploy  tasks using various channels, moving away from the parameter size metric, which is more pertinent to researchers and developers. Assuming each task is accommodable across all channels, we quantify the complexity as follows:\nFor fine-tuned LLMs, prefixes, adapters and output layers, each task-specific adjustment equates to a complexity of , with\n denoting the total number of tasks. Additionally,  text patterns are devised per task, resulting in a complexity of , where  represents the number of patterns per task. The complexity for verbal free-form context is considered negligible, as these are formulated spontaneously by users at the time of use.\nFrom this framework, we can deduce the complexity inherent to each deployment paradigm. For instance, LLM fine-tuning, which necessitates one LLM and one output layer per task, carries a complexity of ."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluating Usability of Deployment Channels",
            "text": "This section introduces a framework for assessing the usability of language model deployment channels, focusing on their customizability, transparency, and complexity, as summarized in Table 1  ###reference_###.\nEssentially, any task can be articulated in human languages, such as English, using free-form context. This adaptability is a testament to the evolution of human language over thousands of years, which has been refined to describe a vast array of everyday and complex scientific problems. Typically, in a zero-shot learning context, the channel consists solely of task instructions within the prompts, capable of encompassing a wide range of tasks. For instance, Wang et al., Wang et al. (2022  ###reference_b35###  ###reference_b35###) have converted standard NLP datasets designed for optimized channels into instruction-based formats for 76 different tasks.\nMoreover, free-form task instructions allow for nuanced control mechanisms, including explicit directives (such as specifying output formats or initiating reasoning processes) and subtle cues (such as inducing cognitive behaviors through few-shot examples). These aspects will be further explored in Section 4  ###reference_###  ###reference_### and summarized in Table 2  ###reference_###  ###reference_###.\nIn contrast, since other channels are set during the optimization process for specific tasks, they lack the flexibility for user-directed modifications. Channels that require adjustments, such as fine-tuning the LLM, adapter tuning, or prefix tuning, rely on supervised learning methods for configuration. Although prompting in AE-LLMs could, in theory, facilitate task adjustments at inference time without prior task-specific fine-tuning\u2014akin to AR-LLMs\u2019 prompting approach\u2014it often requires task-specific optimization to achieve effective channel performance. For example, techniques like Pattern Exploitation Training (PET) (Schick and Sch\u00fctze, 2021  ###reference_b28###  ###reference_b28###) utilize mathematical optimization to adapt models to specific patterns, whereas Auto-prompt (Shin et al., 2020  ###reference_b29###  ###reference_b29###) optimizes text patterns for language models. The question of whether this need for optimization arises from the inherent complexities of auto-encoding language models invites further research.\nThe focus here is on the understandability of the channels themselves to lay users, rather than their functional effectiveness, as this greatly influences the user experience. For example, the objective of an output layer is clear \u2014 transforming LLM representations into a specific output format. However, the process involving dense representations through matrix multiplication is not intuitively understandable to the non-specialist. Moreover, text patterns refined through AE-LLMs\u2019 Auto-prompting often lack the straightforwardness found in manually created prompts.\nThis analysis evaluates the conceptual load required to deploy  tasks using various channels, moving away from the parameter size metric, which is more pertinent to researchers and developers. Assuming each task is accommodable across all channels, we quantify the complexity as follows:\nFor fine-tuned LLMs, prefixes, adapters and output layers, each task-specific adjustment equates to a complexity of , with\n denoting the total number of tasks. Additionally,  text patterns are devised per task, resulting in a complexity of , where  represents the number of patterns per task. The complexity for verbal free-form context is considered negligible, as these are formulated spontaneously by users at the time of use.\nFrom this framework, we can deduce the complexity inherent to each deployment paradigm. For instance, LLM fine-tuning, which necessitates one LLM and one output layer per task, carries a complexity of ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluating Expressiveness of Modalities",
            "text": "During LLM fine-tuning and adapter tuning, the task-specific output layers strictly limit the range of possible outputs, hindering the potential for detailed expressiveness and, by extension, advanced cognitive behaviors. The output space is tightly defined, with actions or labels being pre-determined and given specific meanings through task-specific supervised learning. Nonetheless, certain probing techniques allow us to uncover the thought processes behind their predictions, a topic we will explore further in Section 4.1  ###reference_###.\nWhen it comes to AE-LLMs prompted with text patterns, these models are limited to generating only specific tokens or words, constrained by the patterns set in advance. These constraints, such as token positions and quantities dictated by the input patterns, along with the need for grammatical and coherent text completion, restrict the models\u2019 ability to articulate complex ideas, plans, and actions.\nOn the other hand, AR-LLMs\u2019 prompting capitalizes on their auto-regressive nature to produce unbounded, free-form text, influenced solely by the given input context. This capability is further demonstrated in Section 4  ###reference_### and summarized in Table 2  ###reference_###, showcasing the open-ended expressiveness unique to the AR-LLM prompting paradigm."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Cognitive Behaviors Under AR-LLMs\u2019 Prompting Paradigm",
            "text": "This section elucidates the capability of AR-LLM prompting paradigms to exhibit cognitive behaviors expressed by the free-form modalities by mainpulating the free-form channels. It\u2019s important to clarify that not every AR-LLM demonstrates cognitive behaviors\u2014smaller models like GPT-2 (Radford et al., 2019  ###reference_b25###) may not.\nSpecifically, we analyze four cognitive behaviors: thinking, reasoning, planning, and feedback learning, leaving the examination of their interrelationships for future research.\nUsing channels trained through task-specific supervised learning can achieve performances that rival or exceed human performance.\nNonetheless, they often struggle with generalizing to data from natural domain shifts, adversarial perturbations and debiased data, as summarized by Li et al., Li et al. (2023  ###reference_b17###). This limitation is consistently attributed to shortcut learning, such as classifying sentences containing the word \u201cNo\u201d as \u201ccontradiction\u201d in text entailment tasks (Wallace et al., 2019  ###reference_b33###; Du et al., 2021  ###reference_b5###).\nThe intriguing question arises whether task-specific channels can also develop System 2 \u2014 the fast system.\nWhile the limited expressiveness of task-specific outputs does not offer straightforward evidence, Li et al., Li and Liu (2023  ###reference_b16###) employ a technical probe (Sundararajan et al., 2017  ###reference_b32###) to reveal that indulgence in shortcut learning during task-specific training impedes the development of the slow system.\nWhile the mentioned research primarily examines the LLM fine-tuning paradigm, it\u2019s our contention that shortcut learning and the fast thinking are likely prevalent across all the parametric channels, including prefixes and adapters, trained on supervised datasets to some degree. This is attributed to the inherent characteristics of gradient descent optimization, as demonstrated by empirical findings in Li et al., Li and Liu (2023  ###reference_b16###).\nAnother empirical evidence shows that methods like prefix and adapter tuning, although more resilient, still notably falter under distribution shifts and adversarial attacks (Han et al., 2021  ###reference_b6###; Yang and Liu, 2022  ###reference_b41###).\nThe mitigated impact observed in prefix and adapter tuning is attributed to the fact that the underlying LLMs are not directly engaged as task-specific channels, as explored by (Han et al., 2021  ###reference_b6###). While we draw parallels between reliance on shortcuts and fast thinking within human cognition, some research within the NLP field argues that such dependency on shortcuts (dataset biases) detracts from the models\u2019 relevance to human-level cognition (Zhong et al., 2023  ###reference_b46###). This perspective arises from the view that the shortcuts might not reflect genuine human cognitive activities within the field of NLP.\nResearch findings (Si et al., 2023  ###reference_b31###; Zhang et al., 2022  ###reference_b44###) consistently indicate the difficulty of inducing fast thinking in AR-LLMs through prompting techniques. These models typically remain unfazed by various distributional shifts, such as domain shift and adversarial perturbations. Min et al., Min et al. (2022  ###reference_b21###) demonstrate that, even with few-shot demonstrations for in-context learning, the models tend to leverage the structure of these demonstrations to organize the generation rather than relying on simplistic input-to-label mappings for predictions.\nAdditionally, Raman et al., Raman et al. (2023  ###reference_b26###) show that PET prompting improve the AE-LLMs\u2019 ability to withstand adversarial attacks. Nonetheless, this enhanced robustness is somewhat restricted. The constrained effectiveness could be attributed to the dependency on task-specific channels inherent during the deployment of the PET prompting.\nThe remainder of this section will illustrate the capacity of AR-LLMs\u2019 prompting to replicate the human slow thinking process through the exhibition of effortful mental activities, as encapsulated in Table 2  ###reference_###.\nTwo primary contexts are employed to facilitate the creation of intermediate reasoning steps: incorporating a Chain of Thought (CoT) triggers in task instructions (zero-shot CoTs), such as \u201cLet\u2019s think step-by-step\u201d (Kojima et al., 2022  ###reference_b13###), within prompts, or integrating manually crafted reasoning steps in a few-shot learning context (few-shot CoTs) (Wei et al., 2022b  ###reference_b40###). To circumvent the manual compilation of few-shot demonstrations with reasoning sequences, Zhang et al.,\nZhang et al. (2023  ###reference_b45###) developed a method to automatically generate few-shot demonstrations by choosing several queries and utilizing zero-shot CoTs to craft reasoning sequences for each query (Auto CoTs).\nGiven that simple greedy decoding (producing a single chain) is prone to error accumulation in intermediate steps, Wang et al.,\nWang et al. (2023b  ###reference_b37###)\npropose generating multiple chains and consolidating them through majority voting, thereby enhancing model accuracy in both scenarios (CoTs-SC).\nSimilar to the activation of reasoning processes, the process of planning can be prompted through the inclusion of specific planning cues in zero-shot scenarios, such as the prompt \u201clet\u2019s carry out the plan\u201d (Wang et al., 2023a  ###reference_b36###), or through the demonstration of planning steps in few-shot examples (Huang et al., 2022  ###reference_b9###). Experimental findings indicate that instructions tailored to tasks significantly enhance the performance of LLMs on various tasks. For instance, directives like \u201cpay attention to calculation\u201d (Hao et al., 2023  ###reference_b7###) or \u201cidentify key variables and their corresponding figures to formulate a plan\u201d (Wang et al., 2023a  ###reference_b36###) have been shown to improve outcomes in tasks requiring numerical reasoning.\nThis ability is essential for addressing problems requiring a series of decisions, especially when deploying LLMs in open-world scenarios like robotics. In such environments, tasks typically need physical actions (grounded), involve translating broad objectives into actionable steps (high-level), and present a vast range of possible actions (open-ended). Research has demonstrated the effectiveness of LLMs in deconstructing complex goals into actionable sequences within such dynamic environments, as seen in projects like ALFWorld (Yao et al., 2023b  ###reference_b43###), VirtualHome (Huang et al., 2022  ###reference_b9###), and Minecraft (Wang et al., 2023c  ###reference_b38###).\nAn example from ALFWorld illustrates this: achieving the objective of \u201cexamining paper under desklamp\u201d necessitates LLMs to devise practical plans (e.g., initially approaching the coffee table, then acquiring the paper and utilizing the desklamp) and subsequently generate textual instructions for execution in real-world settings.\nLLMs are adept at generating feedback by reflecting on their previously given responses or observations from interactions with the external environment. There are two primary scenarios for feedback generation:\n1) Feedback based on previous actions and external feedback: In the work by Yao et al., Yao et al. (2023b  ###reference_b43###), LLMs engaging with a Wikipedia API to search for entities that do not exist, such as \u201cSearch[goddess frigg]\u201d, may encounter a 404 error, delivered in JSON format. In response, LLMs can articulate feedback about the error related to their action, such as stating, \u201cCould not find goddess frigg.\u201d.\n2) Feedback based solely on prior responses:\nThis approach is relevant in various situations where external environmental feedback is absent (Shinn et al., 2023  ###reference_b30###; Hao et al., 2023  ###reference_b7###). In such cases, LLMs can give feedback on previous answers by applying certain evaluation metrics, such as determining the relevance of a sub-question to a broader question requiring intricate, multi-step reasoning (Hao et al., 2023  ###reference_b7###). This process involves using a prompt that asks the LLM to judge the utility of a sub-question in addressing the main question (Prompt: \u201cGiven a question, assess if the subquestion aids in solving the original question. Answer \u2019Yes\u2019 or \u2019No\u2019. Question: {goal}; Subquestion: {action}. Is the subquestion useful?\u201d).\nFurthermore, feedback may be presented as numerical scores, such as the confidence scores (normalized logits) for \u2019Yes\u2019 or \u2019No\u2019 answers, instead of in verbal form. The decision to use numerical rather than verbal feedback is contingent on the specific requirements of the feedback mechanism, as explored in subsequent discussions.\nAfter generating feedback, Shinn et al., Shinn et al. (2023  ###reference_b30###) directly include verbal feedback LLMs have produced to enhance the accuracy of their responses or decisions.\nMeanwhile, Hao et al., Hao et al. (2023  ###reference_b7###) detail a methodology where numerical feedback serves as a reward system for guiding LLMs for action selection. Following this, the LLMs function as world models to predict the subsequent state of state-action pairs during the planning phase, utilizing Monte-Carlo Tree Search (MCTS) to achieve this aim. Instead of allowing LLMs to directly process the feedback, an implicit feedback learning strategy is employed where a feedback loop is deliberately established to influence the sequence of actions undertaken by the LLMs via the update of state values during the propagation phase of MCTS."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Reasoning",
            "text": "Reasoning is a thinking process to conclusions or decisions with the sequential and interconnected nature, i.e., chain-of-thoughts (CoTs) (Wei et al., 2022b  ###reference_b40###). This is the most common definition in the NLP/LLM are to investigate the LLMs\u2019 reasoning ability.\nWith a reasoning path in free-form modality, models can better solve complicated tasks requiring multi-step reasoning compared to the conclusion without CoTs. As an illustration, Wei et al., Wei et al. (2022b  ###reference_b40###) substantially boosts model efficacy in solving mathematical reasoning bechmarks.\nReasoning is defined as the process of arriving at conclusions or decisions through a sequential and interconnected series of thoughts, often referred to as a chain-of-thoughts (CoTs) (Wei et al., 2022b  ###reference_b40###). This definition is widely accepted in the field of Natural Language Processing (NLP) for exploring the reasoning capabilities of LLMs. By employing a reasoning path via the modality of free-form text, models are more adept at tackling complex tasks that necessitate multi-step reasoning, as opposed to reaching conclusions without the aid of CoTs.\nTechnically, the auto-regressive nature employs the thoughts or intermediate steps generated as the prior for generating subsequent thoughts and, ultimately, the final predictions.\nTwo primary contexts are employed to facilitate the creation of intermediate reasoning steps: incorporating a Chain of Thought (CoT) triggers in task instructions (zero-shot CoTs), such as \u201cLet\u2019s think step-by-step\u201d (Kojima et al., 2022  ###reference_b13###  ###reference_b13###), within prompts, or integrating manually crafted reasoning steps in a few-shot learning context (few-shot CoTs) (Wei et al., 2022b  ###reference_b40###  ###reference_b40###). To circumvent the manual compilation of few-shot demonstrations with reasoning sequences, Zhang et al.,\nZhang et al. (2023  ###reference_b45###  ###reference_b45###) developed a method to automatically generate few-shot demonstrations by choosing several queries and utilizing zero-shot CoTs to craft reasoning sequences for each query (Auto CoTs).\nGiven that simple greedy decoding (producing a single chain) is prone to error accumulation in intermediate steps, Wang et al.,\nWang et al. (2023b  ###reference_b37###  ###reference_b37###)\npropose generating multiple chains and consolidating them through majority voting, thereby enhancing model accuracy in both scenarios (CoTs-SC)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Planning",
            "text": "Planning involves the forethought and organization of actions or steps to achieve a predetermined objective. This process fundamentally requires a comprehension or representation of the environment and involves breaking down tasks into smaller, manageable subgoals.\nIt represents a key cognitive behavior modeled within the fields of AI. Typical planning methods break down tasks into subgoals through explicit symbolic representation (Russell and Norvig, 2010  ###reference_b27###). For instance, partial-order planning ensures the logical sequencing of actions by modeling actions, preconditions, effects, and the relations among actions in such a way that actions are logically sequenced to meet the goal\u2019s preconditions.\nDiffering from traditional approaches that rely on explicitly modeled knowledge and reasoning mechanisms, LLMs leverage their inherent knowledge and inferential capabilities to mimic planning. They do this by producing text sequences that suggest a logical progression of steps or actions directed towards an objective (Hao et al., 2023  ###reference_b7###; Wang et al., 2023a  ###reference_b36###; Huang et al., 2022  ###reference_b9###). This skill stems from the models\u2019 proficiency in forecasting the subsequent most likely word sequence based on a context indicative of planning or reasoning processes.\nSimilar to the activation of reasoning processes, the process of planning can be prompted through the inclusion of specific planning cues in zero-shot scenarios, such as the prompt \u201clet\u2019s carry out the plan\u201d (Wang et al., 2023a  ###reference_b36###  ###reference_b36###), or through the demonstration of planning steps in few-shot examples (Huang et al., 2022  ###reference_b9###  ###reference_b9###). Experimental findings indicate that instructions tailored to tasks significantly enhance the performance of LLMs on various tasks. For instance, directives like \u201cpay attention to calculation\u201d (Hao et al., 2023  ###reference_b7###  ###reference_b7###) or \u201cidentify key variables and their corresponding figures to formulate a plan\u201d (Wang et al., 2023a  ###reference_b36###  ###reference_b36###) have been shown to improve outcomes in tasks requiring numerical reasoning.\nThis ability is essential for addressing problems requiring a series of decisions, especially when deploying LLMs in open-world scenarios like robotics. In such environments, tasks typically need physical actions (grounded), involve translating broad objectives into actionable steps (high-level), and present a vast range of possible actions (open-ended). Research has demonstrated the effectiveness of LLMs in deconstructing complex goals into actionable sequences within such dynamic environments, as seen in projects like ALFWorld (Yao et al., 2023b  ###reference_b43###  ###reference_b43###), VirtualHome (Huang et al., 2022  ###reference_b9###  ###reference_b9###), and Minecraft (Wang et al., 2023c  ###reference_b38###  ###reference_b38###).\nAn example from ALFWorld illustrates this: achieving the objective of \u201cexamining paper under desklamp\u201d necessitates LLMs to devise practical plans (e.g., initially approaching the coffee table, then acquiring the paper and utilizing the desklamp) and subsequently generate textual instructions for execution in real-world settings."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Feedback Learning",
            "text": "As Kahneman et al., Kahneman (2011  ###reference_b12###) elucidates, although System 1 may rush to judgments that are biased or erroneous, System 2 has the capacity to identify and rectify these mistakes through introspection on the rapid decisions made by System 1. Similarly, LLMs have shown the ability to mimic this aspect of human cognition.\nLLMs are adept at generating feedback by reflecting on their previously given responses or observations from interactions with the external environment. There are two primary scenarios for feedback generation:\n1) Feedback based on previous actions and external feedback: In the work by Yao et al., Yao et al. (2023b  ###reference_b43###  ###reference_b43###), LLMs engaging with a Wikipedia API to search for entities that do not exist, such as \u201cSearch[goddess frigg]\u201d, may encounter a 404 error, delivered in JSON format. In response, LLMs can articulate feedback about the error related to their action, such as stating, \u201cCould not find goddess frigg.\u201d.\n2) Feedback based solely on prior responses:\nThis approach is relevant in various situations where external environmental feedback is absent (Shinn et al., 2023  ###reference_b30###  ###reference_b30###; Hao et al., 2023  ###reference_b7###  ###reference_b7###). In such cases, LLMs can give feedback on previous answers by applying certain evaluation metrics, such as determining the relevance of a sub-question to a broader question requiring intricate, multi-step reasoning (Hao et al., 2023  ###reference_b7###  ###reference_b7###). This process involves using a prompt that asks the LLM to judge the utility of a sub-question in addressing the main question (Prompt: \u201cGiven a question, assess if the subquestion aids in solving the original question. Answer \u2019Yes\u2019 or \u2019No\u2019. Question: {goal}; Subquestion: {action}. Is the subquestion useful?\u201d).\nFurthermore, feedback may be presented as numerical scores, such as the confidence scores (normalized logits) for \u2019Yes\u2019 or \u2019No\u2019 answers, instead of in verbal form. The decision to use numerical rather than verbal feedback is contingent on the specific requirements of the feedback mechanism, as explored in subsequent discussions.\nAfter generating feedback, Shinn et al., Shinn et al. (2023  ###reference_b30###  ###reference_b30###) directly include verbal feedback LLMs have produced to enhance the accuracy of their responses or decisions.\nMeanwhile, Hao et al., Hao et al. (2023  ###reference_b7###  ###reference_b7###) detail a methodology where numerical feedback serves as a reward system for guiding LLMs for action selection. Following this, the LLMs function as world models to predict the subsequent state of state-action pairs during the planning phase, utilizing Monte-Carlo Tree Search (MCTS) to achieve this aim. Instead of allowing LLMs to directly process the feedback, an implicit feedback learning strategy is employed where a feedback loop is deliberately established to influence the sequence of actions undertaken by the LLMs via the update of state values during the propagation phase of MCTS."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Bridging LLM Deployment Gaps with Insights from Cognitive Behaviors",
            "text": "This section investigates how insights into cognitive behaviors can aid in addressing the tuning and deployment challenges faced by LLMs operating as autonomous agents and within multi-agent systems.\nInstead of relying on explicit contextual cues to trigger advanced cognitive functions, an intelligent system is expected to independently engage in reasoning, planning, and decision-making as it interacts with the external world\u2014for instance, by seeking input from humans or utilizing available tools. To foster such autonomous behaviors, various algorithms aim to tune LLMs for independently exhibiting behaviors that align with human cognitive processes. For instance, Liu et al., Liu et al. (2023  ###reference_b19###) have developed techniques for instruction tuning that facilitates autonomous reasoning.\nYet, the challenge remains in creating instructional data that encapsulates higher-order cognitive functions. A pivotal question emerges:\nHow can various cognitive behaviors be encapsulated within free-form text (instruction data)? Addressing this question is crucial for ensuring that the data used for tuning mirrors human cognitive processes, thereby making the resulting model actions more human-like. Unraveling this issue might necessitate insights from both cognitive psychology and linguistics.\nAnother approach to tuning involves the use of reliable reward models, such as reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022  ###reference_b23###) and behavior cloning (Nakano et al., 2021  ###reference_b22###).\nMany studies (Ouyang et al., 2022  ###reference_b23###; Nakano et al., 2021  ###reference_b22###) develop reward models based on comparisons of model-generated responses, with human evaluators ranking these responses.\nAn unresolved inquiry remains:\nHow can reward models be devised to truly reflect human cognitive preferences?\nIn exploring planning and feedback learning strategies, it becomes evident that multi-agent systems are designed to facilitate interactions between LLMs and the external world, as well as among LLMs themselves. Examples include LLMs acting as both evaluators and actors in feedback learning environments (Shinn et al., 2023  ###reference_b30###) or taking on roles as evaluators, actors, and environmental simulators in planning scenarios (Hao et al., 2023  ###reference_b7###).\nSuch setups necessitate more than a one-time interaction between LLMs and users, requiring instead sustained, multi-turn dialogues.\nChallenges emerge within these complex interactions: How do agents process and integrate information from other agents and their own previous dialogues? How can environmental data be stringified into a format understandable by LLMs?\nWhat strategies can simplify the management of extended sequences of interaction trajectories?\nDespite the recent development of various LLM-based multi-agent systems, there remains an absence of a unified framework across these models.\nExploring such framework serves as a key drive for this research on a cognitive framework.\nOrganizing LLM-based agents by their cognitive behaviors offers a pathway to this unification. For instance, as indicated in Table 2  ###reference_###, the ReAct framework (Yao et al., 2023b  ###reference_b43###) employs a reasoning agent for decision-making, whereas the RAP framework (Hao et al., 2023  ###reference_b7###) utilizes a reasoning agent for decision-making alongside a feedback learning agent for evaluation.\nDelving deeper into the relationships between cognitive behaviors might benefit from insights in cognitive psychology. Taking the concept of Self-Regulated Learning (SRL) as defined by Zimmerman (2000  ###reference_b47###), planning and feedback learning are intertwined, enhancing the learning process.\n\u201cSelf-regulated learning refers to self-generated thoughts, feelings, and actions that are planned and cyclically adapted to the attainment of personal goals\u201d\nThis rationale lays the groundwork for future efforts to combine planning frameworks (like RAP) with feedback learning frameworks (such as Reflexion).\nFurthermore, the structure of certain inference models showcases their interconnections and the presence of common detailed components.\nFor instance, both feedback learning frameworks, exemplified by Reflexion Shinn et al. (2023  ###reference_b30###), and planning frameworks, such as RAP Hao et al. (2023  ###reference_b7###), incorporate a common detailed module: a feedback learning agent. This agent plays a pivotal role in decision-making by facilitating the selection of appropriate actions from an assortment of tools and environments.\nIt is evident that the interconnection within the intricate inference framework can be examined, especially at the level of cognitive behavior.\nOur comprehensive analysis aims to pave the way for the systematic creation of multi-agent LLM-based frameworks, or inversely, to stimulate research in cognitive psychology."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In summary, our analysis seeks to inspire further research in AI, within the domain of language intelligence and beyond, to move away from heavily optimized task-specific channels. Instead, we advocate for the adoption of natural and free-form modalities throughout the pretraining phase via self-supervised learning, followed by straightforward inference-time deployment that eschews the necessity for mathematically optimizing task-specific channels.\nWe developed an analytical framework to examine the deployment of LLMs to reach the conclusion.\nBesides, the auto-regressive nature of free-form modalities, leveraged during pretraining, enhances the capacity for exhibiting a range of human-like cognitive behaviors by utilizing the free-form channel. It is important to clarify that we do not advocate that LLMs possess conscious thought. Rather, our findings illustrate how LLMs, such as ChatGPT, can imitate the outcomes of human cognitive activities via the free-form modality given suitable verbal context.\nLastly, we highlight the opportunity to address challenges in LLM deployment through the integration of cognitive behavior concepts."
        }
    ],
    "url": "http://arxiv.org/html/2405.10474v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.10474v1",
        "paper_title": "Rethinking ChatGPT\u2019s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs\u2019 Prompting",
        "research_background": "Based on the introduction and related work sections provided, here is an overview of the paper's motivation, research problem, and relevant prior work:\n\n### Motivation:\nThe paper is motivated by the recognition of ChatGPT's widespread success and popularity as a representative of Large Language Models (LLMs). This success is often attributed to the extensive scaling of transformer-based neural networks and vast amounts of pre-training data. However, there's a noticeable trend towards creating specialized models tailored to specific tasks rather than utilizing a single, universal model. This trend has raised questions and challenges related to task customizability, transparency, user-level complexity, and the demonstration of advanced cognitive behaviors. The goal of this paper is to delve into the mechanisms behind these deployment paradigms, particularly focusing on auto-regressive LLMs (AR-LLMs) and their prompting strategies, which are foundational to ChatGPT.\n\n### Research Problem:\nThe research problem the paper addresses is the gap in understanding how different deployment paradigms of LLMs, especially AR-LLMs, influence the usability and cognitive behaviors exhibited by these models. There is a need to explore why specialized models often fall short in demonstrating advanced human-like cognitive behaviors and how AR-LLMs can potentially overcome these shortcomings. Specifically, the paper seeks to:\n1. Investigate the principles and effectiveness of different LLM deployment paradigms.\n2. Evaluate why certain models, despite their specialization, struggle with task customizability and advanced cognitive behaviors.\n3. Highlight how AR-LLMs\u2019 prompting strategies can exhibit more natural and complex human-like cognitive behaviors.\n\n### Relevant Prior Work:\n1. **Scaling and Pre-training of Transformer-based Models**:\n   - Radford et al. (2019), Brown et al. (2020): Highlighting the importance of scale in transformer-based neural networks and vast pre-training data in achieving high performance.\n   \n2. **Benchmarks and Evaluation of LLMs**:\n   - Wang et al. (2019): Discussing benchmarks like GLUE, which are used to evaluate linguistic intelligence across various tasks, contributing to the trend of developing specialized models.\n   \n3. **Language Modeling Approaches**:\n   - Jin et al. (2020): Auto-encoding LMs (AE-LMs) exemplified by BERT.\n   - Radford et al. (2018), Brown et al. (2020): Auto-regressive LMs (AR-LMs) exemplified by the GPT series.\n   \n4. **Cognitive Behaviors and Slow Thinking**:\n   - Kahneman (2011): Introducing the concept of slow thinking, where models exhibit iterative reasoning processes similar to human cognitive behaviors.\n\n### Summary:\nThe paper is driven by the need to reassess the deployment and effectiveness of LLMs, particularly ChatGPT, in terms of usability and cognitive behaviors. By scrutinizing auto-regressive prompting strategies, the paper aims to uncover how these strategies facilitate more natural and sophisticated human-like interactions, addressing pitfalls associated with other model specializations. Noteworthy prior works are integrated to contextualize discussions and support the investigation of how AR-LLMs can better demonstrate cognitive behaviors in diverse, real-life scenarios.",
        "methodology": "The methodology section of this paper describes a novel approach to understand and improve ChatGPT's success by examining both usability and cognitive behaviors enabled by auto-regressive large language models (LLMs). \n\n### Key Components:\n\n1. **Dual Objectives**:\n   - **Modeling Joint Probability Distribution**: The primary goal is to estimate the joint probability distribution of text sequences. \n   - **Relevance of Generated Text**: The model aims to produce text that properly aligns with the given context.\n\n2. **Novel Framework**:\n   - **Data Modalities**: The framework defines two types of data modalities that are integral to understanding language. \n   - **Processing Channels**: Six unique channels are described for processing these modalities.\n\n3. **Self-supervised Learning**:\n   - Utilizes extensive unlabeled text corpora for training, as explored in previous studies (e.g., Liu et al., 2019; Wei et al., 2022a).\n\n4. **Token Prediction**:\n   - **Predictive Mechanism**: Auto-regressive models predict the next token in a sequence based on preceding tokens using the chain rule. \n   - **Conditional Probabilities**: This method is articulated as a product of conditional probabilities for sequential tokens.\n\n5. **Auto-encoding Language Modeling**:\n   - **Noise Introduction**: Noise is introduced to input sequences for robust learning.\n   - **Masking Strategies**: \n     - **Token-level Masking**: Random tokens in text are masked and the model is trained to predict the masked tokens (Devlin et al., 2019).\n     - **Reconstruction Loss**: Measures the discrepancy between original and reconstructed sequences.\n    \n### Key Innovations:\n\n1. **Characterization via Novel Framework**: Proposes a unique framework for detailed characterization of deployment paradigms using specific data modalities and processing channels.\n  \n2. **Relaxed Formulations for LLMs**: Introduces two distinct yet relaxed formulations for constructing LLMs to better leverage self-supervised learning methodologies.\n\n3. **Diverse Denoising Techniques**: Explores not just token-level masking but also span-level masking (Joshi et al., 2020) and text infilling (Lewis et al., 2020) among other denoising techniques, which provide more flexibility and robustness.\n\nThis comprehensive approach helps in understanding how intrinsic design factors of language models influence their usability and cognitive behaviors, contributing to the success of tools like ChatGPT.",
        "main_experiment_and_results": "The main experiment in the paper investigates the capability of auto-regressive large language models (AR-LLMs) to exhibit cognitive behaviors \u2014 specifically thinking, reasoning, planning, and feedback learning \u2014 through the use of prompting techniques. The setup and results can be summarized as follows:\n\n### Datasets:\nThe study doesn't focus on specific datasets but rather on methodological approaches to eliciting cognitive behavior through prompting. It references various domain tasks, including textual entailment, numerical reasoning, and open-world scenarios such as robotic planning tasks (e.g., projects like ALFWorld, VirtualHome, and Minecraft).\n\n### Baselines:\nThe baseline comparison involves conventional task-specific supervised learning methods and techniques like prefix and adapter tuning. These methods are recognized for high performance but often fail to generalize under natural domain shifts, adversarial perturbations, and debiased datasets due to reliance on shortcut learning.\n\n### Evaluation Metrics:\n- **Accuracy**: Particularly for tasks with intermediate reasoning steps and planning.\n- **Robustness**: Ability to withstand distribution shifts and adversarial attacks.\n- **Effectiveness of Feedback**: Measured through the incorporation of feedback in decision-making processes.\n\n### Main Experimental Results:\n1. **Intermediate Reasoning Steps**:\n   - **Chain of Thought (CoT) triggers**: Using prompts like \"Let\u2019s think step-by-step\" (zero-shot CoTs) or manually crafted reasoning steps (few-shot CoTs) significantly enhance model accuracy by reducing error accumulation.\n   - **Auto CoTs**: Automatically generating few-shot demonstrations to bypass manual compilation, further streamlining the reasoning process.\n\n2. **Planning**:\n   - **Zero-shot and Few-shot Planning Prompts**: Commands like \u201clet\u2019s carry out the plan\u201d in zero-shot scenarios or demonstrating planning steps in few-shot examples improved performance on tasks requiring sequential decisions.\n\n3. **Feedback Learning**:\n   - **Feedback Based on Previous Actions**: Adjustments based on interactions with external environments, such as handling API errors.\n   - **Feedback Based Solely on Prior Responses**: Evaluating the utility of sub-questions for broader questions and employing numerical feedback as confidence scores to enhance decision accuracy.\n\n4. **Error Reduction through Majority Voting**:\n   - **Generating Multiple Answer Chains**: By consolidating multiple chains via majority voting, the overall accuracy in tasks involving complex intermediate steps improved (CoTs-SC).\n\n5. **Task Performance in Dynamic Environments**:\n   - **Actionable Sequence Decomposition**: Effectively breaking down complex goals into actionable steps within environments like ALFWorld, improving task execution through detailed planning and step-wise instructions.\n\n### Key Observations:\n- Task-specific channels often fall prey to shortcut learning, which impedes the development of more advanced cognitive processing systems.\n- Prefix and adapter tuning offer some resilience against distribution shifts but still struggle under certain conditions.\n- Methods encouraging articulated reasoning and planning (\"Let\u2019s think step-by-step\" prompts) significantly enhance performance, particularly in tasks requiring sequential reasoning and planning.\n\nThis section substantiates that with the appropriate prompting methods, AR-LLMs can replicate cognitive behaviors comparable to human slow thinking through robust intermediate reasoning, planning, and feedback mechanisms."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the complexity and usability of different LLM deployment channels, particularly focusing on the applicability of free-form contexts in zero-shot learning.",
            "experiment_process": "In this study, the authors compare the usability of LLM deployment channels. They use free-form context and explicit task instructions in zero-shot learning settings, inspired by datasets converted by Wang et al. for 76 tasks. The channels compared include fine-tuning, adapter tuning, and prefix tuning, relying on supervised learning methods for configuration. They also reference techniques such as PET and Auto-prompt to illustrate optimization challenges in auto-encoding LLMs.",
            "result_discussion": "The analysis highlights that free-form task instructions enable nuanced control and cognitive behaviors through few-shot examples, lacking in predefined channels. They find that the complexity of deploying tasks with techniques like LLM fine-tuning is higher compared to the spontaneous formation of verbal free-form contexts, which are more intuitive for users.",
            "ablation_id": "2405.10474v1.No1"
        },
        {
            "research_objective": "To assess the expressiveness of different LLM modalities and their impact on advanced cognitive behaviors through prompting techniques.",
            "experiment_process": "The authors evaluate the expressiveness of LLMs during fine-tuning, adapter tuning, and auto-encoding LLMs prompted with text patterns. They contrast these methods with auto-regressive LLMs prompted to produce unbounded, free-form text. The task-specific output layers in fine-tuning and adapter tuning limit the outputs and expressiveness, while AR-LLMs leverage their auto-regressive abilities for more open-ended creation.",
            "result_discussion": "The results suggest that AR-LLMs exhibit a superior capability for unbounded, free-form text generation, driven by input context alone, compared to the constrained expressiveness in fine-tuning and adapter tuning channels. This open-ended expressiveness is unique and crucial for advanced cognitive behaviors, positioning AR-LLMs as more versatile.",
            "ablation_id": "2405.10474v1.No2"
        }
    ]
}