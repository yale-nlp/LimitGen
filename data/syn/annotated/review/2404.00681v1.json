{
    "title": "CoUDA: Coherence Evaluation via Unified Data Augmentation",
    "abstract": "Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models.\nDue to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models.\nHowever, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance.\nIn this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively.\nEspecially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples.\nDuring inference, CoUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse.\nExtensive experiments in coherence evaluation show that, with only 233M parameters, CoUDA achieves state-of-the-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics. 111 https://github.com/dwzhu-pku/CoUDA",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Coherence is a vital aspect of communication that evaluates the structure and organization of discourse (Halliday and Hasan, 1976  ###reference_b10###; Grosz and Sidner, 1986  ###reference_b9###). Consequently, models capable of evaluating coherence of the given text are widely applicable in both discourse generation and assessment. While recent large language models show strong performance in various tasks (brown2020language), they have not presented superiority in coherence evaluation compared with the fine-tuning based models (Fu et al., 2023  ###reference_b7###). Considering both computational efficiency and evaluation performance a good evaluation metric should possess, in this paper, we focus on modeling coherence via a fine-tuning based lightweight model.\n###figure_1### Due to the scarcity of human-annotated data, data augmentation techniques are commonly employed in training a coherence evaluation model\n (Li and Jurafsky, 2017  ###reference_b13###; Jwalapuram et al., 2022  ###reference_b11###).\nAs human-written discourses naturally possess coherence and can thus serve as positive samples, previous research has focused on constructing negative samples, primarily through rule-based methods such as swapping or shuffling sentences (Barzilay and Lapata, 2008  ###reference_b3###; Shen et al., 2021  ###reference_b23###; Jwalapuram et al., 2022  ###reference_b11###).\nHowever, as these methods are heuristically inspired without any design criteria as guidance, they suffer from weak correlation with human judgements (Mohiuddin et al., 2021  ###reference_b17###).\nThis brings up the research question: To effectively model coherence, can we find reasonable criterium as guidance to design augmentation strategies?\nAccording to Grosz and Sidner (1986  ###reference_b9###), discourse coherence is mainly determined by two aspects: the organization of discourse segments (i.e. global coherence), and the transition of attention or focused items (i.e. local coherence). Examples for these two aspects of coherence are presented in Figure 1  ###reference_###.\nThis inspires us to the designing criteria that a good data augmentation strategy should uniformly cover these two aspects of coherence.\nFollowing the criteria, we propose a Coherence evaluation framework via Unified Data Augmentation, namely CoUDA, which unifies both global and local aspects of coherence throughout training and inference phase.\nCoUDA involves global and local augmentation to capture the corresponding aspects of coherence. Regarding global augmentation, we construct negative samples through shuffling, which disrupts the original order of the sentences to induce global incoherence. For local augmentation, our target is to construct negative samples that contain sentences incoherent with the context. While prior rule-based methods, such as swapping a sentence with another from a different text (Shen et al., 2021  ###reference_b23###), can also introduce local incoherence, their constructed samples often lack diversity and complexity, potentially failing to capture nuanced aspects of local coherence. To address this, we propose a novel generative augmentation strategy that involves post-pretraining a generative model, and applying two controlling mechanisms to manipulate the difficulty of generated samples.\nBy sampling from a generative model, and applying difficulty control, we construct high-quality negative samples to disrupt local coherence.\nFinally, in inference phase,\nwe design a unified scoring strategy to incorporate both aspects of coherence for overall assessment.\nWhile previous research on coherence evaluation has traditionally adhered to a pairwise ranking setup, we have pioneered a pointwise coherence scoring setting that we believe is more relevant in real-world scenarios. On SummEvalFabbri et al. (2021  ###reference_b6###), our CoUDA exhibits remarkable improvements in pointwise scoring compared to prior methods, including GPT-4-based metrics. Despite not being specifically tailored for pairwise ranking, our model outperforms previous ranking models on both the INSteD-CNN and INSteD-Wiki datasets Shen et al. (2021  ###reference_b23###). Furthermore, CoUDA is a lightweight model with only 233M parameters.\nTo sum up, our contributions are as follows:\nWe propose CoUDA, a data augmentation framework inspired by linguistic theory of discourse structure, which uniformly models both global and local coherence aspects of a discourse.\nWe propose a novel generative augmentation strategy, which utilizes the power of the pretrained language model via post-pretraining and two mechanisms for sample difficulty control.\nComprehensive experiments in coherence evaluation show CoUDA with only 233M parameters achieves SOTA performance, even surpassing GPT-3.5 and GPT-4 based metrics."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "CoUDA Framework",
            "text": "###figure_2### In this section, we introduce our CoUDA framework, as illustrated in Figure 2  ###reference_###. First, we use global and local augmentation to create negative samples that have relatively poor global coherence and local coherence, respectively. To be specific, we use sentence shuffling for global augmentation, and design a generative strategy for local augmentation. Our generative strategy involves post-pretraining a generative model, and applying two controlling mechanisms to control the difficulty of generated samples. Then we combine the constructed negative samples with the original discourses, which serves as positive samples, to train our metric model for coherence/incoherence classification. In inference phase, we utilize a unified scoring strategy to incorporate global and local coherence for overall assessment.\nGiven a discourse that contains multiple sentences , the goal of a coherence evaluator  is to assess its degree of coherence by a logit score  (the higher the better).\nIdeally,  represents that  is perfectly coherent, while  indicates the opposite. Different from previous work that additionally relies on references  (Zhao et al., 2022  ###reference_b32###) or source inputs  (Zhong et al., 2022  ###reference_b33###), we evaluate coherence in this more concise framework that solely takes the discourse as the input.\nThat is more appropriate for evaluation as coherence is an intrinsic quality of a discourse.\nData augmentation aims to artificially create additional training samples by manipulating existing data.\nFor a discriminative setting, we need both positive and negative samples for training.\nIn terms of coherence evaluation, since a natural discourse  is intrinsically coherent, we focus on applying data augmentation to construct negative samples, i.e. incoherent samples . Afterwards, the created incoherent discourses  and the original discourse  respectively serve as negative and positive samples to train .\nIn the following, we introduce how we construct our two types of negative samples via global augmentation and local augmentation  in details.\nGiven discourse , we uniformly sample  from \u2019s non-opening and non-closing sentences.\nNext, we train a text generation model  by learning to reconstruct  based on the leftover discourse .\nFollowing recent popular text generation paradigm, this can be done by selecting  as a transformer-based sequence-to-sequence model and maximizing the likelihood of  autoregressively.\nWe also notice that Gap Sentences Generation (GSG), the pretraining task of PEGASUS (Zhang et al., 2020  ###reference_b31###), takes the similar form of\nreconstructing sentences.\nBut we cannot directly apply PEGASUS as G, because GSG is specially designed for summarization, which requires predicting multiple salient sentences in the discourse.\nBy contrast, our sentence reconstruction task aims to capture the coherence relation between an arbitrary sentence  and the leftover discourse.\nIn practice, we also find  generated by PEGASUS often serve as summaries of the leftover discourse, rather than being coherent with it.\nThus, instead of directly applying PEGASUS, we leverage this similarity of tasks and use PEGASUS for initialization.\nIn this way, we inherit the effectiveness of pretrained model.\nAfter the generative augmentor is trained, we use it to predict  with two controlling mechanisms:\nDue to the strong generation ability of generative augmentor,  may be highly coherent with , which is not the negative sample we expect.\nTo ensure  to convey the local incoherence, we develop a context truncation mechanism to restrict the model\u2019s generation to only partially coherent with the context.\nSpecifically, given  with  masked, we randomly choose to truncate the context before or after the mask token, i.e., the input for our generative augmentor is either  or . Take the former as an example, without information from subsequent text, the model is only able to generate predictions that are coherent with preceding text.\nIn addition to context truncation, we also perform coherence filtering to remove negative samples that are too easy. We utilize UniEval (Zhong et al., 2022  ###reference_b33###) to score the coherence of each sample and eliminates samples with coherence scores below a filtering threshold .\nWe combine the original discourses with negative samples constructed via global and local augmentation to train our metric model, as illustrated in Figure 2  ###reference_###(b).\nWe utilize the classification setup, based on findings (Steen and Markert, 2022  ###reference_b24###) that indicate its superior performance in coherence evaluation and downstream tasks, as opposed to the commonly used pairwise ranking setup. Specifically, we train our metric model to distinguish each sample as coherent or incoherent through binary cross entropy loss. For implementation details, please refer to Appendix A  ###reference_###.\nFor a comprehensive evaluation of discourse coherence, our CoUDA further includes a unified scoring strategy, as presented in Figure 2  ###reference_###(c). Specifically, our model first assigns a score conditioned on the whole discourse to represent its global coherence level:\nThen, since global scoring may fail to effectively capture the fine-grained coherence between sentences, we extract consecutive sentence pairs  from the discourse and have our model evaluate the inter-sentential coherence  of each pairs, where :\nNotably, although our model is trained for scoring the whole discourse, rather than sentence pairs, the training data includes discourse samples with only two sentences. As a result, our model can generalize to scoring sentence pairs as well. Afterwards, we obtain local coherence score for discourse  by averaging each sentence pair\u2019s coherence score:\nThe global and local scores are then combined via interpolation to form the overall coherence score:\nwhere  controls the weight. This unified design also aligns with the coherence rating process of human readers, who consider both discourse organization as a whole, and smooth transitions of focused items between adjacent sentences."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Preliminaries",
            "text": "Given a discourse that contains multiple sentences , the goal of a coherence evaluator  is to assess its degree of coherence by a logit score  (the higher the better).\nIdeally,  represents that  is perfectly coherent, while  indicates the opposite. Different from previous work that additionally relies on references  (Zhao et al., 2022  ###reference_b32###  ###reference_b32###) or source inputs  (Zhong et al., 2022  ###reference_b33###  ###reference_b33###), we evaluate coherence in this more concise framework that solely takes the discourse as the input.\nThat is more appropriate for evaluation as coherence is an intrinsic quality of a discourse.\nData augmentation aims to artificially create additional training samples by manipulating existing data.\nFor a discriminative setting, we need both positive and negative samples for training.\nIn terms of coherence evaluation, since a natural discourse  is intrinsically coherent, we focus on applying data augmentation to construct negative samples, i.e. incoherent samples . Afterwards, the created incoherent discourses  and the original discourse  respectively serve as negative and positive samples to train .\nIn the following, we introduce how we construct our two types of negative samples via global augmentation and local augmentation  in details."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Global Augmentation",
            "text": "To construct samples that have relatively poor global coherence, we disrupt the original appropriate organization of sentences in .\nConcretely, we shuffle the order of sentences in  to effectively disrupt its global coherence. As\nillustrated in Figure 2  ###reference_###(a), by shuffling , we can construct a negative sample ."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Local Augmentation",
            "text": "Local augmentation aims to construct samples with relatively poor local coherence using the original discourse .\nIntuitively, we can realize it by replacing a sentence  with a substitute  that is incoherent with the leftover discourse .\nThis is based on the insight that, through such a replacement,  will decrease local coherence of the discourse by introducing an incoherent transition of attention between sentences.\nSubsequently, the most important question is how to find a suitable  in practice. However,\nmost prior studies introduce such incoherent elements via heuristic rules, resulting in  that has very weak relevance or even irrelevant with the remaining discourse .\nFor example, INSteD (Shen et al., 2021  ###reference_b23###) obtains  by extracting sentence of the highest n-gram overlap with  from another discourse.\nAs a result, their introduced local augmentation samples are too easy to train a powerful coherence evaluator.\nTo construct samples with a higher level of local incoherence, we propose to construct  in a generative way.\nSpecifically, we train a generative augmentor  to reconstruct  based on  and use its generated sentence  to replace .\nThe strong performance of pretrained generation model will ensure  to meet the basic standard of fluency and relevance with regard to .\nMeanwhile, due to the intrinsic limitation of autoregressive generation, the reconstructed  will frequently be incoherent with , making it possible to construct negative samples in a generative way.\nTo further ensure that  conveys the local incoherence we expect, we design two controlling mechanisms during the inference of .\nThese two mechanisms, context truncation and coherence filtering , constraints  to be neither too strong (perfectly coherent with ) nor too easy (the incoherence that is too obvious).\nOverall, by replacing  with , we construct a much stronger negative sample, which conveys high-level local incoherence while maintaining the basic relevance and fluency with .\nIn the following, we will introduce our generative augmentor, context truncation and coherence filtering  in details.\nGiven discourse , we uniformly sample  from \u2019s non-opening and non-closing sentences.\nNext, we train a text generation model  by learning to reconstruct  based on the leftover discourse .\nFollowing recent popular text generation paradigm, this can be done by selecting  as a transformer-based sequence-to-sequence model and maximizing the likelihood of  autoregressively.\nWe also notice that Gap Sentences Generation (GSG), the pretraining task of PEGASUS (Zhang et al., 2020  ###reference_b31###  ###reference_b31###), takes the similar form of\nreconstructing sentences.\nBut we cannot directly apply PEGASUS as G, because GSG is specially designed for summarization, which requires predicting multiple salient sentences in the discourse.\nBy contrast, our sentence reconstruction task aims to capture the coherence relation between an arbitrary sentence  and the leftover discourse.\nIn practice, we also find  generated by PEGASUS often serve as summaries of the leftover discourse, rather than being coherent with it.\nThus, instead of directly applying PEGASUS, we leverage this similarity of tasks and use PEGASUS for initialization.\nIn this way, we inherit the effectiveness of pretrained model.\nAfter the generative augmentor is trained, we use it to predict  with two controlling mechanisms:\nDue to the strong generation ability of generative augmentor,  may be highly coherent with , which is not the negative sample we expect.\nTo ensure  to convey the local incoherence, we develop a context truncation mechanism to restrict the model\u2019s generation to only partially coherent with the context.\nSpecifically, given  with  masked, we randomly choose to truncate the context before or after the mask token, i.e., the input for our generative augmentor is either  or . Take the former as an example, without information from subsequent text, the model is only able to generate predictions that are coherent with preceding text.\nIn addition to context truncation, we also perform coherence filtering to remove negative samples that are too easy. We utilize UniEval (Zhong et al., 2022  ###reference_b33###  ###reference_b33###) to score the coherence of each sample and eliminates samples with coherence scores below a filtering threshold ."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Training and Unified Scoring",
            "text": "We combine the original discourses with negative samples constructed via global and local augmentation to train our metric model, as illustrated in Figure 2  ###reference_###  ###reference_###(b).\nWe utilize the classification setup, based on findings (Steen and Markert, 2022  ###reference_b24###  ###reference_b24###) that indicate its superior performance in coherence evaluation and downstream tasks, as opposed to the commonly used pairwise ranking setup. Specifically, we train our metric model to distinguish each sample as coherent or incoherent through binary cross entropy loss. For implementation details, please refer to Appendix A  ###reference_###  ###reference_###.\nFor a comprehensive evaluation of discourse coherence, our CoUDA further includes a unified scoring strategy, as presented in Figure 2  ###reference_###  ###reference_###(c). Specifically, our model first assigns a score conditioned on the whole discourse to represent its global coherence level:\nThen, since global scoring may fail to effectively capture the fine-grained coherence between sentences, we extract consecutive sentence pairs  from the discourse and have our model evaluate the inter-sentential coherence  of each pairs, where :\nNotably, although our model is trained for scoring the whole discourse, rather than sentence pairs, the training data includes discourse samples with only two sentences. As a result, our model can generalize to scoring sentence pairs as well. Afterwards, we obtain local coherence score for discourse  by averaging each sentence pair\u2019s coherence score:\nThe global and local scores are then combined via interpolation to form the overall coherence score:\nwhere  controls the weight. This unified design also aligns with the coherence rating process of human readers, who consider both discourse organization as a whole, and smooth transitions of focused items between adjacent sentences."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We obtain positive part of data for our framework by sampling from CNN (Nallapati et al., 2016  ###reference_b19###) and Wikipedia (Yang et al., 2015  ###reference_b29###). For CNN, we utilize its source documents rather than summaries, because the latter is constructed by combining bullet points, hence lacks coherence. For each source document, we randomly select 2 to 5 leading sentences, enabling our\nmetric model to generalize to different lengths. The same length constraint is applied on Wikipedia as well. Concretely, we sample 10,000 documents each from CNN and Wikipedia, hence obtaining 20,000 positive samples.\nFor global coherence, we perform permutation on 5,000 positive samples, and acquire 5,000 negative samples for this aspect. For local coherence, we perform gap sentence generation on the remaining 15,000 positive samples using generative augmentor with context truncation. By setting threshold  for confidence filtering to 0.5, we obtain 10,889 positive and negative pairs for this aspect. Hence, the final size of our synthetic data (including positive samples) is 31,778. We split it into 30,000 / 1,178 for training and validation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluation Tasks",
            "text": "We perform meta-evaluation on the proposed metric model in two task settings, i.e. pointwise scoring and pairwise ranking.\nPointwise Scoring involves assigning coherence scores to text summarization samples and evaluating the correlation between model-assigned scores and human-rated scores. This task closely simulates real-world scenarios. To determine the accuracy of the assigned scores, we compute the correlation coefficients between the model-generated scores and human ratings using Spearman (Sedgwick, 2014  ###reference_b22###), Pearson (Sedgwick, 2012  ###reference_b21###), and Kendall\u2019s tau (Abdi, 2007  ###reference_b1###). Following previous work, these correlation scores are reported at both sample-level and dataset-level (See Appendix A  ###reference_### for their definitions).\nPairwise ranking requires the metric models to determine the more coherent option when presented with two candidates. This task serves as an alternative when absolute scores are unavailable, relying solely on relative coherence rankings. For this task, we use accuracy as performance metric."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation Datasets",
            "text": "For pointwise scoring, we evaluate model performance on SummEval (Fabbri et al., 2021  ###reference_b6###), which is a meta-evaluation benchmark for summarization that contains 100 articles with summaries generated by 16 different systems. For each summary, it offers human annotated scores in terms of fluency, coherence, consistency, and relevance.222In this paper, we focus on discourse coherence, so we neglect coherence evaluation datasets on dialogue.\nIn pairwise ranking, we evaluate model performance on INSteD (Shen et al., 2021  ###reference_b23###), which is an intruder sentence detection dataset constructed using discourses from CNN and Wikipedia. We denote these two parts as INSteD-CNN and INSteD-Wiki. In this dataset, incoherent discourses are created by randomly substituting a sentence with another one selected using n-gram overlap from different discourses."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Baselines Models",
            "text": "Though more applicable in real scenarios, few work in coherence evaluation has pioneered in pointwise scoring. For a comprehensive performance comparison, we include baselines models from three categories: 1) Pairwise Coherence Evaluators: UNC (Moon et al., 2019  ###reference_b18###) and MultiNeg (Jwalapuram et al., 2022  ###reference_b11###). UNC captures different levels of coherence via a LSTM-based Siamese architecture; MultiNeg333The original MultiNeg model is backboned with XLNet and trained on the WSJ dataset. For fair comparison, we retrained this model from ALBERT-xxlarge, using the same part of Wikipedia and CNN data. Notably, due to its use of two encoders, MultiNeg has twice the number of parameters compared to CoUDA. mines hard negative samples constructed via sentence shuffling to train pairwise coherence ranking models. 2) General Evaluators: BartScore (Yuan et al., 2021  ###reference_b30###), UniEval (Zhong et al., 2022  ###reference_b33###). BartScore treats text evaluation as a generation task, utilizing BART to assign quality scores for a specific dimension. UniEval reframes text evaluation as a Boolean Question Answering task. Backboned with T5, it is trained with rule-based local augmentation for coherence evaluation. 3) Large Language Models: G-Eval (Liu et al., 2023  ###reference_b15###) uses LLMs with chain-of-thoughts to assign quality scores. We experiment with two versions using GPT-3.5-Turbo / GPT-4, respectively denoted as G-Eval-3.5 / 4. We include more details about using UniEval and G-Eval in Appendix C  ###reference_### and  D  ###reference_###, respectively."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Details of Synthetic Data",
            "text": "We obtain positive part of data for our framework by sampling from CNN (Nallapati et al., 2016  ###reference_b19###  ###reference_b19###) and Wikipedia (Yang et al., 2015  ###reference_b29###  ###reference_b29###). For CNN, we utilize its source documents rather than summaries, because the latter is constructed by combining bullet points, hence lacks coherence. For each source document, we randomly select 2 to 5 leading sentences, enabling our\nmetric model to generalize to different lengths. The same length constraint is applied on Wikipedia as well. Concretely, we sample 10,000 documents each from CNN and Wikipedia, hence obtaining 20,000 positive samples.\nFor global coherence, we perform permutation on 5,000 positive samples, and acquire 5,000 negative samples for this aspect. For local coherence, we perform gap sentence generation on the remaining 15,000 positive samples using generative augmentor with context truncation. By setting threshold  for confidence filtering to 0.5, we obtain 10,889 positive and negative pairs for this aspect. Hence, the final size of our synthetic data (including positive samples) is 31,778. We split it into 30,000 / 1,178 for training and validation."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Implementation Details.",
            "text": "Our metric model utilizes ALBERT Lan et al. (2020  ###reference_b12###) as the backbone, benefiting from its sentence order prediction task during pretraining to capture information flow between sentences. Specifically, we use ALBERT-xxlarge with a total of 233M parameters. We set batch size to 32 and learning rate to . Convergence is reached within 3,000 steps. We use the best performing checkpoint on the validation part of synthetic data. Details about generative augmentor are presented in Appendix A  ###reference_###. In terms of hyperparameters  and , we simply set both of them to ."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we show that CoUDA framework achieves impressive coherence evaluation results on pointwise scoring and pairwise ranking tasks, even when compared with GPT-4 based models. We report average scores across 3 runs with different random seeds."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Results on SummEval",
            "text": "Table 1  ###reference_### presents the sample-level and dataset-level correlations of each model with human ratings on SummEval. Since UNC and MultiNeg are trained through pairwise ranking, their performance on for pointwise scoring is relatively limited. BartScore and UniEval are general evaluators for multiple dimensions such as informativeness and coherence. The former lacks specific training for these dimensions, leading to lower performance, while the latter gain significant improvement through tailored training for coherence. However, UniEval still relies on heuristic rules for augmentation, resulting in limited improvements. The third block presents the results of G-Eval-3.5 and G-Eval-4, built upon GPT-3.5-Turbo and GPT-4, respectively. Since there are no exact description of how many parameters GPT-3.5/4 takes, we estimate them as >10B and >100B.\nAmong baselines models, G-Eval-4 achieves highest correlation with human ratings, followed by UniEval, which demonstrates strong performance, even surpassing G-Eval-3.5. Compared with UniEval, CoUDA consistently shows its superiority on both sample-level correlation (+3.3/+4.3/+2.4 in ) and dataset-level correlation (+6.9/+8.7/+5.4 in ).\nWith only 233M parameters, it also surpasses G-Eval-4 in both sample-level Spearman and Kendall correlations by 1.8 and 0.3 points, respectively. This remarkable improvement consolidates the efficacy of our designing criteria.\nAdditionally, we notice that performance gain in dataset-level correlation is much greater than that of sample-level."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results on INSteD",
            "text": "Table 2  ###reference_### presents each model\u2019s pairwise ranking accuracy on INSteD-Wiki and INSteD-CNN. Both MultiNeg and UNC achieves impressive accuracy. We suppose it is because they are exactly trained using the pairwise ranking setup.\nUniEval also achieves competitive results, which means that specialized training for coherence greatly enhances model performance. Surprisingly, G-Eval-3.5 obtains merely above chance accuracy on INSteD-Wiki, indicating that current LLMs are unreliable in pairwise ranking tasks, necessitating further investigation and attention from researchers. Our CoUDA, though not directly trained under pairwise ranking settings, achieves best results on both INSteD-CNN and INSteD-Wiki, with a performance gain of 2.2 and 1.8 points, respectively."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Comparison of Augmentation Methods",
            "text": "In this section, we validate the advantage of our unified data augmentation strategy for coherence scoring over previous data augmentation strategies.\nCoherence evaluation emphasizes the sentence structure and organization of a discourse. Due to this special focus, data augmentation strategies designed for other tasks, e.g. EDA (Wei and Zou, 2019  ###reference_b28###), are not directly applicable. Instead, we compare following data augmentation strategies for generating negative samples: 1) G: Global augmentation via sentence shuffling (Barzilay and Lapata, 2008  ###reference_b3###), which is also adopted in our framework. 2) : Rule-based local augmentation through sentence intrusion, which employs n-gram overlap to select locally incoherent samples (Shen et al., 2021  ###reference_b23###). 3) : Our generative local augmentation strategy. 4)  or : Combination of global and local augmentation methods.\nIn Table 3  ###reference_###, we can see that unifying global and local augmentation data yields the best human correlation, better than using global or local augmentation alone. This aligns well with the linguistic theory of discourse structure that the organization of discourse segments (global coherence), and the transition of attention or focused items (local coherence) are two key factors of discourse coherence, from which our unified data augmentation framework are inspired.\nFurther, we compare the result of generative augmentation vs. rule-based augmentation for modeling local coherence. First, metric model trained with  outperforms that of  by a large margin on both sample-level correlation (+3.8/+3.0/+3.0 in ) and dataset-level correlation (+3.5/+1.5/+2.4 in ). Second, when combined with global augmentation,  yields significantly superior performance than . Based on these two aspects, we can conclude that our generative strategy is more effective than rule-based methods."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "First, we study the effectiveness of our unified scoring strategy. Experiment results are demonstrated in Figure 3  ###reference_###. First, both global and local scores are beneficial in improving human correlation. Additionally, global scores correlate better with human ratings than local scores.\n###figure_3### We then analyze the effect of our difficulty controlling mechanisms in local augmentation.\nSpecifically, we train our metric model separately on local augmentation data constructed under different settings to compare their impacts. Table 4  ###reference_### presents the results. First, we can see that context truncation contributes a significant portion of performance, without which our generative augmentor suffers a severe performance drop of more than 20 points. This demonstrates the effectiveness of constructing partially coherent samples. Second, we find that our confidence filtering mechanism, through which we filter out easy negative samples, also helps model performance. We found that 0.6 is an optimal threshold that can filter out easy examples while ensuring enough amount of training data. We have also provided a case study in Appendix B  ###reference_###.\nWe compare our model\u2019s performance with strong baselines (MultiNeg, MultiNeg, G-Eval-3.5) w.r.t. different discourse length. Concretely, we categorize all 1,600 system summaries of SummEval into different groups according to the sentence numbers they have. We calculate the average of dataset-level Spearman / Pearson / Kendall correlation as defined in Equation 6  ###reference_### for each group. Figure 4  ###reference_### presents the results. On average, our model achieves best results when target discourse contains no more than 5 sentences. As the discourse length increases, all models suffer from performance drop, with G-Eval-3.5 being the only exception, which renders very steady correlation against length variance. Since each training sample we construct contains no more than 5 sentences (see Appendix A  ###reference_###), we assume CoUDA\u2019s performance drop can be alleviated by training on samples with more sentences.\n###figure_4###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "General Evaluators",
            "text": "We denote evaluators capable of assessing multiple quality dimensions by altering input and output contents Yuan et al. (2021  ###reference_b30###), or adopting different formulas Scialom et al. (2021  ###reference_b20###); Zhong et al. (2022  ###reference_b33###) as general evaluators. A leading trend is to utilize generation model for quality assessment, such as BartScore Yuan et al. (2021  ###reference_b30###), UniEval Zhong et al. (2022  ###reference_b33###).\nApart from that, DiscoScore Zhao et al. (2022  ###reference_b32###) compared the focus matrix between the candidate and the reference to calculate the overall quality score.\nWith the rise of large language models (LLMs), there has been a growing tendency to use LLMs for evaluation purpose (Wang et al., 2023a  ###reference_b26###; Fu et al., 2023  ###reference_b7###; Wang et al., 2023b  ###reference_b27###; Liu et al., 2023  ###reference_b15###). Wang et al. (2023a  ###reference_b26###) adopted ChatGPT for NLG evaluation and achieved competitive results in terms of correlation with human judgments. Liu et al. (2020  ###reference_b14###) used LLMs with chain-of-thought and a form-filling paradigm to assess the quality of text."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose a unified data augmentation framework called CoUDA, with the designing criteria to unify both global and local aspects of coherence, as inspired by linguistic theory of discourse structure. This data framework includes global and local augmentation, a classification paradigm for training and a unified scoring strategy for inference. We specifically propose a novel generative augmentation strategy, which involves post-pretraining a generative model, and applying two controlling mechanisms to control the difficulty of generated samples. With only 233M parameters, our framework achieves remarkable improvement over previous methods, including GPT-4 based metrics."
        }
    ],
    "url": "http://arxiv.org/html/2404.00681v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "7",
            "7.1",
            "7.2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "4",
            "4.1",
            "4.2",
            "5",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2404.00681v1",
        "paper_title": "CoUDA: Coherence Evaluation via Unified Data Augmentation",
        "research_background": "### Motivation\nThe main motivation behind this paper is the critical need for coherent communication in both discourse generation and assessment. Recently, large language models, despite their impressive performance in various tasks, seem to fall short in coherence evaluation when compared to fine-tuning based models. Given the importance of computational efficiency and evaluation performance, there's a pressing necessity to develop a coherence evaluation model that balances these aspects effectively.\n\n### Research Problem\nThe central research problem addressed in this paper is how to design an effective criterion for data augmentation strategies that can uniformly cover both global and local aspects of coherence. Traditional methods, primarily rule-based techniques for generating negative samples, don't correlate well with human judgments, raising the question of how to enhance coherence modeling.\n\n### Relevant Prior Work\n\n1. **Discourse Coherence Foundations:**\n   - **Halliday and Hasan (1976)** and **Grosz and Sidner (1986)** laid the foundational theories on discourse coherence, focusing on the structure and organization of discourse and the transition of focused items.\n   \n2. **Large Language Models and Coherence:**\n   - Recent models such as those discussed in **brown2020language** exhibit strong performance in various tasks. However, they haven't shown dominance in coherence evaluation over fine-tuning based models (Fu et al., 2023).\n\n3. **Data Augmentation Techniques:**\n   - These techniques are crucial due to the scarcity of human-annotated data. Notable methods include swapping or shuffling sentences as seen in **Barzilay and Lapata (2008)**, **Shen et al. (2021)**, and **Jwalapuram et al. (2022)**. However, the heuristic nature of these methods leads to weak correlations with human judgments (Mohiuddin et al., 2021).\n\n4. **Negative Sample Generation:**\n   - Previous approaches have focused on constructing negative samples using rule-based methods, like swapping sentences, which fail to capture the nuanced aspects of local coherence efficiently.\n\n5. **Evaluation Metric:**\n   - Traditional coherence evaluation has mostly relied on pairwise ranking setups, while this paper introduces the pointwise coherence scoring setting, potentially more relevant to real-world scenarios, and tested its performance on various datasets, including SummEval (Fabbri et al., 2021).\n\n### Paper's Contributions\n\n- **Unified Data Augmentation Framework (CoUDA):** Inspired by the linguistic theory of discourse structure, it models both global and local coherence.\n- **Generative Augmentation Strategy:** Leverages pretrained language models via post-pretraining and two difficulty control mechanisms to generate high-quality negative samples.\n- **Pointwise Coherence Scoring:** Introduced as a more realistic and effective assessment method, which shows superior performance compared to previous methods, including state-of-the-art models like GPT-3.5 and GPT-4.",
        "methodology": "The methodology section of the paper introduces the CoUDA (Coherence Evaluation via Unified Data Augmentation) framework, designed to evaluate the coherence of a given discourse. Here is an outline of the proposed method, including key components and innovations:\n\n1. **Data Augmentation for Negative Samples**:\n    - **Global Augmentation**:\n        - Uses sentence shuffling to create negative samples that have poor global coherence.\n    - **Local Augmentation**:\n        - Uses a generative strategy to produce negative samples with poor local coherence.\n        - Trains a text generation model to reconstruct sentences based on the leftover discourse.\n        - **Post-pretraining**: The generative model is initialized with PEGASUS, a transformer-based sequence-to-sequence model.\n        - **Controlling Mechanisms**:\n            - **Context Truncation**: Limits the context provided to the model to ensure the generated sentences are only partially coherent with the context.\n            - **Coherence Filtering**: Employs UniEval to score and eliminate negative samples that are too coherent.\n\n2. **Training the Metric Model**:\n    - Combines the original discourses (serving as positive samples) with the generated negative samples to train the coherence evaluation model.\n    - Utilizes a classification approach (binary cross entropy loss) to distinguish between coherent and incoherent samples.\n    - Cites findings that indicate the superiority of the classification setup over the commonly used pairwise ranking setup.\n\n3. **Unified Scoring Strategy**:\n    - **Global Coherence Scoring**: Assigns a score based on the entire discourse to represent its global coherence level.\n    - **Local Coherence Scoring**:\n        - Extracts consecutive sentence pairs from the discourse and evaluates their inter-sentential coherence.\n        - Averages the coherence scores of all sentence pairs to obtain the local coherence score.\n    - **Combining Scores**:\n        - Merges global and local scores via interpolation to create an overall coherence score.\n        - This unified strategy mirrors the coherence assessment process of human readers, who consider both the overall organization and smooth transitions between sentences.\n\nOverall, the CoUDA framework combines innovative data augmentation techniques to create robust negative samples and employs a sophisticated scoring strategy to provide a comprehensive coherence evaluation.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Datasets:\n- **CNN**: Source documents are sampled rather than summaries due to better inherent coherence.\n- **Wikipedia**: Similar selection criteria are followed as with the CNN dataset.\n- **Positive Sample Size**: 10,000 documents each from CNN and Wikipedia, amounting to 20,000 positive samples in total.\n- **Global Coherence Negative Samples**: 5,000 samples obtained through permutations.\n- **Local Coherence Negative Samples**: Generated using gap sentence generation with a context truncation strategy and confidence filtering, resulting in 10,889 pairs.\n- **Final Dataset Size**: 31,778 samples, split into 30,000 for training and 1,178 for validation.\n\n#### Baselines:\n- The paper does not specify explicit baselines in the provided text. Typically, baseline methods in coherence evaluation might include existing coherence metrics or previous state-of-the-art models.\n\n#### Evaluation Metrics:\n- **Global Coherence**: Assessed through task-specific metrics related to sentence arrangement and logical flow.\n- **Local Coherence**: Evaluated based on the integrity of immediate sentence connections using methods like gap sentence generation outcomes.\n\n#### Main Experimental Results:\nThe results or performance comparisons of the proposed CoUDA framework against specific metrics or baselines are not provided in the given text. However, based on the described setup, one would expect the results to outline the effectiveness of the CoUDA model in improving coherence evaluation benchmarks through the comprehensive augmentation approach and new synthetic data generation methods. Typically, results would include precision, recall, F1-score, or similar metrics showcasing how well the proposed method performs compared to other models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To validate the advantage of the unified data augmentation strategy proposed by CoUDA for coherence scoring over previous data augmentation strategies specifically designed for other tasks.",
            "experiment_process": "The experiment compares four augmentation strategies for generating negative samples in coherence evaluation: 1) Global augmentation via sentence shuffling, 2) Rule-based local augmentation through sentence intrusion, 3) the generative local augmentation strategy proposed in CoUDA, and 4) a combination of global and local augmentation methods. The performance is assessed by measuring human correlation in coherence scoring.",
            "result_discussion": "The results show that the unified global and local augmentation data yields the best human correlation, outperforming the use of global or local augmentation alone. The generative augmentation strategy proposed by CoUDA is more effective than rule-based methods, significantly improving both sample-level and dataset-level correlations.",
            "ablation_id": "2404.00681v1.No1"
        },
        {
            "research_objective": "To study the effectiveness of the unified scoring strategy and the difficulty controlling mechanisms in local augmentation proposed by CoUDA.",
            "experiment_process": "The experiment involves training the metric model separately on local augmentation data constructed under different settings to compare their impacts. The context truncation and confidence filtering mechanisms are tested, and the performance of models with different discourse lengths is evaluated using dataset-level Spearman, Pearson, and Kendall correlation metrics.",
            "result_discussion": "Results indicate that both global and local scores enhance human correlation, with global scores correlating better with human ratings than local scores. Context truncation significantly improves performance by more than 20 points, while confidence filtering also contributes positively. The model performs best when evaluating discourses with no more than 5 sentences. Performance drops with increasing discourse length, suggesting that training on longer samples may improve results.",
            "ablation_id": "2404.00681v1.No2"
        }
    ]
}