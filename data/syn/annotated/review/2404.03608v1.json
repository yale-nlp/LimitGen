{
    "title": "Sailor: Open Language Models for South-East Asia",
    "abstract": "We present Sailor, a family of open language models ranging from 0.5B to 7B parameters, tailored for South-East Asian (SEA) languages.\nThese models are continually pre-trained from Qwen1.5, a great language model for multilingual use cases.\nFrom Qwen1.5, Sailor models accept 200B to 400B tokens, primarily covering the languages of English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao.\nThe training leverages several techniques, including BPE dropout for improving the model robustness, aggressive data cleaning and deduplication, and small proxy models to optimize data mixture.\nExperimental results on four typical tasks indicate that Sailor models demonstrate strong performance across different benchmarks, including commonsense reasoning, question answering, reading comprehension and examination.\nEmbracing the open-source spirit, we share our insights through this report to spark a wider interest in developing large language models for multilingual use cases.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Takeaway",
            "text": "(1) Language models struggle with multiple languages, and continual pre-training presents an opportunity to improve specific language capabilities. (2) Code-switching techniques can be beneficial in multilingual scenarios, improving the ability to handle language mixing. (3) Language models are sensitive to subword segmentation, and techniques like BPE dropout can improve model robustness. (4) Even available high-quality multilingual corpora may require further data deduplication and cleaning. (5) Simulation experiments on smaller models can provide insights into performance trends for large-scale experiments.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Insights",
            "text": "During our development, we perform ablation studies on small LMs to understand the impact of various strategies. We then apply the key insights gained from these studies to improve LLMs.\nMost of the experimental results are obtained from three series of models: our internal 120M model trained on 20B English tokens using SlimPajama (Soboleva et al., 2023  ###reference_b39###), the TinyLlama 1.1B model (Zhang et al., 2024  ###reference_b51###), and the Qwen1.5-0.5B model (Bai et al., 2023  ###reference_b5###).\nAll techniques we have considered are listed in Table 1  ###reference_###.\nSeveral studies have emphasized the importance of deduplication (Lee et al., 2022  ###reference_b25###), with some popular corpora undergoing deduplication at the paragraph level, resulting in a final corpus comprising unique paragraphs such as CC100 (Wenzek et al., 2020a  ###reference_b46###).\nThe approach enhances data efficiency by maximizing the number of unique tokens the model encounters, but it can adversely impact model performance as the connections between different pieces within the context become less relevant.\nTo mitigate the issue, we have employed a simple method of randomly combining several adjacent examples into one example before applying a global shuffle.\nThe method can be applied because the deduplicated paragraphs still retain the order in which they appear in the original documents, allowing for the reconstruction of context when necessary.\nMoreover, the method is also applied to certain sources, such as subtitles, which are inherently composed of short sentences.\nCode-switching refers to the phenomenon where different languages are used within the same context. We categorize code-switching into two types: document-level and word-level. For document-level code-switching, when preparing the dataset for pre-training sequences, we pack documents from various languages instead of separately packing them within each language. Regarding word-level code-switching, we randomly select some words in each documents written in SEA languages (e.g., Indonesian) and replace 10% of them with their corresponding English phrases, if available.\nOur preliminary experiments on TinyLlama show that the document-level code-switching alone performs better than the word-level code-switching alone or a combination of both approaches.\nTherefore, we only apply the document-level code-switching method during continual pre-training.\nInterestingly, despite the intuitive expectation that incorporating translation data, such as CCAligned (El-Kishky et al., 2020  ###reference_b16###), would enhance model performance on document-level code-switching, the experimental results did not demonstrate a significant improvement.\nIn contrast, using translation data alone (i.e., without document-level code-switching) can lead to improved model performance over the baseline on general tasks (e.g., question answering), suggesting that translation data plays a role similar to document-level code-switching to a certain degree.\nNonetheless, to facilitate translation capabilities, our dataset incorporates translation datasets, which will be discussed later.\nThe data quality is crucial during continual pre-training.\nWe employ aggressive cleaning parameters, extended filtering list for each language, and multi-round deduplication.\nConsequently, even though we started with well-curated open datasets, e.g., MADLAD-400 clean set (Kudugunta et al., 2023  ###reference_b24###), we still further removed % in data cleaning and % in data deduplication.\nBy extensively filtering out noisy, harmful, and duplicated content, we are able to significantly improve the efficiency of the pre-training process and the stability of the optimization procedure.\nFurthermore, LLMs are less prone to memorization issues when training data has undergone thorough deduplication (Lee et al., 2022  ###reference_b25###).\nWe have observed that the model is unreasonably sensitive to small variations of the prompt, especially on spaces. As illustrated in Figure 2(a)  ###reference_sf1###,\nwhen prompting the model with the string \u201cAnswer:\u201d without any trailing space yields a substantially improved performance compared to \u201cAnswer: \u2423\u201d 111We use \u201c\u2423\u201d to represent space..\nThe same phenomenon are observed in Qwen1.5, Mistral and Llama 2, and a similar issue has been discussed at lm-evaluation-harness library222https://github.com/EleutherAI/lm-evaluation-harness/issues/614  ###reference_n-harness/issues/614### (Gao et al., 2023  ###reference_b17###).\nWe attribute this kind of vulnerability to the tokenization strategy used in data processing.\nModern tokenization methods usually employ the Byte Pair Encoding (BPE) (Sennrich et al., 2016  ###reference_b36###) under the greedy segmentation setting 333The default BPE class is initialized with no dropout in the HuggingFace tokenizers library., which means that sentences are segmented into subwords using the optimal tokenization strategy.\nHowever, the always-optimal strategy can lead to vulnerability in the model when it encounters noisy subwords, such as an unexpected space in \u201cAnswer: \u2423\u201d.\nTypically, a space is segmented into subwords alongside the subsequent chars (e.g., \u201c\u24231\u201d constitutes a single subword).\nYet, if a space is left at the end of the prompt, it becomes an isolated subword \u201c\u2423\u201d, deviating from the segmentation strategy in the demonstration examples.\nTo alleviate the problem, we employ the BPE-Dropout (Provilkov et al., 2020  ###reference_b33###) during continual pre-training, which stochastically corrupts the segmentation procedure of BPE to achieve subword regularization.\nExperimental results indicate that although BPE Dropout slightly increases the loss on greedy subword segmentation, it enhances both the performance and the robustness of models, as shown in Figure 2(b)  ###reference_sf2###.\n###figure_2### We have tried our best to do vocabulary expansion on models like Mistral (Jiang et al., 2023a  ###reference_b19###) and Llama-2 (Touvron et al., 2023b  ###reference_b42###).\nHowever, similar to the observation in concurrent works (Zhao et al., 2024  ###reference_b53###), it is challenging to expand the vocabulary with maintaining the original performance.\nAccording to our investigation, without sufficient continual pre-training, the performance of vocab expanded model could not even recover to the baseline version.\nFor example, after being trained on 20B tokens with an expanded vocabulary of 15,000 subwords, Mistral\u2019s question answering performance over Thai remains 10% lower than that of the original model.\nWe have also explored several methods to eliminate the problem, including warmup the embedding layer first, or modularized continual-training (Kim et al., 2024  ###reference_b22###).\nDespite our efforts, the methods did not perform as effectively as we expected.\nWe acknowledge this interesting yet challenging problem as an opportunity for future research.\nFinally, we decided to develop Sailor models based on Qwen1.5 (Bai et al., 2023  ###reference_b5###), which is inherently multilingual-friendly and possesses a large vocabulary size, thereby guaranteeing a high compression rate for SEA languages.\nFigure 3  ###reference_### provides a visual representation of the relationship between the model\u2019s performance on English and SEA languages when subjected to the same token budget (e.g., 0.25B).\nIt clearly illustrates that the trade-off that exists among different languages.\nIn other words, when performing continual pre-training on an English-centric language model, increasing the proportion of SEA language corpus always results in a degradation of the model\u2019s performance on English, even when a high-quality English corpus is included for replay.\nThese findings align with previous studies on the curse of multilinguality (Conneau et al., 2020  ###reference_b12###; Chang et al., 2023  ###reference_b9###), which posit that modeling multiple languages within a single model leads to competition among languages for the fixed model capacity.\nFigure 3  ###reference_### also demonstrates an inverse relationship between the number of tokens and the loss on English.\nAs more tokens are consumed (e.g., 0.25B  2.25B), the curve shifts towards the upper-left area, signifying an increase in the loss on English.\nInterestingly, the loss trend on the source domain (i.e., English) is primarily influenced by two factors: the proportion of English data during continual pre-training and the learning rate.\nUnder the same token budget, the model\u2019s loss on English can be accurately modeled as a quadratic function of , as shown in Figure 4(a)  ###reference_sf1###.\nIn other words, while keeping the proportion of English data constant, increasing the learning rate may adversely affect the model\u2019s performance on English.\nMeanwhile, the loss trend on the target domain (i.e., SEA languages) is also mainly affected by the proportion of the target domain and the learning rate.\nHowever, there is a different modeling among the model loss on SEA languages, the proportion and the learning rate, as demonstrated by Figure 4(b)  ###reference_sf2###.\nFrom the observation, it becomes evident that the learning rate serves as a crucial hyper-parameter.\nA well-tuned learning rate plays a pivotal role in striking a balance between the acquisition of SEA languages and the forgetting of English.\nAs shown in Figure 4(c)  ###reference_sf3###, considering that increasing the learning rate beyond 1e-4 does not yield significant improvements in the loss on SEA languages, we set the peak learning rate to 1e-4 in our experiments.\n###figure_3### We aim to develop an improved LLM tailored for the entire SEA region, with a focus on ensuring balanced representation across all target languages. To achieve this, we have developed a new algorithm that determines the appropriate weights for various languages during continual pre-training. This method involves conducting a series of randomized data mixture experiments, while adhering to a predetermined learning rate.\nOur goal is to determine the most effective data mixture.\nTo this end, we suggest employing simulations in conjunction with linear regression models.\nAs depicted in Figure 5  ###reference_###, we begin by training a set of proxy models (e.g., 64 in total here) on a variety of data mixtures for a limited number of training steps (e.g., 1000 steps).\nWe then fit a linear regression model, using the data mixture as the input feature and the joint loss considering all languages 666We use the product of individual losses as the joint loss. as the target.\nWith this model, we can perform numerous simulation experiments (e.g., 1,000,000) on randomly sampled data mixtures to explore the vast array of possibilities within seconds.\nThe linear model then guides us in selecting the combination that yields the lowest predicted joint loss.\nOnce this data mixture has been optimized, it can be directly applied to large-scale training.\nMore details and findings will be discussed in our upcoming paper."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Data",
            "text": "Several studies have emphasized the importance of deduplication (Lee et al., 2022  ###reference_b25###  ###reference_b25###), with some popular corpora undergoing deduplication at the paragraph level, resulting in a final corpus comprising unique paragraphs such as CC100 (Wenzek et al., 2020a  ###reference_b46###  ###reference_b46###).\nThe approach enhances data efficiency by maximizing the number of unique tokens the model encounters, but it can adversely impact model performance as the connections between different pieces within the context become less relevant.\nTo mitigate the issue, we have employed a simple method of randomly combining several adjacent examples into one example before applying a global shuffle.\nThe method can be applied because the deduplicated paragraphs still retain the order in which they appear in the original documents, allowing for the reconstruction of context when necessary.\nMoreover, the method is also applied to certain sources, such as subtitles, which are inherently composed of short sentences.\nCode-switching refers to the phenomenon where different languages are used within the same context. We categorize code-switching into two types: document-level and word-level. For document-level code-switching, when preparing the dataset for pre-training sequences, we pack documents from various languages instead of separately packing them within each language. Regarding word-level code-switching, we randomly select some words in each documents written in SEA languages (e.g., Indonesian) and replace 10% of them with their corresponding English phrases, if available.\nOur preliminary experiments on TinyLlama show that the document-level code-switching alone performs better than the word-level code-switching alone or a combination of both approaches.\nTherefore, we only apply the document-level code-switching method during continual pre-training.\nInterestingly, despite the intuitive expectation that incorporating translation data, such as CCAligned (El-Kishky et al., 2020  ###reference_b16###  ###reference_b16###), would enhance model performance on document-level code-switching, the experimental results did not demonstrate a significant improvement.\nIn contrast, using translation data alone (i.e., without document-level code-switching) can lead to improved model performance over the baseline on general tasks (e.g., question answering), suggesting that translation data plays a role similar to document-level code-switching to a certain degree.\nNonetheless, to facilitate translation capabilities, our dataset incorporates translation datasets, which will be discussed later.\nThe data quality is crucial during continual pre-training.\nWe employ aggressive cleaning parameters, extended filtering list for each language, and multi-round deduplication.\nConsequently, even though we started with well-curated open datasets, e.g., MADLAD-400 clean set (Kudugunta et al., 2023  ###reference_b24###  ###reference_b24###), we still further removed % in data cleaning and % in data deduplication.\nBy extensively filtering out noisy, harmful, and duplicated content, we are able to significantly improve the efficiency of the pre-training process and the stability of the optimization procedure.\nFurthermore, LLMs are less prone to memorization issues when training data has undergone thorough deduplication (Lee et al., 2022  ###reference_b25###  ###reference_b25###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Tokenization",
            "text": "###figure_4### We have observed that the model is unreasonably sensitive to small variations of the prompt, especially on spaces. As illustrated in Figure 2(a)  ###reference_sf1###  ###reference_sf1###,\nwhen prompting the model with the string \u201cAnswer:\u201d without any trailing space yields a substantially improved performance compared to \u201cAnswer: \u2423\u201d 111We use \u201c\u2423\u201d to represent space..\nThe same phenomenon are observed in Qwen1.5, Mistral and Llama 2, and a similar issue has been discussed at lm-evaluation-harness library222https://github.com/EleutherAI/lm-evaluation-harness/issues/614  ###reference_n-harness/issues/614###  ###reference_n-harness/issues/614### (Gao et al., 2023  ###reference_b17###  ###reference_b17###).\nWe attribute this kind of vulnerability to the tokenization strategy used in data processing.\nModern tokenization methods usually employ the Byte Pair Encoding (BPE) (Sennrich et al., 2016  ###reference_b36###  ###reference_b36###) under the greedy segmentation setting 333The default BPE class is initialized with no dropout in the HuggingFace tokenizers library., which means that sentences are segmented into subwords using the optimal tokenization strategy.\nHowever, the always-optimal strategy can lead to vulnerability in the model when it encounters noisy subwords, such as an unexpected space in \u201cAnswer: \u2423\u201d.\nTypically, a space is segmented into subwords alongside the subsequent chars (e.g., \u201c\u24231\u201d constitutes a single subword).\nYet, if a space is left at the end of the prompt, it becomes an isolated subword \u201c\u2423\u201d, deviating from the segmentation strategy in the demonstration examples.\nTo alleviate the problem, we employ the BPE-Dropout (Provilkov et al., 2020  ###reference_b33###  ###reference_b33###) during continual pre-training, which stochastically corrupts the segmentation procedure of BPE to achieve subword regularization.\nExperimental results indicate that although BPE Dropout slightly increases the loss on greedy subword segmentation, it enhances both the performance and the robustness of models, as shown in Figure 2(b)  ###reference_sf2###  ###reference_sf2###.\n###figure_5### We have tried our best to do vocabulary expansion on models like Mistral (Jiang et al., 2023a  ###reference_b19###  ###reference_b19###) and Llama-2 (Touvron et al., 2023b  ###reference_b42###  ###reference_b42###).\nHowever, similar to the observation in concurrent works (Zhao et al., 2024  ###reference_b53###  ###reference_b53###), it is challenging to expand the vocabulary with maintaining the original performance.\nAccording to our investigation, without sufficient continual pre-training, the performance of vocab expanded model could not even recover to the baseline version.\nFor example, after being trained on 20B tokens with an expanded vocabulary of 15,000 subwords, Mistral\u2019s question answering performance over Thai remains 10% lower than that of the original model.\nWe have also explored several methods to eliminate the problem, including warmup the embedding layer first, or modularized continual-training (Kim et al., 2024  ###reference_b22###  ###reference_b22###).\nDespite our efforts, the methods did not perform as effectively as we expected.\nWe acknowledge this interesting yet challenging problem as an opportunity for future research.\nFinally, we decided to develop Sailor models based on Qwen1.5 (Bai et al., 2023  ###reference_b5###  ###reference_b5###), which is inherently multilingual-friendly and possesses a large vocabulary size, thereby guaranteeing a high compression rate for SEA languages."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Training",
            "text": "When it comes to continual pre-training, two crucial hyper-parameters to consider are the learning rate and the data mixture.\nIn our practise, we begin by generating a number of training configurations with varying learning rates 444We first divide the logarithmic range between 1e-5 and 4e-4 into 20 equal intervals. For each configuration, we randomly select one interval and use the corresponding value as the learning rate. and language proportions to train several proxy models 555The proxy models are typically small, allowing for cheap experiments, yet they retain key characteristics akin to those of the target base LLM..\nBy analyzing the trade-off between English and SEA languages on these proxy models, we can select a suitable learning rate.\nOnce the learning rate is determined, we then conduct fine-grained data mixture simulation experiments to optimize the joint loss across all languages, which is finally used in large-scale training.\n###figure_6### ###figure_7### ###figure_8### Figure 3  ###reference_###  ###reference_### provides a visual representation of the relationship between the model\u2019s performance on English and SEA languages when subjected to the same token budget (e.g., 0.25B).\nIt clearly illustrates that the trade-off that exists among different languages.\nIn other words, when performing continual pre-training on an English-centric language model, increasing the proportion of SEA language corpus always results in a degradation of the model\u2019s performance on English, even when a high-quality English corpus is included for replay.\nThese findings align with previous studies on the curse of multilinguality (Conneau et al., 2020  ###reference_b12###  ###reference_b12###; Chang et al., 2023  ###reference_b9###  ###reference_b9###), which posit that modeling multiple languages within a single model leads to competition among languages for the fixed model capacity.\nFigure 3  ###reference_###  ###reference_### also demonstrates an inverse relationship between the number of tokens and the loss on English.\nAs more tokens are consumed (e.g., 0.25B  2.25B), the curve shifts towards the upper-left area, signifying an increase in the loss on English.\nInterestingly, the loss trend on the source domain (i.e., English) is primarily influenced by two factors: the proportion of English data during continual pre-training and the learning rate.\nUnder the same token budget, the model\u2019s loss on English can be accurately modeled as a quadratic function of , as shown in Figure 4(a)  ###reference_sf1###  ###reference_sf1###.\nIn other words, while keeping the proportion of English data constant, increasing the learning rate may adversely affect the model\u2019s performance on English.\nMeanwhile, the loss trend on the target domain (i.e., SEA languages) is also mainly affected by the proportion of the target domain and the learning rate.\nHowever, there is a different modeling among the model loss on SEA languages, the proportion and the learning rate, as demonstrated by Figure 4(b)  ###reference_sf2###  ###reference_sf2###.\nFrom the observation, it becomes evident that the learning rate serves as a crucial hyper-parameter.\nA well-tuned learning rate plays a pivotal role in striking a balance between the acquisition of SEA languages and the forgetting of English.\nAs shown in Figure 4(c)  ###reference_sf3###  ###reference_sf3###, considering that increasing the learning rate beyond 1e-4 does not yield significant improvements in the loss on SEA languages, we set the peak learning rate to 1e-4 in our experiments.\n###figure_9### We aim to develop an improved LLM tailored for the entire SEA region, with a focus on ensuring balanced representation across all target languages. To achieve this, we have developed a new algorithm that determines the appropriate weights for various languages during continual pre-training. This method involves conducting a series of randomized data mixture experiments, while adhering to a predetermined learning rate.\nOur goal is to determine the most effective data mixture.\nTo this end, we suggest employing simulations in conjunction with linear regression models.\nAs depicted in Figure 5  ###reference_###  ###reference_###, we begin by training a set of proxy models (e.g., 64 in total here) on a variety of data mixtures for a limited number of training steps (e.g., 1000 steps).\nWe then fit a linear regression model, using the data mixture as the input feature and the joint loss considering all languages 666We use the product of individual losses as the joint loss. as the target.\nWith this model, we can perform numerous simulation experiments (e.g., 1,000,000) on randomly sampled data mixtures to explore the vast array of possibilities within seconds.\nThe linear model then guides us in selecting the combination that yields the lowest predicted joint loss.\nOnce this data mixture has been optimized, it can be directly applied to large-scale training.\nMore details and findings will be discussed in our upcoming paper."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Best Practise for Continual Pre-training",
            "text": "Drawing from the above insights, we highlight the importance of selecting the learning rate and the proportion of source domain data to mitigate issues such as catastrophic forgetting.\nTherefore, we focus on the metric , which we refer to as the magic metric below. We suggest the following steps:\nFit a parametric quadratic function modeling the relationship between loss and the magic metric via experiments varying learning rates and proportions.\nEstimate the boundary of the magic metric value beyond which the model\u2019s loss starts to deviate significantly from the original one.\nBalance the learning progress on the target domain with the retention rate on the source domain by selecting a suitable magic metric larger than the boundary.\nIf the magic metric substantially exceeds the estimated boundary, it indicates that the model retains more knowledge from the source domain; conversely, it facilitates a more rapid learning pace on the target domain.\nThe above guideline can potentially explain why Lemur (Xu et al., 2024  ###reference_b48###) demonstrated negligible performance deterioration on natural language benchmarks (e.g., MMLU) despite undergoing continual pre-training from Llama-2 on an extremely imbalanced data distribution (i.e., text:code as 1:10). The employment of a smaller learning rate (i.e., 4e-5) during Lemur\u2019s training likely preserved the magic metric within an good range, allowing the model to maintain its proficiency in the source natural language domain."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data Sources",
            "text": "Here we describe all the corpus used in our training.\nNote that we performed an additional round of data deduplication and cleaning on these datasets before using them.\nSlimPajama (Soboleva et al., 2023  ###reference_b39###) is a high-quality dataset comprising 627B tokens, curated by rigorously cleaning and deduplicating the RedPajama Corpus (Computer, 2023  ###reference_b11###). It primarily focuses on English, and removes 49.6% of low-quality and duplicate data from RedPajama.\nWe use the released version on HuggingFace 777https://huggingface.co/datasets/cerebras/SlimPajama-627B  ###reference_limPajama-627B###.\nSkyPile (Wei et al., 2023  ###reference_b45###) is a massive, high-quality Chinese dataset for pre-training. It comprises 233M web pages, totaling 150B tokens, carefully filtered and deduplicated from public web sources. We download SkyPile by accessing its hosted dataset on HuggingFace 888https://huggingface.co/datasets/Skywork/SkyPile-150B  ###reference_yPile-150B###.\nCC100 999https://data.statmt.org/cc-100  ###reference_data.statmt.org/cc-100### is a multilingual corpus comprising monolingual data from over  languages. The corpus was original constructed for training the XLM-R model (Conneau et al., 2020  ###reference_b12###), a powerful cross-lingual language model. The data was sourced from the Common Crawl project (Rana, 2010  ###reference_b35###). Specifically, the corpus was generated by processing Common Crawl snapshots from January to December 2018, using the open-source CC-Net repository (Wenzek et al., 2020a  ###reference_b46###). In our pre-training corpus, we take the Indonesian, Malay, Lao, Thai and Vietnamese subsets.\nThe CC100 corpus is a great resource for multilingual languages due to its high quality, but it has already split every document into separate paragraphs, making it serve as a paragraph-level corpus. We believe using paragraphs as examples would greatly hurt the document-level performance of the model, as evidenced by our preliminary study. Therefore, we also consider MADLAD-400 (Kudugunta et al., 2023  ###reference_b24###), a manually audited and large-scale multilingual corpus spanning 419 languages. MADLAD-400 is also based on CommonCrawl, which uses all available corpus till August 2022. In our pre-training corpus, we take its clean version, downloaded from the dataset hosted by HuggingFace 101010https://huggingface.co/datasets/allenai/MADLAD-400  ###reference_DLAD-400###.\nWe utilize the Wikipedia dump (encompassing Malay, Indonesian, Thai, and Vietnamese) up to November 2023 from the Wikipedia dataset hosted on HuggingFace 111111https://huggingface.co/datasets/wikimedia/wikipedia  ###reference_wikipedia###. It should be noted that some of the Wikipedia corpus may be duplicated, as the SlimPajama dataset has already included the multilingual Wikipedia corpora.\nWe collect the Malay, Indonesian, Thai and Vietnamese subtitles from the OPUS OpenSubtitles category 121212https://opus.nlpl.eu/OpenSubtitles-v2018.php  ###reference_hp###. For all subtitles, we use a sliding window of  to concatenate adjacent subtitles to compose longer documents. An example of Indonesian subtitle can be found below:\nWhile our preliminary studies indicate that translation data may have similar effects to document-level code-switching, we still incorporated translation data since translation is an important task.\nWe curate a selection of English-SEA language translation pairs available in the OPUS project 131313https://opus.nlpl.eu/  ###reference_opus.nlpl.eu/### (e.g., TED2020 talks). Notably, we observe substantial duplication within the translation data, thus necessitating a further deduplication step.\nConcurrently, to account for both directions, we processed data for both English-to-SEA and SEA-to-English translation directions for each example. An illustrative example is provided below:"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Dataset for Replay",
            "text": "To mitigate catastrophic forgetting of English and Chinese capabilities of our models, we consider high-quality English and Chinese datasets as part of our data sources during continual pre-training.\nSlimPajama (Soboleva et al., 2023  ###reference_b39###  ###reference_b39###) is a high-quality dataset comprising 627B tokens, curated by rigorously cleaning and deduplicating the RedPajama Corpus (Computer, 2023  ###reference_b11###  ###reference_b11###). It primarily focuses on English, and removes 49.6% of low-quality and duplicate data from RedPajama.\nWe use the released version on HuggingFace 777https://huggingface.co/datasets/cerebras/SlimPajama-627B  ###reference_limPajama-627B###  ###reference_limPajama-627B###.\nSkyPile (Wei et al., 2023  ###reference_b45###  ###reference_b45###) is a massive, high-quality Chinese dataset for pre-training. It comprises 233M web pages, totaling 150B tokens, carefully filtered and deduplicated from public web sources. We download SkyPile by accessing its hosted dataset on HuggingFace 888https://huggingface.co/datasets/Skywork/SkyPile-150B  ###reference_yPile-150B###  ###reference_yPile-150B###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset for SEA Languages",
            "text": "CC100 999https://data.statmt.org/cc-100  ###reference_data.statmt.org/cc-100###  ###reference_data.statmt.org/cc-100### is a multilingual corpus comprising monolingual data from over  languages. The corpus was original constructed for training the XLM-R model (Conneau et al., 2020  ###reference_b12###  ###reference_b12###), a powerful cross-lingual language model. The data was sourced from the Common Crawl project (Rana, 2010  ###reference_b35###  ###reference_b35###). Specifically, the corpus was generated by processing Common Crawl snapshots from January to December 2018, using the open-source CC-Net repository (Wenzek et al., 2020a  ###reference_b46###  ###reference_b46###). In our pre-training corpus, we take the Indonesian, Malay, Lao, Thai and Vietnamese subsets.\nThe CC100 corpus is a great resource for multilingual languages due to its high quality, but it has already split every document into separate paragraphs, making it serve as a paragraph-level corpus. We believe using paragraphs as examples would greatly hurt the document-level performance of the model, as evidenced by our preliminary study. Therefore, we also consider MADLAD-400 (Kudugunta et al., 2023  ###reference_b24###  ###reference_b24###), a manually audited and large-scale multilingual corpus spanning 419 languages. MADLAD-400 is also based on CommonCrawl, which uses all available corpus till August 2022. In our pre-training corpus, we take its clean version, downloaded from the dataset hosted by HuggingFace 101010https://huggingface.co/datasets/allenai/MADLAD-400  ###reference_DLAD-400###  ###reference_DLAD-400###.\nWe utilize the Wikipedia dump (encompassing Malay, Indonesian, Thai, and Vietnamese) up to November 2023 from the Wikipedia dataset hosted on HuggingFace 111111https://huggingface.co/datasets/wikimedia/wikipedia  ###reference_wikipedia###  ###reference_wikipedia###. It should be noted that some of the Wikipedia corpus may be duplicated, as the SlimPajama dataset has already included the multilingual Wikipedia corpora.\nWe collect the Malay, Indonesian, Thai and Vietnamese subtitles from the OPUS OpenSubtitles category 121212https://opus.nlpl.eu/OpenSubtitles-v2018.php  ###reference_hp###  ###reference_hp###. For all subtitles, we use a sliding window of  to concatenate adjacent subtitles to compose longer documents. An example of Indonesian subtitle can be found below:\nWhile our preliminary studies indicate that translation data may have similar effects to document-level code-switching, we still incorporated translation data since translation is an important task.\nWe curate a selection of English-SEA language translation pairs available in the OPUS project 131313https://opus.nlpl.eu/  ###reference_opus.nlpl.eu/###  ###reference_opus.nlpl.eu/### (e.g., TED2020 talks). Notably, we observe substantial duplication within the translation data, thus necessitating a further deduplication step.\nConcurrently, to account for both directions, we processed data for both English-to-SEA and SEA-to-English translation directions for each example. An illustrative example is provided below:"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Preprocessing Pipeline",
            "text": "The data quality is crucial during continual pre-training.\nWe found that several publicly available multilingual datasets could be further cleaned and deduplicated.\nTo improve the data cleaning process for SEA languages specifically, we expanded our list of filtering words, trained new filtering models, and implemented a more aggressive deduplication strategy.\nAs a result of these optimizations, we extracted 61.19% of data for SEA languages from public datasets, and constructed the final SailCraft dataset.\nThe specific removal rates are shown in Figure 6  ###reference_###.\nFor implementation, we employ the Text-Dedup tool (Mou et al., 2023  ###reference_b27###) for data deduplication 222222https://github.com/ChenghaoMou/text-dedup  ###reference_###. It is well-packaged and offers easy command-line interaction. The tool also demonstrates excellent performance in deduplication benchmarking 232323https://tinyurl.com/bdf6zerm  ###reference_tinyurl.com/bdf6zerm###. It utilizes 5-gram MinHashLSH deduplication (Broder, 1997  ###reference_b7###) with a threshold of 0.7.\nThe number of permutations (hashes) is set to 256 to conserve memory usage. The Jaccard similarity threshold is set to 0.7.\nThe TextDedup tool further computes the optimal MinHashLSH parameter that minimizes the weighted sum of probabilities of false positives and false negatives 242424Refer to the optimal_param function at https://github.com/ChenghaoMou/text-dedup/blob/main/text_dedup/utils/analysis.py  ###reference_blob/main/text_dedup/utils/analysis.py###.\nUltimately, the number of bands and the number of rows per band (i.e., two crucial hyper-parameters) are optimized to 25 and 10, respectively.\nFor resource requirements, the data deduplication mainly consumes memory, CPU cores and disk spaces.\nThe memory requirement is primarily determined by the number of documents, rather than the disk size of the documents 252525https://huggingface.co/blog/dedup  ###reference_huggingface.co/blog/dedup###.\nTo facilitate dealing with large files within limited memory resource, we first split them into chunks (30GB) to make it tractable on our CPU server.\nIt takes about 200GB of memory to process a 30GB corpus with 256 permutations.\nIt takes approximately 30 minutes in total to deduplicate the 30GB corpus under 64 CPU cores.\nTo improve the deduplication performance, we iteratively cycled through the process of splitting into chunks, data deduplication, and recombining the chunks for 3 rounds until the chunk size converged.\nDefinitely, more aggressive deduplication hyper-parameters (i.e., more permutations and larger chunk sizes) would further improve the accuracy of data deduplication. We aim to improve it from both algorithmic and engineering perspectives in the next version.\nFor more discussion, please refer to the future work section.\nFigure 7  ###reference_### showcases the most prevalent duplicate sentences across various language subsets identified by our deduplication algorithm.\nThese duplicates span a wide range of domains, including medicine, customer service, and address information.\nThe presence of such noisy and redundant data can impede the pre-training process, as indicated by Penedo et al. (2023  ###reference_b30###).\nAdditionally, sensitive information like emails and phone numbers poses privacy risks.\nOur deduplication approach effectively addresses the prevalent scenario where documents are nearly identical, differing only in the interspersed template fields, as exemplified in Figure 8  ###reference_###. Despite cleaning efforts by CCNet (Wenzek et al., 2020b  ###reference_b47###) and MADLAD-400 (Kudugunta et al., 2023  ###reference_b24###), the quality of open datasets remains sub-optimal, underscoring the challenges in multilingual data cleaning.\nFor more deduplication cases, please refer to Appendix B  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Data Cleaning",
            "text": "###figure_10### The data cleaning mainly follows the BigScience data cleaning recipe 161616https://drive.google.com/file/d/1cCJ8sWE88TRLDAa3eHLmXO4JlkR2QzLY/view  ###reference_TRLDAa3eHLmXO4JlkR2QzLY/view###.\nNote that for most languages, we can make use of the publicly available resource.\nHowever, for several low-resource languages, we have to train model from scratch.\nThe entire data cleaning process is as follows:\nFiltering on the number of words.\nWe first tokenize the documents with sentencepiece model for each language.\nThen we count the number of words and remove the document that is less than min_length or is greater than the max_length.\nThe purpose of filtering short document is to remove the incorrect sentences or the sentences without enough context.\nThe purpose of filtering long document is to remove redundant information or exceed the maximum input length.\nFiltering on the character repetition ratio.\nWe first compile the list of character-level -grams for the given document.\nThen, we calculate the frequency for each -gram.\nWe define the character repetition ratio as the sum of frequencies of the top  most frequent -grams.\nA document is dropped if its character repetition ratio score is above the pre-defined threshold.\nNote that  is determined as a trade-off choice so that it can balance the distribution of short and long documents. Practically, we choose  as the square root of the amount of -grams.\nFiltering on the word repetition ratio.\nThe word repetition ratio is defined as the sum of frequencies of all -grams whose frequency is greater than 2.\nA document is dropped if its word repetition ratio score is above the pre-defined threshold.\nFiltering on the special characters ratio.\nA list is maintained to track special characters.\nIf a document\u2019s ratio of special characters exceeds a pre-defined threshold, it will be dropped. The purpose of this filter is to eliminate documents that consist primarily of special characters.\nFiltering on the stop words ratio.\nA list of stop words for each language is maintained, and a document will be removed if its stop words ratio is above the pre-defined threshold.\nIt is to remove machine-generated text that do not have much semantically meaningful information.\nHowever, one significant challenge arises with languages such as Chinese and Vietnamese that do not use spaces, as it becomes difficult to recognize stop words after tokenization.\nFollowing BigScience practise, we address the issue by expanding the stop list to include both word-level and byte-piece-level stop words, thereby enhancing the coverage and effectiveness of the filtering.\nFor stop words list, we collected those for Thai 171717https://github.com/stopwords-iso/stopwords-th/blob/master/stopwords-th.txt  ###reference_-th/blob/master/stopwords-th.txt### and Malay 181818https://github.com/stopwords-iso/stopwords-ms/blob/master/stopwords-ms.txt  ###reference_-ms/blob/master/stopwords-ms.txt### from available resources.\nHowever, we did not find relevant resources for Lao, and thus we translated the Thai stop words list into Lao.\nFiltering on the flagged words ratio.\nWe maintain a list of flagged words for each language.\nA document is removed if its flagged words ratio is above the pre-defined threshold.\nIt will remove buzzwords about the porn, which is harmful for model training.\nWe create or expand the flagged word list for Thai, Malay, and Lao by translating from English ones developed by BigScience.\nFiltering on the language identification prediction score.\nWe adopt the fastText (Joulin et al., 2016  ###reference_b21###) model 191919https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin  ###reference_pervised-models/lid.176.bin### to get the language identification result for each document and the corresponding confidence score.\nA document will be dropped if its confidence score is below the pre-defined threshold.\nThe filter is to remove unnatural content such as machine-generated text, advertisements, or frequently changing spoken language.\nHowever, it also bring the drawback that it would remove code-switching text that exists in SEA regions like Singlish (Singapore English) and Manglish (Malaysian English).\nFiltering on the perplexity score.\nWe adopt the KenLM (Heafield, 2011  ###reference_b18###) model to calculate the perplexity score of documents for each language.\nKenLM model are trained from the high-quality corpus like Wikipedia.\nA document will be removed if its perplexity score is above the pre-defined threshold.\nThe filter will remove the documents with unrelated words like tags, time, date and lots of repetitions.\nOne main drawback of the filter is that it would inevitably remove necessary documents that have a different distribution from Wikipedia.\nFor KenLM model, we download most language models from BigScience repo 202020https://github.com/bigscience-workshop/data-preparation/tree/main/preprocessing/training/01b_oscar_cleaning_and_filtering#2-download-everything-you-need  ###reference_a-preparation/tree/main/preprocessing/training/01b_oscar_cleaning_and_filtering#2-download-everything-you-need###.\nHowever, there is no KenLM model available for Thai, Malay and Lao.\nThus, we sample a high-quality subset from the Wikipedia corpus and train KenLM models with vocab size 65536 212121https://github.com/bigscience-workshop/data_tooling/tree/master/kenlm_training  ###reference_a_tooling/tree/master/kenlm_training###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Data Deduplication",
            "text": "The data deduplication procedure is the most important and challenging part in our data preprocessing.\nFirstly, it distills the corpus for efficient pre-training.\nMoreover, it further filters the noise information for effective training, like machine-generated advertisements that could not be easily recognized by rule-based cleaning methods.\nMost importantly, LLMs are less prone to exhibit memorization issues when training data has undergone thorough deduplication (Lee et al., 2022  ###reference_b25###).\n###figure_11### ###figure_12### For implementation, we employ the Text-Dedup tool (Mou et al., 2023  ###reference_b27###  ###reference_b27###) for data deduplication 222222https://github.com/ChenghaoMou/text-dedup  ###reference_###  ###reference_###. It is well-packaged and offers easy command-line interaction. The tool also demonstrates excellent performance in deduplication benchmarking 232323https://tinyurl.com/bdf6zerm  ###reference_tinyurl.com/bdf6zerm###  ###reference_tinyurl.com/bdf6zerm###. It utilizes 5-gram MinHashLSH deduplication (Broder, 1997  ###reference_b7###  ###reference_b7###) with a threshold of 0.7.\nThe number of permutations (hashes) is set to 256 to conserve memory usage. The Jaccard similarity threshold is set to 0.7.\nThe TextDedup tool further computes the optimal MinHashLSH parameter that minimizes the weighted sum of probabilities of false positives and false negatives 242424Refer to the optimal_param function at https://github.com/ChenghaoMou/text-dedup/blob/main/text_dedup/utils/analysis.py  ###reference_blob/main/text_dedup/utils/analysis.py###  ###reference_blob/main/text_dedup/utils/analysis.py###.\nUltimately, the number of bands and the number of rows per band (i.e., two crucial hyper-parameters) are optimized to 25 and 10, respectively.\nFor resource requirements, the data deduplication mainly consumes memory, CPU cores and disk spaces.\nThe memory requirement is primarily determined by the number of documents, rather than the disk size of the documents 252525https://huggingface.co/blog/dedup  ###reference_huggingface.co/blog/dedup###  ###reference_huggingface.co/blog/dedup###.\nTo facilitate dealing with large files within limited memory resource, we first split them into chunks (30GB) to make it tractable on our CPU server.\nIt takes about 200GB of memory to process a 30GB corpus with 256 permutations.\nIt takes approximately 30 minutes in total to deduplicate the 30GB corpus under 64 CPU cores.\nTo improve the deduplication performance, we iteratively cycled through the process of splitting into chunks, data deduplication, and recombining the chunks for 3 rounds until the chunk size converged.\nDefinitely, more aggressive deduplication hyper-parameters (i.e., more permutations and larger chunk sizes) would further improve the accuracy of data deduplication. We aim to improve it from both algorithmic and engineering perspectives in the next version.\nFor more discussion, please refer to the future work section.\nFigure 7  ###reference_###  ###reference_### showcases the most prevalent duplicate sentences across various language subsets identified by our deduplication algorithm.\nThese duplicates span a wide range of domains, including medicine, customer service, and address information.\nThe presence of such noisy and redundant data can impede the pre-training process, as indicated by Penedo et al. (2023  ###reference_b30###  ###reference_b30###).\nAdditionally, sensitive information like emails and phone numbers poses privacy risks.\nOur deduplication approach effectively addresses the prevalent scenario where documents are nearly identical, differing only in the interspersed template fields, as exemplified in Figure 8  ###reference_###  ###reference_###. Despite cleaning efforts by CCNet (Wenzek et al., 2020b  ###reference_b47###  ###reference_b47###) and MADLAD-400 (Kudugunta et al., 2023  ###reference_b24###  ###reference_b24###), the quality of open datasets remains sub-optimal, underscoring the challenges in multilingual data cleaning.\nFor more deduplication cases, please refer to Appendix B  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Data Composition",
            "text": "###table_1### As detailed in Section 2.3  ###reference_###, our algorithm involves utilizing proxy models to fit a linear regression model, which then aids in determining the optimal data mixture for large-scale training.\nTo elaborate, we extend the data mixture by extending it beyond language-level considerations to also include the source of the data.\nThis means we treat each language from every source as a distinct dataset and try to optimize the data mixture of these datasets.\nThe Qwen1.5-0.5B model serves as our proxy model, and we apply the optimized data mixture to the continual pre-training process across all model sizes.\nThe effective tokens and equivalent epochs in SailCraft are documented in Table 3  ###reference_###.\nFrom the table, we observe that, in terms of quality or diversity, the CC100 dataset exhibits a relative advantage over the MADLAD-400 dataset, particularly for Indonesian and Vietnamese."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Model Training",
            "text": "We obtain the Sailor models through continual pre-training of Qwen1.5 (Bai et al., 2023  ###reference_b5###) on 140B high-quality SEA tokens and 60B tokens for replay (see Section 4.4  ###reference_###).\nTo balance the training efficiency and debugging convenience, we leverage two codebases for different size model. For relatively large models (i.e., 4B and 7B), we utilize Megatron-LM (Shoeybi et al., 2019  ###reference_b38###), which supports tensor parallel and pipeline parallel to maximize the model flops utilization (MFU) of NVIDIA GPUs.\nBut the original Megatron codebase is a bit tricky to hands on due the absence of documents.\nThus, in practice, we employ the Megatron-LLM codebase 262626https://github.com/epfLLM/Megatron-LLM  ###reference_###.\nIt is an optimized Megatron codebase, paired with detailed documentation 272727https://epfllm.github.io/Megatron-LLM/  ###reference_### and one-stop scripts (e.g., model sharding, data preprocessing).\nFor relatively small models (i.e., 0.5B and 1.8B), we employ TinyLlama (Zhang et al., 2024  ###reference_b51###) codebase 282828https://github.com/jzhang38/TinyLlama  ###reference_###.\nThe codebase follows a compact and well-organised structure, which allows easy modifications for diverse purposes.\nMoreover, its optimisation enhancements significantly boost GPU utilisation.\nThe combination of swift prototyping and efficient training make TinyLlama as a valuable tool in both early develop stage and final continual training stage.\nFor training devices, we use NVIDIA A100 SXM4 40GB GPUs.\nTo accelerate multi-node training, we further employ the InfiniBand for low latency and extreme throughput.\nDuring the training, we employ\n64 GPU cards for 7B\u2009/\u20094B models, and 32 GPU cards for 1.8B\u2009/\u20090.5B model."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Training Infra",
            "text": "To balance the training efficiency and debugging convenience, we leverage two codebases for different size model. For relatively large models (i.e., 4B and 7B), we utilize Megatron-LM (Shoeybi et al., 2019  ###reference_b38###  ###reference_b38###), which supports tensor parallel and pipeline parallel to maximize the model flops utilization (MFU) of NVIDIA GPUs.\nBut the original Megatron codebase is a bit tricky to hands on due the absence of documents.\nThus, in practice, we employ the Megatron-LLM codebase 262626https://github.com/epfLLM/Megatron-LLM  ###reference_###  ###reference_###.\nIt is an optimized Megatron codebase, paired with detailed documentation 272727https://epfllm.github.io/Megatron-LLM/  ###reference_###  ###reference_### and one-stop scripts (e.g., model sharding, data preprocessing).\nFor relatively small models (i.e., 0.5B and 1.8B), we employ TinyLlama (Zhang et al., 2024  ###reference_b51###  ###reference_b51###) codebase 282828https://github.com/jzhang38/TinyLlama  ###reference_###  ###reference_###.\nThe codebase follows a compact and well-organised structure, which allows easy modifications for diverse purposes.\nMoreover, its optimisation enhancements significantly boost GPU utilisation.\nThe combination of swift prototyping and efficient training make TinyLlama as a valuable tool in both early develop stage and final continual training stage.\nFor training devices, we use NVIDIA A100 SXM4 40GB GPUs.\nTo accelerate multi-node training, we further employ the InfiniBand for low latency and extreme throughput.\nDuring the training, we employ\n64 GPU cards for 7B\u2009/\u20094B models, and 32 GPU cards for 1.8B\u2009/\u20090.5B model."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Training Details",
            "text": "We adopt most of the pre-training settings and model architectures from Qwen1.5 (Bai et al., 2023  ###reference_b5###).\nIt follows the standard transformer architecture (Vaswani et al., 2017  ###reference_b43###), adopts the pre-normalization with RMSNorm (Jiang et al., 2023b  ###reference_b20###), SwiGLU activation (Shazeer, 2020  ###reference_b37###) and rotary positional embeddings (Su et al., 2022  ###reference_b40###).\nNotably, Qwen1.5 adds a bias item in attention of the QKV layer to improve the extrapolation ability.\nMeanwhile, for the 0.5B model, we set tie_word_embeddings to False, i.e., not tying the learning of the input embedding (embedding module) and output projection (lm_head module). Thus, the parameter of Sailor 0.5B is approximately 0.6B. However, we still name it 0.5B to be consistent with Qwen1.5.\nDuring training, we utilize a context window length of 4,096, and integrate Flash Attention 2 (Dao, 2023  ###reference_b14###) to improve the training efficiency and reduce the memory usage 292929In contrary to Flash Attention 1 (Dao et al., 2022  ###reference_b15###), Flash Attention 2 makes it possible to train model on an arbitrary dataset that also includes padding tokens..\nWe utilize AdamW (Kingma & Ba, 2014  ###reference_b23###) for optimization, with the hyper-parameters .\nWe use the weight decay of  and the gradient clipping of .\nWe train models with BFloat16 mixed precision to balance the training efficiency and stability.\nNotably, we set attention_softmax_in_fp32 to True to execute attention masking and Softmax operations in fp32, thereby preventing precision underflow 303030https://github.com/huggingface/transformers/pull/17437  ###reference_s/pull/17437###.\nThe final pre-training corpus, SailCraft, is composed of approximately 200B tokens, integrating both SEA tokens and replay tokens, as elaborated in Section 4.4  ###reference_###.\nWe use the batch size of 4M tokens and the learning rate of 1e-4.\nFollowing a warmup period of 500 steps, the learning rate remains constant.\nThis scheduling strategy encourages more transferable conclusions from simulations and allows for easier recovery from interrupted training sessions.\nGenerally Sailor models consume around 200B tokens, completing a full pass through the SailCraft corpus once. However, the Sailor-0.5B model undergoes training with 400B tokens, equivalent to 2 epochs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Sailor models are evaluated on several high-quality benchmarks, including question answering, commonsense reasoning, reading comprehension and examination.\nThe XQuAD dataset (Artetxe et al., 2020  ###reference_b4###) (Thai, Vietnamese) and the TydiQA dataset (Clark et al., 2020  ###reference_b10###) (Indonesian) were selected as the representative benchmarks for question answering. The XQuAD dataset comprises 1,190 question-answer pairs from professional translations of the development set of SQuAD v1.1 (Rajpurkar et al., 2016  ###reference_b34###). The TydiQA dataset covers 204,000 question-answer pairs directly sourced from data in their original languages, with human-written questions.\nThe XCOPA dataset (Ponti et al., 2020  ###reference_b32###) (Indonesian, Thai, Vietnamese) provides two choices for each premise, requiring the model to select one that better addresses either the cause or effect of the event mentioned in the premise.\nThe BELEBELE dataset (Bandarkar et al., 2023  ###reference_b6###) is a large-scale multiple-choice machine reading comprehension benchmark spanning 122 languages. The Indonesian, Thai, and Vietnamese subsets were selected to evaluate model performance. Each question is provided with a short paragraph of context and four possible options.\nThe M3Exam dataset (Zhang et al., 2023  ###reference_b52###) (Javanese, Thai, Vietnamese) is a multilingual exam benchmark collected from official school tests used in nine countries.\nNote that we chose its Javanese subset since the Indonesian version has yet to be released."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Benchmark",
            "text": "The XQuAD dataset (Artetxe et al., 2020  ###reference_b4###  ###reference_b4###) (Thai, Vietnamese) and the TydiQA dataset (Clark et al., 2020  ###reference_b10###  ###reference_b10###) (Indonesian) were selected as the representative benchmarks for question answering. The XQuAD dataset comprises 1,190 question-answer pairs from professional translations of the development set of SQuAD v1.1 (Rajpurkar et al., 2016  ###reference_b34###  ###reference_b34###). The TydiQA dataset covers 204,000 question-answer pairs directly sourced from data in their original languages, with human-written questions.\nThe XCOPA dataset (Ponti et al., 2020  ###reference_b32###  ###reference_b32###) (Indonesian, Thai, Vietnamese) provides two choices for each premise, requiring the model to select one that better addresses either the cause or effect of the event mentioned in the premise.\nThe BELEBELE dataset (Bandarkar et al., 2023  ###reference_b6###  ###reference_b6###) is a large-scale multiple-choice machine reading comprehension benchmark spanning 122 languages. The Indonesian, Thai, and Vietnamese subsets were selected to evaluate model performance. Each question is provided with a short paragraph of context and four possible options.\nThe M3Exam dataset (Zhang et al., 2023  ###reference_b52###  ###reference_b52###) (Javanese, Thai, Vietnamese) is a multilingual exam benchmark collected from official school tests used in nine countries.\nNote that we chose its Javanese subset since the Indonesian version has yet to be released."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Evaluation Protocol",
            "text": "As for evaluation, following established evaluation protocols, we employed the awesome evaluation platform OpenCompass (Contributors, 2023  ###reference_b13###) to build up our evaluation code 313131The code can be found at https://github.com/sail-sg/sailor-llm  ###reference_###..\nThe performance of all models is assessed based on the 3-shot Exact Match (EM) and F1 performance, with prompts provided in native languages (e.g., Indonesian task description for Indonesian tasks).\nNote that we keep the tokenizer consistent when computing the F1 scores of different models.\nFollowing the evaluation approaches adopted in OpenCompass (Contributors, 2023  ###reference_b13###) and the Eleuther AI evaluation framework (Gao et al., 2023  ###reference_b17###) on the popular HellaSwag benchmark Zellers et al. (2019  ###reference_b50###), we reformulated the tasks with limited output spaces (i.e., XCOPA, BELEBELE) as continuation writing tasks.\nIt is to say, each possible answer is appended to the given input or question, and the one that achieves the lowest perplexity score is considered as the model prediction.\nAs for the M3Exam dataset, we adopt the official evaluation method used in Zhang et al. (2023  ###reference_b52###) to evaluate all models. The evaluation approach involves directly prompting LLMs to produce the correct option ID when presented with a question and its corresponding options."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Baseline Setup",
            "text": "We compare Sailor models with SeaLLM (Nguyen et al., 2023b  ###reference_b29###), Sea-Lion (AI Singapore, 2023  ###reference_b2###), Typhoon (Pipatanakul et al., 2023  ###reference_b31###), and VinaLLaMA (Nguyen et al., 2023a  ###reference_b28###).\nOur reporting strictly adheres to the same evaluation methodology to ensure a fair comparison, and we make much effort to closely match the reported results of all baselines."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Experimental Results",
            "text": "Experimental results shown in Table 4  ###reference_###, 5  ###reference_###, 6  ###reference_### indicate that our Sailor models typically outperform the baseline model, Qwen1.5, in terms of performance on SEA languages.\nAdditionally, the performance of Sailor models is either superior or comparable to major SEA LLMs such as SeaLLM, Sea-Lion, Typhoon, and VinaLLaMA on these benchmarks.\nHowever, it is not the case for M3Exam. As shown in Table 7  ###reference_###, our Sailor models exhibit no evident advantage over Qwen1.5 at the 4B parameter scale or lower, and in certain instances, it displays noticeable weaknesses.\nWe have observed that the discrepancy is due to a significant option bias, which leads the Sailor models to favor certain option IDs (e.g., always C) when making predictions. Interestingly, a similar phenomenon was also observed among other baseline LLMs focusing on SEA languages.\nWhile instruction tuning could mitigate the option bias, we have chosen not to tune the Sailor models to maintain fairness and consistency in the evaluation process.\nWe also provide additional results evaluated using the HellaSwag protocol in Appendix C  ###reference_###, which is better aligned with other benchmark results."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "In this paper, we present the Sailor family of open language models, tailored for South-East Asian languages, which exhibit strong performance across various multilingual tasks and benchmarks, fostering advancements in multilingual language models for the SEA region. Here are some of the most important future works:\nWe could improve data deduplication through document-preserving deduplication to ensure the completeness of documentation. It should adhere to the following principles:\n(1) If we perform deduplication at the paragraph level, it\u2019s crucial to recombine these paragraphs into a coherent document.\n(2) If a duplicated paragraph is found across multiple documents, it is advisable to retain only one instance of the paragraph in a single document while removing the duplicate occurrences from the other documents.\nFor example, assuming document A, B, and C each contains paragraphs a1, a2, b1, b2, c1 and c2 respectively.\nThe paragraph a1 is a duplicate of b1, while c1 is a duplicate of b2.\nThen the algorithm should filter out the duplicated paragraphs b1 and b2 from the document B, thereby preserving the integrity and completeness of both document A and C.\nIn the diverse and multilingual context of South-East Asia religion, it is quite common for users to communicate in various languages.\nThis creates a particularly challenging scenario for chat models, which must be adept at understanding and responding to queries in multiple languages.\nFor example, if the user asks the chat model a question in Indonesian, such as \u201cDraf undangan pernikahan dalam bahasa Vietnam\u201d (English: Wedding invitation draft in Vietnamese), the user would expect the model to reply in Vietnamese.\nCurrently, in our internal evaluations, the Sailor models does not address the challenge well.\nWe plan to build cross-lingual instruction datasets to address the problem.\nAlthough we introduce careful code-switching techniques to improve the model performance on language mixing, there are still some natural code-switching scenarios which are challenging for our models.\nIt usually involves the alternation between two or more languages within a single utterance.\nThe natural code-switching behavior is deeply rooted in the multilingual and multicultural societies of South-East Asia, where individuals frequently navigate between multiple linguistic and cultural backgrounds.\nHowever, for multilingual LLMs, it is still challenging to generate code-switching texts (Yong et al., 2023  ###reference_b49###).\nTo broaden the impact of open language models for SEA languages, we are dedicated to expanding our coverage to include more languages from the region.\nWe plan to achieve the goal by gathering high-quality training corpora from all CommonCrawl snapshots and other open resources.\nMoreover, we aim to explore language generalization techniques to transfer knowledge from high-resource languages to low-resource languages, thereby enhancing the capabilities of Sailor models for the underserved languages in the SEA region."
        }
    ],
    "url": "http://arxiv.org/html/2404.03608v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "6",
            "6.1",
            "6.2",
            "6.3",
            "6.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "2.1",
            "2.3",
            "2.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.03608v1",
        "paper_title": "Sailor: Open Language Models for South-East Asia",
        "research_background": "**Motivation**:\nThe paper is motivated by the need to improve language models' capabilities in multilingual settings, specifically focusing on South-East Asian languages. Existing language models face several challenges including handling multiple languages, dealing with code-switching, sensitivity to subword segmentation, and the necessity for high-quality multilingual corpora that often require extensive cleaning and deduplication.\n\n**Research Problem**:\nThe research problem addressed by this paper is the development of more effective language models for South-East Asian languages. To tackle this, it explores continual pre-training, handling mixed language inputs through code-switching techniques, improving subword segmentation robustness, and addressing the quality of multilingual corpora through data cleaning strategies. The goal is to provide insights and methodologies that could enhance the performance of large-scale multilingual language models.\n\n**Relevant Prior Work**:\n1. Studies have shown that language models encounter difficulties when dealing with multiple languages, and continual pre-training is highlighted as a method to enhance specific language capabilities.\n2. Research has demonstrated the effectiveness of code-switching techniques in improving the handling of mixed language inputs in multilingual models.\n3. The sensitivity of language models to subword segmentation has been documented, with techniques like Byte Pair Encoding (BPE) dropout suggested to improve robustness.\n4. High-quality multilingual corpora, though available, often require additional data deduplication and cleaning to be effective for language model training.\n5. Previous simulation experiments on smaller models have been useful in revealing performance trends applicable to larger-scale models, suggesting a foundational approach for the current study.",
        "methodology": "The methodology for developing Sailor, a series of open language models (LMs) for South-East Asia (SEA), involves several key strategies to enhance model performance and ensure balanced representation across languages. The main steps in the proposed method are:\n\n1. **Deduplication and Data Efficiency**:\n   - The deduplication method focuses on maximizing unique tokens while retaining contextual relevance through random combination of adjacent paragraphs.\n   - This technique maintains context coherence, especially useful for subtitles and other short sentences.\n\n2. **Code-Switching**:\n   - Document-level code-switching was employed over word-level due to better performance in preliminary experiments.\n   - This involved packing documents from various languages together rather than separately.\n   - Word-level code-switching involved replacing 10% of words in SEA languages with English phrases.\n\n3. **Data Quality**:\n   - Aggressive cleaning and multi-round deduplication were applied, using well-curated open datasets and further filtering.\n   - Enhanced pre-training efficiency and stability were observed, along with reduced model memorization issues.\n\n4. **Prompt Sensitivity and Tokenization**:\n   - Models showed sensitivity to small variations in prompts, particularly spaces.\n   - BPE-Dropout was employed to handle tokenization vulnerabilities by introducing stochastic corruption in the segmentation procedure, leading to enhanced performance and robustness.\n\n5. **Vocabulary Expansion**:\n   - Expanding vocabulary in models like Mistral and Llama-2 proved challenging without significant continual pre-training.\n   - Various methods to address this problem, such as warming up embedding layers and modularized continual-training, were explored but did not yield expected results.\n\n6. **Multilingual Training**:\n   - Sailor was developed based on Qwen1.5, which is multilingual-friendly and has a large vocabulary size.\n   - A balance between English and SEA languages was pursued, noting the trade-off and competition among languages for fixed model capacity.\n   - Findings highlighted the importance of learning rate tuning to balance language acquisition and forgetting.\n\n7. **Algorithm for Language Representation**:\n   - A new algorithm was created to determine appropriate language weights during continual pre-training.\n   - This involved randomized data mixture experiments with simulations and linear regression models to find the optimal data mixture for large-scale training.\n\n### Key Components and Innovations:\n- **Document-level Code-Switching**: Enhances model performance in a multilingual context more effectively than word-level code-switching.\n- **Deduplication Strategy**: Randomly combines adjacent examples before global shuffling to maintain context, optimizing data efficiency.\n- **BPE-Dropout**: Addresses prompt sensitivity and improves model robustness through stochastic corruption in tokenization.\n- **Optimized Data Mixture Algorithm**: Uses randomized experiments and linear regression to find the most effective balance between target languages, improving model training and representation of SEA languages.\n\nThis methodology underpins the development of Sailor models, aiming to achieve balanced and robust performance across diverse SEA languages.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\n**Question Answering:**\n- **XQuAD:** Includes Thai and Vietnamese subsets with 1,190 question-answer pairs from translations of the SQuAD v1.1 development set.\n- **TydiQA:** Covers the Indonesian subset with 204,000 original question-answer pairs with human-written questions.\n\n**Commonsense Reasoning:**\n- **XCOPA:** Involves Indonesian, Thai, and Vietnamese subsets, with models needing to select the better answering choice for the cause or effect of an event.\n\n**Reading Comprehension:**\n- **BELEBELE:** Indonesian, Thai, and Vietnamese subsets from a large-scale multiple-choice benchmark with context paragraphs and four answer options.\n\n**Examination:**\n- **M3Exam:** Javanese, Thai, and Vietnamese subsets from a multilingual exam benchmark collected from school tests. Used the Javanese subset as the Indonesian version is not yet available.\n\n#### Baselines\nThe main baselines typically include various state-of-the-art natural language processing models and multilingual models known for strong performance in related tasks. Explicit baselines are not detailed in the provided text but generally include models such as BERT, mBERT, XLM-R, and similar architectures.\n\n#### Evaluation Metrics\nEvaluation metrics typically focus on:\n- **Accuracy:** Correct prediction rate for multiple-choice and reading comprehension tasks.\n- **F1 Score:** Harmonic mean of precision and recall, particularly relevant for question answering tasks.\n\n#### Main Experimental Results\nThe original text does not specify the precise results. However, we can infer that results would typically include comparisons of accuracy and F1 scores across different models on the mentioned datasets in various languages, with the aim to show effectiveness of the Sailor models compared to baseline models. The results section would highlight whether Sailor models outperform existing benchmarks and demonstrate their capability in handling South-East Asian languages effectively."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the impact of data deduplication and token combination on the performance of language models during continual pre-training.",
            "experiment_process": "The study involved the use of deduplication at the paragraph level to enhance data efficiency by maximizing the number of unique tokens the model encounters. This method was applied to datasets like CC100. To address the potential adverse effects on model performance due to the loss of contextual connections, the researchers combined several adjacent examples into one before applying a global shuffle. This method was also applied to certain sources like subtitles.",
            "result_discussion": "Randomly combining adjacent examples helped mitigate the issues introduced by paragraph-level deduplication, allowing for the reconstruction of context when necessary. This method ensured better data efficiency and improved the model's performance by maintaining relevant connections within the data.",
            "ablation_id": "2404.03608v1.No1"
        },
        {
            "research_objective": "To assess the effectiveness of different code-switching approaches on the performance of language models in handling SEA languages.",
            "experiment_process": "Two types of code-switching were tested: document-level and word-level. For document-level code-switching, documents from various languages were packed together instead of individually for each language. For word-level code-switching, 10% of words in SEA languages within the documents were replaced with their corresponding English phrases. Experiments were conducted on the TinyLlama model to compare the performance of these approaches.",
            "result_discussion": "Document-level code-switching performed better than word-level code-switching or a combination of both. Translation data did not show significant improvement when used with document-level code-switching but did improve performance when used alone. Therefore, document-level code-switching was chosen for continual pre-training to enhance performance.",
            "ablation_id": "2404.03608v1.No2"
        },
        {
            "research_objective": "To evaluate the impact of employing BPE-Dropout in enhancing the robustness of language models.",
            "experiment_process": "BPE-Dropout was applied during continual pre-training to stochastically corrupt the segmentation procedure of Byte Pair Encoding (BPE), which typically uses a greedy segmentation strategy. This experiment aimed to alleviate the model\u2019s sensitivity to small variations in prompt tokenization by regularizing subword formation.",
            "result_discussion": "BPE-Dropout slightly increased the loss on greedy subword segmentation but significantly enhanced both the performance and robustness of the models. It reduced vulnerabilities related to unexpected spaces in tokenized outputs, as observed in models like Qwen1.5, Mistral, and Llama 2.",
            "ablation_id": "2404.03608v1.No3"
        },
        {
            "research_objective": "To investigate the feasibility and impact of vocabulary expansion on language model performance.",
            "experiment_process": "The study attempted to expand the vocabulary of models like Mistral and Llama-2 by training them on additional subwords. Techniques such as warming up the embedding layer and modularized continual-training were also tested to mitigate performance degradation. Proxy models were used for initial evaluation before applying these changes on larger models.",
            "result_discussion": "Vocabulary expansion without sufficient continual pre-training resulted in lower model performance, with the expanded model\u2019s question-answering performance in Thai remaining 10% lower than the original. The methods explored did not yield the expected improvements, indicating that vocabulary expansion is a challenging problem requiring further research.",
            "ablation_id": "2404.03608v1.No4"
        },
        {
            "research_objective": "To develop a balanced and effective data mixture for continual pre-training of an LLM tailored for the SEA region, ensuring representation across target languages.",
            "experiment_process": "A new algorithm was developed to determine appropriate weights for various languages during continual pre-training. Conducting randomized data mixture experiments with proxy models for a limited number of training steps, followed by linear regression modeling, enabled simulations to optimize the joint loss. The optimized data mixture was then applied to large-scale training.",
            "result_discussion": "The algorithm successfully identified the data mixture that minimized joint loss, ensuring balanced representation across SEA languages. This method demonstrated the potential to achieve improved LLM performance tailored for multilingual use cases.",
            "ablation_id": "2404.03608v1.No5"
        },
        {
            "research_objective": "To identify an optimal learning rate that balances the retention of source domain knowledge and the acquisition of target domain language skills.",
            "experiment_process": "By generating training configurations with varying learning rates and language proportions, proxy models were used to analyze the trade-offs between English and SEA languages. After determining a suitable learning rate, fine-grained data mixture simulations were conducted to optimize joint loss across all languages.",
            "result_discussion": "The learning rate was identified as a critical hyper-parameter, with a peak value of 1e-4 selected based on the performance on SEA languages and English. This setting effectively balanced the acquisition of SEA language skills without significant loss in English performance, illustrating the importance of a well-tuned learning rate.",
            "ablation_id": "2404.03608v1.No6"
        }
    ]
}