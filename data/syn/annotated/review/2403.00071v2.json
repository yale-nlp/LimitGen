{
    "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
    "abstract": "This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position better and more robustly. Our extensive LLM experiments also show superior performance after applying Resonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on both upstream language modeling tasks and a variety of downstream long-text applications.111https://github.com/sheryc/resonance_rope.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent advancements in Large Language Models (LLMs) have demonstrated their potential across a wide spectrum of natural language processing tasks, showcasing their ability to handle complex interactions, document analyses, professional writing, and advanced reasoning with a unified approach (OpenAI, 2023  ###reference_b15###; Touvron et al., 2023a  ###reference_b26###, b  ###reference_b27###; Jiang et al., 2024  ###reference_b9###). As these models are increasingly adapted for complex applications, challenges arise in scenarios requiring the comprehension or generation of long texts. Specifically, the train-short-test-long (TSTL) scenario (Press et al., 2022  ###reference_b17###) highlights a limitation where LLMs, pre-trained on shorter sequences, struggle with out-of-distribution (OOD) token positions in longer sequences, impacting their performance in real-world applications (Zhao et al., 2023  ###reference_b32###).\nRecent efforts to enhance TSTL performance have focused on LLMs equipped with Rotary Position Embedding (RoPE) (Su et al., 2024  ###reference_b24###), such as LLaMA (Touvron et al., 2023a  ###reference_b26###, b  ###reference_b27###) and Mistral (Jiang et al., 2023  ###reference_b8###), owing to their exceptional capabilities and widespread adoption. These initiatives aim to refine the test-time computation of RoPE position embedding by introducing a scaling factor to either the position index of each token (Chen et al., 2023  ###reference_b5###) or RoPE\u2019s base value (Xiong et al., 2023  ###reference_b31###; Liu et al., 2024  ###reference_b12###; Peng et al., 2024  ###reference_b16###). These methods ensure that the position embeddings for out-of-distribution (OOD) positions remain within the range experienced during pre-training. This minimizes the need for the model to adapt to new position embedding value ranges, a task that is inherently difficult.\nIn this paper, we introduce Resonance RoPE, a novel technique designed to further narrow the generalization gap on position embeddings in TSTL scenarios. Recognizing that RoPE\u2019s position embedding is governed by a complex, non-linear function, we posit that minimizing extrapolation on OOD positions, while crucial, is insufficient. We argue that it is equally vital to address the interpolation of RoPE features at the OOD positions. By implementing Resonance RoPE, we slightly scale each RoPE feature to correspond to an integer wavelength. This adjustment aligns each RoPE feature\u2019s wavelength with a specific token span length, enabling it to \"resonate\" with a particular local context length. This simple modification effectively reduces the generalization gap for over half of the position embedding features in LLaMA and LLaMA2 under TSTL scenarios. Furthermore, our approach is compatible with RoPE and any RoPE-based scaling techniques, enhancing their performance in TSTL situations without the need for additional computational resources during training or inference.\nAdditionally, to facilitate further research on position embeddings, we present a new synthetic benchmark tailored for TSTL scenarios, named PosGen. Improving position embeddings for TSTL requires a detailed analysis of the cause of failures in handling longer contexts. However, current benchmarks, such as those measuring perplexity in long context (Rae et al., 2020  ###reference_b18###; Huang et al., 2021  ###reference_b7###; Wu et al., 2022  ###reference_b30###) and most synthetic TSTL tasks (Liu et al., 2023  ###reference_b11###; Kazemnejad et al., 2023  ###reference_b10###) face a common issue: the difficulty of generating the next token increases with context length. This makes it difficult to determine whether a model\u2019s failure is due to its inability to generate more complex tokens or its failure to recognize out-of-distribution (OOD) positions. PosGen addresses this limitation by standardizing the difficulty level of token generation across all positions. This ensures that any observed shortcomings are directly related to the model\u2019s inability to identify and handle new token positions effectively.\nOur contributions in this study are threefold:\nWe propose Resonance RoPE, an innovative modification to RoPE based on an in-depth analysis of the wavelengths of RoPE features, aiming to narrow the generalization gap in TSTL scenarios across RoPE and similar RoPE-based scaling techniques, without necessitating extra computational resources during runtime.\nWe present PosGen, a newly developed synthetic benchmark tailored for TSTL scenarios. This benchmark is specifically designed to disentangle the complexities associated with generating tokens in longer contexts from the challenges posed by recognizing new positions or position embedding values.\nThrough rigorous testing of Resonance RoPE on both RoPE and YaRN within the PosGen benchmark, we demonstrate its ability to enhance performance on out-of-distribution (OOD) positions, surpassing existing methods that do not include Resonance RoPE. Moreover, when applied to YaRN, Resonance RoPE further improves LLM\u2019s length extrapolation ability, as evidenced by lower perplexity in upstream TSTL language modeling and enhanced outcomes in downstream tasks involving lengthy contexts."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Long Context Evaluations",
            "text": "Evaluations of Transformer-based LLMs\u2019 long-context capabilities are twofold: synthetic task assessments for length extrapolation strategies and real-world task evaluations at the LLM scale. Synthetic evaluations target simple tasks such as long sequence classification Tay et al. (2021  ###reference_b25###) and arithmetic language modeling (Liu et al., 2023  ###reference_b11###; Kazemnejad et al., 2023  ###reference_b10###).\nLLM scale evaluations measure metrics such as perplexity (PPL) in extensive text corpora (e.g., PG19 (Rae et al., 2020  ###reference_b18###), GovReport (Huang et al., 2021  ###reference_b7###), GitHub (Wu et al., 2022  ###reference_b30###)) and complex tasks including summarization, question answering, and mathematical reasoning (An et al., 2023  ###reference_b1###; Bai et al., 2023  ###reference_b3###; Shaham et al., 2023  ###reference_b23###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Rotary Position Embedding (RoPE)",
            "text": "In Transformers (Vaswani et al., 2017  ###reference_b28###), the self-attention scores are softmax-normalized scaled attention logits :\nSuppose the input to a single attention head is , where  is the sequence length and  is the dimension of an attention head. RoPE injects the position information of each token into the  and  vectors by the following equations in the complex space:\nwhere  are trainable parameters, and  is a constant called the rotary base, which is set to   (Su et al., 2024  ###reference_b24###) or other integers or fractions (Xiong et al., 2023  ###reference_b31###; Peng et al., 2024  ###reference_b16###). This form makes the dot product between the -th query  and -th key  only depend on the input  and their relative distance :\nRoPE\u2019s real-number implementation divides the -dimension space into multiple -dimensional subspaces and applies real rotation matrix to each of them. Formally, define\na  block-diagonal matrix:\nwhere , and each  is a  rotation matrix:\nRoPE computes the attention logit  as follows:\nFor each two dimensions  of  and , its corresponding  reflects a temporal wavelength . This wavelength describes the token length for the corresponding RoPE features to encounter approximately the same rotary angle  in Equation 3  ###reference_###:\nAs an example, the wavelengths of LLaMA / LLaMA2\u2019s RoPE features range from  for  to  for ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Critical Dimensions of RoPE",
            "text": "In a TSTL scenario (Press et al., 2022  ###reference_b17###), one takes a model trained on texts with lengths up to , and tests it on a task with input lengths up to , with the scaling factor .\nRecently, Liu et al. (2024  ###reference_b12###) discovered that there may exist two \u201ccritical dimensions\u201d in RoPE features, which correspond to the dimensions  that satisfies  and . The dimensions of RoPE features above and below the critical dimension (which we denote as \u201cpost-critical dimensions\u201d and \u201cpre-critical dimensions\u201d, respectively) have different behaviors in TSTL: for post-critical dimensions (i.e., ), since their wavelengths satisfy , the training corpus does not cover all possible rotary angles  on a unit circle. Thus, these dimensions will encounter OOD value range on longer sequences. This is not an issue for pre-critical dimensions due to their shorter temporal wavelengths.\nThe concept of RoPE\u2019s critical dimensions implicitly guides the development of RoPE scaling methods. For example, previous RoPE scaling methods (Chen et al., 2023  ###reference_b5###; Xiong et al., 2023  ###reference_b31###; Peng et al., 2024  ###reference_b16###) mainly focus on reducing or avoiding value extrapolation on post-critical dimensions, and minimize post-training modifications to the pre-critical dimensions."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Yet another RoPE extensioN (YaRN)",
            "text": "YaRN (Peng et al., 2024  ###reference_b16###) is the current state-of-the-art RoPE scaling method for TSTL. It introduces the \u201cNTK-by-parts\u201d scaling for RoPE, which applies different scaling strategies to each RoPE feature according to its temporal wavelength.\nIn a TSTL scenario with scaling factor , YaRN scales the wavelength of the -th RoPE feature  to  and further fine-tune the model:\nwhere  is a piece-wise function depending on its corresponding wavelength , and two hyperparameters  and :\nEmpirically, for the LLaMA family, Peng et al. (2024  ###reference_b16###) suggests using  and . This setting avoids value range extrapolation on post-critical dimensions, while reducing modifications to the original pre-critical dimensions.\nIn addition to the \u201cNTK-by-parts\u201d RoPE scaling strategy mentioned above, YaRN also comprises a scaling strategy on the attention scores, which reduces the change in the entropy of the attention score on longer sequences. We maintain the complete design of YaRN in our experiments, but our analysis will focus on its RoPE scaling strategy."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed Method: Resonance RoPE",
            "text": "In this section, we introduce Resonance RoPE, a universal improvement for RoPE and RoPE-based scaling methods to (further) improve their length extrapolation performance.\nSuppose we abstract RoPE\u2019s Equation 4  ###reference_###, 5  ###reference_###: for any , we define . In a TSTL scenario where we generalize an LLM from length  to length , let us denote a scaled RoPE function by . To perform well on OOD positions it should reduce the feature gap  between token features seen during training and token features after scaling that we can define for each -th feature as:\nwhere  and  is the set of feature vectors to which we apply a position embedding. Note that the formulation of the feature gap is similar to the \u201cembedded vector distance\u201d metric proposed by Xiong et al. (2023  ###reference_b31###). However, these two metrics target totally different aspects of RoPE scaling methods. A more detailed comparison can be found in Appendix B  ###reference_###.\nExisting RoPE scaling methods (Xiong et al., 2023  ###reference_b31###; Peng et al., 2024  ###reference_b16###) mainly focus on the post-critical dimensions  of RoPE, since the rotary angle  on these dimensions extrapolates on OOD positions, hence creating a feature gap. In this section, we argue that reducing RoPE\u2019s feature interpolation on the pre-critical dimensions  is also beneficial for better length extrapolation.\nDue to a non-linear relationship between RoPE feature  and the token position  in Equation 3  ###reference_###, the interpolation on RoPE features is potentially hard for the model to generalize to.\nWe found that such potentially hard interpolation appears on the pre-critical dimensions , which have wavelengths  shorter than the pre-trained sequence length . By default, the rotary base  of RoPE features is an integer or a fraction, which makes their wavelength  not an integer. As the position index  increases, a phase shift of  occurs for the rotary angle  after each full rotation. This could potentially result in a large distribution gap between the RoPE features on positions seen during training and the OOD positions. This phenomenon is illustrated in Figure 1  ###reference_###.\n###figure_1### We tackle this issue by developing a synergistic modification to the conventional RoPE embedding, referred to as Resonance RoPE. It aims to identify the optimal angular frequency that minimizes the interpolation gap, which ensures the corresponding wavelength closely matches the original one while imposing alignment of the wavelength to an integer.\nMore specifically, for a given angular frequency set of RoPE , we round their wavelengths to their nearest integer to eliminate new rotary angles on each feature. We provide a pseudocode for Resonance RoPE in Algorithm 1  ###reference_###.\nAfter applying this technique, each RoPE feature repeats after  tokens, and therefore \u201cresonates\u201d with a specific span length and eliminates the interpolation gap between pre-trained and OOD positions on pre-critical dimensions. We illustrate the effect of Resonance RoPE on RoPE\u2019s feature gap on one of the pre-critical dimensions in Figure 1  ###reference_###. Moreover, we can prove the feature gap reducing ability of our method. As for above, we formalize Resonance RoPE\u2019s computation rule as .\nFor a RoPE-equipped model with context window , Resonance RoPE  reduces the feature gap on pre-critical dimensions to . Specifically, , , we have:\nfor all .\nSee the proof in Appendix A  ###reference_###.\nNote that although each pre-critical RoPE feature  repeats, the combination of all\n\nonly repeats after the least common multiple (LCM) of all pre-critical dimensions\u2019s wavelengths. For LLaMA2, this LCM value is greater than .\nBecause of its simplicity, Resonance RoPE can be applied on top of RoPE and all RoPE-based scaling methods to reduce their feature gap in TSTL and further improve their performance. Meanwhile, this method only involves an offline computation of the scaled , thus introducing no online computation overhead."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluating Position Embeddings with PosGen",
            "text": "###figure_2### In this section, we propose our new position embedding evaluation suite: PosGen, based on an analysis of common failure patterns on existing position embedding evaluation methods.\nWe consider a next token prediction task, where we expect the model to generate the token  given the input sequence . In TSTL scenarios, when a model succeeds in correctly generating a token up to position  but fails systematically afterwards, we observe two failure patterns:\nFailure due to harder algorithmic difficulty on generating later tokens.\nThe rule of generating a new token  may vary with the sequence length .\nGenerally, tokens placed later in the sequence depend on more context tokens, which incurs a more complex dependency pattern. During training on shorter sequences, the model only learns the token dependency rules involving up to  tokens, and might fail on longer sequences because it has never been exposed to the more complex dependency rules.\nFailure due to unrecognized new token positions. The difference between training and testing lengths in the TSTL setting creates a feature gap between the position indices or position embeddings in training and inference. This feature gap makes it difficult for the model to generalize to new positions due to unrecognized features. RoPE scaling methods mainly focus on reducing this type of length extrapolation failure.\nCurrently, neither perplexity-based evaluations (Rae et al., 2020  ###reference_b18###; Huang et al., 2021  ###reference_b7###; Wu et al., 2022  ###reference_b30###) nor synthetic TSTL evaluations (Kazemnejad et al., 2023  ###reference_b10###; Liu et al., 2023  ###reference_b11###) can effectively distinguish these two failure patterns, since the token generation difficulty tends to increase with respect to the sequence length in these tasks. To facilitate research on better position representations, we design PosGen, which controls the difficulty in generating tokens throughout the sequence to be identical, which effectively distinguishes the two types of TSTL failures. Failures in this benchmark are only due to the inability to recognize new token positions in TSTL scenarios.\nOur PosGen framework comprises three sub-tasks, with each extracting the general token dependency pattern of a different type of reasoning task. Suppose that we define a fixed function , where  is the model\u2019s vocabulary and  are predefined constants controlling the task\u2019s difficulty. The three subtasks of PosGen are as follows:\nRecursive. This task simulates the token dependency pattern of generating a Fibonacci-style sequence, where new tokens depend on  neighboring tokens only:  when .\nChain-of-Thought (CoT). This task simulates the token dependency pattern of CoT reasoning (Wei et al., 2022  ###reference_b29###), where new tokens depend on  neighboring tokens (simulating the previous reasoning step) and  tokens in the front (simulating the original question):  when .\nSemi-recursive. This task simulates the token dependency pattern of the last-letter concatenation task (Zhou et al., 2023  ###reference_b33###), where new tokens depend on both  neighboring tokens (simulating the current progress) and  tokens with varied distances according to a specific rule (simulating the word sequence):  when .\nBased on the equation for each subtask, when given the first  tokens, one can generate a sequence with unlimited length as the ground truth sequence. We show an example of PosGen in Figure 2  ###reference_###. As a TSTL benchmark, we train a model on a subtask with sequence length up to , and evaluate the model\u2019s accuracy on a longer sequence with length  generated by the same rule on the unseen positions , which we refer to as the \u201cOOD Accuracy\u201d (OOD Acc).\nThis metric measures how well a model can recognize the OOD positions and continue following the generation rule learned during training.\nAs a benchmark for position embeddings, a standard usage of this benchmark is to train a small Transformer (e.g., a 2-layer Transformer as used in our experiments) with different position embeddings on its training set with only short sequences, and test its OOD Accuracy on the test set with longer sequences.\nWe provide our experiment setting for PosGen in more details in Section 6.1.1  ###reference_.SSS1### and Appendix C.1  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate Resonance RoPE on three different TSTL tasks: a small-scale evaluation on our proposed PosGen task, and LLM-scale evaluations with LLaMA2-Chat (Touvron et al., 2023b  ###reference_b27###) on both language modeling perplexity and real-world long context applications."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Synthetic Task Evaluation",
            "text": "###figure_3###"
        },
        {
            "section_id": "6.1.1",
            "parent_section_id": "6.1",
            "section_name": "6.1.1 Experiment Setup",
            "text": "We first apply Resonance RoPE on RoPE and YaRN, assessing the model\u2019s performance on PosGen for unseen position recognition. We test on a modular addition task, which was proved to be learnable by a one-layer Transformer (Nanda et al., 2023  ###reference_b14###). We configured , and defined  with vocabulary .\nOur experiments involved training a two-layer Transformer. Each layer follows T5-Small\u2019s configurations (Raffel et al., 2020  ###reference_b19###) except for the position embeddings. In this model, each attention head has  dimensions. We apply different RoPE-based embeddings with the rotary base equal to . The models are trained on sequences of length , and evaluating on lengths of  for OOD Accuracy. In this experiment setting, each head has  RoPE features, out of which the first  are pre-critical dimensions with a wavelength less than the maximum training length. We generated 10,000 training sequences, and 1,000 each for validation and testing, and ensured that the first  tokens in each sequence do not overlap to testify whether the model learns the correct generation mechanism. We averaged results over  seeds. A more detailed setting is provided in Appendix C.1  ###reference_###."
        },
        {
            "section_id": "6.1.2",
            "parent_section_id": "6.1",
            "section_name": "6.1.2 Results and Analysis",
            "text": "Table 1  ###reference_### displays the comparison of the OOD accuracy. In most cases, Resonance RoPE and Resonance YaRN outperform their counterparts lacking the Resonance technique, showcasing significantly better performance and reduced variance in OOD scenarios. This improvement indicates a superior adaptation to OOD position embeddings through minimized Positional Encoding (PE) interpolation. An exception is observed when applying Resonance RoPE to the Recursive subtask, likely due to the dominance of extrapolated post-critical dimensions in OOD positions. This issue can be mitigated by employing a RoPE scaling technique such as YaRN, which effectively counters the extrapolation of post-critical dimensions. Among all configurations, Resonance YaRN exhibits the highest OOD performance, demonstrating the synergy between RoPE scaling methods and the Resonance technique.\nFigure 3  ###reference_### plots validation losses against training epochs for different PEs, illustrating the training dynamics. The introduction of the Resonance technique leads to a reduction in the lowest validation loss for both RoPE and YaRN, with Resonance RoPE achieving even lower validation losses than YaRN in the Semi-Recursive subtask. Furthermore, the validation loss trajectories for Resonance RoPE and Resonance YaRN remain lower than those of their counterparts in all subtasks, further demonstrating the enhanced OOD generalization capability of our approach."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "LLM Fine-tuning Evaluation",
            "text": ""
        },
        {
            "section_id": "6.2.1",
            "parent_section_id": "6.2",
            "section_name": "6.2.1 Experiment Setup",
            "text": "In this section, we apply our proposed Resonance RoPE to the current state-of-the-art RoPE scaling method, YaRN (Peng et al., 2024  ###reference_b16###).\nMore specifically, we replace the original position embeddings of LLaMA2 7B and 13B (Touvron et al., 2023b  ###reference_b27###) with a series of scaled position embeddings, including the NTK-Aware scaling (bloc97, 2023  ###reference_b4###; Xiong et al., 2023  ###reference_b31###; Liu et al., 2024  ###reference_b12###), Dynamic NTK-Aware Scaling (Peng et al., 2024  ###reference_b16###; Rozi\u00e8re et al., 2023  ###reference_b21###), and YaRN (Peng et al., 2024  ###reference_b16###).\nFor YaRN and Resonance YaRN, We use a scaling factor of  and  for LLaMA2 7B and 13B to extend their context window from K to K and K, respectively.\nFor the configurations that require fine-tuning, we fine-tune the LLM with the scaled position embedding on the training set of PG19 (Rae et al., 2020  ###reference_b18###) with the fine-tuning setting and hyperparameters adopted directly from YaRN (Peng et al., 2024  ###reference_b16###), with the only difference being that we control the total training token count to be approximately M. A more detailed fine-tuning setting can be found in Appendix C.2  ###reference_###. We test the model\u2019s performance on two TSTL scenarios: language modeling evaluation on long-text sequences and long-text downstream application performance."
        },
        {
            "section_id": "6.2.2",
            "parent_section_id": "6.2",
            "section_name": "6.2.2 Perplexity on Long Sequence",
            "text": "###figure_4### We evaluate the model\u2019s language modeling performance on GovReport (Huang et al., 2021  ###reference_b7###) and Proofpile (Azerbayev, 2022  ###reference_b2###). We randomly select  samples from each dataset and report the final perplexity in text fragments of gradually increased length. We report the results in Figure 4  ###reference_###. Of the tested methods, Resonance YaRN achieves the lowest perplexity across all context lengths. Especially, Resonance YaRN achieves a lower perplexity compared to YaRN with the same set of hyperparameters optimized for YaRN, demonstrating the benefit of applying the Resonance technique to existing RoPE scaling methods."
        },
        {
            "section_id": "6.2.3",
            "parent_section_id": "6.2",
            "section_name": "6.2.3 Real-world Task Evaluation",
            "text": "Lastly, we test the real-world task performance of LLaMA2-Chat 7B and 13B\u2019s performance with different RoPE scaling strategies on L-Eval An et al. (2023  ###reference_b1###)\u2019s close ended task suite, a long-text LLM benchmark covering a wide range of domains such as school lectures, long conversations and novels. We fine-tune the model with different RoPE scaling strategies using two different strategies: training on shorter sequences (4K length) for more epochs, and training on longer sequences (32K or 16K length) for less epochs. All settings requiring fine-tuning keep the training token count to be approximately 100M. The results are listed in Table 2  ###reference_###.\nAlthough no single setting in the experiment achieves the best result on all subtasks, we observe that applying Resonance YaRN achieves better average performance in different training settings and model sizes compared to its counterpart YaRN setting. This further proves the compatibility of the Resonance technique and RoPE scaling methods, and the better length extrapolation performance brought by our proposed method."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce Resonance RoPE, a novel enhancement of RoPE that focuses on minimizing the interpolation of RoPE features for OOD positions, thereby reducing the generalization gap and improving LLM\u2019s performance on train-short-test-long (TSTL) scenarios.\nAdditionally, we present a novel synthetic benchmark, PosGen, which provides a fine-grained analysis of the model\u2019s TSTL performance regarding various token dependency patterns.\nExtensive experiments on our proposed PosGen and two LLM-based evaluations demonstrate Resonance RoPE\u2019s efficacy in identifying OOD positions and its compatibility with current RoPE scaling strategies.\nFuture work includes exploring Resonance RoPE\u2019s performance on other foundational models, and the identification of more optimal wavelength combinations for RoPE features."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Proof of Theorem\u00a01",
            "text": "All we need is to prove that for each , each  and each \nwe can find\n , such that .\nBy definition, it is equivalent to solving the equations:\nfor , given , , and .\nThe RoPE feature matrix  is defined as block-diagonal with  blocks given by Equation 3  ###reference_###. Hence, given ,  and , the equation reduces to equality of a linear combination of trigonometric functions:\nfor , depending on  and . This equality clearly holds if  is a multiple of :\nfor some .\nBy our construction,  is a natural number. Hence, to finish the proof that we can solve our initial equation for , we need to show that we can find integer  to satisfy:\nfor .\nThis is where we use the pre-critical dimension condition: for , by definition of , we have the inequality . Taking  will give us the required range for  and hence finish the proof.\n\u220e"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Comparison Between Feature Gap and Embedded Vector Distance",
            "text": "Our proposed feature gap metric, as defined in Equation 8  ###reference_###, shares similarities with the \u201cembedded vector distance\u201d metric introduced by Xiong et al. (2023  ###reference_b31###):\nwhere  represents the set of vectors requiring positional embedding. This equation assesses the discrepancy in Rotary Position Embedding (RoPE) before and after a scaling operation. The distance calculation specifically compares the original RoPE, , to the scaled RoPE, , with token positions beginning at zero. It aims to quantify the alterations in position embedding due to the scaling process.\nIn contrast, our feature gap metric is tailored for a more practical and common scenario, where models are trained or fine-tuned on short sequences using the already scaled RoPE embeddings. This setting emphasizes the generalization gap of the RoPE features between training and testing position ranges. The core hypothesis is that a smaller discrepancy in the RoPE features of new token positions relative to those encountered during training correlates with enhanced model generalization to novel token positions. Our metric diverges from the \u201cembedded vector distance\u201d in two significant aspects to better align with our use-case:\nThe distance computation shifts to compare scaled RoPE across different token positions, reflecting the operational context where training involves short sequences (train-short) and testing involves longer sequences (test-long).\nWe modify the token position ranges,  and , to represent token positions observed during training (in-distribution) and testing (out-of-distribution), respectively, to directly measure the generalization gap.\nThis adaptation of the metric allows for a more targeted evaluation of the model\u2019s ability to generalize across different token positional distributions, which is critical in scenarios where sequence length varies significantly between training and deployment."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Detailed Experiment Settings",
            "text": "In this section, we provide the detailed experiment settings for both our synthetic task evaluation on PosGen and LLM-based evaluations on both upstream language modeling evaluation and downstream real-world application evaluations."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T1.12\" style=\"width:433.6pt;height:148.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(85.1pt,-29.1pt) scale(1.64561525034439,1.64561525034439) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T1.12.12\">\n<tr class=\"ltx_tr\" id=\"S6.T1.12.12.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T1.12.12.13.1\">Setting</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T1.12.12.13.2\">Recursive</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T1.12.12.13.3\">CoT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T1.12.12.13.4\">Semi-Rec.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T1.3.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T1.3.3.3.4\">RoPE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T1.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T1.2.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T1.3.3.3.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T1.6.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T1.6.6.6.4\">Res. RoPE (Ours)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T1.4.4.4.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T1.5.5.5.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T1.6.6.6.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T1.9.9.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T1.9.9.9.4\">YaRN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T1.7.7.7.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T1.8.8.8.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T1.9.9.9.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T1.12.12.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S6.T1.12.12.12.4\">Res. YaRN (Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T1.10.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T1.11.11.11.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T1.12.12.12.3\"></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>The accuracy on OOD Positions (OOD Acc.) on\u00a0<span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.T1.16.1\">PosGen</span>\u2019s test set. All results are in percentage (%). We report both the mean and variance across five runs with different random seeds. We compare the same RoPE-based PE with or without our <span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.T1.17.2\">Resonance</span> scaling. The best performance for each pair of settings on each subtask is marked in <span class=\"ltx_text ltx_font_bold\" id=\"S6.T1.18.3\">Bold</span>.</figcaption>\n</figure>",
            "capture": "Table 1: The accuracy on OOD Positions (OOD Acc.) on\u00a0PosGen\u2019s test set. All results are in percentage (%). We report both the mean and variance across five runs with different random seeds. We compare the same RoPE-based PE with or without our Resonance scaling. The best performance for each pair of settings on each subtask is marked in Bold."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T2.26\" style=\"width:433.6pt;height:199.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-76.0pt,35.0pt) scale(0.74053526128386,0.74053526128386) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T2.26.26\">\n<tr class=\"ltx_tr\" id=\"S6.T2.26.26.27\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.27.1\">Setting</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.26.26.27.2\">Ctx Len.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.27.3\">Coursera</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.27.4\">GSM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.27.5\">QuALITY</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.27.6\">TOEFL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.27.7\">CodeU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.26.26.27.8\">SFiction</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.27.9\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.26.26.28\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\" id=\"S6.T2.26.26.28.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.26.26.28.1.1\">LLaMA2-Chat 7B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.26.26.29\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S6.T2.26.26.29.1\">Dynamic NTK-Aware (no FT)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.26.26.29.2\">32K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.29.3\">31.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.29.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.26.26.29.4.1\">32.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.29.5\">34.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.29.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.26.26.29.6.1\">59.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.29.7\">1.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.26.26.29.8\">36.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.29.9\">32.59</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T2.1.1.1.1\">NTK-Aware (, no FT)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.1.1.1.2\">32K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.3.1\">36.77</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.1.1.4\">3.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.1.1.5\">26.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.1.1.6\">34.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.1.1.7\">1.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.1.1.1.8\">50.78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.1.1.9\">25.43</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.4.4.4\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S6.T2.4.4.4.3\">YaRN (, FT@K,\n epcs.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.4.4.4.4\">32K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.4.4.4.5\">36.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.4.4.4.6\">19.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.4.4.4.7\">33.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.4.4.4.8\">50.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.4.4.4.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.4.4.4.9.1\">4.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.4.4.4.10\">56.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.4.4.4.11\">33.24</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.7.7.7\">\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T2.7.7.7.3\">Resonance YaRN (, FT@K,  epcs.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.7.7.7.4\">32K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.7.7.7.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.7.7.7.5.1\">36.48</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.7.7.7.6\">22.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.7.7.7.7\">34.16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.7.7.7.8\">55.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.7.7.7.9\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.7.7.7.10\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.7.7.7.10.1\">57.03</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.7.7.7.11\">34.24</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.10.10.10\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S6.T2.10.10.10.3\">YaRN (, FT@K,  epcs.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.10.10.10.4\">32K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.10.10.10.5\">35.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.10.10.10.6\">24.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.10.10.10.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.10.10.10.7.1\">37.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.10.10.10.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.10.10.10.8.1\">57.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.10.10.10.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.10.10.10.9.1\">4.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.10.10.10.10\">60.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.10.10.10.11\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.10.10.10.11.1\">36.61</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.13.13.13\">\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T2.13.13.13.3\">Resonance YaRN (, FT@K,  epcs.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.13.13.13.4\">32K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.13.13.13.5\">36.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.13.13.13.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.13.13.13.6.1\">27.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.13.13.13.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.13.13.13.7.1\">40.59</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.13.13.13.8\">56.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.13.13.13.9\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.13.13.13.9.1\">3.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.13.13.13.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.13.13.13.10.1\">61.72</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.13.13.13.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.13.13.13.11.1\">37.58</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.26.26.30\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\" id=\"S6.T2.26.26.30.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.26.26.30.1.1\">LLaMA2-Chat 13B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.26.26.31\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S6.T2.26.26.31.1\">Dynamic NTK-Aware (no FT)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.26.26.31.2\">16K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.31.3\">29.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.31.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.26.26.31.4.1\">39.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.31.5\">40.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.31.6\">63.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.31.7\">1.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.26.26.31.8\">39.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.26.26.31.9\">35.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.14.14.14\">\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T2.14.14.14.1\">NTK-Aware (, no FT)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.14.14.14.2\">16K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.14.14.14.3\">40.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.14.14.14.4\">21.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.14.14.14.5\">38.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.14.14.14.6\">65.43</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.14.14.14.7\">1.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.14.14.14.8\">46.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.14.14.14.9\">35.47</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.17.17.17\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S6.T2.17.17.17.3\">YaRN (, FT@K,  epcs.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.17.17.17.4\">16K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.17.17.17.5\">38.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.17.17.17.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.17.17.17.6.1\">39.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.17.17.17.7\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.17.17.17.7.1\">43.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.17.17.17.8\">65.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.17.17.17.9\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.17.17.17.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.17.17.17.10.1\">63.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.17.17.17.11\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.17.17.17.11.1\">41.48</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.20.20.20\">\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T2.20.20.20.3\">Resonance YaRN (, FT@K,  epcs.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.20.20.20.4\">16K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.20.20.20.5\">38.66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.20.20.20.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.20.20.20.6.1\">39.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.20.20.20.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.20.20.20.7.1\">43.56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.20.20.20.8\">65.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.20.20.20.9\">1.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.20.20.20.10\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.20.20.20.10.1\">62.50</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.20.20.20.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.20.20.20.11.1\">41.65</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.23.23.23\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S6.T2.23.23.23.3\">YaRN (, FT@K,  epcs.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.23.23.23.4\">16K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.23.23.23.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.23.23.23.5.1\">41.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.23.23.23.6\">34.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.23.23.23.7\">41.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.23.23.23.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.23.23.23.8.1\">66.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.23.23.23.9\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.23.23.23.9.1\">2.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.23.23.23.10\">48.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.23.23.23.11\">39.06</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.26.26.26\">\n<td class=\"ltx_td ltx_align_right ltx_border_b\" id=\"S6.T2.26.26.26.3\">Resonance YaRN (, FT@K,  epcs.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S6.T2.26.26.26.4\">16K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T2.26.26.26.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.26.26.26.5.1\">41.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T2.26.26.26.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.26.26.26.6.1\">35.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T2.26.26.26.7\">42.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T2.26.26.26.8\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.26.26.26.8.1\">65.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T2.26.26.26.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.26.26.26.9.1\">5.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S6.T2.26.26.26.10\">48.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T2.26.26.26.11\">39.87</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Long text evaluations on some closed-ended tasks in L-Eval. \u201cCtx Len\u201d means the target context length of the model after scaling its PE. \u201cFT@K,  epcs\u201d means the model is fine-tuned on K sequence length for  epochs. The settings with \u201cno FT\u201d are not fine-tuned after modifying its position embedding. We highlight the best and second-best performance for each base model in <span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.37.1\">Bold</span> and <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S6.T2.38.2\">Underline</span>, respectively.</figcaption>\n</figure>",
            "capture": "Table 2: Long text evaluations on some closed-ended tasks in L-Eval. \u201cCtx Len\u201d means the target context length of the model after scaling its PE. \u201cFT@K,  epcs\u201d means the model is fine-tuned on K sequence length for  epochs. The settings with \u201cno FT\u201d are not fine-tuned after modifying its position embedding. We highlight the best and second-best performance for each base model in Bold and Underline, respectively."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00071v2_figure_1.png",
            "caption": "Figure 1: An illustration of RoPE\u2019s rotation angles m\u2062\u03b86\ud835\udc5asubscript\ud835\udf036m\\theta_{6}italic_m italic_\u03b8 start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT and Resonance RoPE\u2019s rotation angles m\u2062\u03b8~6\ud835\udc5asubscript~\ud835\udf036m\\tilde{\\theta}_{6}italic_m over~ start_ARG italic_\u03b8 end_ARG start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT in Eqn. 3 in a TSTL scenario with training max length 64646464 and testing max length 128128128128. RoPE\u2019s non-integer feature wavelengths create a feature gap between the RoPE features of the training and OOD testing positions, while Resonance RoPE reduces this gap to 0."
        },
        "2": {
            "figure_path": "2403.00071v2_figure_2.png",
            "caption": "Figure 2: An example of the three subtasks of PosGen. This figure shows the process of generating the 12121212th token shown in the red boxes for each subtask. In this example, h\u210ehitalic_h is a modular addition task with the modulus m=7\ud835\udc5a7m=7italic_m = 7 and the difficulty-controlling parameters j=1,k=3formulae-sequence\ud835\udc571\ud835\udc583j=1,k=3italic_j = 1 , italic_k = 3. The output token depends on: (1) only the local j+k\ud835\udc57\ud835\udc58j+kitalic_j + italic_k tokens in the recursive task; (2) k\ud835\udc58kitalic_k local tokens and the beginning j\ud835\udc57jitalic_j tokens in the CoT task; and (3) k\ud835\udc58kitalic_k local tokens and j\ud835\udc57jitalic_j tokens with a varied dependency distance in the semi-recursive task."
        },
        "3": {
            "figure_path": "2403.00071v2_figure_3.png",
            "caption": "Figure 3: The validation loss curves of Transformers using RoPE and YaRN PEs with and without our Resonance scaling on the three subtasks of PosGen."
        },
        "4": {
            "figure_path": "2403.00071v2_figure_4.png",
            "caption": "Figure 4: The perplexity of LLaMA-Chat 7B with different position embeddings on GovReport and Proofpile."
        }
    },
    "references": [
        {
            "1": {
                "title": "L-eval: Instituting standardized evaluation for long context language models.",
                "author": "Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.",
                "venue": "CoRR, abs/2307.11088.",
                "url": "https://doi.org/10.48550/ARXIV.2307.11088"
            }
        },
        {
            "2": {
                "title": "zhangir-azerbayev/proof-pile.",
                "author": "Zhangir Azerbayev. 2022.",
                "venue": null,
                "url": "https://github.com/zhangir-azerbayev/proof-pile"
            }
        },
        {
            "3": {
                "title": "Longbench: A bilingual, multitask benchmark for long context understanding.",
                "author": "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.",
                "venue": "CoRR, abs/2308.14508.",
                "url": "https://doi.org/10.48550/ARXIV.2308.14508"
            }
        },
        {
            "4": {
                "title": "NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.",
                "author": "bloc97. 2023.",
                "venue": null,
                "url": null
            }
        },
        {
            "5": {
                "title": "Extending context window of large language models via positional interpolation.",
                "author": "Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023.",
                "venue": "CoRR, abs/2306.15595.",
                "url": "https://doi.org/10.48550/ARXIV.2306.15595"
            }
        },
        {
            "6": {
                "title": "Flashattention-2: Faster attention with better parallelism and work partitioning.",
                "author": "Tri Dao. 2023.",
                "venue": "CoRR, abs/2307.08691.",
                "url": "https://doi.org/10.48550/ARXIV.2307.08691"
            }
        },
        {
            "7": {
                "title": "Efficient attentions for long document summarization.",
                "author": "Luyang Huang, Shuyang Cao, Nikolaus Nova Parulian, Heng Ji, and Lu Wang. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/V1/2021.NAACL-MAIN.112"
            }
        },
        {
            "8": {
                "title": "Mistral 7b.",
                "author": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023.",
                "venue": "CoRR, abs/2310.06825.",
                "url": "https://doi.org/10.48550/ARXIV.2310.06825"
            }
        },
        {
            "9": {
                "title": "Mixtral of experts.",
                "author": "Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2024.",
                "venue": "CoRR, abs/2401.04088.",
                "url": "https://doi.org/10.48550/ARXIV.2401.04088"
            }
        },
        {
            "10": {
                "title": "The impact of positional encoding on length generalization in transformers.",
                "author": "Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023.",
                "venue": "In Advances in Neural Information Processing Systems, volume 36, pages 24892\u201324928.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/4e85362c02172c0c6567ce593122d31c-Paper-Conference.pdf"
            }
        },
        {
            "11": {
                "title": "Transformers learn shortcuts to automata.",
                "author": "Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023.",
                "venue": "In The Eleventh International Conference on Learning Representations.",
                "url": "https://openreview.net/pdf?id=De4FYqjFueZ"
            }
        },
        {
            "12": {
                "title": "Scaling laws of roPE-based extrapolation.",
                "author": "Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. 2024.",
                "venue": "In The Twelfth International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=JO7k0SJ5V6"
            }
        },
        {
            "13": {
                "title": "Decoupled weight decay regularization.",
                "author": "Ilya Loshchilov and Frank Hutter. 2019.",
                "venue": "In 7th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=Bkg6RiCqY7"
            }
        },
        {
            "14": {
                "title": "Progress measures for grokking via mechanistic interpretability.",
                "author": "Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. 2023.",
                "venue": "In The Eleventh International Conference on Learning Representations.",
                "url": "https://openreview.net/pdf?id=9XFSbDPmdW"
            }
        },
        {
            "15": {
                "title": "GPT-4 technical report.",
                "author": "OpenAI. 2023.",
                "venue": "CoRR, abs/2303.08774.",
                "url": "https://doi.org/10.48550/ARXIV.2303.08774"
            }
        },
        {
            "16": {
                "title": "YaRN: Efficient context window extension of large language models.",
                "author": "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2024.",
                "venue": "In The Twelfth International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=wHBfxhZu1u"
            }
        },
        {
            "17": {
                "title": "Train short, test long: Attention with linear biases enables input length extrapolation.",
                "author": "Ofir Press, Noah A. Smith, and Mike Lewis. 2022.",
                "venue": "In The Tenth International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=R8sQPpGCv0"
            }
        },
        {
            "18": {
                "title": "Compressive transformers for long-range sequence modelling.",
                "author": "Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020.",
                "venue": "In 8th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=SylKikSYDH"
            }
        },
        {
            "19": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.",
                "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
                "url": "http://jmlr.org/papers/v21/20-074.html"
            }
        },
        {
            "20": {
                "title": "Zero-offload: Democratizing billion-scale model training.",
                "author": "Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021.",
                "venue": "In 2021 USENIX Annual Technical Conference, pages 551\u2013564. USENIX Association.",
                "url": "https://www.usenix.org/conference/atc21/presentation/ren-jie"
            }
        },
        {
            "21": {
                "title": "Code llama: Open foundation models for code.",
                "author": "Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023.",
                "venue": "CoRR, abs/2308.12950.",
                "url": "https://doi.org/10.48550/ARXIV.2308.12950"
            }
        },
        {
            "22": {
                "title": "Randomized positional encodings boost length generalization of transformers.",
                "author": "Anian Ruoss, Gr\u00e9goire Del\u00e9tang, Tim Genewein, Jordi Grau-Moya, R\u00f3bert Csord\u00e1s, Mehdi Bennani, Shane Legg, and Joel Veness. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1889\u20131903, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-short.161"
            }
        },
        {
            "23": {
                "title": "ZeroSCROLLS: A zero-shot benchmark for long text understanding.",
                "author": "Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7977\u20137989, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-emnlp.536"
            }
        },
        {
            "24": {
                "title": "Roformer: Enhanced transformer with rotary position embedding.",
                "author": "Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.",
                "venue": "Neurocomputing, 568:127063.",
                "url": "https://doi.org/10.1016/J.NEUCOM.2023.127063"
            }
        },
        {
            "25": {
                "title": "Long range arena : A benchmark for efficient transformers.",
                "author": "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021.",
                "venue": "In 9th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=qVyeW-grC2k"
            }
        },
        {
            "26": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.",
                "venue": "CoRR, abs/2302.13971.",
                "url": "https://doi.org/10.48550/ARXIV.2302.13971"
            }
        },
        {
            "27": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur\u00e9lien Rodriguez, Robert Stojnic, Sergey Edunov,\nand Thomas Scialom. 2023b.",
                "venue": "CoRR, abs/2307.09288.",
                "url": "https://doi.org/10.48550/ARXIV.2307.09288"
            }
        },
        {
            "28": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, pages 5998\u20136008.",
                "url": "https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html"
            }
        },
        {
            "29": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022.",
                "venue": "In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022.",
                "url": "http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html"
            }
        },
        {
            "30": {
                "title": "Memorizing transformers.",
                "author": "Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. 2022.",
                "venue": "In The Tenth International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=TrjbxzRcnf-"
            }
        },
        {
            "31": {
                "title": "Effective long-context scaling of foundation models.",
                "author": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. 2023.",
                "venue": "CoRR, abs/2309.16039.",
                "url": "https://doi.org/10.48550/ARXIV.2309.16039"
            }
        },
        {
            "32": {
                "title": "Length extrapolation of transformers: A survey from the perspective of position encoding.",
                "author": "Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bing Qin, and Ting Liu. 2023.",
                "venue": "CoRR, abs/2312.17044.",
                "url": "https://doi.org/10.48550/ARXIV.2312.17044"
            }
        },
        {
            "33": {
                "title": "Least-to-most prompting enables complex reasoning in large language models.",
                "author": "Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023.",
                "venue": "In The Eleventh International Conference on Learning Representations.",
                "url": "https://openreview.net/pdf?id=WZH7099tgfM"
            }
        },
        {
            "34": {
                "title": "PoSE: Efficient context window extension of LLMs via positional skip-wise training.",
                "author": "Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2024.",
                "venue": "In The Twelfth International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=3Z1gxuAQrA"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00071v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "6",
            "6.1",
            "6.1.1",
            "6.1.2",
            "6.2",
            "6.2.1",
            "6.2.2",
            "6.2.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.1.2",
            "6.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.00071v2",
        "paper_title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
        "research_background": "The paper, \"Resonance RoPE: Improving Context Length Generalization of Large Language Models,\" aims to address a significant challenge in Large Language Models (LLMs) regarding their ability to handle long texts, particularly in scenarios known as train-short-test-long (TSTL).\n\n### Motivation:\n\nThe motivation for this research stems from the demonstrated potential of LLMs in a myriad of natural language processing tasks. Despite these advances, these models often underperform in real-world applications that involve long sequences, particularly when these sequences extend beyond the lengths encountered during training. The TSTL scenario highlights this problem, where models pre-trained on shorter sequences struggle with out-of-distribution (OOD) token positions in longer sequences, which affects their performance.\n\n### Research Problem:\n\nThe primary research problem tackled in this paper is the performance degradation of LLMs in TSTL scenarios. Current methods focus on scaling factors to adjust RoPE values for OOD positions but still fall short in fully addressing the issue.\n\nThe paper introduces \"Resonance RoPE,\" a novel technique aimed at further diminishing the generalization gap in position embeddings under TSTL conditions.\n\n### Relevant Prior Work:\n\n1. **Challenges in Long Texts:** The train-short-test-long (TSTL) scenario, elaborated by Press et al. (2022), underscores a significant limitation where models trained on shorter sequences struggle with longer, out-of-distribution token positions, affecting their performance in practical applications (Zhao et al., 2023).\n\n2. **RoPE and Scaling Methods:** Recent efforts to enhance TSTL performance have involved RoPE-based models such as LLaMA and Mistral. These methods modify the test-time computation of RoPE position embeddings by introducing a scaling factor to the position index of each token or to RoPE\u2019s base value (Chen et al., 2023; Xiong et al., 2023; Liu et al., 2024; Peng et al., 2024). These measures help maintain position embeddings within ranges experienced during pre-training.\n\n3. **Current Benchmarks:** Existing benchmarks for evaluating model performance in TSTL scenarios, such as those measuring perplexity in long contexts (Rae et al., 2020; Huang et al., 2021; Wu et al., 2022), and synthetic TSTL tasks (Liu et al., 2023; Kazemnejad et al., 2023), face challenges in isolating the problem of recognizing OOD positions from the difficulty of generating more complex tokens as contexts lengthen.\n\n### Contributions:\n\n1. Introduced a novel modification of RoPE to enhance TSTL performance by aligning each RoPE feature's wavelength with a specific token span length, thereby minimizing the generalization gaps in position embeddings without additional computational resources during runtime.\n\n2. Developed PosGen, a synthetic benchmark designed specifically to disentangle complexities in token generation from the difficulties in handling new positions or position embedding values, facilitating more precise evaluation of models in TSTL scenarios.\n\n3. Demonstrated through rigorous testing that Resonance RoPE notably improves performance on OOD positions in both upstream and downstream tasks, compared to existing methods without this enhancement.\n\nBy combining these contributions, the authors aim to provide a more robust solution for improving context length generalization in large language models, thus enhancing their applicability and performance in real-world long-text scenarios.",
        "methodology": "In Transformer models (Vaswani et al., 2017), the self-attention mechanism is a core component that calculates attention scores by applying a softmax function to scaled attention logits.\n\nTo incorporate positional information into the model, RoPE (Rotary Positional Encoding) is employed. Specifically, RoPE integrates the position information of each token into the query and key vectors. This is achieved through computations in the complex plane. The transformation equations involve trainable parameters and a constant called the rotary base, which can be set to values like 10000 or other integers or fractions. One key aspect of RoPE is that it makes the dot product between the query and key vectors depend solely on their relative distances.\n\nFor practical implementation, RoPE represents the real-number encoding in the following manner:\n1. It splits the dimensional space into several 2-dimensional subspaces.\n2. For each subspace, a real rotation matrix is applied.\n\nThe attention logits are thereafter computed based on the encoded query and key vectors:\n\nEach pair of dimensions of the vectors corresponds to a specific temporal wavelength. This wavelength specifies the token length over which the associated RoPE features undergo a periodic change.\n\nFor example, in models such as LLaMA and LLaMA2, the wavelengths of RoPE features are organized to range from about 10000 for the smallest dimension pairs to significantly larger values (or potentially infinite) for higher dimensions. This structure helps in managing the periodicity of positional encoding effectively over longer contexts, thereby enhancing the model's ability to generalize across varying context lengths.\n\nIn essence, the key innovations of RoPE include:\n- The injection of position information into query and key vectors using complex number transformations.\n- The implementation of block-diagonal matrices with multiple rotation matrices for real-number encoding.\n- The strategic assignment of temporal wavelengths to dimensions to adeptly capture long-range dependencies in text analysis.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n**Datasets:**\n1. PosGen task \u2013 a small-scale evaluation specific to the study.\n2. Language modeling perplexity task \u2013 evaluated on LLaMA2-Chat.\n3. Real-world long context applications \u2013 evaluated on LLaMA2-Chat.\n\n**Baselines:**\n- Standard RoPE (Rotary Position Embedding).\n- LLaMA2-Chat baseline implementations for language modeling and long context applications.\n\n**Evaluation Metrics:**\n1. **Language Modeling Perplexity:** Perplexity score for assessing the effectiveness of Resonance RoPE compared to standard RoPE.\n2. **Performance on Real-World Long Context Applications:** Qualitative and quantitative metrics specific to the applications being assessed.\n\n**Main Experimental Results:**\n- Resonance RoPE demonstrated improved language modeling perplexity compared to the standard RoPE.\n- Resonance RoPE enhanced the performance in real-world long context applications, indicating better context length generalization as compared to the baselines.\n\nThis setup and results indicate that Resonance RoPE provides notable advantages in handling extended context lengths in LLMs, offering improvements both in perplexity metrics and practical application performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of Resonance RoPE in improving out-of-distribution (OOD) token position recognition in TSTL scenarios when compared to traditional RoPE and RoPE enhanced by YaRN scaling.",
            "experiment_process": "The experiments were set up by comparing models with Resonance RoPE and Resonance YaRN against counterparts without the Resonance technique. Specific attention was given to various subtasks such as Recursive and Semi-Recursive tasks. Key metrics evaluated include OOD accuracy, which is displayed in Table 1, and validation loss dynamics, as illustrated in Figure 3.",
            "result_discussion": "Resonance RoPE and Resonance YaRN generally outperformed their non-Resonance counterparts, demonstrating significantly better OOD accuracy and reduced performance variance. Except for the Recursive subtask, where the dominance of extrapolated post-critical dimensions in OOD positions caused a performance drop, mitigated by using YaRN. Resonance YaRN showed the highest OOD performance, suggesting superior synergy with RoPE scaling techniques. The validation losses for models with the Resonance technique were consistently lower, indicating enhanced OOD generalization capability.",
            "ablation_id": "2403.00071v2.No1"
        },
        {
            "research_objective": "To assess the language modeling performance, specifically perplexity, of models using Resonance RoPE and Resonance YaRN on long sequence text fragments.",
            "experiment_process": "The models were evaluated on the GovReport and Proofpile datasets. Random samples from each dataset were selected, and the perplexity was reported across text fragments of increasing lengths. The results are depicted in Figure 4.",
            "result_discussion": "Resonance YaRN consistently achieved the lowest perplexity across all context lengths compared to other tested methods. This performance indicates the benefits of incorporating the Resonance technique into existing RoPE scaling methods, as evidenced by Resonance YaRN performing better than YaRN even with identical hyperparameters optimized for YaRN.",
            "ablation_id": "2403.00071v2.No2"
        }
    ]
}