{
    "title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean",
    "abstract": "Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human evaluation and GPT4. Experimental results showed that our proposed Bllossom model exhibited superior performance in qualitative analyses compared to previously proposed Korean monolingual models.\n\n\n\nKeywords:\u2009Large Language Model, Less-resourced Languages, Instruction-tuning",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "A large language model (LLM) comprehends linguistic information and knowledge via pretraining to predict the subsequent word based on the given context Zhao et al. (2023  ###reference_b38###).\nHowever, the growth of LLMs increases the computing resources required for training, posing a challenge for smaller research groups to utilize them realistically Hoffmann et al. (2022  ###reference_b9###). To meet the demands of this era, numerous big tech companies and research institutes have been competing to launch multilingual LLMs (MLLMs) Touvron et al. (2023a  ###reference_b31###, b  ###reference_b32###); Workshop et al. (2023  ###reference_b37###). However, less-resourced languages (LRLs) are being overlooked Gu et al. (2018  ###reference_b8###).\nThe recently launched Llama2 Touvron et al. (2023b  ###reference_b32###) is an MLLM trained in more than 28 languages\u037e however, only 0.06% of the data was used for the Korean language. This leads to two significant syntactic and semantic challenges. First, during the MLLM training, LRLs use minimal vocabulary based on the scarce training data, which limits their expression owing to the inadequate lexicon. Second, greater semantic knowledge is required to employ LLMs for specific tasks, such as question-answering, thereby rendering the models inapplicable or prone to hallucinations Zheng et al. (2023  ###reference_b39###); Peng et al. (2023a  ###reference_b23###).\nNumerous methods have been proposed to enhance the LRL performance. These include expanding the vocabulary of word embeddings Wang et al. (2019  ###reference_b34###); Schuster et al. (2019  ###reference_b29###), aligning multilingual embeddings by combining them with other languages Artetxe et al. (2017  ###reference_b1###, 2018  ###reference_b2###), and reinforcing the utility of LLMs using minimal training data. The Less Is More for Alignment (LIMA) study proposed a method to maximize the utility of LLMs using 1,030 high-quality instruction dataZhou et al. (2023  ###reference_b40###).\nBased on the existing research, it remains to be investigated whether MLLMs can expand their vocabulary and enhance the semantic inferencing capability of a specific language using minimum additional data.\nThis study explores the aforementioned aspects by proposing a method to enhance the Korean language capabilities of a representative MLLM, i.e., Llama2. The specific language abilities of the MLLM are enhanced using the following strategies:\n(1) Vocabulary expansion: To enhance its fundamental vocabulary, the MLLM was augmented with the dictionary of a specific language.\n(2) Knowledge enrichment: The vocabulary and knowledge information of this language were enhanced in the MLLM via pretraining.\n(3) Usability enhancement: High-quality instruction data were generated in the Korean language to improve its LLM applicability. Korean is not an LRL as it comprises various language resources and evaluation data Park et al. (2021  ###reference_b22###). However, Korean is experimentally suitable as a relatively less-resourced language because the Llama2 model uses limited Korean data and vocabulary during training.\nTo enhance the vocabulary, knowledge reinforcement, and usability, 7,478 Korean vocabulary entries were added and pretraining was performed using a Korean\u2013English corpus. The 1,030 English data, proposed by LIMA  Zhou et al. (2023  ###reference_b40###), were restructured by three Korean linguistics to ensure their practical similarity with the Korean language, thus enhancing their usability.\nThe effectiveness of the three enhancement methods were validated by addressing the following questions: (1) What are the advantages of expanding the Korean vocabulary? (2) Is it effective to connect the knowledge of high- and low-resource languages via pretraining? (3) Do the actual usability and accuracy improve using the proposed Korean LIMA data? We propose the Bllossom model that applies the three aforementioned methods. Quantitatively, this model demonstrated an average performance improvement from 1.8% to 8% across eight tasks compared to the model without vocabulary expansion. For the qualitative evaluation of each model, the answers to 300 queries were compared using human and preference evaluations based on GPT4. Consequently, the Bllossom model outperformed the other Korean models of the same size by 93%. The contributions of this study are as follows:\nA method for enhancing the vocabulary, knowledge, and usability of LRLs using MLLMs was proposed.\nA method for constructing instruction data based on language-specific features was presented and demonstrated by constructing a Korean LIMA dataset.\nFor easy utilization, the data, models, and services used to construct and evaluate the Korean LLM were made publicly accessible111github.com/Anonymous-introduce-paper/LR  ###reference_er/LR###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Open-source Korean LLMs",
            "text": "EleutherAI developed Polyglot-Ko, which is a monolingual Korean LLM pretrained on 1.2 TB of Korean data and contains models with sizes up to 12.8B Ko et al. (2023  ###reference_b13###). KoAlpaca222https://github.com/Beomi/KoAlpaca  ###reference_github.com/Beomi/KoAlpaca### is a model based on Polyglot-Ko that automatically translates the SFT data of the English Alpaca Taori et al. (2023  ###reference_b30###) into Korean and performs SFT with a total of 21K data. Similarly, Kullm Lab and research (2023  ###reference_b15###) was proposed based on Polyglot-Ko and tuned using additional instruction data. Kullm used 153K SFT training data by translating English SFT datasets, including the GPT-4-LLM Peng et al. (2023b  ###reference_b24###), Vicuna Chiang et al. (2023  ###reference_b4###), and Dolly from Databricks Conover et al. (2023  ###reference_b5###). Additionally, models that perform the Korean SFT based on multilingual models, such as Llama2, have been launched. Komt is an instruction-tuned model based on Llama2 using a total of 1,543K data processed from existing Korean natural language processing data. Ko-Platypus2 Lee et al. (2023a  ###reference_b16###) enhances the logic knowledge of LLMs using a translated dataset from English Open-Platypus into Korean. This model is tuned using Llama2 with 25K SFT data. The aforementioned Korean models were used in the experiment, and the access links and summary information for each model are listed in Table 4  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Enriching the MLLM vocabulary",
            "text": "This section introduces the following two approaches to the three language enhancement methods proposed in the Introduction: (1) vocabulary expansion and (2) knowledge enrichment. We propose a method to expand the Korean vocabulary in Llama2, which is a representative multilingual LLM, and reinforce the knowledge information between the Korean and English languages via CLM-based pretraining.\n###table_1### \u2018_\u2019, \u2018<0xED>\u2019, \u2018<0x96>\u2019, \u2018<0x84>\u2019, \u2018<0xEB>\u2019, \u2018<0xB2>\u2019, \u2018<0x84>\u2019, \u2018<0xEA>\u2019, \u2018<0xB1>\u2019, \u2018<0xB0>\u2019, \u2018\ub97c\u2019, \u2018_\u2019, \u2018<0xEB>\u2019, \u2018<0xA8>\u2019, \u2018<0xB9>\u2019, \u2018\ub294\u2019, \u2018_\u2019, \u2018\uacf5\u2019, \u2018<0xEB>\u2019, \u2018<0xA3>\u2019, \u2018<0xA1>\u2019\n\u2018\ud584\u2019, \u2018\ubc84\u2019, \u2018\uac70\u2019, \u2018\ub97c\u2019, \u2018_\uba39\ub294\u2019, \u2018_\u2019, \u2018\uacf5\u2019, \u2018\ub8e1\u2019"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Vocabulary expansion",
            "text": "The training data of Llama2 consisted of 89.7% English words, and the tokenizer dictionary () was composed of 90% English (or Latin) words. The majority of the remaining words were rare words, neologisms, and LRLs categorized as out-of-vocabulary (OOV). To address this issue, Llama2 employed the SentencePiece tokenizer Kudo and Richardson (2018  ###reference_b14###) that uses a UTF-8 byte fallback mechanism to handle the OOV words by decomposing them to the UTF-8 byte level. Therefore, the words not in  are represented without expanding the tokenizer vocabulary.\nThe Korean vocabulary was expanded despite having a method to represent the language. Table 1  ###reference_### compares the tokenizing results of Llama2 and the proposed model with an expanded Korean vocabulary for the sentence \u201c\ud584\ubc84\uac70\ub97c \uba39\ub294 \uacf5\ub8e1\u201d. In the original Llama2, the Korean word \u201c\ud584\u201d was decomposed into the tokens \u201c<0xED>\u201d, \u201c<0x96>\u201d, and \u201c<0x84>\u201d, and \u201c\ubc84\u201d was decomposed into \u201c<0xEB>\u201d, \u201c<0xB2>\u201d, and \u201c<0\u00d784>\u201d at the byte level. Contrastingly, the tokenizer with an expanded vocabulary tokenized \u201c\ud584\u201d and \u201c\ubc84\u201d in their original forms. The tokenizing results of the existing Llama2 model can lead to the following two problems, as indicated in the Chinese ALPACA Cui et al. (2023  ###reference_b6###):\nIncreased token length: The model cannot represent an OOV using a single token that requires three or four byte tokens. This reduces the possible input length of the model and increases the encoding and decoding times.\nDuplication of byte tokens: \u201c\ud584\u201d and \u201c\ubc84\u201d are unrelated tokens; however, they are represented using the same byte token \u201c<0x84>\u201d. Therefore, the model may experience confusion while learning two semantically unrelated words with partially identical representations.\nThese limitations were overcome by introducing the Korean vocabulary, as shown in Equation 1  ###reference_###. A new embedding was generated by combining the existing Llama2 vocabulary  and KoBERT 333https://github.com/SKTBrain/KoBERT  ###reference_### vocabulary .\nThe KoBERT vocabulary, designed by considering Korean morphemes, consists of  words, whereas . The union of the two vocabularies has a size of . Therefore, the size of the newly added dictionary was .\nHere,  used the word embeddings trained on the original Llama2, and the newly added word embeddings  were randomly initialized."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Enriching the knowledge information by MLLM pretraining",
            "text": "This section introduces methods to reinforce the word and knowledge information of MLLM via CLM-based pretraining. For queries in the Korean language, the publicly released Llama2 13b model responds in English or alternates between English and Korean (code-switching), indicating a limited Korean expression. However, the content is often accurate when Llama2 responds to a Korean query in English. When asked \u201c\uc774\ud0c8\ub9ac\uc544 \uc218\ub3c4\uc5d0 \ub300\ud574 \ud55c\uad6d\uc5b4\ub85c \uc18c\uac1c\ud574\uc918\u201d (\u201cIntroduce me to the Italian capital in Korean\u201d), the model replies, \u201c\ub85c\ub9c8 is the capital city of Italy and\u2026\u201d where the proper nouns \u201c\ub85c\ub9c8\u201d (Rome) and \u201c\ucf5c\ub85c\uc138\uc6c0\u201d (Colosseum) are generated in Korean but the detailed explanations are provided in English. This is because the knowledge acquired through pretraining was predominantly in English.\nThis limitation can be overcome by aligning the knowledge of the Korean and English languages in the MLLM by further pretraining it on a small amount of data. The MLLM expanded with Korean vocabulary was trained on the English and Korean Wikipedia, thereby bridging the extensive English knowledge (accounting for 89.7%) and limited Korean knowledge (0.06%). This method aligns with that of the multilingual BERT approach, which was pretrained on the Wikipedia data from 104 languages Pires et al. (2019  ###reference_b25###).\nEquation 2  ###reference_### shows that the proposed model was pretrained using CLM. Given an input token sequence  the model predicts the next token , computes the loss by taking the negative log-likelihood of the predicted token probability, and minimizes this loss.\nHere,  represents the loss function of the language model over the pretraining dataset ,  signifies the model parameters,  is the target token for prediction, and  refers to the dictionary expanded using the Korean vocabulary. Table 3  ###reference_### lists the specific compositions and sources of . The loss function accounts for the prediction accuracy of each token within the pretraining dataset. The Korean and English bilingual corpora were adopted in the training method and the model was trained on 33 GB of pretraining data for one epoch with a batch size of 8.\n###table_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Instruction Tuning on LIMA",
            "text": "The Korean language capability was enhanced by pretraining and the existing knowledge between the English and Korean languages was bridged. However, models trained during pretraining have limited applicability because they are specialized for predicting only the subsequent token. Consequently, high-quality Korean SFT data are required to accurately understand the user intent and generate desired responses. This section describes the method for reconstructing Korean SFT data based on English LIMA and introduces an instruction-tuning approach using these data."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Building the Korean LIMA",
            "text": "The Korean LIMA dataset for SFT was constructed based on a version that underwent machine translation using the English LIMA dataset. Consequently, post-processing was required to address the following issues: (1) discrepancies within the authentic Korean linguistic styles owing to machine translation and (2) exclusion of the Korean cultural context stemming from the characteristics of raw sources in the English LIMA dataset. The Korean LIMA dataset used in this study underwent a human review of the initial machine-translated text and modifications to reflect the Korean cultural context, which involved replacing the named entities and changing the main topic. For the human review process, we recruited the reviewers with Korean as their native language, ensuring that they calibrated all the translated data to the most natural Korean linguistic style.\n\nThe raw sources for the English LIMA dataset were posts from the English-speaking community forums, such as Stack Exchange and WikiHow, which reflected the cultural context of English speakers. The cultural context refers to a broad spectrum encompassing everything from daily consciousness to political, economic, and social systems. For instance, a sample from the English LIMA dataset, \u201cHow to make banana muffins?\u201d may be irrelevant to the Korean culture because \u201cbanana muffins\u201d are neither a popular consumable nor a frequently baked item in Korea. To reflect the Korean cultural context in the dataset, we modified the instances in the English LIMA data that featured Western cultural contexts, particularly the American contexts. These modifications ranged from narrow changes, such as renaming the entities, to broader adjustments, such as entirely altering the dataset topic to fit the Korean context (see examples in Table 2  ###reference_###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   MLLM Training using the Korean LIMA",
            "text": "The Stanford Alpaca Taori et al. (2023  ###reference_b30###) is an instruction tuned model based on the Llama trained on 52k instruction data. The corresponding training code is open-source444github/tatsu-lab/stanford_alpaca  ###reference_atsu-lab/stanford_alpaca###. We adapted the training script to instruct our model using the Korean LIMA dataset. Instruction-tuning follows the SFT method, wherein prompts are provided as inputs to the model which is subsequently trained to produce the user-desired responses. While this process is similar to pretraining, it differs in that only the output of the prompt is used to compute the loss. This can be mathematically represented as follows:\nWhere  represents the model\u2019s parameters,  denotes the SFT dataset, and  signifies the token sequence of the template containing the instruction and output.\nPretraining and instruction-tuning require substantial GPU resources. Recent research proposals have focused on training only specific portions of the model that require minimum GPU resources. LoRA Hu et al. (2022  ###reference_b10###) is a representative method that involves freezing a pretrained model and infusing each of its layers with trainable rank-decomposition matrices for further training. To apply LoRA, one must choose which parts of the entire model to train. This study trained only the linear layers of the transformer attention, including the query, key, and value, along with the expanded word embedding (as shown in Figure 1  ###reference_###).\nConsequently, 5.977% of the total Llama2 parameters were used for training, thereby facilitating the training of our model on a single A6000 GPU.\nFigure 1  ###reference_### shows the three proposed enhancement methods. The final model underwent the following sequence: (1) vocabulary expansion, (2) bilingual pretraining, and (3) instruction tuning (SFT). Within this context, \u201cTrainable\u201d (red) and \u201cFrozen\u201d (blue) refer to regions where the parameters were updated and not updated during training, respectively. SFT was concurrently performed using the constructed Korean and English LIMA datasets.\n###figure_1###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Quantitative evaluation",
            "text": "This section describes the following experiments to explore the three research objectives presented in the introduction: (1) comparison between the models with and without the expanded Korean vocabulary; (2) comparison between the model pretrained on the Korean\u2013English bilingual training data and that trained only on the Korean data; and (3) variations in performance owing to instruction-tuning using the LIMA dataset."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Evaluation Environment",
            "text": "###table_3### To ensure a fair comparison of LLMs, it is essential to define the task selection for evaluation and specify the LLM model used in the evaluation. To quantitatively evaluate the problem-solving capability of LLMs from various perspectives, the tasks involve language comprehension and inference, sentiment analysis, etcetera Park et al. (2021  ###reference_b22###); Zhou et al. (2023  ###reference_b40###). The Korean LLMs were comprehensively evaluated based on eight datasets.\nThe benchmarks for the evaluation tasks were KLUE\u2019s NLI, STS, and YNAT, and Naver AI\u2019s Ko-SBI Lee et al. (2023b  ###reference_b17###), and KOBEST\u2019s BoolQ, HellaSwag, SentiNeg, COPA Jang et al. (2022  ###reference_b11###), which are described as follows.\nNatural Language Inference (NLI): A classification dataset predicting the relationship between two sentences.\nSemantic Textual Similarity (STS): A classification dataset measuring the semantic equivalence between two sentences.\nYNAT: A classification dataset that infers the topic of a given sentence.\nSBI: A classification dataset aimed at identifying social stereotypes or biases.\nBoolQ: A question answering dataset for yes/no questions.\nHellaSwag: A commonsense NLI dataset.\nSentiNeg: A sentiment classification data.\nCOPA: A classification dataset determining the cause/effect based on a paragraph.\nThe experiments were performed using the Polyglot team\u2019s public branch of EleutherAI\u2019s lm-evaluation harness Gao et al. (2021  ###reference_b7###) to ensure reproducibility and compare the models. Each model was evaluated using the same data and prompt commands. Table 5  ###reference_### lists the STS evaluation prompts for which each system generated an answer.\nFor fair evaluation, the model to be evaluated must accurately represent the backbone of the training model and size of the used data, which are defined for the proposed model in Table 4  ###reference_###. The Model column is structured in the format \u201cModel-Pretrain Language-Option\u201d. The Pretrain Language value is \u2018bi\u2019 denotes a model that simultaneously uses the Korean and English languages for pretraining. The Option field denotes the application of SFT, where \u201cbiSFT\u201d represents the implementation of the Korean and English LIMA data, whereas \u201ckoSFT\u201d denotes the usage of only the Korean LIMA data. The Bllossom model refers to the model with vocabulary expansion applied to Llama2. For instance, Llama2-ko is a model pretrained on Llama2 in Korean and Bllossom-bi is a model pretrained in Korean and English after vocabulary expansion. The Bllossom-bi-koSFT is a Bllossom-bi model tuned using the Korean LIMA data. Polyglot-ko and KoAlpaca, are presented in Section 2  ###reference_###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Experiment Results",
            "text": "(Overall) Table 6  ###reference_### shows the performances of various models proposed in Table 4  ###reference_###. Compared to the monolingual models (such as Polyglot-Ko, KoAlpaca, and Kullm), the proposed multilingual Bllossom models (referred to as \u201cours\u201d) exhibited an average performance with an increment of approximately 4.57 points. The MLLM performance was affected by the presence or absence of pretraining. The difference between the performances of Llama2-ko, which underwent only pretraining, and Llama2-koSFT, which underwent only SFT, was a substantial 6.2 points.\n(The influence of vocabulary expansion)\nIn Table 6  ###reference_###, Bllossom-ko outperformed Llama2-ko by approximately 1.8 points. For NLI and STS which infer the relationship between two sentences, the Bllossom-ko model with an expanded vocabulary outperformed by 9.15 points. Contrastingly, Llama2-ko, which did not undergo vocabulary expansion, performed better on SBI by 8.8 points. Thus, vocabulary expansion improves the overall comprehension, reasoning, cognition, and causal understanding of the Korean language.\n(The influence of bilingual pretraining)\nThe Bllossom-ko and Bllossom-bi models in Table 6  ###reference_### differ on the usage of English and Korean bilingual training data during pretraining. The models exhibited similar performances with scores of 58.9 and 58.6, respectively. However, the following observations were made: (1) In contrast to Bllosson-bi, Bllossom-ko exhibited a bias issue wherein the model responded in Korean even when queried in English. (2) For the SBI tasks, Bllossom-bi outperformed by 11.6 points than Bllossom-ko. And it underperformed 11.2 and 10.6 points on the STS and HellaSwag tasks, respectively. Quantitatively, the impact of bilingual pretraining was minimal; however, a significant performance difference was qualitatively observed owing to bilingual pretraining.\n(The influence of SFT on the Korean LIMA) This experiment evaluated the impact of 1K Korean LIMA data by comparing Llama2 and Llama2-koSFT, which performed SFT on Llama2.\nIn Table 6  ###reference_###, Llama2, the backbone, outperformed by an average of 2.2 points. Similarly, the performance of Komt, which underwent an extensive SFT on Llama2, was approximately reduced by six points. The models based on Polyglot-ko, such as KoAlpaca and Kullm, exhibited lower performance than that of the backbone. Therefore, SFT may not significantly influence the quantitative evaluations of classification tasks. However, Llama2-koSFT empirically produced better responses than Llama2 based on qualitative factors, such as the quality of the generated responses, vocabulary, and completeness. Therefore, the following section analyzes the effects of SFT based on qualitative evaluations performed by humans Lee et al. (2023c  ###reference_b18###) and GPT."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Qualitative evaluation",
            "text": "Based on LIMA Zhou et al. (2023  ###reference_b40###), the qualitative evaluation was performed by humans and GPT. The former involved posing the same question to LLMs A and B and the evaluators subsequently deciding among the responses based on the following three options: Model A is better, Model B is better, or neither is significantly better. Contrastingly, GPT4-based evaluation enabled the GPT to decide among these options. We translated 300 entries from the LIMA human evaluation test dataset into Korean and proceeded with evaluation. According to the LIMA study, the 1k LIMA training data and LIMA human evaluation test dataset were designed to have completely different topics, styles, and tasks. Therefore, tuning the model using LIMA training data negligibly improves the performance Zhou et al. (2023  ###reference_b40###).\n(Overall) Figure 2  ###reference_### and Figure 3  ###reference_### show the results of human and GPT4 evaluations, respectively. When comparing Bllossom to Koalpaca and Kullm models of the same size, Bllossom outperformed them in human and GPT4 evaluations and even outperformed the larger Llama2-70b-chat model. Another interesting point is the qualitative evaluation results for human and GPT4 were similar. This was also observed in LIMA."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   Human-assisted preference evaluation",
            "text": "###figure_2### In Figure 2  ###reference_###, the number of times Bllossom won both Koalpaca and Llama2 in human evaluation was 124. The 124 tasks included answering real-world user requests for recommendations, answering questions requiring imagination or creativity, organizing travel itineraries and writing code. In contrast, there were 40 instances where Bllossom gave inferior answers compared to both models, mostly for factual QA. This suggests that Bllossom is not yet as knowledgeable as the larger models, which is likely due to the difference in data size from the pre-training phase."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Preference Evaluation using GPT4",
            "text": "Using the methodology proposed by LIMA, GPT4 was used to compare the performance of Bllossom with six other models. The evaluation was conducted on the Korean LIMA test data.\n(Comparing Bllossom with Korean models based on Llama2) Figure 3  ###reference_### shows that Komt and Ko-Platypus2 are models based on Llama2-13b that underwent SFT, similar to Bllossom. However, these two models were exclusively subjected to SFT without vocabulary expansion or pretraining. They were fine-tuned using extensive datasets that were either autogenerated or translated, and thus, of lower quality. Qualitatively, Bllossom exhibited superior performance with a margin exceeding 40%, indicating that pretraining significantly influences the Korean proficiency. During the human evaluation of history-related questions, Komt and Ko-Platypus2 either failed to provide answers or exhibited hallucinations more frequently compared to Bllossom. This can be attributed to Bllossom gaining additional knowledge during pretraining.\n###figure_3### (Comparing Bllossom with Polyglot-ko-based Korean models) We discuss whether the proposed Bllossom model exhibits a better qualitative evaluation than Korean monolingual LLMs. Polyglot-ko is a representative Korean monolingual model pretrained on vast Korean datasets and KoAlpaca and Kullm are models trained based on Polyglot-ko. Figure 3  ###reference_###, shows that the Bllossom model has a 9~56% higher probability of producing superior answers than the two monolingual models utilizing Polyglot-Ko as their backbone. During pretraining, Llama2 incorporated a relatively limited set of Korean data; however, its training dataset significantly expanded when English data was included, surpassing the dataset size of Polyglot-ko. This suggests that the bilingual pretraining, which was carried out to augment the deficient proficiency in Korean, has somewhat assisted in bridging the knowledge between Korean and English Cui et al. (2023  ###reference_b6###).\n(Comparing Bllossom with GPT4 and Llama) We discuss the Korean-language proficiency of the proposed Bllossom model. The Llama2-70b model, which has significantly more parameters, was evaluated. Based on the results in Figure 3  ###reference_###, the Bllossom model was selected for approximately 14% of the answers than Llama2-70B. Therefore, in case of an extreme difference in the number of parameters, the differences in performance can be fairly compensated via techniques such as word expansion and pretraining. The qualitative evaluation results for OpenAI\u2019s much larger GPT4 model indicated its superiority in frequent answering.\n(The effect of bilingual dataset for SFT) In Figure 4  ###reference_###, the Bllossom-bi-koSFT model and the Bllossom-bi-biSFT model differ based on whether bilingual data was utilized for SFT. We conducted a comparative evaluation of the two models using both Korean and English. For the Korean and English LIMA test data, the win ratio for the Bllossom-bi-biSFT model was overwhelmingly high at 67% and 95%, respectively. This indicates that contrary to qualitative evaluation, the effect of bilingual SFT in quantitative evaluation is significant.\n###figure_4### (The effect of English language)\nWe investigated whether a model tuned to Korean based on Llama2 would perform poorly in English. As shown in Figure 5  ###reference_###, all the models appear to have significantly lost their English proficiency compared to the original Llama2 model. Nevertheless, Bllossom showed much better performance compared to Komt and Ko-Platypus2. From this, we can infer that while acquiring Korean proficiency, the Bllossom model has a lesser reduction in English capabilities compared to other Korean LLMs.\n###figure_5###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "This study proposed three methods for enhancing the MLLM capability of LRLs. First, to improve the Korean vocabulary capability of the existing Llama2 model, the vocabularies from KoBERT and Llama2 were merged to create a new embedding. Second, pretraining was performed using bilingual data to enhance knowledge information by aligning high- and low-resource languages. Third, instruction-tuning was performed using the English and refined Korean LIMA datasets to accurately understand the user intent and produce the desired response. Quantitative assessments were performed using eight benchmark datasets and qualitative assessments were conducted using humans and the GPT4 model to investigate the proposed model. The experimental results revealed that the proposed Bllossom model outperformed the pre-existing Korean monolingual models that require vast computing resources and supervised data."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Ethical Considerations",
            "text": "While we have no ethical concerns regarding the current work, our commitment to upholding the highest ethical standards in all our activities and human evaluations remains unwavering."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Limitations",
            "text": "In this paper, we proposed a method to enhance Korean language in MLLM. However, to apply the same method to other languages, the following efforts are required. (1) For building LIMA data, one needs to translate 1,030 data, (2) one also need to translate 300 training data for testing."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1\">\n<td class=\"ltx_td ltx_align_left\" colspan=\"2\" id=\"S3.T1.1.1.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.1.1\">\\hlineB</span><span class=\"ltx_text\" id=\"S3.T1.1.1.1.2\" style=\"font-size:90%;\">4\n Sentence: \ud584\ubc84\uac70\ub97c \uba39\ub294 \uacf5\ub8e1</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2\">\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" id=\"S3.T1.1.2.1\"><span class=\"ltx_text\" id=\"S3.T1.1.2.1.1\" style=\"font-size:90%;\">(A dinosaur eating a hamburger)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.1\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" id=\"S3.T1.1.3.1.1\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.3.1.2\" style=\"font-size:90%;\">Model</span><span class=\"ltx_text\" id=\"S3.T1.1.3.1.3\" style=\"font-size:90%;\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.3.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.3.2.1\" style=\"font-size:90%;\">Tokenization results</span><span class=\"ltx_text\" id=\"S3.T1.1.3.2.2\" style=\"font-size:90%;\"></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.1\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" id=\"S3.T1.1.4.1.1\" style=\"font-size:90%;\">\nLlama2</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.1.4.2\" style=\"width:159.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.4.2.1\"><span class=\"ltx_text\" id=\"S3.T1.1.4.2.1.1\" style=\"font-size:90%;\">\u2018_\u2019, \u2018&lt;0xED&gt;\u2019, \u2018&lt;0x96&gt;\u2019, \u2018&lt;0x84&gt;\u2019, \u2018&lt;0xEB&gt;\u2019, \u2018&lt;0xB2&gt;\u2019, \u2018&lt;0x84&gt;\u2019, \u2018&lt;0xEA&gt;\u2019, \u2018&lt;0xB1&gt;\u2019, \u2018&lt;0xB0&gt;\u2019, \u2018\ub97c\u2019, \u2018_\u2019, \u2018&lt;0xEB&gt;\u2019, \u2018&lt;0xA8&gt;\u2019, \u2018&lt;0xB9&gt;\u2019, \u2018\ub294\u2019, \u2018_\u2019, \u2018\uacf5\u2019, \u2018&lt;0xEB&gt;\u2019, \u2018&lt;0xA3&gt;\u2019, \u2018&lt;0xA1&gt;\u2019</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T1.1.5.1\"><span class=\"ltx_text\" id=\"S3.T1.1.5.1.1\" style=\"font-size:90%;\">Proposed</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.1.5.2\" style=\"width:159.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.1.5.2.1\"><span class=\"ltx_text\" id=\"S3.T1.1.5.2.1.1\" style=\"font-size:90%;\">\u2018\ud584\u2019, \u2018\ubc84\u2019, \u2018\uac70\u2019, \u2018\ub97c\u2019, \u2018_\uba39\ub294\u2019, \u2018_\u2019, \u2018\uacf5\u2019, \u2018\ub8e1\u2019</span></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T1.1.6.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.6.1.1\">\\hlineB</span><span class=\"ltx_text\" id=\"S3.T1.1.6.1.2\" style=\"font-size:90%;\">4</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.1.6.2\" style=\"width:159.3pt;\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span> Comparison of tokenization results between Llama2 and the proposed model</figcaption>\n</figure>",
            "capture": "Table 1:  Comparison of tokenization results between Llama2 and the proposed model"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T2.1\" style=\"width:455.2pt;height:161.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-5.4pt,1.9pt) scale(0.976683951223674,0.976683951223674) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.1.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T2.1.1.1.1.1\">\\hlineB</span>4\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.2\">Category</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.2.1\">LIMA dataset <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"(huggingface.co/datasets/GAIR/lima)\" title=\"\">(huggingface.co/datasets/GAIR/lima)</a></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.2.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.2.1.1\">NE change</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.2.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.2.2.1\">(EN) I heard north ridge of <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.2.2.1.1\">mount Stuart</span> from my friends, can you tell me more?</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.3\">\n<td class=\"ltx_td\" id=\"S3.T2.1.1.3.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.3.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.3.2.1\">(KO) I heard north ridge of \u2018<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.3.2.1.1\">Bukhansan Mountain</span>\u2019 from my friends, can you tell me more?</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.4.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.4.1.1\">NE change</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.4.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.4.2.1\">(EN) How to claim tax back (in USA)?</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.5\">\n<td class=\"ltx_td\" id=\"S3.T2.1.1.5.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.5.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.5.2.1\">(KO) How to claim tax back <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.5.2.1.1\">in Korea</span>?</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.6.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.6.1.1\">topic change</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.6.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.6.2.1\">(EN) What are the primary objections <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.6.2.1.1\">Democrats</span> have to a <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.6.2.1.2\">border wall</span>?</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.7\">\n<td class=\"ltx_td\" id=\"S3.T2.1.1.7.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.7.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.7.2.1\">(KO) What is the <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.7.2.1.1\">Korean Democratic Party\u2019s</span> opinion on <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.7.2.1.2\">voting rights</span> for overseas Koreans?</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.8.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.8.1.1\">topic change</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.8.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.8.2.1\">(EN) How to make <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.8.2.1.1\">creepy</span> food??</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.9\">\n<td class=\"ltx_td\" id=\"S3.T2.1.1.9.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.9.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T2.1.1.9.2.1\">(KO) How to make <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.9.2.1.1\">bizarre</span> food??</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.1.10.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T2.1.1.10.1.1\">\\hlineB</span>4</td>\n<td class=\"ltx_td\" id=\"S3.T2.1.1.10.2\"></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Instances of modifications in the English LIMA dataset to reflect the Korean cultural context</figcaption>\n</figure>",
            "capture": "Table 2: Instances of modifications in the English LIMA dataset to reflect the Korean cultural context"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.1.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1\">\\hlineB</span><span class=\"ltx_text\" id=\"S3.T3.1.1.1.2\" style=\"font-size:90%;\">4\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.3\" style=\"font-size:90%;\">Language</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.2.1\" style=\"font-size:90%;\">Source</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.3.1\" style=\"font-size:90%;\">Size(GB)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.4.1\" style=\"font-size:90%;\">Content</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.2.1\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" id=\"S3.T3.1.2.1.1\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.2.1.2\" style=\"font-size:90%;\">Korean</span><span class=\"ltx_text\" id=\"S3.T3.1.2.1.3\" style=\"font-size:90%;\"></span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.2.2\"><code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.T3.1.2.2.1\" style=\"font-size:90%;\">Public</code></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.2.3\"><code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.T3.1.2.3.1\" style=\"font-size:90%;\">22.41</code></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.2.4\"><span class=\"ltx_text\" id=\"S3.T3.1.2.4.1\" style=\"font-size:90%;\">news, web</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.3\">\n<td class=\"ltx_td\" id=\"S3.T3.1.3.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.3.2\"><code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.T3.1.3.2.1\" style=\"font-size:90%;\">WIKI-ko</code></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.3.3\"><code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.T3.1.3.3.1\" style=\"font-size:90%;\">0.76</code></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.3.4\"><span class=\"ltx_text\" id=\"S3.T3.1.3.4.1\" style=\"font-size:90%;\">wikipedia</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.4.1.1\" style=\"font-size:90%;\">English</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.4.2\"><code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.T3.1.4.2.1\" style=\"font-size:90%;\">WIKI-en</code></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.4.3\"><code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.T3.1.4.3.1\" style=\"font-size:90%;\">9.92</code></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.4.4\"><span class=\"ltx_text\" id=\"S3.T3.1.4.4.1\" style=\"font-size:90%;\">wikipedia</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.5.1\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.0pt;background:black;display:inline-block;\">\u00a0</span><span class=\"ltx_text\" id=\"S3.T3.1.5.1.1\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.5.1.2\" style=\"font-size:90%;\">Total</span><span class=\"ltx_text\" id=\"S3.T3.1.5.1.3\" style=\"font-size:90%;\"></span>\n</td>\n<td class=\"ltx_td\" id=\"S3.T3.1.5.2\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.5.3\"><code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.T3.1.5.3.1\" style=\"font-size:90%;\">33.09</code></td>\n<td class=\"ltx_td\" id=\"S3.T3.1.5.4\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T3.1.6.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.6.1.1\">\\hlineB</span><span class=\"ltx_text\" id=\"S3.T3.1.6.1.2\" style=\"font-size:90%;\">4</span>\n</td>\n<td class=\"ltx_td\" id=\"S3.T3.1.6.2\"></td>\n<td class=\"ltx_td\" id=\"S3.T3.1.6.3\"></td>\n<td class=\"ltx_td\" id=\"S3.T3.1.6.4\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span> The composition of the pretraining data. The <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T3.7.1\">Public</span> data is in (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"www.aihub.or.kr\" title=\"\">www.aihub.or.kr</a>) </figcaption>\n</figure>",
            "capture": "Table 3:  The composition of the pretraining data. The Public data is in (www.aihub.or.kr) "
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T4.1\" style=\"width:455.2pt;height:228.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-2.2pt,1.1pt) scale(0.990519788167657,0.990519788167657) ;\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T4.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.1.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T4.1.1.1.1.1\">\\hlineB</span><span class=\"ltx_text\" id=\"S5.T4.1.1.1.1.2\" style=\"font-size:90%;\">4\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.1.3\" style=\"font-size:90%;\">Model</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.2.1\" style=\"font-size:90%;\">Features</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.3.1\" style=\"font-size:90%;\">Backbone</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.4.1\" style=\"font-size:90%;\">Instruction</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.5.1\" style=\"font-size:90%;\">Pre-training</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.2.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T4.1.1.2.1.1\">\\hlineB</span><span class=\"ltx_text\" id=\"S5.T4.1.1.2.1.2\" style=\"font-size:90%;\">4\n</span><a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/EleutherAI/polyglot-ko-12.8b\" style=\"font-size:90%;\" title=\"\">polyglot-ko-12.8b</a>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.2.2.1\" style=\"font-size:90%;\">Monolingual</span><span class=\"ltx_text\" id=\"S5.T4.1.1.2.2.2\" style=\"font-size:90%;\"> model</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.2.3\"><span class=\"ltx_text\" id=\"S5.T4.1.1.2.3.1\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.2.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.2.4.1\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.2.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.2.5.1\" style=\"font-size:90%;\">1.2TB</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.3.1\"><a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B\" style=\"font-size:90%;\" title=\"\">KoAlpaca-Polyglot-12.8b</a></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.3.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.3.2.1\" style=\"font-size:90%;\">+/mono SFT\u00a0(21K)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.3.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.3.3.1\" style=\"font-size:90%;\">polyglot-ko-12.8b</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.3.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.3.4.1\" style=\"font-size:90%;\">21K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.3.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.3.5.1\" style=\"font-size:90%;\">None</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.4.1\"><a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2\" style=\"font-size:90%;\" title=\"\">kullm-polyglot-12.8b-v2</a></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.4.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.4.2.1\" style=\"font-size:90%;\">+/mono SFT\u00a0(153K)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.4.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.4.3.1\" style=\"font-size:90%;\">polyglot-ko-12.8b</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.4.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.4.4.1\" style=\"font-size:90%;\">153K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.4.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.4.5.1\" style=\"font-size:90%;\">None</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.5.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T4.1.1.5.1.1\">\\hlineB</span><span class=\"ltx_text\" id=\"S5.T4.1.1.5.1.2\" style=\"font-size:90%;\">4\n</span><a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/meta-Llama/Llama-2-13b-chat-hf\" style=\"font-size:90%;\" title=\"\">Llama2</a>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.5.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.5.2.1\" style=\"font-size:90%;\">Multilingual</span><span class=\"ltx_text\" id=\"S5.T4.1.1.5.2.2\" style=\"font-size:90%;\"> model</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.5.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.5.3.1\" style=\"font-size:90%;\">Llama-2-13b-hf</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.5.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.5.4.1\" style=\"font-size:90%;\">27K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.5.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.5.5.1\" style=\"font-size:90%;\">2 trillion-token</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.6.1\"><a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kyujinpy/KO-Platypus2-13B\" style=\"font-size:90%;\" title=\"\">Ko-Platypus2-13B</a></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.6.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.6.2.1\" style=\"font-size:90%;\">+/ mono SFT\u00a0(25K)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.6.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.6.3.1\" style=\"font-size:90%;\">Llama-2-13b-hf</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.6.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.6.4.1\" style=\"font-size:90%;\">25K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.6.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.6.5.1\" style=\"font-size:90%;\">None</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.7.1\"><a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/davidkim205/komt-Llama-2-13b-hf\" style=\"font-size:90%;\" title=\"\">komt-Llama-2-13b-hf</a></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.7.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.7.2.1\" style=\"font-size:90%;\">+/ mono SFT\u00a0(154K)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.7.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.7.3.1\" style=\"font-size:90%;\">Llama-2-13b-chat-hf</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.7.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.7.4.1\" style=\"font-size:90%;\">1,543K</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.7.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.7.5.1\" style=\"font-size:90%;\">None</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.1.8.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.8.1.1\" style=\"font-size:90%;\">Llama2-koSFT</span><span class=\"ltx_text\" id=\"S5.T4.1.1.8.1.2\" style=\"font-size:90%;\">\u00a0(ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.1.8.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.8.2.1\" style=\"font-size:90%;\">+/\u00a0mono SFT\u00a0(1K)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.1.8.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.8.3.1\" style=\"font-size:90%;\">Llama-2-13b-chat-hf</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.1.8.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.8.4.1\" style=\"font-size:90%;\">1K\u00a0(Ko LIMA)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.1.8.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.8.5.1\" style=\"font-size:90%;\">None</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.9.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.9.1.1\" style=\"font-size:90%;\">Llama2-ko</span><span class=\"ltx_text\" id=\"S5.T4.1.1.9.1.2\" style=\"font-size:90%;\">\u00a0(ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.9.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.9.2.1\" style=\"font-size:90%;\">+/\u00a0mono PT\u00a0(33GB)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.9.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.9.3.1\" style=\"font-size:90%;\">Llama-2-13b-chat-hf</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.9.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.9.4.1\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.9.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.9.5.1\" style=\"font-size:90%;\">33Gb\u00a0(Ko)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.10.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.10.1.1\" style=\"font-size:90%;\">Bllossom-ko</span><span class=\"ltx_text\" id=\"S5.T4.1.1.10.1.2\" style=\"font-size:90%;\">\u00a0(ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.10.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.10.2.1\" style=\"font-size:90%;\">\u2003+/\u00a0expand_vocab</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.10.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.10.3.1\" style=\"font-size:90%;\">Llama-2-13b-chat-hf</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.10.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.10.4.1\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.10.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.10.5.1\" style=\"font-size:90%;\">33Gb\u00a0(Ko)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.11.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.11.1.1\" style=\"font-size:90%;\">Bllossom-bi</span><span class=\"ltx_text\" id=\"S5.T4.1.1.11.1.2\" style=\"font-size:90%;\">\u00a0(ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.11.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.11.2.1\" style=\"font-size:90%;\">+/\u00a0bilingual PT, expand_vocab</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.11.3\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.11.3.1\" style=\"font-size:90%;\">Llama-2-13b-chat-hf</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.11.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.11.4.1\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.11.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.11.5.1\" style=\"font-size:90%;\">33Gb\u00a0(Ko:En=7:3)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.12.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.12.1.1\" style=\"font-size:90%;\">Bllossom-bi-koSFT</span><span class=\"ltx_text\" id=\"S5.T4.1.1.12.1.2\" style=\"font-size:90%;\">\u00a0(ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.12.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.12.2.1\" style=\"font-size:90%;\">\u2003+/\u00a0mono SFT(1K)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.12.3\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.12.3.1\" style=\"font-size:90%;\">Bllossom-bi</span><span class=\"ltx_text\" id=\"S5.T4.1.1.12.3.2\" style=\"font-size:90%;\">(ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.12.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.12.4.1\" style=\"font-size:90%;\">1K\u00a0(Ko LIMA)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.12.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.12.5.1\" style=\"font-size:90%;\">None</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.13.1\">\n<a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://github.com/Anonymous-introduce-paper/LR\" style=\"font-size:90%;\" title=\"\">Bllossom-bi-biSFT</a><span class=\"ltx_text\" id=\"S5.T4.1.1.13.1.1\" style=\"font-size:90%;\">\u00a0(ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.13.2\"><span class=\"ltx_text\" id=\"S5.T4.1.1.13.2.1\" style=\"font-size:90%;\">\u2003+/\u00a0bilingual SFT(2K)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.13.3\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.13.3.1\" style=\"font-size:90%;\">Bllossom-bi</span><span class=\"ltx_text\" id=\"S5.T4.1.1.13.3.2\" style=\"font-size:90%;\">(ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.13.4\"><span class=\"ltx_text\" id=\"S5.T4.1.1.13.4.1\" style=\"font-size:90%;\">2K\u00a0(Ko-En,LIMA)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.13.5\"><span class=\"ltx_text\" id=\"S5.T4.1.1.13.5.1\" style=\"font-size:90%;\">None</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.1.14.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T4.1.1.14.1.1\">\\hlineB</span><span class=\"ltx_text\" id=\"S5.T4.1.1.14.1.2\" style=\"font-size:90%;\">4</span>\n</td>\n<td class=\"ltx_td\" id=\"S5.T4.1.1.14.2\"></td>\n<td class=\"ltx_td\" id=\"S5.T4.1.1.14.3\"></td>\n<td class=\"ltx_td\" id=\"S5.T4.1.1.14.4\"></td>\n<td class=\"ltx_td\" id=\"S5.T4.1.1.14.5\"></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span> Overview of the Korean LLMs (The model is from <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co\" title=\"\">https://huggingface.co</a>)</figcaption>\n</figure>",
            "capture": "Table 4:  Overview of the Korean LLMs (The model is from https://huggingface.co)"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T5.1\" style=\"width:433.6pt;height:247.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(72.3pt,-41.3pt) scale(1.50012452771584,1.50012452771584) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.1.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T5.1.1.1.1.1\">\\hlineB</span>4\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.2\">Prompt</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.1.1.2.1\" style=\"padding-bottom:-6.0pt;\">\uc9c8\ubb38: \ubb38\uc7a5 1\uacfc \ubb38\uc7a5 2\ub294 \uc11c\ub85c \uc720\uc0ac\ud55c \uc758\ubbf8\ub97c \uac00\uc9c0\ub098\uc694?</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.3.1\"><sub class=\"ltx_sub\" id=\"S5.T5.1.1.3.1.1\">(Question: Do sentence 1 and sentence 2 have similar meanings?)</sub></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.4.1\" style=\"padding-bottom:-6.0pt;\">\ubb38\uc7a5 1: \uc2b5\ub3c4\uae30 \ubcf4\uba74 \uc548\ub41c\ub2e4\uace0 \uacbd\uace0\ud588\uc5b4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.5.1\"><sub class=\"ltx_sub\" id=\"S5.T5.1.1.5.1.1\">(Sentence 1: I warned not to look at the humidifier.)</sub></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.6.1\" style=\"padding-bottom:-6.0pt;\">\ubb38\uc7a5 2: \uc2b5\ub3c4\uae30 \uc790\uafb8 \ubcf4\ub824\uace0 \ud558\uc9c0\ub9c8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.7.1\"><sub class=\"ltx_sub\" id=\"S5.T5.1.1.7.1.1\">(Sentence 2: Don\u2019t keep trying to look at the humidifier.)</sub></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.8.1\" style=\"padding-bottom:-6.0pt;\">\uc815\ub2f5:</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.9.1\"><sub class=\"ltx_sub\" id=\"S5.T5.1.1.9.1.1\">(Answer:)</sub></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.1.1.10.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T5.1.1.10.1.1\">\\hlineB</span>4</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Evaluation prompt of STS task</figcaption>\n</figure>",
            "capture": "Table 5: Evaluation prompt of STS task"
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T6\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S5.T6.1\" style=\"width:455.2pt;height:325.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(54.4pt,-38.9pt) scale(1.3143203450983,1.3143203450983) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.1.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T6.1.1.1.1.1\">\\hlineB</span>4\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.1.2\">Model</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.2.1\">NLI</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.3.1\">STS</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.4.1\">SBI</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.5.1\">YNAT</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.6.1\">BoolQ</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.7.1\">H-Swag</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.8.1\">S-Neg</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.1.9.1\">COPA</span></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.1.10\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.2\">\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T6.1.1.2.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.2\">ACC</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.3\">ACC</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.4\">F1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.5\">ACC</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.6\">F1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.7\">F1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.8\">F1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.9\">F1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.2.10\">AVG</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.3.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T6.1.1.3.1.1\">\\hlineB</span>4\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.3.1.2\">polyglot-ko-12.8b</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.2\">35.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.3\">50.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.4\">48.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.5\">31.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.6\">59.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.7.1\">48.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.8.1\">95.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.9.1\">81.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.3.10\">56.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.4.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.4.1.1\">KoAlpaca-Polyglot-12.8b</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.2\">38.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.3\">42.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.4\">48.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.5\">26.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.6\">66.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.7\">44.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.8\">84.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.9\">80.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.4.10\">53.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.5.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.5.1.1\">kullm-polyglot-12.8b-v2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.2\">33.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.3\">44.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.4\">52.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.5\">24.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.6\">44.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.7\">48.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.8\">89.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.9\">79.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.5.10\">52.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.6.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T6.1.1.6.1.1\">\\hlineB</span>4\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.6.1.2\">Llama2</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.2\">44.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.3\">45.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.4\">56.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.5\">25.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.6\">73.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.7\">40.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.8\">78.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.9\">60.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.6.10\">53.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.7.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.7.1.1\">Ko-Platypus2-13B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.2\">50.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.3\">59.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.4\">37.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.5\">28.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.6\">72.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.7\">41.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.8\">85.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.9\">63.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.7.10\">54.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.8.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.8.1.1\">Komt-Llama-2-13b-hf</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.2\">33.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.3\">51.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.4\">48.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.5\">24.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.6\">52.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.7\">39.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.8\">62.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.9\">64.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.8.10\">47.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T6.1.1.9.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.9.1.1\">Llama2-koSFT</span>\u00a0(ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.2\">44.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.3\">50.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.4\">38.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.5\">23.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.6\">71.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.7\">41.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.8\">77.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.9\">60.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.9.10\">50.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.10.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.10.1.1\">Llama2-ko</span>\u00a0(ours)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.2\">41.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.3\">47.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.4\">61.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.5\">32.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.6\">72.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.7\">43.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.8\">89.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.9\">68.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.10.10\">57.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.11.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.11.1.1\">Bllossom-ko</span>\u00a0(ours)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.2\">49.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.3\">57.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.4\">52.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.5\">33.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.6\">73.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.7\">48.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.8\">87.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.9\">69.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.11.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.11.10.1\">58.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.12.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.12.1.1\">Bllossom-bi</span>\u00a0(ours)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.2\">48.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.3\">46.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.12.4.1\">64.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.5\">32.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.6\">74.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.7\">38.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.8\">93.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.9\">71.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.12.10\">58.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.13.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.13.1.1\">Bllossom-bi-koSFT</span>\u00a0(ours)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.13.2.1\">49.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.13.3.1\">54.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.4\">55.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.5\">33.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.13.6.1\">74.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.7\">40.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.8\">92.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.9\">68.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.13.10\">58.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.14.1\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T6.1.1.14.1.1\">Bllossom-bi-biSFT</span>\u00a0(ours)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.2\">45.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.3\">46.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.4\">63.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.14.5.1\">36.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.6\">69.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.7\">39.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.8\">89.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.9\">70.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.1.1.14.10\">57.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T6.1.1.15.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T6.1.1.15.1.1\">\\hlineB</span>4</td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.2\"></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.3\"></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.4\"></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.5\"></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.6\"></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.7\"></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.8\"></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.9\"></td>\n<td class=\"ltx_td\" id=\"S5.T6.1.1.15.10\"></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span> Benchmarking Korean LLMs: Accuracy (ACC) and F1 score metrics across tasks</figcaption>\n</figure>",
            "capture": "Table 6:  Benchmarking Korean LLMs: Accuracy (ACC) and F1 score metrics across tasks"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.10882v2_figure_1.png",
            "caption": "Figure 1:  Architecture of the proposed model. During training, the LoRA adapter takes Query, Key, and Value from LLM and trains it with new parameters."
        },
        "2": {
            "figure_path": "2403.10882v2_figure_2.png",
            "caption": "Figure 2: Preference evaluation results by human"
        },
        "3": {
            "figure_path": "2403.10882v2_figure_3.png",
            "caption": "Figure 3: Preference evaluation results by GPT4"
        },
        "4": {
            "figure_path": "2403.10882v2_figure_4.png",
            "caption": "Figure 4: Comparing biSFT and koSFT models"
        },
        "5": {
            "figure_path": "2403.10882v2_figure_5.png",
            "caption": "Figure 5: Comparing English performance in Llama2-backboned models"
        }
    },
    "references": [
        {
            "1": {
                "title": "Learning bilingual word embeddings with (almost) no bilingual data.",
                "author": "Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451\u2013462, Vancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-1042"
            }
        },
        {
            "2": {
                "title": "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.",
                "author": "Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 789\u2013798, Melbourne, Australia. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P18-1073"
            }
        },
        {
            "3": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.",
                "venue": "In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf"
            }
        },
        {
            "4": {
                "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
                "author": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.",
                "venue": null,
                "url": "https://lmsys.org/blog/2023-03-30-vicuna/"
            }
        },
        {
            "5": {
                "title": "Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm.",
                "author": "Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023.",
                "venue": null,
                "url": "https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm"
            }
        },
        {
            "6": {
                "title": "Efficient and effective text encoding for chinese llama and alpaca.",
                "author": "Yiming Cui, Ziqing Yang, and Xin Yao. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2304.08177"
            }
        },
        {
            "7": {
                "title": "A framework for few-shot language model evaluation.",
                "author": "Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021.",
                "venue": null,
                "url": "https://doi.org/10.5281/zenodo.5371628"
            }
        },
        {
            "8": {
                "title": "Universal neural machine translation for extremely low resource languages.",
                "author": "Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 344\u2013354, New Orleans, Louisiana. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N18-1032"
            }
        },
        {
            "9": {
                "title": "An empirical analysis of compute-optimal large language model training.",
                "author": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Kar\u00e9n Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. 2022.",
                "venue": "In Advances in Neural Information Processing Systems, volume 35, pages 30016\u201330030. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf"
            }
        },
        {
            "10": {
                "title": "LoRA: Low-rank adaptation of large language models.",
                "author": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=nZeVKeeFYf9"
            }
        },
        {
            "11": {
                "title": "KoBEST: Korean balanced evaluation of significant tasks.",
                "author": "Myeongjun Jang, Dohyung Kim, Deuk Sin Kwon, and Eric Davis. 2022.",
                "venue": "In Proceedings of the 29th International Conference on Computational Linguistics, pages 3697\u20133708, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.",
                "url": "https://aclanthology.org/2022.coling-1.325"
            }
        },
        {
            "12": {
                "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans.",
                "author": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020.",
                "venue": "Transactions of the Association for Computational Linguistics, 8:64\u201377.",
                "url": "https://doi.org/10.1162/tacl_a_00300"
            }
        },
        {
            "13": {
                "title": "A technical report for polyglot-ko: Open-source large-scale korean language models.",
                "author": "Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Jiwung Hyun, Sungho Park, and Kyubyong Park. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2306.02254"
            }
        },
        {
            "14": {
                "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
                "author": "Taku Kudo and John Richardson. 2018.",
                "venue": null,
                "url": "http://arxiv.org/abs/1808.06226"
            }
        },
        {
            "15": {
                "title": "Kullm: Korea university large language model project.",
                "author": "NLP, AI Lab and Human-Inspired AI research. 2023.",
                "venue": "https://github.com/nlpai-lab/kullm.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Platypus: Quick, cheap, and powerful refinement of llms.",
                "author": "Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2308.07317"
            }
        },
        {
            "17": {
                "title": "KoSBI: A dataset for mitigating social bias risks towards safer large language model applications.",
                "author": "Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Gunhee Kim, and Jung-woo Ha. 2023b.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 208\u2013224, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-industry.21"
            }
        },
        {
            "18": {
                "title": "Qasa: Advanced question answering on scientific articles.",
                "author": "Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. 2023c.",
                "venue": "In Proceedings of the 40th International Conference on Machine Learning.",
                "url": null
            }
        },
        {
            "19": {
                "title": "On faithfulness and factuality in abstractive summarization.",
                "author": "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.173"
            }
        },
        {
            "20": {
                "title": "BEEP! Korean corpus of online news comments for toxic speech detection.",
                "author": "Jihyung Moon, Won Ik Cho, and Junbum Lee. 2020.",
                "venue": "In Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media, pages 25\u201331, Online. Association for Computational Linguistics.",
                "url": "https://www.aclweb.org/anthology/2020.socialnlp-1.4"
            }
        },
        {
            "21": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.",
                "venue": "In Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf"
            }
        },
        {
            "22": {
                "title": "Klue: Korean language understanding evaluation.",
                "author": "Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Ji Yoon Han, Jangwon Park, Chisung Song, Junseong Kim, Youngsook Song, Taehwan Oh, Joohong Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy Park, Lucy Park, Alice Oh, Jung-Woo Ha (NAVER AI Lab), Kyunghyun Cho, and Kyunghyun Cho. 2021.",
                "venue": "In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran.",
                "url": "https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/98dce83da57b0395e163467c9dae521b-Paper-round2.pdf"
            }
        },
        {
            "23": {
                "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback.",
                "author": "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2302.12813"
            }
        },
        {
            "24": {
                "title": "Instruction tuning with gpt-4.",
                "author": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023b.",
                "venue": "arXiv preprint arXiv:2304.03277.",
                "url": null
            }
        },
        {
            "25": {
                "title": "How multilingual is multilingual BERT?",
                "author": "Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996\u20135001, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1493"
            }
        },
        {
            "26": {
                "title": "Improving language understanding by generative pre-training.",
                "author": "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. a.",
                "venue": null,
                "url": null
            }
        },
        {
            "27": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. b.",
                "venue": null,
                "url": null
            }
        },
        {
            "28": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.",
                "venue": "J. Mach. Learn. Res., 21(1).",
                "url": null
            }
        },
        {
            "29": {
                "title": "Cross-lingual transfer learning for multilingual task oriented dialog.",
                "author": "Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. 2019.",
                "venue": null,
                "url": "http://arxiv.org/abs/1810.13327"
            }
        },
        {
            "30": {
                "title": "Stanford alpaca: An instruction-following llama model.",
                "author": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.",
                "venue": "https://github.com/tatsu-lab/stanford_alpaca.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2302.13971"
            }
        },
        {
            "32": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2307.09288"
            }
        },
        {
            "33": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
            }
        },
        {
            "34": {
                "title": "Improving pre-trained multilingual models with vocabulary expansion.",
                "author": "Hai Wang, Dian Yu, Kai Sun, Janshu Chen, and Dong Yu. 2019.",
                "venue": null,
                "url": "http://arxiv.org/abs/1909.12440"
            }
        },
        {
            "35": {
                "title": "Finetuned language models are zero-shot learners.",
                "author": "Jason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Mingbo Dai, and Quoc V. Le. 2022a.",
                "venue": null,
                "url": "https://openreview.net/forum?id=gEZrGCozdqR"
            }
        },
        {
            "36": {
                "title": "Emergent abilities of large language models.",
                "author": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2206.07682"
            }
        },
        {
            "37": {
                "title": "Bloom: A 176b-parameter open-access multilingual language model.",
                "author": "BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,\nIan Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina,\nVeronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u015far, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von\nPlaten, Pierre Cornette, Pierre Fran\u00e7ois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u011bk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona\nUnldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu\u00f1oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito,\nChenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya\nKainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2211.05100"
            }
        },
        {
            "38": {
                "title": "A survey of large language models.",
                "author": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.18223"
            }
        },
        {
            "39": {
                "title": "Why does chatgpt fall short in providing truthful answers?",
                "author": "Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2304.10513"
            }
        },
        {
            "40": {
                "title": "Lima: Less is more for alignment.",
                "author": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2305.11206"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.10882v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "6",
            "6.1",
            "6.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.10882v2",
        "paper_title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean",
        "research_background": "To describe the paper's motivation, research problem, and relevant prior work based on provided excerpts from the introduction and related work sections:\n\n### Motivation:\nThe motivation behind this study stems from the significant challenge faced by smaller research groups in utilizing large language models (LLMs) due to the extensive computing resources required for training. Although big tech companies and research institutes have launched multilingual LLMs (MLLMs) to cater to various languages, less-resourced languages (LRLs) are often overlooked, with minimal data and vocabulary available for these languages. Specifically, the paper focuses on the Korean language, which, despite not being an LRL, faces challenges due to limited training data in models like Llama2, which used only 0.06% Korean data. This limited data leads to significant syntactic and semantic challenges when leveraging MLLMs for Korean-specific tasks.\n\n### Research Problem:\nThe research problem addressed in this study is exploring methods to enhance the capabilities of MLLMs for the Korean language with minimal additional data. Specifically, the study investigates:\n1. How expanding the vocabulary of an MLLM using language-specific dictionaries can benefit the model.\n2. The effectiveness of enriching the model's semantic knowledge via pretraining with additional data.\n3. Improving the usability and accuracy of the MLLM for tasks in the Korean language by generating high-quality instruction data.\n\n### Relevant Prior Work:\nSeveral prior works are referenced to contextualize the research:\n- **Lowering Computing Costs in LLMs**: Hoffmann et al. (2022) discuss the issue of growing computing resources required for LLMs, highlighting the challenge for smaller research institutions.\n- **Development of Multilingual LLMs**: Touvron et al. (2023a, b) and Workshop et al. (2023) focus on the creation of MLLMs, but note the oversight of LRLs in these models.\n- **Challenges with Less-Resourced Languages**: Gu et al. (2018) underline the general neglect of LRLs in LLM training, which impacts their expressiveness and applicability.\n- **Vocabulary and Knowledge Expansion Techniques**: \n  - Earlier studies proposed various methods to improve LRL performance, such as expanding word vocabulary (Wang et al., 2019; Schuster et al., 2019), aligning multilingual embeddings (Artetxe et al., 2017, 2018), and reinforcing LLMs with minimal data.\n  - The LIMA study by Zhou et al. (2023) proposed maximizing LLM utility using high-quality instruction data.\n\nBy building upon these prior works, this study aims to investigate whether MLLMs can truly improve their language-specific capabilities using strategies focused on vocabulary expansion, knowledge enrichment, and usability enhancement, particularly for the Korean language.",
        "methodology": "In the methodology section titled \"Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean,\" the paper introduces two main approaches for enhancing the capabilities of the Llama2 model in handling the Korean language. These approaches are (1) vocabulary expansion and (2) knowledge enrichment.\n\n### Vocabulary Expansion\nThe first approach involves expanding the Korean vocabulary in Llama2. This methodology seeks to include more Korean tokens to help the language model better understand and generate Korean text. The list provided, including elements such as \u2018_\u2019, \u2018<0xED>\u2019, \u2018<0x96>\u2019, \u2018<0x84>\u2019, etc., indicates the inclusion of specific bytes and characters commonly used within the Korean language and other unseen tokens relevant to its syntax and structure. By incorporating these tokens, the model's ability to handle and process Korean-specific text is greatly improved.\n\n### Knowledge Enrichment\nThe second approach focuses on knowledge enrichment. This is done by reinforcing the relationship between Korean and English languages using CLM (Causal Language Modeling)-based pretraining. This method involves training the language model on a large corpus of bilingual text, enhancing the model's knowledge with a wider and more robust understanding of contextual and cultural nuances between the two languages. This cross-linguistic enrichment ensures that the model does not just translate text but understands the semantic and pragmatic aspects of Korean in its context.\n\nBy implementing these two strategies, the study aims to significantly improve the performance of Llama2 when it comes to processing Korean language data, making it more efficient and accurate in handling multilingual capabilities.\n\n### Key Components:\n1. **Vocabulary Expansion**: Involving the inclusion of specific Korean tokens as shown in the table with elements such as (\u2018_\u2019, \u2018\uacf5\u2019, \u2018\ub8e1\u2019, etc.)\n2. **Knowledge Enrichment**: Using CLM-based pretraining to enhance the understanding of bilingual (Korean-English) knowledge.\n\n### Innovations:\n- **Custom Token Integration**: Uniquely tailored Korean tokens, combined with byte-level tokens, are integrated into the model\u2019s vocabulary system.\n- **Bilingual Pretraining**: Utilization of CLM-based pretraining to build deeper semantic connections between Korean and English languages, beyond mere translation capabilities.\n\nThese combined methods represent a significant step in optimizing multilingual large language models, particularly in efficiently handling Korean.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiments Overview\nThe main experiments aim to explore three core research objectives related to optimizing language augmentation for multilingual large language models, specifically for Korean. The experiments include:\n\n1. **Comparison between the models with and without the expanded Korean vocabulary**: This examines the impact of vocabulary expansion on model performance.\n2. **Comparison between the model pretrained on the Korean-English bilingual training data and that trained only on the Korean data**: This evaluates the benefits of bilingual training data.\n3. **Performance variations owing to instruction-tuning using the LIMA dataset**: This assesses the effect of instruction tuning on model capabilities.\n\n#### Datasets\n1. **Korean Data**: Utilized for both monolingual training and as part of bilingual training setup.\n2. **Korean\u2013English Bilingual Training Data**: Used to train models to understand both languages.\n3. **LIMA Dataset**: Employed for instruction-tuning experiments to fine-tune model instructions across different tasks.\n\n#### Baselines\n1. **Model without expanded Korean vocabulary**: Used as a baseline to evaluate the advantages of vocabulary expansion.\n2. **Model trained only on Korean data**: Baseline to compare against the bilingual training setup.\n\n#### Evaluation Metrics\n1. **Performance across Various NLP Tasks**: Metrics such as BLEU score (for translation tasks), F1 score, and accuracy are used.\n2. **Multilingual Understanding**: Evaluation on tasks involving both Korean and English to assess bilingual proficiency.\n3. **Instruction-tuning Efficiency**: Performance improvement due to instruction tuning with LIMA dataset.\n\n#### Main Experimental Results\n- **Impact of Expanded Korean Vocabulary**: The model with expanded Korean vocabulary exhibited significant improvements in performance on Korean language tasks compared to the model without such an expansion.\n- **Bilingual vs. Monolingual Training**: The model pretrained on Korean-English bilingual data outperformed the model trained solely on Korean data, showcasing enhanced multilingual understanding and better cross-lingual transfer capabilities.\n- **Instruction-tuning with LIMA Dataset**: Models subjected to instruction tuning using the LIMA dataset showed marked improvements in executing language instructions across a variety of tasks, suggesting that instruction-tuning is a beneficial step in model optimization.\n\nThese results highlight the benefits of expanded vocabulary, bilingual training, and instruction tuning in optimizing multilingual large language models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare the performance of a model with and without the expanded Korean vocabulary and understand the influence of vocabulary expansion on the model's ability to comprehend and reason in Korean.",
            "experiment_process": "The experiment compared the performances of two versions of the Llama2 model: one with an expanded Korean vocabulary (Bllossom-ko) and one without (Llama2-ko). The models were evaluated across various tasks, such as NLI and STS, which infer the relationship between two sentences, and SBI which assesses single sentence understanding. Performance was measured using a quantitative comparison in Table 6.",
            "result_discussion": "The expanded vocabulary model, Bllossom-ko, outperformed the Llama2-ko by 1.8 points on average. Specifically, for NLI and STS tasks, Bllossom-ko outperformed by 9.15 points, indicating better comprehension and reasoning faculties. However, for the SBI task, Llama2-ko performed better by 8.8 points, suggesting that while vocabulary expansion improves overall comprehension and reasoning, it may not be beneficial for all types of tasks.",
            "ablation_id": "2403.10882v2.No1"
        },
        {
            "research_objective": "To investigate the influence of bilingual pretraining on model performance by comparing a model pretrained on Korean-English bilingual data with one trained only on Korean data.",
            "experiment_process": "This experiment compared the performances of two models: Bllossom-ko, which was pretrained on Korean data, and Bllossom-bi, which used Korean-English bilingual training data. The models were evaluated for bias and performance across various tasks, including SBI, STS, and HellaSwag. Quantitative scores from Table 6 were used for comparison.",
            "result_discussion": "Bllossom-ko and Bllossom-bi showed similar overall performance scores of 58.9 and 58.6, respectively. Bllossom-ko exhibited a bias issue, responding in Korean even when queried in English. For the SBI task, Bllossom-bi outperformed by 11.6 points, while it underperformed on STS and HellaSwag tasks by 11.2 and 10.6 points, respectively. Although the quantitative impact of bilingual pretraining was minimal, qualitative observations indicated significant performance differences, particularly in bias mitigation.",
            "ablation_id": "2403.10882v2.No2"
        },
        {
            "research_objective": "To evaluate the impact of Small-scale Fine-Tuning (SFT) using a high-quality Korean instruction dataset on the performance of multilingual LLMs.",
            "experiment_process": "This experiment assessed the effect of SFT on the Llama2 model using 1K Korean LIMA data. Two models were compared: Llama2 and Llama2-koSFT, which underwent SFT. Performances were quantitatively measured, and the models' output qualities were qualitatively evaluated based on human assessment and AI models like GPT.",
            "result_discussion": "Llama2, the backbone model, outperformed Llama2-koSFT by an average of 2.2 points. A significant performance reduction (~6 points) was observed for Komt, another extensively fine-tuned variant of Llama2. Models based on Polyglot-ko, such as KoAlpaca and Kullm, performed worse than the backbone. This suggests that SFT might not significantly enhance classification tasks' quantitative evaluations. However, Llama2-koSFT produced qualitatively better responses in terms of vocabulary use and completeness, suggesting a qualitative benefit from SFT.",
            "ablation_id": "2403.10882v2.No3"
        }
    ]
}