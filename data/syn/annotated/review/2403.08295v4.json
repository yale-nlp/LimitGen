{
    "title": "Gemma: Open Models Based on Gemini Research and Technology",
    "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "We present Gemma, a family of open models based on Google\u2019s Gemini models (Gemini Team, 2023  ###reference_b18###).\nWe trained Gemma models on up to 6T tokens of text, using architectures, data, and training recipes inspired by the Gemini model family. Like Gemini, these models achieve strong generalist capabilities in text domains, alongside state-of-the-art understanding and reasoning skills at scale. With this work, we release both pre-trained and fine-tuned checkpoints, as well as an open-source codebase for inference and serving.\nGemma comes in two sizes: a 7 billion parameter model for efficient deployment and development on GPU and TPU, and a 2 billion parameter model for CPU and on-device applications. Each size is designed to address different computational constraints, applications, and developer requirements. At each scale, we release raw, pretrained checkpoints, as well as checkpoints fine-tuned for dialogue, instruction-following, helpfulness, and safety. We thoroughly evaluate the shortcomings of our models on a suite of quantitative and qualitative benchmarks. We believe the release of both pretrained and fine-tuned checkpoints will enable thorough research and investigation into the impact of current instruction-tuning regimes, as well as the development of increasingly safe and responsible model development methodologies.\nGemma advances state-of-the-art performance relative to comparable-scale (and some larger), open models (Jiang et al., 2023  ###reference_b22###; Touvron et al., 2023b  ###reference_b47###, a  ###reference_b46###; Almazrouei et al., 2023  ###reference_b1###) across a wide range of domains including both automated benchmarks and human evaluation. Example domains include question answering (Clark et al., 2019  ###reference_b13###; Kwiatkowski et al., 2019  ###reference_b27###), commonsense reasoning (Sakaguchi et al., 2019  ###reference_b37###; Suzgun et al., 2022  ###reference_b44###), mathematics and science (Cobbe et al., 2021  ###reference_b15###; Hendrycks et al., 2020  ###reference_b19###), and coding (Austin et al., 2021  ###reference_b4###; Chen et al., 2021  ###reference_b10###). See complete details in the Evaluation  ###reference_### section.\n###figure_1### Like Gemini, Gemma builds on recent work on sequence models (Sutskever et al., 2014  ###reference_b43###) and transformers (Vaswani et al., 2017  ###reference_b48###), deep learning methods based on neural networks (LeCun et al., 2015  ###reference_b28###), and techniques for large-scale training on distributed systems (Barham et al., 2022  ###reference_b6###; Roberts et al., 2023  ###reference_b36###; Dean et al., 2012  ###reference_b16###). Gemma also builds on Google\u2019s long history of open models and ecosystems, including Word2Vec (Mikolov et al., 2013  ###reference_b29###), the Transformer (Vaswani et al., 2017  ###reference_b48###), BERT (Devlin et al., 2018  ###reference_b17###), and T5 (Raffel et al., 2019  ###reference_b34###) and T5X (Roberts et al., 2022  ###reference_b35###).\nWe believe the responsible release of LLMs is critical for improving the safety of frontier models, for ensuring equitable access to this breakthrough technology, for enabling rigorous evaluation and analysis of current techniques, and for enabling the development of the next wave of innovations. While thorough testing of all Gemma models has been conducted, testing cannot cover all applications and scenarios in which Gemma may be used. With this in mind, all Gemma users should conduct rigorous safety testing specific to their use case before deployment or use. More details on our approach to safety can be found in section Responsible Deployment  ###reference_###.\nIn this technical report, we provide a detailed overview of the model architecture, training infrastructure, and pretraining and fine-tuning recipes for Gemma, followed by thorough evaluations of all checkpoints across a wide-variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. We then discuss in detail our approach to safe and responsible deployment. Finally, we outline the broader implications of Gemma, its limitations and advantages."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Model Architecture",
            "text": "The Gemma model architecture is based on the transformer decoder (Vaswani et al., 2017  ###reference_b48###). The core parameters of the architecture are summarized in Table 1  ###reference_###. Models are trained on a context length of 8192 tokens.\nWe also utilize several improvements proposed after the original transformer paper, and list them below:\n###table_1### Multi-Query Attention (Shazeer, 2019  ###reference_b39###).\nNotably, the 7B model uses multi-head attention while the 2B checkpoints use multi-query attention (with ), based on ablations that showed that multi-query attention works well at small scales (Shazeer, 2019  ###reference_b39###).\nRoPE Embeddings (Su et al., 2021  ###reference_b42###). Rather than using absolute positional embeddings, we use rotary positional embeddings in each layer; we also share embeddings across our inputs and outputs to reduce model size.\nGeGLU Activations (Shazeer, 2020  ###reference_b40###). The standard ReLU non-linearity is replaced by the approximated version of the GeGLU activation function.\nRMSNorm. We normalize the input of each transformer sub-layer, the attention layer and the feedforward layer, with RMSNorm (Zhang and Sennrich, 2019  ###reference_b53###) to stabilize the training."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Training Infrastructure",
            "text": "We train the Gemma models using TPUv5e; TPUv5e are deployed in pods of 256 chips, configured into a 2D torus of 16 x 16 chips. For the 7B model, we train our model across 16 pods, totaling to 4096 TPUv5e.\nWe pretrain the 2B model across 2 pods, totaling 512 TPUv5e.\nWithin a pod, we use 16-way model sharding and 16-way data replication for the 7B model.\nFor the 2B, we simply use 256-way data replication.\nThe optimizer state is further sharded using techniques similar to ZeRO-3.\nBeyond a pod, we perform data-replica reduce over the data-center network, using Pathways approach of (Barham et al., 2022  ###reference_b6###).\n###table_2### We follow Gemini and we leverage the \u2019single controller\u2019 programming paradigm of Jax (Roberts et al., 2023  ###reference_b36###) and Pathways (Barham et al., 2022  ###reference_b6###). This simplifies the development process by enabling a single Python process to orchestrate the entire training run; we also leverage the GSPMD partitioner (Xu et al., 2021  ###reference_b52###) for the training step computation and the MegaScale XLA compiler (XLA, 2019  ###reference_b51###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Carbon Footprint",
            "text": "We estimate the carbon emissions from pretraining the Gemma models to be  . This value is calculated based on the hourly energy usage reported directly from our TPU datacenters; we also scale this value to account for the additional energy expended to create and maintain the data center, giving us the total energy usage for our training experiments. We convert total energy usage to carbon emissions by joining our hourly energy usage against hourly per-cell carbon emission data reported by our data centers.\nIn addition, Google data centers are carbon neutral, achieved through a combination of energy efficiency, renewable energy purchases, and carbon offsets. This carbon neutrality applies to our experiments and the machines running them."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Pretraining",
            "text": ""
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Filtering",
            "text": "We filter the pre-training dataset to reduce the risk of unwanted or unsafe utterances, and filter out certain personal information or other sensitive data. This includes both heuristics and model-based classifiers to remove harmful or low-quality content. Further, we filter all evaluation sets from our pre-training data mixture, run targeted contamination analyses to check against evaluation set leakage, and reduce the risk of recitation by minimizing proliferation of sensitive outputs.\nThe final data mixture was determined through a series of ablations on both the 2B and 7B models. Similar to the approach advocated in (Gemini Team, 2023  ###reference_b18###), we stage training to alter the corpus mixture throughout training to increase the weight of relevant, high-quality data towards the end of training."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Instruction Tuning",
            "text": "We finetune Gemma 2B and 7B with supervised fine-tuning (SFT) on a mix of text-only, English-only synthetic and human-generated prompt-response pairs and reinforcement learning from human feedback (RLHF) with the reward model trained on labelled English-only preference data and the policy based on a set of high-quality prompts. We find that both stages are important for improved performance on downstream automatic evaluations and human preference evaluations of model outputs."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Supervised Fine-Tuning",
            "text": "We selected our data mixtures for supervised fine-tuning based on LM-based side-by-side evaluations (Zheng et al., 2023  ###reference_b54###).\nGiven a set of held-out prompts, we generate responses from a test model, generate responses on the same prompts from a baseline model, shuffle these randomly, and ask a larger, high capability model to express a preference between two responses. Different prompt sets are constructed to highlight specific capabilities, such as instruction following, factuality, creativity, and safety. Our LM-based judges employ a number of known strategies, such as chain-of-thought prompting (Wei et al., 2022  ###reference_b49###), rubrics and constitutions (Bai et al., 2022  ###reference_b5###), to be aligned with human preferences."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Filtering",
            "text": "When using synthetic data, we run several stages of filtering over it, removing examples that show certain personal information, unsafe or toxic model outputs, mistaken self-identification data, or duplicated examples. Following Gemini, we find that including subsets of data that encourage better in-context attribution, hedging, and refusals to minimize hallucinations improves performance on factuality metrics, without degrading model performance on other metrics.\nThe final data mixtures and supervised fine-tuning recipe, which includes tuned hyperparameters, were chosen on the basis of improving helpfulness while minimizing model harms related to safety and hallucinations."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Formatting",
            "text": "Instruction tuned models are trained with a specific formatter that annotates all instruction tuning examples with extra information, both at training and inference time. It has two purposes: 1) indicating roles in a conversation, such as the User role, and 2) delineating turns in a conversation, especially in a multi-turn conversation. Special control tokens are reserved in the tokenizer for this purpose. While it is possible to get coherent generations without the formatter, it will be out-of-distribution for the model, and will very likely produce worse generations.\nThe relevant formatting control tokens are presented in Table 3  ###reference_###, with a dialogue example presented in Table 4  ###reference_###.\n###table_3### ###table_4###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Reinforcement Learning from Human Feedback",
            "text": "We further finetuned the supervised fine-tuned model using RLHF (Christiano et al., 2017  ###reference_b12###; Ouyang et al., 2022  ###reference_b31###). We collected pairs of preferences from human raters and trained a reward function under the Bradley-Terry model (Bradley and Terry, 1952  ###reference_b8###), similarly to Gemini. The policy was trained to optimize this reward function using a novel reinforcement learning algorithm. Similar to the SFT phase, and in order to tune hyperparameters and additionally mitigate reward hacking (Amodei et al., 2016  ###reference_b2###; Skalse et al., 2022  ###reference_b41###) we relied on a high capacity model as an automatic rater and computed side-by-side comparisons against baseline models."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We evaluate Gemma across a broad range of domains, using both automated benchmarks and human evaluation.\nPaLM 2 compared with PaLM by evaluating on a shared subset of their training corpora. However, there is even less overlap between the Gemma pretraining data with the PaLM models, and so using this same methodology, we observe much lower memorization rates (Figure 2  ###reference_### left). Instead, we find that estimating the \u201ctotal memorization\u201d across the entire pretraining dataset gives a more reliable estimate (Figure 2  ###reference_### right) where we now find the Gemma memorizes training data at a comparable rate to PaLM.\n###figure_2### Perhaps of higher importance is the possibility that personal data might be memorized. As part of making Gemma pre-trained models safe and reliable, we used automated techniques to filter out certain personal information and other sensitive data from training sets.\nTo identify possible occurrences of personal data, we use Google Cloud Sensitive Data Protection222Available at: https://cloud.google.com/sensitive-data-protection  ###reference_otection###. This tool outputs three severity levels based on many categories of personal data (e.g., names, emails, etc.). We classify the highest severity as \u201csensitive\u201d and the remaining two as simply \u201cpersonal\u201d. Then, we measure how many memorized outputs contain any sensitive or personal data. As shown in Figure 3  ###reference_###, we observe no cases of memorized sensitive data. We do find that the model memorizes some data we have classified as potentially \u201cpersonal\u201d according to the above, though often at a much lower rate. Further, it is important to note that these tools are known to have many false positives (because they only match patterns and do not consider the context), meaning that our results are likely overestimates of the amount of personal data identified.\n###figure_3### In Figure 4  ###reference_###, we observe that roughly 50% more data is approximately memorized (note the log scale) and that this is nearly consistent across each of the different subcategories over the dataset."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Human Preference Evaluations",
            "text": "In addition to running standard academic benchmarks on the finetuned models, we sent final release candidates to human evaluation studies to be compared against the Mistral v0.2 7B Instruct model (Jiang et al., 2023  ###reference_b22###).\nOn a held-out collection of around 1000 prompts oriented toward asking models to follow instructions across creative writing tasks, coding, and following instructions, Gemma 7B IT has a 61.2% positive win rate and Gemma 2B IT has a 45% win rate over Mistral v0.2 7B Instruct. On a held-out collection of around 400 prompts oriented towards testing basic safety protocols, Gemma 7B IT has a 63.5% win rate, while Gemma 2B IT has a 60.1% win rate. We report the corresponding numbers in Table 5  ###reference_###.\n###table_5###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Automated Benchmarks",
            "text": "###table_6### We measure Gemma models\u2019 performance on domains including physical reasoning (Bisk et al., 2019  ###reference_b7###), social reasoning (Sap et al., 2019  ###reference_b38###), question answering (Clark et al., 2019  ###reference_b13###; Kwiatkowski et al., 2019  ###reference_b27###), coding (Austin et al., 2021  ###reference_b4###; Chen et al., 2021  ###reference_b10###), mathematics (Cobbe et al., 2021  ###reference_b15###), commonsense reasoning (Sakaguchi et al., 2019  ###reference_b37###), language modeling (Paperno et al., 2016  ###reference_b33###), reading comprehension (Joshi et al., 2017  ###reference_b23###), and more.\n###table_7### For most automated benchmarks we use the same evaluation methodology as in Gemini. Specifically for those where we report performance compared with Mistral, we replicated methodology from the Mistral technical report as closely as possible. These specific benchmarks are: ARC (Clark et al., 2018  ###reference_b14###), CommonsenseQA (Talmor et al., 2019  ###reference_b45###), Big Bench Hard (Suzgun et al., 2022  ###reference_b44###), and AGI Eval (English-only) (Zhong et al., 2023  ###reference_b55###). Due to restrictive licensing, we were unable to run any evaluations on LLaMA-2 and cite only those metrics previously reported (Touvron et al., 2023b  ###reference_b47###).\nWe compare Gemma 2B and 7B models to several external open-source (OSS) LLMs across a series of academic benchmarks, reported in Table 6  ###reference_### and Table 7  ###reference_###.\nOn MMLU (Hendrycks et al., 2020  ###reference_b19###), Gemma 7B outperforms all OSS alternatives at the same or smaller scale; it also outperforms several larger models, including LLaMA2 13B. However, human expert performance is gauged at 89.8% by the benchmark authors; as Gemini Ultra is the first model to exceed this threshold, there is significant room for continued improvements to achieve Gemini and human-level performance.\nGemma models demonstrate particularly strong performance on mathematics and coding benchmarks. On mathematics tasks, which are often used to benchmark the general analytical capabilities of models, Gemma models outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021  ###reference_b15###) and the more difficult MATH (Hendrycks et al., 2021  ###reference_b20###) benchmark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021  ###reference_b10###). They even surpass the performance of the code-fine-tuned CodeLLaMA-7B models on MBPP (CodeLLaMA achieves a score of 41.4% where Gemma 7B achieves 44.4%).\n###table_8###"
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Memorization Evaluations",
            "text": "Recent work has shown that aligned models may be vulnerable to new adversarial attacks that can bypass alignment (Nasr et al., 2023  ###reference_b30###). These attacks can cause models to diverge, and sometimes regurgitate memorized training data in the process. We focus on discoverable memorization, which serves as a reasonable upper-bound on the memorization of a model (Nasr et al., 2023  ###reference_b30###) and has been the common definition used in several studies (Carlini et al., 2022  ###reference_b9###; Anil et al., 2023  ###reference_b3###; Kudugunta et al., 2023  ###reference_b26###).\n###figure_4### We test for memorization111Our use of \u201cmemorization\u201d relies on the definition of that term found at www.genlaw.org/glossary.html. of the Gemma pretrained models with the same methodology performed in Anil et al. (2023  ###reference_b3###). We sample 10,000 documents from each corpus and use the first 50 tokens as a prompt for the model. We focus mainly on exact memorization, where we classify texts as memorized if the subsequent 50 tokens generated by the model exactly match the ground truth continuation in the text. However, to better capture potential paraphrased memorizations, we include approximate memorization (Ippolito et al., 2022  ###reference_b21###) using an 10% edit distance threshold. In Figure 2  ###reference_###, we compare the results of our evaluation with the closest sized PaLM (Chowdhery et al., 2022  ###reference_b11###) and PaLM 2 models (Anil et al., 2023  ###reference_b3###).\nPaLM 2 compared with PaLM by evaluating on a shared subset of their training corpora. However, there is even less overlap between the Gemma pretraining data with the PaLM models, and so using this same methodology, we observe much lower memorization rates (Figure 2  ###reference_###  ###reference_### left). Instead, we find that estimating the \u201ctotal memorization\u201d across the entire pretraining dataset gives a more reliable estimate (Figure 2  ###reference_###  ###reference_### right) where we now find the Gemma memorizes training data at a comparable rate to PaLM.\n###figure_5### Perhaps of higher importance is the possibility that personal data might be memorized. As part of making Gemma pre-trained models safe and reliable, we used automated techniques to filter out certain personal information and other sensitive data from training sets.\nTo identify possible occurrences of personal data, we use Google Cloud Sensitive Data Protection222Available at: https://cloud.google.com/sensitive-data-protection  ###reference_otection###  ###reference_otection###. This tool outputs three severity levels based on many categories of personal data (e.g., names, emails, etc.). We classify the highest severity as \u201csensitive\u201d and the remaining two as simply \u201cpersonal\u201d. Then, we measure how many memorized outputs contain any sensitive or personal data. As shown in Figure 3  ###reference_###  ###reference_###, we observe no cases of memorized sensitive data. We do find that the model memorizes some data we have classified as potentially \u201cpersonal\u201d according to the above, though often at a much lower rate. Further, it is important to note that these tools are known to have many false positives (because they only match patterns and do not consider the context), meaning that our results are likely overestimates of the amount of personal data identified.\n###figure_6### In Figure 4  ###reference_###  ###reference_###, we observe that roughly 50% more data is approximately memorized (note the log scale) and that this is nearly consistent across each of the different subcategories over the dataset."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Responsible Deployment",
            "text": "In line with previous releases of Google\u2019s AI technologies (Gemini Team, 2023  ###reference_b18###; Kavukcuoglu et al., 2022  ###reference_b24###), we follow a structured approach to responsible development and deployment of our models, in order to identify, measure, and manage foreseeable downstream societal impacts. As with our recent Gemini release, these are informed by prior academic literature on language model risks (Weidinger et al., 2021  ###reference_b50###), findings from similar prior exercises conducted across the industry (Anil et al., 2023  ###reference_b3###), ongoing engagement with experts internally and externally, and unstructured attempts to discover new model vulnerabilities."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Benefits",
            "text": "We believe that openness in AI science and technology can bring significant benefits. Open-sourcing is a significant driver of science and innovation, and a responsible practice in most circumstances. But this needs to be balanced against the risk of providing actors with the tools to cause harm now or in the future.\nGoogle has long committed to providing broader access to successful research innovations (GraphCast, Transformer, BERT, T5, Word2Vec), and we believe that releasing Gemma into the AI development ecosystem will enable downstream developers to create a host of beneficial applications, in areas such as science, education and the arts. Our instruction-tuned offerings should encourage a range of developers to leverage Gemma\u2019s chat and code capabilities to support their own beneficial applications, while allowing for custom fine-tuning to specialize the model\u2019s capabilities for specific use cases. To ensure Gemma supports a wide range of developer needs, we are also releasing two model sizes to optimally support different environments, and have made these models available across a number of platforms (see Kaggle  ###reference_/frameworks/flax/variations/7b-it### for details). Providing broad access to Gemma in this way should reduce the economic and technical barriers that newer ventures or independent developers face when incorporating these technologies into their workstreams.\nAs well as serving developers with our instruction-tuned models, we have also provided access to corresponding base pretrained models. By doing so, it is our intention to encourage further AI safety research and community innovation, providing a wider pool of models available to developers to build on various methods of transparency and interpretability research that the community has already benefited from (Pacchiardi et al., 2023  ###reference_b32###; Zou et al., 2023  ###reference_b56###)."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Risks",
            "text": "In addition to bringing benefits to the AI development ecosystem, we are aware that malicious uses of LLMs, such as the creation of deepfake imagery, AI-generated disinformation, and illegal and disturbing material can cause harm on both an individual and institutional levels (Weidinger et al., 2021  ###reference_b50###). Providing access to model weights, rather than releasing models behind an API, also raises new challenges for responsible deployment.\nFirst, we cannot prevent bad actors from fine tuning Gemma for malicious intent, despite their use being subject to Terms of Use that prohibit the use of Gemma models in ways that contravene our Gemma Prohibited Use Policy. However, we are cognizant that further work is required to build more robust mitigation strategies against intentional misuse of open models, which Google DeepMind will continue to explore both internally and in collaboration with the AI community.\nThe second challenge we face is protecting developers and downstream users against the unintended behaviours of open models, including generation of toxic language or perpetuation of discriminatory social harms, model hallucinations and leakage of personally identifiable information. When deploying models behind an API, these risks can be reduced via various filtering methods."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Mitigations",
            "text": "Without this layer of defense for the Gemma family of models, we have endeavoured to safeguard against these risks by filtering and measuring biases in pre-training data in line with the Gemini approach, assessing safety through standardized AI safety benchmarks, internal red teaming to better understand the risks associated with external use of Gemma, and subjecting the models to rigorous ethics and safety evaluations, the results of which can be seen in 8  ###reference_###.\nWhile we\u2019ve invested significantly in improving the model, we recognize its limitations. To ensure transparency for downstream users, we\u2019ve published a detailed model card  ###reference_d### to provide researchers with a more comprehensive understanding of Gemma.\nWe have also released a Generative AI Responsible Toolkit to support developers to build AI responsibly. This encompasses a series of assets to help developers design and implement responsible AI best practices and keep their users safe.\nThe relative novelty of releasing open weights models means new uses, and misuses, of these models are still being discovered, which is why Google DeepMind is committed to the continuous research and development of robust mitigation strategies alongside future model development."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Assessment",
            "text": "Ultimately, given the capabilities of larger systems accessible within the existing ecosystem, we believe the release of Gemma will have a negligible effect on the overall AI risk portfolio. In light of this, and given the utility of these models for research, auditing and downstream product development, we are confident that the benefit of Gemma to the AI community outweighs the risks described."
        },
        {
            "section_id": "7.5",
            "parent_section_id": "7",
            "section_name": "Going Forward",
            "text": "As a guiding principle, Google DeepMind strives to adopt assessments and safety mitigations proportionate to the potential risks from our models. Although we are confident that Gemma models will provide a net benefit to the community, our emphasis on safety stems from the irreversible nature of this release. As the harms resulting from open models are not yet well defined, nor does an established evaluation framework for such models exist, we will continue to follow this precedent and take a measured and cautionary approach to open model development. As capabilities advance, we may explore extended testing, staggered releases or alternative access mechanisms to ensure responsible AI development.\nAs the ecosystem evolves, we urge the wider AI community to move beyond simplistic \u2019open vs. closed\u2019 debates, and avoid either exaggerating or minimising potential harms, as we believe a nuanced, collaborative approach to risks and benefits is essential. At Google DeepMind we\u2019re committed to developing high-quality evaluations and invite the community to join us in this effort for a deeper understanding of AI systems."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Discussion and Conclusion",
            "text": "We present Gemma, an openly available family of generative language models for text and code. Gemma advances the state of the art of openly available language model performance, safety, and responsible development.\nIn particular, we are confident that Gemma models will provide a net benefit to the community given our extensive safety evaluations and mitigations; however, we acknowledge that this release is irreversible and the harms resulting from open models are not yet well defined, so we continue to adopt assessments and safety mitigations proportionate to the potential risks of these models. In addition, our models outperform competitors on 6 standard safety benchmarks, and in human side-by-side evaluations.\nGemma models improve performance on a broad range of domains including dialogue, reasoning, mathematics, and code generation. Results on MMLU (64.3%) and MBPP (44.4%) demonstrate both the high performance of Gemma, as well as the continued headroom in openly available LLM performance.\nBeyond state-of-the-art performance measures on benchmark tasks, we are excited to see what new use-cases arise from the community, and what new capabilities emerge as we advance the field together. We hope that researchers use Gemma to accelerate a broad array of research, and that developers create beneficial new applications, user experiences, and other functionality.\nGemma benefits from many learnings of the Gemini model program including code, data, architecture, instruction tuning, reinforcement learning from human feedback, and evaluations. As discussed in the Gemini technical report, we reiterate a non-exhaustive set of limitations to the use of LLMs. Even with great performance on benchmark tasks, further research is needed to create robust, safe models that reliably perform as intended. Example further research areas include factuality, alignment, complex reasoning, and robustness to adversarial input. As discussed by Gemini, we note the need for more challenging and robust benchmarks."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Contributions and Acknowledgments",
            "text": "Core Contributors\nThomas Mesnard \nCassidy Hardin \nRobert Dadashi \nSurya Bhupatiraju \nShreya Pathak \nLaurent Sifre \nMorgane Rivi\u00e8re \nMihir Sanjay Kale \nJuliette Love \nPouya Tafti \nL\u00e9onard Hussenot \nPier Giuseppe Sessa\nContributors\nAakanksha Chowdhery \nAdam Roberts \nAditya Barua \nAlex Botev \nAlex Castro-Ros \nAmbrose Slone \nAm\u00e9lie H\u00e9liou \nAndrea Tacchetti \nAnna Bulanova \nAntonia Paterson \nBeth Tsai \nBobak Shahriari \nCharline Le Lan \nChristopher A. Choquette-Choo \nCl\u00e9ment Crepy \nDaniel Cer \nDaphne Ippolito \nDavid Reid \nElena Buchatskaya \nEric Ni \nEric Noland \nGeng Yan \nGeorge Tucker \nGeorge-Christian Muraru \nGrigory Rozhdestvenskiy \nHenryk Michalewski \nIan Tenney \nIvan Grishchenko \nJacob Austin \nJames Keeling \nJane Labanowski \nJean-Baptiste Lespiau \nJeff Stanway \nJenny Brennan \nJeremy Chen \nJohan Ferret \nJustin Chiu \nJustin Mao-Jones \nKatherine Lee \nKathy Yu \nKatie Millican \nLars Lowe Sjoesund \nLisa Lee \nLucas Dixon \nMachel Reid \nMaciej Miku\u0142a \nMateo Wirth \nMichael Sharman \nNikolai Chinaev \nNithum Thain \nOlivier Bachem \nOscar Chang \nOscar Wahltinez \nPaige Bailey \nPaul Michel \nPetko Yotov \nRahma Chaabouni \nRamona Comanescu \nReena Jana \nRohan Anil \nRoss McIlroy \nRuibo Liu \nRyan Mullins \nSamuel L Smith \nSebastian Borgeaud \nSertan Girgin \nSholto Douglas \nShree Pandya \nSiamak Shakeri \nSoham De \nTed Klimenko \nTom Hennigan \nVlad Feinberg \nWojciech Stokowiec \nYu-hui Chen \nZafarali Ahmed \nZhitao Gong\nProduct Management\nTris Warkentin \nLudovic Peran\nProgram Management\nMinh Giang\nExecutive Sponsors\nCl\u00e9ment Farabet \nOriol Vinyals \nJeff Dean \nKoray Kavukcuoglu \nDemis Hassabis \nZoubin Ghahramani \nDouglas Eck \nJoelle Barral \nFernando Pereira \nEli Collins\nLeads\nArmand Joulin \nNoah Fiedel \nEvan Senter\nTech Leads\nAlek Andreev\nKathleen Kenealy\n\u2020\u2020 equal contribution.\nAcknowledgements\nOur work is made possible by the dedication and efforts of numerous teams at Google. We\nwould like to acknowledge the support from the following teams: Gemini, Gemini Safety, Gemini Infrastructure, Gemini Evaluation, Google Cloud, Google Research Responsible AI, Kaggle, and Keras.\nSpecial thanks and acknowledgment to Adrian Hutter, Andreas Terzis, Andrei Kulik, Angelos Filos, Anushan Fernando, Aurelien Boffy, Danila Sinopalnikov, Edouard Leurent, Gabriela Surita, Geoffrey Cideron, Jilin Chen, Karthik Raveendran, Kathy Meier-Hellstern, Kehang Han, Kevin Robinson, Kritika Muralidharan, Le Hou, Leonard Berrada, Lev Proleev, Luheng He, Marie Pellat, Mark Sherwood, Matt Hoffman, Matthias Grundmann, Nicola De Cao, Nikola Momchev, Nino Vieillard, Noah Constant, Peter Liu, Piotr Stanczyk, Qiao Zhang, Ruba Haroun, Seliem El-Sayed, Siddhartha Brahma, Tianhe (Kevin) Yu, Tom Le Paine, Yingjie Miao, Yuanzhong Xu, and Yuting Sun."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Gemma 1.0 IT results",
            "text": "The core of the paper presents the results of the Gemma 1.1 IT models. We kept the results of the previous Gemma 1.0 IT models for comparison in this appendix. Side-by-side evaluations of Gemma 1.0 IT against Mistral 7b v0.2 can be found in table 9  ###reference_###. Safety academic benchmark results of version 1.0 can be found in table 10  ###reference_###.\n###table_9### ###table_10###"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T1.1\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S2.T1.1.1.1\">Parameters</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S2.T1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.2.1\">2B</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S2.T1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.3.1\">7B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.2.1\">\n<span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.1.2.1.1\">d</span>_model</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.2.2\">2048</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.2.3\">3072</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.3.1\">Layers</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.3.2\">18</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.3.3\">28</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.4.1\">Feedforward hidden dims</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.4.2\">32768</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.4.3\">49152</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.5.1\">Num heads</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.5.2\">8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.5.3\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.6.1\">Num KV heads</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.6.2\">1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.6.3\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.7.1\">Head size</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.7.2\">256</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.7.3\">256</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S2.T1.1.8.1\">Vocab size</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S2.T1.1.8.2\">256128</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S2.T1.1.8.3\">256128</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Key model parameters.</figcaption>\n</figure>",
            "capture": "Table 1: Key model parameters."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.1.1.2\">\n<span class=\"ltx_text\" id=\"S3.T2.1.1.2.1\"></span> <span class=\"ltx_text\" id=\"S3.T2.1.1.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1.1.2.2.1\">\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.2.2.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.1.1.2.2.1.1.1\">Embedding</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.2.2.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.1.1.2.2.1.2.1\">Parameters</span></span>\n</span></span><span class=\"ltx_text\" id=\"S3.T2.1.1.2.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.1.1.3\">\n<span class=\"ltx_text\" id=\"S3.T2.1.1.3.1\"></span> <span class=\"ltx_text\" id=\"S3.T2.1.1.3.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.1.1.3.2.1\">\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.3.2.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.1.1.3.2.1.1.1\">Non-embedding</span></span>\n<span class=\"ltx_tr\" id=\"S3.T2.1.1.3.2.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.1.1.3.2.1.2.1\">Parameters</span></span>\n</span></span><span class=\"ltx_text\" id=\"S3.T2.1.1.3.3\"></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.2.1.1\">2B</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T2.1.2.2\">524,550,144</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T2.1.2.3\">1,981,884,416</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.3.1.1\">7B</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T2.1.3.2\">786,825,216</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T2.1.3.3\">7,751,248,896</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Parameter counts for the Gemma models.\nWe inherit from the large Gemini vocabulary (256k entries), that is designed to work on large quantities of languages, hence, the larger embedding parameter counts compared to models that are limited to one or a few languages.</figcaption>\n</figure>",
            "capture": "Table 2: Parameter counts for the Gemma models.\nWe inherit from the large Gemini vocabulary (256k entries), that is designed to work on large quantities of languages, hence, the larger embedding parameter counts compared to models that are limited to one or a few languages."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T3.1\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.1\" style=\"font-size:80%;\">Context</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.2.1\" style=\"font-size:80%;\">Relevant Token</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.2.1\"><span class=\"ltx_text\" id=\"S5.T3.1.2.1.1\" style=\"font-size:70%;\">User turn</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T3.1.2.2.1\" style=\"font-size:80%;color:#0F75FF;\">user</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.3.1\"><span class=\"ltx_text\" id=\"S5.T3.1.3.1.1\" style=\"font-size:70%;\">Model turn</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T3.1.3.2.1\" style=\"font-size:80%;color:#0F75FF;\">model</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.4.1\"><span class=\"ltx_text\" id=\"S5.T3.1.4.1.1\" style=\"font-size:70%;\">Start of conversation turn</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.4.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T3.1.4.2.1\" style=\"font-size:80%;color:#0F75FF;\">&lt;start_of_turn&gt;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T3.1.5.1\"><span class=\"ltx_text\" id=\"S5.T3.1.5.1.1\" style=\"font-size:70%;\">End of conversation turn</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.1.5.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T3.1.5.2.1\" style=\"font-size:80%;color:#0F75FF;\">&lt;end_of_turn&gt;</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Relevant formatting control tokens used for both SFT and RLHF of Gemma models.</figcaption>\n</figure>",
            "capture": "Table 3: Relevant formatting control tokens used for both SFT and RLHF of Gemma models."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T4.1\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1\">\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S5.T4.1.1.1\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.1.1\" style=\"font-size:80%;\">\n</span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.2\" style=\"font-size:80%;\">User:</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T4.1.1.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.1.2.1\" style=\"font-size:80%;color:#0F75FF;\">&lt;start_of_turn&gt;user</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.2\">\n<td class=\"ltx_td\" id=\"S5.T4.1.2.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.2.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.2.2.1\" style=\"font-size:80%;\">Knock knock.<span class=\"ltx_text\" id=\"S5.T4.1.2.2.1.1\" style=\"color:#0F75FF;\">&lt;end_of_turn&gt;</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.3\">\n<td class=\"ltx_td\" id=\"S5.T4.1.3.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.3.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.3.2.1\" style=\"font-size:80%;color:#0F75FF;\">&lt;start_of_turn&gt;model</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.4\">\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.4.1.1\" style=\"font-size:80%;\">Model:</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.4.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.4.2.1\" style=\"font-size:80%;\">Who\u2019s there?<span class=\"ltx_text\" id=\"S5.T4.1.4.2.1.1\" style=\"color:#0F75FF;\">&lt;end_of_turn&gt;</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.5\">\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T4.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.5.1.1\" style=\"font-size:80%;\">User:</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.5.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.5.2.1\" style=\"font-size:80%;color:#0F75FF;\">&lt;start_of_turn&gt;user</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.6\">\n<td class=\"ltx_td\" id=\"S5.T4.1.6.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.6.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.6.2.1\" style=\"font-size:80%;\">Gemma.<span class=\"ltx_text\" id=\"S5.T4.1.6.2.1.1\" style=\"color:#0F75FF;\">&lt;end_of_turn&gt;</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.7\">\n<td class=\"ltx_td\" id=\"S5.T4.1.7.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.7.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.7.2.1\" style=\"font-size:80%;color:#0F75FF;\">&lt;start_of_turn&gt;model</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.8\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T4.1.8.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.8.1.1\" style=\"font-size:80%;\">Model:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T4.1.8.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T4.1.8.2.1\" style=\"font-size:80%;\">Gemma who?<span class=\"ltx_text\" id=\"S5.T4.1.8.2.1.1\" style=\"color:#0F75FF;\">&lt;end_of_turn&gt;</span></span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Example dialogue with user and model control tokens.</figcaption>\n</figure>",
            "capture": "Table 4: Example dialogue with user and model control tokens."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S6.T5.1\">\n<tr class=\"ltx_tr\" id=\"S6.T5.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S6.T5.1.1.1\"><span class=\"ltx_text\" id=\"S6.T5.1.1.1.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S6.T5.1.1.2\"><span class=\"ltx_text\" id=\"S6.T5.1.1.2.1\" style=\"font-size:80%;\">Safety</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S6.T5.1.1.3\"><span class=\"ltx_text\" id=\"S6.T5.1.1.3.1\" style=\"font-size:80%;\">Instr. Following</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.2.1.1\" style=\"font-size:80%;\">Gemma 1.1 IT 7B</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S6.T5.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.2.2.1\" style=\"font-size:80%;\">63.5%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S6.T5.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.2.3.1\" style=\"font-size:80%;\">61.2%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T5.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S6.T5.1.3.1.1\" style=\"font-size:50%;\">95% Conf. Interval</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T5.1.3.2\"><span class=\"ltx_text\" id=\"S6.T5.1.3.2.1\" style=\"font-size:50%;\">[60.7%, 66.1%]</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T5.1.3.3\"><span class=\"ltx_text\" id=\"S6.T5.1.3.3.1\" style=\"font-size:50%;\">[59.3%, 63%]</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T5.1.4.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S6.T5.1.4.1.1\" style=\"font-size:50%;\">Win / Tie / Loss</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T5.1.4.2\"><span class=\"ltx_text\" id=\"S6.T5.1.4.2.1\" style=\"font-size:50%;\">51.5% / 23.9% / 24.6%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T5.1.4.3\"><span class=\"ltx_text\" id=\"S6.T5.1.4.3.1\" style=\"font-size:50%;\">52.2% / 18.1% / 29.8%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T5.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.5.1.1\" style=\"font-size:80%;\">Gemma 1.1 IT 2B</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T5.1.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T5.1.5.2.1\" style=\"font-size:80%;\">60.1%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T5.1.5.3\"><span class=\"ltx_text\" id=\"S6.T5.1.5.3.1\" style=\"font-size:80%;\">45%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T5.1.6.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S6.T5.1.6.1.1\" style=\"font-size:50%;\">95% Conf. Interval</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T5.1.6.2\"><span class=\"ltx_text\" id=\"S6.T5.1.6.2.1\" style=\"font-size:50%;\">[57.3%, 62.8%]</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S6.T5.1.6.3\"><span class=\"ltx_text\" id=\"S6.T5.1.6.3.1\" style=\"font-size:50%;\">[43.1%, 46.9%]</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S6.T5.1.7.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S6.T5.1.7.1.1\" style=\"font-size:50%;\">Win / Tie / Loss</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S6.T5.1.7.2\"><span class=\"ltx_text\" id=\"S6.T5.1.7.2.1\" style=\"font-size:50%;\">48.5% / 23.2% / 28.3%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S6.T5.1.7.3\"><span class=\"ltx_text\" id=\"S6.T5.1.7.3.1\" style=\"font-size:50%;\">37.1% / 15.8% / 47.1%</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Win rate of Gemma 1.1 IT models versus Mistral 7B v0.2 Instruct with 95% confidence intervals. We report breakdowns of wins, ties, and losses, and we break ties evenly when reporting the final win rate. Gemma 1.0 results can be found in the appendix.</figcaption>\n</figure>",
            "capture": "Table 5: Win rate of Gemma 1.1 IT models versus Mistral 7B v0.2 Instruct with 95% confidence intervals. We report breakdowns of wins, ties, and losses, and we break ties evenly when reporting the final win rate. Gemma 1.0 results can be found in the appendix."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S6.T6.15\">\n<tr class=\"ltx_tr\" id=\"S6.T6.15.16\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S6.T6.15.16.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S6.T6.15.16.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S6.T6.15.16.3\">LLaMA-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T6.15.16.4\">Mistral</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S6.T6.15.16.5\">Gemma</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.17\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.17.1\">Benchmark</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.17.2\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0metric</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.17.3\">7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.17.4\">13B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.17.5\">7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.17.6\">2B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.17.7\">7B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T6.15.18.1\">MMLU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.18.2\">5-shot, top-1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.18.3\">45.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.18.4\">54.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.18.5\">62.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.18.6\">42.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.18.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.18.7.1\">64.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.19\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T6.15.19.1\">HellaSwag</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.19.2\">0-shot</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.19.3\">77.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.19.4\">80.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.19.5\">81.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.19.6\">71.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.19.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.19.7.1\">81.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.20\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.20.1\">PIQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.20.2\">0-shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.20.3\">78.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.20.4\">80.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.20.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.20.5.1\">82.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.20.6\">77.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.20.7\">81.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.2.2.3\">SIQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.2.4\">0-shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.2.5\">48.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.2.6\">50.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.2.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S6.T6.1.1.1.1\"><span style=\"visibility:hidden\"><sup class=\"ltx_sup\" id=\"S6.T6.1.1.1.1.1\">\u2217</sup></span></span>47.0<sup class=\"ltx_sup\" id=\"S6.T6.2.2.2.2\">\u2217</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.2.7\">49.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.2.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.2.2.8.1\">51.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.4.4.3\">Boolq</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.4.4.4\">0-shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.4.4.5\">77.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.4.4.6\">81.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.4.4.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S6.T6.3.3.1.1\"><span style=\"visibility:hidden\"><sup class=\"ltx_sup\" id=\"S6.T6.3.3.1.1.1\">\u2217</sup></span></span><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.4.4.2.2\">83.2<sup class=\"ltx_sup\" id=\"S6.T6.4.4.2.2.1\"><span class=\"ltx_text ltx_font_medium\" id=\"S6.T6.4.4.2.2.1.1\">\u2217</span></sup></span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.4.4.7\">69.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.4.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.4.4.8.1\">83.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.21\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.21.1\">Winogrande</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.21.2\">partial scoring</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.21.3\">69.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.21.4\">72.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.21.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.21.5.1\">74.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.21.6\">65.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.21.7\">72.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.6.6.3\">CQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.6.6.4\">7-shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.6.6.5\">57.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.6.6.6\">67.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.6.6.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S6.T6.5.5.1.1\"><span style=\"visibility:hidden\"><sup class=\"ltx_sup\" id=\"S6.T6.5.5.1.1.1\">\u2217</sup></span></span>66.3<sup class=\"ltx_sup\" id=\"S6.T6.6.6.2.2\">\u2217</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.6.6.7\">65.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.6.6.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.6.6.8.1\">71.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.22\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.22.1\">OBQA</td>\n<td class=\"ltx_td\" id=\"S6.T6.15.22.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.22.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.22.3.1\">58.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.22.4\">57.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.22.5\">52.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.22.6\">47.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.22.7\">52.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.23\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.23.1\">ARC-e</td>\n<td class=\"ltx_td\" id=\"S6.T6.15.23.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.23.3\">75.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.23.4\">77.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.23.5\">80.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.23.6\">73.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.23.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.23.7.1\">81.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.24\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.24.1\">ARC-c</td>\n<td class=\"ltx_td\" id=\"S6.T6.15.24.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.24.3\">45.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.24.4\">49.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.24.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.24.5.1\">54.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.24.6\">42.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.24.7\">53.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.25\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T6.15.25.1\">TriviaQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.25.2\">5-shot</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.25.3\">72.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.25.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.25.4.1\">79.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.25.5\">62.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.25.6\">53.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.25.7\">63.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.26\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.26.1\">NQ</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.26.2\">5-shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.26.3\">25.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.26.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.26.4.1\">31.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.26.5\">23.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.26.6\">12.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.26.7\">23.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.27\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T6.15.27.1\">HumanEval</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.27.2\">pass@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.27.3\">12.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.27.4\">18.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.27.5\">26.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.27.6\">22.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.15.27.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.27.7.1\">32.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.9.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.7.7.1\">MBPP<sup class=\"ltx_sup\" id=\"S6.T6.7.7.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S6.T6.7.7.1.1.1\">\u2020</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.9.9.4\">3-shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.9.9.5\">20.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.9.9.6\">30.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.9.9.3\">\n<span class=\"ltx_text ltx_phantom\" id=\"S6.T6.8.8.2.1\"><span style=\"visibility:hidden\"><sup class=\"ltx_sup\" id=\"S6.T6.8.8.2.1.1\">\u2217</sup></span></span>40.2<sup class=\"ltx_sup\" id=\"S6.T6.9.9.3.2\">\u2217</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.9.9.7\">29.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.9.9.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.9.9.8.1\">44.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.11.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.11.11.3\">GSM8K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.11.11.4\">maj@1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.11.11.5\">14.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.11.11.6\">28.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.11.11.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S6.T6.10.10.1.1\"><span style=\"visibility:hidden\"><sup class=\"ltx_sup\" id=\"S6.T6.10.10.1.1.1\">\u2217</sup></span></span>35.4<sup class=\"ltx_sup\" id=\"S6.T6.11.11.2.2\">\u2217</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.11.11.7\">17.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.11.11.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.11.11.8.1\">46.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.28\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.28.1\">MATH</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.28.2\">4-shot</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.28.3\">2.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.28.4\">3.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.28.5\">12.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.28.6\">11.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.28.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.28.7.1\">24.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.13.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T6.13.13.3\">AGIEval</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S6.T6.13.13.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.13.13.5\">29.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.13.13.6\">39.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.13.13.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S6.T6.12.12.1.1\"><span style=\"visibility:hidden\"><sup class=\"ltx_sup\" id=\"S6.T6.12.12.1.1.1\">\u2217</sup></span></span>41.2<sup class=\"ltx_sup\" id=\"S6.T6.13.13.2.2\">\u2217</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.13.13.7\">24.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.13.13.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.13.13.8.1\">41.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.15.15.3\">BBH</td>\n<td class=\"ltx_td\" id=\"S6.T6.15.15.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.15.5\">32.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.15.6\">39.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.15.2\">\n<span class=\"ltx_text ltx_phantom\" id=\"S6.T6.14.14.1.1\"><span style=\"visibility:hidden\"><sup class=\"ltx_sup\" id=\"S6.T6.14.14.1.1.1\">\u2217</sup></span></span><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.15.2.2\">56.1<sup class=\"ltx_sup\" id=\"S6.T6.15.15.2.2.1\"><span class=\"ltx_text ltx_font_medium\" id=\"S6.T6.15.15.2.2.1.1\">\u2217</span></sup></span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.15.7\">35.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.15.15.8\">55.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.15.29\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S6.T6.15.29.1\">Average</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\" id=\"S6.T6.15.29.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T6.15.29.3\">46.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T6.15.29.4\">52.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T6.15.29.5\">54.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T6.15.29.6\">45.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T6.15.29.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.15.29.7.1\">56.9</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Academic benchmark results, compared to similarly sized, openly-available models trained on general English text data.\n<sup class=\"ltx_sup\" id=\"S6.T6.22.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S6.T6.22.1.1\">\u2020</span></sup> Mistral reports 50.2 on a different split for MBPP and on their split our 7B model achieves 54.5.\n<sup class=\"ltx_sup\" id=\"S6.T6.23.2\">\u2217</sup> evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in <cite class=\"ltx_cite ltx_citemacro_cite\">Touvron et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.08295v4#bib.bib47\" title=\"\">2023b</a>)</cite>.\n</figcaption>\n</figure>",
            "capture": "Table 6: Academic benchmark results, compared to similarly sized, openly-available models trained on general English text data.\n\u2020 Mistral reports 50.2 on a different split for MBPP and on their split our 7B model achieves 54.5.\n\u2217 evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et\u00a0al. (2023b).\n"
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S6.T7.1\">\n<tr class=\"ltx_tr\" id=\"S6.T7.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S6.T7.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T7.1.1.2\">Mistral</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T7.1.1.3\">Gemma</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.1.2.1\">Benchmark</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.2.2\">7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.2.3\">7B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T7.1.3.1\">ARC-c</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T7.1.3.2\">60.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T7.1.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.3.3.1\">61.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.1.4.1\">HellaSwag</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.4.2.1\">83.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.4.3\">82.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.1.5.1\">MMLU</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.5.2\">64.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.5.3.1\">64.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.1.6.1\">TruthfulQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.6.2\">42.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.6.3.1\">44.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.1.7.1\">Winogrande</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.7.2\">78.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.7.3.1\">79.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.1.8.1\">GSM8K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.8.2\">37.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.8.3.1\">50.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S6.T7.1.9.1\">Average</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T7.1.9.2\">61.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T7.1.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.9.3.1\">63.8</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>HuggingFace H6 benchmark.\nThe performance of small models are sensitive to small modifications in prompts and we further validate the quality of our models on an independent implementation of multiple known benchmarks. All evaluations were run by HuggingFace.</figcaption>\n</figure>",
            "capture": "Table 7: HuggingFace H6 benchmark.\nThe performance of small models are sensitive to small modifications in prompts and we further validate the quality of our models on an independent implementation of multiple known benchmarks. All evaluations were run by HuggingFace."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S6.T8.1\">\n<tr class=\"ltx_tr\" id=\"S6.T8.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S6.T8.1.1.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S6.T8.1.1.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T8.1.1.3\">Mistral v0.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S6.T8.1.1.4\">Gemma 1.1 IT</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.2.1\">Benchmark</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.2.2\">metric</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.2.3\">7B*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.2.4\">2B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.2.5\">7B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T8.1.3.1\">RealToxicity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.3.2\">avg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.3.3\">8.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.3.4.1\">7.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.3.5\">8.04</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.4.1\">BOLD</td>\n<td class=\"ltx_td\" id=\"S6.T8.1.4.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.4.3\">46.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.4.4.1\">47.76</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.4.5\">45.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.5.1\">CrowS-Pairs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.5.2\">top-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.5.3\">32.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.5.4\">45.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.5.5.1\">49.67</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.6.1\">BBQ Ambig</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.6.2\">1-shot, top-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.6.3.1\">97.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.6.4\">58.97</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.6.5\">86.06</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.7.1\">BBQ Disambig</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.7.2\">top-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.7.3\">84.45</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.7.4\">53.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.7.5.1\">85.08</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.8.1\">Winogender</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.8.2\">top-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.8.3.1\">64.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.8.4\">50.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.8.5\">57.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.9.1\">TruthfulQA</td>\n<td class=\"ltx_td\" id=\"S6.T8.1.9.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.9.3.1\">48.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.9.4\">44.24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.9.5\">45.34</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.10.1\">Winobias 1_2</td>\n<td class=\"ltx_td\" id=\"S6.T8.1.10.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.10.3.1\">65.72</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.10.4\">55.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.10.5\">59.22</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T8.1.11.1\">Winobias 2_2</td>\n<td class=\"ltx_td\" id=\"S6.T8.1.11.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.11.3\">84.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.11.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.11.4.1\">89.46</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.11.5\">89.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S6.T8.1.12.1\">Toxigen</td>\n<td class=\"ltx_td ltx_border_bb\" id=\"S6.T8.1.12.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T8.1.12.3\">61.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T8.1.12.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.12.4.1\">29.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T8.1.12.5\">38.75</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Safety academic benchmark results of Gemma 1.1 IT models, compared to similarly sized, openly-available models. Evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; we do not report previously-published numbers for LLaMA-2 on TruthfulQA, as we use different, non-comparable evaluation set-ups: we use MC2, where LLaMA-2 uses GPT-Judge. Results for Gemma 1.0 IT models can be found in appendix.\n</figcaption>\n</figure>",
            "capture": "Table 8: Safety academic benchmark results of Gemma 1.1 IT models, compared to similarly sized, openly-available models. Evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; we do not report previously-published numbers for LLaMA-2 on TruthfulQA, as we use different, non-comparable evaluation set-ups: we use MC2, where LLaMA-2 uses GPT-Judge. Results for Gemma 1.0 IT models can be found in appendix.\n"
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T9.1\">\n<tr class=\"ltx_tr\" id=\"A1.T9.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T9.1.1.1\"><span class=\"ltx_text\" id=\"A1.T9.1.1.1.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"A1.T9.1.1.2\"><span class=\"ltx_text\" id=\"A1.T9.1.1.2.1\" style=\"font-size:80%;\">Safety</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"A1.T9.1.1.3\"><span class=\"ltx_text\" id=\"A1.T9.1.1.3.1\" style=\"font-size:80%;\">Instruction Following</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T9.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T9.1.2.1.1\" style=\"font-size:80%;\">Gemma 7B IT</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T9.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T9.1.2.2.1\" style=\"font-size:80%;\">58%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T9.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T9.1.2.3.1\" style=\"font-size:80%;\">51.7%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T9.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T9.1.3.1.1\" style=\"font-size:50%;\">95% Conf. Interval</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T9.1.3.2\"><span class=\"ltx_text\" id=\"A1.T9.1.3.2.1\" style=\"font-size:50%;\">[55.9%, 60.1%]</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T9.1.3.3\"><span class=\"ltx_text\" id=\"A1.T9.1.3.3.1\" style=\"font-size:50%;\">[49.6%, 53.8%]</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T9.1.4.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T9.1.4.1.1\" style=\"font-size:50%;\">Win / Tie / Loss</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T9.1.4.2\"><span class=\"ltx_text\" id=\"A1.T9.1.4.2.1\" style=\"font-size:50%;\">42.9% / 30.2% / 26.9%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T9.1.4.3\"><span class=\"ltx_text\" id=\"A1.T9.1.4.3.1\" style=\"font-size:50%;\">42.5% / 18.4% / 39.1%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T9.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T9.1.5.1.1\" style=\"font-size:80%;\">Gemma 2B IT</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T9.1.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T9.1.5.2.1\" style=\"font-size:80%;\">56.5%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T9.1.5.3\"><span class=\"ltx_text\" id=\"A1.T9.1.5.3.1\" style=\"font-size:80%;\">41.6%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T9.1.6.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T9.1.6.1.1\" style=\"font-size:50%;\">95% Conf. Interval</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T9.1.6.2\"><span class=\"ltx_text\" id=\"A1.T9.1.6.2.1\" style=\"font-size:50%;\">[54.4%, 58.6%]</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A1.T9.1.6.3\"><span class=\"ltx_text\" id=\"A1.T9.1.6.3.1\" style=\"font-size:50%;\">[39.5%, 43.7%]</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T9.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T9.1.7.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T9.1.7.1.1\" style=\"font-size:50%;\">Win / Tie / Loss</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T9.1.7.2\"><span class=\"ltx_text\" id=\"A1.T9.1.7.2.1\" style=\"font-size:50%;\">44.8% / 22.9% / 32.3%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A1.T9.1.7.3\"><span class=\"ltx_text\" id=\"A1.T9.1.7.3.1\" style=\"font-size:50%;\">32.7% / 17.8% / 49.5%</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Win rate of Gemma 1.0 IT models versus Mistral 7B v0.2 Instruct with 95% confidence intervals. We report breakdowns of wins, ties, and losses. Ties are broken evenly in the final win rate.</figcaption>\n</figure>",
            "capture": "Table 9: Win rate of Gemma 1.0 IT models versus Mistral 7B v0.2 Instruct with 95% confidence intervals. We report breakdowns of wins, ties, and losses. Ties are broken evenly in the final win rate."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T10\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T10.1\">\n<tr class=\"ltx_tr\" id=\"A1.T10.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A1.T10.1.1.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"A1.T10.1.1.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T10.1.1.3\">Mistral v0.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"A1.T10.1.1.4\">Gemma IT</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.2.1\">Benchmark</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.2.2\">metric</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.2.3\">7B*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.2.4\">2B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.2.5\">7B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T10.1.3.1\">RealToxicity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.3.2\">avg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.3.3\">8.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.3.4.1\">6.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.3.5\">7.90</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.4.1\">BOLD</td>\n<td class=\"ltx_td\" id=\"A1.T10.1.4.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.4.3\">46.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.4.4\">45.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.4.5.1\">49.08</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.5.1\">CrowS-Pairs</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.5.2\">top-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.5.3\">32.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.5.4\">45.82</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.5.5.1\">51.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.6.1\">BBQ Ambig</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.6.2\">1-shot, top-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.6.3.1\">97.53</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.6.4\">62.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.6.5\">92.54</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.7.1\">BBQ Disambig</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.7.2\">top-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.7.3.1\">84.45</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.7.4\">54.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.7.5\">71.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.8.1\">Winogender</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.8.2\">top-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.8.3.1\">64.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.8.4\">51.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.8.5\">54.17</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.9.1\">TruthfulQA</td>\n<td class=\"ltx_td\" id=\"A1.T10.1.9.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.9.3.1\">48.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.9.4\">31.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.9.5\">44.84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.10.1\">Winobias 1_2</td>\n<td class=\"ltx_td\" id=\"A1.T10.1.10.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.10.3.1\">65.72</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.10.4\">56.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.10.5\">59.09</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.11.1\">Winobias 2_2</td>\n<td class=\"ltx_td\" id=\"A1.T10.1.11.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.11.3\">84.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.11.4\">91.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.11.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.11.5.1\">92.23</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T10.1.12.1\">Toxigen</td>\n<td class=\"ltx_td ltx_border_bb\" id=\"A1.T10.1.12.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T10.1.12.3\">61.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T10.1.12.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.12.4.1\">29.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T10.1.12.5\">39.59</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Safety academic benchmark results of Gemma 1.0 IT models, compared to similar size open models. Evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; we do not report previously-published numbers for LLaMA-2 on TruthfulQA, because we use different, non-comparable evaluation set-ups: we use MC2, where LLaMA-2 uses GPT-Judge.\n</figcaption>\n</figure>",
            "capture": "Table 10: Safety academic benchmark results of Gemma 1.0 IT models, compared to similar size open models. Evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; we do not report previously-published numbers for LLaMA-2 on TruthfulQA, because we use different, non-comparable evaluation set-ups: we use MC2, where LLaMA-2 uses GPT-Judge.\n"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.08295v4_figure_1.png",
            "caption": "Figure 1: Language understanding and generation performance of Gemma 7B across\ndifferent capabilities compared to similarly sized open models. We group together standard academic benchmark evaluations by capability and average the respective scores; see Table 6 for a detailed breakdown of performance."
        },
        "2": {
            "figure_path": "2403.08295v4_figure_2.png",
            "caption": "Figure 2: Comparing average memorization rates across model families. We compare the Gemma pretrained models to PaLM and PaLM 2 models of comparable size and find similarly low rates of memorization."
        },
        "3": {
            "figure_path": "2403.08295v4_figure_3.png",
            "caption": "Figure 3: Measuring personal and sensitive data memorization rates. No sensitive data was memorized, hence it is omitted from the figure."
        },
        "4": {
            "figure_path": "2403.08295v4_figure_4.png",
            "caption": "Figure 4: Comparing exact and approximate memorization."
        }
    },
    "references": [
        {
            "1": {
                "title": "The falcon series of open language models, 2023.",
                "author": "E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah,\n\u00c9tienne Goffinet, D. Hesslow, J. Launay, Q. Malartic, D. Mazzotta, B. Noune,\nB. Pannier, and G. Penedo.",
                "venue": null,
                "url": null
            }
        },
        {
            "2": {
                "title": "Concrete problems in AI safety.",
                "author": "D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man\u00e9.",
                "venue": "arXiv preprint, 2016.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Palm 2 technical report.",
                "author": "R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri,\nE. Taropa, P. Bailey, Z. Chen, et al.",
                "venue": "arXiv preprint arXiv:2305.10403, 2023.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Program synthesis with large language models.",
                "author": "J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang,\nC. J. Cai, M. Terry, Q. V. Le, and C. Sutton.",
                "venue": "CoRR, abs/2108.07732, 2021.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Constitutional ai: Harmlessness from ai feedback, 2022.",
                "author": "Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\nA. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah,\nD. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez,\nJ. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite,\nL. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma,\nR. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk,\nS. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume,\nS. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph,\nS. McCandlish, T. Brown, and J. Kaplan.",
                "venue": null,
                "url": null
            }
        },
        {
            "6": {
                "title": "Pathways: Asynchronous distributed dataflow for ml, 2022.",
                "author": "P. Barham, A. Chowdhery, J. Dean, S. Ghemawat, S. Hand, D. Hurt, M. Isard,\nH. Lim, R. Pang, S. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E. Shafey, C. A.\nThekkath, and Y. Wu.",
                "venue": null,
                "url": null
            }
        },
        {
            "7": {
                "title": "PIQA: reasoning about physical commonsense in natural language.",
                "author": "Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi.",
                "venue": "CoRR, abs/1911.11641, 2019.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Rank analysis of incomplete block designs: I. the method of paired\ncomparisons.",
                "author": "R. A. Bradley and M. E. Terry.",
                "venue": "Biometrika, 39, 1952.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Quantifying memorization across neural language models.",
                "author": "N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang.",
                "venue": "arXiv preprint arXiv:2202.07646, 2022.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Evaluating large language models trained on code.",
                "author": "M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,\nH. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger,\nM. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder,\nM. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P.\nSuch, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss,\nW. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji,\nS. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra,\nE. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer,\nP. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\nW. Zaremba.",
                "venue": "CoRR, abs/2107.03374, 2021.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Palm: Scaling language modeling with pathways, 2022.",
                "author": "A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham,\nH. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko,\nJ. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif,\nN. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari,\nP. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia,\nV. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim,\nB. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M.\nDai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov,\nK. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei,\nK. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel.",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "Deep reinforcement learning from human preferences.",
                "author": "P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei.",
                "venue": "Advances in Neural Information Processing Systems, 30, 2017.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Boolq: Exploring the surprising difficulty of natural yes/no\nquestions.",
                "author": "C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova.",
                "venue": "CoRR, abs/1905.10044, 2019.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Think you have solved question answering? try arc, the ai2 reasoning\nchallenge, 2018.",
                "author": "P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and\nO. Tafjord.",
                "venue": null,
                "url": null
            }
        },
        {
            "15": {
                "title": "Training verifiers to solve math word problems.",
                "author": "K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert,\nJ. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman.",
                "venue": "CoRR, abs/2110.14168, 2021.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Large scale distributed deep networks.",
                "author": "J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. a. Ranzato,\nA. Senior, P. Tucker, K. Yang, Q. Le, and A. Ng.",
                "venue": "In F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors,\nAdvances in Neural Information Processing Systems, volume 25. Curran\nAssociates, Inc., 2012.",
                "url": null
            }
        },
        {
            "17": {
                "title": "BERT: pre-training of deep bidirectional transformers for language\nunderstanding.",
                "author": "J. Devlin, M. Chang, K. Lee, and K. Toutanova.",
                "venue": "CoRR, abs/1810.04805, 2018.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Gemini: A family of highly capable multimodal models, 2023.",
                "author": "Gemini Team.",
                "venue": null,
                "url": null
            }
        },
        {
            "19": {
                "title": "Measuring massive multitask language understanding.",
                "author": "D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt.",
                "venue": "CoRR, abs/2009.03300, 2020.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Measuring mathematical problem solving with the math dataset.",
                "author": "D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and\nJ. Steinhardt.",
                "venue": "NeurIPS, 2021.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Preventing verbatim memorization in language models gives a false\nsense of privacy.",
                "author": "D. Ippolito, F. Tram\u00e8r, M. Nasr, C. Zhang, M. Jagielski, K. Lee, C. A.\nChoquette-Choo, and N. Carlini.",
                "venue": "arXiv preprint arXiv:2210.17546, 2022.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Mistral 7b, 2023.",
                "author": "A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las\nCasas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A.\nLachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E.\nSayed.",
                "venue": null,
                "url": null
            }
        },
        {
            "23": {
                "title": "Triviaqa: A large scale distantly supervised challenge dataset for\nreading comprehension.",
                "author": "M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer.",
                "venue": "CoRR, abs/1705.03551, 2017.",
                "url": null
            }
        },
        {
            "24": {
                "title": "How our principles helped define alphafold\u2019s release, 2022.",
                "author": "K. Kavukcuoglu, P. Kohli, L. Ibrahim, D. Bloxwich, and S. Brown.",
                "venue": null,
                "url": null
            }
        },
        {
            "25": {
                "title": "SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing.",
                "author": "T. Kudo and J. Richardson.",
                "venue": "In E. Blanco and W. Lu, editors, Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pages 66\u201371, Brussels, Belgium, Nov. 2018. Association for\nComputational Linguistics.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Madlad-400: A multilingual and document-level large audited dataset.",
                "author": "S. Kudugunta, I. Caswell, B. Zhang, X. Garcia, C. A. Choquette-Choo, K. Lee,\nD. Xin, A. Kusupati, R. Stella, A. Bapna, et al.",
                "venue": "arXiv preprint arXiv:2309.04662, 2023.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Natural questions: A benchmark for question answering research.",
                "author": "T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti,\nD. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones,\nM. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov.",
                "venue": "Transactions of the Association for Computational Linguistics,\n7:452\u2013466, 2019.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Deep learning.",
                "author": "Y. LeCun, Y. Bengio, and G. Hinton.",
                "venue": "nature, 521(7553):436\u2013444, 2015.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Efficient estimation of word representations in vector space.",
                "author": "T. Mikolov, K. Chen, G. Corrado, and J. Dean.",
                "venue": "In Y. Bengio and Y. LeCun, editors, 1st International\nConference on Learning Representations, ICLR 2013, Scottsdale, Arizona,\nUSA, May 2-4, 2013, Workshop Track Proceedings, 2013.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Scalable extraction of training data from (production) language\nmodels.",
                "author": "M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A.\nChoquette-Choo, E. Wallace, F. Tram\u00e8r, and K. Lee.",
                "venue": "arXiv preprint arXiv:2311.17035, 2023.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,\nS. Agarwal, K. Slama, A. Ray, et al.",
                "venue": "Advances in Neural Information Processing Systems, 35, 2022.",
                "url": null
            }
        },
        {
            "32": {
                "title": "How to catch an ai liar: Lie detection in black-box llms by asking\nunrelated questions, 2023.",
                "author": "L. Pacchiardi, A. J. Chan, S. Mindermann, I. Moscovitz, A. Y. Pan, Y. Gal,\nO. Evans, and J. Brauner.",
                "venue": null,
                "url": null
            }
        },
        {
            "33": {
                "title": "The LAMBADA dataset: Word prediction requiring a broad discourse\ncontext.",
                "author": "D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle,\nM. Baroni, G. Boleda, and R. Fern\u00e1ndez.",
                "venue": "CoRR, abs/1606.06031, 2016.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text\ntransformer.",
                "author": "C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,\nW. Li, and P. J. Liu.",
                "venue": "CoRR, abs/1910.10683, 2019.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Scaling up models and data with t5x and seqio,\n2022.",
                "author": "A. Roberts, H. W. Chung, A. Levskaya, G. Mishra, J. Bradbury, D. Andor,\nS. Narang, B. Lester, C. Gaffney, A. Mohiuddin, C. Hawthorne, A. Lewkowycz,\nA. Salcianu, M. van Zee, J. Austin, S. Goodman, L. B. Soares, H. Hu,\nS. Tsvyashchenko, A. Chowdhery, J. Bastings, J. Bulian, X. Garcia, J. Ni,\nA. Chen, K. Kenealy, J. H. Clark, S. Lee, D. Garrette, J. Lee-Thorp,\nC. Raffel, N. Shazeer, M. Ritter, M. Bosma, A. Passos, J. Maitin-Shepard,\nN. Fiedel, M. Omernick, B. Saeta, R. Sepassi, A. Spiridonov, J. Newlan, and\nA. Gesmundo.",
                "venue": null,
                "url": null
            }
        },
        {
            "36": {
                "title": "Scaling up models and data with t5x and seqio.",
                "author": "A. Roberts, H. W. Chung, G. Mishra, A. Levskaya, J. Bradbury, D. Andor,\nS. Narang, B. Lester, C. Gaffney, A. Mohiuddin, et al.",
                "venue": "Journal of Machine Learning Research, 24(377):1\u20138, 2023.",
                "url": null
            }
        },
        {
            "37": {
                "title": "WINOGRANDE: an adversarial winograd schema challenge at scale.",
                "author": "K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi.",
                "venue": "CoRR, abs/1907.10641, 2019.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Socialiqa: Commonsense reasoning about social interactions.",
                "author": "M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi.",
                "venue": "CoRR, abs/1904.09728, 2019.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Fast transformer decoding: One write-head is all you need.",
                "author": "N. Shazeer.",
                "venue": "CoRR, abs/1911.02150, 2019.",
                "url": null
            }
        },
        {
            "40": {
                "title": "GLU variants improve transformer.",
                "author": "N. Shazeer.",
                "venue": "CoRR, abs/2002.05202, 2020.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Defining and characterizing reward gaming.",
                "author": "J. M. V. Skalse, N. H. R. Howe, D. Krasheninnikov, and D. Krueger.",
                "venue": "In NeurIPS, 2022.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Roformer: Enhanced transformer with rotary position embedding.",
                "author": "J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu.",
                "venue": "CoRR, abs/2104.09864, 2021.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Sequence to sequence learning with neural networks.",
                "author": "I. Sutskever, O. Vinyals, and Q. V. Le.",
                "venue": "CoRR, abs/1409.3215, 2014.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Challenging big-bench tasks and whether chain-of-thought can solve\nthem, 2022.",
                "author": "M. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung,\nA. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei.",
                "venue": null,
                "url": null
            }
        },
        {
            "45": {
                "title": "Commonsenseqa: A question answering challenge targeting commonsense\nknowledge, 2019.",
                "author": "A. Talmor, J. Herzig, N. Lourie, and J. Berant.",
                "venue": null,
                "url": null
            }
        },
        {
            "46": {
                "title": "Llama: Open and efficient foundation language models,\n2023a.",
                "author": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,\nB. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin,\nE. Grave, and G. Lample.",
                "venue": null,
                "url": null
            }
        },
        {
            "47": {
                "title": "Llama 2: Open foundation and fine-tuned chat models,\n2023b.",
                "author": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C.\nFerrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu,\nB. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou,\nH. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S.\nKoura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao,\nX. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton,\nJ. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith,\nR. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan,\nP. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang,\nA. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom.",
                "venue": null,
                "url": null
            }
        },
        {
            "48": {
                "title": "Attention is all you need.",
                "author": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin.",
                "venue": "CoRR, abs/1706.03762, 2017.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Chain of thought prompting elicits reasoning in large language\nmodels.",
                "author": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou.",
                "venue": "CoRR, abs/2201.11903, 2022.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Ethical and social risks of harm from language models.",
                "author": "L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P. Huang, M. Cheng,\nM. Glaese, B. Balle, A. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins,\nT. Stepleton, C. Biles, A. Birhane, J. Haas, L. Rimell, L. A. Hendricks,\nW. Isaac, S. Legassick, G. Irving, and I. Gabriel.",
                "venue": "CoRR, abs/2112.04359, 2021.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Xla: Optimizing compiler for tensorflow, 2019.",
                "author": "XLA.",
                "venue": "URL https://www.tensorflow.org/xla.",
                "url": null
            }
        },
        {
            "52": {
                "title": "GSPMD: general and scalable parallelization for ML computation\ngraphs.",
                "author": "Y. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang, R. Joshi, M. Krikun,\nD. Lepikhin, A. Ly, M. Maggioni, R. Pang, N. Shazeer, S. Wang, T. Wang,\nY. Wu, and Z. Chen.",
                "venue": "CoRR, abs/2105.04663, 2021.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Root mean square layer normalization.",
                "author": "B. Zhang and R. Sennrich.",
                "venue": "CoRR, abs/1910.07467, 2019.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.",
                "author": "L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li,\nD. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica.",
                "venue": null,
                "url": null
            }
        },
        {
            "55": {
                "title": "Agieval: A human-centric benchmark for evaluating foundation models,\n2023.",
                "author": "W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and\nN. Duan.",
                "venue": null,
                "url": null
            }
        },
        {
            "56": {
                "title": "Representation engineering: A top-down approach to ai transparency,\n2023.",
                "author": "A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin,\nM. Mazeika, A.-K. Dombrowski, S. Goel, N. Li, M. J. Byun, Z. Wang, A. Mallen,\nS. Basart, S. Koyejo, D. Song, M. Fredrikson, J. Z. Kolter, and D. Hendrycks.",
                "venue": null,
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.08295v4",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6"
        ],
        "methodology_sections": [
            "2",
            "3",
            "4",
            "5"
        ],
        "main_experiment_and_results_sections": [
            "6",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "4.2",
            "5.1"
        ]
    },
    "research_context": {
        "paper_id": "2403.08295v4",
        "paper_title": "Gemma: Open Models Based on Gemini Research and Technology",
        "research_background": "### Motivation:\nThe motivation behind the paper is to contribute to the ongoing development and open discourse on large language models (LLMs) by presenting \"Gemma\", a family of open models based on Google\u2019s Gemini models. The authors aim to provide comprehensive resources to further research and development in the domain, including pre-trained and fine-tuned model checkpoints, as well as an open-source codebase. This initiative seeks to enhance the performance of current models, bridge the gap between research and practical deployment, and foster responsible usage of LLMs to ensure equitable access and advancement in technology.\n\n### Research Problem:\nThe research problem addressed by the paper is the current challenges in developing robust, safe, and high-performing large language models that can be efficiently deployed across different computational environments (such as GPUs, TPUs, and CPUs). The authors aim to create models that not only achieve state-of-the-art performance in various text domains but also ensure they are safe, responsible, and accessible to the broader research community. A specific focus is given to balancing efficiency, capability, and safety assurance across different scales of models.\n\n### Relevant Prior Work:\n1. **Previous Models and Architectures:**\n   - The work builds directly on Google\u2019s Gemini models (Gemini Team, 2023), the transformer architecture (Vaswani et al., 2017), and other foundational sequence models (Sutskever et al., 2014).\n   - Various influential models such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2018), T5 (Raffel et al., 2019), and T5X (Roberts et al., 2022) have influenced Gemma's development.\n\n2. **Large-Scale Training Techniques:**\n   - Gemma adopts large-scale training methodologies suitable for distributed systems, drawing from works such as Barham et al., (2022), Roberts et al., (2023), and Dean et al., (2012).\n\n3. **Benchmarks and Evaluations:**\n   - Previous state-of-the-art open models like those detailed in works by Jiang et al. (2023), Touvron et al. (2023a, 2023b), and Almazrouei et al. (2023) serve as benchmarks for evaluating Gemma's performance.\n   - The paper utilizes a broad range of domain-specific benchmarks, including question answering (Clark et al., 2019; Kwiatkowski et al., 2019), commonsense reasoning (Sakaguchi et al., 2019; Suzgun et al., 2022), math and science (Cobbe et al., 2021; Hendrycks et al., 2020), and coding (Austin et al., 2021; Chen et al., 2021).\n\n### Conclusion:\nThe paper presents Gemma as a significant advancement in open LLMs, addressing both the computational efficiency and the complex needs of different application scenarios. It builds on a robust foundation of prior research while contributing to the ongoing discourse on equitable, safe, and innovative development in large language models.",
        "methodology": "The Gemma model architecture is fundamentally built around the transformer decoder paradigm as established by Vaswani et al. (2017). Here, we detail the core components and innovations utilized in our methodology:\n\n1. **Transformer Decoder Architecture**: The foundational architecture of the model is the transformer decoder. This configuration has a core parameter set that is summarized in an accompanying table (Table 1).\n\n2. **Context Length**: During training, the models operate with a context length of 8192 tokens. This expansive context length supports the model's capacity to understand and generate extensive sequences of text.\n\n3. **Rotary Positional Embeddings (RoPE)**: Instead of traditional absolute positional embeddings, the model incorporates rotary positional embeddings (Su et al., 2021). These embeddings are shared across both inputs and outputs, a strategy that helps in reducing the overall model size.\n\n4. **GeGLU Activations**: The standard ReLU non-linearity is substituted with an approximated version of the GeGLU activation function (Shazeer, 2020). This substitution aims to enhance the non-linear transformation capabilities of the network.\n\n5. **RMSNorm**: To ensure stable training, RMSNorm (Zhang and Sennrich, 2019) is used to normalize the input of each transformer sub-layer, including both the attention layer and the feedforward layer. This normalization step contributes to the stability and efficiency of the model training process.\n\nBy integrating these advancements, the Gemma model strives to push the boundaries of transformer decoder architectures, ensuring more efficient and effective processing of extensive text sequences.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n### Experiment Setup:\n\n#### **Datasets:**\n1. **Pretraining Data:** Gemma is evaluated on a broad range of domains, but the specifics of the datasets are not provided. The comparison is made with PaLM models using a shared subset of common training corpora.\n2. **Google Cloud Sensitive Data Protection Data:** This is used to identify occurrences of personal and sensitive data within the pretraining datasets.\n\n#### **Baselines:**\n1. **PaLM Models:** The primary baseline used for comparison in terms of memorization rates.\n\n#### **Evaluation Metrics:**\n1. **Memorization Rate:** This measures the extent to which the models memorize their training data, both overall and in relation to personal/sensitive data.\n2. **Sensitive Data Detection:** Measures the occurrence of memorized \u201csensitive\u201d and \u201cpersonal\u201d data using Google Cloud Sensitive Data Protection.\n3. **Estimated Total Memorization:** A more comprehensive metric to capture how much of the entire pretraining dataset is memorized.\n\n### Main Experimental Results:\n1. **Memorization Rates:**\n   - **Gemma vs. PaLM:** When comparing memorization on a shared subset of training corpora, Gemma exhibits much lower memorization rates compared to PaLM models. When estimating the total memorization across the entire pretraining dataset, Gemma's rate of memorization is found to be comparable to PaLM.\n  \n2. **Sensitive Data Memorization:**\n   - **Personal Data:** No cases of memorized sensitive data are detected in Gemma. However, some data classified as potentially \u201cpersonal\u201d is memorized at a lower rate. \n   - **False Positives:** Due to the automated techniques used, there are likely many false positives in the identification of personal data (as these tools are pattern-based without context consideration).\n\n3. **Consistency Across Subcategories:**\n   - **Data Memorization:** Roughly 50% more data is approximately memorized, and this trend is nearly consistent across different subcategories of the dataset.\n\nIn summary, the main experiment shows Gemma's overall comparable memorization rate to PaLM when considering the full pretraining dataset, an absence of memorized sensitive data, and consistent data memorization across subcategories."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the effectiveness of multi-query attention at smaller scales in comparison to multi-head attention.",
            "experiment_process": "The experiment utilized two core model checkpoints: one with 7 billion parameters using multi-head attention and the other with 2 billion parameters employing multi-query attention. Model performance and scalability were analyzed through these configurations, based on suggestions from the original Multi-Query Attention paper by Shazeer (Shazeer, 2019).",
            "result_discussion": "The ablations showed that multi-query attention works well at small scales, thus validating the use of multi-query attention for the smaller 2B parameter checkpoints.",
            "ablation_id": "2403.08295v4.No1"
        },
        {
            "research_objective": "To determine the optimal data mixture for pre-training the 2B and 7B models by mitigating the risk of harmful or low-quality data.",
            "experiment_process": "Pre-training datasets were filtered to remove unwanted, unsafe, or low-quality content using heuristics and model-based classifiers. Further, evaluation sets were excluded from the pre-training data mixture, and contamination analyses were run to prevent evaluation set leakage. The corpus mixture was altered throughout training via staged selection to increase the weight of high-quality data towards the end of the training process. Multiple ablations were conducted on the 2B and 7B models to finalize the data mixture.",
            "result_discussion": "The staged training approach and filtering ensured that the final pre-training data mixture was optimized for relevance and high quality, reducing the risk of producing sensitive outputs while retaining strong model performance.",
            "ablation_id": "2403.08295v4.No2"
        }
    ]
}