{
    "title": "Feedback-aligned Mixed LLMs for Machine Language-Molecule Translation",
    "abstract": "The intersection of chemistry and Artificial Intelligence (AI) is an active area of research focused on accelerating scientific discovery. While using large language models (LLMs) with scientific modalities has shown potential, there are significant challenges to address, such as improving training efficiency and dealing with the out-of-distribution problem.\nFocussing on the task of automated language-molecule translation, we are the first to use state-of-the art (SOTA) human-centric optimisation algorithms in the cross-modal setting, successfully aligning cross-language-molecule modals. We empirically show that we can augment the capabilities of scientific LLMs without the need for extensive data or large models. We conduct experiments using only 10% of the available data to mitigate memorisation effects associated with training large models on extensive datasets. We achieve significant performance gains, surpassing the best benchmark model trained on extensive in-distribution data by a large margin and reach new SOTA levels. Additionally we are the first to propose employing non-linear fusion for mixing cross-modal LLMs which further boosts performance gains without increasing training costs or data needs. Finally, we introduce a fine-grained, domain-agnostic evaluation method to assess hallucination in LLMs and promote responsible use.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Chemistry plays a crucial role in developing innovative scientific solutions that are scalable and cost-effective, whether it involves pioneering new drugs Ferguson and Gray (2018  ###reference_b11###), advanced materials Kippelen and Br\u00e9das (2009  ###reference_b15###), or improving chemical processes Zhong et al. (2023  ###reference_b45###). However, navigating the vast possibilities in chemistry but also biology and materials science necessitates the involvement of Artificial Intelligence (AI) technology to enable the acceleration of scientific discovery AI4Science and Quantum (2023  ###reference_b1###).\nMulti-modal models combining language with molecules have recently gained attention as a promising approach for studying and comprehending molecules, offering significant potential to tackle pressing global issues Zhang et al.  ###reference_b42###. Existing work has applied successful paradigms from natural language processing (NLP) and multimodal representation learning to the chemistry domain. One common approach involves converting the inherent three-dimensional structures of molecules into SMILES, which provide a mapping to symbolic character-level representations. Subsequently, researchers have explored learning language-molecule representations in separate yet coordinated spaces Edwards et al. (2021  ###reference_b7###, 2022  ###reference_b8###); Liu et al. (2023a  ###reference_b17###), in a joint space Liu et al. (2023b  ###reference_b18###), or through hybrid approaches Luo et al. (2023  ###reference_b20###); Christofidellis et al. (2023  ###reference_b5###). Despite significant advancements in the field, existing work does not effectively tackle the inherent challenges in training such models; they often rely on low quality or noisy synthetic data and typically require significantly more data compared to ordinary NLP tasks Edwards et al. (2021  ###reference_b7###).\nHowever training larger models and utilising extensive datasets does not always guarantee better performance. Ongoing research in NLP is focused on innovative training methods to enhance the performance of small-sized LLMs Liu et al. (2024  ###reference_b16###). For instance, ALMA Xu et al. (2023  ###reference_b38###) fine-tuned LLaMA-2 Touvron et al. (2023  ###reference_b33###) with extensive non-English monolingual data to enhance the model\u2019s multilingual abilities. Subsequently, supervised fine-tuning (SFT) with high-quality parallel data was performed to instruct the model in generating translations, surpassing all prior moderate-sized LLMs, including large models like GPT-3.5 Brown et al. (2020  ###reference_b3###). Another successful paradigm, Reinforcement Learning with Human Feedback (RLHF) Ouyang et al. (2022  ###reference_b23###), aims to enhance the capabilities of LLMs across multiple NLP tasks, including machine translation. A recent trend is favouring closed-form losses that directly operate on offline preferences Rafailov et al. (2024  ###reference_b26###); Xu et al. (2024  ###reference_b39###); Ethayarajh et al. (2024  ###reference_b10###), simplifying over RHLF without loss in performance.\nHere, we address challenges of effectively training robust LLMs and dealing with the out-of-distribution problem when integrating scientific modalities, specifically focusing on machine language-molecule translation. Our approach is the first to experiment with human-centric optimisation objectives in the cross-modal representation learning setting to align cross language-molecule modals with preferred offline translations. To mitigate the memorisation effects inherent in training on large datasets, we conduct experiments using only 10% of the L+M-24 benchmark dataset Edwards et al. (2024  ###reference_b9###), achieving new state-of-the-art (SOTA) levels. Additionally, we propose a novel approach that fuses cross-modal LLMs to amplify performance gains without increasing training costs or data needs. The contributions of this work are as follows:\nWe obtain robust LLMs for machine language-molecule translation by experimenting with recent shifts in RLHF within the cross-modal representation setting, namely, direct preference optimisation (DPO), contrastive preference optimisation (CPO), and Kahneman-Tversky optimisation (KTO) ( 3.2  ###reference_###). To the best of our knowledge, this is the first comprehensive study that investigates human-centric optimisations in LLMs in the multidisciplinary field of AI for living sciences. Our experiments show that both DPO and KTO cannot cope within agnostic cross-modal settings. In contrast, CPO, which is invariant to biases in these settings, achieves substantial performance gains by a large margin, reaching new SOTA levels compared to the best benchmark, Meditron, trained on the entire dataset ( 4.4.1  ###reference_.SSS1###).\nWe introduce an approach that mixes dominant-modality LLMs, based on non-linear fusion algorithms, namely TIES and SLERP ( 3.3  ###reference_###). This addresses inequalities between the informativeness of different modalities as well as inherent limitations in optimisations operating on preference data, all without increase in training costs. We empirically demonstrate that this approach facilitates RLHF optimisation algorithms, which, to varying degrees, depend on how well the pre-trained model is suited to specific cross-modal settings, thereby further boosting performance gains ( 4.4.2  ###reference_.SSS2###).\nWe introduce a fine-grained evaluation method with multi-aspect criteria for assessing the presence of hallucinations in LLMs. Our domain-agnostic method can help ensure the responsible use of chemical LLMs ( 3.4  ###reference_###). We critically discuss our new insights and compare them with established metrics in the literature. We conclude by highlighting the limitations of both our evaluation criteria and previous evaluation and discuss potential future directions for responsibly assessing generative outputs from chemical LLMs (see Section 4.4.3  ###reference_.SSS3###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Foundations in RLHF for LLMs",
            "text": "Feedback-aligned LLMs traditionally undergo fine-tuning with RLHF, wherein human preferences serve as a reward signal in optimisation Stiennon et al. (2020  ###reference_b32###); Ouyang et al. (2022  ###reference_b23###). To train a LLM with RLHF, a reinforcement learning optimisation algorithm such as PPO Schulman et al. (2017  ###reference_b27###) is typically deployed on offline preference data, involving commonly three steps:\nRoll-out: Typically, a model  is trained for auto-regressive language generation on a large generic corpus. This training operates under the premise that the probability distribution of a sequence of words can be broken down into the product of conditional distributions for the next word Radford et al. (2019  ###reference_b25###).\nEvaluation: A reference model  is employed to optimise  for a downstream task. Typically, the  model undergoes fine-tuning with an auto-regressive objective, using data pertinent to the downstream task. This often involves instruction tuning  to regulate the generated outputs.\nOptimisation:\nThe optimisation of  with respect to  operates on a triple dataset , where  represents the input, and  and  denote preferred and dis-preferred outputs, respectively, such that  for . In Bradley and Terry (1952  ###reference_b2###), the probability of  being preferred over  in pairwise comparisons can be formulated as follows:\nHere,  represents the logistic function, and  denotes the \u201ctrue\u201d reward function that underlies the preferences. As obtaining the true reward directly from a human would be prohibitively expensive, a reward model  is trained to act as a surrogate. This is achieved by minimising the negative log-likelihood in human preference data;\nAdditionally, the Kullback-Leibler (KL) divergence between the outputs generated by  and the parameterised  models serves as an additional reward signal, ensuring that the generated responses closely align with the reference model. Consequently, an optimal model  is one that maximises;\nwhere  is the temperature parameter typically .\nRLHF can present challenges due to its inherent slowness and instability, especially in the case of highly varied outputs Zheng et al. (2024  ###reference_b44###). Recently, there has been a shift towards using closed-form losses in RLHF to align language models (LLMs) with human preferences. These losses are predominantly HALOs that model human biases, as discussed in Tversky and Kahneman (1992  ###reference_b34###), aiming to maximise the margin between preferred and dis-preferred generated outputs (for the definition refer to Appx. B  ###reference_###). This approach offers a mathematical equivalence with RLHF, while effectively addressing its inherent limitations."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Existing approaches for chemical models heavily rely on expensive pre-training using large mono-modality datasets or per-task fine-tuning for each multi-modal task. Meanwhile, other efforts concentrate on multi-task learning, integrating vast multi-task datasets to enhance the capabilities of chemical models (for related work please refer to Appx. A  ###reference_###). Here, we adopt a distinct approach by reducing reliance on extensive data and exploring optimisation algorithms and modality fusion techniques to enhance the capabilities of chemical LLMs.\nA model is optimised by minimising the disparity between the generated and intended target outputs. For a model parameterised by , denoted as , the loss function is defined as the negative log-likelihood of source-target pairs , and can be expressed as follows:\nwhere the quality of  is crucial to the effectiveness of the translation model Zhao et al. (2022  ###reference_b43###).\nDPO Rafailov et al. (2024  ###reference_b26###) provides a more human-centric optimisation objective aimed at aligning the translation model  with human intentions. Here  is an offline dataset of comparisons , where  represents the preferred translation (e.g. human gold standard), and  represents the dis-preferred translation (typically synthetic outputs obtained by a translation model). The loss function for DPO is constructed as a maximum likelihood objective for a parameterised policy ;\nwhere  is the fine-tuned translation model used as the reference,  is the Sigmoid function, and  is a hyper-parameter (\u00a7 2  ###reference_###).\nDPO requires high-quality data or, at the very least, appropriately fine-tuned models. In real-world scenarios, it may not be feasible to acquire high-quality data, especially for challenging domains such as chemistry and biology. CPO is a general approximation of Eq.5  ###reference_### that addresses the limitations of DPO by training a model to avoid generating outputs that are merely adequate, but not perfect, tested in machine translation Xu et al. (2024  ###reference_b39###). Specifically, in CPO, Eq. 5  ###reference_### is effectively approximated using a uniform reference model, which assumes equal likelihood for all possible generated outputs;\nEquation 6  ###reference_### implies that the loss is calculated based on how well generated outputs match this uniform distribution of possible outputs, rather than being biased towards any particular one. To maintain  close to the preferred data distribution, a behaviour cloning (BC) Hejna et al. (2023  ###reference_b13###) regulariser is injected;\nHere,  denotes a small positive constant, and  signifies the Kullback-Leibler divergence. Finally, the regulariser is streamlined with an additional SFT term on the preferred data;\nKTO Ethayarajh et al. (2024  ###reference_b10###) combines the implicit reward under the RLHF objective with the concept of loss aversion from Kahneman-Tversky prospect theory Tversky and Kahneman (1992  ###reference_b34###).\nUnlike DPO and CPO, KTO expects a specific dataset format , where  represents the source,  denotes the output, and  indicates whether  is preferred or dis-preferred. Thus, KTO does not rely on preferences but rather on knowing the desirability of output . Intuitively, KTO\u2019s loss is determined by a value function  applied to a generated output , with respect to a reference point  and a weighting function  that assigns weights to the loss for preferred and dis-preferred generated outputs. More specifically, for training purposes, the value function is cast as a logistic function ;\nHere,  is set to be the implicit reward under the RLHF objective in Eq 5  ###reference_###;\nwhere  is a scale factor and  is calculated in relation to all pairs  as an approximation of the optimal policy, rather than being computed individually for each pair ;\nUltimately, the KTO loss boils down to the following function:\nwhere  represents two hyper-parameters that weigh the losses for preferred and dis-preferred completions, respectively, as follows:"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem Definition: Machine Language-Molecule Translation (LMolT)",
            "text": "Let  denote the source language space and  denote the target molecule space. Consider a dataset , comprising pairs of source language sequences  represented in space , and their corresponding target molecule sequences  in space , represented as , where  is the total number of pairs. Here, we cast the problem as a cross-modal translation task, aiming to learn an optimal function  through a model  parameterised by . We coordinate the language and molecule spaces through instructional modelling to regulate the translation process across both directions, i.e.,  and  (please refer to Appx. C  ###reference_###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Feedback-Aligned Optimisations Algorithms",
            "text": "Here we investigate approximation policies for feedback alignment optimisation for the LMolT task.\nA model is optimised by minimising the disparity between the generated and intended target outputs. For a model parameterised by , denoted as , the loss function is defined as the negative log-likelihood of source-target pairs , and can be expressed as follows:\nwhere the quality of  is crucial to the effectiveness of the translation model Zhao et al. (2022  ###reference_b43###  ###reference_b43###).\nDPO Rafailov et al. (2024  ###reference_b26###  ###reference_b26###) provides a more human-centric optimisation objective aimed at aligning the translation model  with human intentions. Here  is an offline dataset of comparisons , where  represents the preferred translation (e.g. human gold standard), and  represents the dis-preferred translation (typically synthetic outputs obtained by a translation model). The loss function for DPO is constructed as a maximum likelihood objective for a parameterised policy ;\nwhere  is the fine-tuned translation model used as the reference,  is the Sigmoid function, and  is a hyper-parameter (\u00a7 2  ###reference_###  ###reference_###).\nDPO requires high-quality data or, at the very least, appropriately fine-tuned models. In real-world scenarios, it may not be feasible to acquire high-quality data, especially for challenging domains such as chemistry and biology. CPO is a general approximation of Eq.5  ###reference_###  ###reference_### that addresses the limitations of DPO by training a model to avoid generating outputs that are merely adequate, but not perfect, tested in machine translation Xu et al. (2024  ###reference_b39###  ###reference_b39###). Specifically, in CPO, Eq. 5  ###reference_###  ###reference_### is effectively approximated using a uniform reference model, which assumes equal likelihood for all possible generated outputs;\nEquation 6  ###reference_###  ###reference_### implies that the loss is calculated based on how well generated outputs match this uniform distribution of possible outputs, rather than being biased towards any particular one. To maintain  close to the preferred data distribution, a behaviour cloning (BC) Hejna et al. (2023  ###reference_b13###  ###reference_b13###) regulariser is injected;\nHere,  denotes a small positive constant, and  signifies the Kullback-Leibler divergence. Finally, the regulariser is streamlined with an additional SFT term on the preferred data;\nKTO Ethayarajh et al. (2024  ###reference_b10###  ###reference_b10###) combines the implicit reward under the RLHF objective with the concept of loss aversion from Kahneman-Tversky prospect theory Tversky and Kahneman (1992  ###reference_b34###  ###reference_b34###).\nUnlike DPO and CPO, KTO expects a specific dataset format , where  represents the source,  denotes the output, and  indicates whether  is preferred or dis-preferred. Thus, KTO does not rely on preferences but rather on knowing the desirability of output . Intuitively, KTO\u2019s loss is determined by a value function  applied to a generated output , with respect to a reference point  and a weighting function  that assigns weights to the loss for preferred and dis-preferred generated outputs. More specifically, for training purposes, the value function is cast as a logistic function ;\nHere,  is set to be the implicit reward under the RLHF objective in Eq 5  ###reference_###  ###reference_###;\nwhere  is a scale factor and  is calculated in relation to all pairs  as an approximation of the optimal policy, rather than being computed individually for each pair ;\nUltimately, the KTO loss boils down to the following function:\nwhere  represents two hyper-parameters that weigh the losses for preferred and dis-preferred completions, respectively, as follows:"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Mixed Cross-Modals Fusion",
            "text": "We propose a novel fusion approach that mixes cross-modal capabilities of LLMs excelling in unidirectional LMolT without increasing training costs or data needs. Our approach builds on early work in model merging, which linearly combines weights of pre-trained models into a unified, robust model Wortsman et al. (2022  ###reference_b37###). This addresses challenges related to disparities in the informativeness of different modalities and limitations of optimisation algorithms requiring LLMs trained on extensive data. We have employed the following non-linear fusion algorithms for mixing cross-modal LLMs :\nTrim, Elect Sign & Merge (TIES) Yadav et al. (2024  ###reference_b40###): This method addresses inference challenges caused by parameter interference from merging different models. It involves three steps: (1) resetting parameters that changed minimally during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters aligned with the final agreed-upon sign.\nSpherical Linear Interpolation (SLERP) Shoemake (1985  ###reference_b30###): It is a geometric approach that blends two models while preserving their distinctive characteristics and curvature in high-dimensional spaces. The process consists of three steps: (1) normalising vectors to unit length, prioritising directions over magnitudes, (2) calculating the scale factor, which determines the interpolation factor and the angle between vectors across the fused models, and (3) weighting and summing the vectors to derive the interpolated representation in the fused model."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Evaluation Method for Assessing Hallucination in Chemical LLMs",
            "text": "Previous work has employed embedding representations Jaeger et al. (2018  ###reference_b14###) to assess semantics in chemical-domain models Edwards et al. (2021  ###reference_b7###); Christofidellis et al. (2023  ###reference_b5###). However, these approaches often require domain adaptation for out-of-distribution data Edwards et al. (2024  ###reference_b9###) and might yield opaque and arbitrary outcomes Steck et al. (2024  ###reference_b31###). Here, in addition to performing evaluation according to standard generation metrics, we also introduce fine-grained multi-aspect evaluation criteria for assessing the presence of hallucinations111Hallucination in LLMs refers to a phenomenon where the generated outputs are inaccurate, nonsensical, or contradictory to the provided factual information. in generated outputs.\nSpecifically, for our language-to-molecule translation evaluation, we introduce the following metrics:\n: Measures the deviation in character length between generated and reference molecule pairs.\nChr-F Popovi\u0107 (2015  ###reference_b24###): Utilises the F-score statistic for character n-gram matches between prediction-reference pairs. Here it assesses matches in translated molecules against their references by averaging the scores of unigram, bigram, and trigram matches. A higher Chr-F indicates better performance.\nWin Rate: Determines the percentage of successfully generated molecules within the test subset. Each sample in the test set is considered a win if it meets several strict criteria: a) Chr-F score > 0.3, b) absolute length deviation from the reference < 5, and c) a valid generated output adhering to standard molecular structure conventions222Threshold values are determined experimentally, while validity is automatically assessed using rdkit..\nFor our molecule-to-language translation evaluation, we introduce similar metrics:\n: Calculates the token-length deviation in generated-reference pairs of texts.\nNatural Language Inference (NLI) Williams et al. (2018  ###reference_b36###): We utilise scores from an NLI model333We use NLI-DeBERTa based on  He et al. (2021  ###reference_b12###).. Specifically, we obtain the probabilities of predictions being entailed in references. Higher probabilities indicate better performance.\nWin Rate: Measures the percentage of successfully generated outputs within the test subset. A generated output, in regard to its corresponding reference, is considered a win if classified by the NLI model as entailment rather than neutral or contradiction."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Benchmark Models",
            "text": "We compare our results with established language-molecule models as reported in the literature:\nTxtChem-T5 Christofidellis et al. (2023  ###reference_b5###): A T5XL model trained on both linguistic and molecule modalities with a multi-task objective across various datasets, including the CheBI-20 dataset Edwards et al. (2022  ###reference_b8###), similar to L+M-24.\nChem-LLM Zhang et al. (2024  ###reference_b41###): An InternLM2-Base-7B model, trained on an extensive chemical domain knowledge dataset, with a direct preference optimisation objective Rafailov et al. (2024  ###reference_b26###), achieving results comparable to GPT-4.\nMeditron Chen et al. (2023  ###reference_b4###): A Meditron-7B model fine-tuned on the entire L+M-24 dataset for unidirectional language-molecule translation.\nSFT-Meditron: Our supervised Meditron-7B fine-tuned on a 10% subset of L+M-24 for bi-directional machine language-molecule translation."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "All experiments are conducted using Meditron Chen et al. (2023  ###reference_b4###), a model previously noted for its performance on L+M+24. When investigating optimisation algorithms ( 3.2  ###reference_###), we initialise cross-modals from Meditron trained for unidirectional language-to-molecule translation555Initialisation of cross-modals is based on the task reported as most challenging in Edwards et al. (2024  ###reference_b9###).. In investigations of mixed modality fusion strategies ( 3.3  ###reference_###), we obtain cross-modals from separate Meditron models trained for unidirectional language-to-molecule and molecule-to-language translation, fusing them in a 19:1 ratio666The experimental ratio of cross-modals aims to maintain information integrity across them.. All models are trained with QLoRA Dettmers et al. (2024  ###reference_b6###), with experimental settings detailed in Appx. F  ###reference_###. For evaluation metrics please refer to Appx D  ###reference_###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Quantitative Analysis on Feedback-Aligned Optimisation Algorithms",
            "text": "Table 1  ###reference_### presents a summary of the molecule-to-language translation results. We observe a notable decrease in performance for benchmark models trained on extensive data with SFT when evaluated on unseen (out-of-distribution data). Among the baselines, Meditron exhibited the highest performance, likely due to its training on the entire L+M-24 dataset used in our experiments. However, training with both DPO and KTO results in failure to translate molecules to language when initialised from agnostic cross-modals, such as those trained from language to molecule translation ( 4.3  ###reference_###). This suggests that these optimisation algorithms rely heavily on the model\u2019s suitability for a specific task. Our SFT demonstrated competitive performance when trained on a 10% subset of L+M-24 with a bi-directional LMolT objective, but it underperformed compared to Meditron, trained on the entire dataset, and lacked efficiency (for efficiency details, refer to Appendix G  ###reference_###). By contrast, CPO showed the ability to cope in the agnostic cross-modal setting, achieving up to a 20% performance increase compared to Meditron (see Table 1  ###reference_###).\nPerformance gains in language-to-molecule translation were notably higher when cross-modals were known (see Table 2  ###reference_###). Both DPO and CPO demonstrated similar performance, achieving an increased accuracy of up to 42% compared to Meditron trained on the entire dataset. Conversely, KTO exhibited low performance even in a known cross-modal setting. We suspect that KTO is sensitive to overfitting (refer to Appendix G  ###reference_###).\nIn summary, our experiments revealed that CPO operates independently of cross-modal considerations, whereas DPO cannot cope with an agnostic cross-modal setting. Contrary, KTO displayed signs of overfitting even in a known cross-modal setting. In the following section, we explore fusion approaches aimed at enhancing the capabilities of these algorithms within the cross-modal context."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Quantitative Analysis on Mixed Cross-Modals Fusion",
            "text": "Her we focus on the most effective optimisation algorithms, DPO and CPO (  4.4.1  ###reference_.SSS1###), and investigate TIES and SLERP ( 3.3  ###reference_###) fusion algorithms in the cross-modal context. We thus aim to address disparities between language and molecule modalities and enhance optimisation algorithm performance. Tables 3  ###reference_### and 4  ###reference_### provide an overview of the mixed cross-modal fusion results for LMolT, maintaining consistent training data. Combining DPO with mixed cross-modals via TIES shows promising enhancements in molecule-to-language translation (see  in Table 3  ###reference_###), yet significant performance loss in language-to-molecule translation (see  in Table 4  ###reference_###). Conversely, fusing CPO with mixed cross-modals via SLERP notably improves molecule-to-language capabilities (see  in Table 3  ###reference_###) while minimally impacting language-to-molecule translation performance (see  in Table 4  ###reference_###), showcasing overall gains compared to Meditron trained on the entire dataset. Overall, we consistently observe a trade-off in performance when fusing cross-modals, prompting future exploration of nuanced algorithms for modelling cross language-molecule dynamics."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Evaluation Results on Assessing Hallucination in Chemical LLMs",
            "text": "Hallucinations can negatively impact the usability of LMoT models. Figure 1  ###reference_### illustrates the distribution of metrics introduced in  3.4  ###reference_### for our best-performing models, CPO and SLERP+CPO, in different cross-modal settings compared to Meditron, which was trained on the entire dataset. In the cross-language-to-molecule setting, unlike Meditron, which tends to generate molecules shorter by an average of 27 characters, our models produced molecules with lengths similar to the actual ones. When evaluating Char-F, we observed a substantial rightward shift in distributions (Fig. 1  ###reference_### (A)), indicating that our models are significantly better at capturing uni-, bi-, and tri-modal character-level dynamics in molecules. In the molecule-to-language setting, our models generated language descriptions of similar length to the actual ones, whereas Meditron consistently produced shorter predictions. When measuring entailment probabilities, we observed a decrease in distributions on the left side and an increase on the right side for our models (Fig. 1  ###reference_### (B)), indicating that our models understand underlying concepts rather than merely memorising language patterns. Figure 2  ###reference_### (B) illustrates a case study of this phenomenon. Additional examples can be found in Appendix E  ###reference_###.\n###figure_1### We proceeded by selecting models that competed with Meditron in at least one cross-modal setting and conducted a cross win rate comparison among them, as illustrated in Figure 2  ###reference_###. For language-to-molecule translation, we observed a decline in molecule generation performance for Meditron and SFT when evaluated across multiple criteria, as outlined in  3.4  ###reference_###. Conversely, CPO and SLERP+CPO demonstrated success in the multi-aspect evaluation, although previous SLERP+CPO showed a slight performance decrease when evaluated on single-aspect benchmark metrics (see  in Table 4  ###reference_###). For molecule-to-language translation, the NLI win rates ( 3.4  ###reference_###) of SFT and TIES+DPO contradicted with the performance as reported in Tables 1  ###reference_### and 3  ###reference_###, while CPO and SLERP+CPO exhibited consistency with the evaluation metrics in those tables. Although NLI evaluation generally surpasses statistical methods when assessing generated outputs against expected references, it still faces limitations, including accuracy issues with lengthy texts and dependence on the quality of training dataMcIntosh et al. (2024  ###reference_b21###). In the future, we aim to explore more nuanced methods that utilise appropriate LLMs for evaluating generated language Min et al. (2023  ###reference_b22###).\n###figure_2###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We focuse on the machine language-molecule translation, aiming to empirically tackle the challenges of effectively training robust large language models (LLMs) and addressing the out-of-distribution problem when integrating with scientific modalities. Our approach marks the first endeavour to experiment with human-centric optimisation algorithms in the context of cross-modal representation learning, aligning cross-language-molecule modalities with preferred offline translations. To combat memorisation effects inherent in training large models on extensive datasets, we conduct experiments utilising only 10% of the available data. This strategy yields significant performance enhancements, surpassing the best benchmark model trained on extensive data by a wide margin, thus achieving new state-of-the-art levels. Additionally, we are the first to propose employing non-linear fusion for integrating cross-modal LLMs, addressing challenges related to disparities in the informativeness of different modalities and limitations of optimisation algorithms requiring LLMs trained on extensive data. This approach further enhances performance gains without increasing training costs or data requirements. Finally, we introduce a freely-tied domain evaluation method with multi-aspect criteria for assessing the presence of hallucinations in chemical LLMs. In the future, we aim to investigate more nuanced algorithms for capturing language-molecule dynamics and evaluation methods to promote their responsible use."
        }
    ],
    "url": "http://arxiv.org/html/2405.13984v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4.1",
            "4.4.2",
            "4.4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4.1",
            "4.4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.13984v1",
        "paper_title": "Feedback-aligned Mixed LLMs for Machine Language-Molecule Translation",
        "research_background": "### Paper's Motivation\n\nThe paper is motivated by the critical role of chemistry in developing innovative and cost-effective scientific solutions across various fields, such as drug discovery, advanced materials, and chemical processes. Despite the rapid advancements driven by AI technologies, effective training of large language models (LLMs) that integrate scientific modalities, specifically language and molecules, remains challenging. Existing training methods often rely on low-quality synthetic data and require extensive datasets, making the process inefficient and prone to performance issues.\n\n### Research Problem\n\nThe primary research problem addressed in this paper is the challenge of training robust LLMs for machine language-molecule translation, especially in dealing with the out-of-distribution problem. The paper investigates the effectiveness of innovative training methods, specifically human-centric optimization objectives, in improving the performance of cross-modal LLMs. It aims to demonstrate that high performance can be achieved without excessive data or training costs.\n\n### Relevant Prior Work\n\n1. **Application of NLP and Multimodal Representation Learning in Chemistry:**\n   - Previous work has adapted successful paradigms from natural language processing (NLP) and multimodal representation learning to study molecules by converting their three-dimensional structures into SMILES for symbolic representation (Edwards et al., 2021, 2022; Liu et al., 2023a, b; Luo et al., 2023; Christofidellis et al., 2023).\n   - Approaches involve learning representations in either separate yet coordinated spaces, a joint space, or through hybrid methods.\n\n2. **Challenges in Existing Methods:**\n   - Existing models often struggle due to reliance on low-quality or noisy synthetic data and the need for significantly larger datasets compared to typical NLP tasks (Edwards et al., 2021).\n\n3. **Innovative Training Methods in NLP:**\n   - ALMA demonstrated enhanced multilingual abilities of LLaMA-2 through fine-tuning with non-English monolingual data and supervised fine-tuning with high-quality parallel data, surpassing prior moderate-sized LLMs (Xu et al., 2023).\n   - Reinforcement Learning with Human Feedback (RLHF) has been used to enhance LLM capabilities across multiple NLP tasks (Ouyang et al., 2022).\n   - Recent trends favor closed-form losses operating on offline preferences to simplify RLHF without performance loss (Rafailov et al., 2024; Xu et al., 2024; Ethayarajh et al., 2024).\n\nBy exploring new training paradigms such as Direct Preference Optimization (DPO), Contrastive Preference Optimization (CPO), and Kahneman-Tversky Optimization (KTO), this paper attempts to advance the field significantly by addressing both the quality of data and the training efficiency challenges in integrating language and molecule modalities.",
        "methodology": "The proposed methodology outlines a novel framework for enhancing the capabilities of large language models (LLMs) tailored to machine translation between languages and molecular representations. It challenges the existing heavy reliance on extensive mono-modality datasets and per-task fine-tuning by focusing on the optimization algorithms and modality fusion techniques. Here is a summary of the method and innovations detailed in the methodology section:\n\n### Key Components and Innovations:\n\n1. **Optimization Objective**:\n   - The model is optimized by minimizing the loss between generated and target outputs.\n   - The primary loss function for the model parameters is based on the negative log-likelihood of source-target pairs.\n\n2. **Dynamic Policy Optimization (DPO)**:\n   - DPO introduces a human-centric optimization objective aligning the translation model with human intentions.\n   - The objective uses an offline dataset of comparisons between preferred (human gold standard) and dis-preferred (synthetic outputs) translations.\n   - The loss function combines a maximum likelihood objective for the parameterized policy and the fine-tuned translation model as the reference.\n   - This approach requires high-quality data or well-tuned models, limiting its feasibility in data-scarce domains like chemistry and biology.\n\n3. **CPO - Approximation of DPO**:\n   - CPO addresses DPO's limitations by approximating a uniform reference model.\n   - The loss function in CPO calculates how well-generated outputs match a uniform likelihood distribution rather than preferring specific ones.\n   - A behavior cloning (BC) regularizer is integrated to keep the model aligned with preferred data distribution using Kullback-Leibler divergence.\n   - An SFT term is added to regularize against the preferred data.\n\n4. **Kahneman-Tversky Optimization (KTO)**:\n   - KTO blends implicit rewards under the RLHF objective with loss aversion concepts from Kahneman-Tversky prospect theory.\n   - KTO leverages data in a specific format indicating source, output, and whether the output is preferred or dis-preferred.\n   - It uses a value function and a weighting function to balance the losses for preferred and dis-preferred outputs.\n   - The logistic function defines the value function, with the implicit reward calculated under the RLHF objective.\n\n5. **Overall Architecture**:\n   - The methodology integrates multiple novel optimization approaches, each tailored to align generated outputs more closely with desirable characteristics without needing extensive, high-quality datasets.\n   - The combination of these techniques promises to enhance the performance and applicability of chemical language models, particularly under realistic data constraints.\n\nThis approach signifies a significant departure from conventional methods, introducing a more efficient pathway to developing high-performing chemical models suitable for complex, multi-modal tasks.",
        "main_experiment_and_results": "The main experiment is designed to evaluate the performance of Feedback-aligned Mixed LLMs for the task of machine language-molecule translation on the L+M-24 benchmark dataset. This dataset involves integrating molecular and linguistic modalities and is segmented into four significant application categories: biomedical; light and electricity; human interaction and organoleptics; and agriculture and industry. The dataset is derived from chemical databases, with chemical properties transformed into natural language using a template composition procedure with GPT-4.\n\n### Experiment Setup\n- **Dataset**: \n  - **L+M-24 Benchmark**: Approximately 127,000 training and 34,000 validation language-molecule pairs are provided. \n  - 10% of these subsets are utilized for the training and validation processes.\n  - Triple datasets for optimization comprise preferred outputs (golden references from L+M-24) and synthetically generated dis-preferred outputs using MolT5.\n  \n- **Sampling**: \n  - An additional random selection of 3,000 unseen pairs is conducted for evaluation from a separate dataset, provided as a supplementary resource at the Language + Molecules @ ACL2024 workshop.\n\n### Baselines\n- The preferred outputs are based on golden references from the L+M-24 benchmark.\n- Dis-preferred outputs are synthetically generated using MolT5.\n\n### Evaluation\n- Evaluation is conducted on 3,000 unseen pairs randomly selected from the supplementary dataset.\n  \n### Main Results\n- The specific results of the main experiment are not provided in the excerpt. However, the setup entails assessing the quality and effectiveness of machine language-molecule translation through quantitative and qualitative comparison against the baseline models (golden references and MolT5-generated outputs).\n\nTo summarize the main experiment, the setup involves a well-characterized dataset (L+M-24) and carefully constructed baselines, with the evaluation focusing on unseen pairs to ascertain the model's translation capabilities. The actual performance outcomes would typically involve metrics such as accuracy, relevance, and appropriateness of the translations, but those details need to be referred to in the full experimental results of the paper."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to empirically evaluate the performance of various feedback-aligned optimization algorithms (CPO, DPO, KTO, and SFT) in molecule-to-language and language-to-molecule translation tasks, particularly focusing on their robustness to cross-modality and out-of-distribution data.",
            "experiment_process": "The experiment involves training different benchmark models (Meditron, SFT, DPO, KTO, and CPO) on the L+M-24 dataset. Different subsets (e.g., 10% or the entire dataset) are used to mitigate memorization effects. The models are tested on unseen data to evaluate their translation performance. The evaluation metrics include performance gains and accuracy improvements. Performance is quantitatively measured and compared across models using Tables 1 and 2.",
            "result_discussion": "The key findings revealed that CPO achieved up to a 20% performance increase over Meditron in an agnostic cross-modal setting, while DPO failed in the same setting. SFT showed competitive performance but was less efficient than Meditron. KTO displayed low performance in both settings, likely due to sensitivity to overfitting. CPO demonstrated robustness across different modalities, whereas DPO was limited by cross-modal considerations.",
            "ablation_id": "2405.13984v1.No1"
        },
        {
            "research_objective": "To investigate the efficacy of mixed cross-modal fusion algorithms (TIES and SLERP) in enhancing the performance of optimal algorithms (DPO and CPO) in the cross-modal language-to-molecule and molecule-to-language translation tasks.",
            "experiment_process": "The study applies TIES and SLERP fusion algorithms to combine DPO and CPO with mixed cross-modals. Consistent training data and procedures as outlined in section 3.3 are maintained. Performance metrics for molecule-to-language and language-to-molecule translation are documented in Tables 3 and 4. The algorithms\u2019 effectiveness is assessed based on translation accuracy gains and overall performance changes.",
            "result_discussion": "Fusing DPO with mixed cross-modals via TIES significantly improved molecule-to-language translation but resulted in a performance loss for language-to-molecule translation. On the other hand, SLERP fusion with CPO enhanced molecule-to-language translation while only minimally impacting language-to-molecule translation performance. This showcases overall gains compared to Meditron but also highlights a persistent trade-off in performance, which calls for future exploration of nuanced fusion algorithms.",
            "ablation_id": "2405.13984v1.No2"
        }
    ]
}