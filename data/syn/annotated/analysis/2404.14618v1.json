{
    "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
    "abstract": "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have become the dominant force in natural language processing in recent years [Zhao et al., 2023  ###reference_b44###]. Their impact has been especially striking in generative applications where it has extended beyond standard language understanding and question-answering benchmarks like [Hendrycks et al., 2020  ###reference_b12###, Srivastava et al., 2022  ###reference_b33###] to several successful real-world deployments. These include the wildly popular ChatGPT [OpenAI, b  ###reference_b28###] and several other chatbots [Zheng et al., 2023  ###reference_b45###] powered by different LLMs [Taori et al., 2023  ###reference_b35###, Touvron et al., 2023  ###reference_b36###, OpenAI, 2023  ###reference_b30###], which allow users to engage in natural language conversations and obtain informative responses on a range of practically useful tasks like creative writing, translation, code completion, etc. An important added attraction of these models is their accessibility. Users can input queries and receive responses in natural language, without any specialized data or code, and this is what has created such a widespread demand for their services across regions, professions, and disciplines.\nThe best performing LLMs are based on the transformer architecture of [Vaswani et al., 2017  ###reference_b40###] and generally have tens of billions of parameters. E.g., Alpaca [Taori et al., 2023  ###reference_b35###] has 13 billion parameters, the best version of Llama-2 [Touvron et al., 2023  ###reference_b36###] has 70 billion parameters, and OpenAI\u2019s GPT-3.5 [OpenAI, a  ###reference_b27###], and GPT4 [OpenAI, 2023  ###reference_b30###] are rumored to be much larger. Their enormous size and the autoregressive nature of text generation in their transformer architectures means that these models typically have a high compute and memory requirement that can only be met by expensive cloud servers [Yu et al., 2022  ###reference_b42###]. This can potentially impose an enormous cost on developers and users as more LLM-based services are introduced. In response to this there has been a surge of interest in designing smaller, cost-effective LLMs \u2013 e.g., [Touvron et al., 2023  ###reference_b36###] provides multiple versions of Llama-2, with the smallest having only 7 billion parameters, small enough to run on a laptop111https://github.com/microsoft/Llama-2-Onnx, while the smallest offering of Google\u2019s Palm-2 model can even run on mobile devices222https://blog.google/technology/ai/google-palm-2-ai-large-language-model/. However empirical evaluations in [Chung et al., 2022  ###reference_b6###, Touvron et al., 2023  ###reference_b36###] as well as our own evaluation in Figure 1(a)  ###reference_sf1### show that smaller models generally lag behind in terms of response quality.\n###figure_1### ###figure_2### ###figure_3### Faced with this tradeoff between response quality and inference cost, we propose a hybrid inference approach which provides the best of both worlds. Our approach is motivated by the observation that most tasks for which LLMs are useful, like creative writing, translation, code completion, etc., include a range of queries of different difficulty levels and there is always a subset of \u201ceasy\u201d queries for which responses of a small (inexpensive and weak) model may be comparable to, and sometimes even better than those of a large (expensive and powerful) model. This is also illustrated in Figure 1(b)  ###reference_sf2### where we plot the tail of the quality gap (defined in Section 3  ###reference_###) between the 13 billion parameter version of Llama-2 and OpenAI\u2019s GPT-3.5-turbo, the model that powers ChatGPT. Quality gap is non-negative for examples where the response quality of Llama-2 is comparable to or better than that of GPT-3.5-turbo which is the case for around 20% queries in our dataset (described in Section 4  ###reference_###).\nWe leverage this insight to train a router that takes a large model and a small model as input, and learns to identify these easy queries as a function of the desired level of response quality, while taking into account the generative nature of tasks, inherent randomness in LLM responses, and response quality disparity between the two models. At test time, the router seamlessly adjusts to different response quality requirements and assigns the corresponding \u201ceasy\u201d queries to the small model, leading to significant inference cost reduction with minimal drop in response quality. In Figure 1(c)  ###reference_sf3### our router assigns  of queries to Llama-2 (13b) 333We term the fraction of queries routed to the small model as the cost advantage (see \u00a72.3  ###reference_###) with less than  drop in response quality measured in BART scores [Yuan et al., 2021  ###reference_b43###]. The gains are even higher for pairs where the small model is closer in terms of response quality to the large model (see Section 4  ###reference_###).\nWith the explosion in the complexity and costs of LLM deployments, small companies and individual consumers, have started to rely on the pre-existing LLMs hosted on platforms like HuggingFace [Face,  ###reference_b9###] and OpenAI [OpenAI, c  ###reference_b29###]. This is an instance of the broader Machine-Learning-As-A-Service (MLaaS) paradigm, wherein users (small companies/individual consumers) interact with the models through an API where they submit their queries [Kang et al., 2022  ###reference_b19###] and have limited visibility into the models themselves. In this context, our hybrid inference approach can reduce the costs incurred by both consumers and platform owners because a) consumers can use it to route easy queries to small models hosted on their edge devices (laptops/smartphones) and only call the API for the more complex queries (illustrated in Figure 2  ###reference_###) and b) platform owners can automatically route queries to lower cost models at the backend without affecting the user experience, as long as the response quality levels are maintained. Thus our hybrid inference approach offers a flexible and cost-effective solution for harnessing the full potential of LLMs while accommodating diverse cost budgets and quality requirements.\nThe main technical contributions of this work are: a) we are the first to explore cost-effective and quality-aware hybrid LLM inference, b) we design a novel query router which routes queries based on an estimate of the response quality gap between models (Section 3.1  ###reference_###), c) we incorporate uncertainty due to randomness in LLM responses in our router design to improve performance (Section 3.2  ###reference_###), d) we identify challenges for our router when the small model is significantly weaker than the large model and introduce a novel data transformation to address this issue (Section 3.3  ###reference_###), and e) we provide extensive experimental results (Section 4  ###reference_###) on a large benchmark dataset of real world natural language queries and responses [Jiang et al., 2023  ###reference_b17###] thereby demonstrating the value of the approach and its superiority over baseline approaches, enabling LLM providers and consumers to cost-efficiently enable LLM-backed experiences.\n###figure_4###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Problem Formulation",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Related Work",
            "text": "Large Language Models (LLMs). The advent of LLMs has led to a paradigm shift in the study of natural language processing (NLP), computer vision, information retrieval, and other domains[Menghani, 2023  ###reference_b26###, Chen et al., 2023  ###reference_b5###, Jiang et al., 2023  ###reference_b17###]. The impressive effectiveness and generalizability of LLMs has come at the price of a drastic increase in LLM sizes [Treviso et al., 2023  ###reference_b37###] and consequent challenges, including huge amounts of computational resources and data required to train, and prohibitive expenses at both training and deployment stages [Bender et al., 2021  ###reference_b1###].\nEfficient Machine Learning (ML) Inference. LLMs belong to a class of models called foundation models [Bommasani et al., 2021  ###reference_b3###] \u2013 models that are trained once and can then be used to serve a wide variety of tasks. As such, we expect inference cost to dominate the overall cost of such models and hence focus on works that reduce the cost of ML inference [Menghani, 2023  ###reference_b26###]. The most common approach for efficient ML inference is model compression i.e., replacing a large model with a smaller model of comparable accuracy. Common techniques for model compression include (i) model pruning [Hassibi et al., 1993  ###reference_b10###, LeCun et al., 1989  ###reference_b22###] which drops parts of the model with minimal accuracy loss, (ii) quantization [Jacob et al., 2018  ###reference_b16###, Vanhoucke et al., 2011  ###reference_b39###] which reduces model memory footprints and inference latency by reducing the precision of data representation (e.g., FP32 to INT8), (iii) knowledge distillation [Hinton et al., 2015  ###reference_b13###, Urban et al., 2016  ###reference_b38###] which trains small student models to mimic large teacher models, and (iv) Neural Architecture Search [Elsken et al., 2019  ###reference_b8###, Zoph and Le, 2016  ###reference_b46###] which tunes model architecture to improve model performance, under inference cost constraints. Such static efficiency optimizations typically produce a fixed model with lower inference cost and lower accuracy compared to the large model which may not suffice for foundation models like LLMs, whose core premise is that the same model will serve a range of tasks, each with its own accuracy/cost constraints. This is already manifesting in inference platforms described in Section 1  ###reference_### which need more dynamic optimizations to meet the demands of all users.\nHybrid ML Inference. Recent works [Kag et al., 2022  ###reference_b18###, Ding et al., 2022  ###reference_b7###] have introduced a new inference paradigm called hybrid inference which uses two models of different sizes instead of a single model for inference. The smaller model (e.g. Llama2 [Touvron et al., 2023  ###reference_b36###]) generally has lower inference cost but also lower accuracy than the larger model (e.g. GPT-4 [OpenAI, 2023  ###reference_b30###]). The key idea is to identify and route easy queries to the small model so that inference cost can be reduced while maintaining response quality. By tuning a threshold on query difficulty we can dynamically trade off quality and cost for the same inference setup. [Kag et al., 2022  ###reference_b18###] study this setup for image classification and propose to train the small model, large model, and router from scratch. However LLM training is expensive and retraining LLMs from scratch for every scenario goes against the very premise of inference with pre-trained foundation models. Moreover text generation [Iqbal and Qureshi, 2022  ###reference_b15###] is often more ambiguous and challenging than image classification due to which novel techniques are required for effective hybrid LLM inference for text generation.\nInference with Multiple LLMs. Some recent works [Jiang et al., 2023  ###reference_b17###, Chen et al., 2023  ###reference_b5###, Leviathan et al., 2023  ###reference_b23###, Kim et al., 2023  ###reference_b20###] use multiple LLMs for inference but these approaches typically call more than one LLM for a single query that can incur significant computational overheads. Specifically [Jiang et al., 2023  ###reference_b17###] calls an ensemble of LLMs at inference time due to which the inference cost will be proportional to the number of models in the system. [Chen et al., 2023  ###reference_b5###] performs inference using a cascade of LLMs where responses to the query are generated sequentially by the LLMs in the cascade until one of the models has a confidence score higher than a predefined threshold. Our work provides high quality responses while always making a single LLM call for all queries and will thus incur much lower computational cost than both of these works on average. Speculative decoding, introduced in [Leviathan et al., 2023  ###reference_b23###, Kim et al., 2023  ###reference_b20###] speeds up decoding of expensive models by invoking small-and-efficient decoders on the \u201ceasy\u201d decoding steps. Instead, in our work we are interested in query routing which assigns \u201ceasy\u201d queries to small models to reduce overall inference costs while maintaining high performance. While the two approaches have different goals, an interesting line of future work would be to combine these so that our router assigns queries to the small or large model based on query difficulty and then speculative decoding is applied on top to speed up inference for queries assigned to the large model thereby leading to further cost reduction."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Problem Setting",
            "text": "We extend the hybrid ML paradigm to LLM inference by routing queries between two models with different inference costs and accuracy. This allows platforms [Face,  ###reference_b9###, OpenAI, c  ###reference_b29###] to route queries across backend LLMs to lower costs while dynamically tuning the ratio of queries assigned to each model as per user quality requirements. It also allows users with small models on local (edge) devices to only call the platform for hard queries (Figure 2  ###reference_###), thus significantly reducing their expenses.\nWe use  and  to denote the input query space and the set of all possible output responses respectively. Let  denote the large model and  denote the small model. Formally, the objective in our paradigm is to learn a router  such that each user query  is routed to the small model  if , and to the large model , otherwise. Note that we always route each query to a single LLM at inference time as opposed to using an ensemble [Jiang et al., 2023  ###reference_b17###] or a cascade [Chen et al., 2023  ###reference_b5###] of LLMs, which may call multiple LLMs to resolve a single query and incur significant computational overheads."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Evaluation Metric",
            "text": "Response Quality\nAutomatic evaluation for text generation is a challenging and widely studied problem. Traditional metrics, such as BLEU and ROUGE, initially designed for machine translation and summarization, have been found to be of limited concordance with human judgment and restricted applicability across diverse NLP tasks [Blagec et al., 2022  ###reference_b2###].\nSignificant research efforts have been devoted to implementing task-agnostic evaluation metrics with neural networks. GPT-ranking [Jiang et al., 2023  ###reference_b17###], as a representative example, employs GPT models (e.g., GPT-4 [OpenAI, 2023  ###reference_b30###]) to provide relative rankings between pairs of generated outputs. In spite of the high correlation with human perception, GPT-ranking suffers from high computational costs and inability to distinguish between examples with the same ranking. Instead, we use the BART score [Yuan et al., 2021  ###reference_b43###] to evaluate response quality of different models since (1) it is inexpensive to compute in comparison to LLM-based metrics such as GPT-ranking, and (2) it has been shown in prior work [Jiang et al., 2023  ###reference_b17###] that this metric correlates well with the ground truth. We also provide a case study in Appendix C.2  ###reference_### to empirically justify using BART score as the response quality metric.\nWe use  to denote the BART score (response quality) of model responses .\nCost Advantage\nThe absolute costs of running a model may not be known a priori, and may be expressed using a variety of metrics, including latency, FLOPs, energy consumption, etc. In LLM inference, however, each of these metrics is affected by several underlying confounders such as different prompt templates, hardware capability, network connectivity, etc. Moreover different platforms/users may be interested in different metrics. However the common underlying assumption in this and previous works on efficient ML inference is that smaller models are more efficient than larger models and therefore we expect to obtain an improvement in all the metrics by routing more queries to the smaller model. Hence we define cost advantage as the percentage of queries routed to the smaller model. Note that the notion cost advantage has been used as a generic efficiency metric in previous hybrid ML work [Kag et al., 2022  ###reference_b18###], where it is termed as coverage."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Hybrid LLM Inference",
            "text": "Easy Queries.  We refer to queries for which the response quality of the small model is close to the response quality of the large model as \u201ceasy\u201d queries. The goal of our hybrid inference framework is to identify the easy queries and route them to the small model thereby ensuring significant inference cost reduction without much drop in response quality. Note that the easy queries as defined here, need not necessarily be queries that are easy/inexpensive to respond to, they are just queries for which the small model can match up to the large model. Examples of easy and hard queries as per this definition are provided in Section C.1  ###reference_###.\nQuality Gap. We define quality gap of a query  as  i.e. the difference in quality of the small model\u2019s response  and the large model\u2019s response . The quality gap is a random variable since LLM responses are typically non-deterministic. This is illustrated in Figure 3  ###reference_### below where the blue and orange plots correspond to the distribution of responses from FLAN-t5 (800m) 444We use the FLAN-t5-large model from https://huggingface.co/google/flan-t5-large. [Chung et al., 2022  ###reference_b6###] and Llama-2 (13b) [Touvron et al., 2023  ###reference_b36###] for a single query.\n###figure_5### Proposed Orchestration Framework. Queries are routed using a BERT-style encoder model (e.g., DeBERTa, [He et al., 2020  ###reference_b11###]) which is trained on a dataset of representative queries and learns to predict a score. Since the router is an encoder model, a single pass of the query through it is sufficient to generate the score and we assume that the cost of this step is negligible compared to the cost of running autoregressive decoding using the large model  [Sun et al., 2019  ###reference_b34###]. Thus, we expect that using the router to route queries to the small model will not detract significantly from the realizable cost advantage.\nRouter Score. We design the router score to be large for easy queries as defined above. Intuitively, an estimate of  is a suitable candidate since a large value of  corresponds to queries for which there is a high likelihood that the response quality of the small model will be at least as high as that of the large model. However we show below that in scenarios where the large model is significantly more powerful than the small model i.e.  in general, one can train more effective routers by relaxing the definition of easy queries to  for an appropriate . At test time we achieve the desired performance accuracy tradeoff by tuning a threshold on the score and routing queries with score above the threshold to the small model. For a router with parameters , we denote router score by . We discuss different router score designs in the rest of this section assuming a training set of  queries ."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Deterministic Router",
            "text": "Previous work on hybrid ML [Ding et al., 2022  ###reference_b7###, Kag et al., 2022  ###reference_b18###] makes the assumption that neural models are deterministic functions that map input features to a single point in the output space. To realize this for LLMs, we sample a single response per query from each model. We assign boolean labels  to each training query with the BART score as the quality function . Our router is trained by minimizing the binary cross-entropy loss [Ruby and Yendapalli, 2020  ###reference_b31###].\nObserve that the assigned labels  can be viewed as an estimate for  given a single response per query from each model and thus minimizing the above loss encourages the router score  to be close to  for test queries. We refer to this deterministic router as ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Probabilistic Router",
            "text": "The determinism assumption can be justified for tasks where the ground truth labels are often explicit and unique such as image classification [Masana et al., 2022  ###reference_b25###] and video segmentation [Yao et al., 2020  ###reference_b41###]. When it comes to NLP tasks, however, there is usually no single best answer due to the intrinsic ambiguity and complexity of natural languages. LLMs are widely used as non-deterministic generators to capture the intrinsic uncertainty of NLP tasks, as shown in Figure 3  ###reference_### (ignore the dashed curve for now). The non-determinism mainly comes from the randomness in the decoding phase. Users typically control the level of uncertainty by choosing different decoding strategies such as nucleus sampling [Holtzman et al., 2019  ###reference_b14###], as well as the values of the hyper-parameter temperature. Intuitively, higher temperature values result in a higher level of randomness and diversity among the generated responses. For black-box LLM APIs such as GPT-4 [OpenAI, 2023  ###reference_b30###], it has been observed that even upon setting temperature to the minimum value , it can still provide different responses for the same input queries. The underlying mechanism is still an open problem while a recent study hints at the instability of the MoE backbone [Skyward, 2023  ###reference_b32###].\nWe propose to incorporate the uncertainty due to the non-deterministic nature of LLM comparisons into the router training loss by relaxing the hard labels  to the soft labels  where  denotes the expectation. In practice, we estimate expectation by sampling  responses from each model and computing the sample average of the corresponding indicator function values. Observe that the hard label  is a higher-variance estimate of  (since it is obtained from a single sample) and hence we expect improved performance with the following training loss,\nWe refer to this probabilistic router as ."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Probabilistic Router with Data Transformation",
            "text": "###figure_6### ###figure_7### ###figure_8### While so far we have designed scores that try to estimate , we observe that the empirical estimate of  tends to be extremely small when the large model is significantly more powerful than the small model (0 for almost  of the queries in Figure 4(a)  ###reference_sf1### with Flan-t5 (800m) as the small model and Llama-2 (13b) as the large model). Because  for most queries in this case, it provides an extremely weak signal for training using Equation 2  ###reference_### and as shown in Section 4  ###reference_### both  and  fail to provide much improvement over random query assignment in this case.\nTraditional approaches for learning with imbalanced data have their own shortcomings [Krawczyk, 2016  ###reference_b21###]. Moreover our goal is to only design a router that can reduce inference cost while maintaining response quality as much as possible and so we are not tied to a particular definition of class labels to achieve this. We leverage this flexibility to introduce new labels  for some . Since ,  by definition of the tail distribution and so we expect this relaxation to provide a stronger signal for router training while still allowing us to identify the easy queries i.e. those queries for which  has a high likelihood of being close to  (). Visually, this corresponds to comparing the distribution of the small model\u2019s response with a shifted distribution of the large model\u2019s response to a query (dotted curve in Figure 3  ###reference_###).\nNow the question is how to choose the best relaxation ? Given that tail probability  lies in , we choose  by maximizing the average pairwise differences between the transformed labels to push them as far apart as possible and provide a strong signal for training. Thus we set,\nWe currently solve the above optimization problem via grid-search and leave more sophisticated approaches for future work. We plot the optimization objective for different values of  for our training dataset in Figure 4(b)  ###reference_sf2### and show the distribution of transformed labels  in Figure 4(c)  ###reference_sf3###. As we see, the distribution is significantly more balanced now and we expect the resulting router to be much more effective. Once again, we train the router by minimizing the loss\nand we refer to this probabilistic router as ."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Evaluation Setup",
            "text": "Dataset. We use the MixInstruct dataset from [Jiang et al., 2023  ###reference_b17###] to evaluate the effectiveness of different routing strategies across a wide range of tasks (e.g., question answering, summarization, information extraction). MixInstruct is a large-scale collection of real-world instructions and consists of examples from four public datasets (see Table 5  ###reference_### in Appendix B  ###reference_###). The broad range of tasks in the dataset enables us to train a generic router that will be effective across different scenarios. We uniformly sample k training examples from the training split of MixInstruct, for each of which we generate  responses from all LLMs under consideration. Our validation and test splits are the same as the MixInstruct dataset, which consist of k instruction examples separately.\nRouter Model. We use DeBERTa-v3-large [He et al., 2020  ###reference_b11###] (300M) as the backbone to train our routers.\nWe train each router with the corresponding loss from Section 3  ###reference_### for  epochs and use the validation set to choose the best checkpoints for final evaluation. All experiments are conducted with \nNVIDIA A100 GPU of 80GB GPU RAM.\nWe have made our source code available at https://github.com/m365-core/hybrid_llm_routing.\nEvaluation Measures. We use BART score [Yuan et al., 2021  ###reference_b43###] as the quality metric and use fraction of queries routed to the small model (cost advantage) as the efficiency metric (see Section 2.3  ###reference_###).\nBaselines. To the best of our knowledge, there has been no prior work specifically on routing between LLMs. We consider three straightforward baselines: all-at-large, all-at-small, and random.\nAll-at-large routes all queries to the large model, while all-at-small routes all queries to the small model.\nRandom generates a random number in [0,1] and selects the large model if it is below the probability threshold.\nExperiments. We investigate all three routers (see Section 3  ###reference_###): the deterministic router , the probabilistic router , and the probabilistic router augmented with data transformation . We select candidate model pairs from FLAN-T5 (800m), FLAN-T5 (11b), Llama-2 (7b), Llama-2 (13b), and GPT-3.5-turbo for our experiments. At test time the trained router (, , or ) takes a threshold value as input and routes all queries with router score higher than the threshold to the small model as these are the easy queries. We evaluate the router performance in Section 4.2  ###reference_### in terms of both BART score and cost advantage (Figure 5  ###reference_### and Table 1  ###reference_###), validate that the router is indeed routing easy queries to the small model in Section 4.3  ###reference_###, demonstrate that our routers are of negligible compute overhead in Section 4.4  ###reference_###, show how to choose routing thresholds in practice in Section 4.5  ###reference_###, evaluate the effectiveness of our routers using a response quality metric other than the BART score in Section 4.6  ###reference_###, and test the generalizability of routers across model pairs in Section 4.7  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Router Validation Results",
            "text": "###figure_15### ###figure_16### ###figure_17### We also validate that the router is functioning as intended, that is, routing easy queries to the small model and hard queries to the large model. To see this, in Figure 6  ###reference_### we plot the difference between the average quality gaps of queries routed to the small model and those routed to the large model for our router and the random baseline v/s different values of cost advantages (i.e., the fraction of queries routed to the small model). Since the random baseline randomly assigns queries the average difference is nearly always zero. However our router routes easy queries i.e. queries with large quality gap () to the small model and queries with small quality gap to the large model. Hence the difference between the average quality gaps always has a significant positive value indicating that more easy queries are routed to the small model than to the large model in our approach as compared to the random assignment approach at all cost advantages."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Router Latency",
            "text": "We measure the latency of our router and compare it to the latency of the different LLMs \u2013 Flan-t5 (800m), Llama-2 (7b), and Llama-2 (13b) that we use in our experiments for generating responses to user queries. Note that the latency of all the routers , , and  will be the same since they use the same model (DeBERTa-v3-large [He et al., 2020  ###reference_b11###]) and are just trained differently. Also, we do not measure the latency of GPT-3.5-turbo since its responses are generated by querying the OpenAI API [OpenAI, c  ###reference_b29###] as the model weights are not publicly available due to which it is not possible to disentangle the inference latency from the network latency, queueing delay, latency of the API call, etc. However we note that the inference latency of all other LLMs we consider is significantly larger than that of the router (see Table 2  ###reference_###) and therefore we expect the same to hold for GPT-3.5-turbo as well.\nThe latency results are reported in Table 2  ###reference_### where we measure the average latency per query averaged over 200 randomly chosen queries from our dataset (confidence bounds correspond to one standard error). As expected the router processes queries significantly faster than all the LLMs (nearly  faster than the fastest LLM \u2013 FLAN-t5(800m)). This is both due to its smaller size (300m parameters) and the fact that it performs a single forward pass over the query to generate the score while the LLMs generate the response token-by-token in an autoregressive fashion due to which the inference latency is proportional to the response length. Thus the router adds minimal overhead to the inference cost due to its small size and extremely low latency."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Empirical Determination Of Routing Threshold",
            "text": "Recall that at test time the model owner is required to set a threshold on the router score which serves to separate the easy queries from the hard ones (see Section 3  ###reference_###). All queries with router score higher than the threshold will be routed to the small model. Thus the threshold is a user-defined parameter controlling the achieved efficiency-performance trade-off, to best serve the interests of different users. In this section we show how to empirically choose thresholds on router scores to achieve cost reduction with little to no performance drops. For this, we use a small calibration set to recommend default thresholds to users. We investigate all three routers , , and  with different LLM pairs that we use in our experiments. For each LLM pair, we randomly draw  samples from the validation set and use grid search to determine the threshold that delivers the highest cost advantages i.e., cost savings on the validation set while keeping the performance drop (reduction in BART score) less than . The limit on performance drop can be adjusted as per user requirements. With the selected thresholds, we report the achieved performance drops and cost advantages on the test sets, as summarized in Table 3  ###reference_###.\nAs seen from the table the performance and the cost advantage obtained on the test sets closely follows that on the validation sets for all categories of LLM pairs. This clearly illustrates that a threshold chosen on the validation set generalizes well to the test set. We note that there is a slight increase in the performance drop from the validation to the test set for the LLama-2 (7b) and Llama-2 (13b) pair, i.e the LLM pair with small performance gap as per the categorization in Section 4  ###reference_###. However this is also the pair with the highest cost advantage or cost savings ( for all routers) and thus the issue can be addressed by just using a more conservative limit on the performance drop while choosing the threshold which would still lead to very significant cost savings."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Alternate Evaluation Metrics",
            "text": "To provide a more comprehensive evaluation of our routers, we test the routing performance with metrics in addition to BART score [Yuan et al., 2021  ###reference_b43###]. GPT-4-based evaluators have been found to be well correlated with human assessments [Liu et al., 2023  ###reference_b24###, Chase, 2022  ###reference_b4###]. We generate GPT-4 evaluation scores (integer ratings from  to ) for test responses from Flan-t5 (800m), Llama-2 (7b), Llama-2 (13b), and GPT-3.5-turbo that we investigate in our experiments, using LangChain scoring evaluator [Chase, 2022  ###reference_b4###].\nRecall that our routers are trained with BART score due to efficiency and effectiveness reasons as discussed in Section 2.3  ###reference_###.\nIntuitively, if the quality gaps measured by BART score and GPT-4 score are highly correlated, we could expect good routing performance even under the GPT-4 score as we have seen in Section 4.2  ###reference_###.\nWe compute the correlation between quality gaps measured by BART score and GPT-4 score, and report it along with routing performance evaluated with GPT-4 score, as shown in Figure 7  ###reference_###.\nAligned with our intuition, when the two metrics are well correlated (Figure 7(a)  ###reference_sf1###), our routers trained with BART score are still effective even when evaluated against GPT-4 score. Typically, , , and  are able to achieve  cost advantage with up to  performance drop, and  cost advantage with up to  performance drop. As the correlation gets weaker, the router performance gradually decays, as shown in Figure 7(b)  ###reference_sf2### and 7(c)  ###reference_sf3###. This observation suggests a simple-yet-effective strategy of using BART score in practice to save labelling costs while maintaining routing performance. We can first compute the correlation between BART score and the target metrics (e.g., human assessments) using a small sample and use BART score as training labels whenever there is strong positive correlation with target metrics.\n###figure_18### ###figure_19### ###figure_20###"
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Generalizing To Different Model Pairs",
            "text": "We evaluate the generalizability of our routers by testing their routing performance on LLM pairs different than the pairs they were trained with. We compute the correlation between quality gaps of training and testing LLM pairs, and report it along with routing performance, as shown in Figure 8  ###reference_###.\nSimilar to our observation in Section 4.6  ###reference_###, our routers can generalize well if the quality gaps of testing LLM pairs exhibit strong positive correlation with the quality gaps of the training pairs. In Figure 8(a)  ###reference_sf1###, both pearson and spearman correlation coefficients exceed , and all three routers are able to achieve  cost advantage with up to  performance drop, and  cost advantage with up to  performance drop. As the correlation becomes weaker, the generalizability of our router gets restricted and routing performance decays, as shown in Figure 8(b)  ###reference_sf2### and 8(c)  ###reference_sf3###. This observation sheds light on using the quality gap correlation as an effective indicator to decide if our routers can be applied to new LLM pairs in the early stage. Given a pair of LLMs (source pair) and a router trained on this pair we can measure the correlation between the quality gap of the source pair and the quality gap of any new target pair of LLMs to decide if the router will be effective on the target pair.\n###figure_21### ###figure_22### ###figure_23###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion and Conclusion",
            "text": "Motivated by the need to optimize the trade-off between LLM inference costs and response quality, we have presented a hybrid inference approach based on quality-aware query routing. We train a router to discriminate between \u201chard\u201d and \u201ceasy\u201d queries, enabling the LLM provider to make cost-efficient decisions about which model should serve a given query. Our experimental results on a variety of state-of-the-art LLMs of varying sizes show that such an optimization is possible and that we can realize cost advantages of up to 40% with no significant drop in response quality.\nTo the best of our knowledge, this is the first work exploring the possibilities of cost-effective and quality-aware query routing between LLMs. We identify several important extensions for future work:\n(1) Task-aware routing. Our current routers make routing decisions purely based on query inputs. To improve routing effectiveness, we can provide more informative signals which help routers distinguish easy queries from the hard ones, such as task labels for query examples and can also identify tasks which may be more suited to routing for a given pair of LLMs.\n(2) Generalizing to -model routing. Modern MLaaS platforms typically host a large number of LLM instances of the same or different configurations to efficiently serve users in different scenarios. This naturally forms a more challenging routing problem with richer optimization opportunities (e.g., load balancing)\n(3) Out-of-distribution (OOD) generalization. In this work, the model pair and data distribution is fixed across training and testing. In the real-world it may be cumbersome/infeasible to train a new router for every new model pair and for every new data distribution. Therefore there is a need for techniques to generalize our approach to changes in the model pair and/or data distribution at test time.\n(4) Novel evaluation metrics. Effective evaluation metrics are critical to train high-quality routers. It is intriguing to see how to develop metrics of higher human-judgment correlation and to which extent it will improve the routing performance."
        }
    ],
    "url": "http://arxiv.org/html/2404.14618v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6",
            "4.7"
        ]
    },
    "research_context": {
        "paper_id": "2404.14618v1",
        "paper_title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
        "research_background": "### Motivation and Research Problem\n\nThe paper addresses a practical and pressing issue in the deployment of large language models (LLMs)\u2014balancing the cost of inference with the quality of responses. Although LLMs such as GPT-3.5 and GPT-4 provide exceptional performance, their deployment incurs significant computational expense due to high compute and memory requirements. This cost factor can be prohibitive for many developers and users, especially as LLM-based services proliferate. On the other hand, smaller LLMs have been developed as more cost-effective alternatives, but they generally lag behind in response quality. Given this tradeoff, the paper proposes a hybrid inference approach to route queries effectively between large and small models, thereby optimizing both cost and quality.\n\n### Research Problem\n\nThe central research problem addressed in the paper is the development of a cost-efficient and quality-aware query routing system for LLMs. The problem involves designing a mechanism that can automatically:\n1. Distinguish between \"easy\" and \"hard\" queries.\n2. Route \"easy\" queries to smaller, cost-effective models without a significant loss in response quality.\n3. Route \"hard\" queries to larger models to ensure high-quality responses.\n\n### Prior Work\n\nThe paper builds on an extensive body of prior work. Relevant references include:\n- **Large Language Models and Transformer Architectures:** LLMs, especially those based on the transformer architecture outlined by Vaswani et al. (2017), are instrumental in modern natural language processing tasks. Mentioned models like Alpaca, Llama-2, and OpenAI\u2019s GPT-3.5 and GPT-4 highlight the diversity and scalability of transformers in practice.\n- **Empirical Evaluations of Model Quality:** Prior evaluations, such as those by Chung et al. (2022) and Touvron et al. (2023), demonstrate the performance tradeoffs between smaller and larger models, underpinning the motivation for a hybrid approach.\n- **MLaaS and API Usage:** The paper also references the growing reliance on Machine-Learning-As-A-Service platforms (MLaaS), such as those offered by HuggingFace and OpenAI, wherein users interact with the models through APIs, stressing the relevance of cost-effective solutions for query routing.\n  \nThese prior works underscore the challenges and opportunities in balancing response quality with inference cost, laying the groundwork for the novel hybrid approach proposed in this paper.",
        "methodology": "**Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing**\n\n**Methodology:**\n\n**Easy Queries:** \nWe categorize queries for which the response quality of a small model nearly matches the response quality of a large model as \u201ceasy\u201d queries. Our hybrid inference framework is designed to identify these easy queries and direct them to the small model, thereby achieving a significant reduction in inference costs with minimal loss in response quality. It's important to note that easy queries, as defined here, do not necessarily refer to queries that are simple or inexpensive to respond to, but rather to those for which the small model\u2019s performance is comparable to the large model\u2019s. Examples of easy and hard queries as defined by this criterion are provided in Section C.1.\n\n**Quality Gap:** \nThe quality gap of a query is defined as the difference in response quality between the small model and the large model. Given that LLM responses are typically non-deterministic, the quality gap is treated as a random variable.\n\n**Proposed Orchestration Framework:** \nTo route queries, we use a BERT-style encoder model (e.g., DeBERTa). This encoder model is trained on a dataset of representative queries to predict a score. Since the router operates as an encoder, a single pass of the query through it is sufficient to generate the score, and the cost of this step is assumed to be negligible compared to conducting autoregressive decoding with the large model. Consequently, utilizing the router to direct queries to the small model does not significantly detract from the potential cost savings.\n\n**Router Score:** \nThe router score is designed to be high for easy queries. An estimate of the quality gap fits this criterion since a high score indicates a high probability that the small model\u2019s response quality will be at least as good as the large model\u2019s response quality. However, in scenarios where the large model is notably more powerful than the small model, more effective routers can be trained by relaxing the definition of easy queries. Specifically, for an appropriate constant \\(\\epsilon\\), queries can be defined as easy if \\(\\Delta q < \\epsilon\\). During testing, the desired performance-accuracy tradeoff is achieved by adjusting a threshold on the score, routing queries with a score above this threshold to the small model. For a router with parameters \\(\\theta\\), the router score is denoted by \\(S_{\\theta}(q)\\). The section proceeds to discuss different router score designs, given a training set of \\(M\\) queries \\(\\{q_1, q_2, \\ldots, q_M\\}\\).",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Dataset\nThe main experiment utilizes the MixInstruct dataset from Jiang et al. (2023) to evaluate different routing strategies across a variety of tasks, such as question answering, summarization, and information extraction. MixInstruct is a large-scale collection of real-world instructions, drawing examples from four public datasets. For the experiment, a uniform sample of \\( k \\) training examples is taken from the MixInstruct training split. Each example is used to generate responses from all the considered LLMs. Validation and test splits follow those designated by MixInstruct with \\( k \\) instruction examples each.\n\n#### Router Model\nDeBERTa-v3-large (300M) [He et al., 2020] is used as the backbone for training the routers. The routers are trained using the corresponding loss function over a specified number of epochs, with checkpoints selected based on validation set performance. The experiments are conducted using an NVIDIA A100 GPU with 80GB of GPU RAM.\n\n#### Evaluation Measures\n- **Quality Metric**: BART score [Yuan et al., 2021]\n- **Efficiency Metric**: Fraction of queries routed to the small model (cost advantage)\n\n#### Baselines\nThree straightforward baselines are used:\n1. **All-at-large**: Routes all queries to the large model.\n2. **All-at-small**: Routes all queries to the small model.\n3. **Random**: Generates a random number in [0,1] to select the large model if the number is below a probability threshold.\n\n#### Experiment Configurations\nFor the experiments, three routers are evaluated:\n1. **Deterministic router**\n2. **Probabilistic router**\n3. **Probabilistic router augmented with data transformation**\n\nCandidate model pairs include:\n- FLAN-T5 (800M) and FLAN-T5 (11B)\n- Llama-2 (7B) and Llama-2 (13B)\n- GPT-3.5-turbo\n\nAt test time, each trained router takes a threshold value as input and routes all queries with a router score higher than this threshold to the small model, recognizing these as \"easy\" queries.\n\n#### Results\nThe results indicate the effectiveness of the router in maintaining quality while achieving cost efficiency by appropriately routing \"easy\" queries to the smaller model.\n\nIn conclusion, the main experiment showcases that the proposed routing strategies effectively balance cost and quality, with the deterministic and probabilistic routers exhibiting notable performance improvements over the baselines. The experiments validate that the routers can route easy queries to small models with minimal compute overhead and demonstrate their generalizability across different model pairs."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of different routing strategies in saving costs and maintaining quality while routing queries between small and large language models.",
            "experiment_process": "Dataset: MixInstruct dataset from Jiang et al. (2023). Router Model: DeBERTa-v3-large (300M). The training is conducted on NVIDIA A100 GPU of 80GB GPU RAM. Evaluation Measures: BART score and fraction of queries routed to the small model. Baselines: All-at-large, all-at-small, and random. Experiments involve three routers (deterministic, probabilistic, and probabilistic with data transformation) and candidate model pairs from FLAN-T5 (800M, 11B), Llama-2 (7B, 13B), and GPT-3.5-turbo.",
            "result_discussion": "Small performance gap: All routers achieve significant cost advantages with negligible quality drops. Probabilistic router with data transformation performs best. Medium performance gap: All routers deliver reasonable cost advantages with acceptable quality drops. Probabilistic router with data transformation slightly outperforms the rest. Large performance gap: The deterministic router performs best, achieving 50% cost advantages with lower quality drops compared to other routers. Interesting observations include the router achieving better performance than all-at-large in certain scenarios and challenges faced by routers in modest cost advantage scenarios.",
            "ablation_id": "2404.14618v1.No1"
        },
        {
            "research_objective": "To validate if the router effectively routes easy queries to the small model and hard queries to the large model.",
            "experiment_process": "The difference between the average quality gaps of queries routed to the small model versus the large model is plotted against different cost advantages. The random baseline is used for comparison.",
            "result_discussion": "The router consistently routes easy queries to the small model, resulting in a positive difference in average quality gaps compared to the random baseline, which shows nearly zero difference.",
            "ablation_id": "2404.14618v1.No2"
        },
        {
            "research_objective": "To compare the latency of the router against that of different language models used in the experiments.",
            "experiment_process": "Average latency per query is measured for the router, FLAN-T5 (800M), Llama-2 (7B, 13B), and compared across 200 randomly chosen queries. Latency for GPT-3.5-turbo is not measured because its responses are generated by querying the OpenAI API.",
            "result_discussion": "The router processes queries significantly faster than all other LLMs, nearly 10x faster than the fastest LLM, FLAN-T5 (800M), due to smaller size and single forward pass processing for scoring.",
            "ablation_id": "2404.14618v1.No3"
        },
        {
            "research_objective": "To empirically determine the routing threshold that achieves cost reduction with minimal performance drops.",
            "experiment_process": "Random sampling of 10-20% from validation set for threshold determination using grid search to maximize cost advantages with less than 2% performance drop. Validation and testing with all three routers across various LLM pairs.",
            "result_discussion": "Validation and test results closely align, illustrating that the chosen threshold generalizes well. Slight performance drop observed for pairs with small performance gaps can be mitigated by a more conservative performance drop limit.",
            "ablation_id": "2404.14618v1.No4"
        },
        {
            "research_objective": "To evaluate router performance with metrics other than BART score using GPT-4-based evaluation.",
            "experiment_process": "Generate GPT-4 evaluation scores for test responses. Compute correlation between BART score and GPT-4 score, evaluate routing performance using the GPT-4 score.",
            "result_discussion": "Routers retain effectiveness when evaluated with GPT-4 score, especially with high correlation between BART and GPT-4 scores. As correlation weakens, router performance decays, indicating BART score can be used for training when highly correlated with target metrics.",
            "ablation_id": "2404.14618v1.No5"
        },
        {
            "research_objective": "To assess the generalizability of routers to different LLM pairs beyond those they were trained with.",
            "experiment_process": "Evaluate routing performance on new LLM pairs, compute correlation between quality gaps of training and testing pairs.",
            "result_discussion": "Routers generalize well when the quality gaps of testing pairs strongly correlate with training pairs. Performance decays with weaker correlation, suggesting quality gap correlation as an indicator for router applicability on new LLM pairs.",
            "ablation_id": "2404.14618v1.No6"
        }
    ]
}