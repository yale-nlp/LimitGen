{
    "title": "A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks",
    "abstract": "Recent efforts have evaluated large language models (LLMs) in areas such as commonsense reasoning, mathematical reasoning, and code generation. However, to the best of our knowledge, no work has specifically investigated the performance of LLMs in natural language generation (NLG) tasks, a pivotal criterion for determining model excellence. Thus, this paper conducts a comprehensive evaluation of well-known and high-performing LLMs, namely ChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models, in the context of NLG tasks. We select English and Chinese datasets encompassing Dialogue Generation and Text Summarization. Moreover, we propose a common evaluation setting that incorporates input templates and post-processing strategies. Our study reports both automatic results, accompanied by a detailed analysis.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent studies have emphasized the importance of scaling large language models (LLMs), referring to both the dimensions of the model size themselves and the amount of data used, resulting in enhanced capability of the models for tasks downstream [Chung et al., 2022  ###reference_bx10###]. Numerous investigations have been conducted to explore the limits of performance by training increasingly larger pre-trained language models, such as GPT-3 175B [Brown et al., 2020  ###reference_bx7###] and PaLM 540B [Chowdhery et al., 2022  ###reference_bx9###]. Although scaling primarily involves increasing the model size while maintaining similar architectures and pre-training tasks, these large-sized PLMs exhibit distinct behaviors from their smaller counterparts and demonstrate surprising emergent abilities in solving complex tasks [Zhang et al., 2017  ###reference_bx46###, Frankle and Carbin, 2019  ###reference_bx16###, Zhang et al., 2021  ###reference_bx48###]. An example of this is the contrasting performance of GPT-3 and GPT-2 when it comes to solving few-shot tasks. GPT-3 demonstrates effective problem-solving abilities by utilizing in-context learning, whereas GPT-2 faces difficulties in this aspect. As a result, these large-scale language models (LLMs) has become a huge research topic in current NLP area. In existing literature, remarkable LLMs such as ChatGPT000https://chat.openai.com/, ChatGLM111https://chatglm.cn/, have been widely adopted as powerful AI assistants, benefiting from their exceptional generation capabilities.\nWe hypothesis that a language model\u2019s performance in executing natural language generation (NLG) tasks is a crucial factor in determining its excellence [Dong et al., 2023  ###reference_bx13###]. NLG tasks involve LLMs that are capable of accepting diverse types of input, such as texts and tables, and generating coherent and appropriate output text. We intuitively think that generate fluent, coherent, and consistent texts is the foundation of a language model, so as to large language models [Raffel et al., 2020  ###reference_bx36###]. When some research institutions release their large language models, they tend to evaluate these models first. Community workers are also interested in testing well-known large language models. However, most of these evaluations focus on checking LLMs\u2019 ability of commonsense reasoning [Davis and Marcus, 2015  ###reference_bx12###, Wei et al., 2022  ###reference_bx43###], mathematical reasoning [Saxton et al., 2019  ###reference_bx38###, Wei et al., 2022  ###reference_bx43###], code completion [Allamanis et al., 2018  ###reference_bx2###], etc., but ignore the basic NLG tasks, such as dialogue generation [Chen et al., 2017  ###reference_bx8###], text summarization [Dong et al., 2023  ###reference_bx13###], and story generation [Al-Hussain and\nAzmi, 2022  ###reference_bx1###]. Besides, Some researchers pointed out that the performance of a large model is determined not only by its size and architecture, but more by the quality and quantity of training data. Based on this point of view, researchers open source and propose that some smaller-scale models trained on more and higher-quality data sets can achieve the same performance as models with more parameters than them. For example, LLaMA-13B [Touvron et al., 2023  ###reference_bx40###] outperforms GPT-3 on most benchmarks, despite being 10 times smaller. This notable discovery makes us curious about the performance of models with different architecture, data size, and mode size, trying to figure out which factor is more important. Therefore, we aim to address this gap by conducting a comparative analysis of LLM performance on NLG tasks, considering different architectures and scales throughout the evaluation process.\nIn this paper, we present a systematic evaluation of existing LLMs for NLG tasks. The main objective is to enhance our understanding of instruction and prompt design by conducting a comparative analysis of these models. Initially, we provide an overview of classic NLG tasks, including their definitions and associated English and Chinese datasets. Subsequently, we devise a model input template that includes instructions for each dataset. Following that, we introduce various LLMs, considering factors such as model size and architecture. Finally, we present the results of both automatic and manual evaluation of LLMs on NLG datasets, and discuss the strengths and weaknesses of their performance across different models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Natural Language Generation",
            "text": "In this section, we will introduce the definition of NLG, and its sub-tasks with some corresponding datasets that we will use to evaluate LLMs."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Definition",
            "text": "Natural Language Generation is the process of producing a natural language text in order to meet specified communicative goals. The texts that are generated may range from a single phrase given in answer to a question, through multi-sentence remarks and questions within a dialog, to full-page explanations. In our evaluation, we mainly focus on text-to-text styles.\nIn general, the task of NLG targets at finding an optimal sequence  that satisfies:\nwhere  represents the number of tokens of the generated sequence,  represents a set containing all possible sequences, and  is the conditional probability of the next token  based on its previous tokens  and the source sequence  with model parameters .\nNext, we will introduce some classic and widely-researched sub-tasks of NLG, with several corresponding datasets."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Dialogue Generation",
            "text": "Dialogue generation refers to the process of automatically generating coherent and contextually appropriate responses in a conversational setting [Chen et al., 2017  ###reference_bx8###, Ma et al., 2020  ###reference_bx30###, Dong et al., 2023  ###reference_bx13###]. The ultimate goal of dialogue generation task is to create responses that are relevant, informative, and engaging to the user.We utilize two English dialogue datasets characterized by clear emotional flow and topic constraints, as well as one English dataset that incorporates speakers\u2019 personalities. Furthermore, we employ a Chinese open-domain dialogue dataset for evaluation purposes.\nDailyDialog [Li et al., 2017  ###reference_bx24###] is a comprehensive, human-authored, and relatively noise-free English dataset that captures everyday communication styles and encompasses various topics related to our daily lives.\nPersonaChat [Zhang et al., 2018  ###reference_bx47###] is a persona-grounded dialogue dataset which contains 10k English multi-turn dialogues conditioned on personas, and each persona is described with at least 5 profile sentences.\nEmpatheticDialogue [Rashkin et al., 2019  ###reference_bx37###] is a large-scale multi-turn dialogue English dataset that contains 25k empathetic conversations between a speaker and a listener.\nLCCC [Wang et al., 2020  ###reference_bx41###] is a large-scale cleaned Chinese conversation dataset."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Text Summarization",
            "text": "Text summarization is the process of condensing a piece of text, such as an article, document, or news story, into a shorter version while preserving its key information and main ideas [El-Kassas et al., 2021  ###reference_bx15###, Dong et al., 2023  ###reference_bx13###]. Text summarization can be performed through two main approaches: Extractive Summarization and Abstractive Summarization. In our evaluation, we utilize multiple abstractive summarization datasets, specifically choosing two renowned datasets for the English and Chinese languages.\nCNN/DailyMail [Nallapati et al., 2016  ###reference_bx31###] is a large scale English summarization dataset which contains 93k and 220k articles collected from the CNN and Daily Mail websites, respectively, where each article has its matching abstractive summary.\nXSum [Narayan et al., 2018  ###reference_bx32###] is an extreme English summarization dataset containing BBC articles and corresponding single sentence summaries. In this dataset, 226,711 Wayback archived BBC articles are collected, which range from 2010 to 2017 and cover a wide variety of domains.\nTHUCNews [Li and Sun, 2007  ###reference_bx22###] is a Chinese summarization dataset, which comes from filtering the historical data of the Sina News RSS subscription channel from 2005 to 2011, including 740,000 news documents.\nLCSTS [Liu, 2020  ###reference_bx29###] is a large corpus of Chinese short text summarization dataset constructed from the Chinese micro-blogging website Sina Weibo. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Settings",
            "text": "###figure_1### Flan-T5 [Chung et al., 2022  ###reference_bx10###] is a fine-tuned version model class of T5 that has been trained on a variety of datasets phrased as instructions. It has shown impressive performance on several benchmarks, demonstrating strong zero-shot, few-shot, and Chain-of-Thought (CoT) [Wei et al., 2022  ###reference_bx43###] abilities. Flan-T5-XXL is the largest released checkpoint of this model, boasting a parameter volume of 13B. It inherits the extensive knowledge base of T5 while also being capable of understanding natural language instructions and performing the corresponding tasks.\nFastChat [Zheng et al., 2023a  ###reference_bx51###] is an open platform for training, serving, and evaluating large language model based chatbots. And FastChat-T5 is an open-source chatbot trained on this platform by fine-tuning Flan-T5-XL (3B parameters) on user-shared conversations collected from ShareGPT.\nOpen-LLaMA [Geng and Liu, 2023  ###reference_bx17###] is an open reproduction of LLaMA trained on the RedPajama dataset [Computer, 2023  ###reference_bx11###]. We leverage the 7B version666https://github.com/openlm-research/open_llama of this model for evaluation.\n[Taori et al., 2023  ###reference_bx39###] is fine-tuned based on a 7B LLaMA model using a dataset consisting of 52,000 instances of instruction-following data. This dataset is generated using the techniques outlined in the Self-Instruct paper [Wang et al., 2022  ###reference_bx42###], which aims to address the limited instruction-following capabilities of LLaMA models. To create the training data, the authors initially generate the data using OpenAI\u2019s GPT-3 and subsequently convert it into 52,000 instances of instruction-following conversational data using the Self-Instruct pipeline. This dataset is referred to as the Alpaca dataset. The Alpaca model is then fine-tuned to generate responses in conversations similar to ChatGPT.\nIn our evaluation, we utilize Alpaca-Lora-7B777https://huggingface.co/chainyo/alpaca-lora-7b, a low-rank adapter for LLaMA-7b fit on the Stanford Alpaca dataset, and Chinese-Alpaca-13b888https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf, a Chinese model version of Alpaca.\n[Zheng et al., 2023b  ###reference_bx52###] is fine-tuned based on LLaMA models using user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture. So it is basically fine-tuned with ChatGPT conversations. We utilize the 13B version of Vicuna, which is Vicuna-13B999https://huggingface.co/eachadea/vicuna-13b-1.1.\n[Anand et al., 2023  ###reference_bx3###] is a fine-tuned LLaMA 13B model and the GPT4All community101010https://home.nomic.ai/ has built the GPT4All Open Source datalake as a staging ground for contributing instruction and assistant tuning data for future GPT4All model trains.\nis an open assistant model developed by the Open-Assistant project. It is based on a Pythia 12B model that was fine-tuned on human demonstrations of assistant conversations collected through the Open-Assistant human feedback web app.\nis a Language Model (LLM) with 12B parameters, designed to follow instructions accurately. It has been trained on approximately 15,000 instruction/response fine-tuning records known as databricks-dolly-15k. These records were created by Databricks employees and cover various capability domains sourced from InstructGPT [Ouyang et al., 2022  ###reference_bx33###]. These domains include brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\nWe utilize several common automatic metrics for NLG tasks. PPL is used to assess the difficulty or confusion of a language model in predicting a sequence of words. BLEU (B-1, B-2, B-3, B-4) [Papineni et al., 2002  ###reference_bx34###] is used to assess the quality of machine-generated translations by comparing them to human reference translations. Meteor (MT) [Banerjee and Lavie, 2005  ###reference_bx4###] considers the accuracy and recall based on the entire corpus, and get the final measure. Rouge-L (R-L) [Lin, 2004  ###reference_bx27###] calculates the overlap between the generated output and the reference summaries or translations using various techniques such as N-gram matching. DISTINCT (D-1, D-2) [Li et al., 2016  ###reference_bx23###] quantifies how many distinct or different N-grams are present in the generated text, providing an indication of the model\u2019s ability to produce varied and non-repetitive output.\nBesides these widely-used metrics, we also develop a new metric called PostProcess Rate (PPR), which means the proportion of samples that need to be post-processed to the total number of samples."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview for LLMs",
            "text": "Typically, large language models (LLMs) refer to Transformer-based models containing tens or hundreds of billions of parameters and trained on extensive corpora of texts [Zhao et al., 2023  ###reference_bx50###]. These LLMs demonstrate significant capabilities in understanding natural language and solving complex tasks. Furthermore, they have showcased their ability to perform new tasks based on textual instructions or with just a few examples [Chung et al., 2022  ###reference_bx10###]. The emergence of these few-shot properties is a result of scaling models to a sufficient size, leading to a line of research that focuses on further scaling these models [Rae et al., 2021  ###reference_bx35###].\nPrevious LLMs, such as T5 [Raffel et al., 2020  ###reference_bx36###], GPT-3 [Brown et al., 2020  ###reference_bx7###], OPT [Zhang et al., 2022  ###reference_bx49###], and PaLM [Chowdhery et al., 2022  ###reference_bx9###], primarily emphasized scaling model size rather than considering the quality and quantity of data. However, recent studies have demonstrated that, given a fixed compute budget, the best performance is achieved by smaller models trained on larger datasets [Hoffmann et al., 2022  ###reference_bx19###]. Additionally, most of these models are not open-source and can only be accessed through APIs for inference, which poses inconveniences for model evaluation and usage. In order to address this issue, numerous researchers have proposed excellent open-source architectures and trained models, including GLM-130B [Zeng et al., 2022  ###reference_bx45###], ChatGLM [Du et al., 2022  ###reference_bx14###], LLaMA [Touvron et al., 2023  ###reference_bx40###], and Pythia [Biderman et al., 2023  ###reference_bx6###]. Furthermore, advancements in fine-tuning techniques have contributed to the success of deploying these models with limited resources, such as Lora [Hu et al., 2022  ###reference_bx20###] and P-Tuning [Li and Liang, 2021  ###reference_bx21###]. Therefore, this paper aims to conduct systematic evaluations of these models and their fine-tuned versions, categorized into four groups: ChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "ChatGPT",
            "text": "ChatGPT222https://chat.openai.com/ is a large language model based on OpenAI\u2019s GPT-3.5 architecture [Brown et al., 2020  ###reference_bx7###]. It is designed specifically for generating conversations and answering user queries. ChatGPT employs large-scale pretraining and fine-tuning methodologies, utilizing vast amounts of textual data to learn statistical patterns and semantic knowledge of language, and perform well in zero-shot and few-shot settings, and can understand the input instructions."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "ChatGLM",
            "text": "ChatGLM333https://chatglm.cn/ is a freely available dialogue language model that operates in both Chinese and English languages. It follows the GLM architecture and boasts an impressive parameter count of 6.2 billion. ChatGLM-6B incorporates similar technology as ChatGPT, with a specific focus on Chinese question answering and dialogue. The model undergoes extensive training on a dataset containing approximately 1 trillion tokens in Chinese and English. The training process includes supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback. Despite having only 6.2 billion parameters, the model demonstrates the ability to generate responses that align with human preferences."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "T5-Based models",
            "text": "T5 [Raffel et al., 2020  ###reference_bx36###], which stands for Text-To-Text Transfer Transformer, is a transformer-based language model developed by Google Research. Instead of training separate models for different tasks, T5 is trained in a text-to-text pattern. This means that it is trained to perform a wide range of NLP tasks by transforming the input text into a standardized format that specifies the task to be performed. In our evaluation, we select two new fine-tuned versions of T5, namely: Flan-T5-XXL444https://huggingface.co/google/flan-t5-xxl and FastChat-T5555https://huggingface.co/lmsys/fastchat-t5-3b-v1.0.\nFlan-T5 [Chung et al., 2022  ###reference_bx10###  ###reference_bx10###] is a fine-tuned version model class of T5 that has been trained on a variety of datasets phrased as instructions. It has shown impressive performance on several benchmarks, demonstrating strong zero-shot, few-shot, and Chain-of-Thought (CoT) [Wei et al., 2022  ###reference_bx43###  ###reference_bx43###] abilities. Flan-T5-XXL is the largest released checkpoint of this model, boasting a parameter volume of 13B. It inherits the extensive knowledge base of T5 while also being capable of understanding natural language instructions and performing the corresponding tasks.\nFastChat [Zheng et al., 2023a  ###reference_bx51###  ###reference_bx51###] is an open platform for training, serving, and evaluating large language model based chatbots. And FastChat-T5 is an open-source chatbot trained on this platform by fine-tuning Flan-T5-XL (3B parameters) on user-shared conversations collected from ShareGPT."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "LLaMA-Based Models",
            "text": "LLaMA [Touvron et al., 2023  ###reference_bx40###] is a collection of foundation language models ranging from 7B to 65B parameters proposed by Meta AI. Unlike other famous LLMs, LLaMA is only trained on publicly avaiable data, making it compatible with open-sourcing. Numerous remarkable and impressive models have emerged as a result, built upon the LLaMA framework and trained using diverse datasets. Among these models, we have chosen a few prominent ones for evaluation: Open-LLaMA, Vicuna, Alpaca, and GPT4ALL.\nOpen-LLaMA [Geng and Liu, 2023  ###reference_bx17###  ###reference_bx17###] is an open reproduction of LLaMA trained on the RedPajama dataset [Computer, 2023  ###reference_bx11###  ###reference_bx11###]. We leverage the 7B version666https://github.com/openlm-research/open_llama of this model for evaluation.\n[Taori et al., 2023  ###reference_bx39###  ###reference_bx39###] is fine-tuned based on a 7B LLaMA model using a dataset consisting of 52,000 instances of instruction-following data. This dataset is generated using the techniques outlined in the Self-Instruct paper [Wang et al., 2022  ###reference_bx42###  ###reference_bx42###], which aims to address the limited instruction-following capabilities of LLaMA models. To create the training data, the authors initially generate the data using OpenAI\u2019s GPT-3 and subsequently convert it into 52,000 instances of instruction-following conversational data using the Self-Instruct pipeline. This dataset is referred to as the Alpaca dataset. The Alpaca model is then fine-tuned to generate responses in conversations similar to ChatGPT.\nIn our evaluation, we utilize Alpaca-Lora-7B777https://huggingface.co/chainyo/alpaca-lora-7b, a low-rank adapter for LLaMA-7b fit on the Stanford Alpaca dataset, and Chinese-Alpaca-13b888https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf, a Chinese model version of Alpaca.\n[Zheng et al., 2023b  ###reference_bx52###  ###reference_bx52###] is fine-tuned based on LLaMA models using user-shared conversations collected from ShareGPT. It is an auto-regressive language model, based on the transformer architecture. So it is basically fine-tuned with ChatGPT conversations. We utilize the 13B version of Vicuna, which is Vicuna-13B999https://huggingface.co/eachadea/vicuna-13b-1.1.\n[Anand et al., 2023  ###reference_bx3###  ###reference_bx3###] is a fine-tuned LLaMA 13B model and the GPT4All community101010https://home.nomic.ai/ has built the GPT4All Open Source datalake as a staging ground for contributing instruction and assistant tuning data for future GPT4All model trains."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "Pythia-Based Models",
            "text": "Pythia [Biderman et al., 2023  ###reference_bx6###] is a project by EleutherAI111111https://github.com/EleutherAI/pythia that combines interpret-ability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive Transformers. We utilize two versions of Pythia which are Oasst-Pythia and Dolly.\nis an open assistant model developed by the Open-Assistant project. It is based on a Pythia 12B model that was fine-tuned on human demonstrations of assistant conversations collected through the Open-Assistant human feedback web app.\nis a Language Model (LLM) with 12B parameters, designed to follow instructions accurately. It has been trained on approximately 15,000 instruction/response fine-tuning records known as databricks-dolly-15k. These records were created by Databricks employees and cover various capability domains sourced from InstructGPT [Ouyang et al., 2022  ###reference_bx33###  ###reference_bx33###]. These domains include brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization."
        },
        {
            "section_id": "3.7",
            "parent_section_id": "3",
            "section_name": "Dataset",
            "text": "In our evaluation, we aim to showcase the generation capabilities of LLMs in zero-shot scenarios. Therefore, we refrain from providing any additional information to the model for each of the aforementioned datasets. Specifically:\nFor datasets of Text Summarization task, we input the text, document, or article to allow the model to extract key information and generate concise summaries.\nFor datasets of Dialogue Generation task, we input the dialogue history, enabling the model to generate appropriate responses for the final round of the conversation.\nWe defer the evaluation of LLMs on Chinese datasets and other NLG tasks such as story generation, along with results of manual and GPT-4 rating, to future research endeavors."
        },
        {
            "section_id": "3.8",
            "parent_section_id": "3",
            "section_name": "Input Template",
            "text": "Because LLMs that we evaluate possess the ability to comprehend instructions and perform corresponding tasks, so in order to ensure fairness, we develop an input template that is applied to every dataset for each task, serving as the input for every large language model. This template consists of two components: the instruction and the input. Figure 1  ###reference_### illustrates the templates designed for both the Chinese and English datasets, and Table 1  ###reference_### shows the content of instruction and text for each dataset."
        },
        {
            "section_id": "3.9",
            "parent_section_id": "3",
            "section_name": "Hyperparameters",
            "text": "Although each LLM may have its own optimal decoding strategy, for the sake of fairness, we have standardized these hyperparameters across all LLMs. We employ the Top-k and Top-p sampling, with  and . Additionally, a temperature value of  and a repetition penalty factor of  are imposed. Furthermore, we specify a maximum token length of  and a minimum token length of  for the generated content."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.10 Post-Processing Strategy",
            "text": "Through case study, we observe that despite emphasizing the exclusion of any additional output in the input, regrettably, most LLMs still generate redundant information in their output. Therefore, we find it necessary to apply post-processing to the outputs of these models. To ensure fairness, we adopt the same post-processing strategy for all LLMs. Specifically, we utilize the keywords \u201c### response:\u201d or \u201c### {CJK*}UTF8gbsn\u56de\u590d\uff1a\u201d for segmentation. If the segmented content consists of a single line, we consider it as the final result. If the segmented content spans multiple lines, we use \u201c\\n\u201d as segmentation keywords and select the first sentence with a length not less than 16 as the final result."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.11 Baselines",
            "text": "There have been numerous previous works on datasets we used, and these works have achieved good results. Therefore, despite the fact that most of these works have proposed models much smaller than LLMs and have predominantly utilized supervised fine-tuning methods, we still compare them with LLMs to highlight some characteristics of LLMs. For each dataset, we select several recent works with better performance and report their results.\nFor EmpatheticDialogues, we utilize EP-PG [Li et al., 2022  ###reference_bx25###] that first generates event transition plans and then obtains the final response, and MoEL [Lin et al., 2019  ###reference_bx26###] that are consist of one emotion tracker and  emotion listeners.\nFor DailyDialog, we utilize PLATO [Bao et al., 2020  ###reference_bx5###], a pre-trained dialogue generation model, and DialogWAE [Gu et al., 2019  ###reference_bx18###], a conditional wasserstein autoencoder (WAE) specially designed for dialogue modeling.\nFor PersonaChat, we utilize PLATO as mentioned above, and CTRLStruct [Yin et al., 2023  ###reference_bx44###] for dialogue structure learning to effectively explore topic-level dialogue clusters."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.12 Evaluation Metrics",
            "text": "We utilize several common automatic metrics for NLG tasks. PPL is used to assess the difficulty or confusion of a language model in predicting a sequence of words. BLEU (B-1, B-2, B-3, B-4) [Papineni et al., 2002  ###reference_bx34###  ###reference_bx34###] is used to assess the quality of machine-generated translations by comparing them to human reference translations. Meteor (MT) [Banerjee and Lavie, 2005  ###reference_bx4###  ###reference_bx4###] considers the accuracy and recall based on the entire corpus, and get the final measure. Rouge-L (R-L) [Lin, 2004  ###reference_bx27###  ###reference_bx27###] calculates the overlap between the generated output and the reference summaries or translations using various techniques such as N-gram matching. DISTINCT (D-1, D-2) [Li et al., 2016  ###reference_bx23###  ###reference_bx23###] quantifies how many distinct or different N-grams are present in the generated text, providing an indication of the model\u2019s ability to produce varied and non-repetitive output.\nBesides these widely-used metrics, we also develop a new metric called PostProcess Rate (PPR), which means the proportion of samples that need to be post-processed to the total number of samples."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": ""
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Text Summarization",
            "text": "The automatic metrics results of LLMs on the three datasets are shown in Table 6  ###reference_###, 7  ###reference_### and 8  ###reference_###. Our observations from the two datasets can be summarized as follows:\nThe Flan-T5 and FastChat-T5 models employ an encoder-decoder architecture, exhibiting remarkable proficiency in instruction comprehension, as evident by their minimal requirement for post-processing. This finding is corroborated by the analysis of dialogue generation. Moreover, our investigation on the XSum dataset reveals that both models surpass other LLMs, consistently attaining top positions across various metrics such as BLEU and ROUGE scores. These impressive results are likely attributed to the inherent strengths embedded within their model structures."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Story Generation",
            "text": "The automatic metrics results of LLMs on the three datasets are shown in Table 8  ###reference_### and 9  ###reference_###.\nThese tables provide further analysis of the performance of current LLMs on NLG tasks: when the required generated text is excessively long, the models struggle to follow instructions effectively. This is evidenced by the WritingPrompts dataset from Table 9  ###reference_###, where many models have BLEU scores that are close to or equal to zero."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Finetune LLMs",
            "text": "To illustrate the enhancement in the performance of LLMs using parameter-efficient fine-Tuning methods, we employ LoRA [Hu et al., 2022  ###reference_bx20###] and P-Tuning V2 [Liu et al., 2021  ###reference_bx28###] to fine-tune ChatGLM-6B, LLaMA2-7B, and LLaMA2-7B-Chat. The results on the Empathetic Dialogues, LCCC, ROCStories, LOT, and LCSTS are presented in Table 10  ###reference_### and 11  ###reference_###. As shown in the tables, the scores of various metrics significantly improve after fine-tuning the models compared to the non-fine-tuned results. Furthermore, the results in N-grams Matching metrics (BLEU and Rouge) far surpass the previous SOTA results. This demonstrates that LoRA and P-Tuning V2 can substantially enhance the fitting capability of LLMs to datasets without incurring excessive computational resources."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we conduct a comprehensive assessment of several existing large-scale language models (LLMs) in the context of natural language generation (NLG) tasks. Our evaluation encompasses English and Chinese datasets to examine the multilingual capabilities of these LLMs. The results and analyses from both automatic and manual evaluations of LLMs reveal notable trends and phenomena."
        }
    ],
    "url": "http://arxiv.org/html/2405.10251v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "3.6",
            "3.8"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.7",
            "3.9",
            "3.11",
            "3.12",
            "4",
            "4.1",
            "4.2",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "3.6",
            "3.11"
        ]
    },
    "research_context": {
        "paper_id": "2405.10251v1",
        "paper_title": "A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks",
        "research_background": "**Motivation:**\nThe paper is motivated by the gaps in existing literature surrounding the evaluation of large language models (LLMs) on natural language generation (NLG) tasks. Despite the notable capabilities of LLMs like GPT-3 and PaLM, most evaluations focus on their performance in specific areas like commonsense reasoning, mathematical reasoning, and code completion, while basic NLG tasks such as dialogue generation, text summarization, and story generation are often overlooked. Additionally, the paper is driven by curiosity about the relative importance of different factors such as model architecture, data size, and model size in determining the performance of LLMs.\n\n**Research Problem:**\nThe research problem addressed in this paper is to systematically evaluate existing LLMs on NLG tasks to understand the impact of different factors, such as model size, architecture, and the quality and quantity of training data, on their performance. The aim is to provide a comparative analysis that enhances understanding of instruction and prompt design and helps identify the strengths and weaknesses of various LLMs in generating fluent, coherent, and consistent texts.\n\n**Relevant Prior Work:**\n- **Scaling LLMs and Emerging Abilities:** Studies have shown that scaling LLMs, such as GPT-3 175B [Brown et al., 2020] and PaLM 540B [Chowdhery et al., 2022], primarily by increasing model size, leads to enhanced performance and emergent abilities in solving complex tasks [Zhang et al., 2017; Frankle and Carbin, 2019; Zhang et al., 2021]. \n- **Inspiration from Prominent LLMs:** LLMs like ChatGPT and ChatGLM have been widely recognized for their exceptional generation capabilities, making them important benchmarks in this field.\n- **Data Quality vs. Model Size:** Some studies suggest that the performance of LLMs is influenced not just by their size and architecture, but also by the quality and quantity of the training data. For example, LLaMA-13B has been shown to outperform GPT-3 despite being smaller, due to high-quality training data [Touvron et al., 2023].\n- **Underexplored NLG Tasks:** Prior work has typically focused more on specific reasoning tasks, leaving a gap in the comprehensive evaluation of basic NLG tasks like dialogue generation, text summarization, and story generation [Chen et al., 2017; Dong et al., 2023; Al-Hussain and Azmi, 2022].\n\nIn summary, the paper aims to fill a gap in the systematic evaluation of LLMs on NLG tasks, focusing on understanding the influence of model size, architecture, and training data quality, while building on existing findings about the importance of scaling and the notable performance of various prominent LLMs.",
        "methodology": "The proposed methodology involves a systematic evaluation of Large Language Models (LLMs) in the context of natural language generation tasks, specifically focusing on Transformer-based models with tens or hundreds of billions of parameters that have been trained on extensive text corpora. The methodology acknowledges the significant capabilities of these models in understanding natural language and solving complex tasks, including their ability to perform new tasks using only textual instructions or a few examples, a property that emerges from scaling the models sufficiently.\n\n### Key Components and Innovations:\n\n1. **Model Focus**: This study looks into prominent LLMs, such as:\n   - T5 (Raffel et al., 2020)\n   - GPT-3 (Brown et al., 2020)\n   - OPT (Zhang et al., 2022)\n   - PaLM (Chowdhery et al., 2022)\n\n   These models traditionally emphasize scaling model size with compute budget considerations.\n\n2. **Data Quality and Quantity**: It examines the hypothesis from recent studies which suggest that for a fixed compute budget, smaller models trained on larger datasets can achieve the best performance, challenging the earlier focus merely on scaling model size.\n\n3. **Accessibility and Open-Source Models**: The study addresses the issue of many existing LLMs not being open-source and only accessible via APIs. It highlights new open-source architectures and models such as:\n   - GLM-130B (Zeng et al., 2022)\n   - ChatGLM (Du et al., 2022)\n   - LLaMA (Touvron et al., 2023)\n   - Pythia (Biderman et al., 2023)\n\n4. **Advancements in Fine-Tuning Techniques**: The paper also considers advancements in fine-tuning methods that enable efficient deployment of these models with limited resources:\n   - Lora (Hu et al., 2022)\n   - P-Tuning (Li and Liang, 2021)\n\n### Evaluation Strategy:\n\nThe paper categorizes the evaluation into four distinct groups:\n- **ChatGPT**: Evaluating models based on specific architectures designed for conversational AI.\n- **ChatGLM**: Assessing the performance of models under the GLM framework with enhancements for chat-based tasks.\n- **T5-Based Models**: Analyzing T5 and its derivatives to understand improvements from scaling and fine-tuning within this category.\n- **LLaMA-Based Models**: Focusing on LLaMA\u2019s open-source contributions and comparing their efficacy in natural language tasks.\n- **Pythia-Based Models**: Investigating the specialized models designed under the Pythia framework and their fine-tuned counterparts.\n\nIn summary, the proposed method systematically evaluates various LLMs, paying close attention to the balance between model size and data training quality, along with accessibility improvements through open-source contributions and advanced fine-tuning techniques.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:** \n1. **Flan-T5-XXL** [Chung et al., 2022] model, fine-tuned on a variety of instruction-based datasets.\n2. **FastChat-T5**, an open-source chatbot trained using Flan-T5-XL on user-conversation data from ShareGPT [Zheng et al., 2023a].\n3. **Open-LLaMA** [Geng and Liu, 2023], specifically its 7B variant, trained on the RedPajama dataset.\n4. **Alpaca-Lora-7B** [Taori et al., 2023] and **Chinese-Alpaca-13B**, models fine-tuned on Alpaca datasets with 52,000 instances of instruction following.\n5. **Vicuna-13B** [Zheng et al., 2023b], fine-tuned using user-conversation data from ShareGPT.\n6. **GPT4All**, a 13B parameter model fine-tuned using instruction and assistant tuning data from the GPT4All community [Anand et al., 2023].\n7. **Dolly** is a model (12B parameters) fine-tuned on approx. 15,000 instruction/response records from the Databricks datalake.\n\n**Baselines:**\nThe baselines used in the experiment are various versions of Flan-T5, Open-LLaMA, Alpaca, Vicuna, GPT4All, and Dolly. These models serve as comparison points to measure the performance improvements or differences achieved by newer models or training methods.\n\n**Evaluation Metrics:**\n1. **Perplexity (PPL):** Measures the difficulty a language model has in predicting a sequence of words.\n2. **BLEU (B-1, B-2, B-3, B-4):** Assesses the quality of machine-generated translations by comparing them to human reference translations.\n3. **Meteor (MT):** Takes into account accuracy and recall based on the entire corpus to give a final metric.\n4. **Rouge-L (R-L):** Computes the overlap between the generated output and reference summaries/translations using techniques like N-gram matching.\n5. **Distinct (D-1, D-2):** Quantifies the number of distinct N-grams present in the generated text, indicating the variety and non-repetitiveness of the output.\n6. **PostProcess Rate (PPR):** A newly developed metric that measures the proportion of samples requiring post-processing out of the total samples.\n\n### Main Experimental Results\n\nWhile the exact numerical results are not provided in the text, the descriptions of the models, datasets, and metrics suggest several insights:\n\n1. **Flan-T5-XXL:** Demonstrated strong performance on various benchmarks with impressive zero-shot, few-shot, and Chain-of-Thought capabilities.\n2. **FastChat-T5:** Efficiently trained on user-shared conversation data, suggesting its efficacy in generating chat-based responses.\n3. **Open-LLaMA and Alpaca Models:** These models tailored from LLaMA through extensive fine-tuning on instruction-following datasets showed potential in generating responses aligned with human instructions.\n4. **Vicuna-13B:** Fine-tuned from ChatGPT user-conversation data demonstrated strong capabilities in conversational tasks.\n5. **GPT4All and Dolly:** Both models exhibited robust instruction-following and conversational generation, indicating the effectiveness of their respective training datasets and fine-tuning methods.\n\nIn summary, each model demonstrated the capability to handle a range of natural language generation tasks, evaluated by comprehensive metrics providing insights into their quality, diversity, accuracy, and required post-processing efforts."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the effectiveness of post-processing strategies in handling redundant information generated by LLMs.",
            "experiment_process": "The researchers observed that despite instructions to exclude additional output, many LLMs generated redundant information. To address this issue uniformly across all models, they applied a common post-processing strategy. They used specific keywords ('### response:' or '### {CJK*}UTF8gbsn\u56de\u590d\uff1a') for segmentation. If the segmented content was a single line, it was considered the final result. For content spanning multiple lines, 'n' was used for segmentation, and the first sentence not less than 16 characters was selected as the final result.",
            "result_discussion": "The implementation of a consistent post-processing strategy ensures fairness in evaluating LLMs, reducing the impact of extraneous output and improving the clarity and relevance of generated responses.",
            "ablation_id": "2405.10251v1.No1"
        },
        {
            "research_objective": "Compare the performance of various LLMs (ChatGPT, ChatGLM, T5-based models, LLaMA-based models, and Pythia-based models) against current state-of-the-art models on dialogue datasets.",
            "experiment_process": "The study selected three prominent dialogue datasets: EmpatheticDialogues, DailyDialog, and PersonaChat. They then compared the LLMs with recent high-performing models. For EmpatheticDialogues, they used EP-PG and MoEL; for DailyDialog, PLATO and DialogWAE; for PersonaChat, PLATO and CTRLStruct. These models were previously built using supervised fine-tuning methods and were generally smaller than LLMs.",
            "result_discussion": "The comparison highlights the strengths and limitations of LLMs versus specialized smaller models. The results showcase how LLMs perform relative to existing state-of-the-art models, indicating areas where they excel or need improvement.",
            "ablation_id": "2405.10251v1.No2"
        },
        {
            "research_objective": "Evaluate Large Language Models (LLMs) on various NLG tasks using well-established automatic metrics.",
            "experiment_process": "They utilized common NLG metrics, including PPL to assess language model prediction difficulty, BLEU for translation quality, Meteor for accuracy and recall, Rouge-L for overlap between generated and reference outputs, and DISTINCT to measure output variety. They also developed a new metric called PostProcess Rate (PPR), indicating how many samples required post-processing.",
            "result_discussion": "The comprehensive utilization of multiple metrics offers a detailed performance analysis of LLMs across different dimensions of NLG tasks. The introduction of PPR as a new metric provides additional insights into the models' tendency to generate extraneous information, which is critical for evaluating their real-world applicability.",
            "ablation_id": "2405.10251v1.No3"
        }
    ]
}