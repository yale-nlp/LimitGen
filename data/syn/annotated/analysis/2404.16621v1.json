{
    "title": "Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare",
    "abstract": "The integration of Large Language Models (LLMs) into healthcare promises to transform medical diagnostics, research, and patient care. Yet, the progression of medical LLMs faces obstacles such as complex training requirements, rigorous evaluation demands, and the dominance of proprietary models that restrict academic exploration. Transparent, comprehensive access to LLM resources is essential for advancing the field, fostering reproducibility, and encouraging innovation in healthcare AI. We present Hippocrates, an open-source LLM framework specifically developed for the medical domain. In stark contrast to previous efforts, it offers unrestricted access to its training datasets, codebase, checkpoints, and evaluation protocols. This open approach is designed to stimulate collaborative research, allowing the community to build upon, refine, and rigorously evaluate medical LLMs within a transparent ecosystem. Also, we introduce Hippo, a family of 7B models tailored for the medical domain, fine-tuned from Mistral and LLaMA2 through continual pre-training, instruction tuning, and reinforcement learning from human and AI feedback. Our models outperform existing open medical LLMs models by a large-margin, even surpassing models with 70B parameters. Through Hippocrates, we aspire to unlock the full potential of LLMs not just to advance medical knowledge and patient care but also to democratize the benefits of AI research in healthcare, making them available across the globe.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The remarkable success of Large Language Models (LLMs) across diverse NLP tasks has revolutionized artificial intelligence (Touvron et al., 2023b  ###reference_b42###; Bai et al., 2023  ###reference_b2###; Jiang et al., 2023  ###reference_b15###; OpenAI, 2023  ###reference_b27###; Google, 2023  ###reference_b10###). Despite their impressive generalization capabilities, LLMs encounter challenges in clinical contexts, primarily due to a deficiency in domain-specific knowledge and the intricacies of medical terminology. Bridging this gap, in this work, we introduce Hippocrates (named after the Ancient Greek \u201cFather of Medicine\u201d), a state-of-the-art, fully open-source framework designed to elevate LLMs\u2019 proficiency in medical reasoning. We publicly share our training data, complete training and evaluations codes, along with intermediate model checkpoints. Our framework marks an important step towards democratizing advancements in medical LLMs.\nPrevious attempts to develop advanced medical LLMs yielded promising results by further training them (Labrak et al., 2024  ###reference_b20###), supervised fine-tuning them (Li et al., 2023  ###reference_b23###; Han et al., 2023  ###reference_b12###; Toma et al., 2023  ###reference_b40###), or both (Wu et al., 2023  ###reference_b45###; Chen et al., 2023  ###reference_b6###), via special medical-text corpus and medical instruction datasets. However, the data collection, pre-training, and finetuning stages may include considerable complexity, which makes reproducing, analyzing, and comparing the recent LLMs in that domain challenging. On the other hand, closed models, e.g. GPT4 (OpenAI, 2023  ###reference_b27###), Gemini (Google, 2023  ###reference_b10###), Med-PaLM (Singhal et al., 2023b  ###reference_b37###), trained on closed-domain datasets make their results non-reproducible, not to mention substantial computational costs and further complicate the understanding of which components are crucial to the success of these advanced medical frameworks.\n###figure_1### ###figure_2### ###figure_3### ###figure_4### In this work, we provide full access to our framework, from the data sources to the training configurations and the reproducible evaluation protocols. We conduct a detailed empirical analysis to identify the impact of various design elements on LLM performance, leading to a domain-adapted framework that demonstrates superior performance on multiple medical benchmarks. Based on these insights, we develop a step-by-step guide for the efficient training of medical-LLMs. Our research efforts yield two advanced 7B parameter models, Hippo-    and Hippo-   . As shown in Fig. 1  ###reference_###, our models not only outperform existing 7B and 13B models by a significant margin but also deliver results on par with, and in some cases exceeding, those of 70B models.\nWe argue that the development of a broad, varied collection of open models is crucial for deepening our knowledge of language models and enhancing their applicability across various domains.\n###figure_5### ###figure_6### In addition, we adopt a novel strategy for structuring our instruction tuning (IT) dataset, dividing it into two distinct components: the General Instruction Dataset and the Evaluation Instruction Dataset. The General dataset is designed to enable unbiased assessments by avoiding overlap with downstream task data, marking a departure from previous methodologies. On the other hand, the Evaluation Instruction Dataset, which incorporates training splits from evaluation benchmarks, facilitates direct comparisons with existing models (Chen et al., 2023  ###reference_b6###). Notably, for the first time in the medical domain, our approach incorporates preference learning from medical professionals into the model development process, utilizing RLAIF (Lee et al., 2023b  ###reference_b22###) and GPT4 for annotating preferences.\nFor model evaluation, we employ the well-established EleutherAI framework111https://github.com/EleutherAI/lm-evaluation-harness  ###reference_n-harness### (Gao et al., 2021  ###reference_b9###), conducting tests across a set of six varied medical downstream tasks. These include MedMCQA (Pal et al., 2022  ###reference_b31###), PubmedQA (Jin et al., 2019  ###reference_b17###), MedQA (Jin et al., 2021  ###reference_b16###), and the USMLE-step1, USMLE-step2, and USMLE-step3. Leveraging this framework allows for straightforward replication of any LLM\u2019s results, eliminating the necessity for additional fine-tuning or the repetitive execution of evaluation scripts for each new model."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Hippocrates Framework",
            "text": "Fig. 2  ###reference_### shows the overall workflow of the Hippocrates framework, starting from domain-specific pre-training and progressing through supervised fine-tuning and reinforcement learning from AI-generated feedback to an extensive evaluation phase. This pipeline ensures our models are precisely tailored and rigorously tested for the medical domain.\n###figure_7### This dataset aggregates more than 400K samples from nine different datasets, each derived from the instruction corpora of previous studies (Li et al., 2023  ###reference_b23###; Han et al., 2023  ###reference_b12###; Wu et al., 2023  ###reference_b45###; Lee et al., 2023a  ###reference_b21###). By excluding data from the training or test splits of downstream QA benchmarks, we aim to minimize bias and improve the model\u2019s generalization capabilities across different reasoning tasks. A pre-processing protocol was employed to remove superfluous words and web URLs, ensuring the data\u2019s quality and relevance. The detailed statistics of the dataset are presented in Table 2  ###reference_###.\nThis dataset was formed to examine the effects of including instruction samples directly from downstream tasks, a common practice in existing studies (Chen et al., 2023  ###reference_b6###; Han et al., 2023  ###reference_b12###; Wu et al., 2023  ###reference_b45###). Instruction-response pairs were crafted using the training splits of various benchmarks, following the templates established in Meditron (Chen et al., 2023  ###reference_b6###). We conducted a series of experiments to assess the distinct influence of each split on each task, both individually and collectively. The details about the Evaluation Instruction Data is given in Table 3  ###reference_###.\nBeyond independently utilizing these datasets for supervised fine-tuning, we also examined the impact of individual datasets as well as the collective effect of combining them on model performance (refer to Appendix G  ###reference_###).\nTo reduce annotation costs, we adopted the RLAIF methodology (Lee et al., 2023b  ###reference_b22###) in the medical domain for the first time. Utilizing detailed prompts based on patient inquiries from the iCliniq-10k dataset, we used GPT4 (OpenAI, 2023  ###reference_b27###) to determine the optimal response based on predefined instructions. These instructions were derived from those used in qualitative assessments by medical professionals in Med-PaLM (Singhal et al., 2022  ###reference_b35###; 2023a  ###reference_b36###), with minor modifications. This annotation approach amounted to a cost of $120. The exact prompt structure for applying RLAIF with GPT4 is given in Appendix J  ###reference_###, Figure 7  ###reference_###.\nTo test the reliability of GPT4\u2019s capacity to replicate medical expert annotations, we subjected 250 samples from our dataset to careful examination by two medical doctors, given them the same instructions that we provided in the prompt to GPT4.\nOur analysis revealed compelling results. When comparing GPT4\u2019s annotations against those of MD-1, GPT4 demonstrated a Kappa Score of 0.376, indicating moderate agreement, and an accuracy of 68.9%. The comparison with MD-2 showed even stronger results, with GPT4 achieving a Kappa Score of 0.672, suggesting substantial agreement, alongside an 83.6% accuracy. Interestingly, the inter-annotator agreement between the two doctors themselves yielded a Kappa Score of 0.416 and an accuracy of 70.8%, situating GPT4\u2019s performance firmly within the range of human expert variability. These findings not only affirm GPT4\u2019s aptitude for medical annotation but also highlight its potential to serve as a cost-effective alternative to human annotators in medical research and application settings. These findings suggest that GPT4 is capable of effectively mimicking medical doctor preferences, potentially eliminating the need for costly doctor annotations.\nConsequently, we compiled a comprehensive medical doctor preference dataset, consisting of 15,258 samples, to further align our LLMs with real-world clinical decision-making processes and enhance their accuracy in interpreting and responding to medical queries.\nTo equip our base LLMs with domain-specific medical expertise, we extend their pre-training on a carefully curated medical text corpus as described in Section 2.1  ###reference_###. This stage employs traditional language modeling, focusing on next-token prediction. During this phase, both models undergo continued pre-training using LoRA, specifically adapting the fully connected layers. The parameters for LoRA are carefully set, with the rank () at 8 and alpha () at 16, to optimize learning. We use the AdamW optimizer and adjust the learning rate using a cosine scheduling, starting from an initial value of 1e-4. The batch size per device was initialized to be 8, with gradient accumulations of 2, culminating in an effective global batch size of 16, and the models are trained for a single epoch. The rationale and empirical support for our choices regarding the dataset, LoRA configurations, and overall optimization strategy are comprehensively analyzed in Appendix G  ###reference_.SSS0.Px1###.\nAfter continued pre-training, models undergo fine-tuning with an Instruction Tuning (IT) dataset to closely mirror medical directives, aligning model outputs with clinical requirements. We have tested with the datasets described in Section 2.2  ###reference_### and found that MedQA-train IT works better than the other options. This fine-tuning phase also employs LoRA to all fully connected layers with both rank () and alpha () set to 32 for balanced efficiency and computational overhead. AdamW optimizer is used with a learning rate of . To prevent model overfitting, loss calculation focuses solely on the responses. The training spanned 3 epochs with a batch size of 8 per-device and gradient accumulation set to 2. We also conducted experiments on direct fine-tuning of the base LLMs to evaluate the impact of continued pre-training (see Section 4.1  ###reference_###) and performed a comprehensive analysis on dataset splits and fine-tuning hyperparameters (see Appendix G  ###reference_.SSS0.Px2###).\nFinally, the instruction-tuned models are further trained with a recent and popular technique called direct preference optimization (DPO) (Rafailov et al., 2023  ###reference_b33###). In DPO, reinforcement learning is bypassed which allows for direct optimization based on preference data. Unlike RLHF, the responses in DPO need not be derived from the LLM being optimized. Central to DPO is the development of a loss function that evaluates the likelihood of a preferred response over a less preferred one, steering the LLM towards this goal. This makes DPO more stable and significantly reduces computational demands.\nThe outcome of all this are our medical LLMs, named Hippo-    and Hippo-   , built upon the pre-trained LLaMA2 7B and Mistral 7B models. These models were refined through a comprehensive process that included continued pre-training and/or instruction tuning using our carefully curated medical datasets. Following this, we also explored the impact of aligning the models with clinical preferences by conducting further training on medical preference data.\n###figure_8### ###figure_9###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Continued Pre-training Data",
            "text": "A key aspect of our methodology is the integration of specialized medical knowledge through an extensive pre-training corpus, assembled from three specialized datasets: Medical Guidelines, PMC-Patients, and PubMedQA-contexts. The Medical Guidelines dataset comprises clinical practice guidelines, is used for training Meditron models (Chen et al., 2023  ###reference_b6###). The PMC-Patients dataset (Zhao et al., 2023  ###reference_b46###) consists of patient summaries extracted from case reports within PubMed Central (PMC). Additionally, the PubMedQA-contexts dataset is constructed by extracting the context field of each sample in the training split of the benchmark (Jin et al., 2019  ###reference_b17###). Detailed descriptions and specifications of each dataset are available in Table 1  ###reference_###. This extensive corpus, consisting of roughly 300M training tokens, forms the foundation of our models, ensuring their proficiency in navigating medical terminology and practices. We systematically assessed the impact of each dataset, both individually and in combination, to optimize our model\u2019s performance."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Supervised Fine-Tuning Data",
            "text": "Developing effective medical LLMs requires blending domain-specific knowledge with sophisticated reasoning abilities. Previous models often utilized instruction data consisting of samples from the training or test sets of evaluation benchmarks. We also considered this setup, but additionally investigated an alternative involving generic medical data. Consequently, we constructed two sets of IT datasets: the General Instructions Data and the Evaluation Instructions Data.\nThis dataset aggregates more than 400K samples from nine different datasets, each derived from the instruction corpora of previous studies (Li et al., 2023  ###reference_b23###  ###reference_b23###; Han et al., 2023  ###reference_b12###  ###reference_b12###; Wu et al., 2023  ###reference_b45###  ###reference_b45###; Lee et al., 2023a  ###reference_b21###  ###reference_b21###). By excluding data from the training or test splits of downstream QA benchmarks, we aim to minimize bias and improve the model\u2019s generalization capabilities across different reasoning tasks. A pre-processing protocol was employed to remove superfluous words and web URLs, ensuring the data\u2019s quality and relevance. The detailed statistics of the dataset are presented in Table 2  ###reference_###  ###reference_###.\nThis dataset was formed to examine the effects of including instruction samples directly from downstream tasks, a common practice in existing studies (Chen et al., 2023  ###reference_b6###  ###reference_b6###; Han et al., 2023  ###reference_b12###  ###reference_b12###; Wu et al., 2023  ###reference_b45###  ###reference_b45###). Instruction-response pairs were crafted using the training splits of various benchmarks, following the templates established in Meditron (Chen et al., 2023  ###reference_b6###  ###reference_b6###). We conducted a series of experiments to assess the distinct influence of each split on each task, both individually and collectively. The details about the Evaluation Instruction Data is given in Table 3  ###reference_###  ###reference_###.\nBeyond independently utilizing these datasets for supervised fine-tuning, we also examined the impact of individual datasets as well as the collective effect of combining them on model performance (refer to Appendix G  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Medical Preference Data",
            "text": "Constructing a preference dataset typically involves generating diverse responses to identical queries using LLMs, which are subsequently evaluated by human annotators to identify the most accurate response. This method, however, can become prohibitively expensive, both in terms of computation for generating responses and the financial and time investments required for manual annotation. To circumvent these issues, we leveraged the iCliniq-10k dataset (Li et al., 2023  ###reference_b23###), containing 10K authentic patient-doctor dialogues from icliniq.com. Each dialogue features a patient question accompanied by three different answers: one from an actual doctor, and the others from ChatGPT and ChatDoctor (Li et al., 2023  ###reference_b23###). We conducted a thorough preprocessing of this dataset to eliminate any irrelevant or extraneous information.\nTo reduce annotation costs, we adopted the RLAIF methodology (Lee et al., 2023b  ###reference_b22###  ###reference_b22###) in the medical domain for the first time. Utilizing detailed prompts based on patient inquiries from the iCliniq-10k dataset, we used GPT4 (OpenAI, 2023  ###reference_b27###  ###reference_b27###) to determine the optimal response based on predefined instructions. These instructions were derived from those used in qualitative assessments by medical professionals in Med-PaLM (Singhal et al., 2022  ###reference_b35###  ###reference_b35###; 2023a  ###reference_b36###  ###reference_b36###), with minor modifications. This annotation approach amounted to a cost of $120. The exact prompt structure for applying RLAIF with GPT4 is given in Appendix J  ###reference_###  ###reference_###, Figure 7  ###reference_###  ###reference_###.\nTo test the reliability of GPT4\u2019s capacity to replicate medical expert annotations, we subjected 250 samples from our dataset to careful examination by two medical doctors, given them the same instructions that we provided in the prompt to GPT4.\nOur analysis revealed compelling results. When comparing GPT4\u2019s annotations against those of MD-1, GPT4 demonstrated a Kappa Score of 0.376, indicating moderate agreement, and an accuracy of 68.9%. The comparison with MD-2 showed even stronger results, with GPT4 achieving a Kappa Score of 0.672, suggesting substantial agreement, alongside an 83.6% accuracy. Interestingly, the inter-annotator agreement between the two doctors themselves yielded a Kappa Score of 0.416 and an accuracy of 70.8%, situating GPT4\u2019s performance firmly within the range of human expert variability. These findings not only affirm GPT4\u2019s aptitude for medical annotation but also highlight its potential to serve as a cost-effective alternative to human annotators in medical research and application settings. These findings suggest that GPT4 is capable of effectively mimicking medical doctor preferences, potentially eliminating the need for costly doctor annotations.\nConsequently, we compiled a comprehensive medical doctor preference dataset, consisting of 15,258 samples, to further align our LLMs with real-world clinical decision-making processes and enhance their accuracy in interpreting and responding to medical queries."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Training Methodology",
            "text": "Our training strategy includes several phases: injection of medical knowledge through continued pre-training, domain-specific instruction tuning, and reinforcement learning from AI-generated feedback for improved alignment with medical experts. Employing the LLaMA Factory framework (hiyouga, 2023  ###reference_b13###), we adhere to replicable and high-performance training standards. Moreover, we adopt the Low-Rank Adaptation (LoRA) technique Hu et al. (2021  ###reference_b14###) for training efficiency and precision. LoRA enhances LLMs by selectively updating weights within additional trainable layers, thereby accelerating the training process, minimizing memory usage, and mitigating overfitting and catastrophic forgetting.\nOur foundational models, LLaMA2 7B (Touvron et al., 2023b  ###reference_b42###) and Mistral 7B (Jiang et al., 2023  ###reference_b15###), are selected based on their robust performance across medical benchmarks, demonstrating their capacity to excel without extensive training modifications. The zero-shot performances of these generic baseline models is presented at the beginning of Table 5  ###reference_###.\nTo equip our base LLMs with domain-specific medical expertise, we extend their pre-training on a carefully curated medical text corpus as described in Section 2.1  ###reference_###  ###reference_###. This stage employs traditional language modeling, focusing on next-token prediction. During this phase, both models undergo continued pre-training using LoRA, specifically adapting the fully connected layers. The parameters for LoRA are carefully set, with the rank () at 8 and alpha () at 16, to optimize learning. We use the AdamW optimizer and adjust the learning rate using a cosine scheduling, starting from an initial value of 1e-4. The batch size per device was initialized to be 8, with gradient accumulations of 2, culminating in an effective global batch size of 16, and the models are trained for a single epoch. The rationale and empirical support for our choices regarding the dataset, LoRA configurations, and overall optimization strategy are comprehensively analyzed in Appendix G  ###reference_.SSS0.Px1###  ###reference_.SSS0.Px1###.\nAfter continued pre-training, models undergo fine-tuning with an Instruction Tuning (IT) dataset to closely mirror medical directives, aligning model outputs with clinical requirements. We have tested with the datasets described in Section 2.2  ###reference_###  ###reference_### and found that MedQA-train IT works better than the other options. This fine-tuning phase also employs LoRA to all fully connected layers with both rank () and alpha () set to 32 for balanced efficiency and computational overhead. AdamW optimizer is used with a learning rate of . To prevent model overfitting, loss calculation focuses solely on the responses. The training spanned 3 epochs with a batch size of 8 per-device and gradient accumulation set to 2. We also conducted experiments on direct fine-tuning of the base LLMs to evaluate the impact of continued pre-training (see Section 4.1  ###reference_###  ###reference_###) and performed a comprehensive analysis on dataset splits and fine-tuning hyperparameters (see Appendix G  ###reference_.SSS0.Px2###  ###reference_.SSS0.Px2###).\nFinally, the instruction-tuned models are further trained with a recent and popular technique called direct preference optimization (DPO) (Rafailov et al., 2023  ###reference_b33###  ###reference_b33###). In DPO, reinforcement learning is bypassed which allows for direct optimization based on preference data. Unlike RLHF, the responses in DPO need not be derived from the LLM being optimized. Central to DPO is the development of a loss function that evaluates the likelihood of a preferred response over a less preferred one, steering the LLM towards this goal. This makes DPO more stable and significantly reduces computational demands.\nThe outcome of all this are our medical LLMs, named Hippo-    and Hippo-   , built upon the pre-trained LLaMA2 7B and Mistral 7B models. These models were refined through a comprehensive process that included continued pre-training and/or instruction tuning using our carefully curated medical datasets. Following this, we also explored the impact of aligning the models with clinical preferences by conducting further training on medical preference data.\n###figure_10### ###figure_11###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Main Results",
            "text": "For an objective evaluation of domain-specific knowledge and reasoning capabilities in LLMs, a detailed and fair evaluation framework is essential. In alignment with methodologies adopted in prior research (Singhal et al., 2022  ###reference_b35###; Han et al., 2023  ###reference_b12###; Wu et al., 2023  ###reference_b45###; Toma et al., 2023  ###reference_b40###; Singhal et al., 2023a  ###reference_b36###; Chen et al., 2023  ###reference_b6###), we selected six widely recognized medical question-answering datasets, namely MedMCQA (Pal et al., 2022  ###reference_b31###), MedQA (Jin et al., 2021  ###reference_b16###), PubMedQA (Jin et al., 2019  ###reference_b17###) and USMLE Step 1-3 (Han et al., 2023  ###reference_b12###), to assess models performances (See Table 4  ###reference_### for details). Performance metrics were derived through the use of the EleutherAI evaluation framework (Gao et al., 2021  ###reference_b9###), ensuring a standardized approach to measuring model effectiveness in handling domain-specific queries."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "In our evaluation, we included a spectrum of leading LLMs, spanning general and medical LLMs, varying in scale from 1.5B to an advanced 70B parameters. Here we report the performances of our top-performing models for an accurate comparison. To ensure a fair and easily replicable assessment of these medical models, we utilized the Eleuther AI Language Model Evaluation Harness (Gao et al., 2021  ###reference_b9###), a unified evaluation framework specifically designed for evaluating generative LLMs. This framework also serves as the evaluation tool for the Open LLM Leaderboard222https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard  ###reference_4/open_llm_leaderboard### (Beeching et al., 2023  ###reference_b3###).\nLM-Evaluation-Harness operates on a Log-Likelihood objective, which calculates the negative log-likelihood for each potential answer in response to a given query. The answer is then chosen based on the highest likelihood score, indicating it as the most probable choice. During evaluation, each prompt includes a question and corresponding choices, separated by a new line. For PubMedQA, the abstract provides contextual grounding for the model\u2019s decision-making process. Examples of these prompts are provided in the Appendix I  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "###figure_12### ###figure_13### We present a comparative analysis of our novel models, Hippo-    and Hippo-   , against a set of established base LLMs and medical-specific LLMs, in Table 5  ###reference_###. Our evaluation includes both zero-shot and few-shot (specifically, 5-shot) learning scenarios. Demonstrating superior performance, our Hippo models outperform traditional pretrained models in zero-shot evaluations and maintain their superiority in the 5-shot context. Remarkably, Hippo-    and Hippo-    not only beat models with 7 billion and 13 billion parameters but also exceed the capabilities of those with 70 billion parameters. This outstanding performance highlights the adaptability and precision of our models, showing their remarkable ability to significantly boost prediction accuracy with minimal input examples.\n###figure_14### ###figure_15### ###figure_16### ###figure_17###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "###figure_18### Our evaluation methodology for the LLaMA2 7B model covers successive training stages: Continued Pre-training (CP), Instruction Tuning (SFT), and Direct Preference Optimization (DPO). As listed in Table 6  ###reference_###, the base model LLaMA2 7B initially achieves an average accuracy of 34.0 across benchmarks. The CP stage marginally increases accuracy to 34.4, indicating initial benefits from domain-focused continued pre-training. The subsequent introduction of SFT yields a substantial performance boost to an average accuracy of 50.3, demonstrating the critical role of customized instruction in enhancing the model\u2019s capabilities in understanding and answering medical queries. Integrating CP with SFT further improves this performance to 53.0, highlighting the combined value of domain knowledge and specific instruction tuning. The final DPO stage slightly decreases the model\u2019s performance to 52.5, albeit with a slight increase in accuracy for MedMCQA and PubMedQA, illustrating DPO\u2019s refined impact on model preference alignment. This sequence delineates the incremental enhancements attributable to each training phase, with SFT marking a pivotal improvement. The composite model, LLaMA2 + CP + SFT, is thus designated as Hippo-    for its distinguished performance across our benchmarks.\n###figure_19### ###figure_20### Following the approach for Hippo-   , the training evolution for the Mistral 7B model reveals gradual improvement in the model\u2019s proficiency in medical question-answering. Initial results from the baseline Mistral 7B model, as shown in Table 6  ###reference_###, show an average benchmark accuracy of 39.3. Implementing CP slightly improves this to 41.0, reflecting the positive yet modest impact of domain-specific continued pre-training. The pivotal SFT stage significantly raises the performance, achieving an average accuracy of 61.6, emphasizing the critical role of customized instruction in enhancing the model\u2019s interpretative and response capabilities for medical inquiries. Interestingly, combining CP and SFT results in a slight reduction to 61.1, suggesting a complex interaction between domain pre-training and instruction tuning. The subsequent application of DPO slightly lowers the overall score to 59.6, similar to the pattern observed for Hippo-   , with targeted performance adjustment. Based on comprehensive analysis, Mistral 7b + SFT is selected to represent Hippo-   , credited for its exceptional performance across all benchmarks.\n###figure_21### ###figure_22### ###figure_23###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Chain-of-Thought (CoT) Prompting",
            "text": "The CoT prompting technique (Wei et al., 2023  ###reference_b44###) enhances an LLM\u2019s ability to tackle complex queries by guiding it to articulate intermediate reasoning steps. This method improves the model\u2019s responses by structuring its problem-solving process. In our study, we applied CoT prompting for in-context learning, adopting a slightly altered instruction utilized in (Pal & Sankarasubbu, 2024b  ###reference_b30###): \u201dThe following is a multiple choice question about medical knowledge. Solve it in a step-by-step fashion, starting by summarizing the available information. Output a single option from the four options as the final answer.\u201d. However, the application of CoT prompting in our experiments with downstream medical tasks did not consistently enhance our models\u2019 performance, as shown in Table 6  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Influencing Examples",
            "text": "We explore the application of Influence Functions to understand the behavior of LLMs (Grosse et al., 2023  ###reference_b11###) \u2013 in our context, particularly those trained with domain-specific datasets like medical text. This technique quantifies the effect of single training instances on the model\u2019s predictions, improving the transparency of the AI models. This is increasingly important as the field of Explainable AI (XAI) grows to make AI systems more interpretable and accountable. However, the complexity of LLMs, which process vast amounts of data, highlights the necessity for efficient methods to perform this analysis. We believe incorporating this tool to our evaluation framework will prove useful for future studies.\nIn the supplementary material (Appendix H  ###reference_###), we present our analysis results, highlighting the most and least influential training examples for a MedQA dataset question and its model response. Notably, the most influential example shares overlapping medical concepts, in contrast to no shared concepts with the least influential training example."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Uncertainty Quantification",
            "text": "In our study, we conducted an uncertainty quantification experiment on Hippo-    to understand its performance on the MedMCQA, MedQA, and PubMedQA datasets, as shown in Fig.3  ###reference_###. Our findings reveal that our model consistently assigns higher probabilities to questions it answers correctly across all datasets, suggesting an ability to self-calibrate its certainty. The model\u2019s confidence is notably higher on MedMCQA, possibly reflecting the dataset\u2019s relative simplicity. In contrast, its confidence on PubMedQA is comparatively lower, likely due to the dataset\u2019s complexity. Additionally, the model\u2019s confidence changes with different training stages; CPT leads to more conservative estimates, SFT boosts confidence, and adding DPO leads to variable confidence, with noticeable effects in MedMCQA and MedQA. These outcomes emphasize a complex relationship between training approaches and confidence calibration in the model.\n###figure_32### ###figure_33### ###figure_34### We present additional negative results in Appendix J  ###reference_###, which we anticipate will be beneficial for the community. By sharing these findings, we aim to encourage further investigations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we have introduced Hippocrates, a comprehensive and open-source framework tailored for the medical domain, addressing a wide array of challenges faced by medical LLMs. We provide openly available datasets and establish an intuitive benchmark using the LM-Evaluation-Harness tool. We also introduce Hippo-    and Hippo-   , two 7B models demonstrating superior performance. Our work makes substantial contributions to the field by combining in-depth empirical research with a structured training methodology, offering invaluable insights and tools for future research not only in healthcare but in any area requiring domain-specific adaptation of LLMs.\n###figure_35### ###figure_36###"
        }
    ],
    "url": "http://arxiv.org/html/2404.16621v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "4",
            "4.1"
        ]
    },
    "research_context": {
        "paper_id": "2404.16621v1",
        "paper_title": "Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare",
        "research_background": "**Paper Motivation:**\n\nThe paper is motivated by the impressive success of Large Language Models (LLMs) in various NLP tasks but recognizes that these models struggle in clinical contexts due to a lack of domain-specific knowledge and the complexities of medical terminology. The goal is to bridge this gap and enhance the proficiency of LLMs in medical reasoning. \n\n**Research Problem:**\n\nThe primary research problem is to develop and provide an open-source framework named Hippocrates that improves the performance of LLMs in healthcare. Current methodologies either involve complex, non-reproducible steps or utilize closed models and datasets, making it challenging to replicate, analyze, and compare results. The paper aims to democratize the advancements in medical LLMs by providing a comprehensive framework that includes training data, code, and evaluation protocols.\n\n**Relevant Prior Work:**\n\n1. **Continued Training and Fine-Tuning:** Previous work has developed advanced medical LLMs through further training or supervised fine-tuning using special medical-text corpora and medical instruction datasets (Labrak et al., 2024; Li et al., 2023; Han et al., 2023; Toma et al., 2023; Wu et al., 2023; Chen et al., 2023).\n\n2. **Challenges in Reproducibility:** These previous efforts are marked by considerable complexity, making it difficult to reproduce, analyze, and compare results.\n\n3. **Closed Models:** Existing models like GPT-4 (OpenAI, 2023), Gemini (Google, 2023), and Med-PaLM (Singhal et al., 2023b) rely on closed-domain datasets, contributing to non-reproducibility and high computational costs, thus making it harder to understand key components for the success of advanced medical LLMs.\n\nThe provided framework, Hippocrates, aims to overcome these issues by offering transparency across all stages of model development and evaluation, thus making the results accessible and reproducible for broader research and application in healthcare.",
        "methodology": "The proposed methodology in the paper \"Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare\" describes a comprehensive and multi-faceted approach to developing and refining large language models (LLMs) tailored for the medical domain. This approach involves several stages, each contributing to the model's expertise and performance in healthcare applications.\n\n### Key Components and Innovations:\n\n1. **Domain-Specific Pre-Training**:\n    - **Corpus Compilation**: The initial phase involves the collection of a meticulous medical text corpus. This step is critical for equipping the base LLMs with specialized knowledge pertinent to healthcare.\n    - **Continued Pre-Training**: Models undergo continued pre-training focused on next-token prediction using traditional language modeling techniques. The framework employs LoRA (Low-Rank Adaptation) to specifically adapt the fully connected layers, optimizing learning with a rank of 8 and alpha of 16.\n    - **Optimization**: The training process uses the AdamW optimizer with a cosine scheduling learning rate, initialized at 1e-4, and leverages an effective global batch size of 16 achieved through gradient accumulation.\n\n2. **Supervised Fine-Tuning**:\n    - **Dataset Utilization**: Fine-tuning involves using a specially curated Instruction Tuning (IT) dataset, aiming to align the model outputs with clinical requirements.\n    - **Fine-Tuning Configuration**: This phase employs LoRA configurations set to 32 for rank and alpha to balance efficiency and computational overhead. The training spans 3 epochs with a batch size of 8 per device and gradient accumulation set to 2.\n\n3. **Reinforcement Learning from AI-Generated Feedback (RLAIF)**:\n    - **Innovation**: For the first time in the medical domain, the RLAIF methodology is adopted. Detailed prompts based on patient inquiries are created from the iCliniq-10k dataset, with GPT4 generating optimal responses.\n    - **Annotation Cost Reduction**: This method significantly decreases annotation costs (totaling $120) by using AI for data annotation, which is then validated against human expert annotations to ensure reliability.\n\n4. **Evaluation and Validation**:\n    - **Human Validation**: The AI-generated annotations are evaluated against those of two medical doctors, showing GPT4's performance falls within the variability range of human experts.\n    - **Comprehensive Dataset Creation**: A dataset with 15,258 samples labeled according to medical doctor preferences is compiled to further align LLMs with real-world clinical decision-making, enhancing their accuracy for medical queries.\n\n5. **Direct Preference Optimization (DPO)**:\n    - **Technique**: Recent and popular DPO techniques are applied, bypassing reinforcement learning (RLHF). The preferences do not require responses to come from the optimized LLM, making DPO more stable and efficient in terms of computational demands.\n\n### Outcome:\nThe methodology culminates in the creation of two refined medical LLMs, named **Hippo-** (based on LLaMA2 7B) and **Hippo-** (based on Mistral 7B). These models are the result of a well-orchestrated sequence of continued pre-training, supervised fine-tuning, and alignment with clinical preferences through structured datasets and innovative annotation techniques.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\n**Experiment Setup**:\nTo objectively evaluate the domain-specific knowledge and reasoning capabilities of large language models (LLMs) in healthcare, we implemented a detailed and standardized evaluation framework. This setup follows methodologies established in prior research, including those by Singhal et al. (2022), Han et al. (2023), Wu et al. (2023), Toma et al. (2023), Singhal et al. (2023a), and Chen et al. (2023).\n\n**Datasets**:\nWe selected six widely recognized medical question-answering datasets to assess the performance of the models:\n1. **MedMCQA** (Pal et al., 2022)\n2. **MedQA** (Jin et al., 2021)\n3. **PubMedQA** (Jin et al., 2019)\n4. **USMLE Step 1** (Han et al., 2023)\n5. **USMLE Step 2** (Han et al., 2023)\n6. **USMLE Step 3** (Han et al., 2023)\n\n**Baselines and Metrics**:\nPerformance metrics were derived using the EleutherAI evaluation framework (Gao et al., 2021), ensuring a standardized approach to measuring the models' effectiveness in managing domain-specific queries.\n\n**Main Experimental Results**:\nThe detailed results, including quantitative performance metrics across the various datasets, provide insights into how well each model performed on medical question-answering tasks, demonstrating the models' domain-specific knowledge and reasoning capabilities.\n\n(Note: Specific performance results, numerical scores, and detailed comparisons are typically included in supplementary materials, which are not provided here.)"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of continued pre-training (CP), instruction tuning (SFT), and direct preference optimization (DPO) on the performance of the LLaMA2 7B model in the medical domain.",
            "experiment_process": "The LLaMA2 7B model was subjected to successive training stages: Continued Pre-training (CP) with domain-specific medical texts, Instruction Tuning (SFT) using medical directives, and Direct Preference Optimization (DPO) based on preference data. Each training stage's performance was measured across various medical benchmarks, recording accuracy improvement at each step.",
            "result_discussion": "The base model LLaMA2 7B had an initial accuracy of 34.0. The CP stage slightly increased accuracy to 34.4. SFT significantly improved performance to 50.3, and combining CP with SFT further increased accuracy to 53.0. However, DPO slightly decreased overall performance to 52.5, although it did show minor accuracy gains for some benchmarks. This sequence highlighted the substantial impact of SFT and the balanced benefit of integrating CP and SFT.",
            "ablation_id": "2404.16621v1.No1"
        },
        {
            "research_objective": "To analyze the performance changes in the Mistral 7B model through Continued Pre-training (CP), Instruction Tuning (SFT), and Direct Preference Optimization (DPO).",
            "experiment_process": "The Mistral 7B model underwent similar sequential training phases: CP with medical domain-specific texts, SFT to better align with medical instructions, and DPO to optimize based on user preferences. The performance of each stage was evaluated with multiple medical benchmarks to determine accuracy improvements at each stage.",
            "result_discussion": "The baseline Mistral 7B model started with an average accuracy of 39.3. CP saw a marginal improvement to 41.0. The SFT phase significantly lifted performance to 61.6, showing the value of specialized instruction. Combining CP and SFT resulted in a minor reduction to 61.1, indicating complex interactions. DPO further lowered the accuracy to 59.6. Thus, Mistral 7b with SFT was selected for its notable performance in medical question-answering tasks.",
            "ablation_id": "2404.16621v1.No2"
        }
    ]
}