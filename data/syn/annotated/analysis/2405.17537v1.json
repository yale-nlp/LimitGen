{
    "title": "BIOSCAN-CLIP: Bridging Vision and Genomics for Biodiversity Monitoring at Scale",
    "abstract": "Measuring biodiversity is crucial for understanding ecosystem health.\nWhile prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, DNA barcodes, and textual data in a unified embedding space.\nThis allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse DNA and image data.\nOur method surpasses previous single-modality approaches in accuracy by over 11% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "As environmental change and habitation loss accelerate, monitoring biodiversity is crucial to understand and maintain the health of ecosystems.\nTaxonomic classification of organisms at scale is especially important for understanding regional biodiversity and studying species interactions.\nRecently, computer vision techniques have been used to classify species observed in images [17  ###reference_b17###, 58  ###reference_b58###, 60  ###reference_b60###, 40  ###reference_b40###] including for ecological monitoring [11  ###reference_b11###].\nHowever, relying solely on images for identifying and classifying organisms fails to consider the rich evolutionary relationship between species and may miss fine-grained species differences.\nTo better capture these distinctions, researchers have used DNA sequences for tasks such as genome understanding and taxonomic classification [26  ###reference_b26###, 66  ###reference_b66###, 42  ###reference_b42###, 8  ###reference_b8###, 3  ###reference_b3###].\nIn particular, DNA barcodes [23  ###reference_b23###], small sections of DNA from specific genes such as the COI gene [38  ###reference_b38###] in mitochondrial DNA, are useful for species identification [3  ###reference_b3###].\nHowever, collecting DNA requires specialized equipment and is more expensive and less accessible than images.\nA desirable approach is to use approaches that leverage DNA information at training time but only need images of new specimens at inference time [5  ###reference_b5###].\nMachine learning advances have made it possible to combine information from different modalities.\nFor instance, CLIP [47  ###reference_b47###] used contrastive learning to encode text (e.g., \u201ccat\u201d) and images (e.g., a photo of a cat) into a common space for zero-shot classification.\nA recent model BioCLIP [52  ###reference_b52###] aligned images of organisms with common names and taxonomic descriptions to classify plants, animals, and fungi. Most prior work only uses a subset of modalities (image only [17  ###reference_b17###, 58  ###reference_b58###, 60  ###reference_b60###], DNA only [26  ###reference_b26###, 66  ###reference_b66###, 42  ###reference_b42###, 8  ###reference_b8###, 3  ###reference_b3###], text and image [52  ###reference_b52###]) rather than combining information from images, text, and DNA barcodes.\nThey also often require a text description or complete taxonomic annotations, which are expensive and time-consuming to obtain.\nIn this work, we propose BIOSCAN-CLIP which uses contrastive learning to map biological images, textual taxonomic labels, and DNA barcodes to the same latent space.\nWith this aligned space, we do not need comprehensive or noise-free taxonomic annotations.\nWe flexibly take either images or DNA barcodes as input to predict the taxonomy.\nOur embedding space also enables future research leveraging multiple modalities to examine commonalities and differences between species.\nAside from ecological benefits, building such a foundation model for biodiversity provides a case study of broader challenges in identifying fine-grained differences, both visually and textually.\nVisual differences between species are often subtle, and the DNA and taxonomic labels do not have much semantic overlap with everyday natural language.\nThus, tokens for DNA and text from taxonomic labels are different from tokens typically found in large language models trained on internet data.\nWe showcase the benefits of pretraining with all three modalities through improved taxonomic classification accuracy over prior works in both retrieval and zero-shot settings."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "We review work on pretraining multimodal models on images, DNA, and text, including foundation models and multimodal learning with fine-grained images and their application in biological problems."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Fine-grained multimodal learning",
            "text": "Recent work on vision and language addresses the challenge of distinguishing between highly similar categories [22  ###reference_b22###, 51  ###reference_b51###].\nRadford et al. [47  ###reference_b47###] showcased how natural language supervision can significantly improve visual models. Contrastive learning on over 400 million pairs of images and text enabled matching multi-modal data and zero-shot transfer across diverse tasks. Later work built on this architecture to improve the embedding space [9  ###reference_b9###, 16  ###reference_b16###, 34  ###reference_b34###, 20  ###reference_b20###, 27  ###reference_b27###, 62  ###reference_b62###] or implement support for generation tasks [31  ###reference_b31###, 32  ###reference_b32###, 33  ###reference_b33###].\nWei et al. [60  ###reference_b60###] provide a comprehensive survey on deep learning for fine-grained image analysis.\nRecent works train domain-specific foundation models on large corpora of image or natural language datasets, leveraging similar architectures and training strategies [10  ###reference_b10###, 21  ###reference_b21###, 52  ###reference_b52###]. Other approaches extend the multimodal architecture to more than two modalities, including audio, video, or 3D representations [49  ###reference_b49###, 1  ###reference_b1###, 2  ###reference_b2###, 45  ###reference_b45###, 63  ###reference_b63###, 39  ###reference_b39###, 19  ###reference_b19###].\nWe demonstrate the value of applying similar strategies for pretraining foundation models to the problem of biodiversity monitoring."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Models for DNA and biological images",
            "text": "There is much work on machine learning for DNA data, especially in genome understanding [35  ###reference_b35###, 29  ###reference_b29###, 4  ###reference_b4###, 30  ###reference_b30###].\nRecently, there is increasing interest in developing foundation models of DNA sequences [55  ###reference_b55###, 26  ###reference_b26###, 66  ###reference_b66###, 67  ###reference_b67###, 3  ###reference_b3###, 13  ###reference_b13###, 43  ###reference_b43###, 8  ###reference_b8###].\nMany of these works leverage transformer architectures and self-supervised learning techniques such as masked language modeling [14  ###reference_b14###] to pretrain models on large DNA datasets for downstream genome analysis tasks such as promoter prediction or metagenomics binning [26  ###reference_b26###, 66  ###reference_b66###, 67  ###reference_b67###, 3  ###reference_b3###].\nZhou et al. [67  ###reference_b67###] incorporate curriculum-based contrastive learning to learn a \u201cspecies-aware\u201d embedding space for DNA.\nOther work explored using DNA data for taxonomic classification [36  ###reference_b36###, 42  ###reference_b42###, 3  ###reference_b3###].\nBERTax [42  ###reference_b42###] pretrained a BERT [14  ###reference_b14###] model for hierarchical taxonomic classification. However, they focused on superkingdom, phylum, and genus classification, which are coarser-grained categories and thus easier than classifying species. BarcodeBERT [3  ###reference_b3###] showed that models pretrained on DNA barcodes, rather than general DNA datasets, can be particularly effective for taxonomic classification.\nOur work extends these models for encoding DNA by learning a shared embedding space with images and text as well, addressing issues with the cost of obtaining DNA data in practice by allowing for cross-modal image queries.\nThere is also work on taxonomic classification based on images of plant, bird, and insect species [7  ###reference_b7###, 58  ###reference_b58###, 46  ###reference_b46###]. Methods for coarse supervision help learn fine-grained taxonomic categories, given the number and rarity of species [56  ###reference_b56###, 48  ###reference_b48###, 54  ###reference_b54###]. Contrastive learning has been used to better differentiate between fine-grained species characteristics [12  ###reference_b12###, 61  ###reference_b61###]. However, species differences are not necessarily easily detectable by visual cues alone. To that end, we leverage DNA data in addition to images to classify species, while still preserving the benefits of the relative ease of acquiring visual data of new organisms."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Multimodal models for biology",
            "text": "Prior works explored building pretrained models based on only images or DNA, and largely relied on fine-tuning classifiers on a set of known species. This limits those approaches to a closed set of species, whereas we are concerned with being able to identify unseen species, i.e. those for which we have no examples in the modality of interest. Furthermore, these works are limited to single modalities.\nRecent works have begun building multimodal models for biological applications [25  ###reference_b25###, 37  ###reference_b37###, 64  ###reference_b64###], including several works in taxonomic classification [52  ###reference_b52###, 44  ###reference_b44###, 5  ###reference_b5###, 6  ###reference_b6###].\nNguyen et al. [44  ###reference_b44###] introduced Insect-1M, a large-scale dataset with images annotated with their taxonomic levels (from class to species) and dense text descriptions. Their method applied contrastive learning across text and image modalities with a patch-based attention to build an \u201cinsect foundation\u201d model.\nBioCLIP [52  ###reference_b52###] applied multimodal contrastive pretraining on images and text descriptions to a larger scale, collecting multiple datasets into the TreeOfLife-10M dataset.\nWhile achieving impressive results, these models and datasets only consider images and text, thus being more limited with new species, in which taxonomic labels are not available for alignment. They do not take advantage of the rich taxonomic knowledge available in sources like the Barcode of Life Datasystem (BOLD), that at the time of writing is approaching 15\u2009M validated DNA barcodes, most of which have associated expert-assigned taxonomic labels.\nBadirli et al. [5  ###reference_b5###] employed a Bayesian zero-shot learning approach using DNA to model priors for species classification by image.\nThey relate unseen species to nearby seen species in the DNA embedding space, given barcode data for both, in order to construct Gaussian priors by which to classify images. Badirli et al. [6  ###reference_b6###] employ similar Bayesian techniques and combine image and DNA embeddings in the same space to predicting the genus for unseen species, assuming that genera are largely already discovered. While this approach learns to project between embedding spaces of different modalities, the alignment is not end-to-end optimized, limiting the amount of alignment which can be obtained.\nWe show that taking this BZSL approach and using our aligned embedding space of image and DNA learned using a contrastive loss, we can have a more accurate model.\nBy incorporating text into our model during pretraining, we can also leverage taxonomic annotations more explicitly, when available, without relying on their presence.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "To align representations of images, DNA barcodes, and taxonomy labels as text, we start with pretrained encoders for each modality and then apply LoRA finetuning [24  ###reference_b24###] with a multimodal contrastive loss (see Figure 1  ###reference_###).\nDuring inference, we use our fine-tuned encoders to extract features for a query image and match against a database of image and DNA embeddings (keys) for which the taxonomic information is already known.\nTo classify the query image,\nwe\ntake\nthe taxonomic information associated with the most closely matched key.\nWhile we can also query against the taxonomies themselves, the labels may be incomplete or unknown. Thus, images and DNA barcodes comprise a more robust and defining set of records against which to query."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training",
            "text": "Contrastive learning.\nWe base our approach on a contrastive learning scheme similar to CLIP [47  ###reference_b47###], which uses large-scale pretraining to learn joint embeddings of images and text.\nIn contrastive learning, embeddings for paired samples are pulled together while non-paired samples are pushed apart, thus aligning the semantic spaces for cross-modal retrieval.\nFollowing prior work [49  ###reference_b49###], we extend CLIP [47  ###reference_b47###] to three modalities by considering the modalities in pairs with the NT-Xent loss [50  ###reference_b50###] between two modalities to align their representations.\nLet matrices , , and  represent the batch of -normalized embeddings of the image, DNA, and text modalities.\nThe -th row of each corresponds to the same instance corresponding to a physical specimen, thus rows  and  are features from the same sample, forming a positive pair.\nFeatures in different rows  and , , come from different samples and are negative pairs. The contrastive loss for pair  is\nwhere  is a fixed temperature, based on the initial value used in [47  ###reference_b47###]. We apply the loss symmetrically to normalize over the possible paired embeddings for each modality [65  ###reference_b65###, 49  ###reference_b49###].\nWe repeat this for each pair of modalities and sum them to get the final loss, .\nPretrained encoders.\nFor each data-modality, we a pretrained model to initialize our encoders.\nImages: we use a ViT-B network111Loaded as vit_base_patch16_224 in the timm library. pretrained on ImageNet-21k and fine-tuned on ImageNet-1k [15  ###reference_b15###].\nDNA barcodes: we use BarcodeBERT [3  ###reference_b3###] with -mer tokenization, pretrained on about 893K DNA barcodes using masked language modelling. The training data for BarcodeBERT is highly similar to the DNA barcodes in the BIOSCAN-1M dataset making it ideal for our study.\nText: we use the pretrained BERT-small introduced by Turc et al. [57  ###reference_b57###] to encode taxonomic labels.\nLow-rank adaptation.\nTo efficiently fine-tune the pretrained transformer models, we apply Low-Rank Adaptation (LoRA) [24  ###reference_b24###], a method for fine-tuning large neural network models that greatly reduces the number of trainable parameters.\nFor our LoRA implementation, we add an additional low-rank residual layer to the query and key projectors of each attention module. The projectors thus take the form , where  represents the frozen parameters of the pretrained weights from the projector, whilst  and  represent the added low-rank weights. By choosing a rank  much smaller than the input and output dimensions  and , fewer parameters need to be updated with LoRA than with the original layer ().\nBy using LoRA, we are able to train with less parameters and less memory, allowing us to train with larger batch sizes on limited resources.\nThis is especially important for contrastive learning as\nincreasing the batch size allows for more positive and negative pairs when calculating the contrastive loss, and thereby improves learning effectiveness.\nExperimentally, LoRA reduces the number of trainable parameters from 203M to 1.9M. With batch size 400, the model requires just 71.5GB, allowing us to train on an A100 with 80GB, which would be infeasible without LoRA."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Inference",
            "text": "To use the model for predicting taxonomic labels, we calculate the cosine similarity between the embedded input image (query) and reference image or DNA embeddings (keys) sampled from available species. We take the taxonomic label (order, family, genus, species) associated with the closest key as the prediction.\nThis method allows us to evaluate our model in a zero-shot setting on species which were not seen by the model during the LoRA fine-tuning, as it is not constrained to predict within a fixed set of classes. The embedding space also gives us the flexibility to use the representation in other downstream models, such as a supervised classifier or a Bayesian model similar to [5  ###reference_b5###, 6  ###reference_b6###]."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Task and data",
            "text": "To evaluate our method, we perform taxonomic classification given an input image. The input is a biological image (query) along with a reference set of labelled DNA barcodes, other labelled biological images (key), or known taxonomic labels encoded as text (as demonstrated in Table 5  ###reference_###). We match the input feature with the closest neighbours in a database using aligned representations and assess accuracy across taxonomic levels by averaging over samples and taxon classes.\nThe predictions are evaluated at each taxonomic level by averaging accuracy over samples (micro) and taxon classes (macro).\nUnlike basic fine-tuning with a fully connected layer, our approach identifies unseen species using images or DNA without knowing all potential species upfront. Instead, we use reference features for taxonomic prediction or novel class identification. We split our data so that some species are \u201cunseen\u201d during training, and report prediction accuracy for both seen and unseen species to study model generalization.\n###figure_2### Dataset.\nWe use the BIOSCAN-1M dataset [18  ###reference_b18###], a curated collection of over one million insect data records. Each record in the dataset includes a high-quality insect image, expert-annotated taxonomic label, and a DNA barcode.\nHowever, the dataset has incomplete taxonomic labels, with fewer than 10% of records labelled at the species level. This poses a challenge for conventional supervised methods, which would require species-level annotations, but our method is able to flexibly leverage partial or missing taxonomic information during contrastive learning.\nThe dataset also possesses a long-tailed class imbalance, typical of real-world biological data, presenting a challenge for modelling.\nGiven the vast biodiversity of insects, with an estimated 80% undescribed [53  ###reference_b53###], and the necessity to discern subtle visual differences, this dataset offers a significant challenge and opporutnity for our model.\nData partitioning.\nWe split BIOSCAN-1M into train, validation, and test sets to evaluate zero-shot classification capabilities and model generalization to unseen species.\nRecords for well-represented species (at least 9 records) are partitioned at an 80/20 ratio into seen and unseen, with seen records allocated to each of the splits and unseen records allocated to validation and test.\nAll records without species labels are used in contrastive pretraining, and any species with 2\u20138 records are added evenly to the unseen splits in the validation and test sets.\nImportantly, we ensure that any unseen species are mutually exclusive between the validation and test sets and likewise do not overlap with seen species for labeled records.\nFinally, among each of the seen and unseen sub-splits within the validation and test sets, we allocate equal proportions of records as queries, to be used as inputs during evaluation, and keys, to be used as our reference database.\nSee Figure 2  ###reference_### for split statistics and Appendix A  ###reference_### for further details.\nData preprocessing.\nDuring inference, we resized images to  and applied a  center crop. For the DNA input, following Arias et al. [3  ###reference_b3###], we set a maximum length of 660 for each sequence and tokenized the input into non-overlapping 5-mers. Finally, similar to Stevens et al. [52  ###reference_b52###], we concatenated the taxonomic levels of the insects together as text input. As we did not have the common names of each record, we used the order, family, genus, and species, up to known labels. With this approach, we can still provide the model with knowledge of the higher-level taxonomy, even if some records do not have species-level annotations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "###figure_3### We study the model\u2019s ability to retrieve taxonomic labels using images in the BIOSCAN-1M dataset [18  ###reference_b18###] of species that were either seen or unseen during contrastive learning.\nWe also experiment on the INSECT dataset [5  ###reference_b5###] for Bayesian zero-shot learning (BZSL) species classification. We report the top-1 accuracy for the seen and unseen splits, as well as their harmonic mean (H.M.). In the main paper, we focus on evaluation on the validation set using image embeddings as queries as images are the most readily available modality.\nIn the appendices, we provide full results on both the validation and test set (Appendix B.1  ###reference_###), experiments that query with DNA features (Appendix B.2  ###reference_###), and visualization of the aligned embedding space (Appendix B.3  ###reference_###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Comparison with BioCLIP",
            "text": "Next we compare our aligned embedding space with that of BioCLIP [52  ###reference_b52###] by adapting their zero-shot learning demo script to perform species-level image classification. We use their pretrained model on the BIOSCAN-1M validation set, with image or text embeddings as keys.\nKeeping the experimental setups almost identical, we compared BioCLIP with BIOSCAN-CLIP in Table 5  ###reference_###.\nFor BioCLIP, we combined the four concatenated taxonomic levels with their provided openai_templates as text input, while for BIOSCAN-CLIP, we used the concatenated labels only.\nWhen using images as keys, BIOSCAN-CLIP consistently outperformed BioCLIP, even when BIOSCAN-CLIP was trained only on images and text. Since BioCLIP was trained on a much broader dataset, including but not limited to BIOSCAN-1M, it may have performed worse on insects as it was also trained on non-insect domains.\nBIOSCAN-CLIP can also leverage DNA features during inference, while BioCLIP is limited to image and text modalities.\nWhen using text as keys, BIOSCAN-CLIP performed better than BioCLIP on seen species but marginally worse on unseen species. This is expected, as BIOSCAN-CLIP was not trained on the unseen species names, which we also would not know prior in practice.\nThis illustrates that text typically does not perform well as keys and reinforces the benefit of using DNA in pretraining and inference."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Bayesian zero-shot learning",
            "text": "We also use our learned embeddings in Bayesian zero-shot learning (BZSL) [5  ###reference_b5###] to show the benefit of our learned embeddings for species classification\non the INSECT dataset [5  ###reference_b5###], which contains 21,212 pairs of insect images and DNA barcodes from 1,213 species.\nWe compare different combinations of image and DNA encoders. As baselines, we use a ResNet-101 image encoder, pretrained on ImageNet-1K (used in Badirli et al. [5  ###reference_b5###]), and the ViT-B [15  ###reference_b15###] image encoder, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k. For DNA encoders, we evaluate the baseline CNN from Badirli et al. [5  ###reference_b5###]; DNABERT-2 [66  ###reference_b66###], a BERT-based model trained on multi-species DNA data; and BarcodeBERT [3  ###reference_b3###], which was pretrained on arthropodic DNA barcode data.\nTable 6  ###reference_### shows that using the baseline image encoder with BIOSCAN-CLIP-D surpasses all baseline methods in harmonic mean even without fine-tuning on the INSECT dataset, performing particularly better on unseen species. Furthermore, using BIOSCAN-CLIP-I improves performance in all metrics over the baseline image encoder, with the highest performance after fine-tuning of 56.2% seen accuracy and 28.8% unseen accuracy. Thus, our model demonstrates the benefits of learning a shared embedding space relating image and DNA data, both in performance and the flexibility of applying to downstream tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce BIOSCAN-CLIP, an approach for integrating biological images with DNA barcodes and taxonomic labels to enhance taxonomic classification by using contrastive learning to align embeddings in a shared latent space.\nDue to their low-cost and ease of acquisition, images are the most practical modality for fostering inclusive participation in global biodiversity tracking efforts.\nWe show the BIOSCAN-CLIP embedding space can be applied to fine-grained retrieval tasks for seen and unseen species, and leveraged flexibly for downstream tasks such as zero-shot learning. Underrepresented and unseen species pose the greatest challenge for our model, presenting an opportunity for future work."
        }
    ],
    "url": "http://arxiv.org/html/2405.17537v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.17537v1",
        "paper_title": "BIOSCAN-CLIP: Bridging Vision and Genomics for Biodiversity Monitoring at Scale",
        "research_background": "The paper \"BIOSCAN-CLIP: Bridging Vision and Genomics for Biodiversity Monitoring at Scale\" explores the use of multimodal machine learning techniques to enhance biodiversity monitoring by combining image data, DNA information, and textual taxonomic labels. The motivation, research problem, and relevant prior work can be described as follows:\n\n### Motivation\nThe primary motivation for the paper is the accelerating rate of environmental change and habitat loss, which necessitates effective and scalable methods for monitoring biodiversity. Taxonomic classification at scale is critical for understanding regional biodiversity and studying species interactions. While recent advances in computer vision have improved the ability to classify species from images, these methods often fall short in capturing the rich evolutionary relationships between species and distinguishing fine-grained species differences. DNA sequences, particularly DNA barcodes, provide more accurate species identification but are expensive and technically demanding to collect. Therefore, there is a need for integrated approaches that leverage the strengths of both image and genetic data while minimizing the requirements for complex or costly data collection.\n\n### Research Problem\nThe research problem addressed in the paper is the development of a scalable, multimodal machine learning model that can efficiently and accurately classify biodiversity using a combination of biological images, textual taxonomic labels, and DNA barcodes. The key challenge is to create a unified latent space where these different types of data can be mapped and utilized for taxonomic classification without requiring comprehensive or noise-free taxonomic annotations. This work aims to demonstrate that integrating multiple modalities can lead to improved classification accuracy and facilitate future research into species commonalities and differences.\n\n### Relevant Prior Work\n1. **Computer Vision for Species Classification**: Previous efforts in ecological monitoring have applied computer vision techniques to classify species from images. These methods are limited by their inability to incorporate evolutionary relationships or discriminate fine-grained species differences.\n\n2. **DNA-Based Taxonomic Classification**: Researchers have used DNA sequences for genome understanding and taxonomic classification. DNA barcodes, such as those from the COI gene, are particularly useful for species identification, but their collection is labor-intensive and less accessible compared to image data.\n\n3. **Multimodal Machine Learning**: Advances in machine learning, such as CLIP, have shown the potential of using contrastive learning to encode different types of data (e.g., images and text) into a common latent space for tasks like zero-shot classification. BioCLIP extended this concept to align images of organisms with common names and taxonomic descriptions for classifying plants, animals, and fungi.\n\n4. **Single Modality Limitation**: Most existing works use isolated modalities\u2014image only, DNA only, or a combination of text and image\u2014without integrating all three, thereby missing out on the synergistic benefits of a multimodal approach.\n\nThe proposed BIOSCAN-CLIP model aims to overcome these limitations by using contrastive learning to align biological images, textual taxonomic labels, and DNA barcodes into a unified latent space, and demonstrates improved taxonomic classification accuracy in both retrieval and zero-shot settings.",
        "methodology": "The proposed method, BIOSCAN-CLIP, aligns representations of images, DNA barcodes, and taxonomy labels as text to facilitate large-scale biodiversity monitoring. The methodology employs the following key components and innovations:\n\n1. **Pretrained Encoders**: The approach begins with pretrained encoders for each modality - images, DNA barcodes, and taxonomy labels.\n\n2. **LoRA Finetuning**: These encoders are then fine-tuned using Low-Rank Adaptation (LoRA) along with a multimodal contrastive loss, to better align the different types of data (images, DNA barcodes, and text).\n\n3. **Inference Process**:\n    - During inference, the fine-tuned encoders are used to extract features from a query image.\n    - These features are then matched against a pre-established database of image and DNA embeddings, which are annotated with known taxonomic information.\n\n4. **Classification Approach**: The classification of the query image is achieved by identifying the taxonomic information associated with the most closely matched key (based on the embeddings). While querying against taxonomy labels directly is an option, it is often less reliable due to the potential for incomplete or unknown labels. Therefore, the method relies more robustly on images and DNA barcodes, which provide more substantial and definitive records for querying.\n\nThese innovations ensure that the model can effectively handle the diverse and often incomplete nature of taxonomic data, leveraging the strengths of image and DNA barcode data to enhance the accuracy and robustness of biodiversity monitoring.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Setup Details\n\n**Objective:** The main experiment aims to evaluate taxonomic classification capabilities of the BIOSCAN-CLIP model using biological images and DNA barcodes.\n\n**Input and Reference Data:** \n- **Images**: High-quality insect images.\n- **DNA Barcodes**: Labelled DNA barcodes.\n- **Textual Descriptions**: Known taxonomic labels encoded as text.\n\n**Methodology:**\n- Match the input features (biological image or DNA barcode) with the closest neighbours in a reference database using aligned representations.\n- **Evaluation Metrics:** Accuracy across taxonomic levels by averaging over samples (micro) and taxon classes (macro).\n- **Evaluation Scope:** Assess prediction accuracy for both seen and unseen species to test model generalization capabilities.\n\n**Dataset:**\n- **BIOSCAN-1M Dataset**: A curated collection of over one million insect data records including images, expert-annotated taxonomic labels, and DNA barcodes.\n- **Taxonomic Label Coverage:** Less than 10% of the dataset is labelled at the species level.\n- **Class Imbalance:** Presents a long-tailed class imbalance typical of real-world biological datasets.\n\n**Data Partitioning:**\n- **Seen vs. Unseen Species:** Species with at least 9 records are split 80/20 for seen and unseen, with seen species records in train, validation, and test sets, and unseen species records strictly in validation and test sets.\n- Records with no species labels are used for contrastive pretraining.\n- Species with 2-8 records are divided evenly into unseen splits within validation and test sets.\n- Ensured mutual exclusivity of unseen species between validation and test sets, and no overlap with seen species records.\n- Equal proportions of records among seen and unseen data are allocated as queries (inputs) and keys (reference database).\n\n**Data Preprocessing:**\n- **Images**: Resized and center-cropped.\n- **DNA Sequences**: Tokenized into non-overlapping 5-mers with a maximum length of 660.\n- **Text Input**: Concatenated taxonomic levels (order, family, genus, species) as text, using known labels.\n\n#### Main Experimental Results\n\n- The BIOSCAN-CLIP model **successfully demonstrated** the ability to classify and identify unseen species without requiring all potential species to be known upfront.\n- **Accuracy assessment** showed favorable results both at macro and micro levels, indicating robust performance across taxonomic hierarchies.\n- Despite incomplete species-level labelling and the inherent class imbalance, the model was able to leverage available data effectively during contrastive learning, exemplifying its flexible adaptation to real-world biodiversity monitoring challenges.\n- Experimental outcomes highlighted the model\u2019s potential in identifying subtle visual differences among insect species and generalization to unseen data, validating the efficacy of the proposed approach."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine whether including DNA barcodes as an additional modality enhances taxonomic classification accuracy using contrastive learning in a multimodal setup.",
            "experiment_process": "Experiments were conducted on BIOSCAN-1M to compare the performance of various combinations of modalities: image (I), DNA (D), and text (T). The models were trained on two 80GB A100 GPUs for 15 epochs with a batch size of 800 using the Adam optimizer. Top-1 accuracy was reported for the BIOSCAN-1M validation set (both micro and macro accuracy), analyzing performance for seen and unseen species. Different key types (image alone, DNA alone, and their average) were assessed for image-based querying. Additional experiments combined image embeddings for seen species and DNA embeddings for unseen species using two strategies: a 1-nearest neighbor (NN) classifier and a supervised classifier fine-tuned with the image encoder.",
            "result_discussion": "Contrastive learning improved accuracy across all metrics, with the highest accuracy achieved using all three modalities (I+D+T). Using DNA barcodes as keys resulted in significantly lower accuracy compared to using image keys, indicating imperfect alignment across modalities. Combining image and DNA features sometimes increased accuracy for seen species. The nearest neighbor approach with a combined strategy (IS+DU) performed best in identifying seen species and showed competitive accuracy for unseen species. The BZSL method with BIOSCAN-CLIP embeddings outperformed naive methods in macro-level accuracy but underperformed in micro-accuracy due to data skew.",
            "ablation_id": "2405.17537v1.No1"
        },
        {
            "research_objective": "To compare the performance of BIOSCAN-CLIP with BioCLIP in species-level image classification.",
            "experiment_process": "The BioCLIP model was adapted using their zero-shot learning demo script for species-level image classification on the BIOSCAN-1M validation set, while keeping experimental setups identical. BioCLIP's pretrained model was used with image or text embeddings as keys. BIOSCAN-CLIP was trained on images and text, and taxonomic labels were concatenated for text input.",
            "result_discussion": "BIOSCAN-CLIP consistently outperformed BioCLIP when using images as keys. BioCLIP's broader dataset training, including non-insect domains, might have hindered its insect-specific performance. BIOSCAN-CLIP also leverages DNA features during inference, unlike BioCLIP. When using text as keys, BIOSCAN-CLIP performed better on seen species but slightly worse on unseen species, illustrating the benefit of DNA features in both pretraining and inference.",
            "ablation_id": "2405.17537v1.No2"
        },
        {
            "research_objective": "To demonstrate the benefits of BIOSCAN-CLIP's learned embeddings for species classification in a Bayesian zero-shot learning (BZSL) context.",
            "experiment_process": "The BZSL method was applied on the INSECT dataset, containing 21,212 pairs of insect images and DNA barcodes from 1,213 species. Comparisons involved different combinations of image and DNA encoders. Baseline image encoders included a ResNet-101 pretrained on ImageNet-1K and ViT-B pretrained on ImageNet-21k and fine-tuned on ImageNet-1k. DNA encoders included the CNN from Badirli et al., DNABERT-2, and BarcodeBERT.",
            "result_discussion": "BIOSCAN-CLIP-D with the baseline image encoder surpassed other baseline methods in harmonic mean even without fine-tuning, especially for unseen species. BIOSCAN-CLIP-I improved performance in all metrics over the baseline image encoder, achieving the highest performance after fine-tuning with 56.2% accuracy for seen species and 28.8% for unseen species. This demonstrates the flexibility and performance benefits of learning a shared embedding space relating image and DNA data.",
            "ablation_id": "2405.17537v1.No3"
        }
    ]
}