{
    "title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
    "abstract": "The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application\u2019s functionality and end-user needs.\nWe introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at https://bit.ly/3w3yKcS",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### One of the long-lasting goals for intelligent agents Winograd (1972  ###reference_b58###) is for them to seamlessly interact with humans in natural language and help their end-users with their tasks, such as completing household tasks, math tutoring, and so on.\nThe rapid development of open-source libraries Wu et al. (2023  ###reference_b59###); Li et al. (2023a  ###reference_b29###) helps that goal by simplifying the development of LLM-powered agentic applications for various user-centered tasks Liang et al. (2023b  ###reference_b34###); Hong et al. (2023  ###reference_b18###); Talebirad and Nadiri (2023  ###reference_b49###). To ensure that the application\u2019s behavior meets the requirements of the application developers, it is also crucial to assess its potential utility to end users Dibia et al. (2023  ###reference_b13###), as this can significantly impact its improvement journey. Taking into account a range of applications, it is unrealistic to assume benchmarking for every domain, including but not limited to code generation Liu et al. (2024  ###reference_b36###), health care Andrew (2024  ###reference_b2###), and many others whose development we witness every day Wu et al. (2023  ###reference_b59###).\nMoreover, directly evaluating agentic applications poses challenges, as current approaches predominantly rely on end-to-end success metrics i.e., whether the application accomplishes tasks Shridhar et al. (2020b  ###reference_b47###, 2019  ###reference_b45###); Myers et al. (2023  ###reference_b40###). However, understanding a user\u2019s interactions with an application involves much more than success alone Kiseleva et al. (2022a  ###reference_b24###, b  ###reference_b25###); Zhang et al. (2023  ###reference_b61###). Consider math problem solving, although it is important that the application solves the problem correctly, its ability to present and explain solutions based on various criteria, such as completeness, conciseness, and clarity, is crucial.\nFurthermore, success is not always clearly defined for a task. Recognizing such criteria and being able to quantify them is essential to assess whether developer requirements are being satisfied and if the application brings utility to the end-users. Given the objective of assessing arbitrary applications, relying solely on end-to-end success metrics is untenable, due to the expansive range of tasks requiring automation. The question is how to design a flexible methodology to assess the task utility for diverse set of applications?\nTo bridge this gap, we introduce AgentEval, a framework to gauge the utility of LLM-powered applications. Its goal is to assess the utility by providing application developers with insights into how the current flow can be characterized. AgentEval builds on recent work showing that LLMs can be a scalable and cost-effective alternative to human evaluation for open-ended tasks Li et al. (2023b  ###reference_b30###).\nAgentEval as illustrated in Fig. 1  ###reference_###, consists of the three following agents, formally defined in Sec. 3  ###reference_###:\n(1) CriticAgent suggests the list of criteria based on the task description and a pair of solutions, where one is preferred over the other one (e.g., successful and failed examples). For instance, for math problems, the criteria could be be Efficiency and Clarity of the proposed solution;\n(2) QuantifierAgent quantifies how the solution performs for each criterion and returns the utility function, e.g. for math problems, if the \u2019 Clarity is \u2018not clear\u2019, \u2018moderately clear\u2019, or \u2018very clear\u2019;\n(3) VerifierAgent verifies the quality of the assessment of the suggested criteria to make sure the criteria are essential, robust, informative and have high discriminative power.\nIn summary, our main contributions are:\nIntroducing AgentEval, a novel framework that leverages LLM-powered agents as a scalable and cost-effective alternative to human evaluations, to produce task utility through the collaboration of three agents: CriticAgent, QuantifierAgent and VerifierAgent; and\nAn in-depth analysis of AgentEval robustness for two applications across different solutions, that can be replicated on an unseen domain."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Evaluation of LLMs",
            "text": "Prior work Guo et al. (2023  ###reference_b15###); Ziyu et al. (2023  ###reference_b62###); Chang et al. (2023  ###reference_b9###); Liang et al. (2023a  ###reference_b33###) has extensively studied the evaluation of LLMs on various fronts: how ethically sound they are Stahl and Eke (2024  ###reference_b48###), how they align to human preferences Hendrycks et al. (2021a  ###reference_b16###); K\u00f6pf et al. (2024  ###reference_b28###), their robustness Wang et al. (2023b  ###reference_b53###), and the knowledge, and reasoning capabilities they posses Bian et al. (2023  ###reference_b6###).\nRecent work evaluates LLMs on more specialized tasks, such as medical domain Jin et al. (2019  ###reference_b21###), multi-modal tasks Mialon et al. (2023  ###reference_b39###); Bang et al. (2023  ###reference_b4###), or as agents in interactive environments Liu et al. (2023  ###reference_b37###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "User satisfaction prediction",
            "text": "Studies suggest that users interacting with various systems operate with specific utility functions in mind Li et al. (2020  ###reference_b31###); Azzopardi et al. (2018  ###reference_b3###); Ahmadvand et al. (2022  ###reference_b1###). Traditionally, metrics defining user satisfaction were designed using large-scale collected behavioral signals Kiseleva et al. (2014  ###reference_b22###), and were tailored to specific applications, such as intelligent assistants Kiseleva et al. (2016a  ###reference_b26###, b  ###reference_b27###), web search engines Williams et al. (2016a  ###reference_b55###, b  ###reference_b56###); Williams and Zitouni (2017  ###reference_b57###), dialogue systems See et al. (2019  ###reference_b43###), multi-turn conversations Li et al. (2021  ###reference_b32###) and general-purpose personal assistants Kiseleva and de Rijke (2017  ###reference_b23###).\nIt was demonstrated that assessing users\u2019 satisfaction requires goes beyond a single metric. As such, here, we propose a flexible framework to assess user and developer requirements, which can eventually be used to improve the application flow."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Using LLMs as evaluators",
            "text": "More recently, there has been a growing trend in utilizing LLMs as evaluators Chiang and Lee (2023  ###reference_b11###); Fu et al. (2023  ###reference_b14###), such as for qualitative research Bano et al. (2023  ###reference_b5###), or summarization.\nSpecifically, Jain et al. (2023  ###reference_b20###) studied the efficacy of few-shot prompted LLM evaluators in evaluating summaries that were written by other LLMs. Similarly, Wang et al. (2023a  ###reference_b52###) explore if ChatGPT itself can be used as an evaluator, by prompting it to score texts. Other works Tjuatja et al. (2023  ###reference_b50###); Liu and Sun (2023  ###reference_b35###); Chiang and Lee (2023  ###reference_b11###) look at how LLMs can be used as proxies for human behavior, or work with humans, such as CoEval Li et al. (2023b  ###reference_b30###), which showed how LLMs can make human evaluation easier. Pan et al. (2024  ###reference_b41###) also show how LLM evaluators can help build models that increase performance on downstream task. Building on the above, a different line of works identify weaknesses in single LLMs as direct evaluators Huang et al. (2023  ###reference_b19###), and propose to improve them, such as a multi-step calibration framework Wang et al. (2023c  ###reference_b54###).\nGiven these drawbacks, recent work has looked at how multiple LLM agents can be used as evaluators. Chan et al. (2023  ###reference_b8###), proposed ChatEval, a multi-agent team that discusses and evaluates responses from agents on generation tasks (debate-style), leading to text that aligns with better human preferences. Similarly, Chern et al. (2024  ###reference_b10###) proposed a multiple agent-debate-assisted meta-evaluation framework.\nBuilding on these works, we propose an automatic multi-agent assessment of utility for arbitrary LLM-powered applications, to provide deep insights for developers. Our framework can uncover current flaws in these applications, and may lead to improvements in them, particularly if the application flow changes after it is applied, and then it is re-used."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Task Utility",
            "text": "Fig. 2  ###reference_### outlines a taxonomy of target tasks for LLM-powered applications, in terms of success metrics. At a high level, these tasks can be categorized into:\n1) Success is not clearly defined \u2014 Users use the system in an assistive manner, seeking suggestions from it, rather than expecting it to solve the task. For example, a user can request the system to generate an email. The user usually uses the system\u2019s response as a template, which can later be edited.\nDirectly evaluating assistive tasks like these is hard, particularly for online evaluation, or when dealing with less well-defined tasks. One potential approach is to directly ask users how useful the help was, but this is not well-calibrated Borisov et al. (2018  ###reference_b7###), hard to quantify Sepliarskaia et al. (2018  ###reference_b44###), and expensive.\n2) Success is clearly defined \u2014 It is clear whether the system solved the task or not, for example, assisting with household tasks, where success is clear and measurable. This category can be further divided into two subcategories:\nan optimal solution exists \u2014 only one successful outcome is possible. For example, when asking an assistant to turn on a light, success is clearly defined, as there is only one way to do it.\nmultiple solutions exist \u2014 Increasingly, we observe situations where multiple trajectories of agent behavior can lead to success. For example, when asking an agent to suggest a food recipe, success could be multiple cuisines tasting good, but perhaps the recipe should not be expensive.\n\n###figure_2### AgentEval is currently focused on tasks where success is clearly defined and multiple successful solutions may exist.\nPrevious research on assistive agents suggests human pairwise preferences as one of the most optimal assessments, i.e. when the annotator is presented with two agents side by side and asked for their preferences Kiseleva et al. (2022b  ###reference_b25###). In this setup of side-by-side pairwise comparison, humans tend to suggest a list criteria, explaining why they prefer one agent over the other. For instance,\u2018the first agent was faster\u2019 or \u2018the second agent converses more naturally\u2019. This comparative setup can guide humans to come up with a list of criteria that helps to infer the utility of the task. With this in mind, we designed AgentEval (Fig. 1  ###reference_###),\nby employing LLMs to help us understand, verify, and assess task utility, namely:\nCriticAgent: The goal of this agent is to suggest a set of criteria that can be used to assess task utility. The CriticAgent is given a task description, as well as optionally several pairs of solutions, where preferably some are preferred over the other ones, for instance, successful and failed examples. CriticAgent would return a set of criteria , where each criterion  is accompanied by a set of accepted values  as . For example, for solving math problems, the CriticAgent generated accepted values and criteria such as clarity, efficiency, and more - see Tab. 1  ###reference_###.\nQuantifierAgent: The goal of QuantifierAgent is to quantify each of the suggested criterion, to access the task utility of the system , for the end user. We define the Utility for task  as: . where  represents the task sample and  is the quantifier output for sample  based on the criterion . For example, for math problem solving, given the generated criteria shown in Tab. 1  ###reference_###, the solution\u2019s Accuracy could be quantified as \u201cIncorrect\u201d, \u201cpartially correct\u201d or \u201ccorrect\u201d. Eligible quantified values for quantification process are shown in \u201cAccepted values\u201d column in Tab. 1  ###reference_###\nVerifierAgent: There might be cases where not all the criteria suggested by CriticAgent help assess utility. Some criteria might be redundant, while others may not aid in distinguishing performance. VerifierAgent validates the quality of the criteria in terms of robustness and their distinguishability of noisy samples. Essentially, it checks (1) if the criteria can be quantified robustly over repeated samples, and (2) if QuantifierAgent can identify the adversarial attacked targeted samples from the original ones. If the sanity checks do not pass, VerifierAgent will update the list of criteria, to end up with a set of robust, stable, informative and distinguishable criteria for assessment.\nFinally, we note that AgentEval allows for incorporating a human in the loop in the role of a domain expert. For instance, CriticAgent could be replaced by a human expert who either comes up with the relevant criteria or helps VerifierAgent verify the useful criteria and filter out the unessential ones."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Datasets and Solutions",
            "text": "This section provides an overview of the datasets utilized in our study i.e., Math problem solving and ALFWorld household task.\nThe math dataset is chosen for its widespread usage and complex problem-solving scenarios that are fundamental in evaluating the effectiveness.\nALFWorld dataset offers a scenario involving multi-turn interactions within a moderately approximated multi-modal environment.\nEach dataset plays a critical role in evaluating different aspects of AgentEval\u2019s capabilities, from handling complex theoretical problems to navigating real-world scenarios.\nIn both tasks, although success is clearly defined, multiple solutions exist for accomplishing the objectives. An example of Math problem solving and ALFWorld task is shown in Appendix A.1  ###reference_###.\nDue to space, we report all experiments about Math problem solving in the main paper and we keep all the experiments related to ALFWorld dataset in the Appendix A.3  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "MATH Problem Solving",
            "text": "Dataset: The MATH dataset is a substantial collection of 12,500 challenging mathematics problems from high school competitions Hendrycks et al. (2021b  ###reference_b17###). Each problem comes with a step-by-step solution and is tagged by difficulty levels.\nSimilar to the math problem experimental setup in Wu et al. (2023  ###reference_b59###), we carry out evaluations on 120 problems from level-5 by three different solutions. Due to limited space, for more details about this dataset, we refer readers to Appendix A.2  ###reference_###\nSolutions: In establishing solutions for this task to assess, we draw inspiration from the experiments showcased in Wu et al. (2023  ###reference_b59###). We evaluate the proposed methodology by AutoGen Wu et al. (2023  ###reference_b59###), as well as Langchain ReAct Yao et al. (2022  ###reference_b60###) and a Vanilla solver that employs GPT-4 to tackle the task. These solutions have previously demonstrated promising and competitive performance Wu et al. (2023  ###reference_b59###).\nIn Sec. 5.2  ###reference_###, we explore how the measured performance with AgentEval correlates with the ground truths."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "ALFWorld Household Task",
            "text": "Dataset: ALFWorld presents a set of language-based interactive decision-making tasks within simulated household environments Shridhar et al. (2020b  ###reference_b47###).\nALFWorld is the first interactive parallel environment that aligns text descriptions and commands with physically embodied robotic simulation.\nFinally, the dataset\u2019s inclusion of household chores to more intricate problem-solving scenarios, provides a comprehensive testbed for evaluating the adaptability of multi-agent systems. For more information about the dataset and examples of the test cases, we refer the readers to Appendix A.3.1  ###reference_.SSS1###.\nSolutions:\nAs for the solutions to assess for ALFWorld Household tasks, similar to Wu et al. (2023  ###reference_b59###), we consider ReAct Yao et al. (2022  ###reference_b60###) as well as AutoGen with two agents and AutoGen with three agents Wu et al. (2023  ###reference_b59###). In Appendix A.3.2  ###reference_.SSS2###, we discuss in more details the solutions under assessment.\nWe assess and compare the performance of these three solutions using AgentEval."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Implementation Details",
            "text": "For all experiments, we use GPT-4 version 0613, accessed through Azure OpenAI services, as the LLM model and the temperature of 0. AgentEval utilizes AutoGen Wu et al. (2023  ###reference_b59###)\nfor implementation, since it provides a versatile environment where agents can be finely tuned and customized based on specific application needs. This is crucial for maintaining the flexibility to handle a wide range of applications. We tried to avoid much prompt engineering and tried to keep each agent\u2019s instructions as if we are instructing human annotators. Moreover, another advantages of using AutoGen for implementation of AgentEval is that it has the flexibility to involve human in the loop. Each agent could be replaced by a human annotator. We further provide all the prompts used in our experiments in our Git repository."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Robustness Analysis and Verification",
            "text": "In this section, we first analyze the robustness of AgentEval, then further investigate how VerifierAgent can increase the stability of our assessment."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Diversity of Criteria",
            "text": "Here, our main goal is to study the diversity of the suggested criteria.\nWe investigate the extent inputs to AgentEval (Fig. 1  ###reference_### such as \u2018Task Description\u2019 and \u2018Successful/Failed Executions\u2019) contribute to CriticAgent for creating a more diverse set of criteria. To do so, we use two distinct methods, with CriticAgent generating (1) \u201ctask-based\u201d criteria solely from the task description, and (2) \u201csolution-based\u201d criteria, derived from both the task and execution examples. For example, a solution to a mathematical problem, might satisfy criteria such as \u2018Accuracy\u2019 and \u2018Clarity\u2019, independent of the solution. However, when additional tools such as coding are used to solve the problems, additional criteria like \u2018Code Efficiency\u2019 may be introduced to the set of criteria. This makes sense, since the application leveraged coding to solve math problems.\nFig. 4  ###reference_### displays the number of unique criteria extracted for mathematical problem solving in task-based mode, and three different solution-based approaches.\nTo keep the balance between computational costs and analyzing the robustness, we conducted 50 runs of the CriticAgent with different seeds. Subsequently, for  iterations, we randomly select  samples, as shown on the x-axis of Fig. 4  ###reference_###, and present the average number of unique extracted criteria, along with its 95% confidence interval after repeating this process 50 times. We note that because the total pool of criteria includes 50 iterations in total, the confidence intervals become smaller when  get closer to the\nmaximum number of samples i.e., 50\nTo gain deeper insights into diversity of criteria, we took a closer look at them to study if they are truly unique or to what extent they have similarities. This is important to determine if CriticAgent, when continually generating criteria, will always produce new criteria, or if it will eventually converge to a set.\nWe noted that some criteria are similar but worded differently. For example, \u2018Problem Complexity\u2019 vs. \u2018Problem Difficulty\u2019 or \u2018Time Taken\u2019 vs. \u2018Time to Completion\u2019. Tab. 3  ###reference_### in the Appendix lists such instances.\nTo consolidate the similar criteria and reduce noise in the number of unique criteria and redundancy, inspired from previous work Liu et al. (2022  ###reference_b38###); Vahtola et al. (2022  ###reference_b51###); Reimers and Gurevych (2019  ###reference_b42###), we employ a pre-trained language model fine-tuned for paraphrasing111https://bit.ly/3UgsYOp  ###reference_bit.ly/3UgsYOp###, to measure the semantic similarity of criteria descriptions. Using a threshold , we classify pairs with cosine similarity greater than  as semi-identical ones and select one of them as the representative of the pair.\nFig. 4  ###reference_### illustrates the impact of different  values (0.7, 0.85, 1) on the diversity of criteria. A threshold of 1 means no filtering occurs.\nThis analysis shows that the solution-based approach has potential to produce more diverse criteria than the task-based approach, although this varies by the creativity of the model. For example, while the AutoGen solution demonstrates the highest diversity, task-based methods yield more unique criteria than ReAct and Vanilla Solver. Another interesting observation is that repeating the CriticAgent will eventually lead to a convergence in the number of criteria. This suggests that the\nCriticAgent\u2019s ability to create new criteria will diminish, converging to an almost finite list of criteria, which will reduce the cost as well."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Verification",
            "text": "As outlined in Sec. 3  ###reference_### and illustrated in Fig. 1  ###reference_###, the VerifierAgent\u2019s primary role is to ensure the selected criteria are effective toward evaluating the utility for the end-user, while maintaining robustness and high discriminative power. To achieve this, the VerifierAgent undertakes two main actions:\n(1) Criteria Stability:\nThe criteria should be essential and robust, meaning they should not be redundant and we should be able to quantify them stably if we repeatedly quantify it for an individual solution, showing no divergence.\nAs such, VerifierAgent enhances the criteria by iterating over the generation and quantification phases. It then consolidates these criteria by identifying and eliminating redundancies, followed by evaluating the dispersion of the distribution of the quantified criteria. This step modifies the criteria, ensuring that only the most robust criteria are retained.\n(2) Discriminative Power: A reliable evaluation should detect and withstand noise. To test that, we propose to use adversarial examples and then assess the system\u2019s ability to differentiate between these compromised examples and standard cases. Should the system fail to distinguish effectively, it indicates that the criteria are insufficient for reliable assessment under varied conditions.\nWe note that both steps involve a tunable threshold that can be adapted based on application needs, ensuring flexible criteria validation. The proposed methodology for VerifierAgent is summarized in Algorithm 1  ###reference_### in the Appendix."
        },
        {
            "section_id": "6.2.1",
            "parent_section_id": "6.2",
            "section_name": "6.2.1 Criteria Stability",
            "text": "Our goal here is to explore the stability of criteria and robustness of the quantifier for having a more essential, robust and stable set of criteria.\nWe specifically evaluate the QuantifierAgent\u2019s robustness using criteria for mathematical problems (Table 1  ###reference_###), conducting 50 repeats of runs with different seeds on 120 problems (Section 4.1  ###reference_###). Ideal expected outcomes include consistent performance across all criteria on all the repeats.\nFig. 5  ###reference_### illustrates the distribution of quantifier values for both failed (dark blue) and successful cases (light blue) across all criteria through box plots. The more robust a criterion, the narrower the range of quantified performance (narrower box plots). Also, the less overlap between the successful and failed boxes, the higher the distinguishability of the criteria.\nWe observe that all four criteria, except \u2018error analysis\u2019 allow for easy differentiation between successful and failed cases. Additionally, some criteria prove to be more robust compared to others. We believe that such an analysis of the quantifier agent\u2019s performance will yield valuable insights for enhancing reliability, trustworthiness, and explainability in performance evaluation. A detailed examination of the stability of each criterion, especially how they differentiate between successful and failed cases, is provided in Appendix A.4.2  ###reference_.SSS2###.\n###figure_5### Further, to refine and expand the criteria set without redundancy, we operate the CriticAgent multiple times i.e., we execute CriticAgent 50 times with varied seeds. The criteria are then summarized into one list of useful criteria using the LLM. Additionally, as explained in Section 6.1  ###reference_###, we remove similar and redundant criteria using pre-trained language models, thus obtaining a comprehensive list of criteria.\nThe refined criteria after 50 repeats are detailed in Tab. 4  ###reference_### in the Appendix.\nNow, we aim to determine the stability of these criteria through repeated quantifications. Our goal is to identify criteria that maintain consistent results without significant divergence, even when quantified multiple times.\nUsing this consolidated list, we measure the dispersion of quantified results using the coefficient of variation, a standardized metric that facilitates comparison across various test cases when QuantifierAgent quantifies them.\nGiven the consolidated list of criteria, we use the QuantifierAgent to quantify various test cases and report the coefficient of variation as a measure of the dispersion of the QuantifierAgent\u2019s outputs with respect to each criterion across different seeds and report the mean coefficient of variation across all samples.\nwe run QuantifierAgent with 50 seeds and plot the change () in the sum of mean coefficient of variation across all criteria against the number of seeds, in Figure 6  ###reference_###. For each criterion, we compute the absolute difference with the mean coefficient of variation calculated when using  seeds, summing up the absolute differences across all criteria. According to the plot, after approximately 18 seeds, the magnitude of mean coefficient of variation stabilizes and becomes rather trivial.\nIn almost all cases, the mean coefficient of variation is around or below 0.5, which is relatively small, suggesting that QuantifierAgent is quite robust.\n###figure_6###"
        },
        {
            "section_id": "6.2.2",
            "parent_section_id": "6.2",
            "section_name": "6.2.2 Discriminative Power",
            "text": "It is crucial to ensure the quality of quantification of each criterion. Ideally, this validation would involve comparisons with known pairwise samples, where sample  is definitively superior to  for a given criterion. If the evaluator also confirms superiority of  w.r.t , it has robust quantification. However, due to rapid expansion of LLM-powered applications, obtaining annotated data for many tasks is often unfeasible. Therefore, we propose using synthetically altered versions of samples for verification.\nLet us assume we have an alternative disturbed version of sample , which is called .\nAssuming sample  is more likely to outperform its disturbed version , our assessment should confirm this assumption by assigning better quantified performance  in comparison to . In experiments with mathematical problems, we introduced random noise by removing portions of the solution sentences from AutoGen, VanillaSolver, and ReAct\u2019s results respectively, expecting that criteria like \u2018Completeness\u2019 or \u2018Clarity\u2019 would show be higherin  than in .\nWe disturbed solutions by removing 25% of the sentences and assessed the QuantifierAgent\u2019s performance. As shown in Fig. 7  ###reference_###, criteria measuring aspects like \u2018Clarity\u2019 and \u2018Completeness\u2019 were lower in disturbed solutions (lighter bars), confirming QuantifierAgent\u2019s high discriminative power and effectiveness.\nWe have already filtered out the criteria that were unstable, i.e., those that had a high mean standard deviation and dispersion when being quantified in the previous section. We report the results of the QuantifierAgent quantifying differences between original and disturbed samples on the comprehensive set of criteria shown in Appendix, as shown in Fig. 13  ###reference_### for the math problem-solving. In most cases, the QuantifierAgent quantifies the disturbed output to be worse than the original task output. We believe analyzing the QuantifierAgent\u2019s performance will enhance the reliability, trustworthiness, and explainability in evaluations.."
        },
        {
            "section_id": "6.2.3",
            "parent_section_id": "6.2",
            "section_name": "6.2.3 VerifierAgent",
            "text": "After modifying the list of criteria (Sec. 6.2.1  ###reference_.SSS1###), we have developed a stable and robust list of criteria that the QuantifierAgent can reliably quantify. Further, we also proposed a method for assessing whether the criteria can distinguish between noise-adversarially attacked samples and the original ones.\nThese two tests will serve as input for the VerifierAgent (described in Algorithm 1  ###reference_###), which can also have its threshold tuned for different applications. For instance, one might prioritize the stability of the criteria, while another may value the discriminative power of the AgentEval for specific applications. As such, the VerifierAgent will modify and update the criteria based on to what extend they pass the two tests, i.e., if the mean coefficient of variation is below a specific threshold and the percentage of adversarial testing it has passed. The VerifierAgent will then update the criteria if necessary. We believe that having a VerifierAgent would help continuously updating the criteria as needed because, by improving the systems, we may require new criteria that were not previously necessary for utility assessment.\n###figure_7###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "We introduced the AgentEval framework, designed to swiftly gauge the utility of arbitrary LLM-powered agentic applications. Our framework leverages recent findings suggesting LLMs as a scalable and cost-effective alternative to human evaluations for open-ended tasks. AgentEval consists of three agents: CriticAgent suggests criteria based on task descriptions and executions of the applications, QuantifierAgent quantifies how well the application flow aligns with these criteria, and VerifierAgent modifies the list of criteria if needed. This framework is customizable, adaptable, and can operate in various modes, employing combinations of LLMs, human inputs, and tools.\nWe believe that suggested AgentEval\u2019s utility extends beyond immediate performance. It can uncover new system capabilities over time and adapt to changes in user needs tracked by developers. AgentEval can also enable developers to assess the alignment between application behavior and suggested user requirements, providing them with insights into areas for improvement.\nIn summary, our contributions include introducing the AgentEval framework, and conducting a robust analysis of its performance across various datasets and baselines. AgentEval represents a significant step towards assessing LLM-powered applications."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations and Ethics",
            "text": ""
        },
        {
            "section_id": "8.1",
            "parent_section_id": "8",
            "section_name": "Limitations",
            "text": "Here, we discuss some limitations of the AgentEval framework. Firstly, the performance of the AgentEval is highly dependent on the quality of the output logs of the applications. Flaws or limitations in these outputs can significantly impact the framework\u2019s ability to accurately assess utility.\nSecondly, our experiments were conducted exclusively with closed-source LLMs, specifically with GPT-4. This may limit the generalizability of our findings. Plans to include a broader array of LLMs, including open-source models, are considered for future studies to validate and possibly enhance the robustness of our conclusions.\nAdditionally, the tests conducted were limited to specific scenarios within math problem solving and household tasks. Expanding the diversity of test scenarios could help in understanding the broader applicability of the framework.\nThirdly, while AgentEval employs a novel methodology leveraging LLMs to estimate utility, the absence of human evaluation in our validation process could be viewed as a drawback. Human evaluations provide unique insights, especially in subjective aspects of utility that automated systems might overlook. However, such evaluations are often cost-prohibitive and logistically challenging, restricting our ability to implement them within this study. Especially do developers of agentic LLM-powered applications who needs insights fast as they go with the deployments.\nLastly, as LLM technologies evolve, the criteria and metrics used for evaluation may need to be updated or revised. What works for assessing current LLMs may not hold as these models become more advanced. Continuous updates to the evaluation framework will be necessary to keep pace with technological advancements."
        },
        {
            "section_id": "8.2",
            "parent_section_id": "8",
            "section_name": "Ethics",
            "text": "To the best of our knowledge, we did not violate any code of ethics with the experiments done in this paper. We reported technical details and results, with details in the main paper, Appendix, and code release. Our experimental results are an outcome of a Machine Learning model.\nOur AgentEval system has a variety of uses in real world settings, such as improving applications for end users or helping developers. However, we caution that it must be used carefully, as the outputs are from a ML model and can have real world consequences, if used incorrectly.\nThese and many other related issues are important aspects to consider when deploying a system like AgentEval in the real world."
        }
    ],
    "url": "http://arxiv.org/html/2405.02178v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2",
            "6",
            "6.1",
            "6.2",
            "6.2.1",
            "6.2.2",
            "6.2.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "5.2",
            "6.1",
            "6.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.02178v2",
        "paper_title": "Assessing and Verifying Task Utility in LLM-Powered Applications",
        "research_background": "**Motivation:**\n\nThe paper is motivated by the long-standing goal of creating intelligent agents that can seamlessly interact with humans using natural language to assist with various tasks such as household chores and math tutoring. The rapid development of open-source libraries has simplified the creation of LLM-powered applications across a variety of user-centered tasks. However, it is crucial to assess the utility of these applications from the perspective of end-users to ensure they meet the developer's requirements and are genuinely helpful. The traditional benchmark methods are impractical for every possible domain, and current approaches, which focus predominantly on end-to-end success metrics, are insufficient as they do not capture the nuanced interactions users have with the application.\n\n**Research Problem:**\n\nThe central problem addressed in the paper is how to design a flexible methodology that can assess the task utility of a diverse set of LLM-powered applications. Current evaluation methods mainly focus on whether an application accomplishes its tasks but fail to account for other essential criteria such as the presentation and explanation quality of solutions. Given the wide range of tasks requiring automation, relying solely on end-to-end success metrics is not feasible. The challenge, therefore, is in devising a system that can quantitatively assess various dimensions of task utility across different applications.\n\n**Relevant Prior Work:**\n\n1. **Interaction with Humans in Natural Language:**\n   - Winograd (1972) highlighted long-term goals for intelligent agents to seamlessly interact with humans and assist in various tasks.\n   \n2. **Development of LLM-Powered Applications:**\n   - Open-source libraries by Wu et al. (2023) and Li et al. (2023a) have facilitated the development of such applications.\n   - User-centered tasks development has been addressed by Liang et al. (2023b) and Hong et al. (2023).\n\n3. **Assessment of Application Utility:**\n   - Dibia et al. (2023) emphasized the importance of assessing applications based on their utility to end-users.\n   \n4. **Challenges with End-to-End Success Metrics:**\n   - Shridhar et al. (2019, 2020b) and Myers et al. (2023) discussed the limitations of using end-to-end success metrics alone for evaluating agentic applications.\n   - A broader understanding of user interactions was elaborated by Kiseleva et al. (2022a, b) and Zhang et al. (2023).\n\n5. **Quantitative Criterion-Based Evaluation:**\n   - The need for criteria beyond success, such as clarity and completeness, was underscored in recent evaluations of math-related applications.\n   \n6. **Scalable and Cost-Effective Human Evaluations:**\n   - Li et al. (2023b) demonstrated that LLMs could serve as scalable and cost-effective alternatives to human evaluations for open-ended tasks.\n\n**Proposed Solution:**\n\nThe paper introduces **AgentEval**, a framework designed to assess the utility of LLM-powered applications. The framework uses three agents:\n1. **CriticAgent** - Suggests criteria for evaluation based on task descriptions and distinguishing features of solutions.\n2. **QuantifierAgent** - Quantifies performance based on the proposed criteria.\n3. **VerifierAgent** - Verifies the quality and robustness of the assessment criteria.\n\nThrough AgentEval, the authors aim to provide a comprehensive, scalable, and cost-effective methodology to evaluate the task utility of diverse applications, thereby bridging the evaluation gap left by existing approaches.",
        "methodology": "### Methodology: AgentEval for Assessing Task Utility in LLM-Powered Applications\n\n**Overview:**\nThe methodology describes a model named AgentEval, aimed at evaluating tasks powered by Large Language Models (LLMs) by utilizing specific agents designed for various assessing functions.\n\n**Taxonomy of Target Tasks:**\nTasks are categorized based on the clarity of success metrics:\n1. **Assistive Tasks (Success Not Clearly Defined):**\n   - Users seek suggestions rather than expecting the system to solve the task. \n   - Example: Generating an email template that users might edit later.\n   - Evaluation is challenging due to the subjective nature of user feedback.\n\n2. **Defined Success Tasks (Success Clearly Defined):**\n   - The success is measurable.\n   - Further divided into:\n     - **Single Optimal Solution**: Tasks where only one correct outcome is possible (e.g., turning on a light).\n     - **Multiple Successful Solutions**: Tasks with multiple possible successful outcomes (e.g., suggesting various food recipes).\n\n**Focus of AgentEval:**\nAgentEval is designed for tasks where success is clearly defined but can have multiple acceptable solutions.\n\n**Assessment Through Human Preferences:**\n- Inspired by research on assistive agents, which found human pairwise preferences to be an effective assessment method.\n- Annotators compare two agents and list the criteria for their preference, thus helping to infer task utility.\n\n**Components of AgentEval:**\n1. **CriticAgent:**\n   - Purpose: To suggest criteria for task utility assessment.\n   - Method: Given a task description and pairs of solutions (some preferred), CriticAgent generates criteria with accepted values for evaluation (e.g., clarity, efficiency for math problems).\n   \n2. **QuantifierAgent:**\n   - Purpose: To quantify the suggested criteria for assessing task utility.\n   - Method: Defines Utility for a task based on quantifiable criteria (e.g., Accuracy for math problems: \"Incorrect\", \"Partially correct\", \"Correct\").\n\n3. **VerifierAgent:**\n   - Purpose: To validate the quality of the criteria suggested by CriticAgent.\n   - Method: Ensures criteria are not redundant and are robust, stable, and informative by checking:\n     1. Repeatability over multiple samples.\n     2. Ability of QuantifierAgent to distinguish between original and adversarial samples.\n   - If criteria fail validation, VerifierAgent updates the criteria list.\n\n**Human Integration:**\n- AgentEval can incorporate domain experts who can either generate relevant criteria themselves or assist VerifierAgent in refining and verifying useful criteria.\n\n### Innovation:\n- The innovative aspect of AgentEval lies in its structured approach to defining, quantifying, and verifying criteria for task assessment using LLMs, with the flexibility to integrate expert human oversight for enhanced robustness and relevancy.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets:** The primary datasets utilized for the main experiment are:\n\n1. **Math Problem Solving Dataset:** This dataset is extensively used for evaluating the complex problem-solving capabilities of models. It provides a variety of mathematical problems that are crucial for assessing the effectiveness of the models in handling theoretical questions.\n\n2. **ALFWorld Household Task Dataset:** This dataset involves multi-turn interactions within a simulated multi-modal environment, requiring models to navigate and perform real-world tasks. Due to space constraints, details and experiments for ALFWorld are provided in the appendix.\n\n**Baselines:** The paper does not explicitly specify the baselines used for comparison in the main experiment section. However, since it is focused on assessing the capabilities of the proposed AgentEval framework, typical baselines might include previous state-of-the-art models in math problem-solving tasks.\n\n**Evaluation Metrics:** For both tasks, success is clearly defined but can be achieved through multiple solutions. The specific metrics for evaluation might include:\n\n- **Accuracy:** Correctness of the solutions provided by the model.\n- **Success Rate:** The percentage of tasks completed correctly.\n- **Efficiency:** The number of steps or interactions needed to reach a solution.\n\n**Main Experimental Results:** \nThe results for the Math problem-solving dataset are reported in the main paper. Unfortunately, detailed quantitative results are not provided in the description. However, the results likely demonstrate the effectiveness of AgentEval in handling complex mathematical problems, showcasing its problem-solving accuracy and efficiency.\n\nResults for ALFWorld tasks are detailed in the appendix, emphasizing the framework's capability in navigating and executing tasks in a multi-modal environment. These results help in validating different aspects of AgentEval's performance beyond theoretical problem-solving.\n\n**Note:** For specific success rates, accuracy values, or comparative results, one would need to refer to the detailed data provided in the full paper and its appendices."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the performance of different solutions for math problem solving using the AgentEval framework, and to understand their effectiveness and efficiency.",
            "experiment_process": "The dataset used is the MATH dataset, consisting of 12,500 high school competition mathematics problems. The study evaluates 120 level-5 problems using three solutions: AutoGen, Langchain ReAct, and a Vanilla solver leveraging GPT-4. The performance of these solutions is measured using AgentEval, which quantifies each solution against a set of criteria.",
            "result_discussion": "AgentEval does not quantify the three solutions as if they perform equally well across different criteria. AutoGen outperforms ReAct and the Vanilla GPT-4 solver in terms of accuracy, solution completeness, and efficiency. Successful and failed cases for each solution were distinguished, providing insights into criteria such as clarity and efficiency. This reveals valuable additional insights for developers, beyond mere success rates.",
            "ablation_id": "2405.02178v2.No1"
        },
        {
            "research_objective": "To evaluate the adaptability of multi-agent systems in completing household tasks within simulated environments using the AgentEval framework.",
            "experiment_process": "The ALFWorld dataset, which includes language-based interactive decision-making tasks, is used for this study. Solutions assessed are ReAct, AutoGen with two agents, and AutoGen with three agents. Performance is compared using the AgentEval framework.",
            "result_discussion": "The study highlights the effectiveness and robustness of AgentEval in distinguishing the performance of different solutions. The detailed process and the various agent configurations provide comprehensive data for evaluating the utility of multi-agent systems in ALFWorld tasks.",
            "ablation_id": "2405.02178v2.No2"
        },
        {
            "research_objective": "To study the diversity of criteria generated by the CriticAgent in AgentEval for assessing different tasks and solutions.",
            "experiment_process": "Two methods were used: (1) task-based criteria solely from task descriptions, and (2) solution-based criteria from both task and execution examples. Different configurations and combinations were generated and assessed for uniqueness over 50 iterations with various seeds. A pre-trained language model fine-tuned for paraphrasing was used to measure the semantic similarity of criteria descriptions.",
            "result_discussion": "The solution-based approach has the potential to produce more diverse criteria than the task-based approach, although this depends on the model's creativity. The analysis showed that more iterations lead to a convergence of criteria, reducing cost. AutoGen demonstrated the highest diversity, while task-based approaches were more unique compared to ReAct and Vanilla Solver. The study suggests CriticAgent will eventually converge to a finite list of criteria.",
            "ablation_id": "2405.02178v2.No3"
        },
        {
            "research_objective": "To verify the effectiveness and robustness of the proposed criteria for evaluating the utility of LLM-powered applications using VerifierAgent in AgentEval.",
            "experiment_process": "VerifierAgent ensures criteria stability by iterating over the generation and quantification phases, consolidating criteria, and eliminating redundancies. It assesses the discriminative power using adversarial examples to test the system's ability to differentiate these from standard cases. Thresholds for robustness and discriminative power are tunable based on application needs.",
            "result_discussion": "The methodology ensures the criteria are not redundant and can be quantified stably. It confirms that effective criteria can withstand noise and accurately assess utility under varied conditions. Algorithm 1 in the Appendix summarizes the proposed methodology for VerifierAgent.",
            "ablation_id": "2405.02178v2.No4"
        }
    ]
}