{
    "title": "Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data",
    "abstract": "Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.\n\n\n\nKeywords:\u2009LoRA, PEFT, LLMs, Few-Shot Learning, Text Classification",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Recent years have been characterized by a paradigm shift in text classification. Large Language Models (LLMs) offer valid alternatives to the traditional approach of fine-tuning pre-trained models (e.g., BERT Devlin et al. (2019  ###reference_b6###), RoBERTa Liu et al. (2019  ###reference_b15###)) on annotated datasets. In-Context Learning (ICL) is a first option, where a LLM learns how to solve a task by simply observing a few examples provided in its prompt (i.e., without any fine-tuning stage) Brown et al. (2020  ###reference_b1###). Another alternative is the 0-shot setting, where the LLM directly solves a task by simply following the provided instructions (i.e., without any example). Instruction-fine tuned models like Flan-T5 Chung et al. (2022  ###reference_b4###), Instruct-GPT Ouyang et al. (2022  ###reference_b17###), or ChatGPT excel in this setting. The advantage of these two alternatives with respect to the traditional approach is that they can be used to bootstrap a model when data is scarce or totally absent.\nThe 0-shot setting is generally more appealing since it does not require any data, however, the few-shot ICL setting typically leads to better results by only leveraging a small number of samples (e.g., less than 10 examples). Obtaining a few annotated data might not be a significant drawback, as a practitioner with moderate domain knowledge can readily create a small number of examples manually.\nHowever, a major disadvantage is the higher computational cost, latency, and memory requirements associated with the longer prompt, which needs to contain illustrative examples.\nA possible solution to leverage the few available examples without incurring the ICL inference costs would be to use them for fine-tuning the LLM, which is possible by using some Parameter-Efficient Fine Tuning (PEFT) techniques Liu et al. (2022c  ###reference_b14###); Lester et al. (2021  ###reference_b9###); Hu et al. (2021  ###reference_b8###); Liu et al. (2021  ###reference_b13###). Unfortunately, as we will demonstrate in the experimental section, PEFT is not effective with very few examples, due to under-fitting or over-fitting phenomena.\nIn this paper, we propose a solution to the low-resource PEFT for text classification with LLMs by defining a framework that enables faster, cheaper and more accurate inference than ICL in such a scenario. We hypothesize that LLMs already have some knowledge of how to solve a classification task, but the sub-optimal usage of the available resources (i.e., the few-shot examples) results in low PEFT performance under the low-resource setting. On the contrary, LLMs typically excel in generation tasks, hence we frame an auxiliary data augmentation task that we use to unlock the LLM classification capabilities. Our method consists of three steps. First, we use the LLM to generate synthetic examples for each class of the text classification task we target. Then, we use the same LLM in the ICL setting to classify the examples and clean the data by removing label-inconsistent generated examples. Finally, we fine-tune the LLM with PEFT using the generated and cleaned data. Our experiments show that the resulting classifier reaches accuracy levels comparable to or better than the ICL setting in three different text classification tasks while being a lot more efficient (2x to 5x speed boost).\nIn these generate-filter-train stages we always use the same LLM to demonstrate that what leads to a good accuracy is just a better usage of the few available examples and not the employment of any other resource (e.g., another LLM) which might bring additional knowledge to solve the task.\nThe rest of the paper is organized as it follows: in Section 2  ###reference_### we discuss the related works. In Section 3  ###reference_### we present our method, while in Sections 4  ###reference_### and 5  ###reference_### we discuss the experimental setting and the results, respectively. Finally, Section 6  ###reference_### derives the conclusions.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.    Related Work",
            "text": "LLMs demonstrate impressive capabilities to solve Natural Language Understanding tasks. For instance, classification tasks can be approached in a generative way, i.e., by asking the LLM to generate the class name associated with an input example. 0-shot and ICL are two variants of this paradigm.\nRecent models (e.g., GPT-3, Brown et al. (2020  ###reference_b1###), Flan-T5 Chung et al. (2022  ###reference_b4###), ChatGPT Ouyang et al. (2022  ###reference_b17###), Falcon Penedo et al. (2023  ###reference_b19###) or Vicuna Zheng et al. (2023  ###reference_b26###)) reach impressive results in both settings, and researchers started considering using LLMs as annotators Rosenbaum et al. (2022  ###reference_b21###); Zhu et al. (2023  ###reference_b27###); He et al. (2023  ###reference_b7###). For instance, Rosenbaum et al. (2022  ###reference_b21###) propose a method that uses LLMs to generate and annotate data for Intent Classification and Slot Filling. He et al. (2023  ###reference_b7###) propose a two-step approach where they first use ChatGPT to generate a few-shot Chain-of-Thought prompt, which they then use to annotate unlabeled data. Results are competitive with human annotators, but their classification procedure is relatively slow since the LLM is invoked twice with prompts that need to contain both examples and explanations. Conversely, we propose a solution whose computational complexity at inference time corresponds to the 0-shot setting case.\nEven if these results are impressive, they still might not reach the state-of-the-art performance achievable when LLMs are fully fine-tuned on large data.\nFine-tuning LLMs is extremely expensive, but a viable solution is offered by Parameter Efficient Fine-Tuning (PEFT) techniques Liu et al. (2022c  ###reference_b14###); Lester et al. (2021  ###reference_b9###); Hu et al. (2021  ###reference_b8###); Liu et al. (2021  ###reference_b13###), where a pre-trained model is fine-tuned by only updating a small number (e.g., 0.01%) of added or selected parameters. These methods report results that match the performance of full fine-tuning when large training datasets are available. On the contrary, there has been relatively little focus on (parameter-efficient) fine-tuning in low-resource settings. Our paper targets this scenario, as we assume we can access only a few annotated examples (e.g., four per class) and no unlabeled data. A work operating in a similar setting is Liu et al. (2022b  ###reference_b12###), where the authors propose a novel PEFT technique that is demonstrated to work well in low resource settings when the PEFT weights are pre-trained and multiple tasks are trained in parallel. We differ from their work as we do not pre-train the PEFT weights and we target a single task at a time, without assuming (possibly related) data from other tasks is available. Relaxing this assumption is especially useful when dealing with very peculiar tasks not sharing similarities with other available datasets. Hence, to the best of our knowledge, we are the first to improve few-shot PEFT without additional resources (external datasets or models)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methodology",
            "text": "We explore a low-resource setting where we have few training examples per class and no unlabeled data. ICL methods could achieve reasonable performance with few-shot samples but inference cost is high due to long prompts. PEFT methods like LoRA are known to be more efficient than ICL at inference. However, we find that LoRA performs worse than ICL in data-scarce settings (see Tab. 1  ###reference_###). In this paper, we aim to explore the potential of combining the strengths of PEFT and ICL methods for achieving efficient and effective text classification.\nHence we propose to augment the training data with synthetic data to better align the generation and classification capability of LLMs and to ensure that PEFT is performed on a decent amount of data. Our method has 3 steps: generate data, filter data, and train. An overview of our method is shown in Figure 1  ###reference_###.\nFew examples of movie reviews having positive sentiment are given. Generate more positive reviews\nText: [Positive review 1]\nLabel: Positive\n\u2026\nText: [Positive review 4]\nLabel: Positive\nText: [the model generates this]\nClassify the sentiment of the given movie review into Positive or Negative\nText: [review 1]\nLabel: [Label 1]\n\u2026\nText: [review 8]\nLabel: [Label 8]\nText: [generated review]\nLabel: [Predicted Label]\nGeneration step: Chavan et al. (2023  ###reference_b3###) show that in a few-shot setting, the performance of PEFT significantly improves as the number of training samples per class increases. We also observe similar results in our initial experiments (presented in section 5  ###reference_###. Further, since ICL performs well, we hypothesize that the model has the inherent knowledge to solve the classification task and that the low PEFT results are due to sub-optimal usage of the available resources (the few shot examples). To fill this gap, we first use the LLM  in the ICL setting to generate synthetic data which we can use to augment the few shot examples at our disposal. We generate examples for each class in the targeted classification task. An example of the prompt we used in this step is shown in Figure 2  ###reference_###.\nFiltering step: We first apply a basic filtering step to discard duplicates and malformed generations (i.e., too short or too long texts). On manual inspection of the generated data, we found some label-inconsistent generations (i.e., data that are not valid examples of the class they should represent). We hypothesize this is due to hallucination. To identify and remove these cases, we classify the generated data using ICL with . The prompt used for this stage is similar to the one shown in Figure 3  ###reference_###. If the predicted label does not match the intended label from the generation step, we discard the generated example. We repeat these generation-filtering steps until we produce N new data samples for each class in the targeted classification task.\nTraining step: Finally, we use the filtered data along with the few (4 per class) real examples for the PEFT of the LLM  with LoRA. Note that  is used for all 3 steps, as we want to validate our hypothesis that  does not need additional knowledge to work in the PEFT setting, but only a more stable training process which can be guaranteed by the self-generated synthetic examples.\nInference Conversely to the ICL setting, the LLM does not use any example at inference time. Note that the three steps (generate, filter, train) are used only at training time, i.e., there is no impact on the inference latency.\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Experiments",
            "text": "We use the Vicuna LLM Zheng et al. (2023  ###reference_b26###), which is based on LLaMA Touvron et al. (2023  ###reference_b22###). We selected Vicuna as it is the best-performing model among those whose weights were publicly available at the time of our experiments, according to the Chatbot Arena Leaderboard111https://lmsys.org/blog/2023-05-25-leaderboard/  ###reference_ard/###. We experiment with 2 model sizes - Vicuna-7b and Vicuna-13b which have 7 billion and 13 billion parameters, respectively. We show results on the official test sets of 3 datasets - SST2 (sentiment analysis, 2 classes) Pang and Lee (2005  ###reference_b18###), AG News (news topic classification, 4 classes) Zhang et al. (2016  ###reference_b25###), and TREC (question classification, 6 classes) Li and Roth (2002  ###reference_b10###).\nTo generate synthetic data, we use random sample decoding with temperature = 1.0, top_k = 50 and num_beams = 1 or 2.\nFor a fair comparison across models and methods, throughout our experiments, we do not tune the LoRA hyper-parameters (we set rank=8, alpha=32, and dropout=0.1). Similarly, we make minimal changes to the prompt for all ICL experiments and do not perform any prompt engineering.\nThe code is implemented using hugging face and PEFT library Mangrulkar et al. (2022  ###reference_b16###) with torch backend. All models are trained on 4 v100 GPUs of 16GB each. All LoRA models are trained for 100 epochs. The language modeling loss (next token prediction) is optimized using the Adam optimizer. Batch size is set to 2 for Vicuna-13b and 8 for Vicuna-7b. We run each experiment multiple times with different seeds and different few-shot examples. We observe negligible deviation across runs and report the average accuracy. We also report the results of several baselines, including 0-shot, ICL, and vanilla LoRA trained with different numbers of real examples. We also report LoRA trained with the full training set. This could be considered a potential upper-bound reachable in high resource settings and gives results comparable to the respective SoTA methods on the 3 datasets Raffel et al. (2023  ###reference_b20###); Cer et al. (2018  ###reference_b2###); Yang et al. (2019  ###reference_b23###)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion and Future work",
            "text": "In this paper, we introduced a framework to make LLMs more efficient and effective text classifiers in very low-resource settings. The procedure we proposed consists of three steps. In the first step, the LLM is used to augment a very small training set with synthetic data; then, we adopt the LLM to classify the generated data and remove label-inconsistent examples; finally, we use the resulting data to fine-tune the LLM using LoRA. By running experiments on three different classification datasets we demonstrated how training LoRA using the self-generated synthetic data allowed our model to be comparable to or surpass several baselines operating in low resource settings, including 0-shot, ICL, and vanilla LoRA. In future work, we plan to improve the quality of the generated examples by promoting data diversity. Some strategies to improve data diversity include increasing attribute diversity Yu et al. (2023  ###reference_b24###), logit suppression Chung et al. (2023  ###reference_b5###) etc."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Limitations",
            "text": "Our method might not work on tasks that are particularly challenging and hard to catch with only a few examples. In this case, ICL is expected to fail, and similarly, our first two steps are expected to produce low-quality examples making the entire procedure ineffective. Another limitation is that our approach is fully based on LLMs and cannot be applied to low-resource languages where there is no existing LLM working well."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Ethics",
            "text": "Generating data using LLMs for text classification exposes the resulting classifier to the biases acquired during the LLM pre-training. In our framework, this phenomenon is potentially even amplified, as using the same LLM to generate and filter the data might reinforce such biases. Unfortunately, there is no one-size-fits-all solution for this problem. The biases are dependent on the application domain and on the data distribution to be generated. However, we encourage the readers to be very cautious about using this framework and to take the appropriate actions - for example, compiling a list of the potential biases specific for the target application domain and checking for those in the generated data - to mitigate the potential biases that may get reinforced when using a methodology similar to the one here presented."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.02422v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2404.02422v1",
        "paper_title": "Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data",
        "research_background": "### Motivation:\nThe paper is driven by the limitations of current low-resource text classification methods using Large Language Models (LLMs). Traditional approaches like fine-tuning pre-trained models (BERT, RoBERTa) have been overshadowed by novel strategies such as In-Context Learning (ICL) and zero-shot settings, which are effective but come with notable drawbacks. Specifically, while zero-shot settings do not require any data, they fall short in performance compared to few-shot ICL settings. However, ICL involves substantial computational cost, latency, and memory consumption due to the need for lengthy prompts. Parameter-Efficient Fine Tuning (PEFT) techniques could mitigate these drawbacks, yet they struggle with very few examples because of under-fitting or over-fitting.\n\n### Research Problem:\nThis paper aims to enhance the performance of low-resource text classification using LLMs by overcoming the inefficiencies of PEFT with very few examples. The goal is to create a framework that leverages the few available examples more effectively, thereby achieving faster, cheaper, and more accurate inference than existing ICL methods under low-resource conditions.\n\n### Proposed Solution:\nThe proposed solution involves:\n1. Generating synthetic examples for each class of the text classification task using the LLM.\n2. Classifying and cleaning the generated examples using the same LLM in the ICL setting to remove label-inconsistent examples.\n3. Fine-tuning the LLM with PEFT using the synthesized and cleaned data.\n\n### Relevant Prior Work:\n1. **Text Classification using Pre-trained Models:** The traditional approach to text classification involves fine-tuning pre-trained models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) on annotated datasets.\n  \n2. **In-Context Learning (ICL):** ICL allows LLMs to solve tasks by observing only a few examples within the prompt, avoiding the need for any fine-tuning (Brown et al., 2020).\n\n3. **Zero-Shot Learning:** Instruction-fine-tuned models like Flan-T5 (Chung et al., 2022), Instruct-GPT (Ouyang et al., 2022), and ChatGPT operate effectively in zero-shot settings, where tasks are solved according to provided instructions without examples.\n\n4. **Parameter-Efficient Fine Tuning (PEFT):** PEFT techniques aim to fine-tune LLMs with minimal parameter updates, but they face challenges in low-resource settings with very few examples (Lester et al., 2021; Hu et al., 2021; Liu et al., 2021; Liu et al., 2022c).\n\nThe paper seeks to improve upon these methods by introducing an innovative generate-filter-train approach that maximizes the utility of few examples for fine-tuning LLMs, thereby boosting efficiency and accuracy in text classification tasks.",
        "methodology": "The proposed method in this paper aims to improve the performance of low-resource large language models (LLMs) in text classification tasks by combining the strengths of parameter-efficient fine-tuning (PEFT) methods and in-context learning (ICL). The method focuses on settings where there are few training samples per class and no unlabeled data. The key innovation lies in augmenting the few-shot training data with synthetic data to enhance the performance of PEFT methods like LoRA.\n\n### Key Components and Steps:\n\n1. **Data Generation**:\n   - **Objective**: To generate a larger dataset to improve the alignment between the generation and classification capabilities of LLMs.\n   - **Approach**: Use an LLM in ICL mode to generate synthetic examples for each classification class, leveraging the model's inherent knowledge in the few-shot setting.\n   - **Example**: A few examples of positive movie reviews are given, and the model generates additional positive reviews.\n\n2. **Data Filtering**:\n   - **Objective**: To ensure the quality and consistency of generated data.\n   - **Steps**:\n     - Remove duplicates and malformed texts.\n     - Manually inspect and classify generated texts to remove label-inconsistent examples caused by model hallucinations.\n   - **Filtering Process**: Use ICL to classify the generated data with prompts similar to those in the generation stage. Discard examples where the predicted label does not match the intended label.\n\n3. **Training with PEFT**:\n   - **Objective**: To utilize the filtered synthetic data to fine-tune the LLM in a more efficient and effective manner.\n   - **Approach**: Combine the filtered synthetic data with the few real examples for PEFT using LoRA.\n   - **Specifics**: Use the LLM consistently across data generation, filtering, and training steps to validate that the model does not require additional knowledge, just a stable training process.\n\n4. **Inference**:\n   - **Objective**: To ensure efficient inference without the need for example-based prompts.\n   - **Approach**: Unlike the ICL setting, no examples are used during inference, reducing latency.\n\n### Innovations:\n- **Combining PEFT and ICL**: The method leverages the efficient inference capabilities of PEFT and the strong few-shot performance of ICL, aiming to address the shortcomings in data-scarce settings.\n- **Synthetic Data Augmentation**: Enhances the training dataset by generating and filtering synthetic data, ensuring that PEFT methods can perform better even with limited real examples.\n- **Integrated Process**: The proposed method uses a consistent model across all stages (generation, filtering, and training), emphasizing that better utilization of existing model knowledge through improved training data suffices for enhanced performance.\n\n**Conclusion**:\nThe methodology addresses the challenge of low-resource settings by augmenting the training data with generated examples, ensuring quality through filtering, and making the PEFT process more robust and effective. This approach balances efficiency and effectiveness, making it suitable for practical applications where data is scarce and inference costs need to be minimized.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Model Selection:** \n- Vicuna LLM, based on LLaMA, with two model sizes: Vicuna-7b (7 billion parameters) and Vicuna-13b (13 billion parameters).\n- Chosen due to its superior performance among publicly available models, as per the Chatbot Arena Leaderboard.\n\n**Datasets:**\n- SST2 (sentiment analysis, 2 classes)\n- AG News (news topic classification, 4 classes)\n- TREC (question classification, 6 classes)\n\n**Synthetic Data Generation:**\n- Used random sample decoding with parameters: temperature = 1.0, top_k = 50, and num_beams = 1 or 2.\n\n**Experimental Conditions:**\n- The LoRA hyper-parameters: rank=8, alpha=32, and dropout=0.1 (kept constant for fair comparison).\n- Minimal prompt changes for all ICL experiments, without extensive prompt engineering.\n- Implemented using the Hugging Face and PEFT library with a torch backend.\n- Models trained on four V100 GPUs, each with 16GB of memory.\n- LoRA models trained for 100 epochs, optimizing language modeling loss using the Adam optimizer.\n- Batch size: 2 for Vicuna-13b and 8 for Vicuna-7b.\n- Experiments run multiple times with different seeds and few-shot examples, reporting average accuracy due to negligible deviation across runs.\n\n**Baselines:**\n- 0-shot learning.\n- In-context learning (ICL).\n- Vanilla LoRA trained with different numbers of real examples.\n- LoRA trained with the full training set (considered an upper-bound achievable in high-resource settings).\n\n**Evaluation Metrics:**\n- Average accuracy across multiple runs.\n\n**Main Experimental Results:**\n- The paper reports average accuracy and comparison against several baselines including zero-shot, in-context learning, vanilla LoRA with real examples, and fully trained LoRA as a high-resource upper bound.\n- The results on the official test sets indicate that using PEFT and synthetic data enhances the low-resource LLMs in classification tasks across SST2, AG News, and TREC datasets."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the effectiveness of the filtering step in the generate-filter-train approach using LoRA on synthetic data for text classification tasks.",
            "experiment_process": "The study examined the performance of LoRA trained on unfiltered versus filtered synthetic data on the TREC dataset. The process involved generating synthetic data, filtering it to remove hallucinations or incorrect samples, and then training LoRA models on these datasets. Accuracy was measured on the TREC dataset before and after filtering, and a manual inspection of 125 samples from each dataset was conducted to identify incorrectly labeled examples. For comparison, the SST2 dataset was also evaluated where the filtering step\u2019s performance impact was negligible.",
            "result_discussion": "Training LoRA on unfiltered synthetic data yielded an accuracy of 0.68 on TREC, whereas filtering the data increased accuracy to 0.79, highlighting the importance of the filtering step. Manual inspection revealed 33 incorrect samples out of 125 in the unfiltered data, compared to only 5 in the filtered data. For the SST2 dataset, no incorrect samples were identified, hence filtering had no impact on performance for this dataset.",
            "ablation_id": "2404.02422v1.No1"
        },
        {
            "research_objective": "Assess the impact of data size on the performance of Vicuna-7b model when trained on real versus synthetic data for text classification tasks.",
            "experiment_process": "Various data sizes were used to train the Vicuna-7b model on SST2 and TREC datasets. The study analyzed the effect of increasing real data size versus adding more synthetic data on model performance. Performance metrics were compared across different dataset sizes, and the diversity of the dataset was evaluated by analyzing the number of unique trigram occurrences in the real and synthetic data samples.",
            "result_discussion": "Increasing the size of real data consistently improved performance, whereas adding more synthetic data did not always provide clear benefits due to lower diversity. Real data diversity increased faster with data size compared to synthetic data, demonstrating the challenge in generating diversified synthetic data from a limited number of seed examples.",
            "ablation_id": "2404.02422v1.No2"
        }
    ]
}