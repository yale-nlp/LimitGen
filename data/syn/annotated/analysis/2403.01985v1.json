{
    "title": "Transformers for Low-Resource Languages: Is F\u00e9idir Linn!",
    "abstract": "The Transformer model is the state-of-the-art in Machine Translation. However, in general, neural translation models often under perform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on low-resource language pairs. In this study, hyperparameter optimization of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an in-domain 88k public admin corpus were used for evaluation. A Transformer optimized model demonstrated a BLEU score improvement of 7.8 points when compared with a baseline RNN model. Improvements were observed across a range of metrics, including TER, indicating a substantially reduced post editing effort for Transformer optimized models with 16k BPE subword models. Bench-marked against Google Translate, our translation engines demonstrated significant improvements. The question of whether or not Transformers can be used effectively in a low-resource setting of English-Irish translation has been addressed. Is f\u00e9idir linn - yes we can.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of Neural Machine Translation (NMT) has heralded an era of high-quality translations. However, these improvements have not been manifested in the translation of all languages. Large datasets are a prerequisite for high quality NMT. This works well in the context of well-resourced languages where there is an abundance of data. In the context of low-resource languages which suffer from a sparsity of data, alternative approaches must be adopted.\nAn important part of this research involves developing applications and models to address the challenges of low-resource language technology. Such technology incorporates methods to address the data scarcity affecting deep learning for digital engagement of low-resource languages.\nIt has been shown that an out-of-the-box NMT system, trained on English-Irish data, achieves a lower translation quality compared with using a tailored SMT system (Dowling et al, 2018). It is in this context that further research is required in the development of NMT for low-resource languages and the Irish language in particular.\nMost research on choosing subword models has focused on high resource languages (Ding et al.,, 2019  ###reference_b10###; Gowda and May,, 2020  ###reference_b14###). In the context of developing models for English to Irish translation, there are no clear recommendations on the choice of subword model types. One of the objectives in this study is to identify which type of subword model performs best in this low resource scenario."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Native speakers of low-resource languages are often excluded from useful content since, more often than not, online content is not available to them in their language of choice. Such a digital divide and the resulting social exclusion experienced by second language speakers, such as refugees living in developed countries, has been well documented in the research literature  (MacFarlane et al.,, 2008  ###reference_b20###; Alam and Imran,, 2015  ###reference_b1###).\nResearch on Machine Translation (MT) in low-resource scenarios directly addresses this challenge of exclusion via pivot languages  (Liu et al.,, 2018  ###reference_b19###), and indirectly, via domain adaptation of models  (Ghifary et al.,, 2016  ###reference_b13###). Breakthrough performance improvements in the area of MT have been achieved through research efforts focusing on NMT (Bahdanau et al.,, 2014  ###reference_b3###; Cho et al.,, 2014  ###reference_b9###). Consequently, state-of-the-art (SOA) performance has been attained on multiple language pairs (Bojar et al.,, 2017  ###reference_b7###, 2018  ###reference_b8###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Irish Language",
            "text": "The Irish language is a primary example of such a low-resource language that will benefit from this research. NMT involving Transformer model development will improve the performance in specific domains of low-resource languages. Such research will address the end of the Irish language derogation in the European Commission in 2021 111amtaweb.org/wp-content/uploads/2020/11/MT-in-EU-Overview-with-Voiceover-Andy-Way-KEYNOTE-K1.pdf (Way,, 2020  ###reference_b32###) helping to deliver parity in support for Irish in online digital engagement."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Hyperparameter Optimization",
            "text": "Hyperparameters are employed in order to customize machine learning models such as translation models. It has been shown that machine learning performance may be improved through hyperparameter optimization (HPO) rather than just using default settings (Sanders and Giraud-Carrier,, 2017  ###reference_b24###).\nThe principle methods of HPO are Grid Search (Montgomery,, 2017  ###reference_b21###) and Random Search (Bergstra and Bengio,, 2012  ###reference_b5###)]. Grid search is an exhaustive technique which evaluates all parameter permutations. However, as the number of features grows, the amount of data permutations grows exponentially making optimization expensive in the context of developing long running translation models.\nAn effective, and less computationally intensive, alternative is to use random search which samples random configurations."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1 Recurrent Neural Networks",
            "text": "Recurrent neural networks are often used for the tasks of natural language processing, speech recognition and MT. RNN models enable previous outputs to be used as inputs while having hidden states. In the context of MT, such neural networks were ideal due to their ability to process inputs of any length. Furthermore, the model sizes do not necessarily increase with the size of its input. Commonly used variants of RNN include Bidirectional (BRNN) and Deep (DRNN) architectures. However, the problem of vanishing gradients coupled with the development of attention-based algorithms often leads to Transformer models performing better than RNNs."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2 Transformer",
            "text": "The greatest improvements have been demonstrated when either the RNN or the CNN architecture is abandoned completely and replaced with an attention mechanism creating a much simpler and faster architecture known as Transformer  (Vaswani et al.,, 2017  ###reference_b31###).\nTransformer models use attention to focus on previously generated tokens. The approach allows models to develop a long memory which is particularly useful in the domain of language translation. Performance improvements to both RNN and CNN approaches may be achieved through the introduction of such attention layers in the translation architecture.\nExperiments in MT tasks show such models are better in quality due to greater parallelization while requiring significantly less time to train."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Subword Models",
            "text": "Translation, by its nature, requires an open vocabulary and the use of subword models aims to address the fixed vocabulary problem associated with NMT. Rare and unknown words are encoded as sequences of subword units. By adapting the original Byte Pair Encoding (BPE) algorithm (Gage,, 1994  ###reference_b12###), the use of BPE submodels can improve translation performance (Sennrich et al.,, 2015  ###reference_b26###; Kudo,, 2018  ###reference_b16###).\nDesigned for NMT, SentencePiece, is a language-independent subword tokenizer that provides an open-source C++ and a Python implementation for subword units. An attractive feature of the tokenizer is that SentencePiece trains subword models directly from raw sentences (Kudo and Richardson,, 2018  ###reference_b17###)."
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Byte Pair Encoding compared with Unigram",
            "text": "BPE and unigram language models are similar in that both encode text using fewer bits but each uses a different data compression principle (dictionary vs. entropy). In principle, we would expect the same benefits with the unigram language model as with BPE. However, unigram models are often more flexible since they are probabilistic models that output multiple segmentations with their probabilities.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Proposed Approach",
            "text": "HPO of RNN models in low-resource settings has previously demonstrated considerable performance improvements. The extent to which such optimization techniques may be applied to Transformer models in similar low-resource scenarios is evaluated as part of this study. Evaluations included modifying the number of attention heads, the number of layers and experimenting with regularization techniques such as dropout and label smoothing. Most importantly, the choice of subword model type and the vocabulary size are evaluated.\nIn order to test the effectiveness of our approaches, optimization was carried out on two English-Irish parallel datasets: a general corpus of 52k lines from the Directorate General for Translation (DGT) and an in-domain corpus of 88k lines of Public Administration (PA) data. With DGT, the test set used 1.3k lines and the development set comprised of 2.6k lines. In the case of the PA dataset, there were 1.5k lines of test data and 3k lines of validation. All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece subword model. The impact of using separate source and target subword models was not explored.\nThe approach adopted is illustrated in Figure 1  ###reference_###. Two baseline architectures, RNN and Transformer, are evaluated. On evaluating the hyperparameter choices for Transformer models, the values outlined in Table 1 were tested using a random search approach. A range of values for each parameter was tested using short cycles of 5k training steps. Once an optimal value, within the sampled range was identified, it was locked in for tests on subsequent parameters."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Architecture Tuning",
            "text": "Given the long training times associated with NMT, it is difficult and costly to tune systems using a conventional Grid Search approach. Therefore a Random Search approach was adopted in the HPO of our transformer models.\nWith low-resource datasets, the use of smaller and fewer layers has previously been shown to improve performance  (Araabi and Monz,, 2020  ###reference_b2###). Performance of low-resource NMT has also been demonstrated to improve in cases where shallow Transformer models are adopted  (Van Biljon et al.,, 2020  ###reference_b30###). Guided by these findings, configurations were tested which varied the number of neurons in each layer and modified the number of layers used in the Transformer architecture.\nThe impact of regularization, by applying varying degrees of dropout to Transformer models, was evaluated. Configurations using smaller (0.1) and larger values (0.3) were applied to the output of each feed forward layer."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Subword Models",
            "text": "It has become standard practise to incorporate word segmentation approaches, such as Byte-Pair-Encoding (BPE), when developing NMT models. Previous work shows that subword models may be particularly beneficial for low-resource languages since rare words are often a problem. Reducing the number of BPE merge operations resulted in substantial improvements of 5 BLEU points (Sennrich and Zhang 2019) when tested on RNN models.\nIn the context of English to Irish translation, there is no clear agreement as to what constituted the best approach. Consequently, as part of this study, subword regularization techniques, involving BPE and unigram models were evaluated to determining the optimal parameters for maximising translation performance. BPE models with varying vocabulary sizes of 4k, 8k, 16k and 32k were tested."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Empirical Evaluation",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Datasets",
            "text": "The performance of the Transformer and RNN approaches is evaluated on English to Irish parallel datasets. Two datasets were used in the evaluation of our models namely the publicly available DGT dataset which may be broadly categorised as generic and an in-domain dataset which focuses on public administration data.\nThe DGT, and its Joint Research Centre, has made available all Translation Memory (TM; i.e. sentences and their professionally produced translations) which cover all official European Union languages (Steinberger et al.,, 2013  ###reference_b29###).\nData provided by the Department of Tourism, Culture, Arts, Gaeltacht, Sport and Media in Ireland formed the majority of the data in the public administration dataset. This includes staff notices, annual reports, website content, press releases and official correspondence.\nParallel texts from the Digital Corpus of the European Parliament (DCEP) and the DGT are included in the training data. Crawled data, from sites of a similar domain are included. Furthermore a parallel corpus collected from Conradh na Gaeilge (CnaG), an Irish language organisation that promotes the Irish language, was included. The dataset was compiled as part of a previous study which carried out a preliminary comparison of SMT and NMT models for the Irish language  (Dowling et al.,, 2018  ###reference_b11###)."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Infrastructure",
            "text": "Models were developed using a lab of machines each of which has an AMD Ryzen 7 2700X processor, 16 GB memory, a 256 SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16 GB graphic cards and up to 27GB of memory when available (Bisong,, 2019  ###reference_b6###).\nOur MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT (Klein et al.,, 2017  ###reference_b15###)."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Metrics",
            "text": "As part of this study, several automated metrics were used to determine the translation quality. All models were trained and evaluated on both the DGT and PA datasets using the BLEU (Papineni et al.,, 2002  ###reference_b22###), TER (Snover et al.,, 2006  ###reference_b27###) and ChrF (Popovi\u0107,, 2015  ###reference_b23###) evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Transformer performance compared with RNN",
            "text": "The performance of RNN models is contrasted with the Transformer approach in Figure 2  ###reference_### and Figure 3  ###reference_###. Transformer models, as anticipated, outperform all their RNN counterparts. It is interesting to note the impact of choosing the optimal vocabulary size for BPE submodels. Both datasets demonstrate that choosing a BPE vocabulary of 16k words yields the highest performance.\nFurthermore, the TER scores highlighted in Figure 3  ###reference_### reinforce the findings that using 16k BPE submodels on Transformer architectures leads to better translation performance. The TER score for the DGT Transformer 16k BPE model is significantly better (0.33) when compared with the baseline performance (0.41).\n###figure_3### ###figure_4### ###figure_5###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Environmental Impact",
            "text": "Motivated by the findings of Stochastic Parrots (Bender et al.,, 2021  ###reference_b4###), energy consumption during model development was tracked. Prototype model development used Colab Pro, which as part of Google Cloud is carbon neutral (Lacoste et al.,, 2019  ###reference_b18###). However, longer running Transformer experiments were conducted on local servers using 324 gCO2 per kWh 222https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf(SEAI,, 2020  ###reference_b25###). The net result was just under 10 kgCO2 created for a full run of model development. Models developed during this study, will be reused for ensemble experiments in future work."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Validation accuracy, and model perplexity, in developing the baseline and optimal models for the DGT corpus are illustrated in Figure 4  ###reference_### and Figure 5  ###reference_###. Rapid convergence was observed while training the baseline model such that little accuracy improvement occurs after 20k steps. Including a subword model led to much slower convergence and there were only marginal gains after 60k steps. Furthermore, it is observed that training the DGT model, with a 16k BPE submodel, boosted validation accuracy by over 8% compared with its baseline.\nWith regard to the key metric of perplexity, it is shown to rise after training for 15k steps in the baseline models. PPL was observed to rise at later stages, typically after 40k steps in models developed using subword models. Perplexity (PPL), shows how many different, equally probable words can be produced during translation. As a metric for translation performance, it is important to keep low scores so the number of alternative translations is reduced. Therefore, for future model development it may be worthwhile to set PPL as an early stopping parameter.\nOn examining the PPL graphs of Figure 4  ###reference_### and Figure 5  ###reference_###, it is clear that a lower global minimum is achieved when the Transformer approach is used with a 16k BPE submodel. The PPL global minimum (2.7) is over 50% lower than the corresponding PPL for the Transformer base model (5.5). Such a finding illustrates that choosing an optimal submodel delivers significant performance gains.\nTranslation engine performance was bench-marked against Google Translate\u2019s 333https://translate.google.com/ English to Irish translation service which is freely available on the internet. Four random samples were selected from the English source test file and are presented in Table 6  ###reference_###. Translation of these samples was carried out on the optimal DGT Transformer model and using Google Translate. Case insensitive, sentence level BLEU scores were recorded and are presented in Table 7  ###reference_###. The results are encouraging and indicate well-performing translation models on the DGT dataset.\nThe optimal parameters selected in this discovery process are identified in bold in Table 2. A higher initial learning rate of 2 coupled with an average decay of 0.0001 led to longer training times but more accurate models. Despite setting an early stopping parameter, many of the Transformer builds continued for the full cycle of 200k steps over periods of 20+ hours.\nTraining transformer models with a reduced number of attention heads led to a marginal improvement in translation accuracy with a smaller corpus. Our best performing model on a 55k DGT corpus, with 2 heads and a 16k BPE submodel, achieved a BLEU score of 60.5 and a TER score of 0.33. By comparison, using 8 heads with the same architecture and dataset yielded 60.3 for the BLEU and 0.34 for the TER. In the case of a larger 88k PA corpus, all transformer models using 8 heads performed better than equivalent models using just 2 heads.\nStandard Transformer parameters for batch size (2048) and the number of encoder / decoder layers (6) were all observed to perform well on the DGT and PA corpora. Reducing hidden neurons to 256 and increasing regularization dropout to 0.3 improved translation performance and were chosen when building all Transformer models."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In our paper, we demonstrated that a random search approach to hyperparameter optimization leads to the development of high-performing translation models.\nWe have shown that choosing subword models, in our low-resource scenarios, is an important driver for the performance of MT engines. Moreover, the choice of vocabulary size leads to varying degrees of performance. Within the context of low-resource English to Irish translation, we achieved optimal performance, on a 55k generic corpus and an 88k in-domain corpus, when a Transformer architecture with a 16k BPE submodel was used.\nThe importance of selecting hyperparameters in training low-resource Transformer models was also demonstrated. By reducing the number of hidden layer neurons and increasing dropout, our models performed significantly better than baseline models and Google Translate.\nPerformance improvement of our optimized Transformer models, with subword segmentation, was observed across all key indicators namely a higher validation accuracy, a PPL achieved at a lower global minimum, a lower post editing effort and a higher translation accuracy."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table ltx_align_center\" id=\"S2.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.1\">Hyperparameter</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.2.1\">Values</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.2.2.1\">Learning rate</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.2.2.2\">0.1, 0.01, 0.001, <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.2.2.2.1\">2</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.3.3.1\">Batch size</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.3.3.2\">1024, <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.3.3.2.1\">2048</span>, 4096, 8192</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.4.4.1\">Attention heads</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.4.4.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.4.4.2.1\">2</span>, 4, <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.4.4.2.2\">8</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.5.5.1\">Number of layers</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.5.5.2\">5, <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.5.5.2.1\">6</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.6.6.1\">Feed-forward dimension</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.6.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.6.6.2.1\">2048</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.7.7.1\">Embedding dimension</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.7.7.2\">128, <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.7.7.2.1\">256</span>, 512</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.8.8.1\">Label smoothing</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.8.8.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.8.8.2.1\">0.1</span>, 0.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.9.9.1\">Dropout</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.9.9.2\">0.1, <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.9.9.2.1\">0.3</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.10.10.1\">Attention dropout</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.10.10.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.10.10.2.1\">0.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" id=\"S2.T1.1.11.11.1\">Average Decay</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"S2.T1.1.11.11.2\">0, <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.11.11.2.1\">0.0001</span>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Hyperparameter Optimization for Transformer models. Optimal parameters are highlighted in bold. The highest performing model trained on the 55k DGT corpus uses 2 attention heads whereas the best model trained with the larger 88k PA dataset uses 8 attention heads.</figcaption>\n</figure>",
            "capture": "Table 1: Hyperparameter Optimization for Transformer models. Optimal parameters are highlighted in bold. The highest performing model trained on the 55k DGT corpus uses 2 attention heads whereas the best model trained with the larger 88k PA dataset uses 8 attention heads."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.3.4.1\">Architecture</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1\">BLEU</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.2.2.1\">TER</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.3.3.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.3.3.1\">ChrF3</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.3.5.1\">Steps</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.3.3.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.3.3.6.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.3.6.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.3.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.3.6.1.1.1.1\">Runtime</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.3.6.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.3.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.3.6.1.2.1.1\">(hours)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.3.7.1\">kgCO<sub class=\"ltx_sub\" id=\"S4.T2.3.3.7.1.1\">2</sub></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.4.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.3.4.1.1\">dgt-rnn-base</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.3.4.1.2\">52.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.3.4.1.3\">0.42</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.3.4.1.4\">0.71</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.3.4.1.5\">75k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.3.4.1.6\">4.47</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.3.4.1.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.5.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.5.2.1\">dgt-rnn-bpe8k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.5.2.2\">54.6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.5.2.3\">0.40</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.5.2.4\">0.73</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.5.2.5\">85k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.5.2.6\">5.07</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.5.2.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.6.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.6.3.1\">dgt-rnn-bpe16k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.6.3.2\">55.6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.6.3.3\">0.39</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.6.3.4\">0.74</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.6.3.5\">100k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.6.3.6\">5.58</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.6.3.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.7.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.7.4.1\">dgt-rnn-bpe32k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.7.4.2\">55.3</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.7.4.3\">0.39</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.7.4.4\">0.74</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.7.4.5\">95k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.7.4.6\">4.67</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.3.7.4.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.8.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T2.3.8.5.1\">dgt-rnn-unigram</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T2.3.8.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.8.5.2.1\">55.6</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T2.3.8.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.8.5.3.1\">0.39</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T2.3.8.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.8.5.4.1\">0.74</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T2.3.8.5.5\">105k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T2.3.8.5.6\">5.07</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T2.3.8.5.7\">0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>RNN performance on DGT dataset of 52k lines</figcaption>\n</figure>",
            "capture": "Table 2: RNN performance on DGT dataset of 52k lines"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.4.1\">Architecture</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1\">BLEU</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.2.2.2.1\">TER</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.3.3.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.1\">ChrF3</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.5.1\">Steps</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.3.3.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.3.3.6.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.6.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.3.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.6.1.1.1.1\">Runtime</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.6.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.3.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.6.1.2.1.1\">(hours)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.7.1\">kgCO<sub class=\"ltx_sub\" id=\"S4.T3.3.3.7.1.1\">2</sub></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.4.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.4.1.1\">pa-rnn-base</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.4.1.2\">40.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.4.1.3\">0.47</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.4.1.4\">0.63</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.4.1.5\">60k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.4.1.6\">2.13</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.4.1.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.5.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.5.2.1\">pa-rnn-bpe8k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.5.2.2\">41.5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.5.2.3\">0.46</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.5.2.4\">0.64</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.5.2.5\">110k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.5.2.6\">4.16</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.5.2.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.6.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.6.3.1\">pa-rnn-bpe16k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.6.3.2\">41.5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.6.3.3\">0.46</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.6.3.4\">0.64</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.6.3.5\">105k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.6.3.6\">3.78</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.6.3.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.7.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.7.4.1\">pa-rnn-bpe32k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.7.4.2\">41.9</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.7.4.3\">0.47</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.7.4.4\">0.64</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.7.4.5\">100k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.7.4.6\">2.88</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.3.7.4.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.8.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T3.3.8.5.1\">pa-rnn-unigram</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T3.3.8.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.8.5.2.1\">41.9</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T3.3.8.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.8.5.3.1\">0.46</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T3.3.8.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.8.5.4.1\">0.64</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T3.3.8.5.5\">95k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T3.3.8.5.6\">2.75</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T3.3.8.5.7\">0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>RNN performance on PA dataset of 88k lines</figcaption>\n</figure>",
            "capture": "Table 3: RNN performance on PA dataset of 88k lines"
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.4.1\">Architecture</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1\">BLEU</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.2.1\">TER</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.3.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.3.1\">ChrF3</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.5.1\">Steps</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.3.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T4.3.3.6.1\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3.6.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.3.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.6.1.1.1.1\">Runtime</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3.6.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.3.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.6.1.2.1.1\">(hours)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.7.1\">kgCO<sub class=\"ltx_sub\" id=\"S4.T4.3.3.7.1.1\">2</sub></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.4.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.3.4.1.1\">dgt-trans-base</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.3.4.1.2\">53.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.3.4.1.3\">0.41</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.3.4.1.4\">0.72</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.3.4.1.5\">55k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.3.4.1.6\">14.43</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.3.4.1.7\">0.81</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.5.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.5.2.1\">dgt-trans-bpe8k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.5.2.2\">59.5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.5.2.3\">0.34</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.5.2.4\">0.77</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.5.2.5\">200k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.5.2.6\">24.48</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.5.2.7\">1.38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.6.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.6.3.1\">dgt-trans-bpe16k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.6.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.6.3.2.1\">60.5</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.6.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.6.3.3.1\">0.33</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.6.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.6.3.4.1\">0.78</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.6.3.5\">180k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.6.3.6\">26.90</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.6.3.7\">1.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.7.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.7.4.1\">dgt-trans-bpe32k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.7.4.2\">59.3</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.7.4.3\">0.35</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.7.4.4\">0.77</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.7.4.5\">100k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.7.4.6\">18.03</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.3.7.4.7\">1.02</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.8.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T4.3.8.5.1\">dgt-trans-unigram</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T4.3.8.5.2\">59.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T4.3.8.5.3\">0.35</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T4.3.8.5.4\">0.77</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T4.3.8.5.5\">125k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T4.3.8.5.6\">21.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T4.3.8.5.7\">1.24</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Transformer performance on 52k DGT dataset. Highest performing model uses 2 attention heads. All other models use 8 attention heads.</figcaption>\n</figure>",
            "capture": "Table 4: Transformer performance on 52k DGT dataset. Highest performing model uses 2 attention heads. All other models use 8 attention heads."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.4.1\">Architecture</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.1\">BLEU</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.2.2.2.1\">TER</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.3.3.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.3.1\">ChrF3</span> \n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.5.1\">Steps</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.3.3.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.3.3.6.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.3.3.6.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.3.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.6.1.1.1.1\">Runtime</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.3.6.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.3.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.6.1.2.1.1\">(hours)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.7.1\">kgCO<sub class=\"ltx_sub\" id=\"S4.T5.3.3.7.1.1\">2</sub></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.3.4.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.4.1.1\">pa-trans-base</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.4.1.2\">44.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.4.1.3\">0.44</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.4.1.4\">0.66</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.4.1.5\">20k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.4.1.6\">5.97</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T5.3.4.1.7\">0.34</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.5.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.5.2.1\">pa-trans-bpe8k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.5.2.2\">46.6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.5.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.5.2.3.1\">0.40</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.5.2.4\">0.68</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.5.2.5\">160k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.5.2.6\">20.1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.5.2.7\">1.13</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.6.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.6.3.1\">pa-trans-bpe16k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.6.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.6.3.2.1\">47.1</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.6.3.3\">0.41</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.6.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.6.3.4.1\">0.68</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.6.3.5\">100k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.6.3.6\">14.22</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.6.3.7\">0.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.7.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.7.4.1\">pa-trans-bpe32k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.7.4.2\">46.8</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.7.4.3\">0.41</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.7.4.4\">0.68</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.7.4.5\">70k</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.7.4.6\">12.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T5.3.7.4.7\">0.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.8.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T5.3.8.5.1\">pa-trans-unigram</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T5.3.8.5.2\">46.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T5.3.8.5.3\">0.42</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T5.3.8.5.4\">0.68</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T5.3.8.5.5\">75k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T5.3.8.5.6\">13.34</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T5.3.8.5.7\">0.75</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Transformer performance on 88k PA dataset. All models use 8 attention heads.</figcaption>\n</figure>",
            "capture": "Table 5: Transformer performance on 88k PA dataset. All models use 8 attention heads."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T6.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.1.1\">Source Language (English)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T6.1.1.1.2.1\">Reference Human Translation (Irish)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T6.1.2.2.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T6.1.2.2.1.1\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.2.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.2.2.1.1.1.1\">A clear harmonised procedure, including the</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.2.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.2.2.1.1.2.1\">necessary criteria for disease\u2013free status,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.2.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.2.2.1.1.3.1\">should be established for that purpose.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T6.1.2.2.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T6.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.2.2.2.1.1.1\">Ba cheart n\u00f3s imeachta comhchuibhithe soil\u00e9ir,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.2.2.2.1.2.1\">lena n-\u00e1ir\u00edtear na crit\u00e9ir is g\u00e1 do st\u00e1das saor</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.2.2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.2.2.2.1.3.1\">\u00f3 ghalar, a bhun\u00fa chun na cr\u00edche sin.</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.3.3.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T6.1.3.3.1.1\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.3.3.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.3.3.1.1.1.1\">the mark is applied anew, as appropriate.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.3.3.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T6.1.3.3.2.1\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.3.3.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.3.3.2.1.1.1\">d\u00e9anfar an mharc\u00e1il ar\u00eds, mar is iomchu\u00ed.</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T6.1.4.4.1.1\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.1.1.1.1\">If the court decides that a review is</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.1.1.2.1\">justified on any of the grounds set out in</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.1.1.3.1\">paragraph 1, the judgment given in the</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.1.1.4.1\">European Small Claims Procedure shall</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.1.1.5.1\">be null and void.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T6.1.4.4.2.1\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.2.1.1.1\">M\u00e1 chinneann an ch\u00fairt go bhfuil bonn cirt</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.2.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.2.1.2.1\">le hathbhreithni\u00fa de bharr aon cheann de na</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.2.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.2.1.3.1\">forais a leagtar amach i m\u00edr 1, beidh an</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.2.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.2.1.4.1\">breithi\u00fanas a tugadh sa N\u00f3s Imeachta Eorpach</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.4.2.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T6.1.4.4.2.1.5.1\">um \u00c9ilimh Bheaga ar neamhn\u00ed go hioml\u00e1n.</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S6.T6.1.5.5.1\">households where pet animals are kept;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S6.T6.1.5.5.2\">teaghlaigh ina gcoime\u00e1dtar peata\u00ed;</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Samples of human reference translations</figcaption>\n</figure>",
            "capture": "Table 6: Samples of human reference translations"
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T7\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T7.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T7.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.2.2.3.1\">Transformer (16 kBPE)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.1.1.1.1\">BLEU</span> \n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.2.2.4.1\">Google Translate</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S6.T7.2.2.2.1\">BLEU</span> \n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T7.2.3.1.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T7.2.3.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.1.1.1.1\">Ba cheart n\u00f3s imeachta soil\u00e9ir</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.1.1.2.1\">comhchuibhithe, lena n-\u00e1ir\u00edtear</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.1.1.3.1\">na crit\u00e9ir is g\u00e1 maidir le</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.1.1.4.1\">st\u00e1das saor \u00f3 ghalair, a bhun\u00fa</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.1.1.5.1\">chun na cr\u00edche sin.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T7.2.3.1.2\">61.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T7.2.3.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T7.2.3.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.3.1.1.1\">Ba cheart n\u00f3s imeachta</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.3.1.2.1\">comhchuibhithe soil\u00e9ir, lena</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.3.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.3.1.3.1\">n-\u00e1ir\u00edtear na crit\u00e9ir riachtanacha</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.3.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.3.1.4.1\">maidir le st\u00e1das saor \u00f3 ghalair,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.3.1.3.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.3.1.3.1.5.1\">a bhun\u00fa chun na cr\u00edche sin.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T7.2.3.1.4\">70.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.4.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.4.2.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T7.2.4.2.1.1\">\n<tr class=\"ltx_tr\" id=\"S6.T7.2.4.2.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.4.2.1.1.1.1\">go gcuirtear an marc i bhfeidhme,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.4.2.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.4.2.1.1.2.1\">de r\u00e9ir mar is iomchu\u00ed.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.2.4.2.2\">21.4</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.4.2.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T7.2.4.2.3.1\">\n<tr class=\"ltx_tr\" id=\"S6.T7.2.4.2.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.4.2.3.1.1.1\">cuirtear an marc i bhfeidhm as</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.4.2.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.4.2.3.1.2.1\">an nua, de r\u00e9ir mar is cu\u00ed.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.2.4.2.4\">6.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T7.2.5.3.1.1\">\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.1.1.1.1\">M\u00e1 chinneann an ch\u00fairt go bhfuil</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.1.1.2.1\">bonn cirt le hathbhreithni\u00fa ar aon</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.1.1.3.1\">cheann de na forais a leagtar amach</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.1.1.4.1\">i m\u00edr 1, beidh an breithi\u00fanas a</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.1.1.5.1\">thugtar sa N\u00f3s Imeachta Eorpach</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.1.1.6.1\">um \u00c9ilimh Bheaga ar neamhn\u00ed.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.2.5.3.2\">77.3</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T7.2.5.3.3.1\">\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.3.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.3.1.1.1\">M\u00e1 chinneann an ch\u00fairt go bhfuil</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.3.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.3.1.2.1\">\u00fadar le hathbhreithni\u00fa ar aon</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.3.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.3.1.3.1\">cheann de na forais at\u00e1 leagtha</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.3.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.3.1.4.1\">amach i m\u00edr 1, beidh an</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.3.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.3.1.5.1\">breithi\u00fanas a thugtar sa</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.3.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.3.1.6.1\">N\u00f3s Imeachta Eorpach um</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.5.3.3.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T7.2.5.3.3.1.7.1\">\u00c9ilimh Bheaga ar neamhn\u00ed</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.2.5.3.4\">59.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T7.2.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S6.T7.2.6.4.1\">teaghlaigh ina gcoime\u00e1dtar peata\u00ed;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T7.2.6.4.2\">100</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S6.T7.2.6.4.3\">teaghlaigh ina gcoinn\u00edtear peata\u00ed;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T7.2.6.4.4\">30.2</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Transformer model compared with Google Translate using random samples from the DGT corpus. Full evaluation of Google Translate on the DGT test set, with 1.3k lines, generated a BLEU score of 46.3 and a TER score of 0.44. Comparative scores on the test set using our Transformer model, with 2 attention heads and 16k BPE submodel realised 60.5 for BLEU and 0.33 for TER.</figcaption>\n</figure>",
            "capture": "Table 7: Transformer model compared with Google Translate using random samples from the DGT corpus. Full evaluation of Google Translate on the DGT test set, with 1.3k lines, generated a BLEU score of 46.3 and a TER score of 0.44. Comparative scores on the test set using our Transformer model, with 2 attention heads and 16k BPE submodel realised 60.5 for BLEU and 0.33 for TER."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.01985v1_figure_1.png",
            "caption": "Figure 1: Proposed Approach"
        },
        "2": {
            "figure_path": "2403.01985v1_figure_2.png",
            "caption": "Figure 2: BLEU performance for all model architectures"
        },
        "3": {
            "figure_path": "2403.01985v1_figure_3.png",
            "caption": "Figure 3: TER performance for all model architectures"
        },
        "4": {
            "figure_path": "2403.01985v1_figure_4.png",
            "caption": "Figure 4: Training DGT Transformer baseline"
        },
        "5": {
            "figure_path": "2403.01985v1_figure_5.png",
            "caption": "Figure 5: Training DGT Transformer 16k BPE"
        }
    },
    "references": [
        {
            "1": {
                "title": "The digital divide and social inclusion among refugee migrants.",
                "author": "Alam, K. and Imran, S. (2015).",
                "venue": "Information Technology & People.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Optimizing transformer for low-resource neural machine translation.",
                "author": "Araabi, A. and Monz, C. (2020).",
                "venue": "arXiv preprint arXiv:2011.02266.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Neural machine translation by jointly learning to align and translate.",
                "author": "Bahdanau, D., Cho, K., and Bengio, Y. (2014).",
                "venue": "arXiv preprint arXiv:1409.0473.",
                "url": null
            }
        },
        {
            "4": {
                "title": "On the dangers of stochastic parrots: Can language models be too big?",
                "author": "Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021).",
                "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610\u2013623.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Random search for hyper-parameter optimization.",
                "author": "Bergstra, J. and Bengio, Y. (2012).",
                "venue": "Journal of machine learning research, 13(2).",
                "url": null
            }
        },
        {
            "6": {
                "title": "Google colaboratory.",
                "author": "Bisong, E. (2019).",
                "venue": "In Building Machine Learning and Deep Learning Models on Google Cloud Platform, pages 59\u201364. Springer.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Findings of the 2017 conference on machine translation (WMT17).",
                "author": "Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., Huck, M., Koehn, P., Liu, Q., Logacheva, V., Monz, C., Negri, M., Post, M., Rubino, R., Specia, L., and Turchi, M. (2017).",
                "venue": "In Proceedings of the Second Conference on Machine Translation, pages 169\u2013214, Copenhagen, Denmark. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Findings of the 2018 conference on machine translation (WMT18).",
                "author": "Bojar, O., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Koehn, P., and Monz, C. (2018).",
                "venue": "In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 272\u2013303, Belgium, Brussels. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "9": {
                "title": "On the properties of neural machine translation: Encoder-decoder approaches.",
                "author": "Cho, K., Van Merri\u00ebnboer, B., Bahdanau, D., and Bengio, Y. (2014).",
                "venue": "arXiv preprint arXiv:1409.1259.",
                "url": null
            }
        },
        {
            "10": {
                "title": "A call for prudent choice of subword merge operations in neural machine translation.",
                "author": "Ding, S., Renduchintala, A., and Duh, K. (2019).",
                "venue": "arXiv preprint arXiv:1905.10453.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Smt versus nmt: Preliminary comparisons for irish.",
                "author": "Dowling, M., Lynn, T., Poncelas, A., and Way, A. (2018).",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "A new algorithm for data compression.",
                "author": "Gage, P. (1994).",
                "venue": "C Users Journal, 12(2):23\u201338.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Deep reconstruction-classification networks for unsupervised domain adaptation.",
                "author": "Ghifary, M., Kleijn, W. B., Zhang, M., Balduzzi, D., and Li, W. (2016).",
                "venue": "In European Conference on Computer Vision, pages 597\u2013613. Springer.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Finding the optimal vocabulary size for neural machine translation.",
                "author": "Gowda, T. and May, J. (2020).",
                "venue": "arXiv preprint arXiv:2004.02334.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Opennmt: Open-source toolkit for neural machine translation.",
                "author": "Klein, G., Kim, Y., Deng, Y., Senellart, J., and Rush, A. M. (2017).",
                "venue": "arXiv preprint arXiv:1701.02810.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Subword regularization: Improving neural network translation models with multiple subword candidates.",
                "author": "Kudo, T. (2018).",
                "venue": "arXiv preprint arXiv:1804.10959.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
                "author": "Kudo, T. and Richardson, J. (2018).",
                "venue": "arXiv preprint arXiv:1808.06226.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Quantifying the carbon emissions of machine learning.",
                "author": "Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. (2019).",
                "venue": "arXiv preprint arXiv:1910.09700.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Pivot machine translation using chinese as pivot language.",
                "author": "Liu, C.-H., Silva, C. C., Wang, L., and Way, A. (2018).",
                "venue": "In China Workshop on Machine Translation, pages 74\u201385. Springer.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Responses to language barriers in consultations with refugees and asylum seekers: a telephone survey of irish general practitioners.",
                "author": "MacFarlane, A., Glynn, L. G., Mosinkie, P. I., and Murphy, A. W. (2008).",
                "venue": "BMC Family Practice, 9(1):1\u20136.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Design and analysis of experiments.",
                "author": "Montgomery, D. C. (2017).",
                "venue": "John wiley & sons.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Bleu: a method for automatic evaluation of machine translation.",
                "author": "Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).",
                "venue": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
                "url": null
            }
        },
        {
            "23": {
                "title": "chrf: character n-gram f-score for automatic mt evaluation.",
                "author": "Popovi\u0107, M. (2015).",
                "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392\u2013395.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Informing the use of hyperparameter optimization through metalearning.",
                "author": "Sanders, S. and Giraud-Carrier, C. (2017).",
                "venue": "In 2017 IEEE International Conference on Data Mining (ICDM), pages 1051\u20131056. IEEE.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Sustainable Energy in Ireland.",
                "author": "SEAI (2020).",
                "venue": null,
                "url": null
            }
        },
        {
            "26": {
                "title": "Neural machine translation of rare words with subword units.",
                "author": "Sennrich, R., Haddow, B., and Birch, A. (2015).",
                "venue": "arXiv preprint arXiv:1508.07909.",
                "url": null
            }
        },
        {
            "27": {
                "title": "A study of translation edit rate with targeted human annotation.",
                "author": "Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and Makhoul, J. (2006).",
                "venue": "In Proceedings of association for machine translation in the Americas, volume 200. Citeseer.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Results of the wmt15 metrics shared task.",
                "author": "Stanojevi\u0107, M., Kamran, A., Koehn, P., and Bojar, O. (2015).",
                "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 256\u2013273.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Dgt-tm: A freely available translation memory in 22 languages.",
                "author": "Steinberger, R., Eisele, A., Klocek, S., Pilos, S., and Schl\u00fcter, P. (2013).",
                "venue": "arXiv preprint arXiv:1309.5226.",
                "url": null
            }
        },
        {
            "30": {
                "title": "On optimal transformer depth for low-resource language translation.",
                "author": "Van Biljon, E., Pretorius, A., and Kreutzer, J. (2020).",
                "venue": "arXiv preprint arXiv:2004.04418.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Attention is all you need.",
                "author": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017).",
                "venue": "arXiv preprint arXiv:1706.03762.",
                "url": null
            }
        },
        {
            "32": {
                "title": "MT Developments in the EU: Keynote AMTA 2020.",
                "author": "Way, A. (2020).",
                "venue": null,
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.01985v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2.1",
            "4.2.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.01985v1",
        "paper_title": "Transformers for Low-Resource Languages: Is F\u00e9idir Linn!",
        "research_background": "### Paper's Motivation\n\nThe motivation behind this paper lies in addressing the significant disparity in translation quality between well-resourced and low-resourced languages when using Neural Machine Translation (NMT). While NMT has revolutionized high-quality translations for languages with abundant data, low-resource languages, such as Irish, continue to face challenges due to data scarcity. This research is driven by the need to improve NMT performance for these low-resource languages.\n\n### Research Problem\n\nThe research problem the paper tackles is the development of effective NMT systems tailored for low-resource languages, with a specific focus on the Irish language. The current state-of-the-art NMT systems do not perform well when directly applied to low-resource languages due to the lack of large, high-quality datasets. Therefore, this study aims to investigate alternative methods to enhance NMT performance in these contexts. One key aspect under investigation is identifying the best subword models suited for English-to-Irish translation in a data-scarce environment.\n\n### Relevant Prior Work\n\n1. **Dowling et al., 2018**: This study demonstrated that a generic out-of-the-box NMT system for English-Irish translation yielded lower quality translations compared to a specialized Statistical Machine Translation (SMT) system. This highlights the pressing need for tailored NMT solutions for low-resource languages.\n\n2. **Ding et al., 2019 (###reference_b10###) and Gowda and May, 2020 (###reference_b14###)**: These studies have focused on selecting subword models for high-resource languages, implying a gap in research when it comes to similar investigations for low-resource languages like Irish. The current study seeks to fill this gap by identifying the most effective subword models in low-resource settings.\n\nBy addressing these gaps through targeted research, the paper aims to contribute valuable insights and practical solutions to the field of low-resource language technology, thereby enhancing the digital engagement and translation quality for languages like Irish.",
        "methodology": "The study explores hyperparameter optimization (HPO) of Transformer models in low-resource settings, a practice that has previously shown significant performance improvements for RNN models. The key focus of the research is to assess the extent to which such optimization techniques can be beneficial for Transformer models under similar low-resource conditions. \n\nTo achieve this, the researchers modified several crucial model parameters, including:\n- The number of attention heads\n- The number of layers\n- Regularization techniques such as dropout and label smoothing\n\nA significant emphasis was placed on evaluating the choice of subword model type along with the vocabulary size. This evaluation was carried out using two English-Irish parallel datasets:\n1. A general corpus from the Directorate General for Translation (DGT) containing 52k lines.\n2. An in-domain corpus consisting of 88k lines of Public Administration (PA) data.\n\nFor both datasets, a shared subword vocabulary and a shared SentencePiece model were generated by concatenating the source and target corpora. Notably, the research did not explore the impact of using separate subword models for the source and target languages.\n\nThe evaluation process included the following sets:\n- For the DGT corpus: a test set of 1.3k lines and a development set of 2.6k lines.\n- For the PA dataset: a test set of 1.5k lines and a validation set of 3k lines.\n\nThe methodology involved testing the effectiveness of the proposed optimization techniques.\n\nBy retaining and exploring the impact of these hyperparameters, the methodology aims to determine the potential performance improvements in Transformer models for low-resource language settings.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Setup:**\n\n1. **Datasets:**\n   - **DGT Dataset**: A publicly available dataset categorized as generic, including Translation Memory (TM) covering all official European Union languages.\n   - **Public Administration Dataset**: Focuses on public administration data and includes materials such as staff notices, annual reports, website content, press releases, and official correspondence. This dataset was primarily provided by the Department of Tourism, Culture, Arts, Gaeltacht, Sport and Media in Ireland.\n   - **Additional Data**: Training data also included parallel texts from the Digital Corpus of the European Parliament (DCEP) and DGT, as well as crawled data from domains of similar content. A parallel corpus from Conradh na Gaeilge (CnaG), an organization promoting the Irish language, was also included, compiled as part of a previous study.\n\n2. **Baselines:**\n   - The models used for comparison in this experiment are the Transformer and Recurrent Neural Network (RNN) approaches.\n\n3. **Evaluation Metrics:**\n   - The performance of the models was evaluated using standard machine translation metrics. However, the specific metrics used (e.g., BLEU scores, etc.) are not explicitly mentioned in the provided text.\n\n**Main Experimental Results:**\n\nThe experiment evaluates the performance of Transformer and RNN models on English to Irish translation using the aforementioned datasets. However, specific numerical results or comparative performance outcomes between the Transformer and RNN models are not provided in the text extract.\n\nRetaining the performance outcomes and specific evaluation scores would be essential for a comprehensive understanding, but this detail is not included in the provided section."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the impact of hyperparameter optimization on Transformer models in low-resource English-Irish translation settings and identify key factors driving performance improvements.",
            "experiment_process": "Evaluations included modifying the number of attention heads, the number of layers, and experimenting with regularization techniques such as dropout and label smoothing. Optimization was carried out on two parallel English-Irish datasets: a general corpus of 52k lines from the Directorate General for Translation (DGT) and an 88k-line in-domain Public Administration (PA) dataset. All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece subword model. Two baseline architectures, RNN and Transformer, were evaluated. Hyperparameter choices for Transformer models (e.g., values outlined in Table 1) were tested using a random search approach, within short cycles of 5k training steps.",
            "result_discussion": "The optimized Transformer model demonstrated a substantial BLEU score improvement of 7.8 points over the baseline RNN model. Improvements were observed across a range of metrics, including TER, indicating a significantly reduced post-editing effort for Transformer models with 16k BPE subword models.",
            "ablation_id": "2403.01985v1.No1"
        },
        {
            "research_objective": "Investigate the effectiveness of different Transformer architecture configurations in low-resource settings.",
            "experiment_process": "Configurations were tested that varied the number of neurons in each layer and modified the number of layers used in the Transformer architecture. The impact of regularization by applying varying degrees of dropout (smaller values like 0.1 and larger values like 0.3) to the output of each feedforward layer was also evaluated.",
            "result_discussion": "Configurations with smaller and fewer layers were shown to improve performance in low-resource NMT scenarios. The application of regularization techniques, such as dropout, further optimized the models' performance.",
            "ablation_id": "2403.01985v1.No2"
        },
        {
            "research_objective": "Determine the optimal subword model for maximizing translation performance in low-resource English-Irish translation.",
            "experiment_process": "Evaluated subword regularization techniques, involving BPE and unigram models, tested with varying vocabulary sizes of 4k, 8k, 16k, and 32k.",
            "result_discussion": "The study found substantial improvements by reducing the number of BPE merge operations, with BPE models showing pronounced performance boosts. The use of a 16k BPE subword model in particular led to significant enhancements in translation accuracy.",
            "ablation_id": "2403.01985v1.No3"
        }
    ]
}