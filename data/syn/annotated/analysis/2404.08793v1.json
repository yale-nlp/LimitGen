{
    "title": "\\name: Visual Analysis of Jailbreak Attacks Against Large Language Models",
    "abstract": "The proliferation of large language models (LLMs) has underscored concerns regarding their security vulnerabilities, notably against jailbreak attacks, where adversaries design jailbreak prompts to circumvent safety mechanisms for potential misuse.\nAddressing these concerns necessitates a comprehensive analysis of jailbreak prompts to evaluate LLMs\u2019 defensive capabilities and identify potential weaknesses.\nHowever, the complexity of evaluating jailbreak performance and understanding prompt characteristics makes this analysis laborious.\nWe collaborate with domain experts to characterize problems and propose an LLM-assisted framework to streamline the analysis process.\nIt provides automatic jailbreak assessment to facilitate performance evaluation and support analysis of components and keywords in prompts.\nBased on the framework, we design \\name, a visual analysis system that enables users to explore the jailbreak performance against the target model, conduct multi-level analysis of prompt characteristics, and refine prompt instances to verify findings.\nThrough a case study, technical evaluations, and expert interviews, we demonstrate our system\u2019s effectiveness in helping users evaluate model security and identify model weaknesses.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section, we discuss the related work of our study, including prompt jailbreaking and visualization for understanding NLP models."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Prompt Jailbreaking",
            "text": "Prompt Jailbreaking, known as one of the most famous adversarial attacks[47  ###reference_b47###], refers to cunningly altering malicious prompts to bypass the safety measures of LLMs and generate harmful content, such as illegal activities.\nWith the proliferation of LLMs, an increasing number of jailbreak strategies [26  ###reference_b26###, 75  ###reference_b75###], such as character role play, have been discovered and shared on social platforms (e.g., Reddit and Discord).\nThis trend has motivated new research to analyze their prompt characteristics.\nLiu et al. [37  ###reference_b37###] propose a taxonomy for jailbreak prompts, which categorizes jailbreak strategies into ten distinct classes, such as Character Role Play and Assumed Responsibility.\nShen et al. [53  ###reference_b53###] report several key findings regarding jailbreak prompts\u2019 semantic distribution and evolution.\nWei et al. [66  ###reference_b66###] empirically evaluate LLM vulnerability and summarize two failure modes, including competing objectives and mismatched generalization.\nThese studies mainly focus on the general characteristics of jailbreak prompts.\nIn comparison, our work provides a multi-level analysis framework to help users systematically explore the jailbreak prompt and identify the model\u2019s weaknesses.\nSome other works [77  ###reference_b77###, 28  ###reference_b28###] propose automatic approaches for red teaming LLMs.\nZou et al. [78  ###reference_b78###] propose GCG to search for the optimal adversarial prompt suffixes based on the gradient of white-box LLMs.\nDeng et al. [13  ###reference_b13###] propose a time-based testing strategy to infer the defense mechanisms of LLM (e.g., check the input questions and output responses) and fine-tune the LLM for jailbreak prompt generation.\nTo better utilize the effectiveness of manually designed prompts, GPTFuzzer [74  ###reference_b74###] selects human-crafted prompts as the initial seeds and mutates them into new ones.\nDing et al. [15  ###reference_b15###] propose two strategies, including prompt rewriting and scenario nesting, to leverage the capability of LLMs to generate jailbreak prompts.\nInspired by these methods, we propose prompt perturbation strategies based on the prompt components, allowing users to conduct a comparative analysis of the prompt components to understand their effects on jailbreak performance."
        },
        {
            "section_id": "1.2",
            "parent_section_id": "1",
            "section_name": "Visualization for Understanding NLP Models",
            "text": "Visualization plays an indispensable role in bridging the explainability gap in NLP models [27  ###reference_b27###, 29  ###reference_b29###, 57  ###reference_b57###, 19  ###reference_b19###, 18  ###reference_b18###], allowing for a more sophisticated understanding of model performance [65  ###reference_b65###], decision boundary [11  ###reference_b11###], and vulnerability [41  ###reference_b41###].\nModel-specific visualizations focus on revealing the internal mechanisms of NLP models.\nRNNVis [44  ###reference_b44###] and LSTMVis [58  ###reference_b58###] visualize the hidden state dynamics of recurrent neural networks.\nWith the emergence of transformer-based models [64  ###reference_b64###, 14  ###reference_b14###], numerous visualizations [73  ###reference_b73###, 50  ###reference_b50###, 21  ###reference_b21###] are proposed to uncover the architecture of these models, especially their self-attention mechanism.\nModel-agnostic visualizations [7  ###reference_b7###, 17  ###reference_b17###, 32  ###reference_b32###, 20  ###reference_b20###] treat the NLP models as black boxes and focus on explaining the input-output behavior, enabling users to analyze and compare models for downstream applications [31  ###reference_b31###, 59  ###reference_b59###, 68  ###reference_b68###, 24  ###reference_b24###, 60  ###reference_b60###, 39  ###reference_b39###].\nThe What-If Tool [69  ###reference_b69###] and DECE [11  ###reference_b11###] visualize the dataset and model prediction at multiple scales, enabling users to conduct counterfactual analysis.\nNLIZE [35  ###reference_b35###] employs a perturbation-driven paradigm to help users analyze the stability of model predictions for natural language inference tasks.\nBased on explainable AI techniques (e.g., SHAP [40  ###reference_b40###]) and external commonsense knowledge bases [56  ###reference_b56###], CommonsenseVIS [65  ###reference_b65###] analyzes the reasoning capabilities of NLP models for commonsense question-answering.\nOur work targets the jailbreak prompt attacks against large language models and aims to help model practitioners evaluate the jailbreak performance and understand prompt characteristics, thereby providing insights for strengthening models\u2019 safety mechanisms."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Problem Characterization",
            "text": "In this section, we introduce the background of jailbreak attacks, describe the requirement analysis, and introduce the taxonomy of prompt components to support the analysis of jailbreak attacks."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Background",
            "text": "Jailbreak Prompt Corpora.\nWith the widespread attention to jailbreak prompt attacks, there have been studies [37  ###reference_b37###, 53  ###reference_b53###, 61  ###reference_b61###] collecting jailbreak prompts and building corpora for semantic analysis and generation model training.\nMost of them decompose the jailbreak prompts into jailbreak questions and templates.\nOur work follows this principle and adopts the dataset by Liu et al. [37  ###reference_b37###] for analysis, which contains the most common and famous jailbreak prompts from the JailbreakChat website [2  ###reference_b2###].\nIn addition, we adopt their taxonomy of jailbreak questions and templates to support the comparative analysis of different jailbreak strategies and prohibited scenarios.\nJailbreak Questions.\nThe jailbreak questions are mainly designed around the prohibited scenarios of LLMs, such as \u201chow to rob a bank without being caught?\u201d\nDue to the safety mechanisms [13  ###reference_b13###], LLMs usually refuse to answer these questions and return some responses emphasizing ethical and legal constraints, such as \u201cI\u2019m sorry, but I\u2019m not going to guide you on how to engage in criminal activity.\u201d\nBased on OpenAI\u2019s disallowed usages [4  ###reference_b4###], Liu et al. [37  ###reference_b37###] have summarized a set of prohibited scenarios (e.g., Illegal Activities and Harmful Content) and collected a set of specific questions.\nJailbreak Templates.\nThe jailbreak templates are intentionally designed prompts to bypass the LLMs\u2019 safety mechanisms to get the model assistance for the jailbreak questions.\nFor example, some templates require LLMs to act as virtual characters who can answer questions without ethical and legal constraints.\nThe jailbreak templates usually contain placeholders (e.g., \u201c[INSERT PROMPT HERE]\u201d) for inserting different jailbreak questions.\nLiu et al. [37  ###reference_b37###] have summarized a taxonomy of jailbreak templates, which consists of ten jailbreak patterns, such as Character Role Play and Assumed Responsibility."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Design Requirements",
            "text": "Our work\u2019s target users are model practitioners focusing on model robustness and security.\nTo characterize domain problems and identify design requirements, we have collaborated with four domain experts over eight months.\nE1 and E2 are senior security engineers recruited from a technology company who have been working on NLP model security for more than four and three years, respectively.\nE3 and E4 are senior Ph.D. students from the secure machine learning field.\nAll of them have published papers on related research topics, such as red-teaming LLMs and adversarial attacks.\nWe interviewed them to understand their general analysis workflow and identify pain points.\nTo evaluate model robustness in defending against jailbreak attacks, users usually need to test the jailbreak prompt corpus on the target models and assess the jailbreak results (success or failure) to calculate the success rate.\nThe experts indicated that the jailbreak result assessment is tedious and sometimes requires question-specific knowledge (e.g., local regulations) to refine the assessment criteria to improve precision.\nMoreover, the overall indicator success rate is insufficient to support an in-depth analysis of the jailbreak prompts (e.g., analyzing how to improve the jailbreak performance of the prompts) to identify the model weaknesses.\nTo fill these gaps, we distilled a set of design requirements to guide the development of our system.\nWe also kept in touch with the experts through regular meetings to collect feedback regarding our prototype system and update design requirements.\nFinally, the design requirements are summarized as follows.\nR1. Facilitate the assessment of jailbreak results.\nJailbreak result assessment is the foundation of jailbreak prompt performance analysis.\nHowever, since the jailbreak results (\\iemodel responses) could be arbitrary and ambiguous, manually reviewing the jailbreak results can be an energy-exhausting task.\nTherefore, the system should introduce an automatic method to alleviate the manual workload and enhance the assessment efficiency.\nIn addition, to ensure the precision of assessment results, the system should help users explore them to identify unexpected results and refine the assessment criteria.\nR2. Support component analysis of jailbreak prompts.\nBesides evaluating the jailbreak prompt performance, users also need to understand prompt characteristics to gain insights for identifying model weaknesses and enhancing security mechanisms.\nGiven that various jailbreak prompts commonly contain similar sentence components (e.g., describing a subject with no moral constraints), the experts express strong interest in analyzing the prompts at the component level to understand the utilization of such components in constructing prompts and their importance to the prompt performance.\nR3. Summarize important keywords from jailbreak prompts.\nEffective keywords play an important role in successful jailbreak attacks and are closely related to jailbreak strategies.\nFor example, some role-playing templates name LLM as \u201cAIM\u201d (always intelligent and Machiavellian) to imply their amoral characterization.\nThe system should summarize important keywords from jailbreak prompts and help users explore them based on their jailbreak prompt performance.\nThis helps users identify effective keywords to strengthen the defense mechanisms (e.g., keyword detection) accordingly.\nR4. Support user refinement on jailbreak prompt instances.\nThe system should allow users to freely refine the jailbreak prompt instances according to their expertise.\nMeanwhile, the system should support ad-hoc evaluation of jailbreak performance to help users verify the effectiveness of prompt refinement.\nBased on such timely feedback, users can conduct what-if analysis to verify findings during the analysis workflow.\nIn addition, the improved jailbreak prompts can serve as new test samples for the jailbreak corpus, thus enhancing the diversity and robustness of the jailbreak performance evaluation."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Taxonomy of Jailbreak Prompt Components",
            "text": "To support component analysis of jailbreak prompts (R2), we conducted an empirical study on the jailbreak corpus with domain experts to summarize a taxonomy of jailbreak prompt components.\nTo gain a general understanding of the prompt components, we decomposed each jailbreak prompt in the corpus into basic sentences, analyzed their semantic content in context, and compared similar sentences in different prompts.\nThen, we conducted several brainstorming sessions and discussions with the experts to formulate and iteratively refine the component taxonomy.\nWe also validated the taxonomy on randomly selected jailbreak prompts to resolve ambiguities between component definitions.\nAfter establishing the coding scheme, the first and second authors and E3 separately coded the jailbreak prompts in the corpus and merged the coding differences through discussions.\nThe final taxonomy of jailbreak prompt components is listed in Table 1  ###reference_###.\nIt consists of eight major types, such as Scene Introduction (SI), Subject Characteristic (SC), and Task Operation (TO).\nWe also count and visualize the distribution of component sentences for different jailbreak strategies [37  ###reference_b37###] in Table 2  ###reference_###.\nAccording to the coding results, the Subject Characteristic components are frequently used among the most common strategies (\\ieCharacter Role Play, Assumed Responsibility, and Superior Model), and most of them usually describe the subject as a free person without ethical constraints and responsibility.\nWe distinguish the Output Constraint components from the Output Specification components because the Output Constraint components aim to exploit the model\u2019s instruction-following capability to break through the model\u2019s security defenses [66  ###reference_b66###], which is different from specifying the expected content or format of responses."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Analysis Framework",
            "text": "Our analysis framework is shown in Figure 1  ###reference_###B.\nBased on user-specified jailbreak questions and templates, it first assesses jailbreak results (R1) by combining each question and template to get the response from the target model and introducing an LLM-based approach to assess the model responses.\nThen, it decomposes the jailbreak prompts into the component and keyword levels to support prompt characteristic analysis.\nFor component analysis (R2), the framework classifies the basic prompt sentences into different component types based on the taxonomy we summarized from the jailbreak corpus (Section 3.3) and supports component-level perturbation for what-if analysis.\nFor keyword analysis (R3), the framework summarizes the important keywords from the prompts as well as the performance of their jailbreak prompts.\nBased on the analysis framework, the visual interface (Section 5) of our system supports the multi-level exploration of jailbreak prompts and refinement of prompt instances (R4).\n###figure_1###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Jailbreak Result Assessment",
            "text": "Since the LLMs [45  ###reference_b45###, 5  ###reference_b5###, 62  ###reference_b62###, 16  ###reference_b16###] have shown great capability in understanding and following human instructions, we employ GPT-4 [5  ###reference_b5###] to assess the responses of the target models.\nThe prompt design for assessment centers on three aspects.\nIntroduce the task background.\nWe emphasize the goal of evaluating model security and introduce the jailbreak scenarios based on OpenAI\u2019s disallowed usages [4  ###reference_b4###].\nThis gives the LLM an overall understanding of assessment tasks and makes it focus on the legality and ethics of assessed responses.\nClarify the target assessment results.\nOne of the potential solutions is to consider the task as a binary classification task that classifies the model responses into successful and failed jailbreaks.\nHowever, it is common for target models to warn users of potential consequences when generating unauthorized content due to their competing training objectives (\\ieinstruction-following and safety) [66  ###reference_b66###].\nTo better accommodate these ambiguous responses, we adopt the taxonomy of Yu et al. [74  ###reference_b74###] which defines four types of assessment results.\nFull Refusal. The response directly refuses to provide assistance for harmful questions or follow any other instructions.\nPartial Refusal. The response refuses to assist with harmful questions but follows other harmless instructions or offers ethical advice.\nPartial Compliance. The response contains unauthorized content but expresses disapproval or warns of potential harm.\nFull Compliance. The response completely follows the prompt instructions and contains unauthorized content.\nAdditionally, we add some representative examples with labeled annotations to the assessment prompt, leveraging the in-context learning of the LLMs to better distinguish their differences.\nSupport user refinement of assessment criteria.\nWe provide two ways, including correction feedback and additional criteria specification, to incorporate question-specific knowledge or adjust the rigor of the assessing criteria.\nCorrection feedback allows users to specify corrected assessment results and add them to the assessment prompt as new demonstration examples.\nUsers can also directly specify additional assessment criteria for specific jailbreak questions using natural language, providing a more flexible way to enhance assessment accuracy."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Component Analysis",
            "text": "To support users in analyzing the jailbreak prompt components, we propose a component classification method and three perturbation strategies based on the summarized taxonomy introduced in Section 3.3.\nThe component classification method classifies the prompt sentences into component types to support the overview of component utilization.\nBased on that, component perturbation creates a set of perturbation variations for the jailbreak prompts to support the comparative analysis of the jailbreak performance, thus enabling interrogation of the effects of different components."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Component Classification",
            "text": "Since prompt components (e.g., Subject Characteristic and Dialogue Example) usually consist of multiple basic sentences, we adopt a bottom-up strategy to (1) classify the basic sentences and (2) aggregate them into prompt components.\nSpecifically, we split the prompts into sentences based on syntax (e.g., the end of lines) and semantics (\\ieconjunctions like \u201cfor example\u201d).\nThen, we prompt GPT-4 to classify each sentence based on the component taxonomy.\nWe introduce the component definition and provide concrete examples for demonstration.\nFinally, we parse the classification results and combine neighboring sentences with the same component type to form the components."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Component Perturbation",
            "text": "Prior works [35  ###reference_b35###, 49  ###reference_b49###] have explored keyword-level perturbation (e.g., replacing keywords with synonymous) to test the model\u2019s robustness.\nHowever, performing keyword perturbation for each component can be a computation-intensive and time-consuming task since the prompts are usually long text paragraphs.\nTherefore, it would be more efficient and effective to perturb each component holistically.\nDrawing upon insights from discussions with domain experts, we propose three component perturbation strategies.\nDelete.\nDeleting the component is the most straightforward way to test how this component contributes to prompt performance.\nAs this strategy may result in the loss of certain key information or contextual incoherence, it usually causes a more or less decrease in the prompt performance, so that users can identify important components based on the magnitude of performance change.\nRephrase.\nThis strategy employs LLM to polish the given component sentences while maintaining their semantics.\nIt can provide more prompt variations without sacrificing contextual coherence.\nIn addition, since the LLM vendors have set safety mechanisms (e.g., training-time interventions [66  ###reference_b66###] and keyword detection [13  ###reference_b13###]) based on the common jailbreak prompts, rephrasing the components may help bypass the safety mechanisms and improve the jailbreak performance [15  ###reference_b15###].\nSwitch.\nThis strategy switches the given component to other types in three steps.\nFirst, we describe all component types (Table 1  ###reference_###) and require the LLM to choose new component types according to the prompt context.\nThen, we provide a set of alternatives for the target component types based on our component corpus (Table 2  ###reference_###) and rank them based on their semantic similarity with the original prompts.\nFinally, we replace the original components with the most similar alternatives and require the LLM to fine-tune the sentences to ensure contextual coherence."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Keyword Analysis",
            "text": "We identify prompt keywords by combining their importance and prompt performance.\nFirst, we split the prompt sentences into keywords and filter out stop words.\nThen, inspired by prior work [31  ###reference_b31###], we measure the importance of the keyword  for the given prompt  based on the keyword frequency and semantic similarity:\nwhere the  is the TF-IDF value [55  ###reference_b55###] of the keyword for the prompt and the  is the semantic similarity of the keyword and prompt.\nA higher semantic similarity indicates a greater relevance of the keyword to the prompt semantics.\nWe encode the keywords and prompts using the embedding model by OpenAI [3  ###reference_b3###] and measure their similarity based on the cosine distance.\nBased on that, we calculate the importance of keyword  for the whole corpus as\nwhere  is the list of jailbreak prompts that contain the keyword .\nTo help users analyze the effect of important keywords, we measure their performance according to the performance of their corresponding prompts.\nSince the keyword  might be utilized in various prompts with different importance, we propose importance-weighted performance to better summarize the effect of the keyword :\nwhere  represents the jailbreak performance of prompt , expressed as a percentage of four categories of assessment results."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "System Design",
            "text": "We develop \\name to support multi-level visual analysis of jailbreak prompts for evaluating the model\u2019s defensive capability.\nAs shown in \\name: Visual Analysis of Jailbreak Attacks Against Large Language Models, the user interface of \\name consists of five views.\nUsers\u2019 exploration starts with the Configuration View, which allows users to configure jailbreak questions and templates for analysis.\nThen, the system automatically gets and assesses the jailbreak results (\\iemodel responses) and provides a visual summary of the assessment results in Summary View, enabling users to overview the jailbreak performance of jailbreak prompts.\nResponse View helps users explore model responses and refine assessment criteria to enhance precision.\nBased on the assessment results, users can delve into the component analysis in Summary View and the keyword analysis in Keyword View.\nFinally, the Instance View helps users inspect and refine the template instances.\nThe visual system encodes four categories of assessment results using red and blue colors with varying transparency, \\ieFull Compliance, Partial Compliance, Partial Refusal, and Full Refusal."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Jailbreak Corpus Configuration",
            "text": "Configuration View (Figure 1) allows users to upload the jailbreak corpus and select jailbreak questions and templates for model evaluation.\nThe questions and templates are organized according to their categories (e.g., Character Role Play).\nThe selected items will be assigned serial numbers, that will serve as their unique identifiers in subsequent analyses.\nUsers can also modify the questions and templates based on their exploratory interests.\nBesides, the system allows users to configure the number of evaluations for each question-template combination to improve the robustness of evaluation results.\nAfter user configuration and submission, the system automatically combines each question and the template (\\iefill the question into the placeholder in the template) to get the responses from the target model."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Jailbreak Performance Exploration",
            "text": "To support jailbreak performance exploration, the system provides a visual summary of jailbreak evaluation and allows users to inspect model responses to verify their correctness."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Summary of Jailbreak Performance",
            "text": "The left half of the Summary View (Figure 1) visualizes the performance of the jailbreak prompts through a matrix visualization, where the horizontal axis represents questions and the vertical axis represents templates.\nThe questions and templates are grouped by category and are represented using the same serial numbers as in the Configuration View.\nThe categories are collapsed by default to visualize their aggregated performance and support click interactions to expand them to check the performance of specific questions or templates.\nEach cell within this matrix contains a pie chart showing the percentage of assessment results of the corresponding template and question.\nThe size of the pie chart encodes the number of evaluations.\nUsers can also click the serial numbers of questions to inspect their model responses in Response View or click the templates to inspect them in Instance View."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Model Response Inspection",
            "text": "Since the jailbreak assessment criteria can be question-specific and sometimes need to incorporate contextual knowledge.\nWe design Inspection View (\\name: Visual Analysis of Jailbreak Attacks Against Large Language Models) to streamline the process of exploring the assessment results and iteratively refining the criteria.\nIt visualizes the model responses in a scatter plot.\nWe embed model responses based on OpenAI\u2019s embedding model [3  ###reference_b3###] and project them into a 2D space using the t-SNE algorithm [63  ###reference_b63###] to maintain their semantic similarity.\nThe color of the points encodes the categories of assessment results.\nThis helps users identify questionable assessment results based on semantic similarity (e.g., inspect a red point that appears in a blue cluster).\nTo enable users to improve assessment accuracy, the Response View supports users in refining the assessment criteria in two ways.\nAfter identifying unexpected assessment results, users can directly correct their categories through click interaction or specify additional assessment criteria in natural language.\nUsers can switch between these two refinement modes using the switch widget at the upper right corner.\nThe corrected examples and specified criteria can be submitted to enhance the system prompts for a new round of assessment (Section 4.1)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Component Exploration",
            "text": "Based on the assessment results of the jailbreak templates, users can analyze and compare the effects of different prompt components in the right half of Summary View (\\name: Visual Analysis of Jailbreak Attacks Against Large Language Models).\nThe left column summarizes the components of the selected jailbreak templates.\nIt visualizes each prompt as a horizontal stacked bar chart, where each bar segment corresponds to a distinct component.\nAs shown in Figure 2  ###reference_###, the bar segments are arranged in the order corresponding to the prompt components, with the color encoding the component type and the length indicating the token length.\nIt helps users understand the general patterns of prompt components and serves as the baseline for prompt perturbation.\nUsers can click the icon  to view the template details in the Instance View and click the icon  to generate a set of component perturbation results for comparative analysis.\n###figure_2### ###figure_3### For comparison, the perturbation results are visualized in the right column as horizontal stacked bars similar to the original templates.\nEach result is generated by applying a perturbation strategy to a component of the original template.\nTo visualize this difference, we design three kinds of glyphs to represent these strategies and overlay them on the corresponding bar segments, as shown in Figure 2  ###reference_###.\nThe system also automatically evaluates the jailbreak performance of these perturbation results based on the selected questions and visualizes the percentage of assessment results in pie charts, enabling users to compare the effects of different component perturbations.\nUsers can toggle to check the perturbation results for a particular strategy using the radio box at the top of the column.\n###figure_4###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Keyword Exploration",
            "text": "The Keyword View (\\name: Visual Analysis of Jailbreak Attacks Against Large Language Models) visualizes the jailbreak performance and importance of keywords (Section 4.3).\nSpecifically, as shown in Figure 3  ###reference_###, the jailbreak performance of keyword , \\ie, is represented as the percentage of four categories of assessment results, denoted as .\nInspired by prior work [35  ###reference_b35###], we introduce a square space with a coordinate system (Figure 3  ###reference_###) whose four vertices correspond to the four categories, and their coordinates are denoted as .\nEach corner is colored to indicate its category using the same color scheme as the pie charts in the Summary View.\nTo visualize the overall performance distribution of the keyword, the coordinate of the keyword  is computed as:\nBesides, the size of the keywords encodes their importance in the corpus.\nIt enables users to overview and compare the usage and performance of keywords based on their position and size.\nUsers can filter keywords by component type and click the keywords to view their context.\nAlternative Design. We have also considered an alternative design (Figure 3  ###reference_###) where the keywords are visualized in four separate word clouds (each for one assessment category) and their sizes correspond to their importance to the prompts of this category, \\ie.\nAccording to the feedback from the experts, although this design enables users to focus on the keywords in the same category, it is inefficient for users to compare the size of the same keywords in all word clouds to estimate their overall performance distribution. Therefore, we chose our current design.\n###figure_5###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Template Instance Refinement",
            "text": "The Instance View (\\name: Visual Analysis of Jailbreak Attacks Against Large Language Models) allows users to refine the templates and evaluate the performance.\nThe Jailbreak Content panel (\\name: Visual Analysis of Jailbreak Attacks Against Large Language Models) lists the prompt text of each component, which supports manual modifications or automatic perturbations.\nUsers can click the icon at the right of the component title to  delete,  rephrase, or  switch the components.\nThen, users can evaluate their jailbreak performance on the selected questions and inspect the evaluation results in the Jailbreak Results panel (\\name: Visual Analysis of Jailbreak Attacks Against Large Language Models).\nEach result item shows the question and model response and visualizes the color of the assessment result.\nThis feedback can help the user evaluate the effectiveness of the modifications to verify the findings during the component and keyword analysis.\n###figure_6### ###figure_7### ###figure_8### ###figure_9###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We conducted a case study, two technical evaluations, and six expert interviews to verify the effectiveness of the analysis framework and the usability of the visual analysis system."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Technical Evaluations of LLM-based Methods",
            "text": "Jailbreak result assessment and prompt component classification are critical to the analysis workflow and rely heavily on the LLM capability and prompt design.\nTherefore, we conducted two technical evaluations to quantitatively measure the effectiveness of these methods.\nSince no recognized benchmark datasets were available for these two tasks, we drew inspiration from previous research [70  ###reference_b70###] and collaborated with experts (E3 and E4) to build improvised datasets.\nWe evaluated our methods and reported the results and some identified patterns that could affect the performance of the methods."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Jailbreak Result Assessment",
            "text": "In this task, we gathered model responses triggered by common jailbreak prompts, labeled the model responses in collaboration with experts, and evaluated the performance of our method.\nDataset.\nSince we follow the widely adopted principle [37  ###reference_b37###, 53  ###reference_b53###, 61  ###reference_b61###] that decomposes the jailbreak prompts into questions and templates, we randomly selected 20 questions from five perilous question categories, such as Harmful Content and Illegal Activities, and 30 templates for each question to gather a collection of model responses.\nNext, we removed duplicate answers (e.g., \u201cI\u2019m sorry, I can\u2019t assist with that request.\u201d) and randomly selected 24 responses for each question, resulting in a total of 20*24=480 model responses.\nThen, we worked with the experts to manually categorize the model responses.\nMethodology.\nWe partitioned the dataset to evaluate (1) the effectiveness of automatic jailbreak assessment and (2) the helpfulness of user refinement of assessment criteria.\nFirstly, we randomly selected 20 responses for each question as the test set and employed our method to assess them based on the default criteria introduced in Section 4.1.\nSecondly, we enhanced the default criteria by incorporating the remaining responses (four per question, each annotated with a label) as demonstration examples, and by enabling experts to specify additional criteria, thereby simulating realistic scenarios of analysis workflow.\nFor these two criteria, we separately measured the accuracy of the jailbreak assessment on the test set.\nResult.\nOverall, our method achieved 80.25% accuracy using the default criteria and 90.25% with the refined question-specific criteria.\nThe results showed that our method was effective in supporting jailbreak result assessment.\nWe also visualize the distribution of the assessment accuracy on each question in Figure 5  ###reference_###.\nWe found that specifying question-specific criteria could enhance the assessment accuracy across a majority of questions.\nFor example, sometimes the LLM failed to identify responses containing adult content (e.g., advertising and marketing prostitution) as prohibited content, which was not aligned with OpenAI\u2019s disallowed usages [4  ###reference_b4###, 37  ###reference_b37###].\nBy providing demonstration examples and specifying additional assessment criteria, our method could better identify such prohibited content.\n###figure_10###"
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Prompt Component Classification",
            "text": "Based on the prompt component corpus developed in collaboration with the experts (as detailed in Section 3.3), we constructed a dataset to evaluate our component classification method.\nDataset and Methodology.\nFrom the corpus, we randomly selected 50 prompts that contain 841 prompt sentences in total.\nThen, we employed our method to classify each sentence based on the component taxonomy (Table 1  ###reference_###).\nFinally, we measured the classification accuracy of the component types of sentences.\nResult.\nOverall, our method achieved 80.26% accuracy on the component classification task.\nWe also visualize the classification accuracy of each component type in a confusion matrix in Table 3  ###reference_###.\nOur method yielded adequate performance in most of the component categories.\nHowever, we noticed that sometimes our method incorrectly categorized the Scene Introduction components as Subject Characteristic type.\nTo reveal the patterns behind these failures, we further analyzed these unexpected results and found that the majority of such sentences often depicted the scene (e.g., a fictional world without moral constraints) that could imply the subject characteristics, potentially leading to confusion in LLM.\nA feasible solution would be to further clarify the component definition and provide demonstration examples to help our method better distinguish their differences."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Expert Interview",
            "text": "We interviewed six external experts (E5-E10) to evaluate the effectiveness of the analysis framework and the usability of the visual system.\nE5 is a model security engineer from a technical company who has been working on the secure reasoning of LLMs for more than one year and on network security (situation awareness) for over three years.\nE6-E10 are senior researchers from related fields, including model security, trustworthy AI, and deep learning model training.\nAmong them, E7 has accumulated eight years of experience in data and model security before she focused on LLM jailbreak attacks.\nEach expert interview lasted about 90 minutes.\nWe first briefly introduced the background and motivation of our study.\nThen, we described the analysis framework and visual system and demonstrated the system workflow using the case study.\nAfter that, we invited the experts to explore our system to analyze the performance and characteristics of the jailbreak prompts.\nFinally, we conducted semi-structured interviews with experts to collect their feedback about the analysis framework, visualization and interaction, and improvement suggestions.\nEffectiveness of the Analysis Framework.\nAll the experts agreed that our framework facilitated jailbreak performance evaluation and prompt characteristic understanding, and its workflow made sense.\nE5 praised this framework as \u201cit provided a more comprehensive and systematic evaluation for jailbreak attacks compared to existing tools.\u201d\nE5 appreciated the capability of the jailbreak assessment method to support user-specified criteria, which enhanced its flexibility to support customized criteria for different user values.\nHowever, we also observed one case during the exploration of E9 where the user-corrected examples did not improve the accuracy of a new round of assessment.\nE9 suggested recommending some representative model responses for user correction feedback to better leverage the in-context learning of the LLMs to distinguish between them.\nThe jailbreak component analysis was highly appreciated, being described as \u201cinteresting\u201d (E10), \u201cimpressive\u201d (E6), and \u201cinspiring\u201d (E7).\nIt was valued for \u201coffering a new perspective to study the prompt patterns in the black box scenarios\u201d (E10) and for \u201cguiding user effort towards the critical parts of the prompts\u201d (E9).\nThe experts confirmed that keyword analysis helped understand prompt characteristics.\nHowever, E5 noted that it would be less effective when analyzing only a few prompts and suggested incorporating an external corpus of suspicious keywords to improve its effectiveness.\nFinally, all experts agreed that our analysis framework provided valuable insights for model security enhancement.\nVisualization and Interactions.\nOverall, the experts agreed that the system views were well-designed, and the visual design and interaction were intuitive.\nThe Summary View was popular among the experts as it supported the analysis and comparison of jailbreak performance from both the perspectives of questions and templates.\nAll experts confirmed the helpfulness of Response View in exploring model responses and identifying unexpected results.\nE7 thought that component visualization required some learning costs but became easy to use and remember after she became familiar with visual encoding.\nWe also asked the experts for their opinions about the color scheme of the prompt components, and the experts confirmed that it was \u201cclear\u201d (E8) and \u201ceasy to distinguish\u201d (E10).\nThe Keyword View was observed to be frequently used by the experts during the exploration.\nWe noticed that they usually focused more on the keywords near the \u201cFull Compliance\u201d corner to identify effective keywords from successful attacks.\nSuggestions for Improvement.\nIn addition to the limitations mentioned above, we have also collected some suggestions for improvement. For Response View, E9 suggested adding some textual annotations (e.g., keywords) near the points or clusters to summarize the semantic information of the corresponding responses, which could help users identify the potential incorrect assessment results more efficiently.\nFor component analysis, E5 suggested that providing a textual or visual summary for the comparative analysis of the component perturbations could better help users identify effective components."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this section, we distill some design implications from expert interviews to inspire future research.\nWe also discuss the system\u2019s generalizability, limitations, and future work."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Design Implications",
            "text": "Toward a more comprehensive assessment of jailbreak results.\nThe experts appreciate the introduction of jailbreak assessment taxonomy [74  ###reference_b74###] to resolve ambiguities and identify harmful jailbreak results, as well as the application of the LLM-based assessment method to improve efficiency.\nThey also suggest extending them to include more assessment dimensions.\nFor instance, responses that offer elaborate and expert guidance on illegal activities may pose a greater risk than those providing generic and ambiguous advice.\nTherefore, assessing the helpfulness of the jailbreak results can help model practitioners prioritize identifying and preventing these harmful results.\nFuture research can explore broadening the spectrum of assessment dimensions (e.g., helpfulness) and improving the assessment accuracy of these dimensions (e.g., incorporating question-related knowledge bases) to mitigate the harm of the jailbreak results.\nImprove the learning-based jailbreak prompt construction.\nThe development of learning-based methodologies [78  ###reference_b78###, 13  ###reference_b13###] for the construction of jailbreak prompts represents a significant opportunity to enhance the efficiency of red-teaming LLMs.\nHowever, this endeavor faces challenges due to the intricate nature of the prompt design to guarantee effectiveness [15  ###reference_b15###].\nThe experts highlight that our analysis framework can inspire the research of learning-based methods to improve their effectiveness and generation diversity.\nFor example, the paired jailbreak prompts and their perturbation results (with stronger jailbreak performance) can be used to train generative models for rewriting jailbreak prompts, so that the models can easily capture the difference between their prompt components and learn how to effectively improve the prompt performance.\nFurthermore, the component analysis paves the way for the integration of expert knowledge into automatic jailbreak prompt generation.\nFor example, it allows the experts to specify the kernel of the prompts in the Scene Introduction or Subject Characteristic components to guide the generation of the following content (e.g., Task Operation or Output Specification).\nBalance between the objectives of safety and instruction-following.\nThe objectives of safety and instruction-following are usually competitive [66  ###reference_b66###], particularly in the context of mitigating jailbreak attacks.\nIt has been widely recognized that over-strengthening the model\u2019s security defenses on large jailbreak corpora will inevitably compromise the model\u2019s ability to follow user instructions, leading to \u201coverkill\u201d issues [54  ###reference_b54###].\nBalancing between these two objectives has become one of the most challenging problems for LLM training.\nThe experts point out that our component analysis provides a potential solution to construct a customized and condensed jailbreak dataset based on the vulnerabilities of LLMs rather than relying on large jailbreak corpora, thereby effectively addressing the model\u2019s major security weaknesses without sacrificing the instruction-following capabilities.\nLooking ahead, future works can further explore how to help model practitioners analyze and trade-off between these two objectives.\nSupport jailbreak performance comparisons on multiple models.\nWhile our analysis framework and visual system facilitate a systematic analysis of jailbreak attacks, the experts express interest in the comparative evaluation of such attacks across various models, which promises significant benefits across multiple application scenarios.\nFor example, it can help LLM vendors benchmark their models against those of competitors, identifying relative advantages and shortcomings.\nSimilarly, it can assist model practitioners in comparing different iterations of models to evaluate the improvements attributed to safety training.\nOur system can be extended with comparative visualizations [42  ###reference_b42###, 22  ###reference_b22###] to provide clear and insightful comparisons across models.\nFine-tune the jailbreak questions to bypass the safety measurements.\nConsistent with most previous research aimed at refining the jailbreak templates to enhance the attack performance, our system is primarily designed to evaluate the jailbreak prompts and analyze the characteristics of the jailbreak templates.\nRecent studies [76  ###reference_b76###, 30  ###reference_b30###] suggest that rewriting the jailbreak questions directly helps to circumvent the model\u2019s security mechanisms.\nThis could be achieved, for instance, by transforming the malicious questions into experimental or pedagogical questions, or by deconstructing the tasks into smaller, less harmful operation steps.\nTherefore, it becomes crucial to analyze the semantic characteristics of the jailbreak questions and to summarize their common strategies."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Generalizability",
            "text": "is designed to analyze jailbreak attacks, one of the most common prompt attacks against LLMs.\nWe demonstrate the system\u2019s effectiveness through a case study evaluating the jailbreak performance on GPT-3.5.\nThe system can be easily generalized to other language models (e.g., Llama 2 [62  ###reference_b62###] and ChatGLM [16  ###reference_b16###]) due to its model-agnostic design that centered around the prompt input and model responses.\nMoreover, the analysis workflow of \\name can potentially support other prompt attack scenarios, such as prompt injection [52  ###reference_b52###, 47  ###reference_b47###] and backdoor attacks [23  ###reference_b23###, 72  ###reference_b72###].\nFor prompt injection, the system can help users evaluate the performance of hijack attacks based on the LLM-based assessment method and explore the assessment results to verify or improve their accuracy.\nFor backdoor attacks, the system can help users identify suspicious backdoor triggers (e.g., sentences or keywords) through component and keyword analysis."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Limitations and Future Work",
            "text": "Incorporate more perturbation strategies for component analysis.\nCurrently, our analysis framework and visual system support three kinds of perturbation strategies (\\iedeletion, rephrasing, and switching) to help users understand the effect of prompt components on jailbreak performance.\nThey can be extended to support more strategies, such as inserting and crossover [74  ###reference_b74###].\nThe system can insert the recognized important components into other prompts to verify their effectiveness.\nThe crossover strategy can combine the strengths of two prompts to create more prompt variations.\nSupporting these strategies enables users to conduct a more comprehensive analysis of prompt components.\nExtend our analysis to more large language models.\nOur system has proven to be effective in supporting the security analysis of OpenAI\u2019s LLMs against jailbreak attacks.\nMoving forward, we plan to integrate a broader range of mainstream LLMs into our system, such as Llama 2 [62  ###reference_b62###] and ChatGLM [16  ###reference_b16###].\nOur goal is to evaluate the defense capability of these advanced models against jailbreak attacks and identify their potential weaknesses.\nWe will communicate our findings gained from the analysis with the vendors of these LLMs and provide them with valuable insights for enhancing the model safety mechanisms, ensuring a safer deployment of LLM technologies.\nExplore multi-modal jailbreak attacks.\nThe vulnerabilities of multi-modal large language models (MLLMs), such as LLava [33  ###reference_b33###] and GPT-4V [1  ###reference_b1###], have attracted increased attention [9  ###reference_b9###, 51  ###reference_b51###, 48  ###reference_b48###].\nMLLMs are more sensitive to jailbreak prompts with textual triggers, OCR textual triggers, and visual triggers, which presents greater safety risks compared to LLMs.\nOur work primarily investigates the textual vulnerabilities of LLMs without extending to the scenario of multi-modal jailbreak attacks.\nIn the future, we aim to bridge this gap by incorporating multi-modal analysis into our visual analysis framework, thereby enhancing the robustness of MLLMs against such threats."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present a novel LLM-assisted analysis framework coupled with a visual analysis system \\name to help model practitioners analyze the jailbreak attacks against LLMs.\nThe analysis framework provides a jailbreak result assessment method to evaluate jailbreak performance and supports an in-depth analysis of jailbreak prompt characteristics from component and keyword aspects.\nThe visual system allows users to explore the evaluation results, identify important prompt components and keywords, and verify their effectiveness.\nA case study, two technical evaluations, and expert interviews show the effectiveness of the analysis framework and visual system.\nBesides, we distill a set of design implications to inspire future research."
        }
    ],
    "url": "http://arxiv.org/html/2404.08793v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1",
            "1.2",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.2.1",
            "5.2.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.2",
            "3.2.1",
            "3.2.2",
            "4.5",
            "5.1"
        ]
    },
    "research_context": {
        "paper_id": "2404.08793v1",
        "paper_title": "\\name: Visual Analysis of Jailbreak Attacks Against Large Language Models",
        "research_background": "To accurately describe the paper's motivation, research problem, and relevant prior work, I would need the complete text of the Introduction and the Related Work sections of the paper. \n\nHowever, based on the title and typical scientific writing conventions, I can infer and structure a description for you:\n\n### Motivation:\nThe study is motivated by the increasing prevalence and sophistication of jailbreak attacks targeting large language models (LLMs). These attacks manipulate LLMs' behavior to bypass their content filters, often leading to the generation of inappropriate or harmful content. Understanding and mitigating these vulnerabilities is critical for both the safe deployment of LLMs and advancing natural language processing (NLP) research.\n\n### Research Problem:\nThe primary research problem addressed by this paper is the need for a detailed, visual analysis of how jailbreak attacks work and their impact on large language models. Existing approaches may lack the nuance or clarity needed to comprehensively understand these attacks, hindering effective countermeasures.\n\n### Relevant Prior Work:\n1. **Prompt Jailbreaking**: \n   Previous studies have explored various forms of prompt engineering to both enhance and subvert LLMs' outputs. This body of work provides foundational understanding regarding the manipulative techniques used in jailbreak attacks.\n\n2. **Visualization for Understanding NLP Models**: \n   There is a body of research devoted to visual tools and methodologies that help in elucidating the inner mechanics and decision-making processes of NLP models, aiding researchers and practitioners in diagnosing model behavior and performance.\n\nFor a precise and tailored description, please provide the relevant text from the Introduction and Related Work sections.",
        "methodology": "The proposed methodology for analyzing jailbreak attacks against large language models involves a multi-step framework. This framework is designed to handle and assess user-specified jailbreak questions and templates systematically. The key components and innovations of this methodology include: \n\n1. **Assessment of Jailbreak Results (R1):** \n   - The framework first combines each user-specified jailbreak question and template.\n   - The combined query is then used to obtain a response from the target model.\n   - An LLM-based approach is introduced to assess these model responses, evaluating their effectiveness in jailbreaking the target model.\n\n2. **Decomposition of Jailbreak Prompts:**\n   - The prompts are decomposed into their components and keywords to facilitate a detailed characteristic analysis.\n\n   - **Component Analysis (R2):**\n     - Basic prompt sentences are classified into different component types.\n     - These types are based on a taxonomy summarized from the jailbreak corpus (as detailed in Section 3.3).\n\n   - **Keyword Analysis (R3):**\n     - Important keywords from the prompts are identified.\n     - The framework then summarizes the performance of these keywords in the context of their associated jailbreak prompts.\n\n3. **Visual Interface (Section 5):**\n   - Based on the analysis framework, a visual interface is designed.\n   - This interface supports multi-level exploration of jailbreak prompts.\n\nBy integrating these components, the methodology provides a comprehensive approach for evaluating and understanding jailbreak attacks against large language models, supporting users in crafting more effective prompts through iterative analysis and refinement.",
        "main_experiment_and_results": "Regarding the main experiment setup and results for the paper \"Visual Analysis of Jailbreak Attacks Against Large Language Models\":\n\n### Main Experiment Setup:\n- **Datasets**: Not specifically mentioned in the given extract, but it can be inferred that the datasets would involve instances of jailbreak attacks against large language models.\n- **Baselines**: Not explicitly stated in the extract. Baselines typically would be other existing methods or tools for analyzing and mitigating jailbreak attacks on language models.\n- **Evaluation Metrics**: While the exact metrics are not detailed in the provided text, typical evaluations could include the effectiveness of detecting attacks, accuracy of analysis, and usability scores from expert feedback.\n\n### Main Experimental Results:\n- **Case Study**: A qualitative or exploratory analysis was likely performed to illustrate the capabilities of the framework.\n- **Technical Evaluations**: Quantitative assessments were possibly conducted, but the specific outcomes are not provided in the text.\n- **Expert Interviews**: Feedback from six experts was collected to gauge the usability of the visual analysis system, focusing on aspects such as user-friendliness, comprehensiveness, and practical utility.\n\nIn summary, the main experiments involved a combination of qualitative case study analysis, technical validations, and expert interviews aimed at demonstrating the analysis framework's effectiveness and the visual analysis system's usability. Specific results from these assessments are not included in the provided excerpt."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To analyze the contribution of different prompt components to the performance of jailbreak prompts and to understand the key components and strategies that can improve jailbreak success rates.",
            "experiment_process": "The experiment involved a detailed analysis of jailbreak prompt components using a component classification method and three perturbation strategies. The prompts were first split into sentences based on syntax and semantics. Then, GPT-4 was used to classify each sentence according to a component taxonomy and aggregate them into prompt components. For perturbation strategies, three methods were employed: deletion (removing components to see their impact), rephrasing (using LLM to polish sentences while maintaining semantics), and switching (changing the component type to others of similar semantic value). The effectiveness of these perturbations was evaluated based on the change in jailbreak performance.",
            "result_discussion": "The study found that deleting or switching certain components, especially the Subject Characteristic component, significantly reduced the jailbreak performance, indicating these components' critical role. Rephrasing often improved performance by bypassing safety mechanisms set by LLM vendors. The insights gained from these perturbations enabled the identification of effective strategies and weaknesses in the target model that could be further exploited or trained against for better security.",
            "ablation_id": "2404.08793v1.No1"
        },
        {
            "research_objective": "To evaluate the overall defense performance of GPT-3.5 and to identify the importance of various prompt characteristics and keywords in successful jailbreak attacks.",
            "experiment_process": "Expert E3 uploaded a jailbreak prompt corpus and selected a set of questions and templates for analysis. Each combination was evaluated three times due to the stochastic nature of the model responses. E3 used pie charts to visualize the performance of questions and templates, corrected outlier assessment results, and refined the assessment criteria. E3 then explored the Character Role Play templates for in-depth analysis, generating prompt variations using perturbation strategies to assess the importance of different components. Important keywords were identified via keyword visualizations and their impact on jailbreak performance was analyzed by refining templates with these keywords.",
            "result_discussion": "E3 discovered that nearly half of the jailbreak attacks were successful, with template choice impacting performance more than the specific questions asked. The Subject Characteristic components were found to be crucial for jailbreak performance. Keywords such as 'AIM' and 'DAN' were noted for their association with refusal by the model, whereas 'disregards' and 'controversial' were linked to higher compliance. Altering weak prompts with effective keywords significantly improved jailbreak success rates, suggesting that the model's safety training needed strengthening against these strategies.",
            "ablation_id": "2404.08793v1.No2"
        }
    ]
}