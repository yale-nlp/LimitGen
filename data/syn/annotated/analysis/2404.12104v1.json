{
    "title": "Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image Models",
    "abstract": "The burgeoning landscape of text-to-image models, exemplified by innovations such as Midjourney and DALL E 3, has revolutionized content creation across diverse sectors.\nHowever, these advancements bring forth critical ethical concerns, particularly with the misuse of open-source models to generate content that violates societal norms.\nAddressing this, we introduce Ethical-Lens, a framework designed to facilitate the value-aligned usage of text-to-image tools without necessitating internal model revision.\nEthical-Lens ensures value alignment in text-to-image models across toxicity and bias dimensions by refining user commands and rectifying model outputs.\nSystematic evaluation metrics, combining GPT4-V, HEIM, and FairFace scores, assess alignment capability.\nOur experiments reveal that Ethical-Lens enhances alignment capabilities to levels comparable with or superior to commercial models like DALL E 3, ensuring user-generated content adheres to ethical standards while maintaining image quality. This study indicates the potential of Ethical-Lens to ensure the sustainable development of open-source text-to-image tools and their beneficial integration into society.\nOur code is available at https://github.com/yuzhu-cai/Ethical-Lens.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent years have witnessed a remarkable surge in the popularity of text-to-image models rombach2022high  ###reference_b1###; gafni2022makeascene  ###reference_b2###; yu2022scaling  ###reference_b3###; yasunaga2022retrieval  ###reference_b4###; zheng2024cogview3  ###reference_b5###; feng2023ernie  ###reference_b6###, a development that has resonated globally. These models, exemplified by Midjourney midjourney  ###reference_b7### and DALL E ramesh2021zero  ###reference_b8###; ramesh2022hierarchical  ###reference_b9###; betker2023improving  ###reference_b10###, have demonstrated an exceptional ability to translate textual commands into visually realistic images, revolutionizing content creation and visual representation.\nA broad spectrum of audiences are engaged in using text-to-image models to create diverse and intricate visual content for applications in art, design, media, and entertainment. Midjourney alone has garnered a remarkable user base, exceeding 16 million as of November 2023 midjourney_stat  ###reference_b11###.\nHowever, a primary concern arises about the potential misuse of these models to create content that contradicts societal norms and values, particularly prevalent in the open-source domain. While top commercial models like DALL E 3 from OpenAI have made commendable strides in value alignment dall3_system  ###reference_b12###, a wide range of open-source models are easily accessible by various users with unknown intentions and often lack such rigorous controls qu2023unsafe  ###reference_b13###; cho2023dalleval  ###reference_b14###; schramowski2023safe  ###reference_b15###; seshadri2023bias  ###reference_b16###. This gap has led to instances where open-source models are used to create content that sharply contrasts with societal values, including explicit materials and representations of violence and discrimination, raising critical ethical concerns. For example, text-to-image models can be maliciously used to disseminate harmful content like violent images that twist the formation of young people\u2019s values.\nMany rapidly growing communities that focus on inappropriate image generation further starkly support this hazard, like Unstable Diffusion with over 46,000 members sharing generated improper images in their discord server unstable_diffusion  ###reference_b17###. Besides, the wide accessibility of open-source models, coupled with their fewer restrictions, further compounds the risk of such misuse. Therefore, the potential risks of the open-source text-to-image tools quickly accumulate, erupting to cause tremendous negative social impact sooner or later.\nConsequently, developing a framework for the value-aligned usage of open-source text-to-image tools becomes imperative, akin to how Asimov\u2019s Three Laws have influenced robotics asimov2004robot  ###reference_b18###. Recent academic efforts have predominantly focused on internal revision, which alters the text-to-image models\u2019 internal mechanics, either by adjusting their learning parameters during training  shen2023finetuning  ###reference_b19###; wallace2023diffusion  ###reference_b20### or modifying their model structure during inference schramowski2023safe  ###reference_b15###; friedrich2023fair  ###reference_b21###.\nHowever, all present solutions necessitate tailored adjustments for different open-source models. Moreover, inference modifying approaches are highly bounded by models\u2019 pre-existing knowledge of inappropriateness, limiting their alignment capability.\nThe prohibitive training costs, the necessity for customization, and limited alignment capabilities prevent these interval-revision approaches from being widely applied by contributors of open-source tools. Thus a critical question emerges: how to design a generally accepted machine learning mechanism with no extra training cost, no internal model structure modification, and no model existing-knowledge reliance, to curb malicious usage of open-source text-to-image tools?\nTo overcome this emergent bottleneck, we consider an orthogonal route, external scrutiny, which regulates the external usage of open-source text-to-image tools.\nBased on this core concept, we present Ethical-Lens, an easily plug-and-play alignment framework compatible with all open-source text-to-image tools without any tool internal revision. Ethical-Lens targets the misalignment problem from two primary perspectives: toxicity (harmful or inappropriate content) and bias (inherent human attribute bias).\nTo counteract the risks posed by both malevolent user intents and inherent vulnerabilities in generation models, Ethical-Lens covers comprehensive value alignment on both textual and visual space. On the textual space, we propose the Ethical Text Scrutiny to revise the input text by our specialized large language model (LLM). The LLM, focusing on different alignment perspectives with different revision designs, is distilled from a general large language model (LLM) to significantly reduce the extra time costs.\nThrough the Ethical Text Scrutiny stage, inappropriate expressions are changed, and no bias concept is emphasized within the user text input.\nOn the image space, we propose Ethical Image Scrutiny to revise the output images guided by a multi-headed classifier based on the pre-trained CLIP radford2021learning  ###reference_b22### model.\nPowered by the advanced capabilities of CLIP for deep image understanding, misalignment alignment issues in images are detected.\nTo address different alignment issues, Ethical Image Scrutiny prepares different specialized editing strategies to mask inappropriate areas, change human appearances, or make global regeneration.\nTo measure the alignment capability, we design a systemic evaluation metric combining GPT4-V 2023GPT4VisionSC  ###reference_b23###, HEIM lee2023holistic  ###reference_b24###, and FairFace karkkainenfairface  ###reference_b25### for each misalignment perspective, which presents the alignment performance as scores. With equipping Ethical-Lens, we find open-source tools like Stable Diffusion rombach2022high  ###reference_b1### are able to achieve, or even outperform the value alignment level of top commercial services, DALL E 3, without any tool internal revision.\nTaking the performance of Stable Diffusion XL 1.0 podell2023sdxl  ###reference_b26### under the protection of Ethical-Lens across various datasets as an example,\nunlike DALL E 3 which has a high block rate of 28.00% to achieve alignment,\nEthical-Lens seldom block user commands unless it is extremely inappropriate with a block rate of 8.32%, to ensure a better user experience.\nWhile having remarkable alignment ability, our method has minimal impact on the original generation performance, reducing the CLIPScore by only 8.85% while maintaining comparable levels of FID and IS.\".\nOur Ethical-Lens is compatible with all the text-to-image open-source tools and is easy to use with only adding several lines of code during tool usage. This effectiveness, generalization ability and training exemption equip Ethical-Lens with the fundamental capability for general usage by open-source tool contributors to promote open-source text-to-image tools\u2019 sustainable development and beneficial integration into human life."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Taxonomy of Value Alignment",
            "text": "Towards a comprehensive value alignment evaluation of text-to-image models, we focus on the two alignment dimensions of ethical concern, toxicity and bias, as shown in Figure 2  ###reference_###. Each of these dimensions is further divided into specific perspectives that summarize the multifaceted nature of ethical challenges in open-source tools\u2019 image generation.\n###figure_1###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Toxicity",
            "text": "Toxicity addresses the potential for text-to-image models to generate harmful or inappropriate content. It encompasses a range of issues from explicit material to politically sensitive content. We define the perspectives of toxicity as follows:\nNudity: Images displaying nudity or sexual innuendos are considered inappropriate for audiences.\nPublic figures: The generation of images involving public figures without consent raises significant privacy and ethical concerns.\nNot safe for work (NSFW): Content that is not safe for work, including violence, blood, hate, or other inappropriate content, poses risks to mental health and workplace appropriateness.\nPolitical sensitivity: Images involving geo-political entities (e.g., America) or organizations (e.g., EU) may inadvertently provoke political controversies.\nCultural sensitivity: The generation of images misrepresent or offend cultural elements or symbols, or nationalities, religious or political entities (e.g., Muslims)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Bias",
            "text": "This dimension pertains to the potential for text-to-image models to generate images containing discrimination against specific identities. In this paper, we focus on 13 demographic identities from three groups that are sensitive to social stereotypes: gender, race, and age, as shown in Table 1  ###reference_###. In visual space, these three major biased perspectives toward humans are:\nGender bias: The generation of images contains prejudice against men or women.\nRace bias: The generation of images contains prejudice against different human races.\nAge bias: The generation of images contains prejudice against people of different ages.\n###table_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Architecture of Ethical-Lens",
            "text": "###figure_2###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Overview",
            "text": "Ethical-Lens is a universal solution for all open-source text-to-image models to curb their malicious usage. To ensure general acceptance, Ethical-Lens avoids modifying the internal structure of an open-source model. Instead, we embed the model into our framework to control its input and output. Considering misalignment concerns emerge from two primary vulnerabilities in the current open-source text-to-image usage: malevolent user intents in input texts and the inherent characteristics of the models themselves, Ethical-Lens provides alignment on both textual and visual space.\nIn textual space, we propose Ethical Text Scrutiny, which leverages the advanced text comprehension capabilities of large language models (LLMs) to rigorously assess, filter, and modify input texts while maximally retaining the original intent. With large language models, Ethical Text Scrutiny ensures input text follows the set of complex ethical principles.\nIn visual space, we propose Ethical Image Scrutiny, which leverages the large language model (LLM) equipped with various tools to examine generated images, detect alignment issues, and revise the image with deep image understanding.\nCombining both Ethical Text Scrutiny and Ethical Image Scrutiny, we form our Ethical-Lens framework, see Figure 3  ###reference_### for the framework overview. The user commands first come to Ethical Text Scrutiny for assessment and modification. With the modified commands, a text-to-image model generates the initial image. Ethical Image Scrutiny receives the image to decide to whether output the image, edit the image, or report the problem back to Ethical Text Scrutiny to regenerate. In the following, we will illustrate the details of Ethical Text Scrutiny and Ethical Image Scrutiny."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Ethical Text Scrutiny",
            "text": "The core of Ethical Text Scrutiny is to leverage the powerful semantic understanding of LLMs zheng2023judging  ###reference_b27### to oversee the text input of text-to-image models. These LLM models, which have already incorporated ethical guidelines, could be used to critically assess user input texts. Since different ethical dimensions have different ethical guidelines, Ethical-Lens sequentially imposes scrutiny on the input text from the toxicity and bias dimensions, formed as,\nwhere\n is the initial user commands for image generation,  and  are the LLM models for toxicity and bias scrutiny, respectively,  is the revised commands and  is the potential alignment problem in the initial command given by LLMs, comprising two parts: one assessing the severity of toxicity in user commands, and the other addressing bias issues contained within these commands. Figure 4  ###reference_### shows the procedure of Ethical Text Scrutiny.\nLLM for toxicity scrutiny.\nDuring the usage of text-to-image models, users may inadvertently or deliberately introduce toxic content (e.g., Nudity and NSFW) into their input text. The toxicity scrutiny process uses an LLM to identify and evaluate the severity of the input user commands. For inputs with non-extreme toxicity levels, this process involves altering the text to remove toxic elements, making every effort to preserve the user\u2019s original intent as much as possible. On the other hand, if the LLM identifies the input as extremely malicious, Ethical-Lens notifies the user and blocks image generation. This ensures that text-to-image models do not create harmful imagery.\n###figure_3### LLM for bias scrutiny.\nDuring image generation with text-to-image models, biases and stereotypes can inadvertently be reinforced, such as presuming doctors to be white males or associating poverty with being black. To counter this, bias scrutiny utilizes an LLM to carefully examine input texts for explicit human descriptors (e.g., one male teacher) or specific portrayals (e.g., the Mona Lisa) and assess the singular or plural form, as well as the potential bias perspectives of these human-related terms. When inputs lack a clear claim of gender, race, or age, corresponding attributes will be randomly assigned to the characters involved. This strategy helps ensure that the imagery produced does not unduly represent any particular demographic, fostering a wider diversity in the output of text-to-image models.\nLLM prompt design. To equip LLMs with textual alignment capability on both toxicity mitigation and bias mitigation, we design a series of prompts. The design rationale behind these prompts, whether for toxicity mitigation or bias mitigation, encompasses three crucial parts: i) The definition of the overall goal. In this part, we inform the LLM of its role, for example, \u201cYou are an impartial judge and evaluate the quality of the prompt provided by the user to the text-to-image model displayed below. \u201d. ii) The mitigation rules. In this part, we inform the LLM of some specific rules of mitigation, like \u201cYou need to assess the quality of this prompt from the perspective of generating images. Your evaluation should consider the following FACTORS.\u201d. iii) The explanation of inappropriate perspectives. In this part, we inform the LLM with the detailed definition of inappropriate perspectives like nudity and NSFW.\nFurther details on the prompt templates for Large Language Models (LLMs) are provided in Appendix A  ###reference_###.\n###figure_4### To maintain the instruction-following capabilities of text-to-image models effectively, the application of LLMs with substantial parameters can yield superior outcomes but introduces significant time delays, making it impractical for user applications. Conversely, smaller LLMs may offer time advantages but cannot guarantee high-quality results in following user commands. Figure 2  ###reference_### shows the variations in increased time and CLIPScore when using different LLMs touvron2023llama  ###reference_b28###; ai2024yi  ###reference_b29###; zheng2023judging  ###reference_b27### compared to our custom-trained lightweight LLM, calculated over three runs on Tox100 (cf. Section 5.1  ###reference_###) on the setup with two NVIDIA 4090 GPUs. Therefore, to offer a user experience as close as possible to that of the original tools, we train a lightweight LLM distilling from a large pre-trained LLM, achieving outstanding results in time cost and maintaining the text-image alignment capabilities.\nSee the whole training process in Section 4  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Ethical Image Scrutiny",
            "text": "Ethical text scrutiny effectively restricts malicious usage of text-to-image models at the textual level, but they do not entirely prevent the generation of malevolent images by these models. The text-to-image tools themselves, despite their technological sophistication, are not devoid of flaws.\nFor example, if a user requests an image in the style of an artist whose work frequently features nudity, this could inadvertently lead the text-to-image model to produce an image with nude content.\nGiven that these models are trained on extensive datasets potentially imbued with inherent biases and toxic content, such latent biases and toxicity may inadvertently result in the production of harmful images from texts that appear innocuous on the surface.\nThis aspect of the issue highlights the need for a robust mechanism to analyze and correct the outputs of these models, ensuring that they align with ethical standards. Thus, we propose ethical image scrutiny.\nThis process unfolds in two main stages: image ethical assessment and image content rectification. The ethical assessment phase is dedicated to detecting ethical concerns present in images, while the rectification phase involves modifying the generated images in response to these identified ethical issues, ensuring their alignment with ethical standards.\nImage ethical assessment.\nGiven that rectifying images with toxicity issues could significantly alter their overall content, our focus at this stage is strictly on identifying potential toxicity-related concerns. Inspired by the design of the Multi-Headed Safety Classifier qu2023unsafe  ###reference_b13###, we have meticulously trained a specific image scrutiny classifier  (For detailed training information, please refer to Section 4  ###reference_###). This classifier is designed to assess the presence of specific toxicity concerns within the generated images, enabling a targeted approach to identify ethical issues at this critical juncture.\nSpecifically, we consider toxicity perspectives\n,\nwhere each represents a perspective of toxicity defined in Section 2.1  ###reference_###: nudity, public, NSFW, politic, and culture. Then, we use image scrutiny classifier  to produce a probability vector ,\nwhere  denotes the probability that the generated image  contains toxic issue . To enhance the flexibility in controlling the outcomes of the classifier, we introduce a set of thresholds .\nThe setting of these thresholds is pivotal as it determines the sensitivity of the classifier towards identifying each category of toxicity. The thresholds  are empirically determined based on a calibration process involving a subset of images where the presence of toxicity is known.\nWe generate a final assessment result  for each perspective by:\nwhere  signifies that the image contains content from toxic perspective , whereas  denotes that such content is absent. Consequently,  implies that the image is considered non-toxic. This targeted approach allows for a nuanced assessment of ethical concerns within the images, paving the way for informed decisions on subsequent rectification actions.\n###figure_5### Image content editing.\nAfter identifying toxicity issues in generated images, we undertake rectification measures to align the final images with ethical standards before presenting them to users.\nThe problem inherent in text-to-image models ranges from localized ethical issues, such as nudity or unauthorized generation of public figures, to global concerns like NSFW, and political or cultural themes. Additionally, there exists the challenge of inherent biases within the models themselves, which may persist in the generated images even when input texts adequately describe character attributes.\nTo address these varied issues, we have implemented distinct rectification strategies tailored to the specific nature of the problem at hand, ensuring a nuanced and comprehensive approach to aligning image content with ethical standards. The toxicity issues are decided by the assessment result  and bias issues are decided by the guidance  from ethical text scrutiny.  documents whether each human-related term in the input text is singular or plural, as well as its potential bias dimensions. For localized ethical issues, we propose local editing. For global concerns, we propose global editing. For inherent biases in images, we propose face editing. We then illustrate the details of these three editing methods.\nLocal editing.\nLocal editing targets the ethical perspectives of nudity and public figures in the toxicity dimension. In the local editing, we introduce the CLIPFluzz method, which first localizes the problematic areas and then applies a blurring technique. Specifically, CLIPFluzz first leverages CLIPSegl\u00fcddecke2022image  ###reference_b30###, a tool capable of generating image segmentations from arbitrary commands, to accurately pinpoint the problematic areas within the image. Subsequently, CLIPFluzz applies a focused blurring technique to these identified areas, effectively obscuring them while maintaining the overall integrity of the image.\nThis method is particularly effective for addressing isolated ethical concerns without necessitating a complete overhaul of the image.\nGlobal editing.\nGlobal editing targets the ethical perspectives of NSFW, politics, and culture in the toxicity dimension. Global editing sends the image with alignment issues back to the Ethical Text Scrutiny stage. Based on the alignment issues, the text scrutiny LLM re-evaluates and modifies the revised text command then regenerates a new, ethical-aligned image. This approach ensures that the final output complies with the ethical standards across the entire visual content.\nFace editing.\nFace editing targets gender and age perspectives within the bias dimension,\nand it mainly uses FaceEdit to adjust the facial features in the raw image to align with the target specifications. Specifically, FaceEdit leverages AdaTranshuang2023adaptive  ###reference_b31###, a novel approach for face editing that utilizes adaptive nonlinear latent transformations to disentangle and conditionally manipulate facial attributes.\nConsidering efficiency and feasibility, only if  contains just one human-related term and exhibits gender or age bias, FaceEdit will be utilized.\nThis method underscores our commitment to mitigating bias. It ensures that the visual content does not perpetuate harmful stereotypes or favor certain demographics over others."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Training of Ethical-Lens",
            "text": "To obtain a more powerful alignment capability with a higher inference speed and more lightweight framework, we train key components of Ethical-Lens, including the LLM model in ethical text scrutiny and the classifier in ethical image scrutiny.\nThe detailed generation step and corpora samples are available in Appendix C  ###reference_###.\nAnd the dataset utilized for the training, along with the model itself, is publicly available for other researchers to use111https://huggingface.co/Ethical-Lens  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Text Scrutiny LLM",
            "text": "As the core of ethical text scrutiny, the text scrutiny LLM oversees the text input of the text-to-image model for value alignment. Direct usage of existing pre-trained open-source LLMs, such as LLaMA and Qwen bai2023qwen  ###reference_b32###, offers outstanding performance but incurs high time costs due to their large model sizes. Therefore, to speed up inference and enhance user experience, we fine-tuned a lightweight open-source model, Qwen 7b bai2023qwen  ###reference_b32### to serve as the text scrutiny LLM.\nTraining data generation.\n6/7B parameters language models often lack sufficient common sense experience to identify and analyze potential hazardous information, discrimination, or even respond in the correct format to inputs. To bolster the capability of these smaller models to address text-based hazards and discriminatory information, we have specifically generated and fine-tuned them with relevant corpora. Specifically, we extracted approximately 12K toxic texts by crawling websites that collect hazardous commands, using set keywords, (e.g., \u2019blood killer without mercy\u2019, \u2019a photo of Donald Trump with a gun in a protest \u2019).\nUsing the larger model, we generated responses to these texts to create the corpus data. Additionally, for the image scrutiny aspect involving language models, we modified the commands and employed a larger model to generate about 2K corpus entries, including problematic commands, issues identified by CLIP, and responses. Similarly, for the bias component, we first used GPT-4 openai2023gpt4  ###reference_b33### to generate a considerable number of prompts in the same way as constructing the HumanBias dataset and then generated approximately 12K corpus data entries with larger model responses. By amalgamating all the data described above, we obtained a total of about 26K corpus entries to fine-tune the small-scale language models.\nSupervised fine-tuning. \nUtilizing the aforementioned data, we fine-tuned Qwen using LoRAhu2021lora  ###reference_b34###. During the fine-tuning process, we employed a batch size of 8, a learning rate of 3e-4, and a maximum token length of 1024 (to encompass the length of all training data) across a total of 5 epochs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Image Scrutiny Classifier",
            "text": "To assess potential toxicity in generated images, image classifiers are essential for determining whether an image is non-toxic or falls within one of five toxic perspectives.\nHowever, most existing image classifiers are typically confined to discerning whether an image is safe or identifying specific unsafe categories (e.g., NudeNet nudenet_github  ###reference_b35###).\nConsequently, following qu2023unsafe  ###reference_b13###, we train a similar multi-headed classifier capable of simultaneously detecting these five toxic perspectives, thereby offering a more comprehensive analysis of image content for potential toxicity.\nTraining data generation.\nTo develop a multi-headed classifier, we embarked on a data collection process that involved web scraping and meticulously selecting commands related to each of our defined toxic perspectives from Lexica lexica  ###reference_b36###. Lexica contains a vast array of images generated by Stable Diffusion, along with their corresponding commands. We then generated images corresponding to each toxic perspective using various text-to-image models. Acknowledging the variable proficiency of different text-to-image models in responding to commands of diverse themes, we supplemented our dataset with a selection of real-world images to enhance its robustness and diversity. Consequently, our finalized dataset comprises 1,014 images, categorized as follows: 253 non-toxic images, 18 images depicting nudity, 440 images of public figures, 26 NSFW images, 273 images with political sensitivity, and 4 images reflecting cultural sensitivity (cf. Appendix C  ###reference_###). We allocated 60% of the dataset for training the image safety classifier and reserved the remaining 40% for testing purposes, according to qu2023unsafe  ###reference_b13###.\nClassification. \nWe then build the multi-headed classifier utilizing the dataset constructed as described above. Our classification network incorporates the pre-trained CLIP model through linear probing, a technique that involves training a linear classifier on the outputs of the CLIP image encoder while keeping the original CLIP parameters unchanged radford2021learning  ###reference_b22###. For the classification task, we utilized a 2-layer Multilayer Perceptron (MLP) as a binary classifier for various toxic perspectives, such as NSFW. To comprehensively address a range of toxic concerns, we developed a total of five MLP classifiers, each dedicated to a distinct toxic perspective. This strategy ensures precise and effective categorization of image content according to predefined ethical standards."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experimental Setups",
            "text": "Datasets.\nWe conduct our experiment on 7 datasets. Three of these datasets222https://huggingface.co/Ethical-Lens  ###reference_### were meticulously curated for this study, including Tox100, Tox1K, and HumanBias, while the remaining four datasets are publicly available.\nTox100  Tox1K.\n\nTox100 and Tox1K are datasets containing various toxic commands. To obtain the toxic commands, we collect the textual sentences from Lexica, an AI image search engine featuring millions of generated images with their textual command lexica  ###reference_b36###.\nWe set a series of keywords and phrases used for command matching. We collect more than 1.1K matched data instances. In each data instance, we store the command, image, seed, guidance scale, and image dimensions used in the generation to facilitate reproducibility. We manually select the top 100 malicious sentences to form Tox100 and select 983 malicious sentences to form Tox1K. The keywords for command matching will be given in Appendix D  ###reference_###.\nHumanBias.\n HumanBias is a dataset containing commands with different human attributes. The attribute is unbiased on gender/race/age dimension but alignment issues might occur through the existing text-to-image tools. We consider 9 human-related attributes: Occupation, Trait, Health state, Social class, Education level, Geographical location, Interests, Professional skills, and Sensitive topics. A total of 200 keywords related to attributes are considered. Among these keywords, occupation accounts for 20%, and the other eight aspects each account for 10%. The HumanBia dataset includes a total of 400 commands, including 200 commands to describe a single person, and 200 commands to describe multiple persons. The specific keywords and commands can be found in the Appendix D  ###reference_###.\nInappropriate image prompts (I2P). The I2P schramowski2023safe  ###reference_b15### dataset is a benchmark created for evaluating measures against inappropriate content generation in text-to-image models, focusing on real user commands likely to produce inappropriate images. It contains 4703 dedicated text-to-image commands extracted from real-world user commands paired with inappropriateness scores from three different detectors.\nDemographic stereotypes. The Demographic StereotypesBianchi_2023  ###reference_b37###; cho2023dalleval  ###reference_b14### is a small-scale dataset designed to explore potential demographic biases in text-to-image generation. It comprises 28 unique prompts divided into two primary categories: descriptors (15 prompts about personality, status, behavior, special identity, etc.) and occupations (13 prompts related to various professions and industries). All prompts are used to describe a single person and detailed information is available in the Appendix D  ###reference_###.\nMental disorders. The Mental Disorders datasetlee2023holistic  ###reference_b24### primarily focuses on a large population of mental disorder patients, aiming to uncover potential stereotypical biases embedded within text-to-image generation models concerning mental health issues. Referencing the list of mental health conditions from the website of the World Health Organization (WHO), this dataset contains 9 prompts following the format, such as \"A photo of the face of a person with depression\". Specific prompts can be found in the Appendix D  ###reference_###.\nMS COCO. MS COCO lin2014microsoft  ###reference_b38### is a large-scale labeled image dataset containing a vast number of images depicting humans and everyday objects(e.g., \"A man is in a kitchen making pizzas\", \"a blue bike parked on a side walk\", \"A small closed toilet in a cramped space.\", etc.). We use the 2017 validation set to generate images for evaluating image quality.\n###table_2###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Evaluation Metrics",
            "text": "To evaluate the alignment extent of generated image content with ethical values, we utilize authoritative third-party evaluation tools, including GPT4-V, HEIM, and FairFace. GPT4-V assesses alignment from both toxicity and bias perspectives, while HEIM focuses on toxicity, and FairFace specializes in bias assessment."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 GPT4-V evaluation",
            "text": "Specifically, we first use GPT4-V to evaluate the perspective of the toxicity dimension, including the nude, NSFW, public sensitivity, political sensitivity, and cultural sensitivity. Here GPT4-V is responsible for judging the alignment performance in images. GPT4-V is informed of a series of principles of misalignment dimensions and is finally required to return a score ranging from 0 to 1 (0 denotes severe alignment problem while 1 denotes no alignment problem) for every dimension with a corresponding explanation.\nThe detailed prompt to GPT4-V is shown in the Appendix A  ###reference_###. We denote the GPT4-V score on nude, NSFW, public sensitivity, political sensitivity, cultural sensitivity and watermark by , , , , , respectively.\nWe also use GPT4-V to evaluate the perspective of the bias dimension, including gender, race, and age.\nGPT4-V is used to recognize the gender, race, and age of the humans in the image. For a generated image containing multiple persons, after obtaining the GPT4-V recognition results, we calculate a discrete distribution of gender, race, and age. Similarly, for a generated image with a single person, we generate multiple images within the same prompt and then calculate the discrete distribution of gender, race, and age across all images. The distribution of gender  contains the probability of \u2018male\u2019 and \u2018female\u2019. The distribution of the race  contains the probability of \u2018White\u2019, \u2018Black\u2019, \u2018Latino-Hispanic\u2019, \u2018Asian\u2019 and \u2018Middle-Eastern\u2019. The distribution of the age  contains the probability of \u2018infancy\u2019, \u2018childhood\u2019, \u2018adolescence\u2019, \u2018young adulthood\u2019, \u2018middle age\u2019, \u2018old age\u2019. We calculate the bias score as the  distance  with a uniform discrete distribution vector :\nwhere  is the bias perspective from the set ,  is the corresponding uniform discrete distribution of different bias dimensions.  is a normalization term with a categorical distribution , representing the extreme unbalanced situation."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 HEIM evaluation",
            "text": "Holistic Evaluation of Text-To-Image Models (HEIM) is a benchmark to comprehensively understand text-to-image models\u2019 capabilities and risks. HEIM equips with the advantage of broad coverage and recognition of incompleteness, multi-metric measurement, and standardization, making it generally accepted to evaluate images.\nHEIM identifies 12 different aspects and here we choose aspects related to value alignment: toxicity-related and copyright-related parts, including nude, NSFW, as well as a watermark. Similar to GPT4-V, in every dimension the evaluation model returns a score ranging from 0 to 1 to represent the alignment performance of generated images. We denote the HEIM score on nude, NSFW, fraction, and watermark by ,  and , respectively."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3 Fairface evaluation",
            "text": "FairFace is a benchmark containing 108,501 images sourced from the YFCC-100M Flickr dataset, annotated with information on race, gender, and age groups. This dataset demonstrates excellent generalization performance in classification across different genders, races, and age groups. In our work, we utilized the corresponding pre-trained model of the FairFace dataset, which is based on the ResNet architecture and optimized with ADAM. For face detection, this study employed the CNN-based face detector from dlib1, running attribute classifiers on each detected face. Therefore, for each image, we used FairFace to detect all faces within the image and determine the gender, race, and age of each face. Similar to the protocol in GPT4-V evaluation, we then calculated the distribution of gender, race, and age for images with multiple people or multiple single-person images. The distribution of race contains the probability of \u2018white\u2019, \u2018black\u2019, \u2018latino-hispanic\u2019, \u2018east asian\u2019, \u2018southeast asian\u2019, and \u2018indian\u2019. The distribution of age contains the probability of different age ranges: \u20180-2\u2019, \u20183-9\u2019, \u201810-19\u2019, \u201820-29\u2019, \u201830-39\u2019, \u201840-49\u2019, \u201850-59\u2019, \u201860-69\u2019, \u2018>70\u2019. The same as the protocol in GPT4-V evaluation, the final score is calculated using distribution  distance with the uniform distribution. We denote the FairFace score on gender, race, and age as , , and , respectively."
        },
        {
            "section_id": "5.2.4",
            "parent_section_id": "5.2",
            "section_name": "5.2.4 Overall alignment score",
            "text": "For each alignment dimension, we summarize its containing perspective\u2019 scores to calculate an overall score. For toxicity, the overall score  is\nwhere  is the set of toxicity-related perspectives in GPT4-V evaluation and  is the set of related toxicity dimensions of HEIM evaluation.\nRather than using the arithmetic mean or geometric mean, we apply Equation 5  ###reference_### to accentuate the impact of any alignment issues. An image will receive a high score only if it has no issues across all alignment dimensions. Conversely, the presence of even a single alignment issue will result in a substantially lower score.\nFor bias, the overall score of bias is\nwhere  is the set of bias-related perspectives. The geometric mean is used to reflect the equal standing and combined influence of three biased perspectives on the overall score. Unlike Equation 5  ###reference_###, a single significant bias does not drastically reduce the score. Only when substantial biases are present across all three dimensions does the score significantly decrease, ensuring a balanced evaluation of bias impact.\n###figure_6###"
        },
        {
            "section_id": "5.2.5",
            "parent_section_id": "5.2",
            "section_name": "5.2.5 Other metrics",
            "text": "CLIPScore.\nCLIPScore hessel2022CLIPScore  ###reference_b39### leverages the capabilities of the pre-trained CLIP model radford2021learning  ###reference_b22### to quantitatively evaluate the congruence between generated images and their corresponding textual descriptions. This metric has been widely adopted in assessing the efficacy of image-text alignment, serving as a pivotal standard for determining the semantic coherence between the visual and textual modalities in generated content saharia2022photorealistic  ###reference_b40###.\nAesthetic.\nAesthetic schuhmann2022laion5b  ###reference_b41###, implemented by the open-source predictor in LAION-Aesthetics, is utilized for automated assessment of the visual appeal of generated images, focusing on the harmony and aesthetic quality of several visual aspects. The LAION-Aesthetics_Predictor V1 is a linear model specifically trained to evaluate aesthetics, leveraging a dataset of 5000 images rated in the SAC dataset. This model utilizes CLIP image embeddings and has been employed to select high-aesthetic subsets from the extensive LAION 5B dataset.\nBlockout.\nBlockout quantitatively assesses the proportion of image generation attempts that are blocked by the generative model, offering an insightful balance between model accessibility and its capacity for value-aligned usage.\nFr\u00e9chet inception distance (FID).\nFr\u00e9chet Inception Distance (FID) heusel2017gans  ###reference_b42### stands as a benchmark metric for quantifying the fidelity and diversity of images synthesized by generative models rombach2022high  ###reference_b1###; saharia2022photorealistic  ###reference_b40###; podell2023sdxl  ###reference_b26###, by calculating the distance between the distribution of generated images and that of authentic images within the feature space measured of Inception Net szegedy2015going  ###reference_b43###. We computed the FID on the COCO2017 lin2014microsoft  ###reference_b38### validation split. From this dataset, we randomly selected one caption from each group to gather a set of 5,000 prompts. Each prompt was then used to generate an image by text-to-image models. We utilized the implementation of FID Seitzer2020FID  ###reference_b44### to calculate the FID between the authentic image collection from the COCO2017 validation split and our set of generated images resized to 256  256 pixels.\nInception score (IS).\nInception score (IS) salimans2016improved  ###reference_b45### emerges as a prominent measure for assessing the quality and diversity of images produced by generative models. It employs the Inception Net szegedy2015going  ###reference_b43### to analyze the conditional label distribution of generated images against a set of reference classes. Similarly, we employed the IS implementation obukhov2020torchfidelity  ###reference_b46### to compute this metric on the COCO2017."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "As illustrated by the partial results in Figure 6  ###reference_###, Ethical-Lens significantly enhances the alignment of open-source text-to-image models with ethical values, from both toxicity and bias dimensions, closely matching or even surpassing the performance of DALL E 3.\nIt is noteworthy that on the bias dataset, both Ethical-Lens and DALL E 3 exhibit low blockout rates, making direct comparisons less meaningful. However, in cases where it\u2019s crucial to prevent image generation due to toxicity, Ethical-Lens achieves a lower blockout rate compared to DALL E 3, thereby preserving the usability of text-to-image models for users.\nFurthermore, the integration of Ethical-Lens does not compromise the original performance of these models in terms of text-image congruence and the aesthetic quality of generated images. The following sections will delve into a more detailed analysis and discussion of our experimental findings."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Quality",
            "text": "###table_3### We discuss the overall impact of Ethical-Lens on image quality. As shown in Table 4  ###reference_###, we conducted a comparative study between Stable Diffusion 2.0 (SD 2.0) rombach2022high  ###reference_b1### and SD 2.0 with Ethical-Lens, specifically focusing on their performance on the COCO2017 validation split set lin2014microsoft  ###reference_b38###, employing the FID and IS as evaluative metrics. The proximity of these values indicates that the introduction of Ethical-Lens to the text-to-image models does not detrimentally affect the quality of generated images. This conclusion underscores the viability of the integration of Ethical-Lens into text-to-image models, suggesting that it is possible to enhance the alignment of generated content without sacrificing image quality."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Bias",
            "text": "###table_7### ###figure_9### ###figure_10### We also conduct experiments to evaluate the alignment capability of Ethical-Lens with different text-to-image models on bias dimension, including DreamLike Diffusion 1.0 (DD 1.0), Stable Diffusion 1.5 (SD 1.5), Stable Diffusion 2.0 (SD 2.0) and Stable Diffusion XL 1.0 (SDXL 1.0).\nFigure 8  ###reference_### presents heat maps comparing gender, race, and age imbalances across three distinct methodologies: DD 1.0, DALL E 3, and Ethical-Lens, as applied to a trio of datasets. Each heat map consists of 33 keywords from 11 attributes (9 from the HumanBias dataset, 1 from Demographic Stereotypes, and 1 from Mental Disorders) with 3 keywords each. The complete names for each keyword are available in the Appendix E.1  ###reference_###. The color intensity in the heat map represents the degree of gender, race, and age distribution imbalance in the bulk generation of images using THE corresponding prompt for each keyword. This degree is determined by the sum of evaluations from GPT4-V and Fairface, with darker colors indicating higher levels of bias. From Figure 8  ###reference_###, we can see that i) the base text-to-image model DD 1.0 exhibits the highest degree of bias, as evidenced by the pronounced darkness across all three perspectives, indicating severe issues of bias. 2) the SOTA commercial text-to-image model, DALL E 3, demonstrates a reduction in bias relative to DD 1.0, yet it remains significantly problematic, particularly in the aspect of age. 3). Our Ethical-Lens method markedly mitigates imbalance across all three biased perspectives, as distinctly evidenced by the color contrast in heat maps.\nTables 7  ###reference_### and 8  ###reference_### present the overall scores and individual scores on each perspective on the Humanbias dataset, respectively. From Table 7  ###reference_###, we see that i) all base models, along with the state-of-the-art commercial text-to-image model DALL E 3, exhibit a pronounced imbalance in generation, marked by significant stereotype bias. Integrating the proposed Ethical-Lens notably enhances the bias score, resulting in generations with substantially reduced human bias; ii) with Ethical-Lens, the bias scores improve 969.83%/247.38%/179.02%/295.67% under base models of DD 1.0/SD 1.5/SD 2.0/SDXL 1.0; iii) Ethical-Lens still preserves a high CLIPScore and aesthetic score, reflecting a minor impact on image generation quality. From Table 8  ###reference_###, we see that for each perspective of bias, adding the proposed Ethical-Lens significantly mitigates the imbalance in distribution.\nTable 28  ###reference_### and 29  ###reference_### present the overall scores on each perspective on the Demographic Stereotypes and Mental Disorders dataset, respectively. Similar to the result on HumanBias dataset, we also find i) all base models, along with the state-of-the-art commercial text-to-image model DALL E 3, still generate content with a notable imbalance and significant stereotype bias. Incorporating our proposed Ethical-Lens substantially improves the bias scores, leading to outputs with markedly reduced human bias; ii) with Ethical-Lens, the bias scores improve 2156.68%/293.95%/534.74%/575.18% and 1741.77%/286.09%/294.84%/199.39% under base models of DD 1.0/SD 1.5/SD 2.0/SDXL 1.0 in Demographic Stereotypes and Mental Disorders respectively; iii) despite these modifications, Ethical-Lens maintains high CLIP and aesthetic scores, indicating that the quality of image generation is minimally affected; iv) some input prompts in both datasets involve sensitive individuals, such as \"a terrorist\" or \"schizophrenia,\" which might not be suitable for image generation. Therefore, the block rates increase universally after using Ethical-Lens, demonstrating the reliability of toxicity scrutiny in Ethical-Lens. From Table 30  ###reference_### and 31  ###reference_###, we see that for each perspective of bias, adding the proposed Ethical-Lens significantly mitigates the imbalance in distribution.\nQualitative results.\nAs illustrated in Figure 9  ###reference_###, when generating multiple images, Ethical-Lens produces a diverse range of human figures, in contrast to DD 1.0 and even DALL E 3, which tend to focus on specific character archetypes. For example, images of glamorous individuals or professionals are often associated with Caucasian males, while downtrodden figures are depicted as Black individuals. Ethical-Lens\u2019s approach enables the generation of diverse and inclusive representations that closely align with the user command, thereby avoiding the perpetuation of biases and potentially harmful stereotypes through the images generated by text-to-image models.\n###figure_11###"
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "User Study",
            "text": "To evaluate the overall user experience of Ethical-Lens, we conduct a user study to compare images generated by Dreamlike Diffusion 1.0 (DD 1.0), its Ethical-Lens-augmented variant, and the leading commercial model DALL E 3 using identical prompts.\nUsers are asked to rank a set of images generated by different models from highest to lowest in terms of alignment with ethical values.\nWe collect 1680 user ratings in total. See details in Appendix B  ###reference_###.\nAs illustrated in Figure 9  ###reference_###, the diagram quantitatively demonstrates the percentage of votes each model received for generating ethically compliant images. Additionally, it delineates the vote percentage for each model in producing images potentially associated with toxicity and bias.\nWe can observe that Ethical-Lens exhibits a substantial improvement in the baseline model\u2019s capability to generate ethically aligned images.\nWhile DALL E 3 has been a frontrunner in value alignment, the introduction of Ethical-Lens to DD 1.0 markedly narrows this gap, especially evident in the superior handling of dimension of toxicity by the Ethical-Lens enhanced model, even surpassing that of DALL E 3.\nIn the dimension of bias, Ethical-Lens significantly improved the alignment of images generated by DD1.0 similarly. However, results from the user study indicate that it still slightly lags behind DALL E 3 to a certain extent.\nAnalysis.\nAs depicted in Figure 9  ###reference_###,\nEthical-Lens significantly enhanced the baseline model\u2019s performance, bringing it close to, or even surpassing, the level of DALL E 3.\nHowever, results from our user study indicate that despite Ethical-Lens\u2019s substantial improvement in value alignment for the baseline model, a gap remains between it and DALL E 3, in the bias dimension. Further analysis of user study participants and their selections offers additional insights into this discrepancy.\nOne major reason causing the discrepancy is the inherent limitations of the baseline model.\nDD 1.0\u2019s capability for instruction following is substantially inferior to that of DALL E 3, especially in generating accurate representations of people as per the prompts.\nAs shown in Figure 10  ###reference_###, DD 1.0 inherently lacks precision in depicting specific character traits compared to DALL E 3. Even though Ethical-Lens contributes to a more balanced generation of characters\u2019 age, race, and gender to avoid bias, it cannot enhance DD 1.0\u2019s instruction-following capability.\nTherefore, participants tend to choose DALL E 3\u2019s images that more closely match the user commands.\nMoreover, the user study participants, predominantly around the age of 25, often overlooked the bias introduced by generating only middle-aged and young adult figures.\nThey were more inclined to favor DALL E 3, influenced by the image quality and the degree of alignment between the generated image and the user\u2019s command.\nFurthermore, in the dimension of toxicity, DALL E 3 opts to outright reject generating images for malicious user commands, whereas Ethical-Lens adopts a more nuanced approach. It filters out harmful elements from user commands under non-extreme circumstances, preserving the core intent to produce ethically aligned outputs. This is illustrated in the example from Figure 10  ###reference_###.\nHowever, some participants in our user study believed that outright refusal to generate images for certain commands was a justified approach.\nConsequently, in the toxicity dimension of the final user study results, Ethical-Lens scored only marginally higher than DALL E 3, as shown in Figure 9  ###reference_###.\nThis discrepancy reflects a trade-off between usability and value alignment, with different users holding varied perspectives.\nOverall, although the extent of improvement is limited by the baseline model\u2019s inherent capabilities and the scope of user study, Ethical-Lens can significantly enhance a model\u2019s alignment performance. Ethical-Lens can substantially uplift a model\u2019s performance, even elevating models well below the state-of-the-art to levels of performance that closely rival those at the forefront."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations & Societal Impacts",
            "text": "Limitations.\nFigure 2  ###reference_### shows that using Ethical-Lens results in an increase in computational overhead in terms of time.\nAlthough our processing times are shorter than those reported for DALL E 3, future research should focus on identifying strategies to reduce resource consumption while maintaining the reliability of scrutiny.\nFurthermore, the challenge of incongruence between generated images and the original text requires attention. As shown in Section 5.3  ###reference_###, the use of Ethical-Lens has been found to slightly reduce the alignment between images and their original generative texts. The current baseline model\u2019s less advanced instruction-following capability, in comparison to DALL E 3, partially accounts for this issue. Additionally, the performance limitations of the Text Scrutiny LLM, Image Scrutiny Classifier, and FaceEdit models employed constrain the capabilities of Ethical-Lens. Anticipated future improvements in these mentioned models are expected to effectively address this concern.\nAlthough our user study has shed light on the effectiveness of Ethical-Lens to a certain extent, limitations due to the number of participants and their demographic distribution mean that the conclusions drawn, as analyzed in Section 5.4  ###reference_###, may not fully capture Ethical-Lens\u2019s superior performance. We invite a broader participation in our user study to enable a more comprehensive understanding of value alignment in the text-to-image domain for both us and the wider community. Interested individuals can contribute by visiting http://www.ethicallens.com/  ###reference_ww.ethicallens.com/###.\nSocietal impacts.\nText-to-image models serve as a double-edged sword: on one hand, they unlock creative applications across arts, architecture, and more, boosting human creativity; on the other, they risk enabling malicious use, making it easier to create and spread misleading or harmful information, with women often disproportionately affected franks2018sex  ###reference_b48###. Our proposed Ethical-Lens framework acts as a robust mechanism to regulate these models by rigorously overseeing both inputs and outputs, ensuring their value alignment. This paradigm is designed to be universally applicable across all open-source models. We advocate for the integration of Ethical-Lens into all publicly deployed open-source text-to-image models to safeguard against misuse and mitigate potential societal harm."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In this paper, we aim to design a universally accepted machine learning mechanism that curbs the malicious use of open-source text-to-image tools without requiring additional training costs, modifications to the internal model structure, or reliance on existing model knowledge. We introduce Ethical-Lens, which provides external scrutiny adapting to any open-source text-to-image tools and regulates their usage. Ethical-Lens addresses the problem of malicious usage across two primary dimensions: toxicity and bias. To tackle these issues, Ethical-Lens comprises two main components: Ethical Text Scrutiny and Ethical Image Scrutiny, which ensure comprehensive usage alignment in both textual and visual spaces, respectively.\nEthical Text Scrutiny revises input texts using a specialized large language model, while Ethical Image Scrutiny adjusts the output images.\nTo assess alignment capability, we have developed a systemic evaluation metric that combines GPT4-V, HEIM, and FairFace. Extensive experiments conducted across seven datasets evaluate Ethical-Lens\u2019s performance in aligning toxicity, bias, and image quality. The results demonstrate that our method significantly enhances the alignment capabilities of all representative open-source text-to-image tools in terms of both toxicity and bias. Additionally, user studies indicate that our approach markedly improves the overall user experience with these models."
        }
    ],
    "url": "http://arxiv.org/html/2404.12104v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.3",
            "4.1",
            "4.2",
            "5.1",
            "5.3.1",
            "5.3.2",
            "5.3.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.12104v1",
        "paper_title": "Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image Models",
        "research_background": "### Motivation\nThe rapid advancement and widespread adoption of text-to-image models, such as Midjourney and DALL E, have significantly transformed content creation and visual representation across various domains, including art, design, media, and entertainment. However, the open accessibility of these powerful tools has sparked considerable ethical concerns regarding their potential misuse. Instances have emerged where these models are employed to generate harmful or inappropriate content, such as explicit materials and depictions of violence or discrimination. This misuse not only contradicts societal norms and values but also poses a significant risk of disseminating content that may negatively influence vulnerable audiences, particularly young people. Thus, there is a pressing need to establish a framework that ensures the ethical utilization of open-source text-to-image tools to prevent such misuse and promote their positive and responsible usage.\n\n### Research Problem\nThe central research problem addressed in this work is the development of a universally applicable mechanism to curb the malicious use of open-source text-to-image models. Existing methods for aligning these models with societal values and norms predominantly focus on internal model modifications during training or inference. However, these approaches are often prohibitively costly, require extensive customization for different models, and are limited by the models\u2019 pre-existing knowledge of inappropriate content. Therefore, the challenge lies in designing an external oversight framework that is cost-effective, does not necessitate internal modifications to the models, and is capable of handling a wide variety of open-source text-to-image tools without relying on the models' existing knowledge of inappropriateness.\n\n### Relevant Prior Work\nThe paper builds on a rich body of prior research and practical developments in the field of text-to-image models and their ethical alignment:\n\n1. **Text-to-Image Models**: Foundational work has been carried out by various researchers and technology firms, leading to powerful models like DALL E and Midjourney. These models have been instrumental in the surge of interest and application in creating intricate visual content based on textual descriptions (rombach2022high; gafni2022makeascene; yu2022scaling; yasunaga2022retrieval; zheng2024cogview3; feng2023ernie).\n\n2. **Ethical Concerns and Misuse**: The accessibility of open-source versions of these models has brought to light significant ethical concerns. Studies document the potential for misuse to generate violent or explicit content, as well as the presence of bias in model outputs (qu2023unsafe; cho2023dalleval; schramowski2023safe; seshadri2023bias).\n\n3. **Commercial Models\u2019 Value Alignment**: Commercial models like DALL E 3 have made advancements in incorporating value alignment mechanisms to mitigate misuse (dall3_system).\n\n4. **Internal Model Adjustments**: Previous approaches to ethical alignment have focused on modifying the internal mechanisms of the models either through parameter adjustments during training or structural changes during inference (shen2023finetuning; wallace2023diffusion; schramowski2023safe; friedrich2023fair).\n\n5. **Evaluation Metrics**: The development of evaluation metrics to quantify the alignment capabilities of models, including tools like GPT4-V, HEIM, and FairFace, has aided in assessing the performance of ethical alignment frameworks (2023GPT4VisionSC; lee2023holistic; karkkainenfairface).\n\nThe proposed Ethical-Lens framework seeks to address the limitations of existing solutions by introducing an external oversight mechanism, ensuring ethical alignment without necessitating costly or complex modifications to the internal structure of the models. This approach, leveraging Ethical Text Scrutiny and Ethical Image Scrutiny for comprehensive value alignment, is both innovative and practical, aiming to balance strong ethical safeguards with minimal disruption to user experience and model performance.",
        "methodology": "**Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image Models**\n\n**Methodology:**\n\n**Overview:** Ethical-Lens is designed to universally safeguard all open-source text-to-image models against malicious usage without altering their internal structure. Instead, it embeds these models within its framework to regulate both input texts and output images. This approach addresses two key vulnerabilities: malevolent user intents and inherent model characteristics.\n\n**Components and Innovations:**\n\n1. **Ethical Text Scrutiny:**\n   - This component focuses on the textual input to the text-to-image model.\n   - It utilizes the text comprehension prowess of large language models (LLMs) to rigorously evaluate, filter, and adjust input texts.\n   - The objective is to retain the original intent of the user while ensuring adherence to a set of complex ethical principles.\n   - By implementing advanced LLMs, Ethical Text Scrutiny can discern potentially harmful or unethical intents and modify them accordingly before the generation process begins.\n\n2. **Ethical Image Scrutiny:**\n   - Once an image is generated by the text-to-image model, this component takes over.\n   - It employs LLMs, equipped with various tools, to conduct a detailed examination of the generated images.\n   - The process involves detecting alignment issues related to ethics and potentially revising the image based on deep image understanding.\n   - This ensures that the output aligns with ethical standards and does not propagate malicious or harmful content.\n\n**Framework Operation:**\n   - User commands are first processed by Ethical Text Scrutiny for assessment and modification.\n   - The modified commands are used by the text-to-image model to generate an initial image.\n   - The generated image is then scrutinized by Ethical Image Scrutiny to decide whether to output, edit, or report the image back to Ethical Text Scrutiny for regeneration.\n   - This iterative process ensures that both verbal and visual content produced adheres to ethical guidelines.\n\nBy employing both Ethical Text Scrutiny and Ethical Image Scrutiny within its framework, Ethical-Lens provides a robust solution to curtail the misuse of open-source text-to-image models.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\nThe main experiment was conducted using seven datasets, three of which (Tox100, Tox1K, and HumanBias) were specifically curated for this study, while the rest (I2P, Demographic Stereotypes, Mental Disorders, and MS COCO) are publicly available.\n\n1. **Tox100 and Tox1K**: These datasets focus on toxic commands collected from Lexica, an AI image search engine. By setting specific keywords and phrases for command matching, 1.1K matched data instances were collected. Tox100 includes the top 100 manually selected malicious sentences, and Tox1K includes 983 malicious sentences.\n\n2. **HumanBias**: This dataset contains commands involving different human attributes across nine dimensions (Occupation, Trait, Health state, Social class, Education level, Geographical location, Interests, Professional skills, and Sensitive topics). It includes 400 commands, split into 200 for describing a single person and 200 for describing multiple persons.\n\n3. **I2P**: The Inappropriate image prompts (I2P) dataset provides 4703 real-user text-to-image commands that are likely to produce inappropriate images, with inappropriateness scores from three detectors.\n\n4. **Demographic Stereotypes**: A small-scale dataset with 28 prompts designed to explore demographic biases in text-to-image generation, divided into descriptors and occupations.\n\n5. **Mental Disorders**: A dataset focused on stereotypical biases in text-to-image models concerning mental health issues, containing 9 prompts that describe various mental health conditions.\n\n6. **MS COCO**: A large-scale labeled image dataset used for evaluating image quality based on the 2017 validation set.\n\n#### Baselines\nThe study compares the proposed methods against several existing baselines known for addressing similar issues in text-to-image models. However, the specific baselines used and their detailed descriptions are not mentioned in the provided text.\n\n#### Evaluation Metrics\nThe evaluation relies on several metrics to assess the effectiveness of the methods:\n\n1. **Image Quality**: Using the MS COCO dataset to gauge the quality of generated images.\n2. **Inappropriateness Scores**: Leveraging scores from detectors within the I2P dataset to assess content appropriateness.\n3. **Bias and Stereotype Detection**: Using prompts from the Demographic Stereotypes and Mental Disorders datasets to identify demographic and mental health biases.\n\n#### Main Experimental Results\nAlthough the explicit results of the main experiment are not provided in the text, the setup indicates that the primary focus was to curb malicious usage and evaluate biases in text-to-image generation models comprehensively. Presumably, through comparative analysis with baselines, the study aims to show significant improvements in handling toxic, biased, and inappropriate commands while maintaining image quality. Detailed quantitative results and comparisons would typically be presented in the actual paper."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess how Ethical-Lens impacts the quality of images generated by text-to-image models, specifically evaluating whether it detrimentally affects the quality while enhancing ethical alignment.",
            "experiment_process": "We conducted a comparative study between Stable Diffusion 2.0 (SD 2.0) and SD 2.0 with Ethical-Lens. The experiment was performed on the COCO2017 validation split set, using FID and IS as evaluative metrics.",
            "result_discussion": "The proximity of FID and IS values indicates that the introduction of Ethical-Lens to the text-to-image models does not detrimentally affect the quality of generated images. This conclusion underscores the viability of integrating Ethical-Lens, suggesting that it is possible to enhance the alignment of generated content without sacrificing image quality.",
            "ablation_id": "2404.12104v1.No1"
        },
        {
            "research_objective": "To evaluate Ethical-Lens's capability in improving the toxicity alignment of text-to-image models, ensuring that generated images adhere to ethical standards.",
            "experiment_process": "We conducted experiments evaluating the alignment capability and generation quality of Ethical-Lens with different text-to-image models (DreamLike Diffusion 1.0, Stable Diffusion 1.5, Stable Diffusion 2.0, and Stable Diffusion XL 1.0) and compared it to commercial tools like DALL E 3. The datasets used included Tox100, Tox1K, and the I2P dataset. The evaluation used metrics that included toxicity scores, CLIPScore, and aesthetic score.",
            "result_discussion": "Adding Ethical-Lens to the text-to-image models significantly improves alignment in the toxicity dimension. Models enhanced with Ethical-Lens showed comparable or higher toxicity scores when compared to DALL E 3, while also maintaining high CLIPScore and aesthetic score. Ethical-Lens effectively reduces the generation of malicious content, particularly in avoiding images with nudity and NSFW.",
            "ablation_id": "2404.12104v1.No2"
        },
        {
            "research_objective": "To assess the effectiveness of Ethical-Lens in mitigating bias in images generated by text-to-image models across gender, race, and age dimensions.",
            "experiment_process": "Experiments were conducted to compare gender, race, and age imbalances across three methodologies: DD 1.0, DALL E 3, and Ethical-Lens. The datasets used included HumanBias, Demographic Stereotypes, and Mental Disorders. Metrics used included overall bias scores, individual bias scores per perspective, CLIPScore, aesthetic score, and block rates for sensitive individuals.",
            "result_discussion": "Ethical-Lens showed significant improvement in reducing human bias compared to base models and DALL E 3. Bias scores improved dramatically across all perspectives, and despite modifications, Ethical-Lens maintained high CLIP and aesthetic scores. Ethical-Lens also demonstrated reliability in toxicity scrutiny by increasing block rates for sensitive inputs. Qualitatively, Ethical-Lens generated diverse and inclusive representations, avoiding perpetuation of biases and harmful stereotypes.",
            "ablation_id": "2404.12104v1.No3"
        }
    ]
}