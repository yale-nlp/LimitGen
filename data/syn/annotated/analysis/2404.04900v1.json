{
    "title": "Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models",
    "abstract": "Large language models (LLMs) often struggle with strict memory, latency, and power demands.\nTo meet these demands, various forms of dynamic sparsity have been proposed that reduce compute on an input-by-input basis.\nThese methods improve over static methods by exploiting the variance across individual inputs, which has steadily grown with the exponential increase in training data.\nYet, the increasing depth within modern models, currently with hundreds of layers, has opened opportunities for dynamic layer sparsity, which skips the computation for entire layers.\nIn this work, we explore the practicality of layer sparsity by profiling residual connections and establish the relationship between model depth and layer sparsity.\nFor example, the residual blocks in the OPT-66B model have a median contribution of 5% to its output.\nWe then take advantage of this dynamic sparsity and propose Radial Networks, which perform token-level routing between layers guided by a trained router module.\nThese networks can be used in a post-training distillation from sequential networks or trained from scratch to co-learn the router and layer weights.\nThey enable scaling to larger model sizes by decoupling the number of layers from the dynamic depth of the network, and their design allows for layer reuse.\nBy varying the compute token by token, they reduce the overall resources needed for generating entire sequences.\nOverall, this leads to larger capacity networks with significantly lower compute and serving costs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Large language and vision models have recently achieved state-of-the-art performance across various tasks, yet due to their large computational requirements, they struggle with strict memory, latency, and power demands.\nAs these transformers grow larger, they create opportunities for dynamic layer sparsity, which can skip individual layers on an input-by-input basis, as shown in Figure 2  ###reference_###.\nFor instance, our residual block profiling in Section 4  ###reference_### suggests that modern state-of-the-art transformers likely have a median contribution around 1% to the output at each block, and that these contributions are dynamic, varying token by token.\nThis type of sparsity was impractical at smaller scales and with previous neural architectures.\nAt smaller scales, every layer contributes significantly to the computation for each input, and with previous architectures, e.g., convolutional neural networks (CNNs), models change their intermediate dimensions throughout their depth and skipping causes dimensional mismatches.\nThis work shows that the layer contributions vary among models and tasks, and often the earlier layers of the network contribute more than the later layers.\nThis indicates that early-exit methods, which dynamically prune the later layers in the network, often focus on the wrong set of layers.\nThis dynamic contribution can be exploited at the token-level if it can be predicted accurately and efficiently at runtime.\nThis work explores the opportunities for dynamic sparsity within the modern transformers by focusing on the OPT family of models [9  ###reference_b9###] for language and ViT models [3  ###reference_b3###] for vision.\nIt profiles the residual blocks to quantify the importance of each intermediate layer to its output and then highlights trends across model size and block types.\nThen, it inserts oracles at every layer to calculate various accuracy proxies and simulate greedy decisions on which layers to dynamically skip per token."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "###figure_2### Sparsity research with deep neural networks has a long history, and broadly can be categorized in terms of granularity, structure, and mode (static vs. dynamic) [4  ###reference_b4###].\nFigure 3  ###reference_### shows sparsity granularity, beginning with bits that construct parameter elements, elements that build blocks, and blocks that form layers.\nAs the unit becomes larger, it becomes more difficult to arbitrarily prune without accuracy loss yet easier to accelerate with modern hardware.\nFor instance, unstructured element sparsity in weights leads to high compression levels while maintaining model accuracy, yet it requires specializing sparse accelerators to translate compression into end-to-end speedup.\nIn addition, the sparsity mode can either be static or dynamic.\nStatic sparsity leads to more regular patterns that can be optimized by compilers and simpler architecture that do not need additional sparsity predictors, yet it must apply to all inputs together.\nIn contrast, dynamic sparsity can take advantage of input-dependent characteristics to increase model accuracy at higher levels of compression.\nThis work focuses on dynamic layer sparsity, which can take advantage of the recent explosion in model depth within language models."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dynamic Sparsity",
            "text": "###figure_3### Multiple prior works have proposed dynamic sparsity to accelerate DNNs across granularities.\nFor example, Channel Gating introduced a method for dynamic channel sparsity that reduced the compute of CNN workloads by up to  without significant accuracy loss [5  ###reference_b5###].\nPrecision Gating continued this line of research by applying dynamic sparsity at the bit level to reduce the required compute by up to  [10  ###reference_b10###].\nLater, DejaVu applied a similar approach within LLMs to induce dynamic sparsity on the channels within the FFN layer and across the heads of the attention layer [6  ###reference_b6###]."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Early Exit",
            "text": "In addition to dynamic sparsity along the network width, multiple prior works have explored sparsity in the depth dimension.\nFor instance, early-exit DNNs use dynamic sparsity along the depth dimension by allowing the computation to exit prematurely at fixed points within the network [7  ###reference_b7###, 1  ###reference_b1###].\nThis process must be trained end-to-end using a joint loss function that weights the contributions from each early-exit layer.\nHowever, this work shows that in many models, the earlier layers in the model often contribute more, and therefore early-exits are significantly more difficult to apply post-training."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Layer Sparsity",
            "text": "###figure_4### Transformer layers contain two residual blocks: attention (ATT) and feed-forward network (FFN) [8  ###reference_b8###].\nThese blocks each contain the main residual branch , which comprises multiple individual layers, and the identity branch , which bypasses the residual branch and simply returns its input.\nThey combine these branch outputs together to compute , so that during training the main branch only has to learn the function residual .\nThese blocks offer natural breakpoints within the model to profile and induce layer sparsity since they already provide skip-connections that have been trained along with the model.\nFigure 4  ###reference_### shows a lower-level view of these blocks within two transformer layers.\nIt shows that the main branch and skip connections are combined at an addition node before they are passed to the next block.\nThis structure enables easy profiling of the blocks by measuring the relative magnitudes into these additions.\nThis figure also shows the insertion of oracles that can switch on and off the main branch using various accuracy proxies, such as the residual ratio as defined in Section 4.1  ###reference_###.\nWhen they are switched on, the block operates normally combining the skip and residual branches, and when switched off, only the skip connection is active.\nThis work focuses on the opportunities for dynamic layer sparsity and simulates layer skipping by allowing these oracles to have access to future information."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Profiling",
            "text": "The primary proxy used by these oracles is the residual ratio, which captures the relative importance of the main and skip branches.\nThis section uses this ratio to analyze the layer sparsity within OPT and ViT models with examples taken from WikiText-2 and COCO.\nThe WikiText-2 examples are packed together to avoid the use of padding to simulate batch-size one inference.\nThis batch-size one setting is very common in practice and avoids many complications with dynamic layer sparsity that arise when using batches of examples."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Residual Ratio",
            "text": "To profile these opportunities for dynamic sparsity, this section defines the residual ratio  as:\nThis simple quantity captures the contribution of the residual branch, and acts as an efficient post-training proxy for more expensive metrics, such as empirical layer sensitivities.\nFor example, a block with a 2%\nresidual ratio indicates the main branch provides a 2% average contribution at the output, although there can be large element-wise variance.\nTherefore, skipping blocks with ratios this small should have little overall effect on the output of the network."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Dynamic Depth",
            "text": "Dynamic layer sparsity leads to dynamic depth networks that adjust their depth based on their model inputs.\nFigure 9  ###reference_### shows the residual ratio across the layers of an OPT-13B model (all models shown in Appendix A  ###reference_###).\nAll values shown are mean residual ratios taken across tokens from Wikitext-2 data using a sequence length of 256.\nThe ratio variance is highlighted in lighter colors centered around the mean.\nIt demonstrates that the earlier residual blocks contribute more compared to the later layers, except for the first few layers.\nIn addition, there is significant variance across tokens across layers suggesting the opportunity to apply dynamic layer sparsity to only the tokens with lower ratios.\nThis figure additionally shows the dynamic depth caused by this layer sparsity.\nIt assumes oracles that threshold the residual ratio at each block and skip the residual branch if it falls below this threshold.\nSince computing the ratio requires running the residual branch, this is only used for profiling and simulation purposes.\nEach data point represents an inference of a single token using a ratio threshold of 5% is used.\nThe figure confirms a spread within the network depth, where most tokens only need between 40 and 70 blocks, instead of the full network at 80 blocks."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Routing Traces",
            "text": "For more detailed analysis, Figure 7  ###reference_### shows the routing for the OPT-13B model across a batch of WikiText-2 examples.\nIt reveals how the lower residual ratios in Figure 9  ###reference_### lead to a significant number of skipped layers in the beginning of the model.\nThis again motivates the use of dynamic layer sparsity over early-exit models, since early exit can only skip later layers, which contribute the most to the network."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Vision",
            "text": "###figure_8### This analysis so far has focused on large language models, since they are currently  to  larger than large vision models.\nYet, recent vision transformers have been proposed with tens of billions of parameters [2  ###reference_b2###].\nThese weights are not yet released, yet the trends between the smaller language and vision models can still be aligned at smaller scales to suggest the behavior of large vision models with billions or trillions of parameters.\nFigure 8  ###reference_### shows a comparison for the largest released ViT model, which contains 632M parameters across 24 layers.\nIt shows that vision transformers at this size have comparable residual ratios to the similarly sized OPT-350M.\nIn addition, Appendix A  ###reference_### lists smaller ViT versions and shows a similar trend between model size and residual ratio, suggesting that as vision transformers increase in size they will benefit from the same layer sparsity opportunities as the OPT models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Radial Networks",
            "text": "Given the high degree of dynamic layer sparsity, we propose a new neural architecture that natively supports arbitrary routing between layers.\nAs shown in Fig 1  ###reference_###, each token enters network at step  and then is routed dynamically to the next layer at later steps .\nThis process allows for dynamic computation given the variable number of layers included in each token path.\nAs with standard transformers, the network continues auto-regressively until an end-of-sequence token is produced."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Router",
            "text": "The router is the central component with the responsibility of directing each embedding between layers at each time step.\nBuilding on the success of mixture-of-expert models, we learn this router with a small multi-layer perceptron (MLP) model.\nBeginning from layer , it maps from intermediate embeddings  to output router logits  for each layer.\nThese logits are then passed into a softmax function to produce probabilities .\nThe maximum probability layer is then chosen as the next layer in the forward pass.\nThe iterations stop when the model chooses the output layer, or a set maximum number of layers are seen, which forces the output layer.\nThis maximum number of layers is a hyper-parameter that limits the worst-case dynamic depth in the network."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Unified Cache",
            "text": "In standard sequential transformers, each layer activates for each token, and the attention mechanism references the key-value cache of previous tokens.\nHowever, given the dynamic routing of radial networks, many of the layers are not activated for different tokens, and the key-value cache is sparse.\nTo solve this, we instead use shift the cache from the layers to a shared global cache that stores all of the previous key-value pairs.\nEach embedding then attends to the cached pairs of previous iterations and previous tokens.\nTo distinguish between the cached values for the current token and previous, we use standard positional embeddings.\nThese embeddings are used to encode the give the relative position of tokens within the input sequence.\nA common approach is to use sinusoidal functions; for position  and dimension , the positional embedding  can be defined as follows:\nwhere  is the dimensionality of the embeddings."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In the past, dynamic layer sparsity has not been practical due to small model sizes and incompatible neural architectures, which caused large contributions from each layer and varying internal dimensions.\nFor these reasons, it has only been possible with techniques like early-exit, which requires expensive specialized training.\nYet, as language models grow in size, each layer contributes less to output, creating opportunities for dynamic layer sparsity.\nFollowing the trends in Figure 6  ###reference_###, modern language models with over one trillion parameters likely have median residual ratios less than 1%.\nAnd in the future, as vision and multi-modal models catch up to language models, their residual ratios should follow similar scaling trends.\nTo take advantage of this dynamic sparsity, we propose Radial Networks, which perform token-level routing between layers guided by a trained router module.\nThese networks can be used in a post-training distillation from sequential networks or trained from scratch to co-learn the router and layer weights.\nThey enable scaling to larger model sizes by decoupling the number of layers from the dynamic depth of the network, and their design allows for layer reuse.\nBy varying the compute token by token, they reduce the overall resources needed for generating entire sequences.\nOverall, this leads to larger capacity networks with significantly lower compute and serving costs."
        }
    ],
    "url": "http://arxiv.org/html/2404.04900v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "5",
            "5.1",
            "5.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.04900v1",
        "paper_title": "Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models",
        "research_background": "### Motivation:\nThe emergence of large language and vision models has led to significant advancements in achieving state-of-the-art performance across various tasks. However, as these transformer models become larger, their substantial computational requirements pose challenges for memory, latency, and power efficiency. Addressing these challenges necessitates exploring strategies for dynamic layer sparsity, which involve selectively skipping individual layers based on the specific input, thereby aiming to optimize computational efficiency.\n\n### Research Problem:\nThe key research problem addressed in this paper is the optimization of computational resources in large transformer models through dynamic layer routing. The contribution of each layer to the model's final output can be minimal and varies depending on the token and task. Previous models and architectures were not suitable for dynamic sparsity due to significant contributions at smaller scales and the risk of dimensional mismatches when skipping layers. The paper's goal is to explore and quantify the dynamic contributions of layers in modern transformers and develop methods to accurately and efficiently predict and exploit token-level sparsity.\n\n### Relevant Prior Work:\n1. **Dynamic Layer Sparsity**: Previous attempts at dynamic layer sparsity were not feasible for smaller neural network architectures or for architectures like CNNs, as they changed dimensions through depth, leading to dimensional mismatches if layers were skipped.\n2. **Early-Exit Methods**: Existing early-exit methods that aim to prune later layers assume that earlier layers are more crucial, which the paper argues to be inaccurate. The paper posits that contribution varies dynamically among layers and tokens.\n3. **OPT Family of Models**: The paper specifically explores the OPT family of models [reference ###b9###] for language tasks and ViT models [reference ###b3###] for vision tasks. Understanding the intricacies of these models serves as the foundation for the proposed dynamic layer routing strategies.\n\nBy focusing on the micro-analysis of residual blocks and evaluating various models for different tasks, the study aims to quantify the intermediate layer importance and lay the groundwork for improved accuracy and efficiency through dynamic layer routing.",
        "methodology": "The proposed methodology introduces a novel neural architecture designed to natively support arbitrary routing between layers, accommodating the high degree of dynamic layer sparsity. Here are the key components and innovations of the methodology:\n\n1. **Dynamic Routing Between Layers**:\n    - Tokens are not constrained to follow a fixed sequence of layers; instead, they are dynamically routed through different layers based on certain criteria.\n    - This routing allows for more flexible and potentially more efficient computational paths for each token.\n\n2. **Variable Token Path Lengths**:\n    - Each token in the input can travel through a variable number of layers rather than a fixed, predetermined number.\n    - This dynamic computation means that different tokens might engage different numbers of layers based on their requirements, optimizing the use of computational resources.\n\n3. **Auto-Regressive Processing**:\n    - Similar to standard transformers, the network processes tokens in an auto-regressive manner. This means it predicts the next token based on the previously generated tokens until it determines the end-of-sequence token.\n\n4. **End-of-Sequence Token**:\n    - The process continues until the network produces an end-of-sequence token, marking the conclusion of the sequence generation.\n\nBy supporting this level of dynamic layer routing, the proposed Radial Networks architecture aims to enhance the performance and efficiency of large language models. This innovative approach leverages variable layer engagement, ensuring that different tokens may traverse different computational paths within the model as needed.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n- **WikiText-2**: Used to analyze the dynamic layer routing within the OPT model.\n- **COCO**: Used alongside WikiText-2 examples to analyze the ViT model's layer sparsity.\n\n**Models:**\n- **OPT**: Evaluated using WikiText-2 to understand the behavior of dynamic layer routing.\n- **ViT (Vision Transformer)**: Evaluated using the COCO dataset in conjunction with WikiText-2 examples.\n\n**Evaluation Method:**\n- The main proxy utilized by the oracles to measure layer sparsity is the **residual ratio**, which is used as an indicator of the relative importance between the main computation branch and the skip branch within the network's architecture.\n\n**Experimental Conditions:**\n- **Batch-size one inference**: All WikiText-2 examples are packed together to mimic a batch-size one inference scenario. This setting is used to simulate a common practical scenario and to eliminate the complications associated with the dynamic layer sparsity that can occur with larger batch sizes.\n\n### Main Experimental Results\n\nThe experiments in this section predominantly focus on using the residual ratio to assess and analyze the dynamic layer sparsity within the models. The findings include observations on how dynamically routing layers based on the residual ratio affects model performance and efficiency:\n\n1. **Residual Ratio Analysis**: \n   - The residual ratio is used effectively to determine which layers can be dynamically skipped without significantly impacting model performance.\n   - For both OPT and ViT models, certain layers showed higher residual ratios, indicating a higher propensity for skipping during inference while maintaining performance.\n\n2. **Layer Sparsity Patterns**:\n   - Analysis of layer sparsity via the residual ratio indicated that both the OPT and ViT models could maintain high performance with a significant portion of layers being conditionally skipped based on the residual ratio.\n   - This dynamic routing resulted in more efficient model executions while preserving the high-level efficacy of the models.\n\nIn summary, the primary experiments demonstrate the effectiveness of using the residual ratio as a measure for dynamic layer routing, showing potential benefits in terms of computational efficiency without sacrificing model performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Explore the practical advantages and behavior of dynamic layer sparsity by profiling residual connections in large language models (LLMs)",
            "experiment_process": "The process involves defining the residual ratio as a proxy measure to capture the contribution of the residual branches. This ratio is utilized to profile the residual contribution across various models within the OPT family, ranging from OPT-125M to OPT-66B. Each data point represents an individual token during model generation, with experiments conducted using the Wikitext-2 dataset at a sequence length of 256.",
            "result_discussion": "Findings indicate that as model size increases, the distribution of residual ratios skews left, showing more opportunities for layer sparsity. For instance, the median residual ratio decreases from 20% in OPT-125M to 5.9% in OPT-66B. Insights reveal that residual ratio trends are linked to the overall number of parameters, not just the number of layers, substantiating that larger models will benefit from dynamic layer sparsity.",
            "ablation_id": "2404.04900v1.No1"
        },
        {
            "research_objective": "Demonstrate the dynamic depth behavior stemming from the application of dynamic layer sparsity based on input tokens.",
            "experiment_process": "By analyzing the residual ratio across all layers of an OPT-13B model, mean residual ratios are calculated across tokens fetched from Wikitext-2 data (sequence length 256). Variance in these ratios is visually displayed to highlight dynamic depth. Hypothetical oracles are employed to simulate dynamic depth, using a residual ratio threshold set at 5%, profiling how many layers each token actually requires during inference.",
            "result_discussion": "Results highlight that tokens typically utilize between 40 and 70 blocks rather than being processed across the full 80-layer network. Token-based variance suggests applying dynamic layer sparsity selectively. This reduces computational demand as most tokens necessitate fewer layers for processing, confirming efficient dynamic depth utilization.",
            "ablation_id": "2404.04900v1.No2"
        },
        {
            "research_objective": "Compare the residual ratios and potential for layer sparsity across different types of large models, specifically large language models and vision transformers.",
            "experiment_process": "The analysis includes large LLMs like OPT models and large vision models such as Vision Transformers (ViT). Residual ratios are compared across models like the ViT-632M (24 layers) and smaller ViT versions, checking if trends observed in LLMs like OPT-350M apply to vision transformers.",
            "result_discussion": "The study shows that the residual ratios of vision transformers are comparable to similarly sized OPT models. The pattern of decreasing residual ratios with increasing model size is observed, suggesting that larger vision models with billions or trillions of parameters will also benefit from dynamic layer sparsity like LLMs.",
            "ablation_id": "2404.04900v1.No3"
        }
    ]
}