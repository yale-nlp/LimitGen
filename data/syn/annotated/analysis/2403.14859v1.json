{
    "title": "Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models",
    "abstract": "Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood (LL) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) LL-based performance is still inferior to human performance; (iii) instruction-tuned models have worse LL-based performance than base models. In Experiment 2, we show that LL scores across models are modulated by context in the expected way, showing high performance on three metrics of context-sensitive plausibility and providing a direct match to explicit human plausibility judgments. Overall, LL estimates remain a more reliable measure of plausibility in LLMs than direct prompting.111Our results and code are available at https://github.com/carina-kauf/llm-plaus-prob.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The impressive empirical successes of large language models (LLMs) on many diverse (language) tasks (e.g., Devlin et al., 2019  ###reference_b14###; Liu et al., 2019  ###reference_b37###; Brown et al., 2020  ###reference_b9###; Achiam et al., 2023  ###reference_b1###; Bubeck et al., 2023  ###reference_b10###; Guo et al., 2023  ###reference_b21###) has fueled an explosive increase in their popularity. As LLMs are becoming more and more integrated in people\u2019s everyday lives, it is critical to provide reliable assessments of their capabilities.\nAn important domain to test is LLMs\u2019 general world knowledge. Language training data contains vast amounts of information about the world, including both factual knowledge explicitly stated in the input and distributional knowledge, inferrable via text co-occurrence patterns (Elazar et al., 2022  ###reference_b16###; Kang and Choi, 2023  ###reference_b29###). Leveraging world knowledge is important both for specific NLP tasks (e.g., information retrieval) and for general success of a language model during interactions with a user (e.g., establishing common ground).\nWe focus on one particular way to assess general world knowledge: estimates of sentence plausibility. Plausible sentences conform with world knowledge whereas implausible sentences violate it; thus, the ability to distinguish plausible and implausible sentences is an indicator of underlying world knowledge capabilities.\nTraditionally, NLP researchers evaluated the knowledge that LLMs distill into their weights through a combination of log likelihood comparisons on minimal sentence pairs Futrell et al. (2019  ###reference_b18###); Warstadt et al. (2020  ###reference_b57###); Hu et al. (2020  ###reference_b24###); Aina and Linzen (2021  ###reference_b2###); Pedinotti et al. (2021  ###reference_b48###); Sinha et al. (2022  ###reference_b54###); Hu et al. (2024  ###reference_b26###); Michaelov et al. (2023  ###reference_b40###); Misra et al. (2024  ###reference_b41###), probing the model\u2019s representations of a stimulus Hewitt and Manning (2019  ###reference_b22###); Kim et al. (2019  ###reference_b32###); Eisape et al. (2022  ###reference_b15###); M\u00fcller-Eberstein et al. (2022  ###reference_b45###); Kauf et al. (2023  ###reference_b31###), adversarial datasets McCoy et al. (2019  ###reference_b38###); Kassner and Sch\u00fctze (2020  ###reference_b30###), or causal interventions Geiger et al. (2020  ###reference_b19###), among others. Given the closeness to the unsupervised pretraining regime, minimal sentence pair comparisons of likelihood measures, in particular, have been widely adopted.\nMore recently, however, the focus of NLP researchers has shifted towards LLMs that have been fine-tuned to follow instructions (Chung et al., 2022  ###reference_b13###; Touvron et al., 2023  ###reference_b55###; Almazrouei et al., 2023  ###reference_b3###; Jiang et al., 2023  ###reference_b27###), as instruction tuning improves the alignment of the models with user intent and leads to better generalization to unseen tasks (Ouyang et al., 2022  ###reference_b47###). Because instruction-tuned models are designed to interact directly with a user through LLM-directed queries/prompts, natural language prompting has emerged as a way to directly query LLMs for the knowledge they encode (e.g., Li et al., 2022  ###reference_b36###; Blevins et al., 2023  ###reference_b8###). Critically, as access to log probabilities for newer models becomes restricted, it is important to understand what knowledge can be accessed, and what knowledge is inaccessible to the experimenter if prompting is the only way to interact with LLMs.\nThe link between sentence plausibility and sentence probability is indirect: raw log probabilities have been shown to reflect a number of factors that might not be relevant for a given task, including low-level properties of the stimulus such as sentence length, word frequency Kauf et al. (2023  ###reference_b31###), and the number of surface forms that refer to the same concept Holtzman et al. (2021  ###reference_b23###). Thus, direct prompting approaches might provide a more direct estimate of plausibility by filtering out influences of those additional factors. However, initial direct comparisons of log likelihood and prompting measures on different linguistic/semantic knowledge datasets has revealed that prompting may systematically underestimate the model\u2019s internal knowledge by requiring the models not only to solve the task, but also to correctly interpret the prompt and to translate their answer into the desired output format Hu and Levy (2023  ###reference_b25###); Hu et al. (2024  ###reference_b26###).\nIn this paper, we test LLMs\u2019 knowledge of plausibility in single-sentence (Experiment 1) and contextualized scenarios (Experiment 2). Our findings include:\nLog likelihood (LL) scores, while imperfect, are a more dependable measure of plausibility than natural language prompting evaluations.\nInstruction-tuning often alters an LLM\u2019s log-likelihood scores in such a way that they become less consistent with human plausibility judgments relative to base model versions.\nLL scores can effectively model the contextual plausibility of events and replicate key patterns of human plausibility-judgment behaviors. Nevertheless, the LLMs\u2019 ability to detect an implausibility within a target sentence locally does not reliably affect their evaluation of the full sentence."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Evaluating single-sentence plausibility in LLMs.    In Experiment 1, we evaluate plausibility estimates for single sentences describing common events (Table 1  ###reference_###). Earlier work in NLP aimed at modeling event-based semantic plausibility via distributional models of thematic fit: verbs and arguments were often considered in isolation, and the goal for the models was to estimate a continuous score expressing to what extent an argument noun (e.g., ball) was fitting a given semantic role of a verb (e.g., the patient role of to throw) (Baroni and Lenci, 2010  ###reference_b5###; Sayeed et al., 2016  ###reference_b52###; Santus et al., 2017  ###reference_b51###). In a more natural evaluation setting, researchers used sentence pairs derived from psycholinguistic experiments that differed only for one argument and displayed different degrees of plausibility (e.g., The mechanic was checking the brakes vs. The journalist was checking the brakes, from Bicknell et al., 2010  ###reference_b7###): in this case, a distributional model had to dynamically \u201ccompose\u201d the plausibility of the two argument roles and guess which of the two sentences was the most plausible one (binary judgement task) (Lenci, 2011  ###reference_b35###; Chersoni et al., 2019  ###reference_b11###).\nWith the advent of Transformer-based language models, the analysis of their semantic knowledge has often been framed as a probability comparison between sound and anomalous, or atypical sentences (Michaelov and Bergen, 2020  ###reference_b39###; Beyer et al., 2021  ###reference_b6###; Pedinotti et al., 2021  ###reference_b48###; Kauf et al., 2023  ###reference_b31###; Misra et al., 2023  ###reference_b43###). Similarly to the binary judgement setting, a model has to score a sentence pair where two sentences differ only for the presence of a semantic violation, and assign a higher score to the plausible one.\nPedinotti et al. (2021  ###reference_b48###) and Kauf et al. (2023  ###reference_b31###) specifically tested event plausibility knowledge in LLMs. Pedinotti et al. (2021  ###reference_b48###) showed that LLMs achieve correlation with human judgements on par or better than traditional distributional models. Kauf et al. (2023  ###reference_b31###) investigated event plausibility using minimal sentence pairs, in the task of binary judgements. They showed that Transformer-based models retain a considerable amount of event knowledge from textual corpora and vastly outperform the competitor models (i.e., classical distributional models and LSTM baselines). Nevertheless, both studies show LLMs\u2019 generalization capabilities to novel experimental manipulations of the target sentences are limited and that log probabilities are affected by task-irrelevant information, such as the frequency of words within a target sentence.\nEvaluating context-dependent linguistic judgments in LLMs.    \nIn Experiment 2, we evaluate context sensitivity of LLM plausibility estimates (Table 4  ###reference_###). Initial work in this domain shows that (Dutch) LLMs can modulate their probability estimates to accommodate a previously unlikely target word (e.g., A peanut falls in love) following a short licensing context Michaelov et al. (2023  ###reference_b40###) - such scenarios, similarly, were shown to elicit a reduced N400 amplitude in humans, as a neural signature of a decrease of processing complexity of the event (Nieuwland and Van Berkum, 2006  ###reference_b46###; Rueschemeyer et al., 2015  ###reference_b50###). Nevertheless, probability-based judgements of LLMs can also be adversely influenced by context, for example in cases where the context contains information that is not related to the task (for syntax: e.g., Sinha et al., 2022  ###reference_b54###, for factual knowledge: e.g., Kassner and Sch\u00fctze, 2020  ###reference_b30###).\nComparing log likelihood measurements and prompt-based methods.    The direct interaction with models through natural language prompts is exciting for many reasons, including that it facilitates knowledge exploration in a way that begins to mimic the experimental procedure used for humans (Lampinen, 2022  ###reference_b34###). Nevertheless, Hu and Levy (2023  ###reference_b25###); Hu et al. (2024  ###reference_b26###) showed that the use of metalinguistic prompts for model evaluation may underestimate their true capabilities. They compared LLMs\u2019 syntactic/semantic knowledge across four minimal sentence pair datasets and showed that, on average, direct probability measures were a better indicator of these knowledge types than answers to prompts (they also used the DTFit dataset, but their prompts did not explicitly probe the notion of plausibility).\nEvaluating the alignment of instruction-tuned models with humans.     Even though instruction-tuning has been claimed to better align the representations of LLMs and those computed by the human brain Aw et al. (2023  ###reference_b4###), others show that it does not always help for the alignment at the behavioral level Kuribayashi et al. (2023  ###reference_b33###). However, the work in this domain is still sparse."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiment 1: Explicit vs. Implicit Event Plausibility Judgments",
            "text": "In this section, we compare explicit (prompt-based) and implicit (LL-based) plausibility judgments in base- and instruction-tuned LLMs across base and instruction-tuned models from 3 families."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "###table_1### ###figure_1### ###figure_2### ###figure_3### We use two sentence sets adapted from previous studies and compare model scores with human plausibility judgements. A schematic illustration of the items in each of the datasets can be seen in Table 1  ###reference_###.\nEventsAdapt.    The EventsAdapt dataset Fedorenko et al. (2020  ###reference_b17###) is composed of 391 items, each of which includes (i) a plausible active sentence that describes a transitive event in the past tense (The teacher bought the laptop), (ii) the implausible version of the same sentence, constructed by swapping the noun phrases (The laptop bought the teacher), as well as passive voice alternatives (The laptop was bought by the teacher and The teacher was bought by the laptop). The items fall into one of two categories: a) animate-inanimate items (AI; The teacher bought the laptop), where the swap of the noun phrases leads to impossible sentences; and b) animate-animate ones (AA; The nanny tutored the boy), where role-reversed sentences have milder plausibility violations. Given these differences, we model the two subsets independently.\nDTFit.    The DTFit dataset Vassallo et al. (2018  ###reference_b56###) contains 395 items, each of which includes (i) a plausible active sentence that describes a transitive event in the past tense, where an animate agent is interacting with an inanimate patient that is either typical for the agent (The actor won the award); (ii) or less plausible version of the same sentence, constructed by varying the inanimate patient (The actor won the battle). The different degrees of typicality depend on the interaction of the patient with both the agent and the verb (e.g. a battle may be a typical patient for a winning-event, it is just not typical given that the agent is an actor). Thus, word content and not word order is used to distinguish between plausible and implausible sentences."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Human Plausibility Judgments",
            "text": "For DTFit, participants answered questions of the form \u201cHow common is it for a {agent} to {predicate} a {patient}.\u201d (e.g. \u201cHow common is it for an actor to win an award?\u201d on a Likert scale from  (very atypical) to  (very typical) Vassallo et al. (2018  ###reference_b56###). For EventsAdapt, participants evaluated the extent to which each sentence was \u201cplausible, i.e., likely to occur in the real world\u201d on a Likert scale from  (completely implausible) to  (completely plausible) Kauf et al. (2023  ###reference_b31###). We averaged human judgments to obtain a single score for each sentence, and assigned a hit every time that the plausible version of the sentence was scored higher than the corresponding implausible one by the human participants pool.\nLog Likelihood Score\n{The nanny tutored the boy., The boy tutored the nanny.}\nSentence Choice I\nHere are two English sentences: 1) The nanny tutored the boy. 2) The boy tutored the nanny. Which sentence is more plausible? Respond with either 1 or 2 as your answer. Answer: {1, 2}\nSentence Choice II\nYou are evaluating the plausibility of sentences. A sentence is completely plausible if the situation it describes commonly occurs in the real world. A sentence is completely implausible if the situation it describes never occurs in the real world. Tell me if the following sentence is plausible. The nanny tutored the boy. Respond with either Yes or No as your answer. Answer: {Yes, No}\nLikert Scoring\nYou will be given a sentence. Your task is to read the sentence and rate how plausible it is. Here is the sentence: \"The nanny tutored the boy.\" How plausible is this sentence? Respond with a number on a scale from 1 to 7 as your answer, with 1 meaning \"is completely implausible\", and 7 meaning \"is completely plausible\". Answer:\n{ 7,\n6,\n5,\n4,\n3,\n2,\n1 }\nSentence Judgment\nHere is a sentence: The nanny tutored the boy. Is this sentence plausible? Respond with either Yes or No as your answer. Answer: {Yes, No}\n###figure_4### ###figure_5### ###figure_6###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Model Plausibility Judgments",
            "text": "Models.    For our experiments, we used the Base and the Instruct version of three popular autoregressive LLMs: Mistral (Jiang et al., 2023  ###reference_b27###), Falcon (Almazrouei et al., 2023  ###reference_b3###), and MPT (MosaicML NLP Team, 2023  ###reference_b44###), all of them with 7B parameters. We include GPT2-XL (Radford et al., 2019  ###reference_b49###) (1.5B parameters) as a baseline model.\nMetrics.    We adopt the evaluation paradigm by Hu and Levy (2023  ###reference_b25###) and evaluate models using (i) LL scores, and (ii) several zero-shot prompting methods (Table 2  ###reference_###). The LL score is the sum of the log-probabilities of each token  in a sentence, conditioned on the preceding sentence tokens .\nOur prompts, Sentence Choice I/II, Likert Scoring and Sentence Judgment are designed to explicitly query the LLMs\u2019 knowledge of plausibility, using either the same or similar instructions to the task that humans solved (see \u00a73.2  ###reference_###). For all prompting methods except Likert Scoring, we compare the probabilities that models assign to ground-truth continuations (in green) over implausible continuations (in red). For Likert Scoring, we ask models to generate a number from a constrained set of answers, using the outlines python library222https://github.com/outlines-dev/outlines  ###reference_### and compare the generated scores for plausible vs. implausible sentences (the results remain consistent across free vs. constrained generation prompting, see SI \u00a7B  ###reference_###, Figure 5  ###reference_###).333Note that the DTFit dataset was included in the study by Hu and Levy (2023  ###reference_b25###) and was evaluated using different models and different prompts. Their prompts are not applicable to the EventsAdapt dataset and do not prompt models explicitly for plausibility judgments. We include an evaluation of our models on their best-performing prompt for DTFit as a supplementary analysis (SI \u00a7A  ###reference_###, Figure 4  ###reference_###). In our main experiment, all prompts are framed using the direct plausibility query \u201cis plausible\u201d. Supplementary analyses show that this pattern of results remains consistent for alternative queries, such as \u201cmakes sense\u201d (SI \u00a7B  ###reference_###, Figure 6  ###reference_###) and \u201cis likely\u201d (SI \u00a7A  ###reference_###, Figure 4  ###reference_###).\nBinary accuracy.    For each dataset item, we compare the scores/generations of the minimally different plausible and the implausible sentence conditions, and assign a hit for every time a higher score was assigned to the plausible version, the same as for the human scores. The binary accuracy for all models is the ratio of dataset items in which plausible sentences received a higher probability score. The chance level is 50% for all benchmarks except Sentence Judgment, where, following Hu and Levy (2023  ###reference_b25###), we compare the models\u2019 propensity to output the ground truth answer in both plausible and implausible settings, leading to a chance performance of 25%."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "Result 1: LL scores are a more reliable plausibility measure in LLMs than prompting.\nOur analysis reveals that across model architectures and plausibility datasets, LL scores are a more reliable indicator of plausibility knowledge than prompt-based approaches. This shows that there is a direct connection between plausibility and probability measures derived from the context prediction pretraining objective of LLMs (see also Hu and Levy, 2023  ###reference_b25###). Nevertheless, for the best-performing model (across all prompting metrics, except Sentence Choice II), Mistral Instruct, Sentence Choice I certain prompting setups consistently match (Figure 1  ###reference_###, panels (b), (c)) or even outperform (Figure 1  ###reference_###, panel (a)) log likelihood task performance. Despite this success, our comparison critically shows that there is not a single prompt that reliably taps into plausibility knowledge across model architectures, and none of our tested models are robust against slight variations in the way in which prompting is set up (see also Sclar et al., 2023  ###reference_b53###). In fact, many of the prompting methods lead to chance-level performance or below-chance performance for most models, even though their log probabilities evidence substantial knowledge about what events are plausible vs. implausible. This result is in line with Hu and Levy (2023  ###reference_b25###)\u2019s finding of a competence-performance gap when probing models\u2019 metalinguistic judgments.\nResult 2: LL scores encode substantial plausibility knowledge but fall short of human performance.\nThe LL results in Figure 1  ###reference_### show that LLMs acquire substantial event knowledge from distributional linguistic patterns; all of them performing well above chance on the task. Nevertheless, they consistently fall short of human performance and do not improve reliably over older LLMs (especially in cases where an event is comprised of two animate event participants) (Figure 1  ###reference_###, left panel): On EventsAdapt (AI, impossible), all models were successful in distinguishing plausible and implausible sentences, even though all but one model (Falcon Base) fell short of human performance (all Bonferroni-corrected  except for Falcon base: ). At the same time, none of the models significantly outperformed the GPT2-XL baseline model. On the more challenging EventsAdapt (AA, unlikely) subset, all models performed significantly worse than humans in distinguishing AA plausible from implausible events (all ), and only one model, Mistral Base, significantly improved over the smaller baseline model (; all other ). Lastly, the high task performance on DTFit (AI, unlikely), we observe that LLMs can distinguish plausible and implausible AI event descriptions even when low-level distributional cues (like selectional preference restrictions) cannot be used to distinguish the minimal pairs. Although all models still fall short of human performance for this dataset at , all but two of the tested LLMs significantly improved over the GPT2-XL baseline on this dataset (only Mistral Base and Falcon Instruct are not better, ).\nResult 3: Instruction-tuning worsens LL score alignment with human plausibility judgments.\n###figure_7### ###figure_8### ###figure_9### Next, we zoom in on the comparison of LL scores derived from Base vs. Instruct variants of the same model. Because instruction tuning constrains model behaviors to align with human-desired response characteristics (Zhang et al., 2023  ###reference_b58###; Chia et al., 2023  ###reference_b12###), it is reasonable to assume that the models\u2019 learned probability distributions align better with human expectations of plausible sequences than the base variant, which might be more susceptible to the reporting bias in textual corpora Gordon and Van Durme (2013  ###reference_b20###).\n###figure_10### A comparative analysis of the Base and Instruct results across different model architectures reveals no beneficial effect of instruction-tuning for gauging event plausibility through LL measurements: In all but one instance do instruction-tuned models performed similar or even slightly worse than their corresponding base model (Table 3  ###reference_###). Interestingly, the gap is most noticable for the most challenging dataset, EventsAdapt (AA, unlikely). An investigation of this difference shows that certain low-level features of the input may disproportionately affect the LLs that instruction-tuned models assign to word sequences: much of the performance difference is due to the instruction-tuned models\u2019 worse performance in discerning plausible and implausible active-voice sentences (see Figure 2  ###reference_###). This variance highlights the fact that even though direct measurements of model-derived string probabilities in many cases encode task-relevant information (e.g., modeling of grammaticality, Warstadt et al. (2020  ###reference_b57###), of N400 effects, Michaelov and Bergen (2020  ###reference_b39###), etc.), they are additionally influenced by low-level features of the input Pedinotti et al. (2021  ###reference_b48###); Kauf et al. (2023  ###reference_b31###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment 2: Context-Dependent Plausibility Judgments",
            "text": "Experiment 1 has shown that LLs are the most reliable, albeit imperfect, metric for probing the plausibility of isolated sentences in LLMs. However, most of the time, humans and LLMs to not process sentences in isolation, but rather as part of a larger context. On the other hand, language models have also been shown to be sensitive to priming effects from inter-sentential context (Misra et al., 2020  ###reference_b42###; Kassner and Sch\u00fctze, 2020  ###reference_b30###). In Experiment 2, we investigate whether LLMs appropriately modulate their judgments of event plausibility when provided with different discourse contexts. Specifically, we test how and to what extent judgments of event plausibility from minimal pair accuracies change in English LLMs in the presence of (i) supporting or (ii) non-supporting but related single-sentence contexts."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Metrics",
            "text": "We evaluate the models\u2019 context-aware plausibility judgements on three critical metrics:\nGeneral Plausibility. This metric measures the propensity of models to assign a higher probability to plausible sentences than to minimally different implausible sentence variants when no influencing context is present (similar to \u00a73  ###reference_###). For every dataset item, we assign a model a hit in case\nContext-Dependent Plausibility. This metric measures the ability of models to increase the probability they assign to an a priori implausible sentence in the presence of a licensing context. For every dataset item, we assign a model a hit in case\nContext Sensitivity. This metric measures the models\u2019 ability to selectively update sentence probabilities. For every dataset item, we assign a model a hit in case\nFor each metric, we evaluate model performance the likelihood they assign (i) a critical word within the target sentence and (ii) the target sentence itself. If a critical word consists of multiple tokens, we use the sum of the log likelihood scores of the word tokens. Whereas critical/target word likelihoods measures the ability of models to detect a contextually unexpected linguistic event, target sentence likelihood measures investigate whether implausibility is reliably reflected in the probability the models assign to tokens after encountering a semantically anomalous item. This is because the token likelihood of plausible and implausible sentences are shared up until the first occurrence of a contextually unlicensed word."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "Result 1: Target word LLs are better modulated by context than target sentence LLs.\n###figure_11### When comparing target word vs. target sentence LLs, a clear trend emerges: all models perform extremely well (around %) across all metrics when comparing the probabilities of target words (Table 5  ###reference_###, Word columns); at the same time, when using the likelihoods they assign to sentences as an indicator of event plausibility knowledge, performance degrades for two of the three metrics (Table 5  ###reference_###, Sent. columns). In particular, even though almost all LLMs are able to distinguish plausible and implausible sentences (General Plausibility  ###reference_###, similar to \u00a73  ###reference_###); and are able to modulate the probability they assign an unexpected sentence in the presence of licensing context, they fail to update the sentence probabilities selectively (this is evidenced by the substantial drop in performance for the Context Sensitivity  ###reference_### metric across LLMs (although they perform significantly better than the baseline GPT2-XL model). This pattern suggests that while a semantically licensing context assists the models in up-weighing the probability of an otherwise implausible target word/event description (see Context-Dependent Plausibility  ###reference_###; in line with Michaelov et al., 2023  ###reference_b40###), contextual implausibility is not reliably reflected in LLMs\u2019 sentence likelihoods. In particular, once an unexpected target word has been encountered (which the LLMs are able to discern, see Context Sensitivity  ###reference_###, Word columns), the LLMs appear to quickly adjust the predictions in the post-target region, in some cases assigning even higher probabilities to post-target words than in the Critical condition, with the consequence that the scores for anomalous sentences and contextually-licensed ones differ less significantly at the sentence level.\nThis suggests that a semantically-licensing context helps a model in predicting an otherwise anomalous word, but the global probability of the target sentence benefits less from a the specific context. After meeting an unexpected target word, LLMs seem to be quickly able to adjust the predictions for the following ones, with the consequence that the scores for anomalous sentences and contextually-licensed ones differ less significantly at the sentence level.\nResult 2: Context-modulated LLs align with human contextual judgment patterns.\nFinally, we investigate how contextual plausibility judgments correspond to human behavior for the same stimuli. We focus on the sensibility-judgment task, in which participants were asked to decide if a target sentence made sense (i) to them within the provided context, or (ii) to another person who did not have access to the context sentence Jouravlev et al. (2019  ###reference_b28###). Here, we model this dataset in a \u2018single-participant setting\u2019, by exposing the LLMs to the full items and comparing the log probabilities assigned to the target words in the three experimental conditions, with or without licensing context. Across models, we see a remarkable match between human- and model-derived plausibility scores, both in the isolated sentence and the contextualized setup. For completeness, we report results for the exact replication of the human study in LLMs, using Sentence Judgment prompts in SI \u00a7D  ###reference_###. We note that, again, LLs provide a better fit to human data, even though the prompting results for Instruct models matched the human behavioral patterns qualitatively (see also SI \u00a7C  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Overall, we show through careful investigation that LL scores, reflecting co-occurrence patterns distilled by LLMs from the task of next-word prediction at scale during pre-training, remain a more reliable measure of sentence plausibility than both (i) direct prompting and (ii) log likelihood scores from models finetuned to follow instructions. This is true in scenarios that encompass both isolated and context-dependent sentence plausibility estimates. Even though instruction-tuning has been claimed to align LLMs and human brain representations (Aw et al., 2023  ###reference_b4###), other studies show that it does not always help for the alignment at the behavioral level (Kuribayashi et al., 2023  ###reference_b33###). The results presented in our work are consistent with the latter finding.\nConcerning LLMs\u2019 sensitivity to sentence context, we observe that by using LL scores at the level of the target word, all the models perform around 90% with respect to the ground truth and are well aligned to human judgement patterns. On the other hand, when using sentence-level LL scores, we notice that the models have the tendency to \"re-balance\" the log likelihoods after processing an unexpected word, with the consequence that semantically anomalous sentences and contextually-licensed ones become harder to distinguish.\nAlthough it is possible that model- and task-specific prompts will outperform raw LL scores as a way to estimate sentence plausibility, our work highlights that LL scores are an easy, zero-shot way to assess LLMs\u2019 implicit knowledge. Thus, getting a raw LL estimate of model performance can provide an initial estimate of whether or not custom prompt-based solutions can be successful or\u2014in some cases\u2014obviate the need for prompt tuning altogether."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "CK and this work were partially supported by the MIT Quest for Intelligence. EC was fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU 15612222). This research was also partly funded by PNRR\u2014M4C2\u2014Investimento 1.3, Partenariato Esteso PE00000013\u2014\u201cFAIR\u2014Future Artificial Intelligence Research\u201d\u2014Spoke 1 \u201cHuman-centered AI,\u201d funded by the European Commission under the NextGeneration EU programme."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix x1",
            "parent_section_id": null,
            "section_name": "Supplementary Information",
            "text": ""
        },
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Additional prompting results for DTFit",
            "text": "###figure_12###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Evidence for invariance to prompting variations for DTFit",
            "text": ""
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Replicating the sensibility-judgment task by Jouravlev et\u00a0al. (2019) using sentence log likelihoods",
            "text": "In Figure 7  ###reference_###, we replicate the human experiment by Jouravlev et al. (2019  ###reference_b28###) in LLMs using sentence log likelihood measurements. We generally observe similar trends than the comparison with the target word measurement.\n###figure_13###"
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Replicating the sensibility-judgment task by Jouravlev et\u00a0al. (2019) using prompting",
            "text": "###figure_14### To replicate the human experiment by Jouravlev et al. (2019  ###reference_b28###) in LLMs using prompting, we queried the models using an adjusted Sentence Judgment prompt (see Table 2  ###reference_###): [No context:] Here is a sentence: \u201csentence\u201d. Does this sentence make sense? Respond with either Yes or No as your answer. [With context:] Here is a context: \u201ccontext\u201d, and here is a sentence: \u201csentence\u201d. Does this sentence make sense considering the context? Respond with either Yes or No as your answer. We report our results in Figure 8  ###reference_###.\nWe observe that while GPT2-XL and most base models often favor one answer option (GPT2-XL almost always assigns more probability mass to Yes rather than No in this setup; see also Figure 1  ###reference_###, Sentence Judgment), the instruction-tuned models exhibit more a nuanced behavior: These models are more consistent with human responses in this binary sensitivity judgment task, matching them qualitatively. Nevertheless, instruction-tuned models tend to (i) systematically underestimate the contextual plausibility of the Critical sentences (Figure 8  ###reference_###, upper panel), and (ii) systematically overestimate the plausibility of implausible sentences relative to humans (SemAnom conditions and Critical condition, Figure 8  ###reference_###, lower panel) in the binary sensibility-judgment task setup."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.3.4.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S3.T1.3.4.1.1\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T1.3.4.1.1.1\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.3.4.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.4.1.2.1\">Plausible?</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.3.4.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.4.1.3.1\">Possible?</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.3.4.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.4.1.4.1\">Voice</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T1.3.4.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.4.1.5.1\">Example</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T1.3.4.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.4.1.6.1\">Source</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.1.1.1\" rowspan=\"4\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S3.T1.1.1.1.1.1.1\"><span class=\"ltx_inline-para ltx_minipage ltx_align_middle\" id=\"S3.T1.1.1.1.1.1.1.1\" style=\"width:85.4pt;\">\n<span class=\"ltx_para\" id=\"S3.T1.1.1.1.1.1.1.1.p1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.1.1.1.1.1.1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1.1.1.p1.1.1\">EventsAdapt</span>\u00a0\u00a0<img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"12\" id=\"S3.T1.1.1.1.1.1.1.1.p1.1.g1\" src=\"extracted/5477081/figures/animate-animate.png\" width=\"20\"/></span>\n<span class=\"ltx_p\" id=\"S3.T1.1.1.1.1.1.1.1.p1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1.1.1.p1.2.1\">(AA, unlikely)</span></span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.3\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.4\">Active</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.5\">The nanny tutored the boy.</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.6\" rowspan=\"8\"><span class=\"ltx_text\" id=\"S3.T1.1.1.6.1\"><cite class=\"ltx_cite ltx_citemacro_citet\">Fedorenko et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14859v1#bib.bib17\" title=\"\">2020</a>)</cite></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.5.2\">\n<td class=\"ltx_td\" id=\"S3.T1.3.5.2.1\"></td>\n<td class=\"ltx_td\" id=\"S3.T1.3.5.2.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.5.2.3\">Passive</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.3.5.2.4\">The boy was tutored by the nanny.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.6.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.6.3.1\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.6.3.2\">Yes</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.6.3.3\">Active</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.3.6.3.4\">The boy tutored the nanny.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.7.4\">\n<td class=\"ltx_td\" id=\"S3.T1.3.7.4.1\"></td>\n<td class=\"ltx_td\" id=\"S3.T1.3.7.4.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.7.4.3\">Passive</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.3.7.4.4\">The nanny was tutored by the boy.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.2.2.1\" rowspan=\"4\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S3.T1.2.2.1.1.1.1\"><span class=\"ltx_inline-para ltx_minipage ltx_align_middle\" id=\"S3.T1.2.2.1.1.1.1.1\" style=\"width:85.4pt;\">\n<span class=\"ltx_para\" id=\"S3.T1.2.2.1.1.1.1.1.p1\">\n<span class=\"ltx_p\" id=\"S3.T1.2.2.1.1.1.1.1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.2.1.1.1.1.1.p1.1.1\">EventsAdapt</span>\u00a0\u00a0<img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"12\" id=\"S3.T1.2.2.1.1.1.1.1.p1.1.g1\" src=\"extracted/5477081/figures/animate-inanimate-laptop.png\" width=\"20\"/></span>\n<span class=\"ltx_p\" id=\"S3.T1.2.2.1.1.1.1.1.p1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.2.1.1.1.1.1.p1.2.1\">(AI, impossible)</span></span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.2.2\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.2.3\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.2.4\">Active</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.2.5\">The teacher bought the laptop.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.8.5\">\n<td class=\"ltx_td\" id=\"S3.T1.3.8.5.1\"></td>\n<td class=\"ltx_td\" id=\"S3.T1.3.8.5.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.8.5.3\">Passive</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.3.8.5.4\">The laptop was bought by the teacher.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.9.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.9.6.1\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.9.6.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.9.6.3\">Active</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.3.9.6.4\">The laptop bought the teacher.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.10.7\">\n<td class=\"ltx_td\" id=\"S3.T1.3.10.7.1\"></td>\n<td class=\"ltx_td\" id=\"S3.T1.3.10.7.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.10.7.3\">Passive</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.3.10.7.4\">The teacher was bought by the laptop.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"S3.T1.3.3.1\" rowspan=\"2\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S3.T1.3.3.1.1.1.1\"><span class=\"ltx_inline-para ltx_minipage ltx_align_middle\" id=\"S3.T1.3.3.1.1.1.1.1\" style=\"width:85.4pt;\">\n<span class=\"ltx_para\" id=\"S3.T1.3.3.1.1.1.1.1.p1\">\n<span class=\"ltx_p\" id=\"S3.T1.3.3.1.1.1.1.1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.3.1.1.1.1.1.p1.1.1\">DTFit</span>\u00a0\u00a0<img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"12\" id=\"S3.T1.3.3.1.1.1.1.1.p1.1.g1\" src=\"extracted/5477081/figures/animate-inanimate-laptop.png\" width=\"20\"/></span>\n<span class=\"ltx_p\" id=\"S3.T1.3.3.1.1.1.1.1.p1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.3.3.1.1.1.1.1.p1.2.1\">(AI, unlikely)</span></span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.2\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.3\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.4\">Active</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.3.3.5\">The actor won the award.</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T1.3.3.6\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.3.3.6.1\"><cite class=\"ltx_cite ltx_citemacro_citet\">Vassallo et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14859v1#bib.bib56\" title=\"\">2018</a>)</cite></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.11.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.3.11.8.1\">No</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.3.11.8.2\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.3.11.8.3\">Active</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.3.11.8.4\">The actor won the battle.</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Example stimuli from the datasets used in Experiment 1. Names in parentheses indicate event participant animacy (AI = animate agent, inanimate patient; AA = animate agent, animate patient) and the plausibility type of the implausible sentences in the dataset (impossible vs.\u00a0unlikely).</figcaption>\n</figure>",
            "capture": "Table 1: Example stimuli from the datasets used in Experiment 1. Names in parentheses indicate event participant animacy (AI = animate agent, inanimate patient; AA = animate agent, animate patient) and the plausibility type of the implausible sentences in the dataset (impossible vs.\u00a0unlikely)."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1\" style=\"width:73.7pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.1.1.1.1\">Evaluation type</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.2\" style=\"width:346.9pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"S3.T2.1.1.1.2.1\">Example</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.2.1.1\" style=\"width:73.7pt;padding-bottom:1.99997pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.2.1.1.1\">Log Likelihood Score</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T2.1.2.1.2\" style=\"width:346.9pt;padding-bottom:1.99997pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.2.1.2.1\">{<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.2.1.2.1.1\" style=\"color:#469082;\">The nanny tutored the boy.</span>, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.2.1.2.1.2\" style=\"color:#B96F7D;\">The boy tutored the nanny.</span>}</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.2.1\" style=\"width:73.7pt;padding-bottom:1.99997pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.2.1.1\">Sentence Choice I</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.3.2.2\" style=\"width:346.9pt;padding-bottom:1.99997pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.3.2.2.1\">Here are two English sentences: 1) The nanny tutored the boy. 2) The boy tutored the nanny. Which sentence is more plausible? Respond with either 1 or 2 as your answer. Answer: {<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.3.2.2.1.1\" style=\"color:#469082;\">1</span>, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.3.2.2.1.2\" style=\"color:#B96F7D;\">2</span>}</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.3\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.3.1\" style=\"width:73.7pt;padding-bottom:1.99997pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.3.1.1\">Sentence Choice II</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.4.3.2\" style=\"width:346.9pt;padding-bottom:1.99997pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.4.3.2.1\">You are evaluating the plausibility of sentences. A sentence is completely plausible if the situation it describes commonly occurs in the real world. A sentence is completely implausible if the situation it describes never occurs in the real world. Tell me if the following sentence is plausible. The nanny tutored the boy. Respond with either Yes or No as your answer. Answer: {<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.4.3.2.1.1\" style=\"color:#469082;\">Yes</span>, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.4.3.2.1.2\" style=\"color:#B96F7D;\">No</span>}</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5.4\">\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.4.1\" style=\"width:73.7pt;padding-bottom:1.99997pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.4.1.1\">Likert Scoring</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T2.1.5.4.2\" style=\"width:346.9pt;padding-bottom:1.99997pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.5.4.2.1\">You will be given a sentence. Your task is to read the sentence and rate how plausible it is. Here is the sentence: \"The nanny tutored the boy.\" How plausible is this sentence? Respond with a number on a scale from 1 to 7 as your answer, with 1 meaning \"is completely implausible\", and 7 meaning \"is completely plausible\". Answer:\n{ <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.5.4.2.1.1\" style=\"color:#469082;\">7<span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1.1.1\" style=\"color:black;\">,\n<span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1.1.1.1\" style=\"color:#598B81;\">6</span>,\n<span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1.1.1.2\" style=\"color:#6C8580;\">5</span>,\n<span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1.1.1.3\" style=\"color:#7F8080;\">4</span>,\n<span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1.1.1.4\" style=\"color:#937A7F;\">3</span>,\n<span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1.1.1.5\" style=\"color:#A6747E;\">2</span>,\n<span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1.1.1.6\" style=\"color:#B96F7D;\">1</span></span></span> }</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6.5\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S3.T2.1.6.5.1\" style=\"width:73.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.5.1.1\">Sentence Judgment</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S3.T2.1.6.5.2\" style=\"width:346.9pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T2.1.6.5.2.1\">Here is a sentence: The nanny tutored the boy. Is this sentence plausible? Respond with either Yes or No as your answer. Answer: {<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.6.5.2.1.1\" style=\"color:#469082;\">Yes</span>, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.6.5.2.1.2\" style=\"color:#B96F7D;\">No</span>}</p>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Example evaluation strategies. The prompts are extended and adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Hu and Levy (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14859v1#bib.bib25\" title=\"\">2023</a>)</cite>.</figcaption>\n</figure>",
            "capture": "Table 2: Example evaluation strategies. The prompts are extended and adapted from Hu and Levy (2023)."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S3.T3.3\" style=\"width:433.6pt;height:153pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(89.3pt,-31.5pt) scale(1.6998480661331,1.6998480661331) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.3.3.4.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T3.3.3.4.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T3.3.3.4.1.2\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T3.3.3.4.1.2.1\">Mistral</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T3.3.3.4.1.3\">\n<span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T3.3.3.4.1.3.1\">Falcon</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T3.3.3.4.1.4\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T3.3.3.4.1.4.1\">MPT</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.3.5.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T3.3.3.5.2.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.3.3.5.2.2\">Base</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.3.3.5.2.3\">Instruct</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.3.3.5.2.4\">Base</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.3.3.5.2.5\">Instruct</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.3.3.5.2.6\">Base</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.3.3.5.2.7\">Instruct</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.1.1.1.1\">EventsAdapt\u00a0\u00a0<img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"12\" id=\"S3.T3.1.1.1.1.g1\" src=\"extracted/5477081/figures/animate-animate.png\" width=\"20\"/>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1\">0.82**</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.3\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.4.1\">0.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.5\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.6.1\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.7.1\">0.71</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.2.2.2.1\">EventsAdapt\u00a0\u00a0<img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"12\" id=\"S3.T3.2.2.2.1.g1\" src=\"extracted/5477081/figures/animate-inanimate-laptop.png\" width=\"20\"/>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.2.2.2.2.1\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.2.3\">0.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.2.2.2.4.1\">0.97*</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.2.5\">0.94</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.2.2.2.6.1\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.2.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.2.2.2.7.1\">0.93</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.3.3.3.1\">DTFit\u00a0\u00a0<img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"12\" id=\"S3.T3.3.3.3.1.g1\" src=\"extracted/5477081/figures/animate-inanimate-laptop.png\" width=\"20\"/>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.3.3.2\">0.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.3.3.3.3.1\">0.93*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.3.3.3.4.1\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.3.3.5\">0.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.3.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.3.3.3.6.1\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.3.3.3.7.1\">0.93</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Log Likelihood results across metrics and target regions. Significant differences from dependent t-tests between Base and Instruct models are marked with asterisks (: *; : **).</figcaption>\n</figure>",
            "capture": "Table 3: Log Likelihood results across metrics and target regions. Significant differences from dependent t-tests between Base and Instruct models are marked with asterisks (: *; : **)."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T4.1.1.1.1\"></td>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.1.1.1.2\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T4.1.1.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.3.1\">Target sentence</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\" id=\"S4.T4.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.2.2.1.1\">Condition</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\" id=\"S4.T4.1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.2.2.2.1\">Context sentence (optional)</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.1.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.2.2.3.1\">Prefix</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.2.2.4.1\">Tgt. word</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.1.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.2.2.5.1\">Spill-over region</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.3.3.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T4.1.3.3.1.1\">Control</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.3.3.2\">The kids were looking at a canary in the pet store.</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.3.3.3\">The bird had a little</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.3.3.4\">beak</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.3.3.5\">and a bright yellow tail.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.1.4.4.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T4.1.4.4.1.1\">SemAnom</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.1.4.4.2\">Anna was definitely a very cute child.</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.1.4.4.3\">The girl had a little</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.4.4.4\">beak</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T4.1.4.4.5\">and a bright yellow tail.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.5.5.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T4.1.5.5.1.1\">Critical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.5.5.2\">The girl dressed up as a canary for Halloween.</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.5.5.3\">The girl had a little</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.1.5.5.4\">beak</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T4.1.5.5.5\">and a bright yellow tail.</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Sentence manipulations in the dataset by <cite class=\"ltx_cite ltx_citemacro_citet\">Jouravlev et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14859v1#bib.bib28\" title=\"\">2019</a>)</cite>. Tgt. \u2013 Target.</figcaption>\n</figure>",
            "capture": "Table 4: Sentence manipulations in the dataset by Jouravlev et\u00a0al. (2019). Tgt. \u2013 Target."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T5.1\" style=\"width:433.6pt;height:350.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(116.7pt,-94.4pt) scale(2.16538877904869,2.16538877904869) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T5.1.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T5.1.1.1.1.2\">\n<a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.14859v1#S4.Ex1\" title=\"4.2 Metrics \u2023 4 Experiment 2: Context-Dependent Plausibility Judgments \u2023 Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models\">Gen. Plaus.</a></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T5.1.1.1.1.3\">\n<a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.14859v1#S4.Ex2\" title=\"4.2 Metrics \u2023 4 Experiment 2: Context-Dependent Plausibility Judgments \u2023 Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models\">Context-Dep. Plaus.</a></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T5.1.1.1.1.4\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.14859v1#S4.Ex3\" title=\"4.2 Metrics \u2023 4 Experiment 2: Context-Dependent Plausibility Judgments \u2023 Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models\">Context Sens.</a></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S4.T5.1.1.2.2.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.1.1.2.2.2\">Word</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.1.1.2.2.3\">Sent.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.1.1.2.2.4\">Word</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.1.1.2.2.5\">Sent.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.1.1.2.2.6\">Word</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.1.1.2.2.7\">Sent.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T5.1.1.3.1.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T5.1.1.3.1.1.1\">Mistral (base)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.1.2\">0.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.3.1.3.1\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.1.4\">0.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.3.1.5.1\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.3.1.6.1\">0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.1.7\">0.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T5.1.1.4.2.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T5.1.1.4.2.1.1\">Mistral (instr)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.4.2.2.1\">0.97</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.2.3\">0.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.2.4\">0.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.4.2.5.1\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.4.2.6.1\">0.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.2.7\">0.84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T5.1.1.5.3.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T5.1.1.5.3.1.1\">Falcon (base)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.5.3.2.1\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.3.3\">0.94</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.5.3.4.1\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.3.5\">0.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.5.3.6.1\">0.98</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.3.7\">0.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T5.1.1.6.4.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T5.1.1.6.4.1.1\">Falcon (instr)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.4.2.1\">0.98</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.4.3\">0.91</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.4.4.1\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.4.5.1\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.4.6.1\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.4.7\">0.77</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T5.1.1.7.5.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T5.1.1.7.5.1.1\">MPT (base)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.7.5.2.1\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.5.3\">0.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.5.4\">0.95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.7.5.5.1\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.5.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.7.5.6.1\">0.99</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.5.7\">0.76</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T5.1.1.8.6.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T5.1.1.8.6.1.1\">MPT (instr)</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.8.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.8.6.2.1\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.8.6.3\">0.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.8.6.4\">0.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.8.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.8.6.5.1\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.8.6.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.8.6.6.1\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.8.6.7\">0.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T5.1.1.9.7.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T5.1.1.9.7.1.1\">GPT2-xl</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.9.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.9.7.2.1\">0.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.9.7.3\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.9.7.4\">0.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.9.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.9.7.5.1\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.9.7.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.9.7.6.1\">0.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.9.7.7\">0.64</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.T5.3.1\">LL</span> results for Expt 2. Gen. \u2013 General; Dep. \u2013 Dependent; Plaus. \u2013 Plausibility; Sens. \u2013 Sensitivity; Word/Sent. \u2013 scores for target word/sentence.</figcaption>\n</figure>",
            "capture": "Table 5: LL results for Expt 2. Gen. \u2013 General; Dep. \u2013 Dependent; Plaus. \u2013 Plausibility; Sens. \u2013 Sensitivity; Word/Sent. \u2013 scores for target word/sentence."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.14859v1_figure_1.png",
            "caption": "Table 1: Example stimuli from the datasets used in Experiment 1. Names in parentheses indicate event participant animacy (AI = animate agent, inanimate patient; AA = animate agent, animate patient) and the plausibility type of the implausible sentences in the dataset (impossible vs. unlikely)."
        },
        "2": {
            "figure_path": "2403.14859v1_figure_2.png",
            "caption": "Table 1: Example stimuli from the datasets used in Experiment 1. Names in parentheses indicate event participant animacy (AI = animate agent, inanimate patient; AA = animate agent, animate patient) and the plausibility type of the implausible sentences in the dataset (impossible vs. unlikely)."
        },
        "3": {
            "figure_path": "2403.14859v1_figure_3.png",
            "caption": "Table 1: Example stimuli from the datasets used in Experiment 1. Names in parentheses indicate event participant animacy (AI = animate agent, inanimate patient; AA = animate agent, animate patient) and the plausibility type of the implausible sentences in the dataset (impossible vs. unlikely)."
        },
        "4": {
            "figure_path": "2403.14859v1_figure_4.png",
            "caption": "(a) EventsAdapt (AA, unlikely)"
        },
        "5": {
            "figure_path": "2403.14859v1_figure_5.png",
            "caption": "(b) DTFit (AI, unlikely)"
        },
        "6": {
            "figure_path": "2403.14859v1_figure_6.png",
            "caption": "(c) EventsAdapt (AI, impossible)"
        },
        "7": {
            "figure_path": "2403.14859v1_figure_7.png",
            "caption": "Table 3: Log Likelihood results across metrics and target regions. Significant differences from dependent t-tests between Base and Instruct models are marked with asterisks (p<.05\ud835\udc5d.05p<.05italic_p < .05: *; p<.01\ud835\udc5d.01p<.01italic_p < .01: **)."
        },
        "8": {
            "figure_path": "2403.14859v1_figure_8.png",
            "caption": "Table 3: Log Likelihood results across metrics and target regions. Significant differences from dependent t-tests between Base and Instruct models are marked with asterisks (p<.05\ud835\udc5d.05p<.05italic_p < .05: *; p<.01\ud835\udc5d.01p<.01italic_p < .01: **)."
        },
        "9": {
            "figure_path": "2403.14859v1_figure_9.png",
            "caption": "Table 3: Log Likelihood results across metrics and target regions. Significant differences from dependent t-tests between Base and Instruct models are marked with asterisks (p<.05\ud835\udc5d.05p<.05italic_p < .05: *; p<.01\ud835\udc5d.01p<.01italic_p < .01: **)."
        },
        "10": {
            "figure_path": "2403.14859v1_figure_10.png",
            "caption": "Table 4: Sentence manipulations in the dataset by Jouravlev et al. (2019). Tgt. \u2013 Target."
        },
        "11": {
            "figure_path": "2403.14859v1_figure_11.png",
            "caption": "Figure 3: Target word LLs replicate patterns of human sentence sensibility judgments. Human data from Jouravlev et al. (2019). Bars indicate average plausibility of sentences (Human) and average target word log likelihoods (LLMs). Dots represent individual sentence scores (averaged across the participant pool for Human)."
        },
        "12": {
            "figure_path": "2403.14859v1_figure_12.png",
            "caption": "Figure 7: Replicating the sensibility-judgment task in LLMs using sentence LL measures. Human data from Jouravlev et al. (2019)."
        },
        "13": {
            "figure_path": "2403.14859v1_figure_13.png",
            "caption": "Figure 7: Replicating the sensibility-judgment task in LLMs using sentence LL measures. Human data from Jouravlev et al. (2019)."
        },
        "14": {
            "figure_path": "2403.14859v1_figure_14.png",
            "caption": "Figure 7: Replicating the sensibility-judgment task in LLMs using sentence LL measures. Human data from Jouravlev et al. (2019)."
        },
        "15": {
            "figure_path": "2403.14859v1_figure_15.png",
            "caption": "Figure 7: Replicating the sensibility-judgment task in LLMs using sentence LL measures. Human data from Jouravlev et al. (2019)."
        },
        "16": {
            "figure_path": "2403.14859v1_figure_16.png",
            "caption": "Figure 8: Replicating the sensibility-judgment task in LLMs using prompting via the adjusted Sentence Judgment prompt in \u00a7D. Human data from Jouravlev et al. (2019). We use a barplot to visually set apart this prompt-based comparison vs. LL-based ones in Figures 3, 7."
        }
    },
    "references": [
        {
            "1": {
                "title": "GPT-4 Technical Teport.",
                "author": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.08774.",
                "url": null
            }
        },
        {
            "2": {
                "title": "The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty through Generation.",
                "author": "Laura Aina and Tal Linzen. 2021.",
                "venue": "In Proceedings of the EMNLP BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks.",
                "url": null
            }
        },
        {
            "3": {
                "title": "The Falcon Series of Open Language Models.",
                "author": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023.",
                "venue": "arXiv preprint arXiv:2311.16867.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Instruction-tuning Aligns LLMs to the Human Brain.",
                "author": "Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut. 2023.",
                "venue": "arXiv preprint arXiv:2312.00575.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Distributional Memory: A General Framework for Corpus-Based Semantics.",
                "author": "Marco Baroni and Alessandro Lenci. 2010.",
                "venue": "Computational Linguistics, 36(4):673\u2013721.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models.",
                "author": "Anne Beyer, Sharid Lo\u00e1iciga, and David Schlangen. 2021.",
                "venue": "In Proceedings of NAACL.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Effects of Event Knowledge in Processing Verbal Arguments.",
                "author": "Klinton Bicknell, Jeffrey L Elman, Mary Hare, Ken McRae, and Marta Kutas. 2010.",
                "venue": "Journal of Memory and Language, 63(4):489\u2013505.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Prompting Language Models for Linguistic Structure.",
                "author": "Terra Blevins, Hila Gonen, and Luke Zettlemoyer. 2023.",
                "venue": "In Proceedings of ACL.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Language Models Are Few-shot Learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.",
                "venue": "Advances in Neural Information Processing Systems, 33:1877\u20131901.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Sparks of Artificial General Intelligence: Early Experiments with GPT-4.",
                "author": "S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.12712.",
                "url": null
            }
        },
        {
            "11": {
                "title": "A Structured Distributional Model of Sentence Meaning and Processing.",
                "author": "Emmanuele Chersoni, Enrico Santus, Ludovica Pannitto, Alessandro Lenci, Philippe Blache, and Chu-Ren Huang. 2019.",
                "venue": "Natural Language Engineering, 25(4):483\u2013502.",
                "url": null
            }
        },
        {
            "12": {
                "title": "INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models.",
                "author": "Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2023.",
                "venue": "arXiv preprint arXiv:2306.04757.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Scaling Instruction-finetuned Language Models.",
                "author": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022.",
                "venue": "arXiv preprint arXiv:2210.11416.",
                "url": null
            }
        },
        {
            "14": {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of NAACL-HLT.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Probing for Incremental Parse States in Autoregressive Language Models.",
                "author": "Tiwalayo Eisape, Vineet Gangireddy, Roger Levy, and Yoon Kim. 2022.",
                "venue": "In Findings of EMNLP.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Measuring Causal Effects of Data Statistics on Language Model\u2019s Factual Predictions.",
                "author": "Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Sch\u00fctze, and Yoav Goldberg. 2022.",
                "venue": "arXiv preprint arXiv:2207.14251.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Lack of Selectivity for Syntax Relative to Word Meanings Throughout the Language Network.",
                "author": "Evelina Fedorenko, Idan Asher Blank, Matthew Siegelman, and Zachary Mineroff. 2020.",
                "venue": "Cognition, 203:104348.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State.",
                "author": "Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019.",
                "venue": "In Proceedings of NAACL.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation.",
                "author": "Atticus Geiger, Kyle Richardson, and Christopher Potts. 2020.",
                "venue": "In Proceedings of the EMNLP BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Reporting Bias and Knowledge Acquisition.",
                "author": "Jonathan Gordon and Benjamin Van Durme. 2013.",
                "venue": "In Proceedings of the Workshop on Automated Knowledge Base Construction.",
                "url": null
            }
        },
        {
            "21": {
                "title": "How Close Is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection.",
                "author": "Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023.",
                "venue": "arXiv preprint arXiv:2301.07597.",
                "url": null
            }
        },
        {
            "22": {
                "title": "A Structural Probe for Finding Syntax in Word Representations.",
                "author": "John Hewitt and Christopher D Manning. 2019.",
                "venue": "In Proceedings of NAACL.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Surface Form Competition: Why the Highest Probability Answer Isn\u2019t Always Right.",
                "author": "Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021.",
                "venue": "In Proceedings of EMNLP.",
                "url": null
            }
        },
        {
            "24": {
                "title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models.",
                "author": "Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger P Levy. 2020.",
                "venue": "In Proceedings of ACL.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Prompting Is Not a Substitute for Probability Measurements in Large Language Models.",
                "author": "Jennifer Hu and Roger Levy. 2023.",
                "venue": "In Proceedings of EMNLP.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Language Models Align with Human Judgments on key Grammatical Constructions.",
                "author": "Jennifer Hu, Kyle Mahowald, Gary Lupyan, Anna Ivanova, and Roger Levy. 2024.",
                "venue": "arXiv preprint arXiv:2402.01676.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Mistral 7B.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023.",
                "venue": "arXiv preprint arXiv:2310.06825.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Tracking Colisteners\u2019 Knowledge States during Language Comprehension.",
                "author": "Olessia Jouravlev, Rachael Schwartz, Dima Ayyash, Zachary Mineroff, Edward Gibson, and Evelina Fedorenko. 2019.",
                "venue": "Psychological Science, 30(1):3\u201319.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models.",
                "author": "Cheongwoong Kang and Jaesik Choi. 2023.",
                "venue": "In Findings of EMNLP.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, but Cannot Fly.",
                "author": "Nora Kassner and Hinrich Sch\u00fctze. 2020.",
                "venue": "In Proceedings of ACL.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Event Knowledge in Large Language Models: The Gap Between the Impossible and the Unlikely.",
                "author": "Carina Kauf, Anna A Ivanova, Giulia Rambelli, Emmanuele Chersoni, Jingyuan Selena She, Zawad Chowdhury, Evelina Fedorenko, and Alessandro Lenci. 2023.",
                "venue": "Cognitive Science, 47(11):e13386.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Probing What Different NLP Tasks Teach Machines about Function Word Comprehension.",
                "author": "Najoung Kim, Roma Patel, Adam Poliak, Alex Wang, Patrick Xia, R Thomas McCoy, Ian Tenney, Alexis Ross, Tal Linzen, Benjamin Van Durme, et al. 2019.",
                "venue": "In Proceedings of *SEM.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Psychometric Predictive Power of Large Language Models.",
                "author": "Tatsuki Kuribayashi, Yohei Oseki, and Timothy Baldwin. 2023.",
                "venue": "arXiv preprint arXiv:2311.07484.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Can Language Models Handle Recursively Nested Grammatical Structures? A Case Study on Comparing Models and Humans.",
                "author": "Andrew Kyle Lampinen. 2022.",
                "venue": "arXiv preprint arXiv:2210.15303.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Composing and Updating Verb Argument Expectations: A Distributional Semantic Model.",
                "author": "Alessandro Lenci. 2011.",
                "venue": "In Proceedings of the ACL Workshop on Cognitive Modeling and Computational Linguistics.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Probing via Prompting.",
                "author": "Jiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2022.",
                "venue": "In Proceedings of NAACL.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Roberta: A Robustly Optimized BERT Pretraining Approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": "arXiv preprint arXiv:1907.11692.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference.",
                "author": "R Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019.",
                "venue": "arXiv preprint arXiv:1902.01007.",
                "url": null
            }
        },
        {
            "39": {
                "title": "How Well Does Surprisal Explain N400 Amplitude Under Different Experimental Conditions?",
                "author": "James A Michaelov and Benjamin K Bergen. 2020.",
                "venue": "In Proceedings of CONLL.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Can Peanuts Fall in Love with Distributional Semantics?",
                "author": "James A Michaelov, Seana Coulson, and Benjamin K Bergen. 2023.",
                "venue": "In Proceedings of CogSci.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently.",
                "author": "Kanishka Misra, Allyson Ettinger, and Kyle Mahowald. 2024.",
                "venue": "arXiv preprint arXiv:2401.06640.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Exploring BERT\u2019s Sensitivity to Lexical Cues using Tests from Semantic Priming.",
                "author": "Kanishka Misra, Allyson Ettinger, and Julia Taylor Rayz. 2020.",
                "venue": "In Findings of EMNLP.",
                "url": null
            }
        },
        {
            "43": {
                "title": "COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models.",
                "author": "Kanishka Misra, Julia Rayz, and Allyson Ettinger. 2023.",
                "venue": "In Proceedings of EACL.",
                "url": null
            }
        },
        {
            "44": {
                "title": "Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs.",
                "author": "MosaicML NLP Team. 2023.",
                "venue": "www.mosaicml.com/blog/mpt-7b.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Probing for Labeled Dependency Trees.",
                "author": "Max M\u00fcller-Eberstein, Rob Van Der Goot, and Barbara Plank. 2022.",
                "venue": "In Proceedings of ACL.",
                "url": null
            }
        },
        {
            "46": {
                "title": "When Peanuts Fall in Love: N400 Evidence for the Power of Discourse.",
                "author": "Mante S Nieuwland and Jos JA Van Berkum. 2006.",
                "venue": "Journal of Cognitive Neuroscience, 18(7):1098\u20131111.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Training Language Models to Follow Instructions with Human Feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:27730\u201327744.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge.",
                "author": "Paolo Pedinotti, Giulia Rambelli, Emmanuele Chersoni, Enrico Santus, Alessandro Lenci, and Philippe Blache. 2021.",
                "venue": "In Proceedings of *SEM.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Language Models Are Unsupervised Multitask Learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.",
                "venue": "OpenAI Blog, 1(8):9.",
                "url": null
            }
        },
        {
            "50": {
                "title": "The Social N400 Effect: How the Presence of Other Listeners Affects Language Comprehension.",
                "author": "Shirley-Ann Rueschemeyer, Tom Gardner, and Cat Stoner. 2015.",
                "venue": "Psychonomic Bulletin & Review, 22:128\u2013134.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Measuring Thematic Fit with Distributional Feature Overlap.",
                "author": "Enrico Santus, Emmanuele Chersoni, Alessandro Lenci, and Philippe Blache. 2017.",
                "venue": "In Proceedings of EMNLP.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Thematic Fit Evaluation: An Aspect of Selectional Preferences.",
                "author": "Asad Sayeed, Clayton Greenberg, and Vera Demberg. 2016.",
                "venue": "In Proceedings of the ACL Workshop on Evaluating Vector Space Representations for NLP.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Quantifying Language Models\u2019 Sensitivity to Spurious Features in Prompt Design or: How I Learned to Start Worrying about Prompt Formatting.",
                "author": "Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023.",
                "venue": "arXiv preprint arXiv:2310.11324.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Language Model Acceptability Judgements Are Not Always Robust to Context.",
                "author": "Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, and Adina Williams. 2022.",
                "venue": "arXiv preprint arXiv:2212.08979.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Llama: Open and Efficient Foundation Language Models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Grave Edouard, and Guillaume Lample. 2023.",
                "venue": "arXiv preprint arXiv:2302.13971.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Event Knowledge in Sentence Processing: A New Dataset for the Evaluation of Argument Typicality.",
                "author": "Paolo Vassallo, Emmanuele Chersoni, Enrico Santus, Alessandro Lenci, and Philippe Blache. 2018.",
                "venue": "In Proceedings of the LREC Workshop on Linguistic and Neuro-Cognitive Resources (LiNCR).",
                "url": null
            }
        },
        {
            "57": {
                "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English.",
                "author": "Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R Bowman. 2020.",
                "venue": "Transactions of the Association for Computational Linguistics, 8:377\u2013392.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Instruction Tuning for Large Language Models: A Survey.",
                "author": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023.",
                "venue": "arXiv preprint arXiv:2308.10792.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.14859v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "3.4",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2403.14859v1",
        "paper_title": "Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models",
        "research_background": "### Paper's Motivation\n\nThe motivation for this paper stems from the impressive empirical performance of large language models (LLMs) on a variety of language tasks and the increasing integration of these models into everyday life. With this growing use, there is a critical need to reliably assess the capabilities of these models, particularly their understanding of general world knowledge, which can be inferred from the training data's extensive factual and distributional information. Understanding an LLM's capability to estimate the plausibility of sentences, which is seen as a marker of world knowledge, is crucial, especially as interaction mechanisms shift from log likelihood metrics to more user-friendly methods like natural language prompting due to restricted access to log probabilities in newer models.\n\n### Research Problem\n\nThe research problem addressed in this paper revolves around evaluating and comparing the effectiveness of base and instruction-tuned LLMs in estimating the plausibility of sentences. The authors seek to determine which method\u2014log likelihood measures or natural language prompting\u2014provides a more accurate assessment of an LLM\u2019s internal knowledge regarding sentence plausibility. The study explores how instruction-tuning affects the performance of LLMs, particularly in relation to their alignment with human plausibility judgments and their ability to contextualize events.\n\n### Relevant Prior Work\n\nThe paper builds on a broad spectrum of prior work:\n\n- **Empirical successes of LLMs:** Foundational studies on the capabilities of LLMs for diverse language tasks are referenced, such as those by Devlin et al. (2019), Liu et al. (2019), and Brown et al. (2020).\n- **World knowledge in training data:** Discussion by Elazar et al. (2022) and Kang and Choi (2023) on how LLM training data encompasses both factual and distributional knowledge.\n- **Traditional plausibility assessments:** A wealth of studies using log likelihood comparisons on minimal sentence pairs Futrell et al. (2019), Warstadt et al. (2020), Hu et al. (2020; 2024), probing of model representations Hewitt and Manning (2019), Kim et al. (2019), and causal interventions Geiger et al. (2020).\n- **Shift to instruction-tuning:** Recent focus on fine-tuning LLMs to follow instructions, as highlighted by Chung et al. (2022), Touvron et al. (2023), and Almazrouei et al. (2023), showing enhanced alignment with user intent and better generalization.\n- **Direct prompting as a method:** Development of natural language prompting as a way to query LLMs, noted in studies like Li et al. (2022) and Blevins et al. (2023), and the challenges posed by restricted access to log probabilities in newer models.\n- **Limitations and comparisons:** Previous comparisons indicating that natural language prompts might underestimate LLM internal knowledge because they add the complexity of interpreting prompts and translating responses, as discussed by Hu and Levy (2023) and Hu et al. (2024).\n\nThe paper aims to expand on these insights by performing empirical tests to compare these different methods and evaluate their alignment with human plausibility judgments.",
        "methodology": "The methodology focuses on comparing plausibility estimates between base models and instruction-tuned large language models (LLMs). To achieve this, both explicit (prompt-based) and implicit (log-likelihood-based or LL-based) plausibility judgments are used. The comparison is conducted across base and instruction-tuned models drawn from three different model families.\n\nKey components and innovations in this methodology include:\n\n1. **Explicit Plausibility Judgments**: This involves using specific prompts designed to elicit plausibility judgments directly from the LLMs. The models are given a scenario or statement and asked to explicitly assess its plausibility.\n\n2. **Implicit Plausibility Judgments**: This involves assessing plausibility based on the log-likelihood (LL) of the given statements or scenarios. Instead of directly asking the models, this method examines how likely the models are to generate or endorse a particular statement based on their trained distributions.\n\n3. **Comparison Across Model Variations**: The methodology encompasses a comparison between base models (the original LLMs) and their instruction-tuned counterparts (models fine-tuned with additional instructions to follow more complex commands or tasks).\n\n4. **Model Families**: The study includes models from three different families, ensuring that the comparison is robust and not limited to a single type or architecture of LLM.\n\nBy using both explicit and implicit methods of evaluating plausibility and applying these across various types of models, this methodology seeks to provide a comprehensive understanding of how instruction tuning affects the plausibility judgments of LLMs.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n1. **EventsAdapt (AI, impossible)**: A dataset containing sentences where events with animate and inanimate subjects are classified as plausible or implausible.\n2. **EventsAdapt (AA, unlikely)**: A more challenging subset focusing on events involving two animate participants, marked as either plausible or implausible.\n3. **DTFit (AI, unlikely)**: Contains event descriptions to distinguish between plausible and implausible AI events without depending on low-level distributional cues.\n\n**Baselines:**\n- **GPT2-XL**: Serves as the baseline model against which other models' performances are compared.\n\n**Models:**\n- **Falcon Base**\n- **Falcon Instruct**\n- **Mistral Base**\n- **Mistral Instruct**\n\n**Evaluation Metrics:**\n- **LL Scores**: Used to measure the likelihood of sequences as a proxy for plausibility.\n- **Prompt-based methods**: Various prompting configurations evaluated for their consistency in assessing plausibility knowledge.\n- **Human performance**: Benchmarked to determine model effectiveness relative to human plausibility judgment capabilities.\n  \n### Main Experimental Results\n\n1. **LL Scores as Plausibility Measures:**\n   - **Findings**: LL scores align better with plausibility knowledge than prompting methods. However, only certain prompt setups in models like Mistral Instruct show comparable or better performance than LL scores in some scenarios.\n   - **Issues with Prompting**: Models showed significant variability with prompt-based setups, often performing at or below chance levels.\n\n2. **Performance Comparison with Human Judgments:**\n   - **Overall Performance**: All models demonstrated substantial event knowledge, performing above chance level. However, they consistently lagged behind human performance, especially in events involving two animate participants.\n   - **EventsAdapt (AI, impossible)**: Although most models, except Falcon Base, performed worse than humans, they could still distinguish plausible from implausible sentences effectively.\n   - **EventsAdapt (AA, unlikely)**: All models underperformed compared to humans with only Mistral Base showing significant improvement over the baseline.\n   - **DTFit (AI, unlikely)**: LLMs could distinguish plausible from implausible AI events, but all models fell short of human performance. Most LLMs, except Mistral Base and Falcon Instruct, improved over GPT2-XL.\n\n3. **Impact of Instruction Tuning:**\n   - **Plausibility Judgment Alignment**: Instruction-tuned models did not consistently outperform their base counterparts in LL score alignment with human plausibility judgments.\n   - **Dataset-Specific Performance**: Instruction-tuned models performed worse on challenging datasets, notably on EventsAdapt (AA, unlikely), especially struggling with active-voice sentence discernment, indicating a susceptibility to low-level input features."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Compare explicit (prompt-based) and implicit (LL-based) plausibility judgments in base- and instruction-tuned large language models (LLMs).",
            "experiment_process": "In Experiment 1, base and instruction-tuned models from 3 families were tested. The evaluation involved two approaches: explicit prompting and implicit likelihood estimation. The models\u2019 performance on an English sentence plausibility task was assessed using the log likelihood (LL) scores and the zero-shot prompting method. Evaluation tools consisted of plausibility datasets, where the models assigned probabilities to different strings, and comparisons were made to the human performance benchmarks.",
            "result_discussion": "Experiment 1 revealed that log likelihood (LL) scores are the most reliable indicator of sentence plausibility, outperforming zero-shot prompting which yielded inconsistent and typically poor results. Despite this, LL-based performance was still inferior to human performance. Moreover, instruction-tuned models demonstrated worse LL-based performance compared to base models.",
            "ablation_id": "2403.14859v1.No1"
        },
        {
            "research_objective": "Investigate whether large language models (LLMs) appropriately modulate their judgments of event plausibility when provided with different discourse contexts.",
            "experiment_process": "In Experiment 2, English LLMs were tested for context-dependent plausibility judgments. The models\u2019 responses to event plausibility were measured against minimal pair accuracies in the presence of supporting or non-supporting but related single-sentence contexts. The evaluation examined how these context conditions influenced the log likelihood (LL) scores across the different models, thus determining the models' sensitivity to context.",
            "result_discussion": "Experiment 2 findings showed that LLMs modulate their plausibility judgments in context-dependent ways, performing well on three metrics of context-sensitive plausibility. This modulation provided a direct match to explicit human plausibility judgments, suggesting that LL estimates are more reliable for measuring plausibility in LLMs than direct prompting.",
            "ablation_id": "2403.14859v1.No2"
        }
    ]
}