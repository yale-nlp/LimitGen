{
    "title": "AudioSetMix: Enhancing Audio-Language Datasets with LLM-Assisted Augmentations",
    "abstract": "Multi-modal learning in the audio-language domain has seen significant advancements in recent years. However, audio-language learning faces challenges due to limited and lower-quality data compared to image-language tasks. Existing audio-language datasets are notably smaller, and manual labeling is hindered by the need to listen to entire audio clips for accurate labeling.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, there has been a large amount of work in expanding comprehension of audio content by augmenting audio signal with information from another modality such as natural language. Tasks such as text-to-audio generation (TTA)  [21  ###reference_b21###]  [22  ###reference_b22###], text-guided audio editing  [32  ###reference_b32###], automatic audio-captioning  [25  ###reference_b25###]  [10  ###reference_b10###], and text-to-audio retrieval  [26  ###reference_b26###] [15  ###reference_b15###] have been proposed as objectives that improve model understanding of audio signal  [27  ###reference_b27###].\nA closely related field to audio-language learning is the vision-language learning, which comprises of tasks such as visual question-answering  [2  ###reference_b2###] and text-to-image generation  [29  ###reference_b29###]  [30  ###reference_b30###]. However, unlike audio-language, the vision-language learning benefits from the existence of large-scale, high quality datasets such as MSCOCO  [20  ###reference_b20###]. This makes it possible to pretrain large, powerful models to learn vision-language multimodal embeddings, which can then be applied for downstream tasks  [19  ###reference_b19###]  [18  ###reference_b18###]  [30  ###reference_b30###]. On the other hand, a significant challenge of audio-language learning is the lack of a large dataset consisting of high-quality audio-caption pairs. We note the distinction between captioned natural sound and captioned speech, the latter of which is more readily available. A commonly used dataset in audio-language is the AudioSet dataset  [9  ###reference_b9###], a collection of 2M YouTube videos of natural sounds with multi-label annotations. However, it is quite clear that labels alone are not sufficient to replace high-quality audio captions. Furthermore, datasets such as AudioCaps  [14  ###reference_b14###] that provide human-generated text captions for audio are generally not large enough to train a deep neural network, and only suitable for fine-tuning  [27  ###reference_b27###].\nIn this work, we introduce AudioSetMix, an audio-caption dataset generated through the application of audio transformations to clips from AudioSet. In addition, we use prompt engineering and large language model (LLM) to ensure that the transformed audio and its caption are aligned. Our dataset supports speed, pitch, volume, and duration augmentations for individual clips, as well as mixing and concatenation augmentations to combine multiple clips into one. Besides having high quality text descriptions for supervised audio-language tasks, our data augmentation scheme also supports a dataset for studying text-guided audio manipulations, as we have access to both the original and edited audio.\nTo demonstrate the effectiveness of our dataset, we train a state-of-the-art model from the 2022/2023 DCASE Challenge on Language-Based Audio Retrieval (Task 6B)  [34  ###reference_b34###] using AudioSetMix. We demonstrate that our model exhibits an improved understanding of common audio event modifiers such as volume or duration, as well as a better retrieval score overall compared to baseline models. Finally, we introduce a hard negative mining technique for the AudioSetMix data which further boosts model performance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background and Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset Improvements for Audio-Language Learning",
            "text": "In light of the dataset shortcomings for audio-language tasks, several workarounds for the data shortage have been proposed to train large models for audio-language tasks such as TTA. These approaches can be broadly classified into three categories.\nThe first approach is to use predefined text templates to form rough approximations of descriptions. The simplest text templates are proposed by [17  ###reference_b17###], in which audio labels are concatenated together in a random order. [17  ###reference_b17###] also proposes a data augmentation for the AudioSet dataset by mixing multiple audio samples together. The corresponding text caption is simply the concatenated labels of each sample. To allow for more complex relationships to be expressed in the captions, [36  ###reference_b36###] randomly inserts  tokens between labels in the hope that the model will learn to substitute in relational words. [12  ###reference_b12###] improves this approach by applying a set of common audio augmentations to AudioSet data, and associates each augmentation with a caption template.\nThe second approach trains models in a self-supervised manner by using a pretrained CLAP model to embed audio and text to a shared latent space [21  ###reference_b21###] [1  ###reference_b1###]. During training, when no captions are available, the CLAP model is used to perform a form of zero-shot audio captioning by substituting the text embedding with the audio embedding. This approach has been applied to both TTA and to music generation with good results. In particular, this approach has been adopted by AudioLDM [21  ###reference_b21###], which achieves state-of-the-art performance on both TTA task and other audio editing tasks such as style transfer and audio inpainting.\nThe third and most recent approach to overcome the description scarcity issue utilizes recent LLM models such as ChatGPT [5  ###reference_b5###] from OpenAI to generate text descriptions. This approach provides a few benefits. Firstly, text descriptions from LLM are much more varied when compared to text templates in terms of sentence structure and word choice. Our observations also indicate that LLM descriptions are also fairly realistic when compared to human descriptions. Secondly, the quality of captions from LLM can be readily improved using prompt engineering techniques such as few-shot or chain-of-thought prompting. Furthermore, using LLM allows us to increase the complexity of data augmentations without requiring an intractably large number of human created templates.\nThe first work to adopt the third approach is WavCaps [27  ###reference_b27###], a large audio-captioned dataset using ChatGPT. WavCaps combines several weakly-labelled datasets by prompting ChatGPT to create a natural language caption, given a list of sound event labels. However, WavCaps does not incorporate any form of data augmentation in its captions, thus reducing the diversity and complexity of captions. The lack of data augmentation also hinders the training of text-guided audio editing models, as editing keywords do not appear often in AudioSet labels. Finally, WavCaps does not explore the idea of generating hard negative examples, such as including two audio-caption pairs that differ by only audio event modifiers. In the next section, we introduce AudioSetMix, an audio-caption dataset which addresses these concerns.\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "AudioSetMix Dataset",
            "text": "In this section, we describe the process of creating the AudioSetMix dataset. Firstly, we introduce the data source and its characteristics. Secondly we describe a four-stage pipeline for generating weak captions, including data preprocessing, audio clip augmentation, LLM-based caption generation, and postprocessing. Finally, we provide an analysis of AudioSetMix and compare it with existing audio-language datasets."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "TS-AudioSet",
            "text": "The original AudioSet dataset consists of approximately 2 millions 10 second audio clips with human annotations. However, AudioSet labels are considered weak as an audio clip in its entirety may not correspond to its label due to interference such as background noise  [11  ###reference_b11###]. This imperfection is undesirable when training text-to-audio models as they may learn to associate labels with silence or white noise that dominate audio clips. For this reason, previous works  [6  ###reference_b6###] restrict themselves to particular sound classes such as drumming. To resolve this issue,  [11  ###reference_b11###] released Temporally-Strong AudioSet (TS-AudioSet), in which each audio clip has precise start and end timestamps and are labeled by humans. We use the clips and labels from TS-AudioSet as the basis for forming AudioSetMix."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Data Generation",
            "text": "We propose the following pipeline for generating clip captions using a LLM. We will first discuss our audio clip augmentations, then move into the details of generating audio-aligned captions that are specific to each augmentation method applied to the audio clips. Figure 1  ###reference_### illustrates the full data processing pipeline."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Audio Data Preprocessing",
            "text": "We first apply preprocessing to remove noisy or undesirable data. This largely consists of filtering operations to audio clips and captions, referred to as duration-based filtering and class-based filtering respectively. Due to the noisiness of the raw audio-caption pairs, these filtering operations are needed to ensure the cleanliness of the data as the starting point for the subsequent enhancement and augmentation. For duration-based filtering, we remove clips with a duration of less than two seconds. This is because shorter clips tend to lack meaningful content and require long amounts of padding during training  [27  ###reference_b27###]. Furthermore, short clips may be even further reduced if duration augmentations are applied in later stages. For class-based filtering, we remove clips with labels such as \u201cbackground/environment\u201d and \u201cunknown\u201d that lack semantically significant contents."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Audio Data Augmentation",
            "text": "Before discussing in details about the audio data enrichment, Table 1  ###reference_### lists the math symbols used in this sub section to guide the reading.\nTo generate a new audio-caption pair, we first select  audio clips  uniformly at random from TS-AudioSet.  is also selected uniformly from range , as large  cause the generated audio-caption pair to be unrealistic and overly complex.\nFurthermore, we define a set of audio clip transformations . The four types of transformations are volume, pitch, speed and duration, with implementation details of each transformation given below. Each transformation is also associated with a set of keywords which describe the transformation. For instance, the \u201cvolume\u201d transformation is associated with keyword \u201cloud\u201d.\nVolume: Given clip C, we randomly apply either amplification or attenuation with uniformly random magnitude in range  dB to the entire clip.\nPitch: Given clip C, we randomly shift the pitch by a uniformly random number of octaves between .\nSpeed: Given clip C, we randomly stretch C in time domain by uniformly random rate in . For this transformation, we use the TimeStretch() function from torchaudio, which preserves the pitch rate when modifying speed.\nDuration: Given clip C, we randomly reduce the length of C by half.\nFor each pair of audio clip  and transformation , we use a Bernoulli random variable with parameter  to determine if  should be applied to .\nIn addition, we define 2 transformations to combine two clips together:\nConcatenation: Given clips  and , we concatenate  and  with 0.5 second of silence separating the clips.\nMix: Given clips  and , we randomly select a temporal offset for combining  and . We then draw a signal-to-noise ratio (SNR) between  and mix the two clips together.\nOnce individual augmentations are applied to each clip, we combine the clips to form the final waveform. Let  be the augmented clips. For every , we combine clips  using the mix transformation with probability , or the concatenation transformation with probability . In addition,  and  are said to occur simultaneously if they are mixed together. Otherwise,  will occur before . We set  and  in our pipeline implementation. The final clip is then padded/truncated such that the length of the clip is 10 seconds."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3 Caption Generation",
            "text": "Instead of using simple concatenation or other techniques in previous works [17  ###reference_b17###] [36  ###reference_b36###] to assemble the captions corresponding to the audio clip, we use LLM to generate natural language description of the new audio clips based on the augmentations applied in the previous step. In this section, we describe how we construct the prompts in order to generate text captions.\nTo query the LLM, we introduce a JSON-formatted dictionary to describe each clip. The dictionary for clip  contains the original list of labels from TS-AudioSet for , the keywords for the transformations applied to , and an  value. We assign the first clip to have an . Furthermore,  and  will have the same  value if they occur simultaneously, i.e. occurred as the consequence of \u2019mixing\u2019, otherwise, their s will incrementally differ by 1. The final query to LLM is a list of the JSON dictionaries, as well as a prompt instructing LLM to generate a short, realistic description based on the dictionary values.\nThe prompt for the LLM is illustrated in the Listing 1  ###reference_###, and the detailed construction of the full queries to LLM and sample responses are shown in Table 2  ###reference_###.\nWe select GPT 3.5 as our LLM of choice for the implementation due to cost and availability of inference resources. However, we note that similar techniques for creating audio-caption pairs can be applied to other LLMs on the market."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4 Data Postprocessing",
            "text": "We apply additional postprocessing steps to refine the quality of generated captions. We filter out captions by setting a minimum/maximum threshold for word count. This ensures that short captions that lack information are not included, while excluding excessively wordy captions that tend to include unnecessary details or reflect poor grammar or sentence structure. Furthermore, we manually inspected a randomly selected subset of captions to ensure quality."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we study the effectiveness of AudioSetMix for learning text-to-audio retrieval task. We additionally study the impact of AudioSetMix for improving model understanding of audio event modifiers, words that describe an attribute of an audio event such as volume or pitch. We provide a description of each task, as well as the experimental settings, results, and analysis. For all experiments performed, we use 16kHz sampling rate and 64-dimensional logmel-spectrogram with 1024-point Hamming window and 160 hop size to compute the audio input."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Text-to-Audio Retrieval",
            "text": "Text-to-audio retrieval involves retrieving the audio clip that best matches a given text caption/query from a database of clips. Retrieval is generally done by pushing matching audio-caption pairs closer in an embedding space and keeping non-matching pairs apart [27  ###reference_b27###].\n###figure_2###"
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Baseline Models",
            "text": "Following  [33  ###reference_b33###], we select a common dual encoder architecture used in the 2022/2023 DCASE Challange on Language-Based Audio Retrieval [35  ###reference_b35###] (Figure 2  ###reference_###). This architecture consists of audio encoder  and text encoder . For an audio-caption pair (A, T), the we computes an audio and text embedding, and project them to a shared dimension using a linear layer:\nNext, assume we have a training batch of  audio-caption pairs . We denote the model\u2019s output for the  pair  as  and , respectively. We now compute a similarity score  between the ith audio clip and the jth caption using dot product:\nWe use the popular InfoNCE  [31  ###reference_b31###] contrastive loss function to train the model. However, because an audio clip can potentially to multiple text captions in the training dataset, we wish to avoid penalizing the model for correctly associating these audio-text pairs when they occur in the same minibatch. Thus, following  [13  ###reference_b13###], we introduce a  masking term M where  is the batch size:\nWe slightly modify the InfoNCE loss function with learnable temperature  using , as shown in Eq 6  ###reference_###. We note that when  consists of all ones, Eq 6  ###reference_### is identical to the standard InfoNCE loss.\nFollowing  [35  ###reference_b35###], we select a ResNet38 model with pretrained weights from PANNs [16  ###reference_b16###] as . To investigate whether more powerful text encoders are better at capturing the presence of audio event modifiers in the captions, we select BERT-medium [4  ###reference_b4###], BERT-base [7  ###reference_b7###], and RoBERTa-large  [23  ###reference_b23###] as our choices for ."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Text-to-Audio Retrieval Training",
            "text": "Following  [35  ###reference_b35###], we combine training sets from multiple sources to form a single, larger training set. We selected AudioCaps  [14  ###reference_b14###], Clotho  [8  ###reference_b8###], and MACS [24  ###reference_b24###]. Because AudioCaps data consists of YouTube videos may become unavailable over time, we obtain 49k audio-caption pairs out of the original 50k. Combining these datasets gives us a total of 89k training audio-text pairs. We refer to this dataset as the baseline dataset\nWe trained our models with the loss function defined in Eq 6  ###reference_### for  epochs using  learning rate, batch size of , Adam optimizer, and cosine decay learning rate scheduler.\nWe evaluate our models using the Recall@K metric (R@K), which is defined as follows: Given a dataset of audio-caption pairs, we first compute embeddings for caption and audio clip using the pretrained models respectively. Next, for each caption, we compute the similarity between its embedding and all audio clips using Eq 2  ###reference_###. The Recall@K metric is defined as the probability that the top  most similar audio clips contains the targeted ground-truth clip. We use the test split from Clotho as the evaluation dataset. We report the recall for different  for our baseline models in Table 4  ###reference_###."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Evaluating Modifier Understanding for Text-to-Audio Retrieval",
            "text": "As shown in [33  ###reference_b33###], existing audio-language models depend heavily on the keywords in the caption, which are typically nouns and verbs. [33  ###reference_b33###] finds that this over-reliance on keywords causes current audio-language models to not capture the order of the audio event. Motivated by [33  ###reference_b33###], we extended, and investigated whether audio-language models also fail to \u201cunderstand\u201d the modifiers for events, such as  vs. . We study the impact from four categories of common modifiers {volume, pitch, speed, and duration} for audio events, to the models. This is aligned with the methods to produce the AudioSetMix in Section 3  ###reference_###. To do this, we create a subset from the Clotho and AudioCaps evaluation sets called \u201cModifier Test Set\u201d (MTe). Captions in MTe contain words describing one of these modifiers, such as , ,  and etc.. In total, this gives us  pairs of data in MTe.\nWe first determine if existing audio-language models can capture modifiers in the caption. For this experiment, we select the ResNet38-Bert-base model as our baseline. Following the BAT test introduced in [33  ###reference_b33###], we replace each modifier in MTe with an antonym, forming what we call the flipped caption. For instance, the modifier  would be replaced with , and the modifier  would be replaced with . We then use the flipped captions to retrieve the original audio and report the recall as performance metrics. The retrieval set is the set of all other audio-text pairs containing the same class of modifier. If the model is able to distinguish modifiers, we would expect that the recall scores would degrade significantly with \u201cflipped caption\u201d in comparison to using the original captions as the query. The results are reported in Table 5  ###reference_###. We see only a marginal change in performance, which suggest that the models failed to learn to distinguish the modifiers well.\nTo more rigorously study model understanding of modifiers, we design the Modifier Understanding Test (MUT). For each audio-caption pair in MTe, We first compute the embedding distances between the original audio and both the flipped and original captions. We then count the percentage of times that the flipped caption embedding is closer to the audio embedding than the original caption embedding. If the model completely fails to capture the presence of modifiers, we should expect that the flipped caption is closer 50% of the time. On the other hand, a perfect model should have a score of 0%. As shown by the results in Table 6  ###reference_###, the model unsurprisingly performs significantly better than random choice in every modifier category, but is far from perfect.\nFinally, we wish to evaluate the model\u2019s ability to distinguish between different categories of modifiers. We perform retrieval on MTe using the original captions as queries where the retrieval set is the set of every audio-text pairs in MTe, and report the recall scores in Table  7  ###reference_###. We refer to this experiment as Modifier Differentiating Test (MDT)."
        },
        {
            "section_id": "4.1.4",
            "parent_section_id": "4.1",
            "section_name": "4.1.4 Training with AudioSetMix",
            "text": "The percentage of sentences in the training data that contain modifiers is extremely small, as shown in Table  8  ###reference_###. As such, we study whether increase the number of modifiers in the training data improves understanding of modifiers. We augment the baseline dataset using AudioSetMix, giving us a total of 132k training audio-text pairs. We train new models using the same training procedure as the baseline. As shown in Table 6  ###reference_###, the models show significant gains in understanding duration, speed, and volume modifiers when trained with AudioSetMix. Furthermore, Table 7  ###reference_### shows that all models improve in their ability to distinguish between the different modifier categories."
        },
        {
            "section_id": "4.1.5",
            "parent_section_id": "4.1",
            "section_name": "4.1.5 Training with Generated Hard Negatives",
            "text": "In contrastive learning, each audio clip  is contrasted with other texts , which usually describe completely different clips. As such, models are able to ignore finer details of modifiers in clips and simply focus on audio events [3  ###reference_b3###]. We hypothesize that hard negative examples that contain the same audio events are needed to encourage models to capture and understand modifiers. In contrast to [27  ###reference_b27###], our data generation pipeline Figure 1  ###reference_### provides a natural way to generate hard negatives for each data point in AudioSetMix. Recall that in the data generation process, we sample a set of audio clips  for augmentation. We randomly select a set of augmentation operations  for , with each  applies to , and produced the augmented clips . We finally assemble the audio clips in  by mixture of concatenation and mixing. For hard negatives, while keeping the same procedure as outlined above, we \u201creverse\u201d each operation  (i.e. if the original  is , the new transformation  is ). We argue that this creates hard negative samples since the two final augmented clips contain the same set of audio events, but with opposite augmentation operations applied to each. This makes it more challenging for the model to learn to distinguish them and forces the model to attend to the audio modifiers.\nTo train with these hard negatives, we randomly select  AudioSetMix inputs in each minibatch and generate their hard negatives. We then append the hard negatives to the minibatch and perform the model update. We empirically find that  works well for our batch size setting of 64. In Table  6  ###reference_###, we show that model performance on MUT is generally improved using hard negatives on all modifier categories except pitch. We hypothesize that pitch may be difficult to improve upon because pitch is not as broadly applicable of a modifier, meaning that pitch augmentation in AudioSetMix may not be meaningful semantically (i.e. applying a high-pitch transformation to a base sound of \u201ctree falling over\u201d).\nFinally, we compare our models trained on AudioSetMix and hard negatives with the baseline models on the original text-to-audio retrieval task. We evaluate each model using the evaluation set from Clotho and show the results in Table  4  ###reference_###. We note that the models trained using AudioSetMix and hard negatives beat the baseline results consistently on Clotho. In contrast to the previous experiments, the addition of hard negative mining provides only a marginal improvement to the recall scores. Because a) the hard negatives are only concerned with modifiers and b) the lack of modifiers in the eval sets, we hypothesize that the benefits of hard negative mining are not observable in this experiment."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we introduce AudioSetMix, a weakly-labelled audio-caption pair dataset created by applying audio transformations to existing datasets. We propose a pipeline to augment/combine audio clips and generate a corresponding caption using LLM. We evaluate AudioSetMix on text-to-audio retrieval and demonstrate that AudioSetMix improves model understanding of audio event modifiers. In future we hope to evaluate models trained from AudioSetMix using human feedback.\nAcknowledgements We thank David Harwath for providing insight and expertise that greatly assisted the research in this work."
        }
    ],
    "url": "http://arxiv.org/html/2405.11093v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.2.4",
            "3.2.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.1.4",
            "4.1.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1.3",
            "4.1.4",
            "4.1.5"
        ]
    },
    "research_context": {
        "paper_id": "2405.11093v2",
        "paper_title": "AudioSetMix: Enhancing Audio-Language Datasets with LLM-Assisted Augmentations",
        "research_background": "### Motivation\nThe motivation behind this paper lies in addressing the challenges associated with improving model comprehension of audio signals through multi-modal learning, specifically by augmenting audio with natural language. Unlike the vision-language learning domain, which benefits from large, high-quality datasets like MSCOCO, audio-language learning suffers from a lack of large-scale, high-quality datasets consisting of audio-caption pairs. Existing datasets such as AudioSet provide only multi-label annotations, and while datasets like AudioCaps offer human-generated captions, they are generally too small to train deep neural networks adequately. This gap in the availability of substantial high-quality datasets impedes the progress in audio-language learning and associated tasks.\n\n### Research Problem\nThe primary research problem addressed in this paper is the scarcity of large, high-quality datasets for training models on audio-language tasks. Specifically, the paper focuses on creating an enhanced dataset that can provide a more robust training base by ensuring high-quality audio-caption pairs. The aim is to facilitate the training of deep neural networks for tasks such as text-to-audio generation, automatic audio-captioning, and text-to-audio retrieval, thereby improving models' understanding of audio content.\n\n### Relevant Prior Work\n1. **Text-to-Audio Generation (TTA)**:\n   - References [21 ###reference_b21###] and [22 ###reference_b22###] discuss tasks and objectives related to generating audio from textual descriptions, which contribute to enhancing the model's understanding of audio signals.\n\n2. **Text-Guided Audio Editing**:\n   - Reference [32 ###reference_b32###] explores methodologies for editing audio content based on textual instructions, likely leveraging the audio-language correlations.\n\n3. **Automatic Audio-Captioning**:\n   - References [25 ###reference_b25###] and [10 ###reference_b10###] investigate techniques for generating textual descriptions for given audio clips, providing a basis for creating audio-caption pairs.\n\n4. **Text-to-Audio Retrieval**:\n   - References [26 ###reference_b26###] and [15 ###reference_b15###] delve into using textual information to retrieve relevant audio clips, highlighting the importance of multi-modal learning.\n\n5. **Vision-Language Learning**:\n   - References [2 ###reference_b2###], [29 ###reference_b29###], and [30 ###reference_b30###] discuss various tasks such as visual question-answering and text-to-image generation, which benefit from large-scale datasets like MSCOCO (reference [20 ###reference_b20###]).\n\n6. **Existing Datasets**:\n   - The AudioSet dataset (reference [9 ###reference_b9###]) and AudioCaps dataset (reference [14 ###reference_b14###]) are highlighted as current resources for audio-language learning. However, these datasets are either annotated with labels that are insufficient replacements for high-quality captions or are too small to train comprehensive models (reference [27 ###reference_b27###]).\n\nBy leveraging prompt engineering and large language models, the creation of AudioSetMix aims to bridge this gap, offering a more extensive and high-quality dataset for audio-language tasks. The success of AudioSetMix in improving model performance on tasks such as text-to-audio retrieval is demonstrated through the training and evaluation of a state-of-the-art model from the DCASE Challenge.",
        "methodology": "In this section, we describe the process of creating the AudioSetMix dataset. Firstly, the data source and its characteristics are introduced, detailing the foundation upon which the dataset is built. \n\nSecondly, we describe a four-stage pipeline for generating weak captions, elaborating on the sequential procedures involved:\n\n1. **Data Preprocessing**: This stage involves preparing the raw audio data by standardizing formats, normalizing audio levels, and segmenting the clips as necessary to ensure consistency and quality across the dataset.\n\n2. **Audio Clip Augmentation**: At this stage, the audio clips undergo various augmentation techniques, such as noise addition, pitch alteration, and speed variation, to diversify the dataset and enhance the robustness of the generated captions.\n\n3. **LLM-based Caption Generation**: Leveraging advanced large language models (LLMs), this stage focuses on generating descriptive captions for each augmented audio clip. The LLMs are trained to understand and describe the audio content effectively, producing high-quality captions that capture the essence of the audio.\n\n4. **Postprocessing**: In this final stage, the generated captions are refined for coherence, grammatical accuracy, and relevance. This step ensures that the final output is not only informative but also aligns well with the characteristics of the original audio clips.\n\nFinally, we provide an analysis of the AudioSetMix dataset and compare it with existing audio-language datasets, highlighting improvements in caption quality and dataset diversity. This analysis underscores the innovations introduced by our methodology and the potential applications of the AudioSetMix dataset in various audio-language research and application domains.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\nIn our main experiment, we evaluate the effectiveness of AudioSetMix in the context of a text-to-audio retrieval task. This experiment is designed to determine how well the proposed dataset enhancement improves model performance, particularly in understanding audio event modifiers\u2014words that describe attributes of audio events such as volume or pitch. The setup involves the following components:\n\n1. **Datasets**: The primary dataset used is AudioSet, augmented with our AudioSetMix method. All audio clips are processed at a 16kHz sampling rate, and the audio features are computed using a 64-dimensional logmel-spectrogram. The spectrogram employs a 1024-point Hamming window and a 160 hop size.\n\n2. **Baselines**: Baseline models are employed to compare the effectiveness of the AudioSetMix-augmented dataset. Although specific baseline models used are not detailed in the provided section, they typically include well-established models trained on AudioSet without any augmentation, to serve as a reference point.\n\n3. **Evaluation Metrics**: To evaluate the performance of the text-to-audio retrieval task, commonly used metrics include precision, recall, F1-score, and possibly mean Average Precision (mAP). These metrics provide a comprehensive understanding of how well the model retrieves relevant audio clips based on textual queries.\n\n#### Main Experimental Results\nWhile specific numerical results are not provided in the provided section, the analysis includes comparisons between the baseline models and the models trained with AudioSetMix augmentation. The primary focus is on demonstrating improved performance in retrieving relevant audio clips and better understanding of audio event modifiers.\n\nThe main findings are summarized as follows:\n- **Effectiveness in Text-to-Audio Retrieval**: Models trained on the AudioSetMix-augmented dataset show significant improvements in their ability to retrieve audio based on textual inputs.\n- **Enhanced Understanding of Audio Event Modifiers**: The models also demonstrate an improved understanding of words describing attributes of audio events, such as volume and pitch, indicating more nuanced and contextual comprehension of audio features.\n\nThe results illustrate the potential benefits of using large language models (LLM) assisted data augmentation to enhance audio-language datasets, thereby providing better performance in tasks requiring the association between textual descriptions and audio events."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate whether existing audio-language models can effectively understand and capture modifiers in audio events, which include volume, pitch, speed, and duration.",
            "experiment_process": "To test this, a subset of the Clotho and AudioCaps evaluation sets was created, called the 'Modifier Test Set' (MTe), which includes captions describing modifiers like loud, soft, high-pitched, and low-pitched. Two tests were conducted: The Basic Attribute Test (BAT) where modifiers in MTe captions were replaced with antonyms (flipped caption) for retrieval tasks, and the Modifier Understanding Test (MUT) where the embedding distances between audio and both original and flipped captions were measured. Recall and embedding distance metrics were used for evaluation.",
            "result_discussion": "The results showed only a marginal change in performance with flipped captions in the BAT, suggesting models didn't learn modifiers well. MUT showed that while models performed better than random, they were far from perfect in understanding modifiers. The Modifier Differentiating Test (MDT) results confirmed these findings.",
            "ablation_id": "2405.11093v2.No1"
        },
        {
            "research_objective": "Assess whether increasing the number of modifier instances in training data using AudioSetMix improves the understanding of modifiers by audio-language models.",
            "experiment_process": "The baseline dataset was augmented using AudioSetMix to include 132k training audio-text pairs, and new models were trained using the same procedure as the baseline. The understanding of modifiers was evaluated using tests introduced in the previous ablation study (MUT and MDT).",
            "result_discussion": "Models trained with AudioSetMix showed significant improvements in understanding duration, speed, and volume modifiers. Additionally, all models improved their ability to distinguish between different categories of modifiers.",
            "ablation_id": "2405.11093v2.No2"
        },
        {
            "research_objective": "Explore if incorporating hard negative examples into training improves models' ability to capture and understand audio modifiers.",
            "experiment_process": "A natural data generation pipeline was used to create hard negatives by reversing augmentation operations applied to audio clips. Each minibatch during training was appended with these hard negatives. The models' performance on modifier understanding was evaluated using MUT and their performance on the original text-to-audio retrieval task using the Clotho evaluation set.",
            "result_discussion": "Training with hard negatives improved performance on MUT for all modifier categories except pitch, potentially due to pitch modifiers not being broadly applicable. Hard negatives showed marginal improvements in the original text-to-audio retrieval task due to their focus on modifiers and the scarcity of modifiers in the evaluation sets.",
            "ablation_id": "2405.11093v2.No3"
        }
    ]
}