{
    "title": "SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech",
    "abstract": "The interpretation of human voices holds importance across various applications. This study ventures into predicting age, gender, and emotion from vocal cues, a field with vast applications. Voice analysis tech advancements span domains, from improving customer interactions to enhancing healthcare and retail experiences. Discerning emotions aids mental health, while age and gender detection are vital in various contexts. Exploring deep learning models for these predictions involves comparing single, multi-output, and sequential models highlighted in this paper. Sourcing suitable data posed challenges, resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work showed promise in individual predictions, but limited research considered all three variables simultaneously. This paper identifies flaws in an individual model approach and advocates for our novel multi-output learning architecture Speech-based Emotion Gender and Age Analysis (SEGAA) model. The experiments suggest that Multi-output models perform comparably to individual models, efficiently capturing the intricate relationships between variables and speech inputs, all while achieving improved runtime.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In our increasingly digital world, the ability to glean profound insights from the nuances of human voices has assumed paramount importance. This paper delves into the captivating domain of predicting age, gender, and emotion based on vocal cues, a multidisciplinary field teeming with far-reaching applications.\nVoice analysis technologies have rapidly advanced, bringing transformative breakthroughs to various fields [1  ###reference_b1###]. These advancements not only optimize customer interactions but also hold the potential to revolutionize healthcare diagnostics, fundamentally altering our comprehension and engagement with human communication. In mental healthcare, the ability to discern emotions offers an opportunity to improve emotional and behavioral disorders [2  ###reference_b2###, 3  ###reference_b3###]. Moreover, the application of this technology extends to the retail sector, where it enhances the consumer experience [4  ###reference_b4###]. Simultaneously, the ability to detect age and gender variables finds relevance in evaluating the mental health requirements of distinct demographic groups [5  ###reference_b5###]. It also plays a crucial role in e-services and policy formulation [6  ###reference_b6###]. Thus, the ability to discern age, gender, and emotion from voice data emerges as a multifaceted field with a plethora of discernible applications."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Prior art",
            "text": "This research endeavour embarks on a comprehensive exploration of advanced deep learning architectures tailored for the prediction of age, gender, and emotional states. Additionally, it undertakes an exhaustive comparative analysis, meticulously examining diverse methodologies. These methodologies include individual models, where a single model is used to predict a single variable; multi-output models, which employ a single model to predict all three variables simultaneously; and sequential models, which cascade individual models to create a sequence representing the three variables at distinct stages. This scrutiny aims to elucidate the effectiveness of these approaches in the vital task of detecting these three pivotal variables.\nBefore beginning the experiments, addressing the critical challenge of sourcing a dataset that encompasses all three target labels was necessary. Consequently, a rigorous review of popular, openly available speech datasets was undertaken to test the models. Datasets like RAVDESS and IEMOCAP [7  ###reference_b7###, 8  ###reference_b8###] were found to have emotion labels but lacked age and gender annotations. The TESS dataset [9  ###reference_b9###] featured only two speakers, rendering it unsuitable for our purposes despite including all three variables. Similarly, the DES dataset [10  ###reference_b10###] comprised just four speakers, limiting its utility. The Common Voice dataset [11  ###reference_b11###] had age and gender labels but omitted emotion labels. As a result, the CREMA-D and EMO-DB datasets [12  ###reference_b12###, 13  ###reference_b13###] were identified as the only suitable sources containing all three labels and adequate data. An aggregate dataset was created by amalgamating these two datasets to facilitate our experiments.\nPrior research has produced remarkable results in predicting individual variables using dedicated models. For instance, [14  ###reference_b14###, 15  ###reference_b15###] achieved impressive accuracy rates of 90.47% and 92.73% on CREMA-D for emotion prediction using 1D CNNs and other deep learning architectures like GRU and LSTMs. Meanwhile, [16  ###reference_b16###] attained an accuracy of 76.83% using their copy-pasta architecture. Furthermore, [17  ###reference_b17###] detailed a multilingual speech-based gender classification method employing time-frequency features and the SVM classifier, achieving 81% classification accuracy on the EMO-DB dataset for gender classification. Several studies [18  ###reference_b18###, 15  ###reference_b15###] have explored the prediction of age and gender from speech, employing diverse methodologies involving MLPs and CNNs. Notably, limited research has been conducted on predicting all three variables\u2014namely, age, emotion, and gender\u2014from speech data.\nWhile [19  ###reference_b19###] proposed a \"one-source-to-detect-all\" solution for predicting age, gender, and emotion from one model, we identified methodological flaws in their approach. Specifically, these studies utilized three separate datasets to train three distinct tasks, a strategy that may not be optimal for enabling a model to discern all three variables from a single speech source.\nThe motivation behind employing multi-output models lies in their ability to concurrently specify the relationships between multiple outcome variables (Y) and feature variables (X). As underscored by [20  ###reference_b20###], this approach acknowledges the potential interdependencies between these outcome variables, resulting in enhanced statistical power or improved predictive accuracy. Moreover, [20  ###reference_b20###] observed that variations in dependency structures within the covariates influenced the performance of predictive methods. Notably, this impact was consistent for both univariate and multivariate approaches, indicating that neither approach held a definitive advantage concerning the dependency structures within the covariates.\nGiven these considerations, this study initiates a series of experiments to determine whether multi-output models or univariate approaches more effectively classify age, gender, and emotion from speech. This paper aims to contribute valuable insights to the ongoing discourse surrounding the optimal approach for addressing this multifaceted predictive task.\nThe rest of the article is structured as follows: Section 2 details the methodology of various models used in this experiment, discusses the database and explains the feature extraction along with our novel SEGAA model. Section 3 elaborates on the results based on the experimental study. Section 4 presents the conclusion of this article."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers.\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which validation accuracy metrics for gender, emotion, and age predictions are continuously monitored.\n###figure_1### The network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling.\nAfter these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%. The hyperparameters include a convolutional kernel size of 5, a dropout rate of 20%, the utilization of the Adam optimizer for efficient optimization, and the implementation of an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also implemented, featuring a patience of 3 epochs and a reduction factor of 0.5.\nThe models employed for \u2019emotion\u2019 and \u2019age\u2019 predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the \u2019gender\u2019 prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\nThis model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization. The model undergoes training for 200 epochs with a batch size of 16, and validation accuracy metrics are continuously monitored. Early stopping and learning rate reduction callbacks are strategically used to ensure optimal convergence.\nThe models employed for \u2019emotion\u2019 and \u2019age\u2019 predictions utilize softmax activation functions in their final layers, comprising six neurons. In contrast, the \u2019gender\u2019 prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\n###figure_2### The devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons as described in figure 4, and employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model is trained over 200 epochs with a batch size of 32 while monitoring accuracy metrics for gender, emotion, and age predictions on both training and validation datasets.\n###figure_3### In this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and drop out at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of 1, along with batch normalization and max pooling, completes the cascade.\nFollowing these layers, a flatten operation leads to a shared densely connected layer composed of 32 neurons, each enhanced with batch normalization and dropout set at a rate of 20%. Three distinct output layers then come into play, with softmax activation applied for \u2019emotion\u2019 and \u2019age\u2019 predictions and binary softmax activation for \u2019gender\u2019 predictions. The hyperparameters governing the model\u2019s effectiveness were diligently selected: a convolutional kernel size of 5, dropout rate of 20%, utilization of the Adam optimizer for efficient optimization, and an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also embedded, with patience of 3 epochs and a reduction factor of 0.5.\nThe architecture consists of three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, promoting feature extraction and dimensionality reduction. Subsequently, a flattened layer aggregates the information, leading to a densely connected layer with 64 neurons, further normalized and regularized with Batch Normalization and Dropout as described in figure 5. Three separate output layers cater to each label category: \u2019emotion,\u2019 \u2019age,\u2019 and \u2019gender,\u2019 utilizing softmax activation for the former two and binary softmax for the latter.\nHyperparameters include a kernel size of 3 and stride of 1 for the Convolutional layers, dropout rates of 0.3, and a Nadam optimizer for optimization. The model is trained over 200 epochs using a batch size of 16, monitored by validation accuracy metrics. Early stopping and learning rate reduction callbacks are utilized to ensure optimal convergence.\n###figure_4###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dataset Description",
            "text": "Given these considerations, this study initiates a series of experiments to determine whether multi-output models or univariate approaches more effectively classify age, gender, and emotion from speech. This paper aims to contribute valuable insights to the ongoing discourse surrounding the optimal approach for addressing this multifaceted predictive task.\nThe EMO-DB database, a publicly accessible German emotional database, was developed by the Institute of Communication Science at the Technical University in Berlin, Germany. The data collection involved ten proficient speakers, equally distributed between genders (five males and five females). The database comprises 535 speech utterances covering seven distinct emotions: anger, boredom, anxiety, happiness, sadness, disgust, and neutrality.\nAlong with the EMO-DB dataset, the CREMA-D dataset was also used. CREMA-D consists of 7,442 authentic speech clips from a diverse group of 91 actors. The actors include 48 males and 43 females, with ages ranging from 20 to 74. They represent diverse racial and ethnic backgrounds, including African American, Asian, Caucasian, Hispanic, and Unspecified. During the recording process, the actors delivered a set of 12 distinct sentences, each expressed with one of six specific emotions: Anger, Disgust, Fear, Happiness, Neutrality, and Sadness. The study focused on emotions, and data samples labelled \"neutral\" were integrated into the \"Neutrality\" class.\nThe primary goal with respect to the data augmentation part of this study is to enhance the diversity and quality of the dataset to improve the performance of emotion recognition models. The age and gender of the speakers were extracted from the speaker information provided and appended to the corresponding data sample. Consequently, our comprehensive dataset comprised a fusion of the EMO-DB and CREMA-D datasets.\nThe study focused on emotions, and data samples labelled \"neutral\" were integrated into the \"Neutrality\" class. Simultaneously, we omitted data samples with the label \"boredom\" as we narrowed our scope to include the following emotions: Anger, Disgust, Fear, Happiness,\nNeutrality, and Sadness.\n###figure_5###"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Feature Extraction / Pre-processing",
            "text": "We implemented audio processing functions to augment the original data samples. These functions include noise addition, time stretching, pitch shifting, and signal shifting. The noise addition process introduces random noise to the audio data, while time stretching alters the duration of the samples. Pitch shifting, on the other hand, modifies the audio pitch, and signal shifting shifts the audio signal along the time axis.\nOur feature extraction pipeline involves calculating essential audio features such as the Zero-Crossing Rate (ZCR), Root Mean Square Energy (RMSE), and Mel-frequency cepstral coefficients (MFCC). These features are widely used in audio and speech processing to capture relevant characteristics of the audio signals.\nThe augmentation techniques were applied to the original audio samples to construct the final dataset. Audio features were extracted for each sample, including the original data and its augmented versions. This process enabled the generation of diverse features, encompassing pitch, duration, and signal properties variations. The resulting dataset showcases a significant increase in size and diversity.\nWe split the dataset into input features (X) and target labels (Y). Y was one-hot encoded to represent the categorical variables. The dataset was then divided into training, testing, and validation sets, using a stratified approach to ensure class distribution balance. Input features were standardized using the Standard Scaler to enhance model convergence and performance. To maintain consistency, the testing and validation sets were transformed using the scaler fitted to the training set. This is then used to train and test the MLP and the SEGAA models, which finally output the age, gender and emotion results as described in Figure 1. The results of the respective models will then be compared and inferred from.\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which validation accuracy metrics for gender, emotion, and age predictions are continuously monitored."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Models",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers.\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which validation accuracy metrics for gender, emotion, and age predictions are continuously monitored.\n###figure_6### The network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling.\nAfter these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%. The hyperparameters include a convolutional kernel size of 5, a dropout rate of 20%, the utilization of the Adam optimizer for efficient optimization, and the implementation of an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also implemented, featuring a patience of 3 epochs and a reduction factor of 0.5.\nThe models employed for \u2019emotion\u2019 and \u2019age\u2019 predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the \u2019gender\u2019 prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\nThis model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization. The model undergoes training for 200 epochs with a batch size of 16, and validation accuracy metrics are continuously monitored. Early stopping and learning rate reduction callbacks are strategically used to ensure optimal convergence.\nThe models employed for \u2019emotion\u2019 and \u2019age\u2019 predictions utilize softmax activation functions in their final layers, comprising six neurons. In contrast, the \u2019gender\u2019 prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\n###figure_7### The devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons as described in figure 4, and employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model is trained over 200 epochs with a batch size of 32 while monitoring accuracy metrics for gender, emotion, and age predictions on both training and validation datasets.\n###figure_8### In this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and drop out at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of 1, along with batch normalization and max pooling, completes the cascade.\nFollowing these layers, a flatten operation leads to a shared densely connected layer composed of 32 neurons, each enhanced with batch normalization and dropout set at a rate of 20%. Three distinct output layers then come into play, with softmax activation applied for \u2019emotion\u2019 and \u2019age\u2019 predictions and binary softmax activation for \u2019gender\u2019 predictions. The hyperparameters governing the model\u2019s effectiveness were diligently selected: a convolutional kernel size of 5, dropout rate of 20%, utilization of the Adam optimizer for efficient optimization, and an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also embedded, with patience of 3 epochs and a reduction factor of 0.5.\nThe architecture consists of three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, promoting feature extraction and dimensionality reduction. Subsequently, a flattened layer aggregates the information, leading to a densely connected layer with 64 neurons, further normalized and regularized with Batch Normalization and Dropout as described in figure 5. Three separate output layers cater to each label category: \u2019emotion,\u2019 \u2019age,\u2019 and \u2019gender,\u2019 utilizing softmax activation for the former two and binary softmax for the latter.\nHyperparameters include a kernel size of 3 and stride of 1 for the Convolutional layers, dropout rates of 0.3, and a Nadam optimizer for optimization. The model is trained over 200 epochs using a batch size of 16, monitored by validation accuracy metrics. Early stopping and learning rate reduction callbacks are utilized to ensure optimal convergence.\n###figure_9###"
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Individual Models",
            "text": "The designed multi-layer perceptron architecture incorporates fully connected layers. It commences with an input layer customized to accommodate the extracted features, followed by several hidden layers. The hidden layers consist of 2048, 1024, 512, and 64 neurons as described in figure 2, equipped with Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to address the overfitting issue. For making predictions specific to each label, distinct activation functions are applied in the output layers.\nSigmoid activation is used for gender prediction with binary cross-entropy loss, while softmax activation is employed for emotion and age prediction with categorical cross-entropy. The optimization process relies on Stochastic Gradient Descent (SGD) with a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model undergoes training for 200 epochs with a batch size of 32, during which validation accuracy metrics for gender, emotion, and age predictions are continuously monitored.\n###figure_10### The network comprises several key layers in this architecture. Initially, there is a convolutional layer featuring 256 filters, a kernel size of 5, and a stride of 1. This layer is augmented with batch normalization and is succeeded by max pooling with a pool size of 5 and a stride of 2. Following this, another convolutional layer has 128 filters, a kernel size of 5, and a stride of 1. This layer incorporates batch normalization, max pooling with a pool size of 5 and a stride of 2, and dropout at a rate of 20%. Finally, the architecture culminates with a convolutional layer employing 64 filters, a kernel size of 5, and a stride of 1. Similar to the previous layers, it includes batch normalization and max pooling.\nAfter these convolutional layers, a flattening operation is performed, leading to a shared densely connected layer comprised of 32 neurons. Each of these neurons benefits from batch normalization and dropout, with a rate set at 20%. The hyperparameters include a convolutional kernel size of 5, a dropout rate of 20%, the utilization of the Adam optimizer for efficient optimization, and the implementation of an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also implemented, featuring a patience of 3 epochs and a reduction factor of 0.5.\nThe models employed for \u2019emotion\u2019 and \u2019age\u2019 predictions utilize softmax activation functions in their final layers, each comprising six neurons. In contrast, the \u2019gender\u2019 prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\nThis model architecture is an improvement made upon the previously mentioned SEGAA Gen-0. The architecture comprises three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, facilitating feature extraction and dimensionality reduction. Subsequently, a Flatten layer consolidates the extracted information, leading to a densely connected layer with 64 neurons, further normalized and regularized using Batch Normalization and Dropout as described in figure 3. Key hyperparameters include a kernel size of 3 and a stride of 1 for the Convolutional layers, dropout rates set at 0.3, and the adoption of the Nadam optimizer for efficient optimization. The model undergoes training for 200 epochs with a batch size of 16, and validation accuracy metrics are continuously monitored. Early stopping and learning rate reduction callbacks are strategically used to ensure optimal convergence.\nThe models employed for \u2019emotion\u2019 and \u2019age\u2019 predictions utilize softmax activation functions in their final layers, comprising six neurons. In contrast, the \u2019gender\u2019 prediction model employs binary softmax activation in its final layer, which consists of 2 neurons.\n###figure_11###"
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Multi-output Models",
            "text": "The devised multi-layer perceptron architecture integrates fully connected layers. The input layer is tailored to accommodate the extracted features, leading to subsequent hidden layers, each with 2048, 1024, 512, and 64 neurons as described in figure 4, and employing Rectified Linear Unit (ReLU) activation functions. A dropout layer with a rate of 0.25 is introduced to alleviate overfitting. For label-specific predictions, output layers employ distinct activation functions: sigmoid for gender prediction and softmax for both emotion and age prediction, reflecting the multi-class nature of these tasks. A stochastic Gradient Descent (SGD) optimizer is adopted, featuring a learning rate of 0.0005, a decay rate of 1e-6, momentum set to 0.9, and Nesterov momentum. The model is trained over 200 epochs with a batch size of 32 while monitoring accuracy metrics for gender, emotion, and age predictions on both training and validation datasets.\n###figure_12### In this architecture, the following layers are used: first, a convolutional layer with 256 filters, kernel size of 5, and stride of 1, augmented with batch normalization and then max pooling with pool size 5 and stride 2. Subsequently, a convolutional layer with 128 filters, kernel size of 5, and stride of 1, combined with batch normalization, max pooling with pool size 5 and stride 2, and drop out at a 20% rate. Lastly, a convolutional layer with 64 filters, kernel size of 5, and stride of 1, along with batch normalization and max pooling, completes the cascade.\nFollowing these layers, a flatten operation leads to a shared densely connected layer composed of 32 neurons, each enhanced with batch normalization and dropout set at a rate of 20%. Three distinct output layers then come into play, with softmax activation applied for \u2019emotion\u2019 and \u2019age\u2019 predictions and binary softmax activation for \u2019gender\u2019 predictions. The hyperparameters governing the model\u2019s effectiveness were diligently selected: a convolutional kernel size of 5, dropout rate of 20%, utilization of the Adam optimizer for efficient optimization, and an early stopping strategy with a patience of 5 epochs. A learning rate reduction strategy was also embedded, with patience of 3 epochs and a reduction factor of 0.5.\nThe architecture consists of three convolutional blocks, each followed by Batch Normalization, Max Pooling, and Dropout layers, promoting feature extraction and dimensionality reduction. Subsequently, a flattened layer aggregates the information, leading to a densely connected layer with 64 neurons, further normalized and regularized with Batch Normalization and Dropout as described in figure 5. Three separate output layers cater to each label category: \u2019emotion,\u2019 \u2019age,\u2019 and \u2019gender,\u2019 utilizing softmax activation for the former two and binary softmax for the latter.\nHyperparameters include a kernel size of 3 and stride of 1 for the Convolutional layers, dropout rates of 0.3, and a Nadam optimizer for optimization. The model is trained over 200 epochs using a batch size of 16, monitored by validation accuracy metrics. Early stopping and learning rate reduction callbacks are utilized to ensure optimal convergence.\n###figure_13###"
        },
        {
            "section_id": "2.3.3",
            "parent_section_id": "2.3",
            "section_name": "2.3.3 Sequential Models",
            "text": "In this experimental setup, the individual models described in Section 3.3.1 were cascaded sequentially to predict the first variable from the speech features, followed by predicting the second variable from the speech features and the previously predicted first variable, and finally predicting the third variable from the speech features and the previously predicted second variable as described in figure 5. This experiment was conducted in three different sequences: first for the variables emotion, gender, and age; then for gender, age, and emotion; and lastly for age, emotion, and gender.\n###figure_14###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "This research investigates the task of speech-based detection of emotion, gender, and age using a diverse range of machine-learning models. Our primary objective is to develop accurate and robust models for classifying these three attributes from audio data. The experimental framework includes individual and multi-output Multi-Layer Perceptron (MLP) as well as SEGAA architectures. Additionally, we investigate unique sequences for predicting these attributes with our SEGAA models."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Individual Models",
            "text": ""
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Emotion Detection",
            "text": "For emotion detection, both individual SEGAA and MLP models exhibit robust performance. The SEGAA model attains an accuracy of 96%, while the MLP model achieves 94% accuracy. These models demonstrate well-balanced precision, recall, and F1 scores, ranging from 0.94 to 0.96."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Gender Detection",
            "text": "The individual models demonstrate exceptional accuracy for gender detection. The SEGAA model attains a flawless accuracy of 100%, while the MLP model achieves 98%. Precision, recall, and F1 scores consistently reach 1.00 for both models."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Age Detection",
            "text": "Age detection, too, benefits from the individual SEGAA and MLP models, with the SEGAA model achieving 95% accuracy and the MLP model reaching 92%. These models display high precision, recall, and F1 scores, ranging from 0.92 to 0.95."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Multi-output Models",
            "text": "Our investigation extends to multi-output SEGAA and MLP models, which tries to simultaneously predict emotion, gender, and age in that order. These models yield competitive results with those of the individual models, with accuracy values spanning from 84% to 99% across generations. Although the multi-output SEGAA Gen-0 model maintains strong precision and F1 scores, there is a slight decrease in recall for certain attributes, particularly in emotion detection. In contrast, the proposed SEGAA model effectively addresses these limitations, demonstrating exceptional performance across all aspects, encompassing emotion, age, and gender detection, which is highlighted in Figure 7."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In our experimentation, we conducted a rigorous comparative assessment of univariate and multi-output models to predict gender, age, and emotion from speech data, a crucial task with applications spanning various domains. Our analysis unveiled a noteworthy phenomenon associated with sequentially chaining univariate models: this approach increased error propagation as the inaccuracies generated by preceding models were amplified in subsequent stages. Despite this, univariate models exhibited slightly superior accuracy and F1 scores performance compared to multi-output models.\nIt is essential to emphasize that our experimental findings suggest that SEGAA demonstrates a level of predictive capability comparable to univariate models. Notably, it excels in capturing the intricate interrelationships between the variables and speech inputs with commendable efficiency. Furthermore, SEGAA achieves this without compromising runtime efficiency, making them an attractive alternative for addressing the complexities of predicting gender, age, and emotion from speech data. These insights, we believe, hold valuable implications for both researchers and practitioners in the field."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>MLP &amp; SEGAA Model Metrics</figcaption>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Sequential Model Metrics</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_flex_size_2 ltx_align_middle\" id=\"S3.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.1\">Target</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.2\">Accuracy</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.3\">Precision</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.4\">Recall</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.5\">F1 Score</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.1.2.2.1\">Individual MLP</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.3.3.1\">Age</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.3.3.2\">92%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.3.3.3\">0.92</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.3.3.4\">0.92</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.3.3.5\">0.92</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.4.1\">Emotion</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.4.2\">94%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.4.3\">0.94</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.4.4\">0.94</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.4.5\">0.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.1\">Gender</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.2\">98%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.3\">0.98</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.4\">0.98</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.5\">0.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.1.6.6.1\">Multi-output MLP</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.7.7.1\">Age</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.7.7.2\">94%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.7.7.3\">0.94</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.7.7.4\">0.94</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.7.7.5\">0.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.8.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.8.8.1\">Emotion</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.8.8.2\">95%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.8.8.3\">0.95</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.8.8.4\">0.95</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.8.8.5\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.9.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.9.9.1\">Gender</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.9.9.2\">98%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.9.9.3\">0.98</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.9.9.4\">0.98</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.9.9.5\">0.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.1.10.10.1\">Individual SEGAA</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.11.11.1\">Age</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.11.11.2\">95%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.11.11.3\">0.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.11.11.4\">0.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.11.11.5\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.12.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.12.12.1\">Emotion</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.12.12.2\">96%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.12.12.3\">0.96</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.12.12.4\">0.96</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.12.12.5\">0.96</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.13.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.13.13.1\">Gender</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.13.13.2\">100%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.13.13.3\">1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.13.13.4\">1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.13.13.5\">1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.14.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.1.14.14.1\">Multi-output SEGAA (Gen-0)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.15.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.15.15.1\">Age</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.15.15.2\">90%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.15.15.3\">0.93</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.15.15.4\">0.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.15.15.5\">0.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.16.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.16.16.1\">Emotion</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.16.16.2\">84%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.16.16.3\">0.86</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.16.16.4\">0.84</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.16.16.5\">0.84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.17.17\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.17.17.1\">Gender</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.17.17.2\">98%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.17.17.3\">0.98</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.17.17.4\">0.98</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.17.17.5\">0.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.18.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.1.18.18.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.18.18.1.1\">Multi-output SEGAA</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.19.19\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.19.19.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.19.19.1.1\">Age</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.19.19.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.19.19.2.1\">94%</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.19.19.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.19.19.3.1\">0.96</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.19.19.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.19.19.4.1\">0.94</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.19.19.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.19.19.5.1\">0.95</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.20.20\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.20.20.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.20.20.1.1\">Emotion</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.20.20.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.20.20.2.1\">95%</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.20.20.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.20.20.3.1\">0.95</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.20.20.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.20.20.4.1\">0.95</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.20.20.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.20.20.5.1\">0.95</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.21.21\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.21.21.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.21.21.1.1\">Gender</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.21.21.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.21.21.2.1\">99%</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.21.21.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.21.21.3.1\">0.99</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.21.21.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.21.21.4.1\">0.99</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.21.21.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.21.21.5.1\">0.99</span></td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<table class=\"ltx_tabular ltx_flex_size_2 ltx_align_middle\" id=\"S3.T2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.2.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.2.1.1.1\">Target</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.2.1.1.2\">Accuracy</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.2.1.1.3\">Precision</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.2.1.1.4\">Recall</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.2.1.1.5\">F1 Score</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.2.2.2.1\">CNN Sequence 1 (Gender \u2192 Age \u2192 Emotion)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.3.3.1\">Gender</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.3.3.2\">99%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.3.3.3\">0.99</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.3.3.4\">0.99</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.3.3.5\">0.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.4.4.1\">Age</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.4.4.2\">94%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.4.4.3\">0.95</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.4.4.4\">0.95</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.4.4.5\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.5.5.1\">Emotion</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.5.5.2\">94%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.5.5.3\">0.94</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.5.5.4\">0.94</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.5.5.5\">0.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.2.6.6.1\">CNN Sequence 2 (Age \u2192 Emotion \u2192 Gender)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.7.7.1\">Age</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.7.7.2\">94%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.7.7.3\">0.94</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.7.7.4\">0.94</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.7.7.5\">0.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.8.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.8.8.1\">Emotion</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.8.8.2\">91%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.8.8.3\">0.91</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.8.8.4\">0.91</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.8.8.5\">0.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.9.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.9.9.1\">Gender</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.9.9.2\">99%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.9.9.3\">0.99</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.9.9.4\">0.99</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.9.9.5\">0.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.2.10.10.1\">CNN Sequence 3 (Emotion \u2192 Gender \u2192 Age)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.11.11.1\">Emotion</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.11.11.2\">95%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.11.11.3\">0.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.11.11.4\">0.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.11.11.5\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.12.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.12.12.1\">Gender</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.12.12.2\">99%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.12.12.3\">0.99</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.12.12.4\">0.99</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.12.12.5\">0.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.13.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.13.13.1\">Age</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.13.13.2\">92%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.13.13.3\">0.93</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.13.13.4\">0.92</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.13.13.5\">0.92</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.14.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S3.T2.2.14.14.1\">MLP Sequence (Emotion \u2192 Gender \u2192 Age)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.15.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.15.15.1\">Emotion</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.15.15.2\">91%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.15.15.3\">0.91</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.15.15.4\">0.91</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.2.15.15.5\">0.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.16.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.16.16.1\">Gender</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.16.16.2\">90%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.16.16.3\">0.91</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.16.16.4\">0.9</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.2.16.16.5\">0.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.17.17\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.2.17.17.1\">Age</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.2.17.17.2\">92%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.2.17.17.3\">0.93</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.2.17.17.4\">0.92</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.2.17.17.5\">0.92</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Sequential Model Metrics</figcaption>\n</figure>",
            "capture": "Table 1: MLP & SEGAA Model Metrics"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00887v1_figure_1.png",
            "caption": "Figure 1: Workflow Diagram"
        },
        "2": {
            "figure_path": "2403.00887v1_figure_2.png",
            "caption": "Figure 2: Model architecture for Individual MLP"
        },
        "3": {
            "figure_path": "2403.00887v1_figure_3.png",
            "caption": "Figure 3: Model architecture for Individual SEGAA"
        },
        "4": {
            "figure_path": "2403.00887v1_figure_4.png",
            "caption": "Figure 4: Model architecture for Multi-output MLP"
        },
        "5": {
            "figure_path": "2403.00887v1_figure_5.png",
            "caption": "Figure 5: Model architecture for Multi-output SEGAA (Speech-based Emotion, Gender, Age Analysis)"
        },
        "6": {
            "figure_path": "2403.00887v1_figure_6.png",
            "caption": "Figure 6: Model architecture for Sequential models"
        },
        "7": {
            "figure_path": "2403.00887v1_figure_7.png",
            "caption": "(a) Age Confusion Matrix"
        },
        "8": {
            "figure_path": "2403.00887v1_figure_8.png",
            "caption": "(b) Emotion Confusion Matrix"
        },
        "9": {
            "figure_path": "2403.00887v1_figure_9.png",
            "caption": "(c) Gender Confusion Matrix"
        },
        "10": {
            "figure_path": "2403.00887v1_figure_10.png",
            "caption": "(a) Age Confusion Matrix"
        },
        "11": {
            "figure_path": "2403.00887v1_figure_11.png",
            "caption": "(b) Emotion Confusion Matrix"
        },
        "12": {
            "figure_path": "2403.00887v1_figure_12.png",
            "caption": "(c) Gender Confusion Matrix"
        },
        "13": {
            "figure_path": "2403.00887v1_figure_13.png",
            "caption": "(a) Age Confusion Matrix"
        },
        "14": {
            "figure_path": "2403.00887v1_figure_14.png",
            "caption": "(b) Emotion Confusion Matrix"
        },
        "15": {
            "figure_path": "2403.00887v1_figure_15.png",
            "caption": "(c) Gender Confusion Matrix"
        }
    },
    "references": [
        {
            "1": {
                "title": "Assessing the Role of Age, Education, Gender and Income on the Digital Divide: Evidence for the European Union.",
                "author": "Anca Elena-Bucea, Frederico Cruz-Jesus, Tiago Oliveira, and Pedro Sim\u00f5es Coelho.",
                "venue": "Information Systems Frontiers, 23(4):1007\u20131021, August 2021.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Behavioral and emotional disorders in children during the COVID-19 epidemic.",
                "author": "Wen Yan Jiao, Lin Na Wang, Juan Liu, Shuan Feng Fang, Fu Yong Jiao, Massimo Pettoello-Mantovani, and Eli Somekh.",
                "venue": "The Journal of pediatrics, 221:264\u2013266, 2020.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Identification of Common Neural Circuit Disruptions in Emotional Processing Across Psychiatric Disorders.",
                "author": "Lisa M. McTeague, Benjamin M. Rosenberg, James W. Lopez, David M. Carreon, Julia Huemer, Ying Jiang, Christina F. Chick, Simon B. Eickhoff, and Amit Etkin.",
                "venue": "American Journal of Psychiatry, 177(5):411\u2013421, May 2020.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Building consumer loyalty through e-shopping experiences: The mediating role of emotions.",
                "author": "Silvia Cachero-Mart\u00ednez and Rodolfo V\u00e1zquez-Casielles.",
                "venue": "Journal of Retailing and Consumer Services, 60:102481, 2021.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Mental health needs among lesbian, gay, bisexual, and transgender college students during the COVID-19 pandemic.",
                "author": "Gilbert Gonzales, Emilio Loret de Mola, Kyle A. Gavulic, Tara McKay, and Christopher Purcell.",
                "venue": "Journal of Adolescent Health, 67(5):645\u2013648, 2020.",
                "url": null
            }
        },
        {
            "6": {
                "title": "E-Commerce and the Factors Affecting Its Development in the Age of Digital Technology: Empirical Evidence at EU\u201327 Level. Sustainability 2022, 14, 101, 2021.",
                "author": "R. M. B\u0103d\u00eercea, A. G. Manta, N. M. Florea, J. Popescu, F. L. Manta, and S. Puiu.",
                "venue": "URL https://www.academia.edu/download/79805943/pdf.pdf.",
                "url": null
            }
        },
        {
            "7": {
                "title": "IEMOCAP: interactive emotional dyadic motion capture database.",
                "author": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan.",
                "venue": "Language Resources and Evaluation, 42(4):335\u2013359, December 2008.",
                "url": null
            }
        },
        {
            "8": {
                "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English.",
                "author": "Steven R. Livingstone and Frank A. Russo.",
                "venue": "PloS one, 13(5):e0196391, 2018.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Toronto emotional speech set (TESS).",
                "author": "Kate Dupuis and M. Kathleen Pichora-Fuller.",
                "venue": "University of Toronto, Psychology Department, 2010.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Design, recording and verification of a Danish emotional speech database.",
                "author": "Inger S. Engberg, Anya Varnich Hansen, Ove Andersen, and Paul Dalsgaard.",
                "venue": "In Fifth European conference on speech communication and technology, 1997.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Common Voice: A Massively-Multilingual Speech Corpus, March 2020.",
                "author": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber.",
                "venue": "URL http://arxiv.org/abs/1912.06670.",
                "url": null
            }
        },
        {
            "12": {
                "title": "A database of German emotional speech.",
                "author": "Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter F. Sendlmeier, and Benjamin Weiss.",
                "venue": "In Interspeech, volume 5, pages 1517\u20131520, 2005.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset.",
                "author": "Houwei Cao, David G. Cooper, Michael K. Keutmann, Ruben C. Gur, Ani Nenkova, and Ragini Verma.",
                "venue": "IEEE transactions on affective computing, 5(4):377\u2013390, 2014.",
                "url": null
            }
        },
        {
            "14": {
                "title": "An ensemble 1D-CNN-LSTM-GRU model with data augmentation for speech emotion recognition.",
                "author": "Md Rayhan Ahmed, Salekul Islam, AKM Muzahidul Islam, and Swakkhar Shatabda.",
                "venue": "Expert Systems with Applications, 218:119633, 2023.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Age and gender recognition using a convolutional neural network with a specially designed multi-attention module through speech spectrograms.",
                "author": "Anvarjon Tursunov, Mustaqeem, Joon Yeon Choeh, and Soonil Kwon.",
                "venue": "Sensors, 21(17):5892, 2021.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Copypaste: An augmentation method for speech emotion recognition.",
                "author": "Raghavendra Pappagari, Jes\u00fas Villalba, Piotr \u017belasko, Laureano Moro-Velazquez, and Najim Dehak.",
                "venue": "In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6324\u20136328. IEEE, 2021.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Multilanguage Speech-Based Gender Classification Using Time-Frequency Features and SVM Classifier.",
                "author": "Taiba Majid Wani, Teddy Surya Gunawan, Hasmah Mansor, Syed Asif Ahmad Qadri, Ali Sophian, Eliathamby Ambikairajah, and Eko Ihsanto.",
                "venue": "In Jessnor Arif Mat Jizat, Ismail Mohd Khairuddin, Mohd Azraai Mohd Razman, Ahmad Fakhri Ab. Nasir, Mohamad Shaiful Abdul Karim, Abdul Aziz Jaafar, Lim Wei Hong, Anwar P. P. Abdul Majeed, Pengcheng Liu, Hyun Myung, Han-Lim Choi, and Gian-Antonio Susto, editors, Advances in Robotics, Automation and Data Analytics, volume 1350, pages 1\u201310. Springer International Publishing, Cham, 2021.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Gender and age group predictions from speech features using multi-layer perceptron model.",
                "author": "Saksham Goyal, Vinay Vasanth Patage, and Sourabh Tiwari.",
                "venue": "In 2020 IEEE 17th India Council International Conference (INDICON), pages 1\u20136. IEEE, 2020.",
                "url": null
            }
        },
        {
            "19": {
                "title": "One source to detect them all: gender, age, and emotion detection from voice.",
                "author": "Syed Rohit Zaman, Dipan Sadekeen, M. Aqib Alfaz, and Rifat Shahriyar.",
                "venue": "In 2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC), pages 338\u2013343. IEEE, 2021.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?, January 2022.",
                "author": "Lena Schmid, Alexander Gerharz, Andreas Groll, and Markus Pauly.",
                "venue": "URL http://arxiv.org/abs/2201.05340.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00887v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.3.1",
            "2.3.2",
            "2.3.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2",
            "3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.3.1",
            "2.3.2",
            "2.3.3",
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2",
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.00887v1",
        "paper_title": "SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech",
        "research_background": "### SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech\n\n#### Introduction\n\nIn our increasingly digital world, the ability to glean profound insights from the nuances of human voices has assumed paramount importance. This paper delves into the captivating domain of predicting age, gender, and emotion based on vocal cues, a multidisciplinary field teeming with far-reaching applications.\n\nVoice analysis technologies have rapidly advanced, bringing transformative breakthroughs to various fields [1 ###reference_b1###]. These advancements not only optimize customer interactions but also hold the potential to revolutionize healthcare diagnostics, fundamentally altering our comprehension and engagement with human communication. In mental healthcare, the ability to discern emotions offers an opportunity to improve emotional and behavioral disorders [2 ###reference_b2###, 3 ###reference_b3###]. Moreover, the application of this technology extends to the retail sector, where it enhances the consumer experience [4 ###reference_b4###]. Simultaneously, the ability to detect age and gender variables finds relevance in evaluating the mental health requirements of distinct demographic groups [5 ###reference_b5###]. It also plays a crucial role in e-services and policy formulation [6 ###reference_b6###]. Thus, the ability to discern age, gender, and emotion from voice data emerges as a multifaceted field with a plethora of discernible applications.\n\n---\n\n**Motivation:**\nThe motivation behind this study is the rapid advancement in voice analysis technologies and their potential for transformative breakthroughs across various fields. These advancements can revolutionize healthcare diagnostics, optimize customer interactions, and enhance consumer experiences. Specifically, in mental healthcare, the capacity to discern emotions from vocal cues presents opportunities to improve emotional and behavioral disorders, while detecting age and gender can aid in evaluating mental health requirements for different demographic groups and inform e-services and policy formulation.\n\n**Research Problem:**\nThe primary research problem addressed in this paper is the unified prediction of age, gender, and emotion from voice data. Despite the versatile applications of voice analysis, there is a need for an integrated approach that combines these elements for a comprehensive understanding of vocal cues.\n\n**Relevant Prior Work:**\n1. **Voice Analysis Technologies:** Previous advancements in this area have optimized customer interactions and revolutionized healthcare diagnostics [1].\n2. **Mental Healthcare:** Studies have shown the utility of emotion detection for improving emotional and behavioral disorders [2, 3].\n3. **Retail Sector:** Vocal cue analysis has been applied to enhance consumer experiences in the retail industry [4].\n4. **Demographic Analysis:** Research has highlighted the importance of detecting age and gender for assessing mental health requirements of various demographic groups [5].\n5. **E-Services and Policy Formation:** The relevance of voice analysis in the formulation of e-services and public policies has also been recognized [6].\n\nThese prior studies underscore the diverse applications and importance of voice analysis technologies, setting the stage for developing a more unified and integrated approach to predicting age, gender, and emotion.",
        "methodology": "### SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech\n\n**Methodology:**\n\n1. **Multi-Layer Perceptron Architecture**:\n    - The model starts with an input layer that is designed to handle the extracted features.\n    - It includes several hidden layers with 2048, 1024, 512, and 64 neurons, each using Rectified Linear Unit (ReLU) activation functions. \n    - To prevent overfitting, a dropout layer with a rate of 0.25 is incorporated.\n    - Different activation functions are used in the output layers for specific predictions: \n        - **Sigmoid activation** for gender prediction using binary cross-entropy loss.\n        - **Softmax activation** for emotion and age prediction using categorical cross-entropy.\n\n2. **Optimization and Training**:\n    - The model uses Stochastic Gradient Descent (SGD) optimizer with:\n        - Learning rate: 0.0005.\n        - Decay rate: 1e-6.\n        - Momentum: 0.9.\n        - Nesterov momentum.\n    - Training is carried out for 200 epochs with a batch size of 32, and validation accuracy metrics are monitored for gender, emotion, and age predictions.\n\n3. **Convolutional Layers (Initial Phases)**:\n    - The architecture starts with a convolutional layer with:\n        - 256 filters.\n        - Kernel size: 5.\n        - Stride: 1.\n    - This layer includes batch normalization and is followed by max pooling with a pool size of 5 and a stride of 2.\n    - It is succeeded by another convolutional layer with:\n        - 128 filters.\n        - Kernel size: 5.\n        - Stride: 1.\n    - This layer is supplemented with batch normalization, max pooling (pool size: 5, stride: 2), and dropout (20% rate).\n    - A final convolutional layer has:\n        - 64 filters.\n        - Kernel size: 5.\n        - Stride: 1.\n        - Similar to previous layers, it includes batch normalization and max pooling.\n\n4. **Densely Connected Layers**:\n    - After convolutional layers, a flattening operation leads to a shared dense layer with 32 neurons, applying batch normalization and dropout (20%).\n\n5. **Model Hyperparameters and Training Strategy**:\n    - Kernel size of 5 for convolutional layers, dropout rate of 20%, and the Adam optimizer for efficient optimization.\n    - Early stopping strategy with patience of 5 epochs and learning rate reduction (patience of 3 epochs, reduction factor of 0.5).\n\n6. **Output Layers**:\n    - For emotion and age prediction models:\n        - Softmax activation functions in the final layers, each with six neurons.\n    - For gender prediction model:\n        - Binary softmax activation in the final layer, which consists of two neurons.\n\n7. **Improved Architecture**:\n    - This architecture builds upon the initial SEGAA Gen-0 and includes:\n        - Three convolutional blocks, each with Batch Normalization, Max Pooling, and Dropout layers.\n        - A flattened layer leading to a densely connected layer with 64 neurons, normalized and regularized with Batch Normalization and Dropout.\n    - The model continues to use distinct output layers with softmax activation for emotion and age, and binary softmax for gender.\n    - Hyperparameters:\n        - Kernel size of 3 and a stride of 1 for convolution layers.\n        - Dropout rate of 0.3.\n        - Nadam optimizer.\n    - Training is for 200 epochs with a batch size of 16, with monitored validation accuracy metrics.\n    - Early stopping and learning rate reduction callbacks are employed to assure optimal convergence.\n\nIn essence, the SEGAA model is a robust neural network architecture tailored for multi-label classification, leveraging convolutional and dense layers, optimized learning rates, early stopping, and dropout layers to predict age, gender, and emotion from speech data efficiently.",
        "main_experiment_and_results": "Main Experiment Setup:\n- **Datasets**: The main experiment utilizes a diverse range of audio datasets containing annotations for emotion, gender, and age. The specific datasets used are not explicitly mentioned, but they are comprehensive enough to evaluate the models' performance effectively.\n- **Baselines**: The experimental framework compares individual and multi-output Multi-Layer Perceptron (MLP) models as baseline approaches. These baselines are essential for assessing the performance improvements introduced by the SEGAA architectures.\n- **Models**: The primary models evaluated include:\n  - Individual MLP models for each attribute (emotion, gender, age)\n  - Multi-output MLP models that predict all three attributes simultaneously\n  - SEGAA architectures designed to predict age, gender, and emotion from speech\n\n- **Evaluation Metrics**: The primary metrics for evaluating the models' performance are not explicitly mentioned but typically include accuracy, precision, recall, and F1-score for classification tasks.\n\nMain Experimental Results:\nThe SEGAA architectures demonstrate superior performance compared to the baseline MLP models. The results highlight the effectiveness of SEGAA in accurately and robustly predicting age, gender, and emotion from audio data. Specific numerical results and detailed comparisons between the models are not provided in the text."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The objective is to determine whether individual models or multi-output models are more effective in classifying age, gender, and emotion from speech.",
            "experiment_process": "The study uses two datasets: EMO-DB and CREMA-D. EMO-DB includes 535 speech utterances from 10 speakers covering seven emotions, while CREMA-D consists of 7,442 clips from 91 actors across various demographics and six emotions. The dataset was processed using noise addition, time stretching, pitch shifting, and signal shifting. Audio features like ZCR, RMSE, and MFCC were extracted. Models were trained using MLP and SEGAA architectures, employing different layers and activation functions tailored for gender (sigmoid) and emotion/age (softmax) predictions. Hyperparameters included SGD and Nadam optimizers, early stopping, learning rate reduction, and validation accuracy monitoring.",
            "result_discussion": "Both individual SEGAA and MLP models demonstrated high performance, with accuracies reaching 96% for emotion detection, 100% for gender detection, and 95% for age detection. Multi-output models yielded competitive results, achieving accuracies from 84% to 99%, with slight decreases in recall for certain attributes. Sequential models, particularly the EGA sequence, showed consistent F1 scores above 0.90 for all attributes, although they faced challenges due to error propagation.",
            "ablation_id": "2403.00887v1.No1"
        },
        {
            "research_objective": "To identify the strengths and weaknesses of different sequences in predicting emotion, gender, and age using SEGAA models.",
            "experiment_process": "Sequential models were designed, where individual predictions were cascaded. Three sequences were tested: emotion \u2192 gender \u2192 age; gender \u2192 age \u2192 emotion; and age \u2192 emotion \u2192 gender. The models employed several convolutional and dense layers with batch normalization, max pooling, and dropout. Prediction sequences utilized varying strategies to incorporate previously predicted variables.",
            "result_discussion": "The EGA sequence (Emotion \u2192 Gender \u2192 Age) excelled in gender detection and provided respectable accuracy in emotion detection. However, the overall comparison revealed that individual and multi-output models outperform sequential models due to error propagation issues.",
            "ablation_id": "2403.00887v1.No2"
        }
    ]
}