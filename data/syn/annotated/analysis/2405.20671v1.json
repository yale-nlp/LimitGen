{
    "title": "Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers",
    "abstract": "Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training.\nTo tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer.\nTaking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more \u201crelevant\u201d tokens; for integer addition tasks, we regard digits of the same significance as in the same position.\nOn the empirical side,\nwe show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67 of the trained length).\nOn the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\nWe also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands,  multiplication, copy/reverse, and a two-dimensional task.111Our codebase is available at github.com/HanseulJo/position-coupling.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Since the appearance of a sequence-to-sequence deep neural architecture called Transformer (Vaswani et al., 2017  ###reference_b37###), it has brought tremendous success in various fields including natural language process (NLP) (Thoppilan et al., 2022  ###reference_b35###; Chowdhery et al., 2023  ###reference_b7###; Gemini et al., 2023  ###reference_b13###; OpenAI, 2023  ###reference_b27###) and many applications such as mathematical reasoning and theorem proving (Lewkowycz et al., 2022  ###reference_b21###; Wu et al., 2022  ###reference_b40###; Trinh et al., 2024  ###reference_b36###).\nDespite its triumph, it has recently been illuminated that Transformers often lack the ability of length generalization (Anil et al., 2022  ###reference_b3###; Deletang et al., 2023  ###reference_b9###; Zhang et al., 2023  ###reference_b48###; Press et al., 2022  ###reference_b29###).\nIt refers to a special kind of out-of-distribution generalization capability to extrapolate the model\u2019s performance to longer sequences than those encountered during training.\nUnderstanding length generalization is of great importance because the lack of length generalization provides evidence that language models do not genuinely understand the structure of a given task.\nImproving Transformer\u2019s length generalization has received much attention, particularly because the time/memory complexities for training Transformers grow up to quadratically in the sequence length.\nEven for simple arithmetic tasks such as integer addition, length generalization is still difficult for Transformers (Kim et al., 2021  ###reference_b18###; Nogueira et al., 2021  ###reference_b26###; Kazemnejad et al., 2023  ###reference_b17###; Zhou et al., 2024a  ###reference_b49###; Lee et al., 2024  ###reference_b20###; Zhou et al., 2024b  ###reference_b50###).\nHumans can length-generalize in integer addition because they understand the essential principle of the task.\nNevertheless, it is observed that Transformers typically\nlearn to solve addition only up to the training sequence length (Lee et al., 2024  ###reference_b20###), which is different from the true arithmetic algorithm that humans \u201cimplement\u201d.\nThis raises an important question: can we make a Transformer truly understand the structure of a task so that it can generalize to the longer sequences without training on them?\nIn other words, can we inject the known structure of a task into a Transformer so that it can automatically length-generalize?\n###figure_1### In this paper, we propose position coupling, a simple yet effective method for length generalization that directly embeds the structure of the tasks into a Transformer.\nIn contrast to the vanilla absolute position mechanism assigning unique and consecutive position IDs to each token, we assign the same position IDs to certain input tokens that are semantically relevant. Coupling such tokens together helps the model learn to solve the task regardless of the length of the given input sequence.\nFor example, in the addition task, it is important to consider the significance of digits, so we couple the positions at the same significance."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Summary of Contributions",
            "text": "We propose position coupling to tackle the length generalization problem of decoder-only Transformers. Our approach injects the structure of the task into the absolute position encoding by assigning the same position IDs to relevant tokens (Section 3  ###reference_###).\nWith position coupling, we achieve a robust and near-perfect generalization up to 200-digit additions by training Transformers on up to 30-digit additions, which is a  extrapolation of the operand lengths (Figure 1  ###reference_###, Section 4  ###reference_###).\nIt is promising since it was not clear whether the length generalization on the addition task can be solved reliably with Transformers (Zhou et al., 2024b  ###reference_b50###).\nWe theoretically prove by concrete construction that a small (1-layer, 2-head) Transformer equipped with coupled position IDs can add two decimal integers whose lengths are exponential in the embedding dimension\n(Theorem 5.1  ###reference_theorem1###).\nInterestingly, we observe a striking similarity between the attention patterns from our theoretical construction and those extracted from a Transformer trained with a standard optimizer (Section 5.1.1  ###reference_.SSS1###).\nAs a complementary result, we also prove that any 1-layer Transformer without positional information cannot fully solve any permutation-sensitive tasks such as addition (Section 5.2  ###reference_###).\nWe empirically show that position coupling can effectively address various tasks beyond addition, including addition with multiple summands (Section 6.1  ###reference_###), multiplication between -digit and 2-digit integers (Section 6.2  ###reference_###), and copy/reverse operations (Section 7.1  ###reference_###).\nIn particular, we provide a theoretical construction of a 2-layer Transformer that solves the multiplication between -digit and 2-digit integers for exponentially large  (Theorem 6.1  ###reference_theorem1###).\nWe also verify that position coupling can aid Transformers in learning tasks with multi-dimensional structures (Section 7.2  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "We focus on decoder-only Transformers that solve the tasks using next-token prediction.\nSince we study deterministic tasks with a unique answer, we consider greedy decoding throughout the paper."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Next-token Prediction with Decoder-only Transformers",
            "text": "###figure_2### A decoder-only Transformer returns an output sequence of the same length as the input sequence.\nOne difference from a Transformer encoder is that the attention mechanism in a Transformer decoder occurs only in a single forward direction due to the causal attention mask.\nDue to this causal nature, the Transformer decoder is mostly used for inferring the next token of each token, just based on the information of the current and the previous tokens."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Data Formats",
            "text": "Each task in this work is represented as a collection of sequences of the form \u2018(query)=(response)\u2019: given a query, our task is to infer the response correctly.\nThus, we only care about the result of the next-token prediction for the \u2018=\u2019 token and the tokens in the response (except its last token).\nThat is, we only compute the losses and accuracies for those output tokens, as depicted in Figure 2  ###reference_###.\nPrevious works commonly observe that data formats play an important role in solving downstream tasks with Transformers because a proper data format enables the model to learn a simple function to solve a task.\nHere we overview some well-known methods we apply, focusing on the addition task.\nReversed Format.  \nLee et al. (2024  ###reference_b20###) observe that reversing the response leads to improvement in both performance and sample efficiency.\nFor example, \u2018\u2019 becomes \u2018\u2019 in a reversed format.\nThis enables a decoder-only Transformer to infer the response from the least significant digit to the most significant digit, similar to how humans add two numbers.\nZero-padding.  \nZero-paddings ensure that the length of both operands in a query is the same and the length of a response is fixed when the length of the operand is given.\nThat is, by padding the query and the response of an -digit + -digit addition with 0\u2019s, the input sequence becomes a -digit addition with -digit response.\nFor example, \u2018\u2019 becomes \u2018\u2019.\nWrapping with BOS/EOS token(s).  \nIt is conventional in NLP to put BOS/EOS (beginning-/end-of-sequence) tokens at the beginning/end of the sequence.\nLee et al. (2024  ###reference_b20###) use the same token \u2018$\u2019 for BOS and EOS tokens and observe that it is beneficial to wrap each sequence with the $ token when solving the addition task.\nWe do not observe any significant difference in the performance between sequences with the same and different BOS/EOS tokens."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Positional Embeddings/Encodings (PE)",
            "text": "Vaswani et al. (2017  ###reference_b37###) introduce the absolute positional embedding (APE) to Transformers to inject the positional information into the model.\nThe usual APE works as follows: given an input sequence of tokens, we assign a sequence of consecutive position IDs (integers).\nEach position ID is mapped to a unique PE vector, and the vector is either added or concatenated to the corresponding token embedding vector.\nWe focus on the learned APE initially proposed by Gehring et al. (2017  ###reference_b12###).\nLength Generalization and PE.  \nIt is actively studied whether PE is a crucial factor in solving the length generalization problem of Transformers.\nKazemnejad et al. (2023  ###reference_b17###) argue that decoder-only Transformers with no positional encoding (NoPE) can achieve length generalization of downstream tasks since a Transformer decoder can implicitly capture the generalizable positional information due to its causal nature.\nHowever, there is a line of works proposing new PE methods to improve the length generalization performance of Transformers (Ruoss et al., 2023  ###reference_b32###; Li et al., 2024  ###reference_b22###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Position Coupling: A Method for Length Generalization",
            "text": "We propose position coupling, which assigns position IDs that directly encode the structure of given tasks. Here, we explain the general position ID assignment rule of position coupling in two steps and then move on to the specific example of addition.\nFirst, we partition the tokens of the input sequence.\nThe detailed principles for grouping the tokens differ by task, but the common desiderata are the following: there are two or more groups of consecutive tokens, and each token in a group must have a unique semantic meaning so that a one-to-one correspondence between tokens in different groups can be made.\nNext, for each group of tokens, we assign a sequence of consecutive numbers (usually, positive integers) as position IDs, starting from a random number (at training time) or a fixed predetermined number (at evaluation time).\nWe use random position IDs at training time for inducing length generalization by enabling all position embedding vectors to be trained, up to a pre-defined hyperparameter of maximum position ID (max_pos).222\nIt explicitly determines the maximum testable length of sequence that a transformer can handle. In the case of the decimal integer addition task, the maximum possible generalizable length is .\nModels trained with larger  can handle longer sequences; however, training becomes more difficult as the frequency of each position ID appearing during training decreases.\nVery importantly, we assign the same position IDs to the tokens in all groups that are relevant to each other for solving the given task: we refer to this as \u201ccoupling the positions\u201d.\nLastly, we set 0 as the position IDs of special tokens like BOS/EOS tokens and the PAD token (padding for minibatch training and evaluation)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Position Coupling for Decimal Integer Addition Task",
            "text": "###figure_3### We illustrate position coupling for the decimal integer addition task (or addition task for short).\nTo study the length generalization of the addition task,\nwe regard each digit (0\u20139) as a single token.\nFor illustration, if we need a concrete example, we will use the addition .\nBefore applying the position coupling, we adopt an input format similar to Lee et al. (2024  ###reference_b20###) so that we reverse the response, but we use zero-padding and wrapping with BOS/EOS token \u2018\u2019 at the same time.\nFor example, \u2018\u2019 becomes \u2019\u2019.\nWe partition the tokens in the sequence into three groups: (1) first operand & \u2018\u2019, (2) second operand, and (3) \u2018\u2019 & response (the sum).\nThen each number token is \u201cunique\u201d in the corresponding group in terms of significance, which naturally induces a one-to-one correspondence between (most of) the tokens across different groups.\nWe group \u2018\u2019 and the sum together because these tokens are where we perform next-token prediction.\nNow we assign the coupled position IDs to the tokens.\nLet us say that the random starting number is 6.\nWe assign 6, 7, and 8 to the tokens in the operands, and assign 5, 6, 7, and 8 to the tokens in the sum in a reversed order: see Figure 3  ###reference_###.\nWe remark that, first, we assign 9 as position IDs of \u2018\u2019 and \u2018\u2019 tokens because they are adjacent to the number token with position ID 8, even if there are no \u2018significances\u2019 for those tokens.\nSecond, we assign 5 as a position ID of the most significant digit of the sum (which may be \u20180\u2019 due to the zero-padding) just because it is next to the number token with position ID 6, even though there are no other corresponding tokens in the other groups (operands).\nWe also note that we do not group the \u2018\u2019 token with the second operand to prevent unnecessary coupling between \u2018\u2019 and the most significant digit of the sum (position ID 5).\nRemark.  \nA concurrent work by McLeish et al. (2024  ###reference_b24###) proposes an analogous approach in the context of solving arithmetic tasks, while they employ a different input format; they flip both the query and the response but do not apply any paddings.\nA more detailed comparison with our work is provided in Section 8  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Comparison with Index Hinting",
            "text": "Even though the idea of implanting the structure of a task into the positional encoding is novel, there is an existing approach named index hinting (Zhou et al., 2024a  ###reference_b49###) that applies a similar idea but to the input sequence.\nIndex hinting is an input augmentation technique that places position markers in front of the tokens to couple the semantically relevant tokens.\nFor example, Zhou et al. (2024a  ###reference_b49###) transform \u2018\u2019 into \u2018\u2019 with some zero-paddings, where , , , and  are consecutive index hints.\nHere, the starting hint character  is randomly selected during training, similar to our method of choosing the starting position ID.\nThe reversed format and BOS/EOS tokens can be applied as well.\nOne way in which index hinting differs from position coupling is that it doubles the input sequence length.\nThis is because the position information and the token information do not merge:\nthe index hints and the normal tokens are mapped to separate token embedding vectors which are alternately placed in the input embedding matrix.\nAs a result, a Transformer must figure out the correspondence between each adjacent pair of an index hint and a normal token.\nMoreover, the doubled input length can require up to four times the training time and memory.\nIn contrast, position coupling explicitly combines token and position information: every token embedding and corresponding position embedding are mixed into a single vector.\nHence, a Transformer can effortlessly utilize the positional structure of the task, without hurting the training time.\nWe highlight that, as will be mentioned in Section 4.1  ###reference_###, position coupling exhibits better length generalization than index hinting.\nAnother difference is that the index hints should be inferred by Transformers in addition to the normal tokens in the response, which might be an additional burden.\nOur position coupling circumvents this difficulty, eliminating the need to estimate anything other than the tokens in the original response."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments on Addition Task",
            "text": "In this section, we empirically demonstrate that position coupling allows extensive length generalization of Transformers on the addition task.\nWe delve into the impact of training length and architecture on the length generalization performance and provide comparisons with NoPE, APE with a random starting position ID (we call random-start APE), and index hinting (Zhou et al., 2024a  ###reference_b49###).\nData Sampling.  \nWe opt for the balanced sampling in terms of the number of digits (Nogueira et al., 2021  ###reference_b26###).\nGiven the maximum number of digits , we do balanced sampling for each operand in two steps.\nFirst, we sample the number of digits  uniformly at random.\nNext, we sample an operand from  uniformly at random, except for  where we sample from .\nThis procedure addresses the imbalance problem in the number of digits of operands.\nModel and Training.   As baseline models, we train 1-layer, 4-head, decoder-only Transformers (with roughly 4M trainable parameters) from scratch.\nWe set max_pos as 202 so that the maximum testable length of summands is 200.\nWe do not use packing or shifting for simplicity of implementation.\nSince we manually put coupled position IDs with a random starting index during training, we can train all the position embeddings without packing and shifting."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Theoretical Analyses on 1-layer Transformers",
            "text": "In the previous section, we provided empirical results exhibiting the outstanding performance of position coupling.\nOne might ask why and how position coupling works so effectively.\nIn Section 5.1  ###reference_###, we provide a theoretical explanation by carefully constructing a 1-layer Transformer model that is capable of solving the addition task involving exponentially long operands when the input is encoded with position coupling.\nWe also present the necessity of proper positional information for a 1-layer Transformer to solve the addition task in Section 5.2  ###reference_###."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "1-layer Transformer with Coupled Positions can Perform Long Additions",
            "text": "For the sake of simplicity of presentation, we consider a Transformer without any normalization layers, as conventionally done in theoretical constructions by previous works (Yun et al., 2020a  ###reference_b44###, b  ###reference_b45###; Awasthi and Gupta, 2023  ###reference_b4###).\nFor the sake of completeness, readers can find a mathematical formulation of the decoder-only Transformer architecture in Appendix C  ###reference_###.\nWith the input format described in Section 3.1  ###reference_###, there exists a depth-1 two-head decoder-only Transformer with coupled positions that solves the addition task with next-token prediction.\nHere, the operand length is at most , where the embedding dimension is .\nWe provide a detailed construction in Appendix D  ###reference_###.\nWe highlight that our proof is constructive and does not rely on any universal approximation result of neural networks.\nTheorem 5.1  ###reference_theorem1### shows that a 1-layer 2-head Transformer is sufficient for implementing addition between two exponentially long integers.\nWe remark that this result can be naturally extended to larger architectures with more layers/heads, with the help of residual connections."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 Probing the Attention Patterns in Trained Transformers with Position Coupling",
            "text": "###figure_7### We discover a striking similarity between the attention patterns in our theoretical construction (Theorem 5.1  ###reference_theorem1###) and those extracted from a Transformer trained with position coupling and a standard optimizer.\nIn particular, the manually constructed attention patterns described in Tables 11  ###reference_### and 17  ###reference_### in Appendix D  ###reference_### closely resemble the actual attention patterns in Figure 7  ###reference_###.333Note that they match up to matrix transpose, which is due to the difference in the formulations.\nWe were surprised because the similarity was discovered after we came up with the proof of the theorem.\nDrawn from this discovery, we claim that a Transformer trained with position coupling spontaneously learns two separate components of the addition task: (1) adding two numbers without carries, and (2) predicting the carries.\nLet us revisit the example in Figure 3  ###reference_### and consider predicting \u20187\u2019 (position ID 6) as the next token of \u20180\u2019 (position ID 7).\nNote that the token \u20187\u2019 is the result of combining the digit-wise sum 6+0=6 and a propagated carry 1.\nTo find out the sum without carry, it is enough for the model to attend to the two previous positions with ID 6: tokens \u20186\u2019 and \u20180\u2019.\nOn the other hand, to predict the carry, the model may attend to the three positions with ID 7: tokens \u20185\u2019, \u20184\u2019, and \u20180\u2019.\nThe reason way we should care about \u20180\u2019 is that considering the sum 5+4 (=9) of the two digits in the operands is not sufficient to determine the existence of the carry. By looking at the token \u20180\u2019 in the response (with position ID 7), we can detect that the actual sum in this position is 10 (=5+4+1, where 1 is another carry propagated from the previous position) and hence we need to propagate a carry 1 to the next position (with ID 6).\nNow we inspect the aforementioned claim by examining the attention matrices of an actual trained Transformer.\nIn the model, we discover two different patterns of attention matrices,444Note that the attention matrices depicted in Figure 7  ###reference_### are square, lower-triangular (due to causal attention pattern), and row-stochastic (all entries are nonnegative and the sum of each row equals 1).\nplaying distinct roles.\nThe first attention pattern (top of the figure) seems to correspond to the addition without carries: each token in the response (including \u2018=\u2019) attends to two positions needed to find out the sum without carry.\nConversely, the second attention pattern (bottom of the figure) seems to correspond to the carry prediction: again, each token in the response attends to three positions required to find out the carry.\nRemark.  \nSimilarly to our analysis, Quirke et al. (2023  ###reference_b30###) study the attention patterns of a 1-layer 3-head decoder-only Transformer model trained solely on 5-digit addition. They also observe that each head handles different subtasks of addition, such as digit-wise summation and carry detection."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "1-layer Transformers Require Positional Information",
            "text": "In Section 4.1  ###reference_###, we observed that 1-layer Transformers fail to perform the addition task without position coupling.\nHere, we provide a partial result that theoretically explains why this happens inevitably, particularly in the case of NoPE.\nWe start with a general proposition: a 1-layer Transformer without positional encoding cannot distinguish queries that are identical up to permutation when inferring the first token of the response using greedy next-token prediction.\nConsider any depth-1 finite-head decoder-only Transformer model  without positional encoding (NoPE). Given an input sequence  and its arbitrary permutation , if the last tokens of  and  are identical, then the next tokens predicted by  will also be identical for both sequences when applying a greedy decoding scheme.\nThe proof is deferred to Appendix E  ###reference_###.\nAccording to the proposition above, the 1-layer Transformer without positional encoding will always output the same values starting from the \u2018=\u2019 token, provided that the combination of query tokens is identical, even if their order varies.\nHowever, the addition task is permutation-sensitive, meaning that the permuted queries may result in different responses.\nTherefore, the 1-layer Transformer cannot completely solve the task without positional encoding.\nIt is important to note that this result remains unchanged regardless of the input format: neither reversed format nor index hinting provides any benefit.\nWe also highlight that this impossibility result can be extended to any other permutation-sensitive tasks, such as arithmetic tasks and copy/reverse tasks.\nBased on this fact, we write code to directly calculate the maximum EM accuracy on the -digit addition task that a 1-layer decoder-only Transformer can achieve (see Appendix E  ###reference_### for the code).\nThe accuracies rapidly decrease to zero: % for 3-digit addition, % for 4-digit integers, and % for 5-digit integers.\nWe leave it for future work to investigate the necessary conditions of the architecture for implementing addition when other positional encoding schemes are employed."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Extension to Other Arithmetic Tasks",
            "text": "The possible utilization of position coupling is not limited to adding two decimal integers.\nIn the following subsections, we explore the effectiveness of position coupling for the addition task with more than two operands (Section 6.1  ###reference_###) and the multiplication task with the length of the second operand being fixed (Section 6.2  ###reference_###).\nWe train on sequences with operands of 1\u201340 digits.\nOur choice of max_pos is 102, so we test the operands of up to 100 digits.\nWe investigate the performance of 3 different architectures, each with a different depth.\nThe experimental results are described in Figure 8  ###reference_###.\n1-layer models keep their generalization capability until 100 digits, whereas the 3-layer models exhibit great stability across random seeds and achieve the highest generalizable length of 90.\nLastly, we note that the result of Theorem 5.1  ###reference_theorem1### can be extended to addition tasks with multiple summands with slight adjustments to the feed-forward layer in the construction.\nWe reverse and zero-pad the response, setting the length of it as .\nWe couple the position starting from the least significant digits of both operands and response, decrementing the ID as we move to their most significant digits: see Figure 9  ###reference_###.\n###figure_8### The experimental results showcased in Figure 10  ###reference_### verify the efficacy of position coupling compared to NoPE and random-start APE.\nWe observe that a 1-layer model fails even with position coupling, even for training. However, as the depth increases to 2 or more, it immediately becomes capable of length generalization.\n###figure_9### Unlike addition, position coupling for  multiplication is less intuitive, as predicting the token in the middle of the response requires multiple digits from both operands while each token in the response is linked with at most 2 tokens in the query.\nPerhaps surprisingly, we can still construct a Transformer that provably solves this task for exponentially long sequences.\nGiven an appropriate format of the input sequence, there exists a depth-2 decoder-only Transformer model with coupled positions that can perform the  multiplication task with next-token prediction.\nHere, the number of the total heads is 10 and the length of the first operand is at most , where we denote the token embedding dimension by .\nWe defer the proof to Appendix F  ###reference_###.\nThis result suggests that the proposed position coupling scheme for the  multiplication task sufficiently captures the inherent structure of the task, and thus provides the potential for the trained model to generalize across unseen lengths. Also, we believe that Theorem 6.1  ###reference_theorem1### is optimal in terms of the number of attention layers, as the depth-1 model exhibits total failure even for in-distribution samples in our experiment."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Addition Task with Multiple Summands",
            "text": "The position coupling scheme for the vanilla addition task (with two operands) can naturally extend to the addition task with multiple summands: assign position IDs in ascending order from most significant digits to least significant digits for every operand and the response.\nHere, we focus on the addition of three summands.\nTo the best of our knowledge, there is no prior work considering multiple summands.\n###figure_10### We train on sequences with operands of 1\u201340 digits.\nOur choice of max_pos is 102, so we test the operands of up to 100 digits.\nWe investigate the performance of 3 different architectures, each with a different depth.\nThe experimental results are described in Figure 8  ###reference_###  ###reference_###.\n1-layer models keep their generalization capability until 100 digits, whereas the 3-layer models exhibit great stability across random seeds and achieve the highest generalizable length of 90.\nLastly, we note that the result of Theorem 5.1  ###reference_theorem1###  ###reference_theorem1### can be extended to addition tasks with multiple summands with slight adjustments to the feed-forward layer in the construction."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Position Coupling for  Multiplication Tasks",
            "text": "In this subsection, we study length generalization on the -digit  2-digit multiplication task in terms of the length  of the first operand, while fixing the length of the second operand by 2. Similar tasks have been studied before (Duan and Shi, 2023  ###reference_b10###; Jelassi et al., 2023  ###reference_b16###); Jelassi et al. (2023  ###reference_b16###) investigates  using an encoder-only model and Duan and Shi (2023  ###reference_b10###) studies  with an encoder-decoder Transformer architecture.\nBesides the architectural difference, Jelassi et al. (2023  ###reference_b16###) fail to observe length generalization with RPE and only achieve it by supplementing a small number of long samples to the training set.\nFurthermore, although Duan and Shi (2023  ###reference_b10###) provides perfect length generalization results even for test samples  longer than those observed during training, their approach requires a retraining step with hand-crafted bias correction on attention score matrices.\nWe reverse and zero-pad the response, setting the length of it as .\nWe couple the position starting from the least significant digits of both operands and response, decrementing the ID as we move to their most significant digits: see Figure 9  ###reference_###  ###reference_###.\n###figure_11### The experimental results showcased in Figure 10  ###reference_###  ###reference_### verify the efficacy of position coupling compared to NoPE and random-start APE.\nWe observe that a 1-layer model fails even with position coupling, even for training. However, as the depth increases to 2 or more, it immediately becomes capable of length generalization.\n###figure_12### Unlike addition, position coupling for  multiplication is less intuitive, as predicting the token in the middle of the response requires multiple digits from both operands while each token in the response is linked with at most 2 tokens in the query.\nPerhaps surprisingly, we can still construct a Transformer that provably solves this task for exponentially long sequences.\nGiven an appropriate format of the input sequence, there exists a depth-2 decoder-only Transformer model with coupled positions that can perform the  multiplication task with next-token prediction.\nHere, the number of the total heads is 10 and the length of the first operand is at most , where we denote the token embedding dimension by .\nWe defer the proof to Appendix F  ###reference_###  ###reference_###.\nThis result suggests that the proposed position coupling scheme for the  multiplication task sufficiently captures the inherent structure of the task, and thus provides the potential for the trained model to generalize across unseen lengths. Also, we believe that Theorem 6.1  ###reference_theorem1###  ###reference_theorem1### is optimal in terms of the number of attention layers, as the depth-1 model exhibits total failure even for in-distribution samples in our experiment."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Applying Position Coupling Beyond Arithmetic Tasks",
            "text": "In this section, we investigate the versatility of position coupling beyond arithmetic tasks.\nWe begin by examining copy/reverse tasks in Section 7.1  ###reference_###.\nThen, we investigate the efficacy of position coupling in managing multi-dimensional tasks in Section 7.2  ###reference_###.\nWe couple the positions in the query and the response by their correspondence.\nNote that the position ID assigned to the equal token is different for the two tasks because as per our design principle (Section 3  ###reference_###), the equal token is grouped to the response tokens and position IDs have to be consecutive numbers within each group.\n###figure_13### ###figure_14### We compare the performance of position coupling with NoPE and random-start APE.\nWe train the model on lengths 1\u201340 and evaluate its performance on lengths from 5 to 300, at intervals of 5.\nWhile a 1-layer 4-head model is used for the position coupling, we observe that the same architecture fails to memorize training samples for both NoPE and random-start APE.\nTherefore, we use a 6-layer 8-head model for the latter cases as it is commonly used in the literature (Zhou et al., 2024a  ###reference_b49###).\nThe experimental results are described in Figures 12  ###reference_### and 13  ###reference_###.\nFor both copy and reverse, position coupling exhibits near-perfect accuracy across the entire test length (7.5 for the trained length).\nIn contrast, NoPE and random-start APE immediately fail to length-generalize.\n###figure_15### ###figure_16###"
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Position Coupling for Copy/Reverse Tasks",
            "text": "We consider the copy task and the reverse task, allowing duplicate tokens in the query.\nIn particular, only 10 characters are considered to appear in the query, while the length of the query can be much longer.\nNotably, the copy task with duplicates is identified by Zhou et al. (2024a  ###reference_b49###) as a difficult task for decoder-only Transformers to length-generalize, whereas the Transformers can easily length-generalize if there are no duplicate tokens.\nWe couple the positions in the query and the response by their correspondence.\nNote that the position ID assigned to the equal token is different for the two tasks because as per our design principle (Section 3  ###reference_###  ###reference_###), the equal token is grouped to the response tokens and position IDs have to be consecutive numbers within each group.\n###figure_17### ###figure_18### We compare the performance of position coupling with NoPE and random-start APE.\nWe train the model on lengths 1\u201340 and evaluate its performance on lengths from 5 to 300, at intervals of 5.\nWhile a 1-layer 4-head model is used for the position coupling, we observe that the same architecture fails to memorize training samples for both NoPE and random-start APE.\nTherefore, we use a 6-layer 8-head model for the latter cases as it is commonly used in the literature (Zhou et al., 2024a  ###reference_b49###  ###reference_b49###).\nThe experimental results are described in Figures 12  ###reference_###  ###reference_### and 13  ###reference_###  ###reference_###.\nFor both copy and reverse, position coupling exhibits near-perfect accuracy across the entire test length (7.5 for the trained length).\nIn contrast, NoPE and random-start APE immediately fail to length-generalize.\n###figure_19### ###figure_20###"
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Two-dimensional Position Couplings for Minesweeper Generator Task",
            "text": "The tasks considered thus far are 1D, indicating that both the query and response are structured as 1D objects.\nThis leads us to a natural question: Can position coupling still be effective for multi-dimensional tasks?\nTo examine this question, we explore extending position coupling to accommodate a 2D task, where both the query and response are originally 2D objects.\nSpecifically, we define and investigate a new task called the minesweeper generator.\nGiven a rectangular board where each cell is filled with either \u2018M\u2019 (mine) or \u2018\u2019 (empty cell), the task is to generate a new board of the same size, with each cell containing:\n\u2018M\u2019, if the corresponding cell in the original board contains \u2018M\u2019;\nThe count of mines in 8 adjacent cells, if the corresponding cell in the original board contains \u2018\u2019.\nData Format & Position Coupling.  \nWe introduce two position coupling modules: one for the row direction and another for the column direction.\nFollowing this, we flatten the board to feed it into a Transformer: see Figure 14  ###reference_###.\nWithin the model, an embedding vector for each token (cell) is generated by adding the token embedding vector and corresponding two PE vectors.\nExperiments.  \nTo assess the efficacy of position coupling, we contrast its performance with NoPE.\nThe training samples are designed with the width and height of the board between 5 and 9 inclusively.\nWe allow the width and height to be different for training samples.\nWe evaluate the test performance on a square board with a width between 5 and 14 inclusively.\nWe also employ a 4-layer 8-head model for position coupling and a 6-layer 8-head model for NoPE.\nIn particular, for position coupling, we use the same embedding layer for both position coupling modules, as this approach empirically performs better than using distinct embedding layers for each module (see Section A.2  ###reference_###).\nThe experimental results are described in Figure 15  ###reference_###.\nPosition coupling maintains over 98% accuracy until a width of 12 and near 90% accuracy even at a width of 14.\nIn contrast, NoPE fails even for in-distribution samples.\nOne might worry that the generalizable length of 12 seems only slightly higher than the trained length of 9.\nHowever, we stress that our query is a 2D board, therefore the actual length generalization is from 81 to 144.\n###figure_21### ###figure_22###"
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Despite of simplicity of the tasks, arithmetic and algorithmic tasks have received a lot of attention as subjects to study the length generalization of Transformers.\nShen et al. (2023  ###reference_b34###) propose \u201cRandom Spacing\u201d and \u201cRecursive Scratchpad\u201d, achieving near-perfect generalization from 10-digits to 12-digits addition.\nAwasthi and Gupta (2023  ###reference_b4###) devise an auxiliary task for the sorting task and achieves significant length generalization through multitask learning.\nRuoss et al. (2023  ###reference_b32###) propose Randomized Positional Encodings to address the problem of the appearance of unseen position IDs in test time.\nMost recently, Zhou et al. (2024b  ###reference_b50###) demonstrate a possibility of extrapolation to the length 100 with training length 1\u201340 in the addition task by combining appropriate input format and advanced PE, yet they also observe that the performances are not robust and highly depend on the random seeds.\nAn emerging line of research seeks to theoretically address why length generalization is difficult and under what conditions it can be achieved.\nIn Abbe et al. (2023  ###reference_b1###), the authors demonstrate that various neural network models have an implicit bias towards min-degree interpolators, which may not be ideal for various reasoning tasks.\nXiao and Liu (2023  ###reference_b41###, 2024  ###reference_b42###) investigate problems whose reasoning processes can be formulated as directed acyclic graph (DAG) structures, introducing the concept of maximal input element distance to identify a sufficient condition for length generalization.\nRecently, Ahuja and Mansouri (2024  ###reference_b2###) formulate the conditions of function classes required to guarantee the length generalization of the empirical risk minimizer function.\nA very recent concurrent work by McLeish et al. (2024  ###reference_b24###) proposes a new position embedding method called \u201cAbacus\u201d.\nFrom a methodological perspective, Abacus is almost identical to our position coupling except for two main differences: Abacus reverses both the query and the response and does not use padding.\nFrom now on, we outline the differences between their work and ours beyond the methodology.\nIn terms of the model architecture, they use a depth-16 decoder-only Transformer model.\nThey combine their method with looped Transformers and input injection and report an improved performance.\nIn contrast, our main results are obtained with shallower models (up to 6 layers) with standard Transformer architecture of stacked decoder layers.\nBesides the addition task, they study multiplication, sorting, and Bitwise OR.\nOn the other hand, we study multiplication, triple addition, copy/reverse, and a 2D task.\nSpecifically, for the multiplication task, their study mainly considers the case where the length of both operands could vary up to 15.\nIn contrast, we focus solely on the  task, fixing the length of the second operand by 2.\nWhile we achieve length generalization up to 90-digit multiplication by training the model on up to 40-digit multiplication, they report near-perfect in-distribution performance but poor length generalization.\nFinally and notably, we provide novel theoretical analyses, including (1) the constructive proof that a depth-1 Transformer equipped with position coupling can completely solve the addition task for exponentially long digits and (2) the impossibility of the same model being capable of the addition task.\nWe also present theoretical results for the  multiplication task."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Achieving length generalization of Transformers even in the simple case of the addition task has been a challenge that received a lot of attention.\nWe propose position coupling, a variant of learned APE, which enables capturing task structure to improve the length generalization performance of Transformers for addition.\nWe show that a Transformer trained on 1\u201330 digit addition can generalize up to 200-digit addition.\nWe also provide the construction of a 1-layer Transformer model capable of adding two exponentially long integers when position coupling is applied.\nFurthermore, we verify the efficacy of position coupling for length generalization in other arithmetic and algorithmic tasks.\nWe intentionally limited ourselves to the tasks with an explicit structure between the tokens in each sequence.\nThis is because we are proposing a method to instill the known structure of the task into a Transformer by training on short sequences.\nDesigning the coupling of positions for tasks whose structure is implicit or black-box (e.g., for general NLP tasks) remains a fascinating next step: we leave the methodology for uncovering hidden structures and autonomously creating appropriate couplings (without manually designing them) for future work.\nWe also leave two challenging arithmetic tasks to length-generalize for future work.\nThe first is addition with a varying number of summands, i.e., determining if the model can generalize to adding more integers than the trained samples.\nThe second task is multiplication, where the lengths of both operands can vary."
        }
    ],
    "url": "http://arxiv.org/html/2405.20671v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "6",
            "6.1",
            "6.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4",
            "6.1",
            "6.2",
            "7.1",
            "7.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.20671v1",
        "paper_title": "Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers",
        "research_background": "The paper titled \"Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers\" is motivated by a significant limitation observed in Transformer models: their restricted ability to generalize performance to sequences longer than those encountered during training, known as length generalization. This limitation raises concerns about whether Transformers, as they are currently implemented, genuinely understand the structure of the tasks they are trained for.\n\n### Motivation\n\nThe primary motivation behind this research is the Transformer model's inability to generalize to longer sequences, which diminishes its utility across various applications, especially in fields that require processing or generating sequences of varied lengths. The authors argue that this gap in length generalization is particularly problematic because:\n\n1. It raises doubts about the genuine understanding of task structures by language models.\n2. It imposes significant constraints on memory and computational resources during training due to the quadratic growth of time/memory complexities with sequence length.\n3. It limits the effectiveness of Transformers on tasks like integer addition, where humans can naturally generalize over lengths due to understanding fundamental principles.\n\n### Research Problem\n\nThe core research problem the paper addresses is: Can we make a Transformer truly understand the structure of a task to allow for automatic length generalization without requiring training on longer sequences? Specifically, the paper seeks to explore whether incorporating the known structural properties of a task into a Transformer model can enhance its ability to generalize across lengths.\n\n### Relevant Prior Work\n\nThe introduction references several key studies and advancements that have laid the groundwork for understanding the strengths and limitations of Transformer models:\n\n1. The original Transformer architecture developed by Vaswani et al. (2017) has been incredibly successful in NLP and other applications (Thoppilan et al., 2022; Chowdhery et al., 2023; Gemini et al., 2023; OpenAI, 2023).\n2. Studies highlighting the inability of Transformers to generalize over longer sequences (Anil et al., 2022; Deletang et al., 2023; Zhang et al., 2023; Press et al., 2022).\n3. Research showing the difficulty of solving simple tasks like integer addition with length generalization in Transformers (Kim et al., 2021; Nogueira et al., 2021; Kazemnejad et al., 2023; Zhou et al., 2024a; Lee et al., 2024; Zhou et al., 2024b).\n\n### Method Proposal\n\nTo address the identified problem, the paper proposes a novel method named \"position coupling.\" This method aims to enhance length generalization by embedding task-specific structural knowledge directly into the Transformer. Instead of using the conventional absolute position mechanism, their approach assigns the same position IDs to semantically related tokens within an input sequence. For example, in an addition task, positions of digits with the same significance are coupled. This adjustment helps the Transformer model to abstract the task's structural rules in a manner that enables it to handle longer sequences effectively, regardless of the training sequence lengths.",
        "methodology": "**Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers**\n\n### Methodology:\nWe introduce a novel approach named position coupling, which involves assigning position IDs that encapsulate the structure inherent to specific tasks. The position coupling method can be broken down into two primary steps, which we will first outline generally and then apply to a specific example like the task of addition.\n\n1. **Token Partitioning:**\n   - We start by partitioning the tokens of the input sequence. \n   - The specific principles for grouping tokens differ depending on the task, but generally, we aim for:\n     - Multiple groups of consecutive tokens (minimum of two groups).\n     - Each token in a group must possess a unique semantic meaning to ensure a one-to-one correspondence between tokens across different groups.\n\n2. **Position ID Assignment:**\n   - Within each group of tokens, we assign a sequence of consecutive numbers (typically positive integers) as position IDs. \n   - At the time of training, these numbers start from a random number. During evaluation, they originate from a fixed, predetermined number.\n   - The strategy of using random position IDs during training helps induce length generalization by ensuring that all position embedding vectors up to a pre-defined maximum position ID (denoted as max_pos) are trained. This max_pos hyperparameter sets the upper bound on the sequence length that the transformer can handle during testing.\n   - For example, in the context of a decimal integer addition task, the upper limit for possible generalizable sequence length correlates with the chosen max_pos.\n     - Larger values of max_pos facilitate the handling of longer sequences, albeit at the expense of increased training difficulty due to the lower frequency of each position ID appearing.\n   - Crucially, the same position IDs are assigned to the tokens in all groups that are pertinent to each other for solving the task at hand\u2014this technique is termed as \"coupling the positions\".\n   - Furthermore, position IDs of 0 are reserved for special tokens, such as the Beginning of Sequence (BOS), End of Sequence (EOS), and padding (PAD) tokens used during minibatch training and evaluation.\n\nThis methodology innovatively leverages the structural aspects of tasks to enhance the transformers\u2019 ability to generalize over sequence lengths, thus addressing a key limitation in the existing models.",
        "main_experiment_and_results": "In this main experiment, we assess whether position coupling can facilitate significant length generalization of Transformers, specifically for the addition task. \n\n### Dataset\nThe data sampling method ensures balanced sampling in terms of the number of digits of the operands. The sampling process involves:\n1. Uniformly sampling the number of digits for each operand.\n2. Sampling operands with the chosen number of digits uniformly at random, except for single-digit operands which are sampled differently.\n\n### Model and Training\nThe baseline models are 1-layer, 4-head, decoder-only Transformers with approximately 4 million trainable parameters. To test length generalization up to a summand length of 200, the maximum positional index (max_pos) is set to 202. Position coupling is implemented by manually assigning coupled position IDs with a random starting index during training.\n\n### Baselines\nThe performance of position coupling is compared against:\n1. No Position Embeddings (NoPE)\n2. Absolute Positional Encoding with a random starting position ID (random-start APE)\n3. Index Hinting as per Zhou et al., 2024a\n\n### Evaluation Metrics\nThough specific evaluation metrics aren't explicitly mentioned, it can be inferred that the performance is measured by the models' ability to generalize to longer sequences in the addition task.\n\n### Main Experimental Results\nThe primary results focus on comparing the length generalization performance of the different setups. Position coupling is shown to enhance the length generalization capability of Transformers, outperforming the baseline methods (NoPE, random-start APE, and index hinting). This experiment confirms the effectiveness of position coupling in facilitating better length generalization outcomes in the addition task."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To empirically demonstrate that position coupling allows extensive length generalization of Transformers on the addition task.",
            "experiment_process": "Balanced sampling was used for data sampling based on the number of digits in the operands. Baseline models were 1-layer, 4-head, decoder-only Transformers with roughly 4M parameters. Models were trained with maximum testable summand lengths set at 200. Nine different training lengths were examined, with models trained on additions involving 1-10, 1-20, 1-30, and 1-40 digits. Various PE methods such as NoPE, random-start APE, and index hinting were tested. Experiments were run 8 times with different random seeds.",
            "result_discussion": "Longer training sequences led to longer generalizable lengths. The generalizable length was 70 for 1-10 digit training, 135 for 1-20, and 200 for 1-30 and 1-40 digits. Shallower models (1-layer) generalized better than deeper ones. Simple PE methods like NoPE and random-start APE showed poor length generalization. Index hinting improved results but did not generalize beyond 50 digits, failing completely at lengths of 70 or more. Position coupling approach scaled well up to 500 digits under specific training conditions.",
            "ablation_id": "2405.20671v1.No1"
        },
        {
            "research_objective": "To extend the position coupling scheme to the addition task with multiple summands and examine its effectiveness.",
            "experiment_process": "Position IDs were assigned in ascending order for digits of operands and response. Three different architectures (varying in depth) were trained on sequences with three operands of 1-40 digits, with a max_pos of 102 to test lengths up to 100 digits. Experimental results are discussed with a focus on different model depths.",
            "result_discussion": "1-layer models maintained their generalization capability up to 100 digits, while 3-layer models performed more stably across random seeds and achieved the highest generalizable length of 90 digits. The theoretical extension to multiple summands was validated with slight adjustments to the feed-forward layer.",
            "ablation_id": "2405.20671v1.No2"
        },
        {
            "research_objective": "To investigate the efficacy of position coupling for length generalization in -digit x 2-digit multiplication tasks.",
            "experiment_process": "Positional coupling was applied from the least significant to the most significant digit of both operands and the response. The length of the first operand was varied while fixing the second operand at 2 digits. Experiments were conducted to compare the performance of NoPE, random-start APE, and position coupling on depth-1 and deeper models.",
            "result_discussion": "Position coupling showed strong efficacy compared to NoPE and random-start APE. Depth-1 models failed even with position coupling, but models with a depth of 2+ layers showed immediate length generalization. The approach captured the inherent structure of the multiplication task, suggesting robust length generalization potential.",
            "ablation_id": "2405.20671v1.No3"
        },
        {
            "research_objective": "To test the performance of position coupling on copy/reverse tasks with duplicate tokens in the query.",
            "experiment_process": "Models were trained on lengths 1-40 and evaluated on lengths 5-300. The position coupling and NoPE/random-start APE performances were compared, using a 1-layer 4-head model for position coupling and a 6-layer 8-head model for NoPE/random-start APE, following literature standards.",
            "result_discussion": "Position coupling exhibited near-perfect accuracy across the entire test length (7.5 times the trained length). NoPE and random-start APE immediately failed to length-generalize. The results highlighted the effectiveness of position coupling in achieving generalization for copy/reverse tasks.",
            "ablation_id": "2405.20671v1.No4"
        },
        {
            "research_objective": "To explore the effectiveness of position coupling on a multi-dimensional (2D) task, specifically the minesweeper generator task.",
            "experiment_process": "Two position coupling modules were introduced for row and column directions. Boards were flattened to feed into a Transformer model. Training involved varying board widths and heights between 5 and 9, evaluated on square boards with widths from 5 to 14. A 4-layer 8-head model was used for position coupling, and a 6-layer 8-head model for NoPE.",
            "result_discussion": "Position coupling maintained over 98% accuracy up to a board width of 12 and nearly 90% at a width of 14. NoPE failed even for in-distribution samples. The results emphasized the significant potential of position coupling for 2D tasks, with the effective length generalization extending from 81 to 144.",
            "ablation_id": "2405.20671v1.No5"
        }
    ]
}