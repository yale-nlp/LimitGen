{
    "title": "Fewer Truncations Improve Language Modeling",
    "abstract": "In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity\u2014it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Large language models (LLMs) have achieved unprecedented success on a number of natural language processing and coding benchmarks (Brown et al., 2020  ###reference_b4###; Chen et al., 2021  ###reference_b6###) and in complex real-world tasks (Ouyang et al., 2022  ###reference_b46###). This remarkable progress is driven by large-scale pre-training over a massive amount of unlabeled documents.\nWhen formatting the training inputs, na\u00efvely padding every document to a fixed length is inefficient as short documents lead to an excessive amount of padding. Instead, the common practice is to concatenate all documents together and then split them into sequences of exactly the model\u2019s context length. A sentinel token (e.g., <|endoftext|>) is often added at the end of each document to indicate document boundaries within each training sequence.\nThis concatenate-then-split (hereafter \u201cconcatenation\u201d) approach\nhas been widely adopted in training language models in both natural language (Brown et al., 2020  ###reference_b4###; Chowdhery et al., 2022  ###reference_b8###; Rae et al., 2021  ###reference_b49###; Zhang et al., 2022  ###reference_b70###; Touvron et al., 2023b  ###reference_b64###; Scao et al., 2022  ###reference_b56###) and programming language (Nijkamp et al., 2023  ###reference_b44###), thanks to its optimal training efficiency as no padding is needed.\nHowever, such training efficiency comes at the expense of data integrity\u2014documents that could have been processed in their entirety by the model are instead fragmented into independent segments, which naturally results in loss of information.\nFurther, truncation reduces the amount of context within each segment, causing next-token prediction to be potentially ungrounded to its context, and thus making models more prone to hallucination.\nWe argue that data integrity is the key towards better language modeling. To realize this, we first show that it is feasible to group billions of documents at pre-training scale into sequences in a way that is as token-efficient as concatenation without incurring any unnecessary truncation: only documents beyond model\u2019s context length need to be segmented. Data grouping strategies that preserve the entirety of individual samples have been widely adopted for encoder-only and encoder-decoder models (Liu et al., 2019  ###reference_b35###; Raffel et al., 2020b  ###reference_b51###; Krell et al., 2021  ###reference_b28###). Nonetheless, these existing strategies either exhibit limited scalability or compromise training efficiency, making them less favorable compared to the concatenation method in LLM training at scale.\nIn response, we propose Best-fit Packing to eliminate unnecessary document truncations without sacrificing training efficiency. As illustrated in Figure 1  ###reference_###, we first segment long documents into multiple chunks by model\u2019s context length. Documents shorter than that are kept as singleton chunks. Next, we pack all the chunks into training sequences without breaking them any further. This step is essentially an instance of the bin packing problem111Bin packing is an optimization problem in which items of different sizes must be packed into a finite number of bins or containers, each of a fixed given capacity, in a way that minimizes the number of bins used (Bernhard & Vygen, 2008  ###reference_b2###)., which is NP-hard. We employ Best-Fit-Decreasing (Eilon & Christofides, 1971  ###reference_b16###), an approximation algorithm, and further optimize it to handle billions of documents efficiently. Empirical results show that the packed training sequences only contain a negligible amount of padding, which enables us to maintain the same training efficiency as the concatenation approach while preventing unnecessary document truncation.\nTo validate the effectiveness of truncation reduction,\nwe pre-train a set of models with inputs formatted by concatenation and Best-fit Packing respectively, ranging from 7B to 13B in model size, 2k to 8k in sequence length, on both Natural Language (NL) and Programming Language (PL) data.\nWe evaluate these models on 22 tasks covering reading comprehension, natural language inference, context following, summarization, world knowledge, and program synthesis.\nExperiment results show that models trained with fewer truncations demonstrate superior performance and exhibit less hallucination.\nIn summary, our main contributions are the following.\nWe highlight the truncation issue inherent in the widely-used concatenation method for LLM pre-training (\u00a72  ###reference_###).\nWe analytically show the adverse impact of truncation on learning through a simplified model (\u00a72.1  ###reference_###).\nWe propose Best-fit Packing, a scalable data grouping method that eliminates unnecessary document truncations at almost no cost of training efficiency (\u00a73  ###reference_###).\nWe empirically quantify the benefits of truncation reduction in a variety of downstream scenarios (\u00a74  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The Curse of Truncation",
            "text": "A well-written document in its entirety is naturally coherent and self-contained. In particular, factual statements in the document often logically depend on their aforementioned context through reference, entailment, or more sophisticated reasoning. We refer to the key span(s) of context that serves to establish such a dependency relation as grounding context. When learning from next-token prediction, if the grounding context is missing, the model will be forced to spuriously predict token(s) that in fact cannot be derived from the observed partial context. Consequently, at inference time, the model has a higher chance to ignore the grounding context (even when it is provided) and generate content that either contradicts or cannot be verified from the given context, which is known as closed-domain hallucination222Hallucination is an overloaded term. In this work, we focus on context-based hallucination as opposed to knowledge-based. (OpenAI et al., 2023  ###reference_b45###). We illustrate this point in Figure 2  ###reference_###.\n###figure_2### ###figure_3### ###figure_4### Figure 2(a)  ###reference_sf1### shows an example in Python. Despite the original code being correct, splitting variable definitions and corresponding usages into two distinct training sequences introduces grammatical errors. As self-attention does not cross sequence boundaries, DecoderModel and config are essentially undefined in the latter training sequence. Formatting data in such a fragmented way makes models learn pathological patterns, potentially leading to hallucination in downstream tasks. For example, in a program synthesis task, the model may directly use config without its definition. Even worse, the model may disregard the provided context and fabricate an irrelevant name: if we intend to instantiate an EncoderModel, and specify import EncoderModel in context, the model may still generate model=DecoderModel(\u2026) due to the learned spurious association between model and DecoderModel.\nFigure 2(b)  ###reference_sf2### illustrates the same issue in natural language where truncation harms faithfulness. The phrase Monday morning in the summary cannot be grounded to any part of the context in the same training sequence, and thus turns into a fabrication. Such incomplete samples can reduce models\u2019 context-awareness (i.e., the ability to attend to context) and result in unfaithful generation or nonsensical reasoning.\nBesides exacerbating hallucination, truncation can also impede knowledge acquisition during training, as textual representation of knowledge often takes the form of complete sentences or paragraphs, which is vulnerable to fragmentation. For example, in Figure 2(c)  ###reference_sf3###, the model will not be able to learn the location of ICML because the conference name and its venue are located in different training sequences."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Analytical Study via a Simplified Stochastic Process",
            "text": "As an additional source of intuition, we describe a simplified stochastic process  for which we can analytically show that a model trained on truncated sequences achieves a strictly worse sequence modeling accuracy than a model trained on full sequences, even if the amount of training data is infinite.\nWhile it is difficult to rigorously establish a theory on how truncation impacts learning with transformers models, we would like to make a first attempt on the analytical exploration to better motivate our proposal.\nIn analogy with language modeling, we can think of the \u2019s as tokens in the binary vocabulary . Our process is defined recursively, starting from a Bernoulli variable  which takes the value  with probability  and the value  otherwise.\nFor , the variable  takes the value of  with probability  and  with probability , where  is fixed.\nA graphical model associated with this process would be a tree with  as the root and  as the leaves.\nWe now compare a \u201cmodel A\u201d trained on sequences , against a \u201cmodel B\u201d trained on sequences  and . Thus, training of model B is affected by truncation.\nWe assume that there is a sufficient amount of data for the models to perfectly fit the training sequences.\nFor , the expected classification loss achieved by model A on token  is given by the conditional entropy\nwhere .\nOn the other hand, if we feed a sequence of observations  to Model B, its prediction for the next token is equal to\nThis distribution can be computed analytically thanks to the simplicity of the process (see Appendix A  ###reference_###), allowing us to determine the expected loss of model B.\nThe relative increase in loss from model A to model B is shown in Figure 3  ###reference_### as a function of . We see that model B is always worse, even under the assumption of sufficient data.\nThe relative loss increase always converges to  as  goes to infinity, but the convergence rate hinges on , i.e., on how strongly the visible tokens depend on the truncated one.\nThis exemplifies an effect that exists also in real-world language modeling: if a key piece of information is truncated but then repeated shortly after (high ), then the absence of the first mention does not have a lasting impact; on the other hand, if only vaguely related information about the truncated concept is available (low ), then the effect of truncation lasts longer.\n###figure_5###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Best-fit Packing",
            "text": "We propose a new method to group training data efficiently while eliminating unnecessary truncation, as illustrated in Figure 1  ###reference_###. Given the model\u2019s max sequence length , we first segment every document into chunks that are at most  tokens long. Note that this is the minimally required truncation, constrained by the context length. Then, to construct each training sequence, we select a number of document chunks to fill up as much of the -token space as possible, without breaking any of them.\nThe selection strategy, which we refer to as the packing algorithm, is discussed in \u00a73.1  ###reference_###.\nThere are two challenges: first, as it\u2019s not always feasible to fill up the -token sequence fully, padding tokens are used, leading to an increased number of training sequences compared to the concatenation approach. This increase necessitates more training steps per epoch. Therefore, packed sequences must be compact enough to minimize the use of padding tokens in order to retain training efficiency. Second, the algorithm must be scalable and fast enough so that it can operate on datasets of billions of documents."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "The packing algorithm",
            "text": "We formulate Best-fit Packing as a combinatorial optimization problem. We then present an efficient algorithm that scales linearly with data size, and show the solution achieves the same level of compactness as the usual concatenation, thus incurring a negligible loss in training efficiency. Empirically, we validate our method on large-scale pre-training datasets, specifically the RefinedWeb (Penedo et al., 2023  ###reference_b47###) for text, and the Stack (Kocetkov et al., 2022  ###reference_b26###) for code.\nGiven a set of document chunks , where  is the length of  in tokens and , packing these chunks into training sequences is equivalent to determining a partition of , denoted as , subject to . A training sequence is constructed by concatenating all chunks in an . Our goal is to find a partition  of the smallest possible size, which in practical terms means generating the fewest number of training sequences.\nThe above optimization problem is known as the bin packing problem (Bernhard & Vygen, 2008  ###reference_b2###), in which  items of different sizes must be packed into a finite number of bins or containers, each of a fixed given capacity, in a way that minimizes the number of bins used. Computationally, the problem is NP-hard. There exist several approximation algorithms, among which First-Fit-Decreasing (FFD) and Best-Fit-Decreasing (BFD) are the most popular ones that strike a good balance between efficiency and accuracy. We briefly describe these heuristics in Algorithm 1  ###reference_###.\nFFD: Find\nBFD: Find\nTime Complexity  In general, both FFD and BFD take  sorting time and  packing time. The search step for  is typically implemented with an -sized balanced binary tree that tracks all existing bins. However, in our case, notice that the sequence length is always an integer in , where . This reduces the sorting cost to  via count sort, and more importantly, allows further optimization on the packing part. Since in BFD we do not distinguish among bins with the same remaining capacity, it suffices to track the remaining capacity values instead of the actual bins, which effectively reduces the tree size to , and consequently the packing time to . However, the same does not apply to FFD, because the order of bins matters.\nIn practice, we implemented the above fast search in BFD using a segment tree defined as follows:\nThe tree has  leaf nodes.\nThe value of the -th leaf is  if there exists at least one bin whose remaining capacity is , and zero otherwise.\nInitially, all leaf nodes are set to zero, except the last one which is set to .\nThe value of every internal node is the maximum value of its children.\nTo find the best-fit bin, we query the tree from the root. At every internal node, we go left if the left child is no less than the item weight, and go right otherwise. We end up at a leaf node whose value is the best-fit capacity. A capacity-to-bin map is used to retrieve the best-fit bin. Finally, we update the tree to restore the two properties listed above. Please refer to Appendix B  ###reference_### for a more detailed illustration.\nTable 1  ###reference_### presents a runtime comparison of the Optimized Best-Fit Decreasing (OBFD) algorithm against the standard First-Fit Decreasing (FFD) at 2048 context length on different data scales by up/down-sampling the RefinedWeb dataset which consists of roughly 1 billion documents.\nAs demonstrated, our optimized BFD saves 60% of the running time at 1B scale, and the relative speedup (FFD/OBFD) increases logarithmically to the data size. With this efficient implementation, our method is able to scale up to even larger datasets as the asymptotic time complexity only depends linearly on the data size.\nCompactness  The other important perspective of packing is how compact the resulting training sequences are. Theoretically, both FFD and BFD are guaranteed to use no more than  of the optimal number of bins asymptotically (Johnson et al., 1974  ###reference_b22###).\nHowever, practically, the majority of documents are short compared to context length, as shown by the dashed curves in Figure 4  ###reference_###. The abundance of small-sized items makes packing much easier.\nConsequently, we observe that the training sequences from packing are nearly as compact as those obtained from concatenation. As shown in Table 2  ###reference_###, Best-fit Packing yields no more than 0.01% additional training sequences with either 2k or 8k context length and across NL and PL datasets. From another perspective, Table 2  ###reference_### also indicates that with Best-fit Packing, the amount of padding is negligibly small. The results prove that Best-fit Packing achieves roughly the same training efficiency as concatenation, as measured by the number of non-padding tokens processed using the same amount of compute.\n###figure_6### ###figure_7### Truncation reduction \nWe study to what extent Best-fit Packing alleviates the truncation problem. We count how many times each document is truncated in both NL and PL datasets, and aggregate by document length, as plotted in Figure 4  ###reference_###. Note that most documents contain fewer than  tokens; therefore, truncation caused by concatenation predominantly occurs within this range. By completely eliminating truncation for document lengths below , Best-fit Packing effectively preserves the integrity of an overwhelming majority of documents, limiting truncation only to longer documents where it is absolutely necessary."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments and Results",
            "text": "To empirically validate the effectiveness of Best-fit Packing over concatenation, we pre-train a set of transformer language models using the same architecture as LLaMA (Touvron et al., 2023a  ###reference_b63###), covering different domains, sizes, and context lengths as in Table 3  ###reference_###.\nWe use two popular pre-training datasets in our study: the Falcon RefinedWeb dataset (Penedo et al., 2023  ###reference_b47###) for text, and the Stack (Kocetkov et al., 2022  ###reference_b26###) for code. Please refer to Appendix C.1  ###reference_### for additional details on the training setup.\nWe evaluate models on a variety of downstream tasks with zero-shot and few-shot prompting.\nOur findings reveal that Best-fit Packing improves performance in an array of tasks, most significantly in reading comprehension (+4.7%), natural language inference (+9.3%), context following (+16.8%) and program synthesis (+15.0%).333As the scale of metric varies task by task, we use relative improvement in narratives by default unless otherwise noted. We also show that Best-fit Packing effectively reduces closed-domain hallucination.\nThroughout this section, we denote results that are statistically significant () under a paired t-test by a superscript s, and those that are not significant by n.\nOn every single task evaluated, our method either outperforms or matches the baseline. Importantly, in no case do we observe a statistically significant degradation with Best-fit Packing, showing that the improvement is monotonic."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Reading Comprehension",
            "text": "Reading comprehension tests models\u2019 ability to answer questions based on information from a given passage.\nWe evaluate the 13B natural language models with 5-shot in-context learning on Narrative QA (Ko\u010disk\u00fd et al., 2018  ###reference_b27###), Natural Questions (Kwiatkowski et al., 2019  ###reference_b29###), SQuAD (Rajpurkar et al., 2018  ###reference_b52###), and DROP (Dua et al., 2019  ###reference_b15###); with 1-shot on QuAC (Choi et al., 2018  ###reference_b7###); and with zero-shot on BoolQ (Clark et al., 2019  ###reference_b10###) and RACE (Lai et al., 2017  ###reference_b31###). We report F1 score or exact match (EM) for generation tasks, and accuracy for multiple-choice tasks. We adopt the few-shot examples released by HELM (Liang et al., 2022  ###reference_b33###) for Narrative QA, QuAC, and Natural Questions, and randomly sample from the training split for the rest datasets.\nResults in Table 4  ###reference_### demonstrate the superior performance of Best-fit Packing in reading comprehension at both 2k and 8k context length: packing significantly outperforms concatenation in half of the settings, and shows no degradation on the rest.\nAcross different benchmarks, we observe: 1) between open-ended generation and multiple choice, Best-fit Packing generally achieves more significant gains on the former, presumably because hallucination is less of a problem in multiple choice questions where the answer space is highly constrained. 2) Among generation tasks, the improvement on Natural Questions is minimal. We speculate that this is because these questions are designed to be answerable under both open-book and closed-book settings, which allows models to occasionally bypass the context and still provide correct answers based on their inherent parametric knowledge. 3) On all other generation tasks where the answer must be inferred from context, the improved performance aligns with our hypothesis that Best-fit Packing enhances models\u2019 context-awareness."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Natural Language Inference",
            "text": "We evaluate models\u2019 capability in understanding dependencies between sentences through natural language inference (NLI) tasks. We use 5-shot in-context learning for Multi-NLI (Williams et al., 2018  ###reference_b68###) and RTE (Wang et al., 2019  ###reference_b65###). As shown in Table 5  ###reference_###, Best-fit Packing improves NLI performance by up to +9.3%. With truncation reduction, dependency relations between sentences in training documents are better preserved, which explains the observed improvement."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Context Following",
            "text": "To validate our hypothesis that excessive truncations impair factual consistency and faithfulness of generation with respect to the context, we consider special cases where the context contradicts the model\u2019s parametric knowledge and the model must follow instructions or facts in the context to answer correctly. Specifically, we evaluate with 5-shot on NQ-Swap (Longpre et al., 2021  ###reference_b36###), a perturbed version of Natural Questions by replacing both the answer and answer mentions in the context with a different entity, and with zero-shot on MemoTrap (McKenzie et al., 2023  ###reference_b41###), where the instruction conflicts with models\u2019 memorization.\nTable 5  ###reference_### shows our method excels concatenation in both settings by up to +16.8%, thereby further strengthening our hypothesis. This also suggests that Best-fit Packing can potentially enhance in-context learning (Wei et al., 2023  ###reference_b66###), presenting a promising avenue for future exploration."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Summarization",
            "text": "We conduct 5-shot evaluation on CNN/DailyMail (See et al., 2017  ###reference_b57###; Hermann et al., 2015  ###reference_b19###) and XSUM (Narayan et al., 2018  ###reference_b43###), using few-shot examples released by HELM. We follow HELM to report ROUGE scores (Lin, 2004  ###reference_b34###) for accuracy on both datasets, along with SummaC (zero-shot) (Laban et al., 2022  ###reference_b30###) and QAFactEval (Fabbri et al., 2022  ###reference_b17###) scores for faithfulness on CNN/DailyMail. However, these two metrics are suboptimal for XSUM (see Appendix C.2  ###reference_###). Thus, we report the binary factuality scores from FAVA (Mishra et al., 2024  ###reference_b42###) for faithfulness evaluation on XSUM.444A summary gets a score of 1 if FAVA does not detect any hallucination (with the article as reference evidence), and 0 otherwise.\nIn Table 6  ###reference_###, we observe improvement in all cases except on XSUM with 2k context length, where both methods perform close to each other. Models trained with Best-fit Packing generally obtains not only higher ROUGE scores, but also better faithfulness.\nThe result further strengthens our hypothesis that excessive truncation in training data is one of the reasons for hallucination. By keeping documents in their entirety to the maximum possible extent, Best-fit Packing effectively improves the faithfulness of generations.\nInterestingly, we also find that the choice between packing and concatenation makes a difference in the number of generated sentences on CNN/DM. The prompts we used explicitly ask models to summarize the article in 3 sentences. Table 6  ###reference_### shows that models trained with packing do a better job at following this instruction, while models trained with concatenation tend to compose longer summaries.\nBest-fit Packing seems to ameliorate an issue where the conventional approach causes models to ramble on, despite being prompted with a verbalized length constraint."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Program Synthesis",
            "text": "We evaluate the 7B programming language models on HumanEval (Chen et al., 2021  ###reference_b6###) and MBPP (Austin et al., 2021  ###reference_b1###) for zero-shot code generation. Following standard practice, we generate 200 samples per problem, and report Pass@ with  for functional correctness.\nBesides, a crucial and great aspect of evaluation with programming language is that we can detect hallucination accurately without relying on ML-based models which can be error-prone. We resort to program analysis as a more reliable alternative for hallucination detection. Following (Ding et al., 2023  ###reference_b14###), we identify undefined name errors using static analysis,\nand report the percentage of generations with at least one such error as the hallucination metric.\nAs shown in Table 9  ###reference_###, our method both improves Pass@ (+15.0% for Pass@100 on HumanEval and +5.8% on MBPP), and reduces undefined name errors significantly by up to 58.3%. With Best-fit Packing eliminating most truncations in training inputs (cf. Table 4  ###reference_###), models are exposed to fewer partial code segments that contain undefined names. Consequently, hallucination is suppressed in model-generated code. Besides that, Best-fit Packing also benefits functional correctness as reflected in Pass@ improvement, which we believe is a combined effect of reduced hallucination and a better understanding of programming logic by learning from complete code examples."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Pre-training Data\nPre-training data is pivotal to the quality of language models. There has been multiple high-quality pre-training datasets that were made publicly available, e.g., C4 (Raffel et al., 2020b  ###reference_b51###), Pile (Gao et al., 2021  ###reference_b18###), RefinedWeb (Penedo et al., 2023  ###reference_b47###), RedPajama (Computer, 2023  ###reference_b11###), and the Stack (Kocetkov et al., 2022  ###reference_b26###; Lozhkov et al., 2024  ###reference_b38###). On top of these, multiple papers (e.g., (Lee et al., 2022  ###reference_b32###; Marion et al., 2023  ###reference_b39###; Chen et al., 2023  ###reference_b5###; Chowdhery et al., 2023  ###reference_b9###; Touvron et al., 2023a  ###reference_b63###; Raffel et al., 2020a  ###reference_b50###)) propose various filtering strategies to improve data quality. Our work broadly applies on top of these pre-training datasets.\nData grouping in language model training\nRecent transformer language models have adopted different strategies to group training data into batched sequences in order to tackle the variable document length problem. For encoder-only models, the choice of data formatting was first studied in RoBERTa (Liu et al., 2019  ###reference_b35###), which shows that concatenating sentences from more than one documents in the same training sequence results in very little performance degradation. Krell et al. (2021  ###reference_b28###) proposed an approximation-based combinatorial packing method to accelerate BERT training (Devlin et al., 2019  ###reference_b13###), yet without improving downstream performance.\nIt is worth mentioning that document truncation is less of a concern for encoder models for two reasons: first, they are usually trained on relatively short text spans of 128-512 tokens that only respect sentence boundary. In such case, document-wise truncation is inevitable given the limited context size. Second, they are not intended for open-ended generation, and thus, hallucination is not an issue.\nDecoder-only language models have predominantly adopted the concatenate-then-split strategy to maximize training efficiency (Brown et al., 2020  ###reference_b4###; Chowdhery et al., 2022  ###reference_b8###; Rae et al., 2021  ###reference_b49###; Zhang et al., 2022  ###reference_b70###; Touvron et al., 2023b  ###reference_b64###; Scao et al., 2022  ###reference_b56###). Very recently, Shi et al. (2024  ###reference_b58###) proposed to concatenate semantically relevant documents into the same training sequence, which yields notable improvement on downstream tasks. Nevertheless, the method as a variant of concatenation still suffers from excessive truncation, and it is orthogonal (and possibly complimentary) to our method.\nIntegration with LLM Training Framework\nBest-fit Packing operates on data level and can be handled as an offline process. Thus, it does not require any change in the training implementation and can be integrated into common distributed training frameworks like Megatron-LM (Shoeybi et al., 2019  ###reference_b59###) or DeepSpeed (Rasley et al., 2020  ###reference_b53###) easily.\nHallucination in Language Generation\nWith the rapid development of generative language models of large scale, hallucination has attracted increased attention as it can hinder performance and mislead users with fabricated facts (Ji et al., 2022  ###reference_b20###). Various approaches have been proposed to tackle this problem, including retrieval augmented generation (Peng et al., 2023  ###reference_b48###; Kang et al., 2023  ###reference_b25###), prompt engineering (Si et al., 2023  ###reference_b60###; Ji et al., 2023  ###reference_b21###), context-aware decoding (Shi et al., 2024  ###reference_b58###), and supervised finetuning (Tian et al., 2023  ###reference_b62###). However, hallucination mitigation during the pre-training stage has largely been overlooked, and we are among the first to explore in this direction."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The prevalent concatenate-then-split approach of data grouping in language model training inevitably results in fragmentation of documents. We show that this truncation effect undermines models\u2019 ability to follow the context, and even worse, makes models more prone to hallucination. Motivated by these, we propose Best-fit Packing, a new data grouping method that maximally preserves the entirety of individual documents. The algorithm is scalable for datasets of billions of documents, and maintains the same level of compactness as concatenation. Empirically, we demonstrate the effectiveness of truncation reduction by comparing models trained with different data grouping strategies at various scales across both text and code.\nSpecifically, we show that by eliminating unnecessary truncations, Best-fit Packing excels in a broad range of tasks without compromising performance on others. Additionally, it effectively reduces closed-domain hallucination in language generation. While the experiments conducted in this paper have primarily focused on the pre-training stage, Best-fit Packing is broadly applicable to the finetuning stage as well. Our work contributes to the ongoing efforts in developing more effective and reliable language models."
        }
    ],
    "url": "http://arxiv.org/html/2404.10830v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5",
            "6"
        ],
        "methodology_sections": [
            "3",
            "3.1"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2404.10830v2",
        "paper_title": "Fewer Truncations Improve Language Modeling",
        "research_background": "The paper is motivated by the desire to improve the quality of language modeling by addressing a critical issue with the common practice of data formatting during the training of large language models (LLMs). The authors identify that the concatenate-then-split approach to preparing training data, while efficient in terms of minimizing padding, results in the fragmentation of documents. This fragmentation leads to a loss of context and information, potentially making the models more prone to hallucination.\n\n**Research Problem:**\nThe paper tackles two primary concerns:\n1. How to preserve data integrity by retaining the entirety of documents during the training of language models.\n2. How to achieve this without sacrificing the training efficiency that the concatenate-then-split method provides.\n\nTo address these concerns, the authors propose a new data grouping method called Best-fit Packing that minimizes truncations and retains training efficiency. This method essentially tackles an NP-hard bin packing problem using an optimized Best-Fit-Decreasing algorithm.\n\n**Relevant Prior Work:**\nThe authors build on previous methods and insights from several areas:\n1. Work demonstrating the success of LLMs: The references (Brown et al., 2020; Chen et al., 2021; Ouyang et al., 2022) underscore the effectiveness and widespread adoption of LLMs in various tasks.\n2. Existing data preparation approaches: Prior works (Brown et al., 2020; Chowdhery et al., 2022; Rae et al., 2021; Zhang et al., 2022; Touvron et al., 2023b; Scao et al., 2022; Nijkamp et al., 2023) illustrate the common practice of using the concatenate-then-split method for formatting training data, emphasizing its efficiency due to the elimination of padding.\n3. Existing data grouping strategies: Encoder-only and encoder-decoder models typically use strategies that preserve the entirety of individual samples (Liu et al., 2019; Raffel et al., 2020b; Krell et al., 2021). However, these methods either do not scale well or compromise training efficiency, thus are less favorable at the scale required for LLM training.\n4. The optimization problem basis: Best-fit Packing is grounded in the bin packing problem, a classic optimization challenge (Bernhard & Vygen, 2008; Eilon & Christofides, 1971).\n\nBy integrating these insights, the paper proposes a novel method to address the identified issues, ultimately aiming to enhance the performance of language models by reducing unnecessary document truncations while keeping the training process efficient.",
        "methodology": "### Methodology\n\n#### Proposed Method or Model\n\nThe methodology presents a novel approach to efficiently group training data for language modeling while reducing unnecessary truncations. The key innovation lies in the way documents are segmented and packed into training sequences. Here are the core components and steps:\n\n1. **Document Segmentation**: \n    - Every document is divided into chunks with lengths limited to the model\u2019s maximum sequence length, denoted as \\( L \\). This segmentation ensures minimal truncation, constrained only by the context length.\n    \n2. **Training Sequence Construction**: \n    - To formulate each training sequence, multiple document chunks are selected to occupy as much of the \\( L \\)-token space as possible, doing so without splitting any chunks. This method ensures efficient use of the context window while preserving the integrity of the document chunks.\n\n3. **Packing Algorithm (detailed in \u00a73.1)**:\n    - The packing algorithm is designed to decide the optimal selection and arrangement of chunks into sequences. Two main challenges addressed are:\n        - **Minimizing Padding**: Fully utilizing the \\( L \\)-token space to reduce padding tokens, which are used when it's infeasible to fill the sequence completely. The increase in training sequences due to padding implies more training steps per epoch, which must be optimized for efficiency.\n        - **Scalability and Speed**: The algorithm must be powerful enough to handle very large datasets, potentially comprising billions of documents.\n\nThis combined segmentation and packing strategy aims to enhance the training efficiency of language models by reducing unnecessary breaks in documents and limiting the number of padding tokens needed. With this method, the training process can operate more effectively, particularly on large-scale datasets.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n- **Architecture:** The models follow the same transformer architecture as LLaMA (Touvron et al., 2023a).\n- **Datasets:**\n  - **Text:** Falcon RefinedWeb dataset (Penedo et al., 2023)\n  - **Code:** The Stack (Kocetkov et al., 2022)\n- **Downstream Tasks:** A variety of tasks are used for evaluation, employing zero-shot and few-shot prompting.\n\n**Baselines:**\n\n- **Main Baseline Comparison:** The effectiveness of Best-fit Packing is compared against the more commonly used method of concatenation.\n\n**Evaluation Metrics:**\n\n- **Performance Metrics:** Relative improvements in task performance are reported unless otherwise noted.\n- **Statistical Significance:** Results are verified using a paired t-test, with significant results marked by a superscript **s** and non-significant cases by **n**.\n\n**Main Experimental Results:**\n\n- **Reading Comprehension:** +4.7% improvement\n- **Natural Language Inference:** +9.3% improvement\n- **Context Following:** +16.8% improvement\n- **Program Synthesis:** +15.0% improvement\n  \nOverall, Best-fit Packing consistently outperforms or matches the concatenation baseline across all tasks evaluated. Importantly, no statistically significant degradation in performance was observed, indicating a monotonic improvement with the use of Best-fit Packing."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To empirically validate the effectiveness of Best-fit Packing over concatenation in language model training, covering different domains, sizes, and context lengths.",
            "experiment_process": "Pre-train transformer language models using the same architecture as LLaMA on the Falcon RefinedWeb dataset for text and the Stack for code. Evaluate models on a variety of downstream tasks with zero-shot and few-shot prompting.",
            "result_discussion": "Best-fit Packing improves performance in various tasks, including reading comprehension (+4.7%), natural language inference (+9.3%), context following (+16.8%), and program synthesis (+15.0%). Best-fit Packing effectively reduces closed-domain hallucination and shows no statistically significant degradation in any task.",
            "ablation_id": "2404.10830v2.No1"
        },
        {
            "research_objective": "Assess models' performance on reading comprehension tasks to determine the efficacy of Best-fit Packing.",
            "experiment_process": "Evaluate 13B natural language models with 5-shot in-context learning on Narrative QA, Natural Questions, SQuAD, and DROP; 1-shot on QuAC; and zero-shot on BoolQ and RACE. Report F1 score or exact match (EM) for generation tasks and accuracy for multiple-choice tasks.",
            "result_discussion": "Best-fit Packing outperforms concatenation significantly in half the settings, especially in open-ended generation tasks where context-awareness is crucial. Minimal improvement in Natural Questions is observed due to the open-book nature of the questions.",
            "ablation_id": "2404.10830v2.No2"
        },
        {
            "research_objective": "Evaluate models' capabilities in natural language inference to understand how well they can preserve sentence dependencies.",
            "experiment_process": "Use 5-shot in-context learning for Multi-NLI and RTE and compare the performance of models trained with Best-fit Packing versus concatenation.",
            "result_discussion": "Best-fit Packing improves NLI performance by up to +9.3%, suggesting that reducing truncations helps preserve sentence dependencies.",
            "ablation_id": "2404.10830v2.No3"
        },
        {
            "research_objective": "Validate if excessive truncations affect the models' ability to follow context and instructions.",
            "experiment_process": "Evaluate models with 5-shot on NQ-Swap and zero-shot on MemoTrap to test their ability to follow context instructions that conflict with parametric knowledge.",
            "result_discussion": "Best-fit Packing excels in both settings by up to +16.8%, showing that it enhances models' context-awareness and ability to follow in-context instructions.",
            "ablation_id": "2404.10830v2.No4"
        },
        {
            "research_objective": "Determine if excessive truncations impact the accuracy and faithfulness of model summaries.",
            "experiment_process": "Conduct 5-shot evaluation on CNN/DailyMail and XSUM, using metrics like ROUGE for accuracy and FAVA for factuality on XSUM, and SummaC and QAFactEval for faithfulness on CNN/DailyMail.",
            "result_discussion": "Best-fit Packing generally results in higher ROUGE scores and better faithfulness, reducing hallucinations. Models trained with packing perform better at following length constraints in summaries.",
            "ablation_id": "2404.10830v2.No5"
        },
        {
            "research_objective": "Assess how well models trained with Best-fit Packing perform on commonsense and world knowledge benchmarks.",
            "experiment_process": "Use 5-shot in-context learning for SIQA, ARC, and TriviaQA, and zero-shot for HellaSwag and PIQA. Report exact match for TriviaQA and multiple-choice accuracy for the rest.",
            "result_discussion": "Best-fit Packing is slightly better on average and shows significant gains on ARC-C. The hypothesized reason is that truncation reduction helps in better learning of less frequent 'tail' knowledge.",
            "ablation_id": "2404.10830v2.No6"
        },
        {
            "research_objective": "Evaluate the performance of programming language models on code generation tasks using Best-fit Packing.",
            "experiment_process": "Evaluate 7B programming language models on HumanEval and MBPP for zero-shot code generation. Generate 200 samples per problem and report Pass@ for functional correctness and undefined name errors for hallucination detection.",
            "result_discussion": "Best-fit Packing improves Pass@ (+15.0% for Pass@100 on HumanEval and +5.8% on MBPP) and significantly reduces undefined name errors by up to 58.3%, indicating better functional correctness and reduced hallucination.",
            "ablation_id": "2404.10830v2.No7"
        }
    ]
}