{
    "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences.\nBut training models with RLHF is computationally expensive, and an overall complex process.\nIn this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. (2021).\nWe investigate the setup of \u201cParameter Efficient Reinforcement Learning\u201d (PERL), in which we perform reward model training and reinforcement learning using LoRA.\nWe compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning.\nWe find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory.\nThis enables the high performance of RLHF, while reducing the computational burden that limits its adoption as an alignment technique for Large Language Models.\nWe also release 2 novel thumbs up/down preference datasets: \u2018Taskmaster Coffee\u2019, and \u2018Taskmaster Ticketing\u2019 to promote research around RLHF.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Pretrained Large Language Models (LLMs), such as GPT-4 (OpenAI et al., 2023  ###reference_b54###), and Gemini (Team et al., 2023  ###reference_b70###; Reid et al., 2024  ###reference_b61###) have shown impressive performance in a variety of tasks.\nAligning these models with human preferences is key to ensuring high quality behavior (Bommasani et al., 2022  ###reference_b7###): it improves instruction following (Ouyang et al., 2022  ###reference_b55###), and helps fine-tune models for behaviors which lack a direct mathematical expression of a loss function, such as safety properties (Bai et al., 2022a  ###reference_b5###, b  ###reference_b6###), or characteristics of summarization (Stiennon et al., 2020  ###reference_b68###).\nReinforcement Learning from Human Feedback (RLHF) is one of the most popular methods to achieve this alignment.\nIt involves fitting a reward model (RM) on human preference data, and then uses this RM to tune the parameters of the LLM using Reinforcement Learning (RL).\nWe provide more details about the RLHF method in Appendix A.1  ###reference_###.\nWhile RLHF has been shown to be an effective method for alignment (Stiennon et al., 2020  ###reference_b68###; Bai et al., 2022b  ###reference_b6###), the complexity and computational cost of its process has hindered its adoption:\nthe RL loop, for example, requires at least twice the memory of standard fine-tuning, as it needs multiple instances of LLMs, such as a reward model and an anchor model (for KL regularization) to train properly.\nAnother challenge to the wider adoption of RLHF lies in the difficulty to collect enough high quality training data to create an effective reward model.\nIn this work, we investigate how Parameter Efficient Fine-Tuning (PEFT) approaches (Houlsby et al., 2019  ###reference_b34###) can make RLHF more efficient and accessible.\nWe compare the performance of conventional RLHF, for which all the parameters of the reward model and the policy are fine-tuned, to Parameter Efficient Reinforcement Learning, referred to as PERL, which uses a PEFT method (Low-Rank Adaptation (Hu et al., 2021  ###reference_b35###)) to fine-tune the reward model and an RL Policy.\nOur contributions in this paper are as follows:\nThrough extensive experiments, we evaluate the effects of LoRA on two dimensions of the training of the RM and RL policy: model size, and number of LoRA trainable parameters.\nWe compare PERL to conventional RLHF on 7 datasets, and show that PERL achieves results on par with the conventional setup which is commonly used, while being more memory efficient, and faster to train.\nWe release two new preference datasets (thumbs up/down), the Taskmaster Coffee, and Ticketing datasets, which we hope will be useful when evaluating alignment methods."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
            "text": "We describe how PERL makes the RLHF process more efficient.\nFor an overview on RLHF, please refer to Appendix A.1  ###reference_###.\nThere are 2 model training processes as part of the RLHF process: reward model training, and reinforcement learning.\n###figure_1### ###figure_2### We construct the PERL reward models as language models with LoRA adapters.\nWe attach a LoRA adapter to each attention projection matrix, and only train these adapters, while keeping the language model backbone frozen.\nWe depict this process in Figure 1  ###reference_###.\nThe trained LoRA adapters are saved, and combined with the projection matrices when running inference with the reward model.\nThis one-time addition operation is performed before running the RL loop, and the resulting reward model is equivalent to a non-LoRA one.\nThe PERL reinforcement learning loop similarly trains language models with LoRA adapters as policy models.\nWe attach a LoRA adapter to each attention projection matrix, and only train the LoRA adapters, while keeping the language model backbone frozen.\nThe policy is trained with the policy gradient calculated on the reward score, and a KL regularization with the anchor policy.\nWe depict this process in Figure 2  ###reference_###.\nThe bulk of the memory required to train a reward model or a reinforcement learning loop is due to modern optimizers, like Adam (Kingma and Ba, 2017  ###reference_b43###), or Adafactor (Shazeer and Stern, 2018  ###reference_b65###), which need to track multiple factors for every trainable parameter.\nThe reduction in the number of trainable parameters that PERL operates leads to a significant reduction in memory requirement for training.\nIt also leads to faster training speed, because fewer parameters need to be updated at each training step."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Datasets and Tasks",
            "text": "We describe the datasets and tasks we used to train reward models and perform reinforcement learning.\nThe datasets consist of feedback data of either preference pairs, or classification labels used to train reward models. The preference or classification labels are collected for candidate responses sampled from diverse set of policies. As it is helpful for reward models to observe as broad a domain as possible during training.\nIn the reinforcement learning loop, we use inputs from the datasets as prompts to the LLMs to sample episodes, and make policy updates based on the reward calculated for the sampled episodes."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Text Summarization",
            "text": "Text summarization is one of the tasks that has been the most extensively studied using RLHF.\nStarting from a large text, the goal is to produce a smaller, yet faithful summary.\nWe explore it with two different datasets: Reddit TL;DR (V\u00f6lske et al., 2017  ###reference_b73###), and BOLT (Broad Operational Language Translation) English SMS/Chat (Chen et al., 2018  ###reference_b14###)."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Reddit TL;DR",
            "text": "We experiment with Reddit post summarization, where a Reddit post is provided, and the model produces a summary of it.\nWe base our experiments on the TL;DR dataset (V\u00f6lske et al., 2017  ###reference_b73###), which contains both Reddit posts, and a human preference dataset.\nWe filtered this dataset following the work of Stiennon et al. (2020  ###reference_b68###).\nMore details about this dataset can be found in Appendix A.2.1  ###reference_.SSS1###."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 BOLT Message Summarization",
            "text": "We explore the task of summarizing sequences of messages.\nSuch a model could be used, for instance, to show a short and accurate summary to a user, enabling them to understand the context of a conversation without reading every message in it.\nWe experiment with the BOLT English SMS/Chat dataset (Chen et al., 2018  ###reference_b14###).\nThis dataset was generated by paying participants to upload their conversations from SMS and chat applications, and also by randomly matching\nparticipants for conversations on a chat platform.\nMore details about this dataset can be found in Appendix A.3.1  ###reference_.SSS1###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Generating Harmless Responses",
            "text": "We explore the task of preference modeling on harmless responses by using Anthropic\u2019s Helpfulness and Harmlessness (Red Teaming) dataset (Bai et al., 2022a  ###reference_b5###).\nThis dataset consists of a pair of model generated responses for helpfulness examples seeking informational user queries, and harmlessness examples with users eliciting harmful responses from the model.\nWe use the harmlessness part of the dataset to conduct our experiments.\nThe pairwise harmlessness preference dataset labels the least harmful response generated by the model to a user elicited context.\nMore details about this dataset can be found in Appendix A.4.1  ###reference_.SSS1###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Generating Helpful Responses",
            "text": "The Stanford Human Preferences Dataset (SHP) (Ethayarajh et al., 2022  ###reference_b24###) is derived from Reddit questions/instructions, and top comments.\nIt consists of 385,563 Reddit questions/instructions, and top-level comments for the corresponding posts.\nThe data is split into a training set (), a validation set (), and a test set ().\nThe posts are sampled from 18 domains, such as anthropology, legal advice etc.\nThe SHP dataset, unlike the ELI5 one (Fan et al., 2019  ###reference_b27###), makes use of timestamp information to infer that a comment is preferred to another one only if it has received more votes, and has not been visible for longer (to avoid the introduction of a bias favoring older posts).\nThe SHP dataset differs from the Anthropic-HH dataset in that it focuses on helpfulness only (as opposed to both helpfulness and harmlessness for Anthropic-HH).\nThe data in SHP is also human written, whereas Anthropic-HH is made of machine written responses.\nDetailed information on the domains used, and how they compose the mix of examples can be found in Table 18  ###reference_### of Appendix A.5.1  ###reference_.SSS1###."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "UI Automation",
            "text": "We experiment on the task of controlling a device using a UI automation dataset collected from human demonstrations.\nThis dataset, which will be released as part of concurrent work, is composed of around 3,000 training, and 800 test episodes.\nEach episode is a sequence of observation and action pairs.\nAn observation is derived from the Android View Hierarchy, plus additional information, such as the active app, or the history of previously performed actions.\nAn action is one of a dozen possible actions (such as clicking on a UI element, scrolling, etc.).\nThis UI demonstration dataset only contains ground truth trajectories.\nWe augment it to include negative samples so as to train a classifier that we use as a reward model, by replacing the ground truth action with a different, random one."
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "3.4.1 Training Data Preparation",
            "text": "We split the 8,000 sample dataset into a training set of 7,000 random examples, and a test set of 1,000 samples.\nThese samples are generated from a randomly chosen subset of the dataset.\nOut of the 8,000 examples, we randomly select half of them and replace the correct action by a randomly wrong one.\nWe call the examples leading to the right action \"Positive\", and the ones with a wrong action \"Negative\".\nSplit details on the dataset are given in Table 1  ###reference_###.\nMore information on how this dataset was generated can be found in Appendix A.6.1  ###reference_.SSS1###."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Generating Neutral Point of View Responses",
            "text": "We train a model to generate answers to controversial topics with a neutral point of view.\nWe built a dataset consisting of 300 triplets of (question, answer, rating) spanning 93 topics.\nHumans trained on this task generated both questions and answers, which where then scored between 0 and 5 by high-quality annotators based on both style and neutrality.\nAmong those 300 examples, 136 are deemed adequate answers (i.e. score >= 4).\nThe rest of the answers\u2019 scores spans the whole range between 0 and 4, which helps with training a classifier that we use as a reward model.\nThe dataset was split into train, test and validation.\nMore information about this dataset is given in Appendix A.7.1  ###reference_.SSS1###."
        },
        {
            "section_id": "3.6",
            "parent_section_id": "3",
            "section_name": "Taskmaster Datasets",
            "text": "The Taskmaster datasets were created by paying crowdsourced (Mechanical Turk) workers to produce conversational ordering interactions using a custom-built, text-based dialogue creation tool.\nThis meant either writing turns for both speakers, i.e. the customer and the agent, or filling in specific turns for partially completed conversations.\nEach conversation was annotated with function calls interleaved within the dialogue turns, signalling when to interact with external sources in order to perform tasks in the real world.\nExample tasks that were performed include looking up menu items or movies, adding items to an order, booking tickets, etc.\nThe calls are API-agnostic, so they can easily be converted to any format needed when deployed.\nThe training datasets were used to fine-tune a LaMDA model (Thoppilan et al., 2022  ###reference_b71###) which was served along with an \u2018API adapter\u2019 that mapped each function call to a real API.\nThe trained LaMDA model was deployed to another set of crowdsourced workers who were paid to interact with it, and label its responses (conversation turns, or API calls) with thumbs up/down.\nWe collected 3,000 (dialogue, reward) pairs in this manner, where the reward is a thumbs up/down.\nMore details on the data collection methodology, as well as examples and UI screenshots for creating these datasets are given in Appendix A.8.1  ###reference_.SSS1###."
        },
        {
            "section_id": "3.6.1",
            "parent_section_id": "3.6",
            "section_name": "3.6.1 Taskmaster Coffee",
            "text": "The Taskmaster Coffee dataset consists of 6,500 multi-turn conversations, consisting of 20,000 training examples (conversation turns or API calls), and 3,000 reward examples.\nThe reward datasets contains 2,775 examples labeled with thumbs up, and 453 with thumbs down.\nWe have publicly released this dataset and the instructions used to create it at https://github.com/google-research-datasets/Taskmaster/tree/master/TM-4-2024  ###reference_s/Taskmaster/tree/master/TM-4-2024###."
        },
        {
            "section_id": "3.6.2",
            "parent_section_id": "3.6",
            "section_name": "3.6.2 Taskmaster Ticketing",
            "text": "The Taskmaster Ticketing dataset contains 30,000 multi-turn conversations, consisting of 240,000 training examples and 3,000 reward examples.\nThe reward datasets contains 1,146 examples labeled with thumbs up, and 2,210 with thumbs down.\nThis dataset and related materials are publicly released at https://github.com/google-research-datasets/Taskmaster/tree/master/TM-3-2020  ###reference_s/Taskmaster/tree/master/TM-3-2020###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Reward Model Training",
            "text": "We experiment on the datasets mentioned in Section 3  ###reference_###, varying the configurations of model size, and LoRA adapter rank (which controls the number of trainable parameter).\n###figure_3### We evaluate the performance of the preference based reward models using pairwise accuracy as a metric, measuring the proportion of preferred responses ranked higher by the model among pairs of candidate responses. We evaluate the classification style reward models using the accuracy as a metric, indicating whether the reward model score is close to the label of 0 or 1. We compare High Bandwidth Memory (HBM) usage as estimated by Jax JIT at the time of training (Bradbury et al., 2018  ###reference_b9###).\nOur experiments across datasets show that we can LoRA train reward models to match the performance of fully tuned ones with less than 0.1% of the parameters being tuned.\nWe also observe that LoRA generally becomes more effective at matching full-tuning as we increase the LLM size.\nAcross all our datasets, the reward models can be tuned with approximately 50% of the memory needed for full-tuning.\nThe training converges in a similar number of steps for LoRA and full tuning, and the LoRA models are roughly  faster to train. The HBM usage and training speedup vary a little with each dataset as they depends on sequence lengths of the examples, which is different for all datasets.\nWe report detailed metrics for each dataset below."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Summarization - Reddit TL;DR",
            "text": "We train reward models using preference pairs of summaries on the dataset described in Section 3.1.1  ###reference_.SSS1###.\nWe report the hyperparameters we used in Appendix A.2.2  ###reference_.SSS2###."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 LoRA Adapter Rank",
            "text": "We observe that LoRA fine-tuning can model reward just as well as full-tuning with adapters of rank at least 4, training as little as  of the model\u2019s total parameters.\nThe estimated High Bandwidth Memory (HBM) needed to train a LoRA rank 4 model is just  of that needed to train a fully tuned one.\nWe also observe that the pairwise accuracy doesn\u2019t change significantly with the LoRA rank.\nWe list our results in Table 2  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Harmless Dialogue Generation",
            "text": "We train reward models in different settings using preference pairs of responses as mentioned in section 3.2  ###reference_###.\nWe evaluate the performance of the trained reward models using pairwise accuracy, which measures the proportion of preferred responses ranked higher by this model among pairs of candidate responses.\nWe report the hyperparameters used in Appendix A.4.2  ###reference_.SSS2###."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 LoRA Adapter Rank",
            "text": "The performance of LoRA trained reward models is on par to that of fully trained ones on the Anthropic Harmlessness preference dataset with all the low ranks we experimented with.\nWe obtain our best results with a LoRA rank 8 model, training less than  of the model parameters, with half peak HBM memory, and a  computational speed up.\nWe report our results in the Table 4  ###reference_###."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Model Size",
            "text": "The pairwise accuracies of both the LoRA tuned and fully tuned models benefit from higher model sizes, with LoRA model accuracies increasing more.\nWe report our results in Table 5  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Helpful Dialogue Generation",
            "text": "We train LoRA and fully tuned reward models on the preference dataset introduced in Section 3.3  ###reference_###.\nWe report pairwise accuracies. Details on the hyperparameters used are given in Appendix A.5.2  ###reference_.SSS2###."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 LoRA Adapter Rank",
            "text": "Varying the LoRA rank on the SHP datasets also yields comparable results as with full tuning.\nLoRA trained models have a similar memory and compute time advantage as with the datasets seen so far.\nOur best results are obtained with a LoRA rank of 1 on PaLM XS, training just  of the parameters, saving  on the peak HBM usage, and training  faster.\nWe list our results in the Table 6  ###reference_###."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Model Size",
            "text": "We compare the accuracy of fully tuned and LoRA tuned Reward Models on PaLM 2 XXS, and XS.\nWe report the results with the best LoRA rank out of the set {1, 4, 8, 16}.\nBoth LoRA and fully tuned models benefit slightly from the model size increases, and they have similar pairwise accuracies.\nWe report our results in Table 7  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "UI Automation",
            "text": "We train a reward model to score UI automation actions on the dataset described in Section 3.4  ###reference_###.\nUI automation data has two types of instructions: low-level instruction, e.g. \"Click on the search button\", and high-level instructions, e.g. \"Search for \u2018machine learning\u2019\".\nIt also has two different split schemes:\nUnseen App, for which the test split uses apps not in the train split\nUnseen Task, for which the test split has tasks not in the train split\nWe report classification accuracy numbers in Table 10  ###reference_###.\nWe report the hyperparameters used in Appendix A.6.2  ###reference_.SSS2###."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 LoRA Adapter Rank",
            "text": "We train LoRA and fully tuned reward models on the PaLM 2 S model.\nWe evaluate these models using a classification accuracy.\nAs shown in Table 8  ###reference_###, the LoRA reward models, while training less than  of the total number of parameters, achieve similar accuracy than the fully tuned one.\nFurthermore, the LoRA models train almost twice as fast, and have a  lower peak HBM usage."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Model Size",
            "text": "The same trend of accuracy with model size is observed in the UI automation task, and both fully tuned, and LoRA tuned models seem to benefit from larger architectures.\nThey achieve similar accuracies, as seen in Table 9  ###reference_###."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Reward Model as UI Automation Evaluator",
            "text": "The evaluation of the reward models show that the UI automation agent model outputs can be evaluated with high accuracy using such a model.\nWe compare this reward model as an evaluator to a heuristic previously used, based on the distance between the position of the click of the action, and a ground truth.\nThe reward model outperforms the heuristic model by a large margin, as shown in Table 10  ###reference_###."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Neutral Point of View (NPOV) Responses",
            "text": "We trained a reward model on data with 5-point scale discrete scores (increments of 0.5) to judge the neutrality of statements on sensitive topics, and evaluated it with a pairwise evaluation set of 437 examples.\nThe pairwise accuracy (PA) is computed based on whether the preferred side (by label) has a higher score.\nDuring the evaluation, pairwise examples for which at least one example was used during RM training are excluded.\nMore details on the dataset used are available in Appendix A.7.1  ###reference_.SSS1###."
        },
        {
            "section_id": "4.5.1",
            "parent_section_id": "4.5",
            "section_name": "4.5.1 LoRA Adapter Rank",
            "text": "We study the effect of LoRA rank on the accuracy of our NPOV reward model.\nWe find that the LoRA trained models have similar accuracy as the fully trained ones, while using  of the peak HBM, and training  faster.\nDetailed results are given in Table 11  ###reference_###."
        },
        {
            "section_id": "4.5.2",
            "parent_section_id": "4.5",
            "section_name": "4.5.2 Model Size",
            "text": "We compare the performance of fully tuned reward models to that of LoRA trained ones on different model sizes: PaLM XXS, XS, and S.\nWe report the results with the best LoRA rank out of the set {1, 4, 8, 16} for each model size (8 for XXS and XS, and 1 for S) in Table 12  ###reference_###.\nThe LoRA trained models have similar accuracies as the fully trained ones.\nBoth LoRA models and fully trained ones benefit from bigger model sizes, with LoRA accuracies increasing more as the architecture gets bigger."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Taskmaster Coffee Dataset",
            "text": "We train classification reward models on the Taskmaster Coffee dataset."
        },
        {
            "section_id": "4.6.1",
            "parent_section_id": "4.6",
            "section_name": "4.6.1 LoRA Adapter Rank",
            "text": "To investigate the impact of different LoRA ranks on the learning efficiency of the reward model, we conducted experiments using the PaLM 2 XS model.\nLoRA trained reward models achieve similar accuracies as the fully tuned ones, while reducing the HBM usage by , and training  faster.\nDetailed results are presented in Table 13  ###reference_###."
        },
        {
            "section_id": "4.6.2",
            "parent_section_id": "4.6",
            "section_name": "4.6.2 Model Size",
            "text": "As shown in Table 14  ###reference_###\n, the larger PaLM 2 XS model achieves higher accuracies than the smaller model PaLM 2 XXS.\nThis observation stands for both the LoRA and fully tuned versions."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Reinforcement Learning",
            "text": "###figure_4### We train reinforcement learning policies on the summarization datasets described in Section 3.1  ###reference_###, varying the model size, and LoRA adapter rank (which controls the number of trainable parameters).\nWe conduct these experiments using the \"REINFORCE for Language Models\" algorithm used by Lee et al. (2023a  ###reference_b45###), tuning a LLM policy using reinforcement learning and a standard reward model per dataset trained in Section 4  ###reference_###.\nWe evaluate the performance of the learned policy by the max cumulative reward it can achieve as judged by the reward model.\nWe compare peak High Bandwidth Memory (HBM) usage as estimated by Jax JIT at the time of training (Bradbury et al., 2018  ###reference_b9###).\nWe also compare the speed of training by measuring the time each step takes end-to-end, i.e. episode sampling, reward scoring, calculating KL regularization, and performing the learning step.\nOur experiments across datasets show that we can LoRA train policies with reinforcement learning that come very close to the performance of fully tuned ones with less than 0.1% of the parameters being tuned.\nWe also observe that LoRA generally becomes more effective at matching full-tuning as we increase the LLM size.\nFor both datasets we have experimented with, the policies can be tuned with approximately 76% of the memory needed for full-tuning.\nThe training converges in a similar number of steps for LoRA and full tuning, and the LoRA models are roughly  faster to train."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Reddit TL;DR Summarization",
            "text": "We perform reinforcement learning with input prompts from the Reddit dataset introduced in Section 3.1.1  ###reference_.SSS1###, and episodes sampled from them.\nWe use a fully tuned Palm 2 S reward model trained in Section 4.1  ###reference_###.\nWe report the maximum cumulative reward score as judged by this reward model as an evaluation of the policy.\nWe report our hyperparameter selection process for all experiments in Appendix A.2.2  ###reference_.SSS2###."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 LoRA Rank",
            "text": "We study the effect of LoRA rank on effectiveness of learning reinforcement learned policy on the Reddit rataset by LoRA training an RL policy with varying ranks.\nWe observe a monotonic increase in cumulative reward with increasing LoRA ranks.\nWe observe that a policy trained with LoRA Rank 16 comes the closest to performance of a fully tuned policy in terms of the reward score.\nWe achieve this performance by training only  of the policy model\u2019s total parameters.\nThe LoRA rank 16 policy RL tuning peaks at  of the HBM needed for the RL tuning of the fully tuned policy.\nWe also observed a  speed-up in the training time of the LoRA policy, as compared that of the fully tuned one.\nWe list our results in Table 15  ###reference_###."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2 Model Size",
            "text": "We vary the size of the policy model to see how it affects the maximum reward achieved by the policy learned.\nWe find that the maximum reward increases with the model size for both full RL tuning, and LoRA RL tuning.\nLarger models perform better across full RL tuning and LoRA RL tuning.\nWe report these results in Table 16  ###reference_###."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "BOLT Message Summarization",
            "text": "We perform reinforcement learning with input prompts from the BOLT message summarization dataset introduced in Section 3.1.2  ###reference_.SSS2###, and episodes sampled from them.\nWe use a fully tuned PaLM 2 XS reward model trained in Section 4.1  ###reference_###.\nWe finetune PaLM 2 XXS policies.\nWe detail the hyperparameter selection process for all the experiments in Appendix A.3.2  ###reference_.SSS2###.\nWe evaluate the trained policies using two metrics, in addition to the maximum cumulative reward score, to see whether our results generalize beyond the cumulative reward score:\nSummary Quality: the output of a model trained to predict the probability that a human rater will answer \"Yes\" for all 6 questions defined in SEAHORSE (Clark et al., 2023  ###reference_b18###): comprehensibility, non repetition, grammar, attribution, main ideas, and conciseness.\nNatural Language Inference (NLI): the output of a model trained to predict entailment. Higher numbers are better. Low values likely indicate substantial hallucinations."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 LoRA Rank",
            "text": "As we vary the LoRA rank of the policy model, we observe that we can match the summary quality and the grounding of the fully tuned RL policy with a policy of LoRA rank 8.\nWe achieve this result by training  of the model\u2019s total parameters.\nWe list our results in Table 17  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Pretrained Large Models",
            "text": "Pretrained Large Models (PLMs - (Brown et al., 2020  ###reference_b11###; Smith et al., 2022  ###reference_b66###; Thoppilan et al., 2022  ###reference_b71###; Workshop et al., 2022  ###reference_b77###; Hoffmann et al., 2022  ###reference_b33###; Chowdhery et al., 2022  ###reference_b16###; Touvron et al., 2023  ###reference_b72###; Anil et al., 2023  ###reference_b2###; OpenAI et al., 2023  ###reference_b54###; Jiang et al., 2023  ###reference_b41###)) have shown strong performance in a variety of tasks.\nThey have been used in many different applications, such as summarization (Stiennon et al., 2020  ###reference_b68###), instruction following (Ouyang et al., 2022  ###reference_b55###; Lai et al., 2023  ###reference_b44###), conversational recommender systems (Friedman et al., 2023  ###reference_b29###), question answering (Nakano et al., 2022  ###reference_b52###), personalized dialogue generation (Jandaghi et al., 2023  ###reference_b39###), text annotation tasks (Gilardi et al., 2023  ###reference_b31###), audio generation (Borsos et al., 2023  ###reference_b8###), robotics tasks (Driess et al., 2023  ###reference_b23###), vision tasks (Chen et al., 2023  ###reference_b15###), and more (Ziegler et al., 2020  ###reference_b84###).\nPretrained (Radford et al., 2018  ###reference_b57###; Ramachandran et al., 2016  ###reference_b59###), and instruction-tuned (Wei et al., 2021  ###reference_b76###) large models, while demonstrating impressive performance on a multitude of tasks, still exhibit many limitations, such as returning non-factual outputs, or not following human instructions correctly."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Aligning Pretrained Large Models with Human/AI Preferences",
            "text": "Aligning Pretrained Large Models with human preferences has become an important area of research to overcome some of their limitations (Christiano et al., 2017  ###reference_b17###; Leike et al., 2018  ###reference_b47###; Wang et al., 2023  ###reference_b75###; Ji et al., 2023  ###reference_b40###), and is also a paradigm commonly used to steer the behavior of PLMs towards human desiderata.\nThis alignment is generally achieved by collecting a large dataset of contexts, pairwise generations for these contexts, and related preferences.\nOverfitting the reward function can be an important issue when approximating it (Azar et al., 2023  ###reference_b4###), and methods like early stopping, or reducing the number of trainable parameters like we do in this work can help mitigate this issue, and train better policies.\nWhile most alignment techniques have so far been used on natural language tasks, some recent work has started to apply them on other modalities as well (Lee et al., 2023b  ###reference_b46###; Sun et al., 2023  ###reference_b69###).\nThe size and quality of the human preference data can often be a bottleneck to this alignment process.\nFor that reason, different approaches leveraging AI feedback (Bai et al., 2022b  ###reference_b6###; Lee et al., 2023a  ###reference_b45###) have been developed, such as Generator-Critic architectures (Jandaghi et al., 2023  ###reference_b39###; Yuan et al., 2024  ###reference_b80###).\nDifferent techniques have been developed to align PLMs with human/AI preferences, including Reward rAnked Fine-Tuning (RAFT) (Dong et al., 2023  ###reference_b22###), RRHF (Yuan et al., 2023  ###reference_b81###), Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017  ###reference_b17###; Ouyang et al., 2022  ###reference_b55###; Azar et al., 2023  ###reference_b4###), Direct Preference Optimization (DPO) (Rafailov et al., 2023  ###reference_b58###; Azar et al., 2023  ###reference_b4###), Sequence Likelihood Calibration with Human Feedback (SLIC-HF) (Zhao et al., 2023  ###reference_b82###), Pairwise Cringe Optimization (Xu et al., 2023a  ###reference_b78###), and Self-Rewarding Language Models (Yuan et al., 2024  ###reference_b80###).\nAmong these methods, RL{H/AI}F is one of the most popular.\nAs we show here, by combining RL{H/AI}F with parameter-efficient methods like LoRA, the compute and memory efficiencies of the model tuning can be improved substantially."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Reinforcement Learning from Human/AI Feedback (RL{H/AI}F)",
            "text": "Reinforcement Learning has been shown to be a useful technique to align agents with human preferences (Dewey, 2011  ###reference_b20###; Hadfield-Menell et al., 2016  ###reference_b32###; Everitt and Hutter, 2018  ###reference_b26###).\nReinforcement Learning from Human Feedback (RLHF) works by first fitting a reward model on the preferred outputs.\nThis reward model is then used to train a policy using a reinforcement learning algorithm, typically Proximal Policy Optimization Algorithms (PPO) (Schulman et al., 2017  ###reference_b64###).\nLabeling examples to train a reward model can be costly (Casper et al., 2023  ###reference_b13###), and many of the existing works had to use tens of thousands of comparison examples to obtain good reward models to train the RL loop from (Stiennon et al., 2020  ###reference_b68###; Menick et al., 2022  ###reference_b51###).\nTo alleviate this labour intensive process, some work has been replacing human feedback by AI feedback, leading to RLAIF approaches (Bai et al., 2022b  ###reference_b6###; Kim et al., 2023  ###reference_b42###; Lee et al., 2023a  ###reference_b45###)).\n(Lee et al., 2023a  ###reference_b45###) show that RLHF and RLAIF can have comparable performance."
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Parameter Efficient Fine-Tuning and Low-Rank Adaptation",
            "text": "Parameter Efficient Fine-Tuning (PEFT) methods (Houlsby et al., 2019  ###reference_b34###; Ding et al., 2022  ###reference_b21###; Xu et al., 2023b  ###reference_b79###) reduce the number of trainable parameters of a Pretrained Large Model, while maintaining comparable performance to full fine-tuning.\nThese methods are useful to adapt a PLM to downstream tasks: full fine-tuning on these tasks necessitates large amounts of labeled data to avoid overfitting, and significant compute resources.\nPEFT methods leverage the knowledge already acquired by a PLM, and can adapt a small number of parameters on a relatively small dataset.\nHugging Face\u2019s PEFT library is an example implementation of PEFT methods (HuggingFace, 2023a  ###reference_b36###).\nLow-Rank Adaptation (LoRA) (Hu et al., 2021  ###reference_b35###) is an example PEFT method, and the one that we used in our experiments.\nIt factorizes the weight update into two trainable, low rank matrices (down-projection, and up-projection).\nDuring training, only these low rank matrices are being updated.\nDepending on the rank of the decomposition, we end up training only a small fraction of the total number of parameters of the original model.\nSimilarly to the original LoRA paper, we apply the low rank projections to the attention layers, but we note that other layers can be used.\nLoRA offers the advantages that it doesn\u2019t increase the inference time (unlike adapters (Houlsby et al., 2019  ###reference_b34###), for example, which deepen the model), nor does it reduce the context window (unlike prompt-tuning (Lester et al., 2021  ###reference_b48###), for which some tokens must be used for the tunable prompts).\nHydra-RLHF (Santacroce et al., 2023  ###reference_b63###) achieves memory savings in RLHF by sharing a backbone, similarly to our work.\nThey do not, however, compare Hydra-RLHF to fully tuned RLHF.\nQLoRA (Dettmers et al., 2023  ###reference_b19###) combines quantization, and diverse optimizations with LoRA to achieve memory efficient fine-tuning.\nThese optimizations can be incorporated into our method to lead to further memory gains."
        },
        {
            "section_id": "6.5",
            "parent_section_id": "6",
            "section_name": "Infrastructure",
            "text": "The Transformer Reinforcement Learning (TRL) library (von Werra et al., 2020  ###reference_b74###) is the closest implementation of Parameter Efficient Reinforcement Learning that we are aware of.\nTheir Multi-Adapter RL (MARL) approach uses a shared base model for the entire RLHF pipeline (HuggingFace, 2024  ###reference_b38###).\nThis feature, however, is experimental and has not been tested yet.\nBesides, and to the best of our knowledge, the TRL library has only been used in combination with their Parameter Efficient Fine-Tuning (PEFT) library on (HuggingFace, 2023a  ###reference_b36###, b  ###reference_b37###) in the single GPU setting, and it only supports transformer models for text.\nIn contrast, our implementation is parallelized, and generalizes to modalities other than text.\nWe implemented our work in Python using the PAX library (Paxml, 2022  ###reference_b56###).\nOur input and evaluation pipeline relies on the SeqIO library (Roberts et al., 2022  ###reference_b62###).\nWe used an internally designed RL training loop infrastructure."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "We have presented PERL, a Parameter Efficient Reinforcement Learning technique that can train a Reward Model and RL tune a Language Model Policy with LoRA. Through extensive experiments on various datasets, we have shown that this method achieves comparable results to conventional RLHF, for which all the model parameters are tuned, while reducing memory usage by approx 50%, and speeding up the training by up to 90% for the Reward Model training, and more modest memory savings of 20%, and speed-up of 10% in the RL loop.\nAs we demonstrate the success of the PERL method, there are still some questions that are underexplored in our study.\nWe are planning to investigate the following avenues as part of our future work:\nPERL matches the performance on in-domain held out test sets, but the exploration of broader generalization would be interesting. There is room to explore ensemble models like Mixture-of-LoRA adapters (Zhu et al., 2023  ###reference_b83###) for the RLHF process at a minimal computational cost that will introduce the robustness needed in training to facilitate cross-domain generalization.\nReward models are one of the most critical pieces of the RLHF process, but they are prone to the \u201creward hacking\u201d behavior. Recent works like Ram\u00e9 et al. (2024  ###reference_b60###) show that weight-averaging models mitigate reward hacking. It would be interesting to explore this with PERL, as it can provide similar benefits by weight-averaging multiple adapters at a much lower computational cost."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T1.2.1.1\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text\" id=\"S3.T1.3.2\" style=\"font-size:90%;\">Dataset split for the UI automation task.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.4\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S3.T1.4.1.1.1\">Split</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S3.T1.4.1.1.2\">Total</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.4.1.1.3\">Positive</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.4.1.1.4\">Negative</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.4.2.2.1\">Training</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.4.2.2.2\">8,000</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.4.2.2.3\">4,007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.4.2.2.4\">3,993</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.4.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.4.3.3.1\">Evaluation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.4.3.3.2\">1,000</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.4.3.3.3\">490</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.4.3.3.4\">510</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1: Dataset split for the UI automation task."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T2.7.1.1\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" id=\"S4.T2.8.2\" style=\"font-size:90%;\">Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM, and training speed (Reddit TL;DR dataset)</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.5.6.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T2.5.6.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T2.5.6.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T2.5.6.1.1.1.1\">PaLM 2 S</span>\n<span class=\"ltx_p\" id=\"S4.T2.5.6.1.1.1.2\">Training Setting</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T2.5.6.1.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T2.5.6.1.3\">\n<span class=\"ltx_inline-block\" id=\"S4.T2.5.6.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T2.5.6.1.3.1.1\">Percentage of trainable</span>\n<span class=\"ltx_p\" id=\"S4.T2.5.6.1.3.1.2\">parameters</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T2.5.6.1.4\">\n<span class=\"ltx_inline-block\" id=\"S4.T2.5.6.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T2.5.6.1.4.1.1\">Peak</span>\n<span class=\"ltx_p\" id=\"S4.T2.5.6.1.4.1.2\">HBM</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.5.6.1.5\">\n<span class=\"ltx_inline-block\" id=\"S4.T2.5.6.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T2.5.6.1.5.1.1\">Training</span>\n<span class=\"ltx_p\" id=\"S4.T2.5.6.1.5.1.2\">Speed</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.3\">78.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.5\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.2.2.2\">LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.2.2.3\">77.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.2.2.4\">&lt; 0.01%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.2.2.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.3.3.2\">LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.3.3.1\">79.7%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.3.3.4\">0.02%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.3.3.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.1\">1.7\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.4.4.2\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.4.4.3\">77.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.4.4.4\">0.05%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.4.4.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.4.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.5.5.2\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.5.5.3\">77.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.5.5.4\">0.09%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.5.5.5\">50%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.5.5.1\"></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2: Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM, and training speed (Reddit TL;DR dataset)"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T3.2.1.1\" style=\"font-size:90%;\">Table 3</span>: </span><span class=\"ltx_text\" id=\"S4.T3.3.2\" style=\"font-size:90%;\">Effect of model size on performance of LoRA fine-tuning compared to full-tuning reported with pairwise accuracy (Reddit TL;DR)</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T3.4.1.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T3.4.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T3.4.1.1.1.1.1\">Model Size</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T3.4.1.1.2\">Full Tuning</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.4.1.1.3\">LoRA Rank 4</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.1.1\">PaLM 2 XXS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.1.2\">75.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.1.3\">73.2%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T3.4.3.2.1\">PaLM 2 XS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.4.3.2.2\">78.1%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.3.2.3\">76.8%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T3.4.4.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.3.1.1\">PaLM 2 \u00a0\u00a0S</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.4.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.3.2.1\">78.7%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.4.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.4.3.3.1\">79.7%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 3: Effect of model size on performance of LoRA fine-tuning compared to full-tuning reported with pairwise accuracy (Reddit TL;DR)"
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T4.7.1.1\" style=\"font-size:90%;\">Table 4</span>: </span><span class=\"ltx_text\" id=\"S4.T4.8.2\" style=\"font-size:90%;\">Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM, and training speed (Anthropic-Harmlessness dataset)</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.5.6.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T4.5.6.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T4.5.6.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T4.5.6.1.1.1.1\">PaLM 2 S</span>\n<span class=\"ltx_p\" id=\"S4.T4.5.6.1.1.1.2\">Training Setting</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T4.5.6.1.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T4.5.6.1.3\">\n<span class=\"ltx_inline-block\" id=\"S4.T4.5.6.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T4.5.6.1.3.1.1\">Percentage of trainable</span>\n<span class=\"ltx_p\" id=\"S4.T4.5.6.1.3.1.2\">parameters</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T4.5.6.1.4\">\n<span class=\"ltx_inline-block\" id=\"S4.T4.5.6.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T4.5.6.1.4.1.1\">Peak</span>\n<span class=\"ltx_p\" id=\"S4.T4.5.6.1.4.1.2\">HBM</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.5.6.1.5\">\n<span class=\"ltx_inline-block\" id=\"S4.T4.5.6.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T4.5.6.1.5.1.1\">Training</span>\n<span class=\"ltx_p\" id=\"S4.T4.5.6.1.5.1.2\">Speed</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.2\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.3\">76.56%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.1.5\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.2.2.2\">LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.2.2.3\">75.98%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.2.2.4\">0.005%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.2.2.5\">43.1%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.3.3.2\">LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.3.3.3\">78.71%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.3.3.4\">0.02%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.3.3.5\">43.1%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.3.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.4.4.2\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.4.3.1\">79.1%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.4.4.4\">0.05%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.4.4.5\">43.1%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.4.1\">1.6\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.5.5.2\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.5.5.3\">75.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.5.5.4\">0.09%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.5.5.5\">43.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.5.5.1\"></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 4: Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM, and training speed (Anthropic-Harmlessness dataset)"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T5.2.1.1\" style=\"font-size:90%;\">Table 5</span>: </span><span class=\"ltx_text\" id=\"S4.T5.3.2\" style=\"font-size:90%;\">Effect of model size on performance of Full finetuning and LoRA finetuning (Anthropic-Harmlessness dataset)</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T5.4.1.1.1\">Model Size</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T5.4.1.1.2\">Full Tuning</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.4.1.1.3\">LoRA Rank 8</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.4.2.1.1\">PaLM 2 XXS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.4.2.1.2\">73.44%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.4.2.1.3\">72.5%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.4.3.2.1\">PaLM 2 XS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.4.3.2.2\">76.95%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.4.3.2.3\">76.76%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.4.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T5.4.4.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.4.4.3.1.1\">PaLM 2 \u00a0\u00a0S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T5.4.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.4.4.3.2.1\">76.56%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.4.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.4.4.3.3.1\">79.1%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 5: Effect of model size on performance of Full finetuning and LoRA finetuning (Anthropic-Harmlessness dataset)"
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T6.7.1.1\" style=\"font-size:90%;\">Table 6</span>: </span><span class=\"ltx_text\" id=\"S4.T6.8.2\" style=\"font-size:90%;\">Effect of the LoRA rank on performance, percentage of trainable parameters, peak HBM, and training speed (SHP dataset)</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.5.6.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T6.5.6.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T6.5.6.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T6.5.6.1.1.1.1\">PaLM 2 XS</span>\n<span class=\"ltx_p\" id=\"S4.T6.5.6.1.1.1.2\">\nTraining Setting</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T6.5.6.1.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T6.5.6.1.3\">\n<span class=\"ltx_inline-block\" id=\"S4.T6.5.6.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T6.5.6.1.3.1.1\">Percentage of</span>\n<span class=\"ltx_p\" id=\"S4.T6.5.6.1.3.1.2\">trainable parameters</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T6.5.6.1.4\">\n<span class=\"ltx_inline-block\" id=\"S4.T6.5.6.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T6.5.6.1.4.1.1\">Peak</span>\n<span class=\"ltx_p\" id=\"S4.T6.5.6.1.4.1.2\">HBM</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.5.6.1.5\">\n<span class=\"ltx_inline-block\" id=\"S4.T6.5.6.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T6.5.6.1.5.1.1\">Training</span>\n<span class=\"ltx_p\" id=\"S4.T6.5.6.1.5.1.2\">Speed</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.2\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.3\">82.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.5\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.1.1\">1\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.2.2.2\">LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.2.2.3.1\">82.6%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.2.2.4\">0.005%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.2.2.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.2.2.1\">1.6\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.3.3.2\">LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.3.3.3\">82.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.3.3.4\">0.02%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.3.3.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.3.3.1\">1.6\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.4.4.2\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.4.4.3\">81.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.4.4.4\">0.04%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.4.4.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.4.1\">1.6\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T6.5.5.2\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T6.5.5.3\">81.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T6.5.5.4\">0.08%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T6.5.5.5\">50%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.5.5.1\">1.6\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 6: Effect of the LoRA rank on performance, percentage of trainable parameters, peak HBM, and training speed (SHP dataset)"
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T7\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T7.2.1.1\" style=\"font-size:90%;\">Table 7</span>: </span><span class=\"ltx_text\" id=\"S4.T7.3.2\" style=\"font-size:90%;\">Effect of model size on performance (SHP dataset). We select the best LoRA rank from {1, 4, 8, 16}.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T7.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T7.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T7.4.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T7.4.1.1.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T7.4.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.4.1.1.1.1.1.1\">PaLM 2</span>\n<span class=\"ltx_p\" id=\"S4.T7.4.1.1.1.1.1.2\">Model Size</span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T7.4.1.1.2\">Full Tuning</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T7.4.1.1.3\">LoRA</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T7.4.2.2.1\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T7.4.2.2.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.4.2.2.3\">Rank</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T7.4.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.4.3.1.1\">XXS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.4.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.4.3.1.2.1\">81.6%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.4.3.1.3\">80.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.4.3.1.4\">8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.4.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T7.4.4.2.1\">XS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T7.4.4.2.2\">82.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T7.4.4.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.4.4.2.3.1\">82.6%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T7.4.4.2.4\">1</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 7: Effect of model size on performance (SHP dataset). We select the best LoRA rank from {1, 4, 8, 16}."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T8\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T8.7.1.1\" style=\"font-size:90%;\">Table 8</span>: </span><span class=\"ltx_text\" id=\"S4.T8.8.2\" style=\"font-size:90%;\"> Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM usage, and training speed (UI automation).</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T8.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T8.5.6.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T8.5.6.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T8.5.6.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T8.5.6.1.1.1.1\">PaLM 2 S</span>\n<span class=\"ltx_p\" id=\"S4.T8.5.6.1.1.1.2\">Training Setting</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T8.5.6.1.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T8.5.6.1.3\">\n<span class=\"ltx_inline-block\" id=\"S4.T8.5.6.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T8.5.6.1.3.1.1\">Percentage of trainable</span>\n<span class=\"ltx_p\" id=\"S4.T8.5.6.1.3.1.2\">parameters</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T8.5.6.1.4\">\n<span class=\"ltx_inline-block\" id=\"S4.T8.5.6.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T8.5.6.1.4.1.1\">Peak</span>\n<span class=\"ltx_p\" id=\"S4.T8.5.6.1.4.1.2\">HBM</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.5.6.1.5\">\n<span class=\"ltx_inline-block\" id=\"S4.T8.5.6.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T8.5.6.1.5.1.1\">Training</span>\n<span class=\"ltx_p\" id=\"S4.T8.5.6.1.5.1.2\">Speed</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T8.1.1.2\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T8.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.1.3.1\">93.12%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T8.1.1.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T8.1.1.5\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.1.1\">1\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.2.2.2\">LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.2.2.3\">90.82%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.2.2.4\">&lt;0.01%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.2.2.5\">56%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.2.2.1\">1.9\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.3.3.2\">LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.3.3.3\">91.80%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.3.3.4\">0.02%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.3.3.5\">56%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.3.3.1\">1.9\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.4.4.2\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.4.4.3\">92.00%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.4.4.4\">0.05%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.4.4.5\">56%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.4.4.1\">1.9\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T8.5.5.2\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T8.5.5.3\">92.19%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T8.5.5.4\">0.09%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T8.5.5.5\">56%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.5.5.1\">1.9\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 8:  Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM usage, and training speed (UI automation)."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T9\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T9.2.1.1\" style=\"font-size:90%;\">Table 9</span>: </span><span class=\"ltx_text\" id=\"S4.T9.3.2\" style=\"font-size:90%;\">Effect of model size on accuracy (UI automation). We select the best LoRA rank from {1, 4, 8,16} for each model size.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T9.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T9.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T9.4.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T9.4.1.1.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T9.4.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T9.4.1.1.1.1.1.1\">PaLM 2</span>\n<span class=\"ltx_p\" id=\"S4.T9.4.1.1.1.1.1.2\">Model Size</span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T9.4.1.1.2\">Full Tuning</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T9.4.1.1.3\">LoRA</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T9.4.2.2.1\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T9.4.2.2.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T9.4.2.2.3\">Rank</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T9.4.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T9.4.3.1.1\">XXS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T9.4.3.1.2\">84.42%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T9.4.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.4.3.1.3.1\">87.5%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T9.4.3.1.4\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.4.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T9.4.4.2.1\">XS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T9.4.4.2.2\">88.59%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T9.4.4.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.4.4.2.3.1\">89.65%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.4.4.2.4\">4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.4.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T9.4.5.3.1\">S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T9.4.5.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.4.5.3.2.1\">93.12%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T9.4.5.3.3\">92.19%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T9.4.5.3.4\">16</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 9: Effect of model size on accuracy (UI automation). We select the best LoRA rank from {1, 4, 8,16} for each model size."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T10\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T10.2.1.1\" style=\"font-size:90%;\">Table 10</span>: </span><span class=\"ltx_text\" id=\"S4.T10.3.2\" style=\"font-size:90%;\">Accuracy of a trained RM (PaLM 2 S rank 16), and heuristic matching function as action evaluators (UI automation).</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T10.4\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T10.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T10.4.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T10.4.1.1.1.1\">Task Type</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T10.4.1.1.2\">Low-level Instruction</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T10.4.1.1.3\">High-level Instruction</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.4.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.4.2.2.1\">Heuristic Matching</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T10.4.2.2.2\">RM Evaluator</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.4.2.2.3\">Heuristic Matching</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.4.2.2.4\">RM Evaluator</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.4.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T10.4.3.3.1\">Unseen App</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.4.3.3.2\">76.71%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T10.4.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.4.3.3.3.1\">95.77%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.4.3.3.4\">54.41%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.4.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.4.3.3.5.1\">90.60%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T10.4.4.4.1\">Unseen Task</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.4.4.4.2\">77.71%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T10.4.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.4.4.4.3.1\">95.89%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.4.4.4.4\">56.02%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T10.4.4.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.4.4.4.5.1\">91.61%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 10: Accuracy of a trained RM (PaLM 2 S rank 16), and heuristic matching function as action evaluators (UI automation)."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T11\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T11.7.1.1\" style=\"font-size:90%;\">Table 11</span>: </span><span class=\"ltx_text\" id=\"S4.T11.8.2\" style=\"font-size:90%;\">Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM, and training speed (NPOV dataset)</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T11.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T11.5.6.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T11.5.6.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T11.5.6.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T11.5.6.1.1.1.1\">PaLM 2 S</span>\n<span class=\"ltx_p\" id=\"S4.T11.5.6.1.1.1.2\">Training Setting</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T11.5.6.1.2\">PA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T11.5.6.1.3\">\n<span class=\"ltx_inline-block\" id=\"S4.T11.5.6.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T11.5.6.1.3.1.1\">Percentage of</span>\n<span class=\"ltx_p\" id=\"S4.T11.5.6.1.3.1.2\">trainable parameters</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T11.5.6.1.4\">\n<span class=\"ltx_inline-block\" id=\"S4.T11.5.6.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T11.5.6.1.4.1.1\">Peak</span>\n<span class=\"ltx_p\" id=\"S4.T11.5.6.1.4.1.2\">HBM</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T11.5.6.1.5\">\n<span class=\"ltx_inline-block\" id=\"S4.T11.5.6.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T11.5.6.1.5.1.1\">Training</span>\n<span class=\"ltx_p\" id=\"S4.T11.5.6.1.5.1.2\">Speed</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T11.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T11.1.1.2\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T11.1.1.3\">85.15%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T11.1.1.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T11.1.1.5\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T11.1.1.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.2.2.2\">LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.2.2.3.1\">88.45%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.2.2.4\">&lt;0.01%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.2.2.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.2.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.3.3.2\">LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.3.3.3\">86.80%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.3.3.4\">0.02%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.3.3.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.3.3.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.4.4.2\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.4.4.3\">86.24%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.4.4.4\">0.05%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T11.4.4.5\">50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T11.4.4.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T11.5.5.2\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T11.5.5.3\">86.14%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T11.5.5.4\">0.09%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T11.5.5.5\">50%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T11.5.5.1\"></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 11: Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM, and training speed (NPOV dataset)"
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T12\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T12.2.1.1\" style=\"font-size:90%;\">Table 12</span>: </span><span class=\"ltx_text\" id=\"S4.T12.3.2\" style=\"font-size:90%;\">Effect of model size on performance (NPOV dataset) with HP sweep over LoRA rank and only the best result (by PA) shown. Note that for LoRA, we pick the best LoRA rank from {1, 4, 8, 16}.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T12.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T12.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T12.4.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T12.4.1.1.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T12.4.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T12.4.1.1.1.1.1.1\">PaLM 2</span>\n<span class=\"ltx_p\" id=\"S4.T12.4.1.1.1.1.1.2\">Model Size</span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T12.4.1.1.2\">Full Tuning</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S4.T12.4.1.1.3\">LoRA</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T12.4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T12.4.2.2.1\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T12.4.2.2.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T12.4.2.2.3\">Rank</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T12.4.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T12.4.3.1.1\">XXS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T12.4.3.1.2\">84.49%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T12.4.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T12.4.3.1.3.1\">86.14%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T12.4.3.1.4\">8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T12.4.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T12.4.4.2.1\">XS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T12.4.4.2.2\">87.46%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T12.4.4.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T12.4.4.2.3.1\">88.14%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T12.4.4.2.4\">8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T12.4.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T12.4.5.3.1\">S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T12.4.5.3.2\">85.15%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T12.4.5.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T12.4.5.3.3.1\">88.45%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T12.4.5.3.4\">1</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 12: Effect of model size on performance (NPOV dataset) with HP sweep over LoRA rank and only the best result (by PA) shown. Note that for LoRA, we pick the best LoRA rank from {1, 4, 8, 16}."
        },
        "13": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T13\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T13.7.1.1\" style=\"font-size:90%;\">Table 13</span>: </span><span class=\"ltx_text\" id=\"S4.T13.8.2\" style=\"font-size:90%;\"> Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM usage, and training speed (Taskmaster Coffee Dataset).</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T13.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T13.5.6.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T13.5.6.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T13.5.6.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.6.1.1.1.1\">PaLM 2 S</span>\n<span class=\"ltx_p\" id=\"S4.T13.5.6.1.1.1.2\">Training Setting</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T13.5.6.1.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T13.5.6.1.3\">\n<span class=\"ltx_inline-block\" id=\"S4.T13.5.6.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.6.1.3.1.1\">Percentage of trainable</span>\n<span class=\"ltx_p\" id=\"S4.T13.5.6.1.3.1.2\">parameters</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T13.5.6.1.4\">\n<span class=\"ltx_inline-block\" id=\"S4.T13.5.6.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.6.1.4.1.1\">Peak</span>\n<span class=\"ltx_p\" id=\"S4.T13.5.6.1.4.1.2\">HBM</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T13.5.6.1.5\">\n<span class=\"ltx_inline-block\" id=\"S4.T13.5.6.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.6.1.5.1.1\">Training</span>\n<span class=\"ltx_p\" id=\"S4.T13.5.6.1.5.1.2\">Speed</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T13.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T13.1.1.2\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T13.1.1.3\">96.16%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T13.1.1.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T13.1.1.5\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T13.1.1.1\">1\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.2.2.2\">LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.2.2.3\">93.93%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.2.2.4\">0.005%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.2.2.5\">67%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T13.2.2.1\">1.5\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.3.3.2\">LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.3.3.3\">95.53%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.3.3.4\">0.02%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.3.3.5\">67%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T13.3.3.1\">1.5\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.4.4.2\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.4.4.3.1\">96.81%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.4.4.4\">0.04%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T13.4.4.5\">67%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T13.4.4.1\">1.5\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T13.5.5.2\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T13.5.5.3\">96.16%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T13.5.5.4\">0.08%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T13.5.5.5\">67%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T13.5.5.1\">1.5\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 13:  Effect of the LoRA rank on accuracy, percentage of trainable parameters, peak HBM usage, and training speed (Taskmaster Coffee Dataset)."
        },
        "14": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T14\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T14.2.1.1\" style=\"font-size:90%;\">Table 14</span>: </span><span class=\"ltx_text\" id=\"S4.T14.3.2\" style=\"font-size:90%;\">Effect of model size on performance (Taskmaster Coffee dataset). We select the best LoRA rank from the set {1, 4, 8,16}.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T14.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T14.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T14.4.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T14.4.1.1.1.1\">\n<span class=\"ltx_inline-block\" id=\"S4.T14.4.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.4.1.1.1.1.1.1\">PaLM 2</span>\n<span class=\"ltx_p\" id=\"S4.T14.4.1.1.1.1.1.2\">Model Size</span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T14.4.1.1.2\">Full Tuning</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T14.4.1.1.3\">LoRA</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T14.4.2.2.1\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T14.4.2.2.2\">Accuracy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T14.4.2.2.3\">Rank</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T14.4.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T14.4.3.1.1\">XXS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T14.4.3.1.2\">94.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T14.4.3.1.3\">94.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T14.4.3.1.4\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.4.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T14.4.4.2.1\">XS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T14.4.4.2.2\">96.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T14.4.4.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T14.4.4.2.3.1\">96.8%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T14.4.4.2.4\">8</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 14: Effect of model size on performance (Taskmaster Coffee dataset). We select the best LoRA rank from the set {1, 4, 8,16}."
        },
        "15": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T15\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T15.7.1.1\" style=\"font-size:90%;\">Table 15</span>: </span><span class=\"ltx_text\" id=\"S5.T15.8.2\" style=\"font-size:90%;\">Effect of LoRA Rank on performance, percentage of trainable parameters, peak HBM usage, and training speed for RL (Reddit TL;DR dataset).</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T15.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T15.5.6.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T15.5.6.1.1\">\n<span class=\"ltx_inline-block\" id=\"S5.T15.5.6.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T15.5.6.1.1.1.1\">Policy</span>\n<span class=\"ltx_p\" id=\"S5.T15.5.6.1.1.1.2\">Model Setting</span>\n<span class=\"ltx_p\" id=\"S5.T15.5.6.1.1.1.3\">for PaLM 2 S</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T15.5.6.1.2\">Max Reward</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T15.5.6.1.3\">\n<span class=\"ltx_inline-block\" id=\"S5.T15.5.6.1.3.1\">\n<span class=\"ltx_p\" id=\"S5.T15.5.6.1.3.1.1\">Percentage of trainable</span>\n<span class=\"ltx_p\" id=\"S5.T15.5.6.1.3.1.2\">parameters</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T15.5.6.1.4\">\n<span class=\"ltx_inline-block\" id=\"S5.T15.5.6.1.4.1\">\n<span class=\"ltx_p\" id=\"S5.T15.5.6.1.4.1.1\">Peak</span>\n<span class=\"ltx_p\" id=\"S5.T15.5.6.1.4.1.2\">HBM</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T15.5.6.1.5\">\n<span class=\"ltx_inline-block\" id=\"S5.T15.5.6.1.5.1\">\n<span class=\"ltx_p\" id=\"S5.T15.5.6.1.5.1.1\">Training Speed</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T15.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T15.1.1.2\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T15.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.1.1.3.1\">2.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T15.1.1.4\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T15.1.1.5\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T15.1.1.1\">1\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.2\">LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.3\">2.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.4\">0.005%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.2.2.5\">76%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.2.2.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.3.3.2\">LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.3.3.3\">2.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.3.3.4\">0.02%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.3.3.5\">76%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.3.3.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.4.4.2\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.4.4.3\">2.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.4.4.4\">0.05%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T15.4.4.5\">76%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T15.4.4.1\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T15.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T15.5.5.2\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T15.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T15.5.5.3.1\">2.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T15.5.5.4\">0.09%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T15.5.5.5\">77%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T15.5.5.1\">1.13\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 15: Effect of LoRA Rank on performance, percentage of trainable parameters, peak HBM usage, and training speed for RL (Reddit TL;DR dataset)."
        },
        "16": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T16\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T16.2.1.1\" style=\"font-size:90%;\">Table 16</span>: </span><span class=\"ltx_text\" id=\"S5.T16.3.2\" style=\"font-size:90%;\">Effect of model size on performance of LoRA RL tuning and full RL tuning (Reddit TL;DR). We report the maximum cumulative reward achieved.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T16.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T16.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T16.4.1.1.1\">Model Size</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T16.4.1.1.2\">Full RL Tuning</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T16.4.1.1.3\">LoRA Rank 16 RL</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T16.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T16.4.2.1.1\">PaLM 2 XXS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T16.4.2.1.2\">0.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T16.4.2.1.3\">0.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.4.3.2.1\">PaLM 2 XS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T16.4.3.2.2\">2.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T16.4.3.2.3\">2.08</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T16.4.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T16.4.4.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T16.4.4.3.1.1\">PaLM 2 \u00a0\u00a0S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T16.4.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T16.4.4.3.2.1\">2.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T16.4.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T16.4.4.3.3.1\">2.18</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 16: Effect of model size on performance of LoRA RL tuning and full RL tuning (Reddit TL;DR). We report the maximum cumulative reward achieved."
        },
        "17": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T17\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T17.2.1.1\" style=\"font-size:90%;\">Table 17</span>: </span><span class=\"ltx_text\" id=\"S5.T17.3.2\" style=\"font-size:90%;\">Effect of LoRA Rank on performance, percentage of trainable parameters, and peak HBM needed for RL (BOLT message summarization dataset).</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T17.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T17.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T17.4.1.1.1\">\n<span class=\"ltx_inline-block\" id=\"S5.T17.4.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T17.4.1.1.1.1.1\">Policy</span>\n<span class=\"ltx_p\" id=\"S5.T17.4.1.1.1.1.2\">Model Setting</span>\n<span class=\"ltx_p\" id=\"S5.T17.4.1.1.1.1.3\">For PaLM 2 XXS</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T17.4.1.1.2\">Summary Quality</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T17.4.1.1.3\">NLI</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T17.4.1.1.4\">\n<span class=\"ltx_inline-block\" id=\"S5.T17.4.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S5.T17.4.1.1.4.1.1\">Percentage of trainable</span>\n<span class=\"ltx_p\" id=\"S5.T17.4.1.1.4.1.2\">parameters</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T17.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T17.4.2.1.1\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T17.4.2.1.2\">0.372</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T17.4.2.1.3\">0.876</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T17.4.2.1.4\">100%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.3.2.1\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.3.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.4.3.2.2.1\">0.411</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.3.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T17.4.3.2.3.1\">0.921</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.4.3.2.4\">0.05%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.4.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.4.3.1\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.4.3.2\">0.390</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.4.3.3\">0.904</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.4.4.3.4\">0.09%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.4.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.5.4.1\">LoRA Rank 32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.5.4.2\">0.384</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T17.4.5.4.3\">0.898</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T17.4.5.4.4\">0.19%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T17.4.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T17.4.6.5.1\">LoRA Rank 64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T17.4.6.5.2\">0.398</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T17.4.6.5.3\">0.913</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T17.4.6.5.4\">0.37%</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 17: Effect of LoRA Rank on performance, percentage of trainable parameters, and peak HBM needed for RL (BOLT message summarization dataset)."
        },
        "18": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T18\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T18.2.1.1\" style=\"font-size:90%;\">Table 18</span>: </span><span class=\"ltx_text\" id=\"A1.T18.3.2\" style=\"font-size:90%;\">Stanford Human Preferences Dataset</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T18.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T18.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T18.4.1.1.1\" style=\"padding-bottom:2.15277pt;\">Domain</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T18.4.1.1.2\" style=\"padding-bottom:2.15277pt;\">Train</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T18.4.1.1.3\" style=\"padding-bottom:2.15277pt;\">Validation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T18.4.1.1.4\" style=\"padding-bottom:2.15277pt;\">Test</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T18.4.1.1.5\" style=\"padding-bottom:2.15277pt;\">Total</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T18.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T18.4.2.1.1\">askacademia</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T18.4.2.1.2\">31,450</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T18.4.2.1.3\">2,095</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T18.4.2.1.4\">1,708</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T18.4.2.1.5\">35,253</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.3.2.1\">askanthropology</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.3.2.2\">3,910</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.3.2.3\">203</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.3.2.4\">268</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.3.2.5\">4,381</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.4.3.1\">askbaking</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.4.3.2\">44,007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.4.3.3\">2,096</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.4.3.4\">1,544</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.4.3.5\">47,647</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.5.4.1\">askcarguys</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.5.4.2\">3,227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.5.4.3\">159</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.5.4.4\">117</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.5.4.5\">3,503</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.6.5.1\">askculinary</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.6.5.2\">45,710</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.6.5.3\">2,094</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.6.5.4\">2,563</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.6.5.5\">50,367</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.7.6.1\">askdocs</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.7.6.2\">6,449</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.7.6.3\">315</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.7.6.4\">455</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.7.6.5\">7,219</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.8.7.1\">askengineers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.8.7.2\">57,096</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.8.7.3\">3,154</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.8.7.4\">2,638</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.8.7.5\">62,888</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.9.8.1\">askhistorians</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.9.8.2\">3,264</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.9.8.3\">113</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.9.8.4\">164</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.9.8.5\">3,541</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.10.9.1\">askhr</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.10.9.2\">8,295</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.10.9.3\">641</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.10.9.4\">395</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.10.9.5\">9,331</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.11.10.1\">askphilosophy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.11.10.2\">10,307</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.11.10.3\">608</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.11.10.4\">677</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.11.10.5\">11,592</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.12.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.12.11.1\">askphysics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.12.11.2\">7,364</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.12.11.3\">409</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.12.11.4\">587</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.12.11.5\">8,360</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.13.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.13.12.1\">askscience</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.13.12.2\">13,316</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.13.12.3\">899</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.13.12.4\">977</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.13.12.5\">15,192</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.14.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.14.13.1\">asksciencefiction</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.14.13.2\">29,382</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.14.13.3\">1576</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.14.13.4\">1,987</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.14.13.5\">32,945</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.15.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.15.14.1\">asksocialscience</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.15.14.2\">2,706</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.15.14.3\">147</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.15.14.4\">188</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.15.14.5\">3,041</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.16.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.16.15.1\">askvet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.16.15.2\">3,300</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.16.15.3\">170</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.16.15.4\">224</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.16.15.5\">3,694</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.17.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.17.16.1\">changemyview</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.17.16.2\">38,173</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.17.16.3\">1,637</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.17.16.4\">1,836</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.17.16.5\">41,646</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.18.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.18.17.1\">explainlikeimfive</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.18.17.2\">19,592</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.18.17.3\">1,014</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.18.17.4\">1,070</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.18.17.5\">21,676</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.19.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.19.18.1\">legaladvice</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.19.18.2\">21,170</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.19.18.3\">1,106</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T18.4.19.18.4\">1,011</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T18.4.19.18.5\">23,287</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T18.4.20.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"A1.T18.4.20.19.1\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"A1.T18.4.20.19.2\">348,718</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"A1.T18.4.20.19.3\">18,436</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"A1.T18.4.20.19.4\">18,409</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A1.T18.4.20.19.5\">385,563</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 18: Stanford Human Preferences Dataset"
        },
        "19": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T19\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T19.2.1.1\" style=\"font-size:90%;\">Table 19</span>: </span><span class=\"ltx_text\" id=\"A1.T19.3.2\" style=\"font-size:90%;\">Optimal hyperparameters identified for RM training (SHP dataset)</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T19.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T19.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T19.4.1.1.1\">PaLM 2 XS Setting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T19.4.1.1.2\">Dropout Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T19.4.1.1.3\">Learning Rate</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T19.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T19.4.2.1.1\">Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T19.4.2.1.2\">5e-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T19.4.2.1.3\">2e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T19.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.3.2.1\">LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.3.2.2\">2e-2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T19.4.3.2.3\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T19.4.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.4.3.1\">LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.4.3.2\">2e-2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T19.4.4.3.3\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T19.4.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.5.4.1\">LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.5.4.2\">1e-2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T19.4.5.4.3\">2e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T19.4.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.6.5.1\">LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.6.5.2\">1e-2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T19.4.6.5.3\">2e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T19.4.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.7.6.1\">LoRA Rank 32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T19.4.7.6.2\">1e-2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T19.4.7.6.3\">2e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T19.4.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T19.4.8.7.1\">LoRA Rank 64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T19.4.8.7.2\">2e-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T19.4.8.7.3\">2e-5</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 19: Optimal hyperparameters identified for RM training (SHP dataset)"
        },
        "20": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T20\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T20.2.1.1\" style=\"font-size:90%;\">Table 20</span>: </span><span class=\"ltx_text\" id=\"A1.T20.3.2\" style=\"font-size:90%;\">Optimal hyperparameters for different reward model settings (UI automation).</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T20.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T20.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T20.4.1.1.1\">Setting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T20.4.1.1.2\">Dropout Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T20.4.1.1.3\">Learning Rate</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T20.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T20.4.2.1.1\">PaLM 2 S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T20.4.2.1.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T20.4.2.1.3\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T20.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.3.2.1\">PaLM 2 S LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.3.2.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T20.4.3.2.3\">2e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T20.4.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.4.3.1\">PaLM 2 S LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.4.3.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T20.4.4.3.3\">1e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T20.4.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.5.4.1\">PaLM 2 S LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.5.4.2\">0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T20.4.5.4.3\">2e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T20.4.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.6.5.1\">PaLM 2 S LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.6.5.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T20.4.6.5.3\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T20.4.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.7.6.1\">PaLM2 XXS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.7.6.2\">0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T20.4.7.6.3\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T20.4.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.8.7.1\">PaLM 2 XXS LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.8.7.2\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T20.4.8.7.3\">1e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T20.4.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.9.8.1\">PaLM2 XS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T20.4.9.8.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T20.4.9.8.3\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T20.4.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T20.4.10.9.1\">PaLM 2 XS LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T20.4.10.9.2\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T20.4.10.9.3\">1e-4</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 20: Optimal hyperparameters for different reward model settings (UI automation)."
        },
        "21": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T21\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T21.2.1.1\" style=\"font-size:90%;\">Table 21</span>: </span><span class=\"ltx_text\" id=\"A1.T21.3.2\" style=\"font-size:90%;\">Optimal hyperparameters identified for RM training (NPOV dataset)</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T21.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T21.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T21.4.1.1.1\">Setting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T21.4.1.1.2\">Dropout Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T21.4.1.1.3\">Learning Rate</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T21.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T21.4.2.1.1\">PaLM 2 XXS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T21.4.2.1.2\">5e-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T21.4.2.1.3\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T21.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.3.2.1\">PaLM 2 XXS LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.3.2.2\">1e-4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T21.4.3.2.3\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T21.4.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.4.3.1\">PaLM 2 XS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.4.3.2\">5e-2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T21.4.4.3.3\">1e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T21.4.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.5.4.1\">PaLM 2 XS LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.5.4.2\">5e-4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T21.4.5.4.3\">5e-2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T21.4.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.6.5.1\">PaLM 2 S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.6.5.2\">5e-2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T21.4.6.5.3\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T21.4.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.7.6.1\">PaLM 2 S LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.7.6.2\">5e-2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T21.4.7.6.3\">5e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T21.4.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.8.7.1\">PaLM 2 S LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.8.7.2\">1e-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T21.4.8.7.3\">5e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T21.4.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.9.8.1\">PaLM 2 S LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T21.4.9.8.2\">1e-1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T21.4.9.8.3\">5e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T21.4.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T21.4.10.9.1\">PaLM 2 S LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T21.4.10.9.2\">5e-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T21.4.10.9.3\">5e-4</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 21: Optimal hyperparameters identified for RM training (NPOV dataset)"
        },
        "22": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T22\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T22.2.1.1\" style=\"font-size:90%;\">Table 22</span>: </span><span class=\"ltx_text\" id=\"A1.T22.3.2\" style=\"font-size:90%;\"> Hyperparameter sweep and results for different experiments (Taskmaster Coffee).</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T22.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T22.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T22.4.1.1.1\">PaLM 2 Setting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"A1.T22.4.1.1.2\">Dropout Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T22.4.1.1.3\">Learning Rate</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T22.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T22.4.2.1.1\">XXS, Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T22.4.2.1.2\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T22.4.2.1.3\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T22.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.3.2.1\">XXS, LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.3.2.2\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T22.4.3.2.3\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T22.4.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.4.3.1\">XS, Full Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.4.3.2\">0.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T22.4.4.3.3\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T22.4.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.5.4.1\">XS, LoRA Rank 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.5.4.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T22.4.5.4.3\">2e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T22.4.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.6.5.1\">XS, LoRA Rank 4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.6.5.2\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T22.4.6.5.3\">2e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T22.4.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.7.6.1\">XS, LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.7.6.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T22.4.7.6.3\">2e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T22.4.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.8.7.1\">XS, LoRA Rank 16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T22.4.8.7.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T22.4.8.7.3\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T22.4.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T22.4.9.8.1\">XS, LoRA Rank 8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A1.T22.4.9.8.2\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T22.4.9.8.3\">2e-4</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 22:  Hyperparameter sweep and results for different experiments (Taskmaster Coffee)."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.10704v1_figure_1.png",
            "caption": "Figure 1: PERL reward model fitting trains a small percentage of the number of parameters the conventional approach does."
        },
        "2": {
            "figure_path": "2403.10704v1_figure_2.png",
            "caption": "Figure 2: PERL vs. Conventional Reinforcement Learning Loops"
        },
        "3": {
            "figure_path": "2403.10704v1_figure_3.png",
            "caption": "Figure 3: PERL pairwise accuracy for different LoRA ranks (Reddit TL;DR)"
        },
        "4": {
            "figure_path": "2403.10704v1_figure_4.png",
            "caption": "Figure 4: PERL\u2019s maximum cumulative reward increases with the LoRA rank (Reddit TL;DR). At rank 16, the policy performance comes close to that of the full-tuning."
        },
        "5": {
            "figure_path": "2403.10704v1_figure_5.png",
            "caption": "(a)"
        },
        "6": {
            "figure_path": "2403.10704v1_figure_6.png",
            "caption": "(b)"
        },
        "7": {
            "figure_path": "2403.10704v1_figure_7.png",
            "caption": "(c)"
        },
        "8": {
            "figure_path": "2403.10704v1_figure_8.png",
            "caption": "(d)"
        },
        "9": {
            "figure_path": "2403.10704v1_figure_9.png",
            "caption": "Figure 6: Ticketing dataset example conversation"
        },
        "10": {
            "figure_path": "2403.10704v1_figure_10.png",
            "caption": "Figure 7: General instructions creating dialogues."
        },
        "11": {
            "figure_path": "2403.10704v1_figure_11.png",
            "caption": "Figure 8: Choosing and storing entities for use in conversations"
        },
        "12": {
            "figure_path": "2403.10704v1_figure_12.png",
            "caption": "Figure 9: Rating candidates from a fine-tuned LaMDA model"
        }
    },
    "references": [
        {
            "1": {
                "title": "Concrete problems in ai safety.",
                "author": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and\nDan Man\u00e9.",
                "venue": "arXiv preprint arXiv:1606.06565, 2016.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Palm 2 technical report, 2023.",
                "author": "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,\nAlexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\nEric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy\nMeier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,\nSebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,\nJames Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng,\nColin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment\nCrepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,\nSteven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui,\nJeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao\nJia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine\nLee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek\nLim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma\nMahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John\nNham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker\nRiley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon\nTokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan\nXu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,\nCe Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.",
                "venue": null,
                "url": null
            }
        },
        {
            "3": {
                "title": "The Anthropic-HH Dataset, 2022.",
                "author": "Anthropic.",
                "venue": "https://huggingface.co/datasets/Anthropic/hh-rlhf [Accessed:\n2024-01-03].",
                "url": null
            }
        },
        {
            "4": {
                "title": "A general theoretical paradigm to understand learning from human\npreferences, 2023.",
                "author": "Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele\nCalandriello, Michal Valko, and R\u00e9mi Munos.",
                "venue": null,
                "url": null
            }
        },
        {
            "5": {
                "title": "Training a helpful and harmless assistant with reinforcement learning\nfrom human feedback, 2022a.",
                "author": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,\nSaurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,\nZac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna\nKravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,\nJack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.",
                "venue": null,
                "url": null
            }
        },
        {
            "6": {
                "title": "Constitutional ai: Harmlessness from ai feedback.",
                "author": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,\nAndy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,\net al.",
                "venue": "arXiv preprint arXiv:2212.08073, 2022b.",
                "url": null
            }
        },
        {
            "7": {
                "title": "On the opportunities and risks of foundation models, 2022.",
                "author": "Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\nvon Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,\nNiladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John\nEtchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori\nHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,\nSiddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei\nKoh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal\nLadhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li,\nXuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak\nNarayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJulian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park,\nChris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich,\nHongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher\nR\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan\nSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r,\nRose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie,\nMichihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang,\nXikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.",
                "venue": null,
                "url": null
            }
        },
        {
            "8": {
                "title": "Audiolm: a language modeling approach to audio generation, 2023.",
                "author": "Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier\nPietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco\nTagliasacchi, and Neil Zeghidour.",
                "venue": null,
                "url": null
            }
        },
        {
            "9": {
                "title": "Jax: composable transformations of python+ numpy programs, 2018.",
                "author": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary,\nDougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye\nWanderman-Milne, et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "10": {
                "title": "Rank analysis of incomplete block designs: I. the method of paired\ncomparisons.",
                "author": "Ralph Allan Bradley and Milton E. Terry.",
                "venue": "Biometrika, 39:324, 1952.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al.",
                "venue": "Advances in neural information processing systems,\n33:1877\u20131901, 2020.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Tickettalk: Toward human-level performance with end-to-end,\ntransaction-based dialog systems.",
                "author": "Bill Byrne, Karthik Krishnamoorthi, Saravanan Ganesh, and Mihir Kale.",
                "venue": "In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\nProceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 671\u2013680, Online,\naug 2021. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Open problems and fundamental limitations of reinforcement learning\nfrom human feedback, 2023.",
                "author": "Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my\nScheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro\nFreire, Tony Wang, Samuel Marks, Charbel-Rapha\u00ebl Segerie, Micah Carroll,\nAndi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar,\nAnand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii\nKrasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem B\u0131y\u0131k, Anca\nDragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell.",
                "venue": null,
                "url": null
            }
        },
        {
            "14": {
                "title": "Bolt english sms/chat ldc2018t19.",
                "author": "Song Chen et al.",
                "venue": "Web Download, 2018.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Pali-3 vision language models: Smaller, faster, stronger, 2023.",
                "author": "Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul\nVoigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr\nPadlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong,\nTianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut.",
                "venue": null,
                "url": null
            }
        },
        {
            "16": {
                "title": "Palm: Scaling language modeling with pathways, 2022.",
                "author": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\nLevskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David\nDohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai,\nThanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi\nWang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.",
                "venue": null,
                "url": null
            }
        },
        {
            "17": {
                "title": "Deep reinforcement learning from human preferences.",
                "author": "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario\nAmodei.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Seahorse: A multilingual, multifaceted dataset for summarization\nevaluation, 2023.",
                "author": "Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee\nAharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, and\nAnkur P. Parikh.",
                "venue": null,
                "url": null
            }
        },
        {
            "19": {
                "title": "Qlora: Efficient finetuning of quantized llms, 2023.",
                "author": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.",
                "venue": null,
                "url": null
            }
        },
        {
            "20": {
                "title": "Learning what to value.",
                "author": "Daniel Dewey.",
                "venue": "In International conference on artificial general\nintelligence, pages 309\u2013314. Springer, 2011.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Delta tuning: A comprehensive study of parameter efficient methods\nfor pre-trained language models, 2022.",
                "author": "Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,\nShengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang,\nJuanzi Li, and Maosong Sun.",
                "venue": null,
                "url": null
            }
        },
        {
            "22": {
                "title": "Raft: Reward ranked finetuning for generative foundation model\nalignment, 2023.",
                "author": "Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan,\nShizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang.",
                "venue": null,
                "url": null
            }
        },
        {
            "23": {
                "title": "Palm-e: An embodied multimodal language model, 2023.",
                "author": "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery,\nBrian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,\nVincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng,\nIgor Mordatch, and Pete Florence.",
                "venue": null,
                "url": null
            }
        },
        {
            "24": {
                "title": "Understanding dataset difficulty with -usable\ninformation.",
                "author": "Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta.",
                "venue": "In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,\nGang Niu, and Sivan Sabato, editors, Proceedings of the 39th\nInternational Conference on Machine Learning, volume 162 of\nProceedings of Machine Learning Research, pages 5988\u20136008. PMLR,\n17\u201323 Jul 2022.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Avoiding wireheading with value reinforcement learning.",
                "author": "Tom Everitt and Marcus Hutter.",
                "venue": "In Artificial General Intelligence: 9th International\nConference, AGI 2016, New York, NY, USA, July 16-19, 2016, Proceedings 9,\npages 12\u201322. Springer, 2016.",
                "url": null
            }
        },
        {
            "26": {
                "title": "The alignment problem for bayesian history-based reinforcement\nlearners.",
                "author": "Tom Everitt and Marcus Hutter.",
                "venue": "Under submission, 2018.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Eli5: Long form question answering, 2019.",
                "author": "Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and\nMichael Auli.",
                "venue": null,
                "url": null
            }
        },
        {
            "28": {
                "title": "Taming the noise in reinforcement learning via soft updates.",
                "author": "Roy Fox, Ari Pakman, and Naftali Tishby.",
                "venue": "arXiv preprint arXiv:1512.08562, 2015.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Leveraging large language models in conversational recommender\nsystems.",
                "author": "Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed, Changbo\nLong, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et al.",
                "venue": "arXiv preprint arXiv:2305.07961, 2023.",
                "url": null
            }
        },
        {
            "30": {
                "title": "A theory of regularized markov decision processes.",
                "author": "Matthieu Geist, Bruno Scherrer, and Olivier Pietquin.",
                "venue": "In International Conference on Machine Learning, pages\n2160\u20132169. PMLR, 2019.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Chatgpt outperforms crowd workers for text-annotation tasks.",
                "author": "Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli.",
                "venue": "Proceedings of the National Academy of Sciences, 120(30), July 2023.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Cooperative inverse reinforcement learning, 2016.",
                "author": "Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell.",
                "venue": null,
                "url": null
            }
        },
        {
            "33": {
                "title": "Training compute-optimal large language models, 2022.",
                "author": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.",
                "venue": null,
                "url": null
            }
        },
        {
            "34": {
                "title": "Parameter-efficient transfer learning for nlp, 2019.",
                "author": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\nde Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.",
                "venue": null,
                "url": null
            }
        },
        {
            "35": {
                "title": "Lora: Low-rank adaptation of large language models, 2021.",
                "author": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen.",
                "venue": null,
                "url": null
            }
        },
        {
            "36": {
                "title": "Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU,\n2023a.",
                "author": "HuggingFace.",
                "venue": "https://huggingface.co/blog/trl-peft [Accessed: 2024-01-03].",
                "url": null
            }
        },
        {
            "37": {
                "title": "StackLLaMA: A hands-on guide to train LLaMA with RLHF,\n2023b.",
                "author": "HuggingFace.",
                "venue": "https://huggingface.co/blog/stackllama [Accessed: 2024-01-03].",
                "url": null
            }
        },
        {
            "38": {
                "title": "Multi-Adapter RL (MARL), 2024.",
                "author": "HuggingFace.",
                "venue": "https://huggingface.co/docs/trl/en/multi_adapter_rl [Accessed:\n2024-02-27].",
                "url": null
            }
        },
        {
            "39": {
                "title": "Faithful persona-based conversational dataset generation with large\nlanguage models, 2023.",
                "author": "Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, and Hakim Sidahmed.",
                "venue": null,
                "url": null
            }
        },
        {
            "40": {
                "title": "Ai alignment: A comprehensive survey.",
                "author": "Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang,\nYawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al.",
                "venue": "arXiv preprint arXiv:2310.19852, 2023.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Mistral 7b, 2023.",
                "author": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed.",
                "venue": null,
                "url": null
            }
        },
        {
            "42": {
                "title": "Aligning large language models through synthetic feedback.",
                "author": "Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min\nYoo, and Minjoon Seo.",
                "venue": "arXiv preprint arXiv:2305.13735, 2023.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Adam: A method for stochastic optimization, 2017.",
                "author": "Diederik P. Kingma and Jimmy Ba.",
                "venue": null,
                "url": null
            }
        },
        {
            "44": {
                "title": "Okapi: Instruction-tuned large language models in multiple languages\nwith reinforcement learning from human feedback, 2023.",
                "author": "Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck\nDernoncourt, Ryan A. Rossi, and Thien Huu Nguyen.",
                "venue": null,
                "url": null
            }
        },
        {
            "45": {
                "title": "Rlaif: Scaling reinforcement learning from human feedback with ai\nfeedback.",
                "author": "Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton\nBishop, Victor Carbune, and Abhinav Rastogi.",
                "venue": "arXiv preprint arXiv:2309.00267, 2023a.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Aligning text-to-image models using human feedback.",
                "author": "Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier,\nPieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu.",
                "venue": "arXiv preprint arXiv:2302.12192, 2023b.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Scalable agent alignment via reward modeling: a research direction.",
                "author": "Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane\nLegg.",
                "venue": "arXiv preprint arXiv:1811.07871, 2018.",
                "url": null
            }
        },
        {
            "48": {
                "title": "The power of scale for parameter-efficient prompt tuning, 2021.",
                "author": "Brian Lester, Rami Al-Rfou, and Noah Constant.",
                "venue": null,
                "url": null
            }
        },
        {
            "49": {
                "title": "Learning UI navigation through demonstrations composed of macro\nactions, October 2021.",
                "author": "Wei Li.",
                "venue": null,
                "url": null
            }
        },
        {
            "50": {
                "title": "Uinav: A maker of ui automation agents, 2023.",
                "author": "Wei Li, Fu-Lin Hsu, Will Bishop, Folawiyo Campbell-Ajala, Oriana Riva, and Max\nLin.",
                "venue": null,
                "url": null
            }
        },
        {
            "51": {
                "title": "Teaching language models to support answers with verified quotes,\n2022.",
                "author": "Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song,\nMartin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,\nGeoffrey Irving, and Nat McAleese.",
                "venue": null,
                "url": null
            }
        },
        {
            "52": {
                "title": "Webgpt: Browser-assisted question-answering with human feedback,\n2022.",
                "author": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew\nKnight, Benjamin Chess, and John Schulman.",
                "venue": null,
                "url": null
            }
        },
        {
            "53": {
                "title": "Learning to Summarize from Human Feedback Dataset, 2022.",
                "author": "OpenAI.",
                "venue": "https://huggingface.co/datasets/openai/summarize_from_feedback\n[Accessed: 2024-01-03].",
                "url": null
            }
        },
        {
            "54": {
                "title": "Gpt-4 technical report, 2023.",
                "author": "OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge\nAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam\nAltman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie\nBalcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,\nJake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,\nOleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks,\nMiles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann,\nBrittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang,\nFotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben\nChess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah\nCurrier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien\nEcoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges,\nChristian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,\nJonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen\nHe, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost\nHuizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang,\nHaozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan,\n\u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim,\nHendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz\nKondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen\nKrueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade\nLeung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin,\nMateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie\nMayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey,\nPaul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke\nMetz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00e9ly, Ashvin Nair, Reiichiro\nNakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long\nOuyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila\nBelbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael,\nPokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris\nPower, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri\nRoussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish\nSastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla\nSheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon\nSidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl,\nBenjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such,\nNatalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley,\nJerry Tworek, Juan Felipe Cer\u00f3n Uribe, Andrea Vallone, Arun Vijayvergiya,\nChelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang,\nJonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi\nWeng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel\nWolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai\nXiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan\nZellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang\nZhuang, William Zhuk, and Barret Zoph.",
                "venue": null,
                "url": null
            }
        },
        {
            "55": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.",
                "venue": "Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Paxml: a Jax-based machine learning framework for training\nlarge scale models, 2022.",
                "author": "Paxml.",
                "venue": "https://github.com/google/paxml [Accessed: 2024-01-03].",
                "url": null
            }
        },
        {
            "57": {
                "title": "Improving language understanding by generative pre-training, 2018.",
                "author": "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.",
                "venue": null,
                "url": null
            }
        },
        {
            "58": {
                "title": "Direct preference optimization: Your language model is secretly a\nreward model.",
                "author": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D\nManning, and Chelsea Finn.",
                "venue": "arXiv preprint arXiv:2305.18290, 2023.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Unsupervised pretraining for sequence to sequence learning.",
                "author": "Prajit Ramachandran, Peter J Liu, and Quoc V Le.",
                "venue": "arXiv preprint arXiv:1611.02683, 2016.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Warm: On the benefits of weight averaged reward models, 2024.",
                "author": "Alexandre Ram\u00e9, Nino Vieillard, L\u00e9onard Hussenot, Robert Dadashi, Geoffrey\nCideron, Olivier Bachem, and Johan Ferret.",
                "venue": null,
                "url": null
            }
        },
        {
            "61": {
                "title": "Gemini 1.5: Unlocking multimodal understanding across millions of\ntokens of context.",
                "author": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy\nLillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan\nFirat, Julian Schrittwieser, et al.",
                "venue": "arXiv preprint arXiv:2403.05530, 2024.",
                "url": null
            }
        },
        {
            "62": {
                "title": "Scaling up models and data with t5x and seqio,\n2022.",
                "author": "Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury,\nDaniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin,\nCurtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin,\nSebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko,\nAakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo\nNi, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan\nGarrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten\nBosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick,\nBrennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea\nGesmundo.",
                "venue": null,
                "url": null
            }
        },
        {
            "63": {
                "title": "Efficient rlhf: Reducing the memory usage of ppo, 2023.",
                "author": "Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen.",
                "venue": null,
                "url": null
            }
        },
        {
            "64": {
                "title": "Proximal policy optimization algorithms, 2017.",
                "author": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.",
                "venue": null,
                "url": null
            }
        },
        {
            "65": {
                "title": "Adafactor: Adaptive learning rates with sublinear memory cost, 2018.",
                "author": "Noam Shazeer and Mitchell Stern.",
                "venue": null,
                "url": null
            }
        },
        {
            "66": {
                "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, a\nlarge-scale generative language model.",
                "author": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam\nRajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,\nVijay Korthikanti, et al.",
                "venue": "arXiv preprint arXiv:2201.11990, 2022.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Stanford Human Preferences Dataset, 2022.",
                "author": "Stanford.",
                "venue": "https://huggingface.co/datasets/stanfordnlp/SHP [Accessed:\n2024-01-03].",
                "url": null
            }
        },
        {
            "68": {
                "title": "Learning to summarize with human feedback.",
                "author": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul F Christiano.",
                "venue": "Advances in Neural Information Processing Systems,\n33:3008\u20133021, 2020.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Aligning large multimodal models with factually augmented rlhf.",
                "author": "Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen,\nChuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al.",
                "venue": "arXiv preprint arXiv:2309.14525, 2023.",
                "url": null
            }
        },
        {
            "70": {
                "title": "Gemini: A family of highly capable multimodal models, 2023.",
                "author": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,\nJiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie\nMillican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou,\nJulian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy\nLillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard,\nPaul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,\nYuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford,\nErica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras,\nMaxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs,\nAna\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran\nKazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette,\nCharlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes,\nNathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone,\nTomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam\nBloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred\nAlcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel\nBarth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun\nAhuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu,\nQiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh\nTomar, Xavier Garcia, Evan Senter, Emanuel Taropa,\nThanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego\nde Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri\u00e0 Puigdom\u00e8nech\nBadia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin,\nShariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler,\nEmilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki,\nAntoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown,\nElliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe\nAshwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay\nKannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur\nBapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy\nWang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys,\nThibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan\nDamoc, Alex Kaskasoli, S\u00e9bastien M. R. Arnold, Vijay Vasudevan, Shubham\nAgrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan,\nHyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand,\nAnkush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz,\nZaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery,\nDipanjan Das, Dominika Rogozi\u0144ska, Vitaly Nikolaev, Pablo Sprechmann,\nZachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro,\nGaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis,\nClara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti\nRijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin,\nAdnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan,\nReinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur\nGuez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin\nVillela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim\u00e9nez, Legg Yeung,\nHanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem\nHaykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu\nSharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian\nHutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao\nWang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Lu\u010di\u0107,\nGuodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong\nCheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina\nButterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur\nMensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren\nMaggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong,\nTaylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya,\nSebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo,\nAmbrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya,\nJonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi,\nSophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu,\nRichard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James\nBesley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy\nGreer, Guolong Su, Martin Polacek, Rapha\u00ebl Lopez Kaufman, Simon Tokumine,\nHexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya\nSiddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen\nAshraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory\nBlevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar\nChang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna\nHaridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel,\nMingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu,\nJaime Alonso Lorenzo, Lars Lowe Sj\u00f6sund, S\u00e9bastien Cevey, Zach Gleicher,\nThi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May,\nKonstantinos Aisopos, L\u00e9onard Hussenot, Livio Baldini Soares, Kate Baumli,\nMichael B. Chang, Adri\u00e0 Recasens, Ben Caine, Alexander Pritzel, Filip\nPavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan,\nKartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, V\u00edctor Campos,\nAlex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran\nLang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross\nHemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng,\nPhoebe Thacker, \u00c7a\u011flar \u00dcnl\u00fc, Zhishuai Zhang, Mohammad Saleh, James\nSvensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina\nTsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom\nKwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald,\nKeren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir\nFeinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi,\nRichard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd,\nLe Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste\nLespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van\nAmersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew\nBrock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki\u0107evi\u0107, Mostafa\nDehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener,\nFantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed,\nEd Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane,\nDylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam Sadovsky,\nOskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan, Edouard\nLeurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan,\nPam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar,\nMichael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez\nElias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino\nVieillard, Sahitya Potluri, Jane Park, Elnaz Davoodi, Jiageng Zhang, Jeff\nStanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar,\nLuowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia, Anselm\nLevskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu Mao, Alberto\nMagni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan\nPalmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin\nChen, Izhak Shafran, Michael Fink, Alfonso Casta\u00f1o, Irene Giannoumis,\nWooyeol Kim, Miko\u0142aj Rybi\u0144ski, Ashwin Sreevatsa, Jennifer Prendki, David\nSoergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba,\nJeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana\nKulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin\nRamirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu,\nRicardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom\nvan der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li,\nOlivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi,\nNathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro\nValenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer\nGreene, Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian\nKrause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng,\nBiao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le,\nElena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga\nBolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad,\nRemi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Sid Lall, Ken\nFranko, Egor Filonov, Anna Bulanova, R\u00e9mi Leblond, Vikas Yadav, Shirley\nChung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu,\nChristina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek\nDimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark\nOmernick, Colton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan\nJain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid,\nFrancesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila Noga,\nPremal Shah, Daniel J. Mankowitz, Alex Polozov, Nate Kushman, Victoria\nKrakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu,\nMeghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth\nMudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael\nKwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila\nSinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Severyn,\nJonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, Denis\nVnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian\nEisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong\nDao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller, Tian Huey\nTeh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel\nToyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman,\nJohn Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya\nGrunina, Rishika Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese\nOwusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna\nNarayana, Jing Li, Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria,\nTao Zhu, Yeongil Ko, Laura Knight, Am\u00e9lie H\u00e9liou, Ning Niu, Shane Gu,\nChenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich, Norbert\nKalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin\nStrudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay,\nHyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael\nHoffmann, Dan Holtmann-Rice, Olivier Bachem, Summer Yue, Sho Arora, Eric\nMalmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim\nP\u00f5der, Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian\nIonita, Mojtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu,\nAnmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi,\nTianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton,\nChenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li,\nTJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz\nBahargam, Rob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins,\nMarco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy,\nFedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivi\u00e8re,\nAlanna Walton, Cl\u00e9ment Crepy, Alicia Parrish, Yuan Liu, Zongwei Zhou,\nClement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm,\nAndreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna\nKlimczak-Pluci\u0144ska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria\nMendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey\nGuseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi\nYenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez,\nSteve Yadlowsky, Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil\nDua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi,\nAnanth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle,\nLei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan\nSaeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz,\nManish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma,\nTaylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James\nManyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh\nTripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua\nAinslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang,\nRiham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai,\nWarren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof\nAngermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil\nKoukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson,\nParashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki,\nChrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev,\nElahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan\nRedshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman,\nParker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor\nStrohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu,\nJeffrey Dean, and Oriol Vinyals.",
                "venue": null,
                "url": null
            }
        },
        {
            "71": {
                "title": "Lamda: Language models for dialog applications.",
                "author": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv\nKulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al.",
                "venue": "arXiv preprint arXiv:2201.08239, 2022.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric\nHambro, Faisal Azhar, et al.",
                "venue": "arXiv preprint arXiv:2302.13971, 2023.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Tl; dr: Mining reddit to learn automatic summarization.",
                "author": "Michael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein.",
                "venue": "In Proceedings of the Workshop on New Frontiers in\nSummarization, pages 59\u201363, 2017.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Trl: Transformer reinforcement learning.",
                "author": "Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan\nThrush, Nathan Lambert, and Shengyi Huang.",
                "venue": "https://github.com/huggingface/trl, 2020.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Aligning large language models with human: A survey.",
                "author": "Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang,\nLifeng Shang, Xin Jiang, and Qun Liu.",
                "venue": "arXiv preprint arXiv:2307.12966, 2023.",
                "url": null
            }
        },
        {
            "76": {
                "title": "Finetuned language models are zero-shot learners.",
                "author": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M Dai, and Quoc V Le.",
                "venue": "arXiv preprint arXiv:2109.01652, 2021.",
                "url": null
            }
        },
        {
            "77": {
                "title": "Bloom: A 176b-parameter open-access multilingual language model.",
                "author": "BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha\nLuccioni, Fran\u00e7ois Yvon, et al.",
                "venue": "arXiv preprint arXiv:2211.05100, 2022.",
                "url": null
            }
        },
        {
            "78": {
                "title": "Some things are more cringe than others: Preference optimization with\nthe pairwise cringe loss, 2023a.",
                "author": "Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston.",
                "venue": null,
                "url": null
            }
        },
        {
            "79": {
                "title": "Parameter-efficient fine-tuning methods for pretrained language\nmodels: A critical review and assessment, 2023b.",
                "author": "Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang.",
                "venue": null,
                "url": null
            }
        },
        {
            "80": {
                "title": "Self-rewarding language models, 2024.",
                "author": "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing\nXu, and Jason Weston.",
                "venue": null,
                "url": null
            }
        },
        {
            "81": {
                "title": "Rrhf: Rank responses to align language models with human feedback\nwithout tears.",
                "author": "Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang.",
                "venue": "arXiv preprint arXiv:2304.05302, 2023.",
                "url": null
            }
        },
        {
            "82": {
                "title": "Slic-hf: Sequence likelihood calibration with human feedback, 2023.",
                "author": "Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and\nPeter J. Liu.",
                "venue": null,
                "url": null
            }
        },
        {
            "83": {
                "title": "Sira: Sparse mixture of low rank adaptation, 2023.",
                "author": "Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han\nLu, Canoee Liu, Liangchen Luo, Jindong Chen, and Lei Meng.",
                "venue": null,
                "url": null
            }
        },
        {
            "84": {
                "title": "Fine-tuning language models from human preferences, 2020.",
                "author": "Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford,\nDario Amodei, Paul Christiano, and Geoffrey Irving.",
                "venue": null,
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.10704v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6",
            "6.1",
            "6.2",
            "6.3"
        ],
        "methodology_sections": [
            "2",
            "5",
            "5.1",
            "5.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6",
            "5.1.1",
            "5.1.2",
            "5.2.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1.1",
            "4.1.2",
            "4.2.1",
            "4.2.2",
            "4.3.1",
            "4.3.2",
            "4.4.1",
            "4.4.2",
            "4.5.1",
            "4.5.2",
            "4.6.1",
            "4.6.2",
            "5.1.1",
            "5.1.2",
            "5.2.1"
        ]
    },
    "research_context": {
        "paper_id": "2403.10704v1",
        "paper_title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
        "research_background": "### Paper's Motivation:\n\nThe paper is motivated by the need to align pretrained Large Language Models (LLMs) with human preferences to ensure high-quality performance. This alignment is crucial for improving instruction following and fine-tuning models for behaviors that do not have a direct mathematical expression of a loss function (e.g., safety properties, summarization characteristics). Despite the effectiveness of Reinforcement Learning from Human Feedback (RLHF) in achieving this alignment, its complexity and computational cost hinder wider adoption. The paper aims to investigate how Parameter Efficient Fine-Tuning (PEFT) approaches can make RLHF more efficient and accessible.\n\n### Research Problem:\n\nThe primary research problem addressed in the paper is the high computational cost and memory requirements of the conventional RLHF process, which involve fine-tuning all the parameters of the reward model and the policy. This process requires significant computational resources because it involves multiple instances of LLMs, such as a reward model and an anchor model. Another challenge is the difficulty in collecting enough high-quality training data to create an effective reward model. The paper proposes the use of PEFT methods, particularly Low-Rank Adaptation (LoRA), to make the RLHF process more parameter-efficient and accessible.\n\n### Relevant Prior Work:\n\n1. **Pretrained LLMs and Their Performance:** \n   - GPT-4 (OpenAI et al., 2023) \n   - Gemini (Team et al., 2023; Reid et al., 2024)\n\n2. **Importance of Aligning Models with Human Preferences:**\n   - Improves instruction following (Ouyang et al., 2022)\n   - Improves fine-tuning for behaviors lacking direct mathematical loss functions (Bai et al., 2022a, b; Stiennon et al., 2020)\n\n3. **Reinforcement Learning from Human Feedback (RLHF):**\n   - One of the most popular methods for model alignment (Bommasani et al., 2022)\n   - Shown to be effective in alignment (Stiennon et al., 2020; Bai et al., 2022b)\n\n4. **Challenges in Conventional RLHF:**\n   - High computational cost and memory requirements\n   - Complexity and requirement for multiple LLM instances (e.g., reward model and an anchor model)\n   - Difficulty in collecting high-quality human preference data\n\n5. **Parameter Efficient Fine-Tuning (PEFT):**\n   - PEFT approaches like those proposed by Houlsby et al. (2019) offer a potentially more efficient way to fine-tune models.\n\n6. **Low-Rank Adaptation (LoRA):**\n   - LoRA is a specific PEFT method introduced by Hu et al. (2021) that is evaluated in this paper for its effectiveness in RLHF.",
        "methodology": "### Methodology: PERL - Parameter Efficient Reinforcement Learning from Human Feedback \n\n**Proposed Method:**\n\nPERL aims to make the Reinforcement Learning from Human Feedback (RLHF) process more efficient. The RLHF process typically involves two main model training stages:\n1. Reward Model Training\n2. Reinforcement Learning\n\n**Key Components and Innovations:**\n\n1. **Reward Model Training with LoRA Adapters:**\n   - **Construction:** The PERL reward models are built as language models integrated with LoRA (Low-Rank Adaptation) adapters.\n   - **Attachment of LoRA:** A LoRA adapter is attached to each attention projection matrix within the language model.\n   - **Training Process:** Only the LoRA adapters are trained, keeping the core language model (backbone) frozen. This selective training significantly reduces the number of trainable parameters.\n   - **Inference Combination:** The trained LoRA adapters are saved and combined with the projection matrices during inference. This combination is performed as a one-time addition before running the RL loop, making the resultant reward model functionally equivalent to a non-LoRA model.\n\n2. **Reinforcement Learning Loop with LoRA Adapters:**\n   - **Policy Model:** Similar to the reward model, the policy models in the PERL reinforcement learning loop are language models equipped with LoRA adapters.\n   - **Attachment of LoRA:** A LoRA adapter is attached to each attention projection matrix.\n   - **Training Process:** Once again, only the LoRA adapters are trained while keeping the backbone of the language model frozen. \n   - **Policy Gradient and KL Regularization:** The policy is optimized based on the policy gradient calculated using the reward score and includes a KL regularization term with the anchor policy.\n\n**Efficiency Gains:**\n- **Memory Reduction:** Modern optimizers like Adam or Adafactor, which track multiple factors for each trainable parameter, demand substantial memory. By reducing the number of trainable parameters through the exclusive training of LoRA adapters, PERL significantly cuts down on memory requirements.\n- **Faster Training:** With fewer parameters to update in each training step, training speed is notably increased.\n\nIn summary, PERL enhances the RLHF process by incorporating LoRA adapters to reduce memory usage and accelerate training, without compromising the performance of the resulting models.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets and Tasks**:  \nThe main experiment utilized feedback datasets comprising either preference pairs or classification labels. These feedback data were employed to train reward models, which are crucial for reinforcement learning. The preference or classification labels were collected from candidate responses sampled from a diverse array of policies, allowing the reward models to gain broad exposure to varied domains during training.\n\n**Reinforcement Learning Loop**:  \nIn the reinforcement learning phase, the datasets were used to provide input prompts to large language models (LLMs). These prompts facilitated the sampling of episodes, upon which policy updates were made based on the rewards calculated for each sampled episode. This loop iteratively refined the policies of the LLMs by leveraging human feedback encoded in the reward models.\n\n**Evaluation Metrics**:  \nThough the text does not specify the exact evaluation metrics used in the main experiment, they likely included standard metrics for reinforcement learning and human feedback evaluation. These could include reward-based metrics, accuracy of classification labels, and the quality of generated responses as judged by human evaluators or automated metrics.\n\n**Baselines**:  \nThe details regarding baselines used for comparison are not provided in the provided text. Generally, such experiments would compare against baseline models that do not incorporate human feedback or use simpler forms of reinforcement learning without complex reward models.\n\n### Main Experimental Results\n\nThe exact results are not outlined in the provided section. However, the text mentions the use of a diverse set of policies and broad exposure domains, suggesting an improvement in the learning process efficiency and policy quality due to the richness of the reward signals derived from a varied dataset. \n\nIn summary, the main experiment setup focuses on using human feedback in the form of preference pairs or classification labels to train reward models. These models drive the policy updates in a reinforcement learning loop, using LLMs prompted by dataset inputs to generate and refine episodes. The success or failure of this setup would typically be measured by how well the reward models and updated policies perform relative to human-defined criteria or baseline models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate how the LoRA adapter rank affects the capability of fine-tuning models to align with human preferences using different datasets.",
            "experiment_process": "Using a variety of datasets, reward models are fine-tuned with different LoRA ranks (e.g., 1, 4, 8, 16). The experimental setup includes measuring pairwise accuracy, peak High Bandwidth Memory (HBM) usage, and computational speed. Results are benchmarked with both fully tuned models and LoRA fine-tuned models.",
            "result_discussion": "LoRA fine-tuned models achieve on-par performance with fully tuned models across ranks, including rank 1 and rank 16. Significant computational benefits are observed, such as reduced HBM usage and faster training times. Specific datasets, such as Anthropic Harmlessness and SHP, showed that optimal ranks like 8 provided the best results, with ranks as low as 1 also being quite effective in other cases.",
            "ablation_id": "2403.10704v1.No1"
        },
        {
            "research_objective": "Understand the impact of model size variations on the performance of both fully tuned and LoRA fine-tuned reward models and RL policies.",
            "experiment_process": "Experiments are conducted to compare pairwise accuracies or cumulative rewards with different backbone LLM model sizes (e.g., PaLM 2 XXS, XS, and S). Both LoRA tuning and full tuning are applied, and the best LoRA ranks for different model sizes are reported. Evaluation metrics include pairwise accuracy for reward models and cumulative reward for RL policies.",
            "result_discussion": "Results indicate that larger model sizes lead to higher performance for both fully tuned and LoRA fine-tuned approaches. LoRA fine-tuning shows greater benefits from model size increases compared to full tuning, with noted improvements in both pairwise accuracy and cumulative reward metrics. For example, larger architecture in the RL policy training settings showed better performance across both full and LoRA tuning, and the increase in model sizes yielded higher accuracies for reward models.",
            "ablation_id": "2403.10704v1.No2"
        }
    ]
}