{
    "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
    "abstract": "Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model\u2019s utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse\u2014a drastic degradation of the model\u2019s utilities.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs), pretrained on massive corpora of internet data, possess the capability to memorize portions of their training data (Carlini et al., 2021  ###reference_b4###, 2022  ###reference_b5###). However, this capability raises significant concerns, as the training data may contain sensitive or private information, potentially leading to societal challenges. For instance, language models could breach individual privacy by outputting personal information such as social security numbers from the memorized data (Carlini et al., 2021  ###reference_b4###; Huang et al., 2022  ###reference_b14###). They might also violate copyright by generating text from memorized books, such as the Harry Potter novels (Eldan & Russinovich, 2023  ###reference_b9###). Furthermore, LLM assistants for biology could inadvertently aid in the development of biological weapons by troubleshooting bottlenecks, increasing the risk of such attempts (Sandbrink, 2023  ###reference_b33###; Li et al., 2024  ###reference_b19###). In response to these concerns, regulations like the EU\u2019s General Data Protection Regulation (GDPR) (Mantelero, 2013  ###reference_b25###; Voigt & Von dem Bussche, 2017  ###reference_b40###) and the US\u2019s California Consumer Privacy Act (CCPA) (CCPA, 2018  ###reference_b6###) have mandated the Right to be Forgotten, requiring applications to support the deletion of information contained in training samples upon user requests. This has motivated a line of research on machine unlearning, aiming to address these challenges.\nMachine unlearning (Cao & Yang, 2015  ###reference_b3###; Bourtoule et al., 2021  ###reference_b2###) aims to delete the influence of specific training samples from machine-learning models while preserving other knowledge and capabilities (Liu et al., 2024a  ###reference_b21###; Zhang et al., 2023  ###reference_b45###; Nguyen et al., 2022  ###reference_b28###; Xu et al., 2023  ###reference_b42###; Si et al., 2023  ###reference_b36###). Notably, a straightforward approach to unlearning is to retrain a language model from scratch. However, as retraining from scratch is typically computationally expensive, cheaper methods for removing undesirable information is highly desirable. Recently, several works (Jang et al., 2022  ###reference_b16###; Wang et al., 2023  ###reference_b41###; Chen & Yang, 2023  ###reference_b7###; Yao et al., 2023  ###reference_b44###; Eldan & Russinovich, 2023  ###reference_b9###; Yao et al., 2024  ###reference_b43###; Liu et al., 2024b  ###reference_b22###; Li et al., 2024  ###reference_b19###) proposed scalable and practical techniques for unlearning LLMs through directly fine-tuning the trained model. Core to many of these works is a gradient ascent procedure on the prediction loss over the dataset to be unlearned (i.e., the forget set), building on the intuition that gradient ascent is an approximation of \u201creverting\u201d gradient descent optimization.\n###figure_1### Despite its simplicity and widespread use, the performance of gradient ascent based approaches remain unsatisfactory. A notable example concerns the recently released benchmark dataset TOFU (Maini et al., 2024  ###reference_b24###), which consists of synthetically generated biographies of 200 fictitious authors, and the task is to unlearn the biographies of 1%, 5%, and 10% of the 200 authors from a model that is already fine-tuned on all 200 authors. In their evaluation of forgetting 10% of the authors, Maini et al. (2024  ###reference_b24###) demonstrated that gradient ascent and its variants fail to provide a satisfactory balance between forget quality (the difference between the unlearned model and retrained model evaluated on the forget set) and model utility (the general performance on other tasks).\nIn this work, we begin by observing that gradient ascent can often cause a rapid deterioration of model utility during unlearning\u2014a phenomenon we term catastrophic collapse\u2014which we believe is responsible for its unsatisfactory performance. Towards fixing this, we propose a simple yet effective objective function for unlearning termed Negative Preference Optimization (NPO). NPO takes inspiration from preference optimization (Rafailov et al., 2024  ###reference_b32###; Ouyang et al., 2022  ###reference_b29###; Bai et al., 2022  ###reference_b1###), and can be viewed as its variant that only uses negative samples. Through both theory and experiments, we show that NPO resolves the catastrophic collapse issue associated with gradient ascent, provides more stable training dynamics, and achieves a better trade-off between forget quality and model utility. Coupled with a cross-entropy loss on the retain set, NPO achieves state-of-the-art performance on the TOFU dataset, and achieves the first non-trivial unlearning result on the challenging task of forgetting 50% of the TOFU data.\nWe outline existing gradient ascent based methods for machine unlearning, and find that these methods suffer from catastrophic collapse (Section 2  ###reference_###). We identify the linear divergence speed of gradient ascent as a main reason for catastrophic collapse.\nWe introduce Negative Preference Optimization (NPO), a simple alignment-inspired loss function for LLM unlearning that addresses the catastrophic collapse issue of gradient ascent (GA; Section 3  ###reference_###). We demonstrate that NPO reduces to gradient ascent (GA) in the high-temperature limit. We show in theory the progression towards catastrophic collapse when minimizing the NPO loss is exponentially slower than with GA. See Figure 1  ###reference_### for an illustration of NPO and its connections with existing objectives.\nWe test NPO-based methods on a synthetic binary classification task (Section 4  ###reference_###), where we find that NPO-based methods outperform other baselines by providing a superior Pareto frontier between the Forget Distance and Retain Distance. Furthermore, NPO-based methods exhibit greater learning stability compared to GA-based methods.\nWe evaluate a variety of unlearning methods on the TOFU dataset (Maini et al., 2024  ###reference_b24###) and find that NPO-based methods exhibit superior balance between Forget Quality and Model Utility compared to all baselines (Section 5  ###reference_###). Additionally, NPO-based methods improve the stability of the unlearning process and the readability of the output. Notably, we show that NPO-based methods are the only effective unlearning methods for forgetting 50%-90% of the data, a significant advance over all existing methods which already struggle with forgetting 10% of the data (Section 5.3  ###reference_###).\nThere is a vast literature on machine unlearning and LLM unlearning. Due to limited space, we discuss these related work in Appendix 1.1  ###reference_###."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Related work",
            "text": "Since its proposal by Cao & Yang (2015  ###reference_b3###), machine unlearning has been extensively studied in the classification literature (Bourtoule et al., 2021  ###reference_b2###; Golatkar et al., 2020  ###reference_b12###; Ginart et al., 2019  ###reference_b11###; Thudi et al., 2022  ###reference_b38###; Izzo et al., 2021  ###reference_b15###; Koh & Liang, 2017  ###reference_b18###; Guo et al., 2019  ###reference_b13###; Sekhari et al., 2021  ###reference_b34###). For reviews of existing works, see Liu et al. (2024a  ###reference_b21###); Zhang et al. (2023  ###reference_b45###); Nguyen et al. (2022  ###reference_b28###); Xu et al. (2023  ###reference_b42###); Si et al. (2023  ###reference_b36###). In particular, Ginart et al. (2019  ###reference_b11###); Guo et al. (2019  ###reference_b13###); Sekhari et al. (2021  ###reference_b34###) introduced theoretical metrics for machine unlearning based on the notion of differential privacy and proposed provably efficient unlearning methods based on Newton update removal mechanisms. However, these algorithms require computing the Hessian of loss functions, which is intractable for LLMs.\nRecent research has explored unlearning methods for LLMs (Jang et al., 2022  ###reference_b16###; Wang et al., 2023  ###reference_b41###; Chen & Yang, 2023  ###reference_b7###; Yao et al., 2023  ###reference_b44###; Eldan & Russinovich, 2023  ###reference_b9###; Yao et al., 2024  ###reference_b43###; Liu et al., 2024b  ###reference_b22###; Li et al., 2024  ###reference_b19###). Notably, the methods proposed in Jang et al. (2022  ###reference_b16###); Yao et al. (2023  ###reference_b44###); Chen & Yang (2023  ###reference_b7###); Maini et al. (2024  ###reference_b24###) are based on gradient ascent (GA) on the loss of the forget set. In this work, we demonstrate that the NPO approach consistently outperforms GA across various tasks. On the other hand, Eldan & Russinovich (2023  ###reference_b9###) proposed generating positive samples using LLMs and carefully designed prompts, then fine-tuning the model based on the positive samples using a supervised loss. Furthermore, the method of Liu et al. (2024b  ###reference_b22###) is based on knowledge negation, while the approach of Li et al. (2024  ###reference_b19###) relies on controlling model representations. These methods are orthogonal and complementary to the NPO approach.\nOur method, NPO, draws inspiration from the framework of reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022  ###reference_b29###; Bai et al., 2022  ###reference_b1###; Stiennon et al., 2020  ###reference_b37###; Rafailov et al., 2024  ###reference_b32###), particularly the Direct Policy Optimization (DPO) method (Rafailov et al., 2024  ###reference_b32###). We note that recent work (Ethayarajh et al., 2024  ###reference_b10###) proposes the Kahneman-Tversky Optimization (KTO) method for alignment with only non-paired preference data, and a more recent concurrent work (Duan et al., 2024  ###reference_b8###) proposes the Distributional Dispreference Optimization (O) approach for unlearning. Both methods share a similar formulation to NPO. We compare the performance of NPO with KTO in simulations.\nRecent work has proposed several benchmark datasets and evaluation metrics for unlearning methods (Ji et al., 2024  ###reference_b17###; Eldan & Russinovich, 2023  ###reference_b9###; Maini et al., 2024  ###reference_b24###; Li et al., 2024  ###reference_b19###; Lynch et al., 2024  ###reference_b23###). In particular, some studies have utilized the PKUSafe dataset (Ji et al., 2024  ###reference_b17###) for benchmarking unlearning methods. Eldan & Russinovich (2023  ###reference_b9###) crafts a specific task of \u201cforgetting Harry Potter\u201d. Maini et al. (2024  ###reference_b24###) introduces TOFU, a task of fictitious unlearning for LLMs, which is the benchmark we adopted in this paper. Additionally, Li et al. (2024  ###reference_b19###) proposes the Weapons of Mass Destruction Proxy (WMDP) for measuring hazardous knowledge in LLMs. Lynch et al. (2024  ###reference_b23###) proposes eight methods to evaluate robust unlearning in LLM, which incorporate robust metrics against jailbreak attacks.\nFinally, we note the existence of attack methods for extracting data from unlearned models (Shi et al., 2023  ###reference_b35###; Patil et al., 2023  ###reference_b30###), and other unlearning methods including model editing (Mitchell et al., 2022  ###reference_b27###; Meng et al., 2022  ###reference_b26###) and in-context unlearning (Pawelczyk et al., 2023  ###reference_b31###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries on Machine Unlearning",
            "text": "refers to the following problem: Given an initial model (also the reference model)  that is already trained on a dataset , how to make the model forget a specific subset (henceforth the forget set)  of the training data? More precisely, we aim to fine-tune111There are alternative approaches such as prompt engineering (Pawelczyk et al., 2023  ###reference_b31###) for performing unlearning tasks. the model to make it behave like the retrained model , a model trained only on the retain set . In other words, we would like the model to behave as if the samples in the forget set  were never used to train it.\nBy definition, the best approach for machine unlearning, in principle, is to retrain the model from scratch on  only, which is, however, often intractable in practice.\nis a key component in many existing LLM unlearning methods and an important baseline method for LLM unlearning on its own. The idea is simply to perform gradient ascent on the (next-token prediction) loss over the forget set, which can be viewed equivalently as gradient descent on the negative prediction loss, denoted as :\nThe rationale of gradient ascent is that since the initial model  is trained on , a subsequent maximization of prediction loss on the forget set  would approximately \u201crevert\u201d the optimization on the forget set , thus unlearning  and approximating a model trained on  only.\nBuilding on gradient ascent, a large class of unlearning methods perform gradient-based optimization on a linear combination of the GA loss  and several other loss functions that either encourage unlearning or preserve utility (Jang et al., 2022  ###reference_b16###; Yao et al., 2023  ###reference_b44###; Chen & Yang, 2023  ###reference_b7###; Maini et al., 2024  ###reference_b24###; Eldan & Russinovich, 2023  ###reference_b9###). Notable examples include\nForget (FG) loss: , where  and  is any \u201cuninformed\u201d response for prompt  which the unlearned model could aim to output. Examples of such \u2019s include replacing true information by random (but appearingly sensible) information (which requires hand-crafting such as Eldan & Russinovich (2023  ###reference_b9###)), or simply answering \u201cI don\u2019t know\u201d (Maini et al., 2024  ###reference_b24###).\nRetain (RT) loss: , which encourages the model to still perform well on the retain set ;\n, which measures the distance to the initial model  (in KL divergence) on the forget set;\n, which measures the distance to the initial model  (in KL divergence) on the retain set.\nFor example, Yao et al. (2023  ###reference_b44###) minimize a combination of , and Chen & Yang (2023  ###reference_b7###) minimize a combination of . Maini et al. (2024  ###reference_b24###) find that incorporating the retain loss  usually improves the performance of unlearning.\nUnlearning methods should not only unlearn the forget set, i.e., achieve a high forget quality, but also maintain the model\u2019s performance on the retain set, i.e., maintain the model utility. For example, letting the model simply output \u201cI don\u2019t know\u201d is an unlearning method that achieves good forget quality (in certain sense) but bad model utility. While there is not yet a consensus on the right metrics for forget quality and model utility (and we will present our choices momentarily), a general rule of thumb is that unlearning methods should achieve a good tradeoff between these two goals."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Catastrophic collapse of gradient ascent",
            "text": "###figure_2### We begin by testing gradient ascent as a standalone method (as opposed to combining it with other losses), and find that gradient ascent exhibits a common failure mode dubbed as catastrophic collapse: Along the unlearning process, the model utility quickly drops to zero, and the forget quality improves temporarily for a very short time horizon before quickly dropping too (Figure 2  ###reference_### left/middle-left). Along the same training trajectory, the model diverges quickly from the initial model (as measured by the KL distance to the initial model), after which the model generates gibberish outputs (Figure 2  ###reference_### middle-right/right).\nWe attribute the catastrophic collapse to the divergent nature of the gradient ascent algorithm due to the fact that it maximizes (instead of minimizes) the standard next-token prediction loss. Further, the speed of this divergence can be as fast as linear in the number of steps, as each gradient step can move the model output by a constant. To see this on a toy example, consider a linear-logistic -class classifier given by , . For any \u201calready unlearned\u201d sample  with true label  and model prediction  (so that  does not predict ), standard calculation shows that the gradient of GA loss with respect to  is , which has a constant scale (not diminishing along the unlearning progress) and can cause the model to diverge in a linear speed. Therefore, the divergent dynamics may initially bring the model closer to  but would ultimately send the model to infinity (c.f. Theorem 2  ###reference_orem2###).\nWhile we believe some kind of divergent behavior is necessary and perhaps unavoidable (as the goal of unlearning is to \u201crevert\u201d optimization), the fast divergence speed of gradient ascent is a rather undesired feature and motivates the proposal of our NPO method which diverges at a slower speed."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Negative Preference Optimization",
            "text": "We introduce Negative Preference Optimization (), a simple drop-in fix of the GA loss. The  loss reduces to the GA loss in the high-temperature limit, but remains lower-bounded and stable at any finite temperature, unlike the GA loss.\nWe take inspiration from preference optimization (Rafailov et al., 2024  ###reference_b32###) and derive NPO as a method of preference optimization with negative examples only.\nIn preference optimization (Ouyang et al., 2022  ###reference_b29###; Bai et al., 2022  ###reference_b1###; Stiennon et al., 2020  ###reference_b37###; Rafailov et al., 2024  ###reference_b32###), we are given a dataset with preference feedbacks , where  are two responses to  generated by a pre-trained model , and the preference  is obtained by human comparison (here \u201c\u201d stands for \u201cwin\u201d and \u201c\u201d stands for \u201close\u201d in a comparision). The goal is to fine-tune  using  to better align it with human preferences. A popular method for preference optimization is Direct Preference Optimization (DPO) (Rafailov et al., 2024  ###reference_b32###), which minimizes\nHere,  is the sigmoid function,  is the inverse temperature, and  is a reference model.\nWe observe that the unlearning problem can be cast into the preference optimization framework by treating each  as only providing a negative response  without any positive response . Therefore, we ignore the  term in DPO in Eq. (2  ###reference_###) and obtain the Negative Preference Optimization (NPO) loss:\nMinimizing  ensures that the prediction probability on the forget set  is as small as possible, aligning with the goal of unlearning the forget set.\nWe can recover the GA loss from NPO loss by eliminating the additional  in the logarithm of NPO loss in Eq. (3  ###reference_###), i.e., replacing  to . Furthermore, we show that the NPO loss also reduces to the GA loss in the limit of , indicating that NPO is a strict generalization of GA.\nFor any , we have\nMoreover, assuming  is differentiable with respect to , we have\nThe proof of Proposition 1  ###reference_orem1### is deferred to Appendix A.1  ###reference_###. Figure 3  ###reference_### provides an illustration of the reduction from the  loss to the  loss as .\n###figure_3### We now look at intuition for why we expect NPO to resolve catastrophic collapse. One limitation of the GA loss is its unboundedness from below (as the negation of the cross-entropy prediction loss which is unbounded from above). The NPO loss resolves this issue and remains lower-bounded for any finite .\nFurthermore, the gradients of NPO and GA are as follows:\nwhere  can be interpreted as an adaptive smoothing weight\u2014When example  is already unlearned in the sense that , we have , so that  and thus NPO could diverge much slower than GA."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Theoretical analysis of divergence speed",
            "text": "We formalize the above intuition by theoretically analyzing the divergence speed of NPO and GA in a standard logistic regression setting. We consider a binary classification problem () with a logistic model . The initial model is denoted as  with . We aim to unlearn a forget set  by minimizing either GA or NPO loss using gradient descent with stepsize  for  iterations.\nLet \nConsider the high-dimensional regime where  and assume  is invertible.\nSuppose ,  for all  for some . Let  denote the -th iterates of gradient descent with stepsize  on the empirical loss , respectively.\n( diverges linearly)\nThere exist some -dependent constants  such that when  \n,\n( diverges logarithmically) Suppose .\nThere exist some -dependent constants  \n such that when ,\nTheorem 2  ###reference_orem2### demonstrates that NPO diverges exponentially slower than GA in a simple setting. The proof of Theorem 2  ###reference_orem2### is contained in Appendix A.2  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Synthetic Experiments",
            "text": "We consider a forget set  and a retain set  which are both generated from Gaussian-logistic models. More specifically, we assume\nHere we choose , , and  for some . We consider two choices of the hyper-parameter : (1). , which creates a gap between the Gaussian means of forget covariates  and retain covariates ; (2). , which implies that covariates in the forget and retain set are both isotropic Gaussian. We remark that we shift by  in the sigmoid function to create a discrepancy in the label frequencies between the forget and retain sets \u2014 this ensures that the forget labels  are more likely to be , while the retain labels  are more likely to be .\nWe consider a random feature model , where  is fixed during the training and unlearning process, whose entries are generated i.i.d. from , and  is the trainable parameter. To generate the initial model  and the retrained model , we optimize over  using the cross-entropy loss over the entire dataset  and the retain dataset , respectively. In the unlearning phase, starting from the initial model , we perform gradient descent on various loss functions for  steps. We select the learning rate for each method via grid search.\nWe evaluate the performance of vanilla  (; minimizing ),  plus a retain loss term (+; minimizing +), gradient ascent (; minimizing ), gradient ascent plus a retain loss term (+; minimizing +), cross-entropy loss of forget and retain sets where the positive labels of the forget set are given by  (+; minimizing +), and DPO plus a retain loss term (+; minimizing +, where the positive labels are given by ). We conduct the grid search to select the optimal  for -based and -based methods. We note that -based methods are sensitive to the choice of learning rates, and therefore, we select the learning rates so that the training remains stable within  steps.\nWe measure the performance of unlearning methods via two metrics: the forget distance and the retain distance. The forget distance is , the KL divergence between the retrained model  and unlearned model  on the forget set. Similarly, the retain distance is given by . Ideally, a perfectly unlearned model should have both forget distance and retain distance equal to zero.\nAs illustrated in Figure 4  ###reference_### (a1) and (a2), all methods except for + reach a small forget distance (less than ) within  steps. On the other hand, the retain distances of  and + diverge (the catastrophic collapse) as unlearning proceeds, while the retain distances of + and + slowly increase and stabilize. This suggests that + and + are more stable compared with -based methods, in accordance with the theoretical findings in Theorem 2  ###reference_orem2###.\nFigure 4  ###reference_### (a3) shows that + outperforms other baseline methods by achieving a better Pareto frontier. Furthermore, when restricting to methods that do not use the retain set,  also outperforms the baseline method . Figure 4  ###reference_### (b) illustrates the  scenario where the covariate distributions for forget and retain sets are identical, resulting in equal forget and retain distances. In this scenario, + also attains the smallest forget and retain distances.\n###figure_4###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setup",
            "text": "We consider a forget set  and a retain set  which are both generated from Gaussian-logistic models. More specifically, we assume\nHere we choose , , and  for some . We consider two choices of the hyper-parameter : (1). , which creates a gap between the Gaussian means of forget covariates  and retain covariates ; (2). , which implies that covariates in the forget and retain set are both isotropic Gaussian. We remark that we shift by  in the sigmoid function to create a discrepancy in the label frequencies between the forget and retain sets \u2014 this ensures that the forget labels  are more likely to be , while the retain labels  are more likely to be .\nWe consider a random feature model , where  is fixed during the training and unlearning process, whose entries are generated i.i.d. from , and  is the trainable parameter. To generate the initial model  and the retrained model , we optimize over  using the cross-entropy loss over the entire dataset  and the retain dataset , respectively. In the unlearning phase, starting from the initial model , we perform gradient descent on various loss functions for  steps. We select the learning rate for each method via grid search.\nWe evaluate the performance of vanilla  (; minimizing ),  plus a retain loss term (+; minimizing +), gradient ascent (; minimizing ), gradient ascent plus a retain loss term (+; minimizing +), cross-entropy loss of forget and retain sets where the positive labels of the forget set are given by  (+; minimizing +), and DPO plus a retain loss term (+; minimizing +, where the positive labels are given by ). We conduct the grid search to select the optimal  for -based and -based methods. We note that -based methods are sensitive to the choice of learning rates, and therefore, we select the learning rates so that the training remains stable within  steps.\nWe measure the performance of unlearning methods via two metrics: the forget distance and the retain distance. The forget distance is , the KL divergence between the retrained model  and unlearned model  on the forget set. Similarly, the retain distance is given by . Ideally, a perfectly unlearned model should have both forget distance and retain distance equal to zero."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "As illustrated in Figure 4  ###reference_###  ###reference_### (a1) and (a2), all methods except for + reach a small forget distance (less than ) within  steps. On the other hand, the retain distances of  and + diverge (the catastrophic collapse) as unlearning proceeds, while the retain distances of + and + slowly increase and stabilize. This suggests that + and + are more stable compared with -based methods, in accordance with the theoretical findings in Theorem 2  ###reference_orem2###  ###reference_orem2###.\nFigure 4  ###reference_###  ###reference_### (a3) shows that + outperforms other baseline methods by achieving a better Pareto frontier. Furthermore, when restricting to methods that do not use the retain set,  also outperforms the baseline method . Figure 4  ###reference_###  ###reference_### (b) illustrates the  scenario where the covariate distributions for forget and retain sets are identical, resulting in equal forget and retain distances. In this scenario, + also attains the smallest forget and retain distances.\n###figure_5###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments on the TOFU Data",
            "text": "We evaluate unlearning methods on the TOFU dataset (Maini et al., 2024  ###reference_b24###). It contains 200 fictitious author profiles, each consisting of 20 question-answer pairs.\nTOFU introduces three levels of tasks, each aiming to forget 1% , 5% , and 10% of the data, referred to as Forget01, Forget05, and Forget10, respectively.\nWe measure the effectiveness of unlearning methods via Forget Quality and Model Utility as in Maini et al. (2024  ###reference_b24###). Forget quality assesses how well the unlearned model mimics the retrained model (defined as the model trained only on the retain set), while model utility measures the general capacities and the real-world knowledge of the unlearned model. Since the forget quality is defined as the p-value of the Kolmogorov-Smirnov test, which tests the similarity between some distributions generated by the unlearned model and the retrained one, we treat a forget quality greater than  as evidence of a meaningful forgetting. More details are deferred to Section E.1.1  ###reference_.SSS1### and Section E.1.2  ###reference_.SSS2###.\nWe compare the NPO-based methods with three variants of GA: GA (Jang et al., 2022  ###reference_b16###; Yao et al., 2023  ###reference_b44###), GA plus a retain loss (GA+RT), and GA plus a KL-divergence regularization (GA+KL). We also evaluate the IDK+RT method which replaces GA with a cross-entropy loss on the forget set with answers replaced by \u201dI don\u2019t know\u201d. Besides, we examine DPO and its regularized variants (DPO+RT, DPO+KL), as well as KTO (Ethayarajh et al., 2024  ###reference_b10###) and its variant (KTO+RT). All experiments on TOFU are conducted on Llama-2-7B-chat (Touvron et al., 2023  ###reference_b39###). See Section E.1  ###reference_### for more details.\nFigure 5  ###reference_### illustrates the trade-off between forget quality and model utility for various unlearning methods in the Forget01, Forget05, and Forget10. We found that NPO-based methods consistently outperform GA-based ones in all scenarios. Notably, in Forget10, NPO+RT stands out as the only method that maintains meaningful forget quality while greatly preserving model utility. In contrast, all baseline methods fail to achieve a forget quality above 0.05.\n###figure_6### ###figure_7### Figure 6  ###reference_### illustrates the evolution of forget quality and model utility along the unlearning process. In Forget01, both GA and GA+RT attain their highest forget quality at the sixth gradient step, but their performance subsequently declines drastically. Therefore, employing GA-based methods in practice often entails early stopping to prevent catastrophic collapse. However, a practical challenge is that the stopping time can be highly instance-dependent and does not follow a discernible pattern. In contrast, NPO-based methods display considerably greater stability, with forget quality consistently reaching and maintaining a plateau.\nLLMs unlearned via GA-based methods tend to output repeated words or gibberish sentences with unreasonably low diversity (Yao et al., 2023  ###reference_b44###). Moreover, IDK and DPO-based methods tend to show excessive ignorance. These answers may be tolerable if one only wants to prevent LLMs from generating undesirable content. Still, they will definitely be unsatisfactory under the stronger goal of approximate unlearning, which aims to mimic the retrained model. We show in Figure 7  ###reference_### that NPO+RT outputs incorrect sentences with similar templates for questions in the forget set while generating fluent and correct answers for other questions, greatly enhancing the fluency and diversity of the generated content.\n###figure_8### To further investigate the role of retain loss beyond Maini et al. (2024  ###reference_b24###), we evaluate NPO+RT with the weights of the retain loss varying from 0 to 5 (Figure 11  ###reference_###). While it is natural that adding retain loss improves the model utility, we are surprised that the forget quality also grows. Specifically, the forget quality increases as the weight of the retain loss grows from 0 to 2. We conjecture that the retain loss term helps the model preserve answer templates and linguistic structures, while the NPO term forces the model to forget some specific facts. Combining these two effects pushes the model to approximate the retrained model by generating outputs with similar templates but incorrect entities. We also note that further increasing the weight of the retain loss (e.g., from 2 to 5) leads to a drop in forget quality.\nHaving demonstrated that NPO-based methods can effectively unlearn 10% of the TOFU data, we now expand our scope to the tasks of forgetting 20%, 30%, and 50% of the TOFU data (referred to as Forget20, Forget30, Forget50, respectively). Details about the extended dataset are deferred to Section E.1.1  ###reference_.SSS1###. We show in Section E.2  ###reference_### that NPO+RT is the sole method to exhibit meaningful forget quality (a p-value above 0.05) in Forget20 and Forget30. Even in Forget50, where the vanilla NPO+RT achieves a forget quality around , it still significantly outperforms other methods.\nThe TOFU framework allows us to aim to forget at most 90% of the data since at least 10% is left out as the retain set for evaluation. We thus ask the question of whether there exist methods that could effectively forget 50%-90% of the TOFU data. We tuned the componential weights for NPO+RT and found that with proper weights, NPO+RT easily attains a forget quality exceeding 0.05 and model utility above 0.55 on Forget50 and Forget90, as reported in Figure 8  ###reference_###."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experimental setup",
            "text": "We evaluate unlearning methods on the TOFU dataset (Maini et al., 2024  ###reference_b24###  ###reference_b24###). It contains 200 fictitious author profiles, each consisting of 20 question-answer pairs.\nTOFU introduces three levels of tasks, each aiming to forget 1% , 5% , and 10% of the data, referred to as Forget01, Forget05, and Forget10, respectively.\nWe measure the effectiveness of unlearning methods via Forget Quality and Model Utility as in Maini et al. (2024  ###reference_b24###  ###reference_b24###). Forget quality assesses how well the unlearned model mimics the retrained model (defined as the model trained only on the retain set), while model utility measures the general capacities and the real-world knowledge of the unlearned model. Since the forget quality is defined as the p-value of the Kolmogorov-Smirnov test, which tests the similarity between some distributions generated by the unlearned model and the retrained one, we treat a forget quality greater than  as evidence of a meaningful forgetting. More details are deferred to Section E.1.1  ###reference_.SSS1###  ###reference_.SSS1### and Section E.1.2  ###reference_.SSS2###  ###reference_.SSS2###.\nWe compare the NPO-based methods with three variants of GA: GA (Jang et al., 2022  ###reference_b16###  ###reference_b16###; Yao et al., 2023  ###reference_b44###  ###reference_b44###), GA plus a retain loss (GA+RT), and GA plus a KL-divergence regularization (GA+KL). We also evaluate the IDK+RT method which replaces GA with a cross-entropy loss on the forget set with answers replaced by \u201dI don\u2019t know\u201d. Besides, we examine DPO and its regularized variants (DPO+RT, DPO+KL), as well as KTO (Ethayarajh et al., 2024  ###reference_b10###  ###reference_b10###) and its variant (KTO+RT). All experiments on TOFU are conducted on Llama-2-7B-chat (Touvron et al., 2023  ###reference_b39###  ###reference_b39###). See Section E.1  ###reference_###  ###reference_### for more details."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Forgetting beyond 10% of TOFU",
            "text": "###figure_12### Having demonstrated that NPO-based methods can effectively unlearn 10% of the TOFU data, we now expand our scope to the tasks of forgetting 20%, 30%, and 50% of the TOFU data (referred to as Forget20, Forget30, Forget50, respectively). Details about the extended dataset are deferred to Section E.1.1  ###reference_.SSS1###  ###reference_.SSS1###. We show in Section E.2  ###reference_###  ###reference_### that NPO+RT is the sole method to exhibit meaningful forget quality (a p-value above 0.05) in Forget20 and Forget30. Even in Forget50, where the vanilla NPO+RT achieves a forget quality around , it still significantly outperforms other methods.\nThe TOFU framework allows us to aim to forget at most 90% of the data since at least 10% is left out as the retain set for evaluation. We thus ask the question of whether there exist methods that could effectively forget 50%-90% of the TOFU data. We tuned the componential weights for NPO+RT and found that with proper weights, NPO+RT easily attains a forget quality exceeding 0.05 and model utility above 0.55 on Forget50 and Forget90, as reported in Figure 8  ###reference_###  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose Negative Preference Optimization (NPO), a simple objective for LLM unlearning. NPO makes steps towards addressing the catastrophic collapse issue in the gradient ascent method. We show that unlearning methods based on NPO objective achieves state-of-the-art performance on LLM unlearning, and achieves the first effective unlearning result on forgetting a high percentage of the training data. We believe our work opens up many exciting directions for future work, such as testing NPO on more datasets or harder scenarios (such as with adversarial prompts). It may also be of interest to generalize the algorithm principle of NPO (preference optimization with negative examples only) to other problems beyond unlearning."
        }
    ],
    "url": "http://arxiv.org/html/2404.05868v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1"
        ],
        "methodology_sections": [
            "3",
            "3.1"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "3",
            "4",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2404.05868v1",
        "paper_title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
        "research_background": "**Paper's Motivation:**\n\nThe paper is motivated by concerns around the potential misuse of large language models (LLMs) that memorize training data, which may include sensitive or private information. This could lead to privacy breaches, copyright infringement, or even aiding in harmful activities such as biological weapon development. These concerns are amplified as regulations like GDPR and CCPA mandate the right to be forgotten, necessitating effective methods for machine unlearning.\n\n**Research Problem:**\n\nThe primary research problem addressed by the paper is to develop an effective method for machine unlearning in LLMs that overcomes the limitations of existing approaches, particularly gradient ascent-based methods, which suffer from a phenomenon termed catastrophic collapse. This collapse leads to a rapid deterioration of model utility during the unlearning process. The goal is to establish a method that provides a better trade-off between forget quality and model utility while ensuring stable training dynamics.\n\n**Relevant Prior Work:**\n\n1. **Carlini et al. (2021, 2022)** - These studies highlighted how LLMs can memorize portions of their training data, raising concerns around privacy and sensitive information leakage.\n   \n2. **Huang et al. (2022)** - Discussed potential privacy breaches by LLMs outputting personal information.\n   \n3. **Eldan & Russinovich (2023)** - Examined copyright issues, such as LLMs generating text from copyrighted content like books.\n   \n4. **Sandbrink (2023) and Li et al. (2024)** - Explored the risks of LLMs aiding in the development of biological weapons.\n   \n5. **Cao & Yang (2015) and Bourtoule et al. (2021)** - Initial foundational works on the concept of machine unlearning.\n   \n6. **Various recent works (Jang et al., 2022; Wang et al., 2023; Chen & Yang, 2023; Yao et al., 2023; Eldan & Russinovich, 2023; Liu et al., 2024b; Li et al., 2024)** - Proposed scalable and practical techniques for unlearning LLMs through fine-tuning, primarily using gradient ascent.\n\nThe paper also refers to the **TOFU dataset (Maini et al., 2024)** as a benchmark for evaluating unlearning methods, which revealed the inadequacies of gradient ascent techniques in providing satisfactory unlearning performance.\n\n**New Contributions:**\n\n- **Negative Preference Optimization (NPO):** A new unlearning objective function inspired by preference optimization techniques, designed to mitigate the catastrophic collapse issue of gradient ascent.\n- **Theoretical and Empirical Validation:** The efficacy of NPO is demonstrated through theoretical analysis and experimental results, showing it achieves a superior balance between forget quality and model utility.\n- **Benchmark Performance:** The proposed NPO method achieves state-of-the-art performance on the TOFU dataset, successfully handling scenarios that existing methods struggle with, such as forgetting a significant portion of the data.",
        "methodology": "### Methodology: Negative Preference Optimization (NPO)\n\n#### Introduction to NPO\n- **Negative Preference Optimization (NPO)** is proposed as a simple adjustment to the Generalized Adversarial (GA) loss. \n- Unlike GA loss, which can become unstable at finite temperatures, NPO remains lower-bounded and stable at any finite temperature.\n\n#### Derivation and Key Components\n- **Inspiration Source:** NPO is derived from the concept of preference optimization, as discussed by Rafailov et al. (2024).\n- **Preference Optimization Background:** \n  - In standard preference optimization, a dataset with preference feedbacks \\(\\{r_{\\text{win}}, r_{\\text{lose}}\\}\\) is used. These preferences are obtained from human comparisons.\n  - The goal is to fine-tune a model \\(\\pi\\) using these preferences to align it better with human choices.\n  - Direct Preference Optimization (DPO) minimizes a specific loss function, combining responses \\(r_{\\text{win}}\\) and \\(r_{\\text{lose}}\\), centered around a sigmoid activation and a reference model \\(\\pi_0\\).\n\n#### Transition to NPO for Unlearning\n- NPO reinterprets this framework by considering the unlearning problem as a form of preference optimization:\n  - Instead of having both positive and negative responses, unlearning focuses solely on negative responses \\(r_{\\text{neg}}\\).\n  - The standard DPO is modified by removing the \\(\\log \\sigma(f(r_{\\text{pos}}))\\) term, resulting in the NPO loss.\n\n#### NPO Loss Function\n- The NPO loss is defined directly in terms of avoiding negative examples:\n  \\[\n  \\mathcal{L}_{\\text{NPO}}(\\pi) = -\\frac{1}{1 + \\exp(-\\beta (\\pi(r_{\\text{neg}}) - \\pi_0(r_{\\text{neg}})))}.\n  \\]\n- The goal of minimizing this loss is to reduce the prediction probability on the forget set \\(\\mathcal{F}\\), which helps with unlearning.\n\n#### Relationship with GA Loss\n- The NPO can revert to GA loss:\n  - By removing the \\(\\log(1 + \\exp(-u))\\) term introduced in the NPO formula.\n  - NPO converges to GA loss as the inverse temperature \\(\\beta\\) goes to infinity (\\(\\beta \\to \\infty\\)).\n  - Thus, NPO is a strict generalization of GA.\n\n#### Stability and Advantage Over GA\n- **Lower-boundedness:** Unlike the GA loss, which can approach negative infinity, NPO remains lower-bounded at any finite temperature, preventing the issue of catastrophic collapse.\n- **Adaptive Smoothing:** Gradients of NPO incorporate an adaptive smoothing weight, ensuring the divergence rate is controlled:\n  \\[\n  \\mathcal{L}_{\\text{GA}}' = \\int_{\\mathcal{F}} \\frac{\\sigma'(\\cdot)}{1-\\sigma(\\cdot)}; \\quad \n  \\mathcal{L}_{\\text{NPO}}' = \\int_{\\mathcal{F}} \\frac{\\sigma'(\\beta(\\cdot))}{\\sigma(\\beta(\\cdot))(1-\\sigma(\\beta(\\cdot)))}\n  \\]\n- When an example \\(r_{\\text{neg}}\\) is already unlearned (\\(\\pi(r_{\\text{neg}}) \\to \\pi_0(r_{\\text{neg}})\\)), NPO\u2019s gradient becomes negligible, making it more robust.\n\n#### Conclusion\nNPO offers a more stable and bounded alternative to GA loss, addressing the issue of catastrophic collapse in preference optimization and providing an effective mechanism for unlearning.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n- The main experiment involves a forget set and a retain set both generated from Gaussian-logistic models.\n- There are two configurations for the Gaussian distributions: \n  1. Where a gap exists between the Gaussian means of forget covariates \\( \\mu_f \\) and retain covariates \\( \\mu_r \\).\n  2. Where the covariates in the forget and retain sets are isotropic Gaussian.\n- The distributions are adjusted so that forget labels \\( y_f \\) are more likely to be 1, and retain labels \\( y_r \\) are more likely to be 0, ensuring label frequency discrepancy.\n- A random feature model \\( f(x;w) = \\sigma(\\mathbf{A}x) \\) is employed, with \\( \\mathbf{A} \\) fixed during training and unlearning, and entries generated i.i.d from \\( \\mathcal{N}(0,1) \\). The trainable parameter is \\( w \\).\n- The initial model \\( w_0 \\) and retrained model \\( w_r \\) are optimized using cross-entropy loss on the entire dataset \\( S \\) and the retain dataset \\( S_{\\text{retain}} \\) respectively.\n- During unlearning, starting from the initial model \\( w_0 \\), gradient descent is performed on various loss functions for \\( T \\) steps, with learning rates selected via grid search.\n\n**Baselines and Methods:**\n- Vanilla \\( f(x; w_t) \\) (minimizing \\( \\mathcal{L} \\))\n- \\( \\mathcal{L} \\) plus a retain loss term (+\\( \\mathcal{L}_{\\text{retain}} \\)) (minimizing \\( \\mathcal{L} + \\( \\mathcal{L}_{\\text{retain}} \\))\n- Gradient ascent on \\( -\\mathcal{L} \\) (minimizing \\( -\\mathcal{L} \\))\n- Gradient ascent plus a retain loss term (+\\( -\\mathcal{L}_{\\text{retain}} \\)) (minimizing \\( -\\mathcal{L} + \\( \\mathcal{L}_{\\text{retain}} \\))\n- Cross-entropy loss of forget and retain sets with \\( y_f = 1 \\) (minimizing \\( \\mathcal{L}_{\\text{pos}} + \\( \\mathcal{L}_{\\text{retain}} \\))\n- DPO plus a retain loss term (minimizing \\( \\mathcal{L}_{\\text{DPO}} + \\( \\mathcal{L}_{\\text{retain}} \\)) with \\( y_f = \\sigma(x \\cdot v) \\))\n\n**Evaluation Metrics:**\n- **Forget Distance:** \\( D_{\\text{KL}}(P_{w_r}^{(i)} \\| P_{w_T}^{(i)}) \\) - the KL divergence between the retrained model \\( w_r \\) and the unlearned model \\( w_T \\) on the forget set.\n- **Retain Distance:** \\( D_{\\text{KL}}(P_{w_r} \\| P_{w_T}) \\) - the KL divergence between the retrained model \\( w_r \\) and the unlearned model \\( w_T \\) on the retain set.\n\n**Main Experimental Results:**\n- All methods reach a small forget distance (less than \\( 10^{-3} \\)) within \\( T = 5000 \\) steps.\n- The retain distances for some methods diverge as unlearning progresses, indicating catastrophic collapse.\n- The retain distances for other methods slowly increase and stabilize, implying more stability compared to those that diverge.\n- One method outperforms other baselines by achieving a better Pareto frontier.\n- For methods not utilizing the retain set, one performs better than the baseline approach.}"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate various unlearning methods including NPO-based methods and GA-based methods on a synthetic dataset and measure their effectiveness in terms of forget distance and retain distance.",
            "experiment_process": "The experiment uses a synthetic forget set and retain set generated from Gaussian-logistic models with different hyper-parameter settings. A random feature model is trained using cross-entropy loss over the entire dataset and retain dataset. In the unlearning phase, various loss functions are applied and evaluated including vanilla NPO, NPO plus a retain loss term, gradient ascent, gradient ascent plus a retain loss term, cross-entropy loss, and DPO plus a retain loss term. Performance is measured in terms of forget distance (KL divergence between retrained and unlearned model on the forget set) and retain distance (KL divergence between retrained and unlearned model on the retain set).",
            "result_discussion": "All methods except for gradient ascent plus retain term reached a small forget distance within 1000 steps. The retain distance for gradient ascent and gradient ascent plus retain term diverged, suggesting catastrophic collapse, while NPO plus retain term and DPO plus retain term's retain distances increased slowly and stabilized. NPO plus retain term outperformed other methods by achieving a better Pareto frontier.",
            "ablation_id": "2404.05868v1.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of unlearning methods on the TOFU dataset in terms of forget quality and model utility across different levels of forgetting tasks.",
            "experiment_process": "The experiment uses the TOFU dataset containing 200 fictitious author profiles with tasks to forget 1%, 5%, and 10% of the data. Unlearning methods including NPO, NPO with retain loss, GA-based methods, IDK+RT, DPO-based methods, and KTO-based methods are applied and evaluated on Llama-2-7B-chat model. Forget quality is assessed via p-value of the Kolmogorov-Smirnov test; model utility measures general capacities and real-world knowledge.",
            "result_discussion": "NPO-based methods consistently outperformed GA-based methods. NPO+RT notably maintained meaningful forget quality while preserving model utility in Forget10 tasks, unlike the baseline methods. NPO+RT generated fluent answers for retain set questions while providing incorrect answers for forget set questions, enhancing fluency and diversity. Increasing the weight of retain loss improved both forget quality and model utility up to a certain point. NPO+RT achieved meaningful forget quality in tasks up to Forget50 and Forget90, demonstrating its effectiveness in larger-scale unlearning tasks.",
            "ablation_id": "2404.05868v1.No2"
        }
    ]
}