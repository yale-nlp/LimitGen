{
    "title": "Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation",
    "abstract": "Large language models have consistently struggled with complex reasoning tasks, such as mathematical problem-solving. Investigating the internal reasoning mechanisms of these models can help us design better model architectures and training strategies, ultimately enhancing their reasoning capabilities. In this study, we examine the matching mechanism employed by Transformer for multi-step reasoning on a constructed dataset. We investigate factors that influence the model\u2019s matching mechanism and discover that small initialization and post-LayerNorm can facilitate the formation of the matching mechanism, thereby enhancing the model\u2019s reasoning ability. Moreover, we propose a method to improve the model\u2019s reasoning capability by adding orthogonal noise. Finally, we investigate the parallel reasoning mechanism of Transformers and propose a conjecture on the upper bound of the model\u2019s reasoning ability based on this phenomenon. These insights contribute to a deeper understanding of the reasoning processes in large language models and guide designing more effective reasoning architectures and training strategies.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, LLMs have emerged and demonstrated remarkable capabilities across various tasks [1  ###reference_b1###, 2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###]. These models have shown impressive in-context learning abilities [7  ###reference_b7###, 8  ###reference_b8###, 9  ###reference_b9###] and have been applied to logical reasoning problems, such as matching top human contestants at the International Mathematical Olympiad (IMO) level [10  ###reference_b10###] and solving math problems [11  ###reference_b11###].\nHowever, even the most advanced models still struggle with complex reasoning tasks (Fig. 1  ###reference_###), which indicates the ability of LLMs to handle multi-step logical reasoning remains constrained. To truly enhance the reasoning capabilities of LLMs, it is crucial to investigate their intrinsic reasoning mechanisms. In this work, we focus on how LLMs process multi-step reasoning within their architecture, which can help develop more effective strategies for improving their multi-step reasoning abilities.\n###figure_1### Multi-step reasoning refers to the process of directly outputting the desired result by integrating the logical reasoning information flow presented in the given context. For example, if the context contains a structure like \"[A][B]...[B][C]...[A]\", where \"...\" represents other textual content unrelated to logical reasoning, we expect the model to directly output \"[C]\".\nWhen a sentence contains only one logical reasoning step, it is often handled by the so-called induction head in Transformer[7  ###reference_b7###, 12  ###reference_b12###]. However, multi-step reasoning is not merely a linear accumulation of multiple induction heads but involves more complex mechanisms. Our work aims to uncover these mechanisms and provide insights into how Transformer process and integrate logical information across multiple layers to perform multi-step reasoning.\nState-of-the-art models such as GPT-4 usually employ the horizontal thinking strategy, such as Chain-of-Thought (CoT) prompting[13  ###reference_b13###, 14  ###reference_b14###], which calls the model multiple times to generate explicit intermediate reasoning steps. This approach enhances the model\u2019s reasoning capacity by prolonging the thought process horizontally. With the CoT prompt, all of the models can output the correct answer in the example task shown in Fig. 1  ###reference_###. Complementary to the horizontal approach, our investigation focuses on the vertical thinking ability of Transformer models, i.e., the inherent capacity to perform multi-step reasoning within the model architecture itself. We aim to uncover how the model\u2019s reasoning ability scales with its depth, without relying on external prompts or multiple calls. The insights from CoT prompting and our multi-step reasoning analysis offer complementary perspectives on enhancing the reasoning performance of LLMs.\nTo delve into the reasoning mechanisms of Transformer models, we design three types of multi-step reasoning datasets and analyze the model\u2019s internal information flow. Our research reveals that Transformer models mainly achieve multi-step reasoning through matching operations. We propose the concept of a matching matrix to measure the model\u2019s matching ability in each layer and find that even for untrained random embedding vectors, the model can maintain good matching ability, suggesting that Transformer models may have learned the essence of reasoning tasks.\nFurthermore, we explore methods to enhance the model\u2019s matching ability, including initialization, LayerNorm position, and adding orthogonal noise. We discover that small initialization and post-LayerNorm significantly facilitate Transformer in learning reasoning tasks. These findings provide valuable insights for designing more effective reasoning architectures and training strategies.\nLastly, we find that Transformers can perform multiple reasoning steps simultaneously within each layer when the reasoning steps exceed or equal the number of model layers, exhibiting parallel reasoning ability. This discovery inspires us to propose a conjecture on the upper bound of the reasoning ability of Transformers, suggesting that their reasoning capability may grow exponentially with the increase of model depth.\nThe main contributions of this work are as follows:\nWe uncover the matching mechanism employed by Transformer models for multi-step reasoning and propose the concept of a matching matrix to measure the model\u2019s matching ability.\nWe discover that small initialization, post-LayerNorm, and adding orthogonal noise to certain modules of the Transformer can significantly enhance the model\u2019s matching ability.\nWe find that Transformer models can exhibit parallel reasoning capabilities. We provide an understanding of this phenomenon from the perspective of subspace division and propose a conjecture that the reasoning ability of Transformers may grow exponentially with the model depth.\nOur research deepens the understanding of the reasoning mechanisms in Transformer models and provides new perspectives for further enhancing their reasoning capabilities. The insights gained from this study can contribute to the design of more efficient reasoning models and the exploration of reasoning mechanisms in general artificial intelligence systems."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Formulation",
            "text": "Dataset. To understand the mechanism of multi-step reasoning in Transformers, we design three types of multi-step reasoning tasks. As shown in Fig. 2  ###reference_###, different reasoning chains are serialized into a sequence. Every two tokens in the sentence represent a reasoning relation. We use different labeling methods to generate the following three types of datasets:\nType 1: The last token is the starting point, and the label is the reasoning result with a fixed-step reasoning starting from the starting point.\nType 2: The last token is the starting point, and the label is the endpoint of the reasoning chain where the starting point is located.\nType 3: The last two tokens are the starting point and the specified reasoning steps, respectively. The label is the reasoning result with a specified-step reasoning starting from the starting point.\nWe design three chain structures: single chain, double chain, and forest chain. The chain structure is unique in each task. We use the cross-entropy loss function to supervise the learning of the last token in the output sequence, rather than performing the next token prediction for all tokens.\n###figure_2### Train and Test Data. We designed a method to partition the data so that every 1-step reasoning pair in the training set is different from those in the test set. Specifically, for the serialized reasoning chain [x][x][x] of the training set, all tokens satisfy the following condition:\nFor the reasoning chains in the test set, all tokens satisfy:\nThe value of each token ranges from 20 to 100, i.e., . Under this setting, we examine the Transformer\u2019s ability to perform zero-shot in-context learning[7  ###reference_b7###, 12  ###reference_b12###], as each reasoning pair is not seen during in-weight learning.\nModel Architecture. We employ a decoder-only Transformer. Given an input sequence , where  is the sequence length and  is the dictionary size, the model first applies an embedding layer (target embedding and position embedding) to obtain the input representation . The single-head attention in each layer is computed as follows:\nwhere  represents the transpose of . The output of the -th layer is obtained as:\nAfter that, a projection layer is applied to map the output to the target space . The final output is obtained by taking the argmax of the softmax function applied to .\nA detailed description of the model architecture and notation can be found in Appendix A  ###reference_###.\nInformation Flow. The information flow is constructed as follows: the -th point in layer  and the -th point in layer  are connected by a solid line, with the line thickness positively correlated with the value of . The intuitive and clear representation of the information flow facilitates our study of complex mechanisms such as multi-step reasoning."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Mechanism of Multi-Step Reasoning in Transformers",
            "text": "In this section, we delve into the intrinsic mechanism of multi-step reasoning by examining how a 3-layer Transformer handles 2-step Type 1 tasks. The training details can be found in Appendix D  ###reference_###. Fig. 3  ###reference_### provides a visual representation of the trained model\u2019s internal reasoning process when presented with a double-chain structured reasoning sequence.\n###figure_3### Layer 0-1: Information Fusion. The primary function of the first layer is the information infusion of odd-even pairs, which is a consequence of the training set\u2019s data structure, as odd-positioned tokens in the training sequences can infer their subsequent even-positioned tokens. This layer\u2019s implementation mainly relies on positional embeddings. More evidence is discussed in Appendix D  ###reference_###.\nLayer 1-2: Information Matching. After information fusion, the even positions in the first layer possess information from two tokens, which are not simply added together but stored in the form of \"\", where . Consequently, a matching operation occurs in layer 1. Specifically, denoting the start point as [A], its query will have the largest inner product with the key of \"\", thereby transmitting the information of [B] to the last position. Our research reveals that this matching operation does not require the participation of \"[B]\" and the positional encoding of the sequence. Instead, it is achieved solely through the query of \"[A]\", i.e.,  and the key of \"\", i.e., , where  is the embedding of [A] and .\nMatching Matrix for the 1st Matching Layer. Based on the analysis of the matching mechanism in layer 1, we define the following matching matrix to characterize the matching ability of layer 1:\nAs shown in Fig. 4  ###reference_###(a),  satisfies the maximum diagonal property, which indicates that layer1 performs a matching operation. We further discover that for the matching operation, FNN and LayerNorm are unnecessary. Therefore, we can define a simplified matching matrix:\nFig. 9  ###reference_###(c) (Appendix D  ###reference_###) shows the maximum diagonal property still being held in .\n###figure_4### Properties of Matching Matrix. We discover that even for untrained random tokens (the 0-20 and 100-120 portions in Fig. 4  ###reference_###(a)),  can still maintain the maximum diagonal property. These tokens can be sampled from arbitrary distributions (Fig. 4  ###reference_###(c)). As a result, even if we replace the tokens in the sequence with untrained random tokens, the model can still obtain the correct results (Fig. 4  ###reference_###(e)). This phenomenon aligns with findings from studies on induction heads [15  ###reference_b15###]. We observe that this is due to  being approximately an identity matrix (Fig. 4  ###reference_###(d)). This \"simplicity bias\" endows the Transformer model with the ability to perform matching out of the data distribution[16  ###reference_b16###, 17  ###reference_b17###]. Furthermore, we find that for the matching matrix, the trained and untrained portions only differ by a constant factor (Appendix D  ###reference_### Fig. 9  ###reference_###(b)).\nLayer 2-3: Information Matching. The second step of reasoning is matching the information of \"[B]+[C]\" with the information of \"[B]\" in the last node. Therefore, the matching matrix and simplified matching matrix for the 2nd layer can be defined as:\nAs shown in Fig. 4  ###reference_###(b)(c)(d), the second layer also exhibits simplicity bias and the ability to perform matching out of the data distribution.\nGeneral Matching Matrix. More generally, for deeper Transformer models and multi-step reasoning tasks, the matching matrix for the th-step reasoning is:"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Mechanism of Parallel Reasoning and Upper Bound Conjecture",
            "text": "Mechanism of Parallel Reasoning. Fig. 7  ###reference_###(a) illustrates the information flow of a 4-layer model completing 4-step reasoning. We observe that when the reasoning steps exceed or equal the number of model layers, the model performs multiple matching operations parallelly in one layer. In the 2nd layer, different information undergoes two matching operations, enabling the subsequent three layers to achieve 4-step reasoning. Moreover, starting from the 2nd layer, the model\u2019s matching approach transits from matching tokens\u2019 values to matching tokens\u2019 positions. Token value matching is only utilized in the 1st layer.\nBased on the mechanism of information propagation, at the -th layer, if the information is transmitted through attention, it should be multiplied by a coefficient , whereas no such term is required if it is transmitted through residual connections. Consequently, in Fig. 7  ###reference_###(c), we present the coefficients by which each information is multiplied. It is observed that for each positional information, the coefficients are distinct, and the same holds true for the value information. The variation in coefficients also indicates that the information is stored in different subspaces, thereby enabling parallel reasoning.\n###figure_7### Upper Bound of Model\u2019s Reasoning Ability. Through the study of the matching mechanism in Transformer models, we can estimate the upper bound of the reasoning ability for a general L-layer Transformer network. First, we assume that the model satisfies the following assumption.\nAssume that the hidden space dimension  is sufficiently large, allowing different information to be stored in independent subspaces without interference. Moreover, information transmission is achieved solely through the matching operation of the Attention module, without considering the influence of the FNN module.\nWe regard  tokens of a sequence as  nodes, each storing position and value information. The Transformer model follows the Information Propagation (Info-Prop) Rules (denoting the node transmitting information as  and the node receiving information as , considering the existence of the attention mask, we require  < ):\nRule 1: Odd positions can pass information to subsequent even positions, i.e., node  stores an odd number  in its position information, and node  stores  in its position information.\nRule 2: The position information stored in node  and node  has common elements.\nRule 3: The value information stored in node  and node  has common elements.\nIf any of the above three rules are satisfied, we consider that the information from node  can be transmitted to node . When information transmission occurs, the position and value information stored in node  will take the union of the corresponding information from node  and node .\nPseudocode 1  ###reference_### (Appendix J  ###reference_###) provides a detailed process of the Info-Prop Rules. Fig. 7  ###reference_###(b) shows the number of value information stored in the last token with respect to the number of iterations under our set of rules. It can be observed that the stored information exhibits an approximately exponential growth pattern with the number of iterations. Fig. 19  ###reference_9### (Appendix J  ###reference_###) discusses the minimum number of steps required to transmit the final reasoning result to the last token. Based on these results, we propose the following conjecture:\nUnder the premise of Assumption 1  ###reference_umption1###, in the L-th layer, the subspaces of the hidden space for value information and position information can only be divided through . Therefore, the maximum amount of information in the last layer is . Considering the influence of the mask, the upper bound of the model\u2019s reasoning ability is .\nHowever, in practice, the hidden space dimensions , , , and  of large language models are far from meeting the requirements of Assumption 1  ###reference_umption1###. Consequently, the actual reasoning ability of large language models is also constrained by the hidden space dimensions. On the other hand, due to the presence of FNN and other attention mechanisms, the model\u2019s ability to integrate various types of information is further enhanced. Therefore, considering all factors, we believe that the reasoning ability of Transformers lies between linear and exponential growth."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In-Context Learning and Induction Head The concept of in-context learning (ICL) was first introduced by Brown et al. [7  ###reference_b7###]. Since then, a series of studies have utilized induction heads to investigate ICL, yielding remarkable research outcomes [12  ###reference_b12###, 21  ###reference_b21###, 22  ###reference_b22###, 23  ###reference_b23###, 24  ###reference_b24###, 25  ###reference_b25###, 26  ###reference_b26###, 27  ###reference_b27###, 28  ###reference_b28###]. It is worth noting that induction heads can be considered as a special case of multi-step reasoning tasks, where the reasoning step is limited to 1. However, multi-step reasoning is not a simple linear combination of single-step reasoning. In this work, we study the matching mechanism that enables multi-step reasoning and the phenomenon of parallel reasoning, which have not been explored in previous studies.\nInitialization and Layernorm Initialization is a crucial factor influencing model performance. Different initialization settings can lead to various network states, such as linear behavior under large initialization[29  ###reference_b29###, 30  ###reference_b30###, 31  ###reference_b31###, 32  ###reference_b32###, 33  ###reference_b33###] and condensation phenomenon under small initialization [34  ###reference_b34###, 35  ###reference_b35###]. Researchers have also explored the use of different initialization techniques to improve the stability of large models [36  ###reference_b36###, 37  ###reference_b37###, 38  ###reference_b38###]. Zhang et al. [39  ###reference_b39###] suggests that large initialization tends to memorize, while small initialization tends to learn inference. LayerNorm [40  ###reference_b40###] is a widely used normalization technique in Transformer models, which has been shown to improve the stability and performance of deep neural networks. Studies have investigated the effect of LayerNorm on the training dynamics, generalization, and gradient flow in Transformers [41  ###reference_b41###, 42  ###reference_b42###]. The use of LayerNorm in conjunction with different initialization techniques has also been explored to enhance the training stability of Transformers [38  ###reference_b38###].\nAttention Mechanism Our work builds upon previous studies on the attention mechanism [43  ###reference_b43###, 44  ###reference_b44###, 45  ###reference_b45###, 46  ###reference_b46###]. Numerous researchers have proposed various approaches to identify the roles of different heads in Transformers [47  ###reference_b47###, 48  ###reference_b48###, 21  ###reference_b21###, 49  ###reference_b49###, 50  ###reference_b50###]. These methods predominantly employ the concept of perturbation. As a complementary approach, we propose a direct computation method to identify heads with matching functionality. Similar to the observations made by Wang et al. [51  ###reference_b51###] and Dutta et al. [52  ###reference_b52###], who noted that large language models typically perform information aggregation in shallow layers and information induction in deeper layers, we have also observed comparable phenomena in our study. Our idea of constructing language-like datasets is inspired by Poli et al. [53  ###reference_b53###], Zhang et al. [54  ###reference_b54###]."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Conclusion. In this work, by designing rigorous datasets and analyzing the model\u2019s internal information flow, we uncovered the crucial role of the matching operation in enabling multi-step reasoning. We proposed the concept of a matching matrix to quantify the model\u2019s matching ability and explored factors that influence this ability. Furthermore, we investigated the relationship between model depth and reasoning capability, revealing the potential for parallel reasoning in Transformers and proposing a conjecture on the upper bound of their reasoning ability.\nLimitations and Future Work. Our analysis focused on Transformer models with a single attention head, which allowed us to simplify the problem and gain a clear understanding of the matching operation. However, most Transformer models employ multi-head attention mechanisms, and the dynamics of multi-step reasoning in these networks may involve more complex interactions between different attention heads. Further research is needed to extend our analysis to multi-head attention networks and explore the theoretical implications of our conjecture on the upper bound of the reasoning ability of Transformers."
        }
    ],
    "url": "http://arxiv.org/html/2405.15302v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6"
        ],
        "methodology_sections": [
            "2",
            "3",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "2",
            "3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2405.15302v1",
        "paper_title": "Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation",
        "research_background": "### Paper's Motivation:\nThe motivation for this paper stems from the need to enhance the multi-step reasoning capabilities of large language models (LLMs). Despite recent advancements and the notable success of models like GPT-4 in tasks such as in-context learning and logical reasoning, even the most sophisticated LLMs still exhibit limitations when dealing with complex multi-step reasoning tasks. The authors aim to investigate the intrinsic reasoning mechanisms within Transformer architectures, as understanding these mechanisms is crucial for developing strategies to improve the multi-step reasoning abilities of such models.\n\n### Research Problem:\nThe central research problem addressed in this paper is understanding how Transformer models perform multi-step reasoning operations. Specifically, the study focuses on discovering the mechanisms through which Transformers integrate logical reasoning information across multiple layers, particularly when the task requires more than a single logical step. This involves identifying whether Transformers can inherently handle multi-step reasoning without relying on external prompting strategies, such as Chain-of-Thought (CoT) prompting, and how their reasoning abilities scale with depth.\n\n### Relevant Prior Work:\n1. **LLM Capabilities and In-Context Learning**:\n   The impressive capabilities of large language models in various tasks have been well-documented [1, 2, 3, 4, 5, 6], including their in-context learning abilities [7, 8, 9].\n\n2. **Logical Reasoning Tasks**:\n   Advanced models have matched top human contestants at the International Mathematical Olympiad (IMO) and solved complex math problems [10, 11]. Although these achievements are commendable, models still struggle with multi-step reasoning tasks.\n\n3. **Induction Heads in Transformers**:\n   Previous research has identified induction heads in Transformers as being responsible for handling single-step logical reasoning [7, 12], but multi-step reasoning is not merely a linear extension of this.\n\n4. **Horizontal Thinking Strategies**:\n   Techniques such as Chain-of-Thought (CoT) prompting [13, 14] are used to enhance reasoning by horizontally extending the thought process with multiple model calls, demonstrating that models can reach correct answers with this approach.\n\nThe authors build on this prior work by examining the vertical thinking capabilities of Transformer models. They investigate how multi-step reasoning can be performed intrinsically within the model architecture, without resorting to external prompts or iterative model calls, and propose new methods for enhancing these capabilities based on their findings.",
        "methodology": "### Methodology: Understanding Multi-Step Reasoning in Transformers\n\n**Dataset Design**\n\nTo explore how Transformers perform multi-step reasoning, we devised three distinct types of reasoning tasks, described as follows:\n\n1. **Type 1**: The sequence ends with a 'starting point' token, with the label being the result of fixed-step reasoning starting from this point.\n2. **Type 2**: Similar to Type 1, but the label is the 'endpoint' of the reasoning chain that originates from the starting point.\n3. **Type 3**: The last tokens indicate both the starting point and the number of reasoning steps. The label is the reasoning result after the specified number of steps from the starting point.\n\nThree unique chain structures were employed for the tasks:\n- **Single Chain**\n- **Double Chain**\n- **Forest Chain**\n\nFor supervision, we used a cross-entropy loss function focused on the final token in the output sequence rather than predicting the next token for the entire sequence.\n\n**Data Partitioning**\n\nThe training and test data are partitioned such that all 1-step reasoning pairs in the training set are unique compared to those in the test set. Specifically, for a serialized reasoning chain [x][x][x], tokens in the training set differ from those in the test set within a value range of 20 to 100. This setting helps evaluate the Transformer's zero-shot in-context learning capabilities, as each reasoning pair is not experienced during prior weight learning.\n\n**Model Architecture**\n\nWe utilized a **decoder-only Transformer** for our experiments. Here is how the model processes the input:\n\n- Input sequence \\( X \\) of length \\( L \\) and dictionary size \\( D \\) is processed through embedding layers (target embedding and position embedding) to obtain the input representation.\n- **Single-Head Attention** in each layer is computed using:\n\n  \\[\n  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\n  \\]\n\n  where \\( Q \\), \\( K \\), and \\( V \\) are projections of the input representation, and \\( K^T \\) represents the transpose of \\( K \\).\n  \n  The output of the \\( i \\)-th layer is:\n\n  \\[\n  H^{(i)} = \\text{LayerNorm}(H^{(i-1)} + \\text{Attention}(Q, K, V))\n  \\]\n\n  followed by an additional feed-forward transformation and normalization.\n  \n- After obtaining the layer outputs, a projection layer maps the output to the target space. The final classification is conducted using the **softmax** function, taking the **argmax** to determine the most likely token.\n\n**Information Flow**\n\nWe visualized the information flow within the model layer-by-layer. Specifically, the connection between point \\( i \\) in layer \\( l \\) and point \\( j \\) in layer \\( l+1 \\), depicted by solid lines, with line thickness denoting the value of attention weights. This clear and intuitive representation aids the study of complex mechanisms such as multi-step reasoning.\n\nA more detailed description, including model architecture and notation, is available in Appendix A.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### **Dataset:**\nTo investigate the mechanism of multi-step reasoning in Transformers, we design three types of multi-step reasoning tasks. Different reasoning chains are serialized into sequences with each pair of tokens representing a reasoning relation. The three types of datasets are:\n\n**Type 1:**\n- The last token is the starting point, and the label is the reasoning result from a fixed-step reasoning starting from this point.\n\n**Type 2:**\n- The last token is the starting point, and the label is the endpoint of the reasoning chain where the starting point is located.\n\n**Type 3:**\n- The last two tokens are the starting point and the specified reasoning steps, respectively. The label is the reasoning result with the specified number of reasoning steps starting from the starting point.\n\nThe datasets feature three distinct chain structures: single chain, double chain, and forest chain, each unique to one of the tasks.\n\n#### **Train and Test Data:**\n- The training and test data are partitioned so that every 1-step reasoning pair in the training set is different from those in the test set, enforcing a zero-shot in-context learning scenario. Specifically, for serialized reasoning chains [x][x][x] in the training set, all tokens satisfy certain conditions to ensure uniqueness.\n- The value of each token ranges from 20 to 100.\n\n#### **Model Architecture:**\nA decoder-only Transformer model is employed, with the following components:\n- **Input Sequence (S):** Length \\(L\\), dictionary size \\(d\\).\n- **Embedding Layer:** Combines target and position embeddings for input representation.\n- **Attention Mechanism:** Single-head attention computed as:\n  \\[\n  \\text{Attention}(Q, K, V) = \\text{softmax}(QK^T / \\sqrt{d_k}) V\n  \\]\n- **Output:** Projection to target space followed by a softmax function, with the final result obtained via argmax.\n\n#### **Evaluation Metrics:**\n- The primary evaluation metric involves examining the model's zero-shot in-context learning capabilities by its performance on unseen reasoning pairs in the test set.\n- A cross-entropy loss function is utilized to supervise the learning of the final token in the output sequence.\n\n### Main Experimental Results:\n\nThe results demonstrate the ability of the Transformer model to perform multi-step reasoning effectively even in zero-shot contexts, where the specific reasoning pairs had not been seen during training. The experimental outcomes indicate:\n\n- **Type 1 Reasoning Task:** The model achieves a high accuracy in determining the reasoning result from a fixed-step starting point.\n- **Type 2 Reasoning Task:** The model successfully identifies the endpoint of the reasoning chain given the starting point.\n- **Type 3 Reasoning Task:** The model effectively computes the reasoning result given both a starting point and a specified number of reasoning steps.\n\nThe findings highlight the Transformer\u2019s proficiency in executing complex multi-step reasoning tasks by leveraging its self-attention mechanism and in-context learning abilities. The clear information flow representation further elucidates how the model processes these reasoning chains across different layers. \n\nOverall, these results underscore the model's robustness in multi-step reasoning tasks, demonstrating the potential for Transformers to handle sophisticated reasoning scenarios in a zero-shot learning setup."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate how different initialization methods and LayerNorm positions affect the Transformer model's multi-step reasoning capabilities.",
            "experiment_process": "Three different initialization methods were tested: small initialization, default initialization, and large initialization, with parameters set from a uniform distribution scaled by factors of small (, ), default (, ), and large (, ). For LayerNorm position, two variations were considered: post-LayerNorm, where LayerNorm is placed after attention and FNN modules, and pre-LayerNorm, where it is placed before these modules. The performance of these configurations was evaluated using matching scores derived from a training sequence of Type 1 data. Additionally, real-world language model Phi-3 was tested for reasoning on Type 3 data. Metrics like diagonal elements of matching matrices were optimized by adding orthogonal noise, with varied replacements of matrices to maximize induction capability.",
            "result_discussion": "Small initialization resulted in the best generalization on test datasets for reasoning tasks, and did not exhibit the Grokking phenomenon shown by default initialization. Post-LayerNorm significantly improved reasoning capability compared to pre-LayerNorm. High matching scores were indicative of strong generalization ability in models with small initialization and post-LayerNorm settings. Experiments with Phi-3 revealed a high correlation between reasoning performance and matching score per layer. Adding orthogonal noise by replacing certain matrices optimized the model's induction capability, enhancing performance on Type 1 data.",
            "ablation_id": "2405.15302v1.No1"
        },
        {
            "research_objective": "To enhance Transformer model\u2019s matching ability by adding orthogonal noise.",
            "experiment_process": "Orthogonal noise was added to the matching matrices to make them closer to identity matrices. This was done by replacing matrices  and  with  and  respectively, where  and  are learnable parameters and  is a random matrix following the normal distribution . The performance of the improved model on Type 1 data was visualized and analyzed.",
            "result_discussion": "Replacing matrices with orthogonal noise improved the induction capability of the model. This approach avoided degeneration problems and maximized the diagonal elements of the matching matrices, enhancing the model's performance on reasoning tasks as evidenced by improved results on Type 1 data.",
            "ablation_id": "2405.15302v1.No2"
        }
    ]
}