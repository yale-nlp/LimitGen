{
    "title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level",
    "abstract": "Scientific document summarization has been a challenging task due to the long structure of the input text. The long input hinders the simultaneous effective modeling of both global high-order relations between sentences and local intra-sentence relations which is the most critical step in extractive summarization. However, existing methods mostly focus on one type of relation, neglecting the simultaneous effective modeling of both relations, which can lead to insufficient learning of semantic representations. In this paper, we propose HAESum, a novel approach utilizing graph neural networks to locally and globally model documents based on their hierarchical discourse structure. First, intra-sentence relations are learned using a local heterogeneous graph. Subsequently, a novel hypergraph self-attention layer is introduced to further enhance the characterization of high-order inter-sentence relations. We validate our approach on two benchmark datasets, and the experimental results demonstrate the effectiveness of HAESum and the importance of considering hierarchical structures in modeling long scientific documents111Our code will be available at https://github.com/MoLICHENXI/HAESum.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Extractive summarization aims to select a set of sentences from the input document that best represents the information of the whole document. With the advancement of pre-trained models and neural networks over the years, researchers have achieved promising results in news summarization (Liu and Lapata, 2019  ###reference_b17###; Zhong et al., 2020  ###reference_b40###). However, when applying these methods to long scientific documents, they encounter challenges due to the relatively lengthy inputs.\n###figure_1### The considerable length of the text hinders sequential models from capturing both long-range dependencies across sentences and intra-sentence relations simultaneously (Wang et al., 2020  ###reference_b29###).Moreover, the extended context exceeds the input limits of the Transformer-based model (Vaswani et al., 2017  ###reference_b27###) due to the quadratic computational complexity of self-attention.\nRecently, the application of large language models (LLM) such as ChatGPT to text summarization tasks has gained significant interest and attracted widespread attention. A recent study by (Zhang et al., 2023b  ###reference_b37###) evaluated the performance of ChatGPT on extractive summarization and further enhanced its performance through in-context learning and chain-of-thought. Another study (Ravaut et al., 2023  ###reference_b24###) conducted experiments on abstractive summarization using various LLMs on a variety of datasets that included long inputs. While the use of LLMs in text summarization tasks has demonstrated exciting potential, there are still several limitations that have not been addressed. The most important of these is the phenomenon of lost-in-the-middle (Liu et al., 2023  ###reference_b16###; Ravaut et al., 2023  ###reference_b24###), where LLMs ignore information in the middle and pay more attention to the context at the beginning and end. This bias raises concerns especially in summarization tasks where important text may be scattered throughout the document (Wu et al., 2023  ###reference_b30###). Additionally, as the input length increases, even on explicitly long-context models, the model\u2019s performance gradually declines (Liu et al., 2023  ###reference_b16###).\nAs a result, researchers have turned to graph neural networks to model long-distance relations. They represent a document as a graph and update node representations in the graph using message passing. These works use different methods to construct a graph from documents, such as using sentence similarity as edge weights to model cross-sentence relations (Zheng and Lapata, 2019  ###reference_b39###). Another popular approach is to construct a word-document heterogeneous graph (Wang et al., 2020  ###reference_b29###), using words as intermediate connecting sentences. Phan et al. (2022  ###reference_b22###) further added passage nodes to the heterogeneous graph to enhance the semantic information. Zhang et al. (2022  ###reference_b35###) proposed a hypergraph transformer to capture high-order cross-sentence relations.\nDespite the impressive success of these approaches, we observe that the current work still lacks a comprehensive consideration on relational modeling. More specifically, two limitations are mentioned: (1) Most of the existing approaches focus on modeling intra-sentence relations but often overlook cross-sentence high-order relations. Inter-sentence connections may not only be pairwise but could also involve triplets or higher-order relations (Ding et al., 2020  ###reference_b5###). In the hierarchical discourse structure of scientific documents, sentences within the same section often express the same main idea. It is difficult to fully understand the content of a document by merely considering intra-sentence and cross-sentence relations in pairwise. (2) These approaches rely on updating relations at different levels simultaneously but ignore the hierarchical structure of scientific documents. Sentences are composed of words and, in turn, contribute to forming sections. By understanding the meaning of individual tokens, we get the meaning of the sentence and thus the content of the section. Therefore, bottom-to-top structured modeling is crucial to understand the content of the document.\nTo address the above challenges, we propose HAESum (Hierarchical Attention Graph for Extractive Document Summarization), a method that leverages a graph neural network model to fully explore hierarchical structural information in scientific documents. HAESum first constructs a local heterogeneous graph of word-sentence and updates sentence representations at the intra-sentence level. The local sentence representations are then fed into a novel hypergraph self-attention layer to further update and learn the cross-sentence sentence representations through a self-attention mechanism that fully captures the relations between nodes and edges. Figure 1  ###reference_### is an illustration showing the modeling of local and global context information from a hierarchical point of view, and the resulting representations contain both local and global hierarchical information. We validate HAESum with extensive experiments on two benchmark datasets and the experimental results demonstrate the effectiveness of our proposed method. In particular, we highlight our main contributions as follows:\n(i) We introduce a novel graph-based model utilizing the hierarchical structure of scientific documents for modeling. In contrast to simultaneously updating nodes in the graph, we learn intra-sentence and inter-sentence relations separately from both local and global perspectives. To the best of our knowledge, we are the first approach to hierarchical modeling using different graphs on this task.\n(ii) We propose a novel hypergraph self-attention layer that utilizes the self-attention mechanism to further aggregate high-order sentence representations. Moreover, our approach does not rely on pre-trained models as encoders, making it easily applicable to other low-resource languages.\n(iii) We validate our model on two benchmark datasets, and the experimental results demonstrate the effectiveness of our approach against strong baselines."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "###figure_2###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Scientific Paper Summarization",
            "text": "Scientific document summarization has been a hot topic due to the challenges of modeling long texts (Frermann and Klementiev, 2019  ###reference_b9###). Cohan et al. (2018  ###reference_b1###) introduced two benchmark datasets for long documents, Arxiv and PubMed, and employed a hierarchical encoder and discourse-aware decoder for the document summarization task. Cui and Hu (2021  ###reference_b2###) proposed a sliding selector network accompanied by dynamic memory to alleviate information loss between context segments. Gu et al. (2021  ###reference_b11###) presented a reinforcement learning-based method that achieved impressive performance by considering the extraction history at each time step. Recently, Ruan et al. (2022  ###reference_b25###) proposed a method to inject explicit hierarchical structural information such as section titles and sentence positions into a pre-trained model to further improve the performance and interpretability."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Graph based Summarization",
            "text": "Graph neural networks have been widely used for extractive summarization due to their flexibility and scalability. Dong et al. (2020  ###reference_b7###) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences. Cui et al. (2020  ###reference_b3###) injected latent topic information into graph neural networks to further improve performance. Wang et al. (2020  ###reference_b29###) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences. Zhang et al. (2022  ###reference_b35###) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization. Our paper follows this line of work, but the main difference is that our approach combines both intra-sentence relations and high-order cross-sentence relations and efficiently leverages the hierarchical discourse structure of scientific documents to learn sentence representations that incorporate both local and global information."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Given an arbitrary document  consisting of  sentences, each sentence consists of  words . The goal of extractive summarization is to predict labels  for all sentences, where  indicates that the current sentence should be included in the summary. The overall structure of HAESum is shown in Figure 2  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Local-level Heterogeneous Graph",
            "text": "As the lowest level of the hierarchical structure, in this section, we will first introduce how to capture local intra-sentence relations between sentences and their corresponding words using a heterogeneous graph. We will start by explaining how to construct the heterogeneous graph and initialize it, followed by detailing how to use a heterogeneous self-attention layer to update node representations. Finally, we will feed the updated sentence node representations into the next module."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Graph Construction",
            "text": "Given an input document , we first construct a heterogeneous graph , where  represents a set of nodes and  represents edges between nodes. In order to utilize the natural hierarchy between words and sentences of a document, the nodes can be defined as , where  denotes  different words in the document, and  denotes the  sentences in the document. The edges are defined as , where  is a real-valued edge weight that denotes the cross-connection between a sentence node  and a word node  contained by it."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Graph Initializers",
            "text": "Let ,  denote the feature matrices of the input word and sentence respectively.  and  correspond to the feature dimensions of words and sentences, respectively. We first use Glove (Pennington et al., 2014  ###reference_b21###) to initialize word representations. Instead of using pre-trained model as a sentence encoder, we first use CNN (LeCun et al., 1998  ###reference_b13###) with different kernel sizes to get the n-gram feature  of the sentence followed by using BiLSTM (Hochreiter and Schmidhuber, 1997  ###reference_b12###) to obtain the sentence-level feature .The features obtained from CNN and BiLSTM are concatenated as initialized sentence representations ."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Heterogeneous Attention Modules",
            "text": "Following the previous work (Wang et al., 2020  ###reference_b29###), we employ the heterogeneous graph attention layer for node representations updating. Specifically, when a node  aggregates information from its neighbours, the attention coefficient  for node  is computed as follows:\nwhere , ,  are trainable weights.  denotes concatenation. We also inject the edge features  into the attention mechanism for computation.\nWe also add multi-head attention and Feed-Forward layer (FFN) (Vaswani et al., 2017  ###reference_b27###) to further improve the performance. The final representation  of node  is then obtained as follows:\nWe begin by aggregating the sentence nodes around the word to update word representations. Subsequently, we utilize the updated word representations to further update the sentence representations.\nIn this section, we use the local heterogeneous graph to learn the intra-sentence relations at the lowest level of the document hierarchy."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Global-level Hypergraph",
            "text": "In this section, we first introduce how to construct a hypergraph. Subsequently, we present a novel hypergraph self-attention layer designed to fully capture high-order global inter-sentence relations. Finally, the resulting sentence representations are used to decide whether to include them in the summary."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Hypergraph Construction",
            "text": "A hypergraph is defined as , where  represents a set of nodes and  represents hyperedges in the graph. Unlike edges in regular graphs, hyperedges can connect two or more nodes and thus represent multivariate relations. A hypergraph is typically represented by its incidence matrix  :\nwhere ,  and if the hyperedge  connects node  there is .\nWe denote a sentence  in a document  as a node  in the hypergraph. In order to capture global higher-order inter-sentence relations, we consider creating section hyperedges for each part (Suppe, 1998  ###reference_b26###). A hyperedge  will be created if a set of child nodes  belongs to the same section in the document. The node representations in the hypergraph are initialized to the output of the previous module.\nThe initialized node features  and incidence matrix  will be fed into the hypergraph self-attention network to learn effective sentence representations."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Hypergraph Self-Attention Modules",
            "text": "Hypergraph attention networks (HGAT) are designed to learn node representations using a mutual attention mechanism. This mutual attention mechanism divides the computational process into two steps, i.e., node aggregation and hyperedge aggregation. First the hyperedge representations are updated with node information. Subsequently, the hyperedge information is fused back to the nodes from hyperedges.\nThe HGAT has mainly been implemented based on graph attention mechanism (Veli\u010dkovi\u0107 et al., 2017  ###reference_b28###), such as HyperGAT (Ding et al., 2020  ###reference_b5###). However, this attention mechanism employs the same weight matrix for different types of nodes and hyperedges information and could not fully exploit the relations between nodes and hyperedges, which prevents the model from capturing higher-order cross-sentence relations (Fan et al., 2021  ###reference_b8###).\nTo address the limitations of HGAT, we propose the hypergraph self-attention layer. Inspired by the success of Transformer (Vaswani et al., 2017  ###reference_b27###) in textual representation and graph learning (Ying et al., 2021  ###reference_b32###), we use the self-attention mechanism to fully explore the relations between nodes and hyperedges. The entire structure we propose is described below.\nNode-level Attention \u2009 To solve the problem of initializing the hyperedge features, we first encode hyperedge representations from node aggregation information using node-level attention. Given node features  and incidence matrix, hyperedge representations  can be computed as follows:\nwhere the superscript  denotes the model layer. , ,  are trainable parameters.  is the attention coefficient of node  in the hyperedge . Through the node-level attention mechanism, we initialize the hyperedge representation.\nEdge-level Attention \u2009 As an inverse procedure, the self-attention mechanism is applied to compute the importance scores to highlight the hyperedges that are more critical for the next layer of node representation . Given the node feature matrix  and the hyperedge feature matrix , similar to the self-attention mechanism we compute the output matrix as follows:\nwhere ,, are trainable parameters.  is the feature dimension of the hidden layer.\n represents the self-attention mechanism.\nAfter obtaining the enhanced node representations  using the hypergraph self-attention layer, we applied a feature fusion layer to generate the final representations , which can be represented by the formula:\ndenotes concatenation. Fusing hyperedge information and node information, we obtain a semantic representation of sentence nodes."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Opimization",
            "text": "After passing L hypergraph self-attention layers, we obtain the representations of sentences . We then add a multi-layer perceptron (MLP) followed by a LayerNorm layer and obtain a score , indicating whether it will be selected as a summary. Formally, the prediction score for a sentence node  is computed as follows:\nwhere , are trainable parameters.\nFinally, the output sentence scores  are optimized with the true labels  by binary cross-entropy loss:\nwhere  denotes the number of sentences in the document."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiment setup",
            "text": "We validate our proposed model on two scientific document datasets and compare it to the strong baselines. In the following, we start with the details of the datasets.\nDatasets \u2009 We perform extensive experiments on two benchmark datasets: Arxiv and PubMed (Cohan et al., 2018  ###reference_b1###). Arxiv is a long document dataset containing different scientific domains. PubMed contains articles in the biomedical domain. We use the original train, validation, and testing splits as in (Cohan et al., 2018  ###reference_b1###). Detailed statistics for the two benchmark datasets are shown in Table 1  ###reference_###.\nCompared Baselines \u2009 We make a systematic comparison with recent approaches in this area. We categorize these methods into the following four types:\nUnsupervised methods: graph-based models PacSum (Zheng and Lapata, 2019  ###reference_b39###), HIPORANK (Dong et al., 2020  ###reference_b7###), FAR (Liang et al., 2021  ###reference_b14###).\nNeural extractive model: Seq2Seq-based models HiStruct+ (Ruan et al., 2022  ###reference_b25###); local and global context model ExtSum-LG (Xiao and Carenini, 2019  ###reference_b31###); graph-based models Topic-GraphSum (Cui et al., 2020  ###reference_b3###), SSN-DM (Cui and Hu, 2021  ###reference_b2###), HEGEL (Zhang et al., 2022  ###reference_b35###), MTGNN (Doan et al., 2022  ###reference_b6###), CHANGES (Zhang et al., 2023a  ###reference_b36###).\nNeural abstractive model: encoder-decoder based Model TLM-I+E (Pilault et al., 2020  ###reference_b23###), PEGASUS (Zhang et al., 2020  ###reference_b38###) , BigBird (Zaheer et al., 2020  ###reference_b33###), divide-and-conquer approach Dancer (Gidiotis and Tsoumakas, 2020  ###reference_b10###).\nLarge language model: ChatGLM3-6k-32k (Zeng et al., 2022  ###reference_b34###).\nMore details on the evaluation of the large language model can be found in Appendix A.1  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "Regarding the encoding of word nodes, the vocabulary size is 50000 and the word embedding is initialized with a dimension of 300 using the Glove pre-trained model(Pennington et al., 2014  ###reference_b21###). The feature dimensions of sentence nodes and edges in the heterogeneous graph are set to 64 and 50, respectively. The hyperedge feature dimension is 64. We set the maximum sentence length of each document to 200 and the maximum number of words per sentence to 100. In our experiments, we stacked two layers of heterogeneous graph attention modules (HEGAT) and hypergraph self-attention modules (HSAGT). The multi-head of the HEGAT layer is set to 8 and 6, respectively.\nThe model is optimized using the Adam optimizer (Loshchilov and Hutter, 2017  ###reference_b18###) with a learning rate of 0.0001 and a dropout rate of 0.1. We train the model on an RTX A6000 GPU with 48GB of memory for 12 epochs. The training process stops if the validation set loss does not decrease three times. The training time for one epoch on the PubMed dataset is 3 hours, while on the Arxiv dataset, it is 6 hours.\nWe use a greedy search algorithm similar to (Nallapati et al., 2017  ###reference_b20###) to select sentences from documents as the gold extractive summaries (Oracle). Following previous work, we use ROUGE (Lin and Hovy, 2003  ###reference_b15###) to evaluate the quality of summaries. We use ROUGE-1/2 to measure summary informativeness and ROUGE-L to measure the fluency of the summary."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Experiment Results",
            "text": "Table 2  ###reference_### shows the comparison between our model HAESum and the baseline model on PubMed and Arxiv datasets. The first block covers the ground truth ORACLE and unsupervised methods for extractive summarization. The second block covers state-of-the-art supervised extractive baselines. The third block reports abstractive methods.\nBased on the results, we find that HIPORANK (Dong et al., 2020  ###reference_b7###) achieves strong performance on graph-based unsupervised modeling. Compared to other unsupervised methods, HIPORANK adds section information, which demonstrates the effectiveness and importance of taking the natural hierarchical structure of scientific documents into account when modeling cross-sentence relations.\nIn the extractive baseline, MTGNN (Doan et al., 2022  ###reference_b6###) achieves state-of-art performance, MTGNN considers more intra-sentence level modeling, which shows the necessity of modeling from low-level structure. HEGEL (Zhang et al., 2022  ###reference_b35###) is the most similar approach to ours. HEGEL injects external information such as keywords and topics into the model and models higher-order cross-sentence relations through a hypergraph transformer to achieve a competitive performance. However, compared to MTGNN, HEGEL does not consider low-level intra-sentence relations, which proves the necessity of considering and modeling hierarchical structure. Interestingly, CHANGES (Zhang et al., 2023a  ###reference_b36###) achieves equally impressive results in hierarchical modeling by considering high-level intra-section and inter-section relations, further confirming the importance of hierarchical modeling. Among the extractive methods, the transformer-based HiStruct+ (Ruan et al., 2022  ###reference_b25###) shows a competitive performance, which demonstrates the effectiveness of the self-attention mechanism. HiStruct+ also incorporates the inherent hierarchical structure into the pre-trained language models to achieve strong performance. In addition, the extractive approaches largely outperform the abstractive approaches, which may be due to the fact that long input is more challenging for the decoding process of the abstractive models.\nThrough the table, the results of using the large language model are not satisfactory compared to our proposed method. By analyzing the output of the large language model, the model sometimes incorrectly outputs content from other languages and also occasionally outputs duplicate content. In addition, the model sometimes misinterprets extractive summarization as abstractive summarization.\nAccording to the experimental results, our model HAESum outperforms all extractive and abstractive strong baselines. In particular, our model neither requires injection of external knowledge (e.g., topics and keywords (Zhang et al., 2022  ###reference_b35###)) to enhance global information nor pre-trained model\u2019s (e.g., BERT (Devlin et al., 2018  ###reference_b4###)) knowledge (Doan et al., 2022  ###reference_b6###). The outstanding performance of HAESum demonstrates the importance of hierarchical modeling of local intra-sentence relations and global inter-sentence relations.\n###figure_3###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Ablation Study",
            "text": "We first analyze the effect of different components of HAESum in Table 3  ###reference_###. The second row shows that removing the heterogeneous graph part represents not learning intra-sentence relations. The third row removes the hypergraph component, representing the absence of learning higher-order cross-sentence relations. As shown in table 3  ###reference_###, removing either part hurts the model performance, which indicates that learning both local intra-sentence relations and global higher-order cross-sentence relations is necessary for scientific document summarization.\nInterestingly, these two components are almost equally important for modeling long documents. This indicates the importance of simultaneously modeling semantic aspects from diverse perspectives and hierarchical discourse structures in scientific documents."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Performance Analysis",
            "text": "Hierarchical discourse \u2009 We also analyze different update approaches for obtaining the final sentence representations in HAESum. As shown in Table 4  ###reference_###, the second row represents our hierarchical updating. The third row represents parallel updating, where intra-sentence and inter-sentence relations are updated simultaneously, and the final sentence representations are concatenated. The superior performance of hierarchical updating over parallel updating once again emphasizes the critical importance of the bottom-to-top modeling sequence we propose for understanding the content of long documents.\nAttention mechanism \u2009 We then analyze the performance of our proposed novel hypergraph self-attention layer and hypergraph attention network (HGAT). As shown in Table 5  ###reference_###, our hypergraph self-attention layer outperforms HGAT (Ding et al., 2020  ###reference_b5###). We speculate that the main reason is the utilization of the self-attention mechanism and different weight matrices, which fully exploit relations between nodes and edges, thereby enhancing the learning of high-order relations.\nHyperparameter sensitivity \u2009 In our experiments, we set the maximum input length for each sentence to be 100, and the maximum sentence length for each input document to be 200. We conduct an analysis of these two hyperparameters. In addition, more information about the distribution of the sentence lengths and the number of sentences in the document is presented in the Appendix A.3  ###reference_###. As shown in Figure 3  ###reference_###, when the maximum number of tokens in each sentence is reduced from 100 to 60, the performance does not significantly decrease. This indicates that under this range of hyperparameter settings, the model has already processed most of the tokens in each sentence. However, as the length continues to decrease, the model\u2019s performance starts to decline, as the input length limits the capture of local intra-sentence relations.\nSimultaneously, when the maximum number of sentences in a document is increased from 50 to 200, the model\u2019s performance continues to improve. This improvement is attributed to the consideration of more sentences, capturing more complex higher-order cross-sentence relations. However, persistently increasing this hyperparameter leads to significant computational consumption. Specifically, in future work, we intend to increase the maximum input sentences per document while minimizing computational consumption as much as possible."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents HAESum for scientific document summarization. HAESum employs a graph-based model to comprehensively learn local intra-sentence and high-order inter-sentence relations, utilizing the hierarchical discourse structure of scientific documents for modeling. The impressive performance of HAESum demonstrates the importance of simultaneously considering multiple perspectives of semantics and hierarchical structural information in modeling scientific documents."
        }
    ],
    "url": "http://arxiv.org/html/2405.10202v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.10202v1",
        "paper_title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level",
        "research_background": "### Motivation\nThe paper is motivated by the limitations of existing methods in extractive summarization, especially when applied to lengthy scientific documents. Although pre-trained models and neural networks have shown promising results in news summarization, these methods face significant challenges with longer texts due to limits in capturing long-range dependencies and the quadratic computational complexity of self-attention mechanisms. The recent application of large language models (LLMs) like ChatGPT has shown potential but remains imperfect, particularly with issues such as the \"lost-in-the-middle\" phenomenon and declining performance with increasing input length. These challenges necessitate the need for more effective approaches to handle long documents and complex dependencies.\n\n### Research Problem\nThe research problem is to develop a model for extractive summarization of scientific documents that can effectively capture both local (intra-sentence) and global (cross-sentence) hierarchical structures. Existing graph-based methods and LLMs have limitations, such as overlooking high-order relations beyond pairwise connections and ignoring the hierarchical structure of documents. Therefore, the problem is to design a method that comprehensively models the relational and hierarchical structure of scientific texts for better summarization.\n\n### Relevant Prior Work\n1. **Pre-trained Models and Neural Networks:** Previous works like those of Liu and Lapata (2019) ###reference_b17### and Zhong et al. (2020) ###reference_b40### have shown success in news summarization but struggle with lengthy scientific documents due to the complexity of capturing long-range dependencies and the computational constraints of Transformer-based models.\n\n2. **Large Language Models (LLMs):** Studies by Zhang et al. (2023b) ###reference_b37### and Ravaut et al. (2023) ###reference_b24### investigated LLMs like ChatGPT for summarization. These studies highlighted the potential of LLMs but also identified challenges such as the \"lost-in-the-middle\" issue and performance degradation with longer inputs.\n\n3. **Graph Neural Networks:** Researchers have adopted graph neural networks to model long-distance relations in texts. Methods like those proposed by Zheng and Lapata (2019) ###reference_b39### and Wang et al. (2020) ###reference_b29### involve constructing graphs based on sentence similarity or word-document connections. Phan et al. (2022) ###reference_b22### and Zhang et al. (2022) ###reference_b35### extended these ideas to include passage nodes and hypergraph transformers, respectively. However, these approaches often overlook comprehensive relational modeling and the hierarchical structure of documents.\n\n### Contributions of This Paper\nThe paper addresses these challenges with a novel method, HAESum (Hierarchical Attention Graph for Extractive Document Summarization). It introduces a hierarchical modeling approach that separately analyzes intra-sentence and inter-sentence relations and utilizes a novel hypergraph self-attention layer to aggregate high-order sentence representations. This method does not rely on pre-trained models, making it more adaptable to low-resource languages. Extensive experiments on benchmark datasets validate the effectiveness of this approach.\n\nBy leveraging hierarchical structures and novel attention mechanisms, HAESum seeks to provide a more nuanced and effective summarization of scientific documents compared to existing methods.",
        "methodology": "The **methodology** of the proposed **Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level (HAESum)** is described as follows:\n\n### Methodology:\n\n**Objective:**\nGiven an arbitrary document consisting of \\( n \\) sentences, where each sentence \\( s_i \\) consists of \\( m_i \\) words \\( w_{ij} \\), the goal of extractive summarization is to predict labels \\( y_i \\) for all sentences \\( s_i \\). A label \\( y_i = 1 \\) indicates that the current sentence should be included in the summary, whereas a label \\( y_i = 0 \\) indicates it should not be included.\n\n**Overall Structure:**\nThe overall structure of HAESum can be understood as involving several key components and innovations to achieve effective summarization:\n\n**Key Components:**\n1. **Hierarchical Representation:**\n   - The document is processed hierarchically, starting from the word level to the sentence level, and then to the document level. This hierarchical structure enables the model to capture and represent the contextual information at different granularities.\n\n2. **Attention Mechanisms:**\n   - Multiple attention mechanisms are used to focus on the most relevant parts of the text. These mechanisms operate at both the global (document) and local (sentence) levels, enabling the model to weigh the importance of individual words and sentences differentially.\n\n3. **Graph-based Processing:**\n   - The document is modeled as a graph where sentences and their relationships are represented as nodes and edges respectively. This graph-based representation facilitates the capture of inter-sentence relationships and the overall document structure.\n\n**Innovations:**\n1. **Hierarchical Attention:**\n   - HAESum\u2019s hierarchical attention strategy is innovative in that it systematically integrates local-level attention (within sentences) and global-level attention (across sentences). This two-tiered approach ensures that the summarization process considers both fine-grained details and broader contextual information.\n\n2. **Extractive Summarization Focus:**\n   - The methodology targets extractive summarization, aiming to directly identify and extract the most important sentences from the document rather than generating new sentences. This focus aligns with the needs of scientific document summarization where precision and context preservation are crucial.\n\n3. **Prediction of Summary Inclusion:**\n   - HAESum predicts binary labels for each sentence indicating its inclusion in the summary. This allows for straightforward construction of the final summary by simply including sentences with positive labels.\n\n**Overall:**\nHAESum attempts to effectively summarize scientific documents by leveraging hierarchical attention mechanisms and graph-based representations, capturing both local and global contextual information. Its extractive approach ensures that the generated summaries maintain the original meaning and context of the source document.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\nWe conducted our experiments on two benchmark datasets, Arxiv and PubMed (Cohan et al., 2018). Arxiv is composed of long documents from various scientific domains, while PubMed includes articles from the biomedical field. We adhered to the original training, validation, and testing splits detailed in (Cohan et al., 2018).\n\n- **Arxiv:** Contains long documents across different scientific fields.\n- **PubMed:** Comprising biomedical domain articles.\n- **Splits:** Train, validation, and test sets as defined by Cohan et al. (2018).\n\n#### Baselines\nWe compared our proposed model against various state-of-the-art methods, categorized into four types:\n\n1. **Unsupervised Methods:**\n   - **PacSum** (Zheng and Lapata, 2019)\n   - **HIPORANK** (Dong et al., 2020)\n   - **FAR** (Liang et al., 2021)\n\n2. **Neural Extractive Models:**\n   - **Seq2Seq-based Models:**\n     - **HiStruct+** (Ruan et al., 2022)\n   - **Local and Global Context Model:**\n     - **ExtSum-LG** (Xiao and Carenini, 2019)\n   - **Graph-based Models:**\n     - **Topic-GraphSum** (Cui et al., 2020)\n     - **SSN-DM** (Cui and Hu, 2021)\n     - **HEGEL** (Zhang et al., 2022)\n     - **MTGNN** (Doan et al., 2022)\n     - **CHANGES** (Zhang et al., 2023a)\n\n3. **Neural Abstractive Models:**\n   - **Encoder-Decoder Based Models:**\n     - **TLM-I+E** (Pilault et al., 2020)\n     - **PEGASUS** (Zhang et al., 2020)\n     - **BigBird** (Zaheer et al., 2020)\n   - **Divide-and-Conquer Approach:**\n     - **Dancer** (Gidiotis and Tsoumakas, 2020)\n\n4. **Large Language Model:**\n   - **ChatGLM3-6k-32k** (Zeng et al., 2022)\n\n#### Evaluation Metrics\nTo evaluate the performance of our model and the baselines, we used standard summary evaluation metrics such as ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores, including ROUGE-1, ROUGE-2, and ROUGE-L, to measure the overlap of n-grams and longest common subsequence between the generated and reference summaries.\n\n#### Main Experimental Results\nOur proposed model demonstrated superior performance compared to the baselines across both datasets, Arxiv and PubMed. The detailed results, including specific ROUGE scores, showed that our approach effectively captures the hierarchical and contextual information, leading to more accurate and informative summaries.\n\n- **Arxiv Results:** The proposed model achieved significantly higher ROUGE-1, ROUGE-2, and ROUGE-L scores compared to all baseline methods.\n- **PubMed Results:** Consistent improvements were observed over the baselines, with higher metrics indicating better summary coherence and relevance.\n\nOverall, our model's ability to integrate hierarchical attention mechanisms allowed it to outperform existing approaches in the scientific document summarization task. For further details on the performance of large language models and additional metrics, refer to Appendix A.1."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Analyze the effect of different components of HAESum on its summarization performance.",
            "experiment_process": "The experiment involves three main setups. The baseline setup incorporates all components of HAESum, including the heterogeneous graph for intra-sentence relations and the hypergraph component for higher-order cross-sentence relations. The second setup removes the heterogeneous graph part, hence not learning intra-sentence relations. The third setup removes the hypergraph component, representing the absence of learning higher-order cross-sentence relations. The performance of each setup is evaluated and compared.",
            "result_discussion": "Removing either the heterogeneous graph or the hypergraph component hurts the model performance, suggesting that both components are crucial. The findings indicate that both local intra-sentence relations and global higher-order cross-sentence relations are necessary for effective summarization of scientific documents.",
            "ablation_id": "2405.10202v1.No1"
        },
        {
            "research_objective": "Analyze the impact of different sentence representation update approaches and the self-attention mechanism on HAESum performance.",
            "experiment_process": "Various update approaches and attention mechanisms are tested. Hierarchical updating (bottom-to-top sequence) is compared to parallel updating (simultaneous intra- and inter-sentence relation updates with concatenation). Additionally, the proposed hypergraph self-attention layer is compared against the hypergraph attention network (HGAT). Performance is measured and compared across these setups.",
            "result_discussion": "Hierarchical updating outperforms parallel updating, highlighting the importance of a bottom-to-top modeling sequence. The proposed hypergraph self-attention layer shows superior performance over HGAT, attributed to the effective utilization of self-attention and different weight matrices enhancing high-order relation learning.",
            "ablation_id": "2405.10202v1.No2"
        },
        {
            "research_objective": "Investigate the sensitivity of HAESum to various hyperparameters, specifically input sentence length and the number of sentences per document.",
            "experiment_process": "The analysis involves varying the maximum input length for each sentence from 100 to 60 tokens and the maximum number of sentences per document from 50 to 200. The changes in model performance are observed and documented, and more details are provided in Appendix A.3.",
            "result_discussion": "Performance does not significantly decrease when reducing sentence token length from 100 to 60, indicating sufficient token capture within this range. However, performance declines with further reduction. Increasing the number of sentences per document from 50 to 200 enhances model performance by capturing more complex relations, although further increases result in significant computational costs. Future work aims to balance the maximum input sentences with computational efficiency.",
            "ablation_id": "2405.10202v1.No3"
        }
    ]
}