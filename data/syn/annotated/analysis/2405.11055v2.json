{
    "title": "Leveraging Discourse Structure for Extractive Meeting Summarization",
    "abstract": "We introduce an extractive summarization system for meetings that leverages discourse structure to better identify salient information from complex multi-party discussions. Using discourse graphs to represent semantic relations between the contents of utterances in a meeting, we train a GNN-based node classification model to select the most important utterances, which are then combined to create an extractive summary. Experimental results on AMI and ICSI demonstrate that our approach surpasses existing text-based and graph-based extractive summarization systems, as measured by both classification and summarization metrics. Additionally, we conduct ablation studies on discourse structure and relation type to provide insights for future NLP applications leveraging discourse analysis theory.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, the task of meeting summarization has garnered significant attention Rennard et al. (2023b  ###reference_b35###) due in part to the rising adoption of videoconferencing Kost (2020  ###reference_b15###) and the accumulated amount of meeting recordings, highlighting the need for automatic, novel and efficient processing methods.\nDespite recent advancements in natural language processing, the challenge of generating concise and coherent summaries from lengthy, multi-party meeting transcriptions remains.\nLarge language models suffer from the middle curse (Liu et al., 2024  ###reference_b21###) and struggle to use information in the middle of their context window, particularly for summarization (Ravaut et al., 2023  ###reference_b32###).\nMoreover, dealing with meetings presents several challenges that are not typically encountered in traditional document summarization (Shang, 2021  ###reference_b37###). Meetings are often characterized by spontaneous interaction, leading to phenomena such as poorly formed utterances and overlapping speech that negatively impact the accuracy of speech-to-text models\u2014the first step in an automated meeting summarization pipeline. Spontaneous speech also leads to digressions, reformulations and repetitions, which can increase transcript length while simultaneously diluting information density. A further problem is the private nature of many meetings, which greatly limits the quantity of potential public training data.\nOne strategy to offset data scarcity and content sparseness is to enrich conversation transcripts with additional information in the hopes of enhancing a system\u2019s dialogue understanding. One might add information about the discursive function that an utterance plays, for instance; that is, whether it serves to introduce or answer a question, to acknowledge another utterance or so on. In this vein, Feng et al. (2020  ###reference_b8###) showed that representing the content of a meeting transcript as a discourse graph in the style of Segmented Discourse Representation Theory (SDRT; Asher, 1993  ###reference_b3###; Lascarides and Asher, 2008  ###reference_b17###) can improve performance on abstractive meeting summarization, while Goo and Chen (2018  ###reference_b9###) demonstrated the value of dialogue acts (Jurafsky et al., 1997  ###reference_b12###; Allen and Core, 1997  ###reference_b2###) for the same task.\nSimilarly, Liu et al. (2019  ###reference_b20###) showed that supplementing transcripts with multi-modal information about participants\u2019 head orientation and eye gaze can help to identify salient information in a meeting.\nIn this paper, we exploit information on discourse structure to improve extractive summarization. While abstractive summarization is generally preferable for spontaneous conversation (Murray et al., 2010  ###reference_b28###), focusing on extractive summarization is valuable for multiple reasons.\nFirst, even if it is not entirely immune to hallucinations (Zhang et al., 2023  ###reference_b49###), extractive summarization does not suffer from this phenomenon at the level that its abstractive counterpart does (Cao et al., 2018  ###reference_b6###). On the one hand, this makes extractive summarization an attractive final product in itself in contexts where reliability is crucial. On the other hand, this, combined with the fact that extractive summarization is easier to evaluate than abstractive summarization, makes it a clearer lens through which to study the interaction between discourse structure and content salience. Finally, as a part of a pipeline for abstractive summarization (Shang et al., 2020  ###reference_b39###), it can be used to reduce the number of tokens given as input to a generative system with context length constraints, such as a transformer-based model.\nOur approach involves a novel combination of Graph Neural Networks (GNN) and graph-based representations of discourse in which each node in a graph represents the content of an individual utterance and each edge represents a semantic relation between two utterances whose nature is specified by a label, e.g., Explanation, Correction, Question-Answer Pair or Acknowledgment.\nThe task of extractive summarization is then couched as one of binary node-classification in which nodes are determined to be important or not, and the content of the nodes judged to be important is what determines the final extractive summary. This approach, in contrast to the generation at the whole graph-level (Feng et al., 2020  ###reference_b8###), allows us to gain fine-grained insight into the interaction between the importance of an utterance in a conversation and its role in the overall discourse.\nA further advantage of a graph-based approach is that GNN, whose attention is focused only on the neighbors of a given node, do not suffer from the context length restrictions imposed by the pairwise attentional computations of transformer-based models. This means that our extractive summarization approach can take an entire long meeting transcript as input without special treatment.\nTo validate our discourse-structure approach, we conduct extensive experiments on the AMI (Mccowan et al., 2005  ###reference_b26###) and ICSI (Janin et al., 2003  ###reference_b11###) corpora, comparing our method against diverse extractive summarization systems. The results demonstrate that our approach outperforms traditional techniques across evaluation metrics for both classification and summarization, including F1 score, ROUGE and BERTScore. Additionally, we present a detailed analysis of the impact of various graph construction strategies on summarization quality, offering insights into the mechanisms through which discourse structure influences content selection. Our study not only provides a novel methodological contribution to the field of automatic meeting summarization but also sheds light on the underlying discourse processes that shape effective summaries.\nFinally, discourse structure has seldom been explored in the literature for its potential utility in downstream tasks. Our work is among the few studies that focus on this effort, especially for dialogues."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Most work focuses on abstractive summarization and tends to be organized in three interconnected streams (Rennard et al., 2023b  ###reference_b35###). The first focuses on enhancing transcripts with additional information\u2014such as annotations for dialogue acts (Goo and Chen, 2018  ###reference_b9###), discourse structure (Feng et al., 2020  ###reference_b8###) or visual cues (Liu et al., 2019  ###reference_b20###)\u2014that is assumed to be relevant for abstracting important content from a transcript. While adding information does show improvement, few papers actually quantify how much information is gained from adding linguistical features; Indeed, Goo and Chen (2018  ###reference_b9###) worked on a version of AMI that lacked the current version of abstractive summaries, making it difficult to draw conclusions that are comparable to more recent works. Additionally, Feng et al. (2020  ###reference_b8###) used a global node to represent the discourse graph, aggregating all features into a single representative vector sent to an LSTM-based pointer decoder network for summarization. This approach is not ideal for evaluating the role of discourse structure in enhancing abstractive summaries. The second stream aims to transform and compress meeting transcripts in order to produce cleaner and more condensed intermediate documents that can then be passed to downstream generation modules (Krishna et al., 2021  ###reference_b16###; Oya et al., 2014  ###reference_b31###). The third stream is dedicated to the development of general generation frameworks, such as DialogLM (Zhong et al., 2022  ###reference_b51###), that are specifically designed to handle dialogue content. While we focus on extractive summarization, in exploiting discourse structure, our approach overlaps with work in the first stream above and lays groundwork for the second stream by providing a means of compressing meeting transcripts.\nTurning to extractive approaches to meeting summarization, Murray et al. (2005  ###reference_b29###) evaluated the effectiveness of latent semantic analysis, TF-IDF, and MMR algorithms for this task, providing benchmarks for all three methodologies. More recently, Tixier et al. (2017  ###reference_b43###) developed a technique to construct a graph from key words in a text and then apply submodular optimization to summarize content effectively.\nWe note that while significant strides have been made in developing sophisticated algorithms and techniques for meeting summarization, a problem that persists, for both extractive and abstractive summarization, is that of evaluation. Kirstein et al. (2024  ###reference_b14###) details the complications posed by different evaluation metrics and how they correlate with various challenges specific to meeting summarization. These problems are aggravated by the lack of readily available data in English that could be used to evaluate summarization of long format dialogues Janin et al. (2003  ###reference_b11###); Mccowan et al. (2005  ###reference_b26###); Hu et al. (2023  ###reference_b10###)\u2014a problem that only becomes worse when we look at languages beyond English (Wu et al., 2023  ###reference_b46###; Rennard et al., 2023a  ###reference_b34###; Nedoluzhko et al., 2022  ###reference_b30###).\nTwo primary theoretical frameworks have been developed to analyze complete discourse structures in the form of graphs, namely Rhetorical Structure Theory (RST; Mann and Thompson, 1987  ###reference_b25###) and Segmented Discourse Representation Theory (SDRT; Asher, 1993  ###reference_b3###; Lascarides and Asher, 2008  ###reference_b17###). The framework of RST has been successfully applied for extractive summarization of newspaper articles (Xu et al., 2020  ###reference_b47###; Liu and Chen, 2019  ###reference_b23###). However most work on discourse graphs has concentrated primarily on producing high-quality graphs (Shi and Huang, 2019  ###reference_b41###; Liu and Chen, 2021  ###reference_b24###; Bennis et al., 2023  ###reference_b5###), with less emphasis on their applications to downstream tasks such as summarization, and only SDRT has been applied to build discourse structures for multi-party conversation (Asher et al., 2016  ###reference_b4###), which is needed for meeting summarization.\nA major limitation for research on discourse structure is of course the intense effort that goes in to annotating datasets for discourse structure. There are only two readily available discourse-annotated corpora for multiparty conversation: STAC (Asher et al., 2016  ###reference_b4###), a corpus of chats from an online version of the game Settlers of Catan, and Molweni (Li et al., 2020  ###reference_b18###), an annotated version of the Ubuntu Chat Corpus. Both corpora are annotated in the style of SDRT.\nNumerous models for building graphs have been trained on the STAC and/or Molweni corpora. Shi and Huang (2019  ###reference_b41###) proposed a model that incrementally predicts both graph edges and their labels at a time  by taking into account edge and label predictions before . Liu and Chen (2021  ###reference_b24###) enhanced Shi and Huang  ###reference_b41###\u2019s model by incorporating cross-domain information, namely by using cross-domain pretraining and vocabulary refinement. Wang et al. (2021  ###reference_b45###) uses a structure transformer to incorporate node and edge information of utterance pairs, and Bennis et al. (2023  ###reference_b5###) utilizes a BERT-based framework to encode utterance pairs, predicting discourse attachments (graph edges) with a linear layer and then using a multitask approach to predict edge labels. Yang et al. (2021  ###reference_b48###) also exploit a multi-task approach to jointly learn relation edges and labels and pronoun resolution, demonstrating the synergistic interaction of these tasks. Finally, Fan et al. (2023  ###reference_b7###) combined discourse parsing with addressee recognition in a multitask approach. In our evaluations, we focus on the frameworks of Shi and Huang (2019  ###reference_b41###); Wang et al. (2021  ###reference_b45###) and Bennis et al. (2023  ###reference_b5###) as they do not require additional data."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "Due to its appropriateness for multiparty dialogue (Asher et al., 2016  ###reference_b4###) and to the fact that it is the framework adopted for the STAC and Molweni corpora, we focus on SDRT in what follows.\n###figure_1###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Task definition",
            "text": "Extractive summarization aims at creating a summary  from an input meeting , in which  consists of  elementary discourse units (EDUs)  and  consists of  EDUs, where  and . We borrow the notion of an EDU from SDRT, in which an EDU corresponds to the content of a single atomic clause so that the sentence \u201cThe remote should glow in the dark so that it\u2019s easier to find\u201d would be two EDUs111We note that while the STAC and Molweni are segmented in this way, the transcripts of AMI and ICSI, which we use for our evaluations, are segmented slightly differently. The main difference is that when two consecutive clauses play the same discourse function in AMI and ICSI, they are not segmented. This means a sentence like \u201cMy name is Ed and I am the project manager\u201d would not be segmented in these corpora because both independent clauses serve just to introduce the speaker. Because of the subtlety of this difference, we use the term EDU for simplicity..\nIt is worth to note that the input length of meeting  can be quite large and is often longer than the input size of most language models. E.g., the average length of ICSI meetings is 13317 tokens, while Llama\u2019s (Touvron et al., 2023  ###reference_b44###) context window is of 4096 tokens, which highlights the advantage of our graph-based method."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Discourse graphs",
            "text": "Discourse structure in SDRT is represented as a weakly-connected graph where each node represents the content of an EDU222SDRT allows for complex discourse units (CDUs) as well, which are subgraphs, but we do not treat CDUs in this paper and they are not annotated in the corpora that we use., and an edge between two nodes indicates that the corresponding EDUs are related through a discourse relation. Typical SDRT relations, which are indicated as edge labels, include: Comment, Clarification-question, Elaboration, Acknowledgement, Continuation, Explanation, Conditional, Question-Answer pair, Alternation, Question-Elaboration, Result, Background, Narration, Correction, Parallel, Contrast.\n.\n\\a. ID: Does anyone know if VCRs are the same across <disfmarker> international?\n.\u0331 PM: They\u2019re not <disfmarker> no. \n.\u0327 ME: They\u2019re not, no.\n.\u0323 ID: [Okay,] [so you\u2019d need like a whole different set of buttons for everybody\u2019s VCRs.]\n\\e. PM: Yeah, that\u2019s right, yeah.\nThe graph for example 3.2  ###reference_###333Example 3.2  ###reference_### is a slightly cleaned up version of an extract from meeting ES2011b of the AMI corpus. is given in Figure 2  ###reference_5###, where QAP stands for Question-Answer Pair and Ack is short for Acknowledgment."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Our model",
            "text": "Our model is illustrated in Figure 1  ###reference_###. This section details the components it uses to produce an extractive summary from a meeting transcription.\nTo perform node-classification on a graph, we first need to provide an initial representation of the nodes. Since we are dealing with text, we simply use a text embedding model to get representations for EDUs. We chose MiniLM Reimers and Gurevych (2019  ###reference_b33###), as it is a widely available sentence embedding model.\nTo generate the graph structure, we need to add edges and edge labels for different discourse relations. Because discourse annotations require expert annotators and are very labor intensive, we opted to generate the edges and labels for our graphs with an automatic parser. In this paper, except for the ablation studies in which we consider the parsers of (Liu and Chen, 2021  ###reference_b24###) and (Bennis et al., 2023  ###reference_b5###), we use Deep Sequential Shi and Huang (2019  ###reference_b41###).\nFor a meeting  with  EDUs, the Graph Generator outputs a meeting graph , where  is the set of  nodes ; , a set of  edges  for some ;\nand , a set of  relation labels  = , where  is one of the relation types listed in Subsection 3.2  ###reference_###.\nWith the initial features  of each node  created by the EDU Embedding Module and the graph  in place, we use two different architectures to evaluate the impact of discourse structure on node classification.\nFirst, we use a Relational Graph Convolutional Network (RGCN; Schlichtkrull et al., 2018  ###reference_b36###) to gather local hidden features weighted by the edge label. The RGCN does message passing, calculating the representation of a node  at the step  ( being the number of layers in the RGCN) computed as:\nin which  denotes the set of neighbors of node  under relation  and  denotes learnable parameters at the  layer for the relation . Finally, we send the final representation  to be classified by a sigmoid function on whether its content should be included or not in extractive summary.\nSecond, to evaluate the influence of graph structure alone while ignoring edge labels, we use MixHop GCN (Abu-El-Haija et al., 2019  ###reference_b1###). This model architecture supports multiple \u201chops\u201d, allowing it to access and incorporate broader structural information from nodes that are more than one edge away. This relation-agnostic GNN allows us to quantify the extent to which the graph structure itself contributes to the model\u2019s performance."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We experiment on two conversational datasets containing extractive summaries: AMI (Mccowan et al., 2005  ###reference_b26###) and ICSI (Janin et al., 2003  ###reference_b11###). Following Shang et al. (2018  ###reference_b38###), we split AMI and ICSI into training, validation, and test sets. Basics statistics are provided in Table 1  ###reference_###.\n###table_1### ###table_2### We evaluate our model based on both node classification and content of the overall summary. For the former, we use the traditional  score for classification. For the latter, we report the  score for ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004  ###reference_b19###), as well as BERTScore (Zhang et al., 2020  ###reference_b50###) to measure the difference between a ground truth extractive summary and a system generated one. We conduct a budgetization process by cutting the generated summaries (from the start) to be of the average length of the ground truth summaries when evaluating our model and baselines described below.\nWe use a variety of models to establish baselines that are both graph-based and text-based. From the node classification perspective (see Table 2  ###reference_###), we first evaluate the strength of the input embeddings by classifying the given utterance representation with a simple Logistical Regression, as well as with a Multilayer perceptron (MLP). Additionally, Graph Convolutional Network (GCN; Kipf and Welling, 2016  ###reference_b13###), which yields a GNN baseline score that does not take any of the relations into account.\nFrom the text summarization perspective (see Table 3  ###reference_###), in addition to all the above baselines we evaluate: BERTSumExt (Liu and Lapata, 2019  ###reference_b22###), which embeds individual input sentences with BERT and uses multiple inter-sentence transformer layers stacked on top of the BERT outputs to capture document-level features for extracting summaries; TextRank (Mihalcea and Tarau, 2004  ###reference_b27###), a simple, graph-based and unsupervised extractive method;\nCoreRank (Tixier et al., 2017  ###reference_b43###), whose approach relies on a Graph-of-Words representation of the meeting, graph degeneracy and submodularity for unsupervised extractive summarization;\nFinally, we use Lead-N as a heuristic metric, which takes the first N words of the meeting as its summary, N being the budgeted length of summaries.\nWe can observe from both Table 2  ###reference_### and Table 3  ###reference_### that our method using MixHop GCN and RGCN consistently achieves higher scores in terms of both classification and summarization metrics. Additionally, we can see that embedding-based methods such as MLP and Logistic Regression are more competitive than the heuristic-based method or unsupervised graph-based ones. We also evaluate different ranking methods for budgetization to improve the final summary. One approach involves selecting the longest utterances identified as important by the model until the required budget length is met (MixHop GCN + Ranking by length). Another approach involves selecting utterances with the highest model probability of being included in the summary until the budget is reached (MixHop GCN + Ranking by logits). Both methods enhance the summary quality. We believe that further research on ranking techniques could yield significantly better results."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ablation studies",
            "text": "We conducted extensive ablations to quantify the impact of both discourse structure and relation type on extractive summarization. All the experiments were run five times and reported results are the average. RGCN and MixHop GCN have three layers and 128 base parameters.\n###figure_2###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Structural ablation",
            "text": "We conducted further experiments to investigate the impact of graph edges on the model\u2019s overall classification performance. As shown in Figure 6  ###reference_###, hiding edges decreases the performance of the classifiers for both AMI and ICSI. It appears that the graph structure generally has a more robust impact on classification than relation types do. Figure 7  ###reference_### shows that the more central a node is, the higher the chances of it being a part of the extractive summary is; in other words, the more common it is to refer to a certain EDU, the higher the odds of it being important."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Studying different parsers",
            "text": "As the previously described experiments were all carried out on graphs produced by Deep Sequential (Shi and Huang, 2019  ###reference_b41###) only, we decided to also test the impact that different graph parsers have on our model. In this section, we compare the influence of graphs produced by Deep Sequential () with that of graphs produced by Knowledge Enhanced (; Liu and Chen, 2021  ###reference_b24###) and BERTLine (; Bennis et al., 2023  ###reference_b5###).\nResults in Table 4  ###reference_### show that the three parsers lead to different classification results,  scoring higher in F1 score than the other two, and  having the highest precision score. This difference suggests that we should further investigate the nature of the differences between the predictions of these three models.\nIn order to compare between the resulting graphs produced by different parsers, we employ the Weisfeiler-Lehman graph kernel (Shervashidze et al., 2011  ###reference_b40###; Siglidis et al., 2020  ###reference_b42###) to map these\ngraphs into an embedding space in which structurally alike graphs are placed closer to each other, resulting in the following similarity scores: , , .\nThese scores indicate varying levels of resemblance between the pairs, with the  and  showing the highest similarity.\nIn the end, Table 5  ###reference_### shows some statistics of the graphs. Density represents a ratio between the number of edges and nodes.\nClustering coefficient measures the degree to which vertices in a graph tend to be clustered together. It quantifies how likely it is that the neighbors of a vertex are also connected to each other. In this sense, a value of 0 shows that no two neighbors of a node are also connected (i.e., no triangle structure). We can observe that  and  graphs, have a higher number of edges but lack any triangle structure. On the other hand, we observed that  graphs have entirely disconnected nodes that will have no impact in the classification process for a GNN, explaining its relatively lower classification power."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we have demonstrated the utility of discourse graphs in the context of node classification for extractive summarization through an analysis with systems lacking both relational information or a graph structure. Additionally, we conducted ablation studies aimed at assessing the significance of various relations across different datasets. This approach allowed us to discern the impact of specific relational dynamics on the performance of classification algorithms, highlighting the integral role of discourse structures in enhancing the understanding and processing of textual information."
        }
    ],
    "url": "http://arxiv.org/html/2405.11055v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4"
        ],
        "main_experiment_and_results_sections": [
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.11055v2",
        "paper_title": "Leveraging Discourse Structure for Extractive Meeting Summarization",
        "research_background": "### Motivation\n\nMeeting summarization has become increasingly pertinent due to the proliferation of videoconferencing and the resulting surge in meeting recordings. The complexity of multi-party meeting transcriptions highlights the need for automatic summarization techniques that can generate concise and coherent summaries. Current methodologies face significant challenges, such as large language models' struggle with information situated in the middle of their context window and unique difficulties presented by the nature of meetings themselves, like poorly formed utterances, overlapping speech, and spontaneous interactions.\n\n### Research Problem\n\nThe fundamental research problem tackled in this paper is to enhance extractive summarization of meeting transcripts by leveraging discourse structure. While abstractive summarization is generally preferred for spontaneous conversations, extractive summarization offers crucial advantages like reduced hallucinations and easier evaluation, making it a suitable candidate for studying how discourse structure affects content salience. The paper aims to explore how graph neural networks (GNN) combined with discourse-graph representations can improve extractive summarization of lengthy and complex meeting transcripts.\n\n### Relevant Prior Work\n\n1. **Spontaneous Speech Challenges:**\n   - **Shang (2021):** Highlighted unique challenges for summarizing meeting transcripts, such as poorly formed utterances, overlapping speech, digressions, reformulations, and repetitions.\n   \n2. **Discourse Enrichment:**\n   - **Feng et al. (2020):** Demonstrated that representing meeting transcript content as a discourse graph improves abstractive summarization.\n   - **Goo and Chen (2018):** Validated the utility of dialogue acts for enhancing meeting summarization.\n\n3. **Multi-modal Information:**\n   - **Liu et al. (2019):** Showed that adding information about participants' head orientation and eye gaze can identify salient information in meetings.\n\n4. **Extractive Summarization Motivations:**\n   - **Murray et al. (2010):** Suggested that extractive summarization is valuable for its reliability and ease of evaluation.\n   - **Zhang et al. (2023):** Indicated that extractive summarization suffers less from hallucinations compared to abstractive summarization.\n   - **Cao et al. (2018):** Noted that reliability positions extractive summarization as a preferable method in contexts where accuracy is critical.\n\nBy combining these insights with a novel GNN approach, the paper demonstrates a methodology that not only enhances extractive summarization but also provides detailed insights into how discourse structure influences content selection.",
        "methodology": "The proposed method in the paper leverages discourse structure to enhance extractive meeting summarization through a node-classification approach on a graph derived from meeting transcriptions. Here is a summary of the key components and innovations of the methodology:\n\n1. **Node Representation**:\n   - **Text Embedding**: The model begins by representing textual units (EDUs) through embeddings. Specifically, it utilizes the MiniLM sentence embedding model (Reimers and Gurevych, 2019) to obtain these initial node features.\n\n2. **Graph Generation**:\n   - **Automatic Parser**: Given the labor-intensive nature of manual discourse annotations, the model instead employs an automatic parser to generate graph structures. The Deep Sequential parser (Shi and Huang, 2019) is preferred for this task.\n   - **Graph Composition**: The generated graph for a meeting comprises a set of nodes (EDUs), edges (indicating the relations between these nodes), and relation labels (different discourse relation types).\n\n3. **Graph Architectures**:\n   - **RGCN (Relational Graph Convolutional Network)**: This architecture is used to leverage the discourse structure by incorporating edge labels into the message-passing process. The RGCN computes node representations layer-by-layer, taking the edge types into account, and employs a sigmoid function to classify nodes on whether their content should be included in the extractive summary.\n\nBy employing this graph-based architecture, the methodology aims to analyze the significance of discourse relations and the inherent graph structure in improving the accuracy and effectiveness of extractive meeting summaries.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n- **AMI Dataset (Mccowan et al., 2005)**\n- **ICSI Dataset (Janin et al., 2003)**\n\nBoth datasets contain conversational meetings with extractive summaries. Following Shang et al. (2018), the datasets are divided into training, validation, and test sets.\n\n**Baselines:**\n1. **Node Classification Baselines:**\n   - **Logistic Regression:**\n     - Classifies utterances based on their representations.\n   - **Multilayer Perceptron (MLP):**\n     - Another classifier using utterance representations.\n   - **Graph Convolutional Network (GCN; Kipf and Welling, 2016):**\n     - A GNN baseline that does not utilize relational data.\n  \n2. **Text Summarization Baselines:**\n   - **BERTSumExt (Liu and Lapata, 2019):**\n     - Uses BERT to embed sentences and employs transformer layers for document-level feature extraction.\n   - **TextRank (Mihalcea and Tarau, 2004):**\n     - A simple, graph-based, unsupervised method for extractive summarization.\n   - **CoreRank (Tixier et al., 2017):**\n     - Utilizes Graph-of-Words, graph degeneracy, and submodularity for unsupervised summarization.\n   - **Lead-N:**\n     - Takes the first N words of the meeting as the summary, where N is the budgeted length of summaries.\n\n**Evaluation Metrics:**\n- **Node Classification:**\n  - Traditional F1 score\n- **Summary Content:**\n  - ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004)\n  - BERTScore (Zhang et al., 2020)\n\n**Budgetization Process:**\n- The generated summaries are truncated (from the start) to match the average length of ground truth summaries for evaluation.\n\n### Main Experimental Results\n\n**Node Classification:**\n- **Our method using MixHop GCN and RGCN consistently achieves higher F1 scores**, indicating superior performance in classification tasks compared to baselines such as Logistic Regression, MLP, and traditional GCN.\n\n**Text Summarization Metrics:**\n- **Our proposed model also outperforms all baselines in terms of ROUGE and BERTScore metrics**. This includes:\n  - Embedding-based methods like BERTSumExt\n  - Heuristic-based methods like Lead-N\n  - Unsupervised graph-based methods such as TextRank and CoreRank"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of each relation type on the RGCN classifier for extractive summarization.",
            "experiment_process": "The experiment used the AMI and ICSI datasets and trained an RGCN classifier that was only aware of one relation type at a time. All edges in the graph were initialized with a value of 0, except for the edges corresponding to the target relation type. This setup was repeated for each relation type to observe their individual impact.",
            "result_discussion": "In the AMI dataset, Correction, Acknowledgement, and Explanation relations significantly impacted classification, with the best performance occurring when relationships were not used at all. Conversely, in the ICSI dataset, the Result relation significantly influenced classification performance. Randomly assigning relationships among nodes worsened performance, indicating the importance of coherent graph structures.",
            "ablation_id": "2405.11055v2.No1"
        },
        {
            "research_objective": "To investigate the impact of graph edges on the model\u2019s classification performance.",
            "experiment_process": "The classifier\u2019s performance was evaluated by hiding edges in the graph for both the AMI and ICSI datasets. The centrality of nodes and their inclusion in the extractive summary were analyzed to understand the importance of graph structure.",
            "result_discussion": "Hiding edges led to decreased classification performance for both datasets, indicating the importance of the graph structure. Higher centrality of a node increased the likelihood of its inclusion in the extractive summary, emphasizing the resulting utility of more central nodes.",
            "ablation_id": "2405.11055v2.No2"
        },
        {
            "research_objective": "To compare the impact of different graph parsers on the classification performance of the summarization model.",
            "experiment_process": "Graphs produced by Deep Sequential, Knowledge Enhanced, and BERTLine parsers were used to train and test the summarization model. The classification results were measured using F1 and precision scores. The Weisfeiler-Lehman graph kernel was used to map the graphs into an embedding space for structural similarity comparison. Graph statistics such as density and clustering coefficient were also analyzed.",
            "result_discussion": "Different parsers led to varying classification outcomes. Deep Sequential graphs scored higher in F1 score, while Knowledge Enhanced graphs had the highest precision. The embedding space analysis showed the highest similarity between Deep Sequential and Knowledge Enhanced graphs. Closer examination showed that graphs by Knowledge Enhanced and Deep Sequential had a higher number of edges but lacked triangle structures, unlike BERTLine, leading to its relatively lower classification performance.",
            "ablation_id": "2405.11055v2.No3"
        }
    ]
}