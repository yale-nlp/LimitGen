{
    "title": "MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection",
    "abstract": "The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources such as social platforms, messaging apps, and traditional online news outlets. Notably, such news has been fact-checked by 14 authoritative fact-checking agencies worldwide. In addition, various existing Chinese fake news detection methods are thoroughly evaluated on our proposed dataset in cross-source, multi-source, and unseen source ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news detection approaches in real-world scenarios.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "It has been prevalent for the public to consume news through various online sources, such as social platforms and news websites. Such sources are efficient media for spreading fake news. For instance, the latest Weibo annual report on fake news (Weibo, 2023  ###reference_b39###) revealed that Weibo\u2019s official fact-checking agency identified 82,274 pieces of fake news in 2022. Given the devastating consequences of fake news on both individuals and society, fake news detection has become an urgent and essential task that needs to be addressed (A\u00efmeur et al., 2023  ###reference_b2###; Hu et al., 2022  ###reference_b10###; Nan et al., 2021  ###reference_b19###; Wang et al., 2018  ###reference_b37###; Sheng et al., 2021a  ###reference_b28###, b  ###reference_b29###, 2022  ###reference_b27###). To this end, Chinese fake news detection datasets have been constructed for the development of Chinese fake news detection (Hu et al., 2022  ###reference_b10###; Jin et al., 2017a  ###reference_b12###, b  ###reference_b13###; Zhang et al., 2021  ###reference_b42###; Nan et al., 2021  ###reference_b19###; Yang et al., 2021  ###reference_b41###; Hu et al., 2023  ###reference_b11###).\n###figure_1### The existing Chinese fake news detection datasets are limited to one single source, Weibo, where both true and fake news are collected. However, in the real world, news emerges from multiple sources, such as social platforms, messaging apps, and traditional online news outlets, etc. (Hu et al., 2022  ###reference_b10###; Pierri and Ceri, 2019  ###reference_b21###; Bondielli and Marcelloni, 2019  ###reference_b4###; Li et al., 2023a  ###reference_b14###, b  ###reference_b15###). News, in particular, fake news, from different sources is characterized by diverse dimensions, such as news content, topics, publishing methods, and the utilization of sophisticated linguistic styles intended to mimic real news (Shu et al., 2021  ###reference_b32###; Wang et al., 2020  ###reference_b38###; Pathak and Srihari, 2019  ###reference_b20###; Przybyla, 2020  ###reference_b22###; A\u00efmeur et al., 2023  ###reference_b2###). For example, Fig. 1  ###reference_### shows four instances of fake news, each sourced from a distinct news source, exemplifying different news characteristics. Existing Weibo-based Chinese fake news detection datasets that fail to capture the above data diversity can impede the effectiveness of machine learning (ML) based fake news detection in practice, including but not limited to the robustness to intricately crafted fake news and the generalization to fake news from other sources (Wang et al., 2023  ###reference_b35###; Przybyla, 2020  ###reference_b22###; A\u00efmeur et al., 2023  ###reference_b2###; Hu et al., 2022  ###reference_b10###; Nan et al., 2021  ###reference_b19###).\nPilot Experiment. To verify such limitations, we conducted evaluations on 817 pieces of fake news we collected. They were verified between Jan. 2015 and Mar. 2023, from the China Internet Joint Rumor Refuting Platform111https://www.piyao.org.cn/  ###reference_www.piyao.org.cn/###, a government-backed fact-checking agency supported by authoritative experts and various government departments. The agency covers fake news originating from a wide variety of sources, including but not limited to, Douyin, Wechat, TouTiao, Zhihu, Weibo, etc.222\nThe websites for the mentioned news sources are as follows:\nDouyin: https://www.douyin.com/  ###reference_www.douyin.com/###;\nWechat: https://www.wechat.com/  ###reference_www.wechat.com/###;\nTouTiao: https://www.toutiao.com/  ###reference_www.toutiao.com/###;\nZhihu: https://www.zhihu.com/  ###reference_www.zhihu.com/###; and Weibo: https://m.weibo.cn/  ###reference_m.weibo.cn/###.\n\nWe trained the state-of-the-art fake news detection model BERT-EMO (Zhang et al., 2021  ###reference_b42###) on the Weibo-21 dataset (Nan et al., 2021  ###reference_b19###).\nThe model demonstrated strong performance with F1 scores of 0.943 on the Weibo-21, 0.932 on the Weibo-20 (Zhang et al., 2021  ###reference_b42###) dataset, and 0.908 on the Weibo-16 dataset (Jin et al., 2017a  ###reference_b12###), respectively.\nNevertheless, when we used the same model to detect fake news collected from all the diverse sources on the platform, failed to identify 35.34% of fake news. The macro F1 score dropped to 0.470, a decline of 52.03%.\nThe following may account for such experimental results.\nFirst, during the training phase, ML models lack exposure to a diverse range of data from various sources, which leads to an overfitting of specific characteristics of Weibo fake news. This limits their capability to generalize and effectively identify fake news from different sources.\nFurther, during the testing phase, these models are evaluated using Weibo data exclusively, overlooking a comprehensive assessment of their performance across different news sources.\nWhen faced with real-world fake news that emerges from multiple sources, the applicability of models trained and tested on existing datasets is questionable. Our analytical results validate the limitations of current Chinese fake news detection datasets. Therefore, it is imperative to construct a comprehensive dataset that consists of (real and fake) from diverse sources.\nTo bridge this gap, we constructed the first Multi-source benchmark dataset for Chinese FakE News Detection (MCFEND), which contains 23,974 pieces of authoritatively verified Chinese news from 14 fact-checking agencies covering numerous news sources.333The Weibo Community Management Center, Weibo\u2019s official fact-checking agency, exclusively examines news sourced from Weibo, whereas other fact-checking agencies cover various news sources. Note that the inclusion of additional fact-checking agencies can potentially enhance the variety of news sources considered. Please refer to Table 2  ###reference_### for the full list of included fact-checking agencies. These fact-checking agencies are further divided into three distinct groups. The first group encompasses nine Chinese fact-checking agencies. That means the collected news has been verified by experts as active and authoritative. The second group corresponds to three existing annotated English fake news detection datasets. Specifically, for an English news piece paired with its corresponding authenticity label from an existing English fake news dataset, we employ the state-of-the-art cross-lingual identical news retrieval system to collect its Chinese equivalent while retaining its original label. The third group consists of Weibo\u2019s official fact-checking agency exclusively, i.e., the Weibo Community Management Center. For this group of news, we utilized news data from the Weibo-21 dataset (Nan et al., 2021  ###reference_b19###). Furthermore, we conducted comprehensive evaluations on six established baseline models for Chinese fake news detection, including state-of-the-art methods, under cross-source and multi-source scenarios on our MCFEND dataset. The experimental results characterize the challenge of accurately spotting fake news from different sources that the dataset presents.\nOur contributions are summarized as follows:\nFirst, We constructed the initial multi-source Chinese fake news detection dataset MCFEND, which comprises multi-modal content and social context of 23,974 real-world Chinese news pieces collected from 14 authoritative fact-checking agencies in three distinct groups. Additionally, to the best of our knowledge, MCFEND is the largest open-sourced Chinese fake news detection dataset, being at least 2.63 times larger than any existing ones. The dataset aims to benchmark the evaluations of Chinese fake news detection methods in real-world scenarios, where news originates from diverse sources, and to encourage further research in this field.\nSecond, we conducted comprehensive cross-source, multi-source, and unseen source evaluations on six established baseline models for Chinese fake news detection, including state-of-the-art methods. Our experimental results reveal that the models trained on existing datasets are not applicable in real-world scenarios. Incorporating multi-source data is necessary, which can enhance the models\u2019 robustness substantially."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Preliminaries and Related work",
            "text": "Fake news detection, also referred to as false news detection, and related to information credibility evaluation, is commonly defined as a binary classification task (Hu et al., 2022  ###reference_b10###; Przybyla, 2020  ###reference_b22###; Pierri and Ceri, 2019  ###reference_b21###; Bondielli and Marcelloni, 2019  ###reference_b4###).\nIn this context, we define the output space as  to classify news items as real () or fake (). The input space  includes multidimensional data, comprising both the content of the news and its social context.\nLet  represent a collection of  news pieces, each associated with a label indicating its authenticity. Here,  and .\nThe goal is to design fake news detection models capable of learning a function , such that for any given news item ,  predicts its label  with high accuracy.\nNumerous datasets have been constructed to address fake news detection. Representative English fake news detection datasets, such as BuzzFace (Santia and Williams, 2018  ###reference_b26###), LIAR (Wang, 2017  ###reference_b36###), FakeNewsNet (Shu et al., 2020  ###reference_b31###), PHEME (Zubiaga et al., 2017  ###reference_b44###), KaggleFakeNews (Risdal, 2016  ###reference_b25###), FakeNewsCorpus (Pathak and Srihari, 2019  ###reference_b20###), and FakeHealth (Dai et al., 2020  ###reference_b7###), were constructed on English news from social platforms like Twitter and Facebook, as well as fact-checking websites such as BuzzFeed, PolitiFact, and NewsGuard.444The websites for these social platforms are as follows: Twitter: https://www.twitter.com/  ###reference_www.twitter.com/###; Facebook: https://www.facebook.com/  ###reference_www.facebook.com/###; BuzzFeed: https://www.buzzfeed.com/  ###reference_www.buzzfeed.com/###; PolitiFact: https://www.politifact.com/  ###reference_www.politifact.com/###; and NewsGuard: https://www.newsguardtech.com  ###reference_www.newsguardtech.com###. A few Chinese fake news detection datasets have also been proposed. For instance, Ma et al. introduced the Weibo-16 dataset (Jin et al., 2017a  ###reference_b12###), collected from the Chinese social platform Weibo. This dataset contains verified fake news sourced from the Weibo Community Management Center555https://service.account.weibo.com  ###reference_service.account.weibo.com###, an official fact-checking agency for posts on Weibo. Real news was collected from regular posts that were not categorized as fake. While Weibo-16 focuses exclusively on textual data, Jin et al. (Jin et al., 2017b  ###reference_b13###) later introduced Media-Weibo, the first multi-modal dataset for detecting Chinese fake news. Media-Weibo includes textual content, user profiles, and supplementary images for each post. Zhang et al. (Zhang et al., 2021  ###reference_b42###) then extended Media-Weibo dataset to Weibo-20 dataset by adding 850 real news pieces authenticated by NewsVerify666https://www.newsverify.com/  ###reference_www.newsverify.com/###, a fact-checking website dedicated to verifying posts on Weibo, from April 2014 to November 2018, and 1,806 fake news pieces that were officially verified by the Weibo Community Management Center within the same timeframe. Using a similar approach, Yang et al. (Yang et al., 2021  ###reference_b41###) constructed CHECKED dataset, aiming at detecting COVID-19-related fake news on Weibo. Additionally, Nan et al. (Nan et al., 2021  ###reference_b19###) proposed the Weibo-21 dataset, the first multi-domain Chinese fake news detection dataset. Weibo-21 contains both fake and real news pieces collected from Weibo spanning from December 2014 to March 2021, covering nine different domains, such as Science, Military, and Education. Most recently, Hu et al. (Hu et al., 2023  ###reference_b11###) constructed the multi-modal retrieval augmented dataset MR2. This dataset consists of two subsets from Weibo and Twitter, respectively, covering news with images and texts, and provides evidence retrieved from the Internet for both modalities.\nExisting datasets for Chinese fake news detection rely heavily on Weibo. Different from them, we constructed the pioneering multi-source Chinese fake news detection dataset, termed MCFEND, which contains 23,974 real-world Chinese news pieces collected from multiple sources across three distinct categories. Table 1  ###reference_### compares the Chinese fake news detection datasets."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. MCFEND Dataset",
            "text": "In this section, we introduce our Chinese multi-source fake news detection dataset, MCFEND. Additionally, we perform data analysis to investigate into the differences between various sources."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Overview",
            "text": "The MCFEND dataset contains news verified by 14 fact-checking agencies from a wide range of sources, such as messaging apps, social platforms, and traditional news outlets. As mentioned above, these agencies are categorized into three groups (see Table 2  ###reference_###. The websites for the 14 fact-checking agencies are detailed in Appendix A  ###reference_###.\nThe first group includes nine Chinese fact-checking agencies identified as active by Duke Reporters777https://reporterslab.org/fact-checking/  ###reference_###, along with four Chinese fact-checking agencies manually verified by experts as active and authoritative.\nThe second group corresponds to four English fact-checking agencies, including Politifact, Gossipcop, BS Detector, and FakeNewsCorpus. This group contains the Chinese counterparts of the English news fact-checked by the aforementioned agencies, which are collected through a carefully designed cross-lingual identical news retrieval method. An in-depth description of the method can be found in Sec. 3.2.2  ###reference_.SSS2###.\nGroup 3 exclusively covers the Weibo Community Management Center, from which news data was directly sourced from Weibo-21 dataset (Nan et al., 2021  ###reference_b19###).\nThe overall MCFEND dataset contains 23,974 news pieces, including 8,144 sourced from the nine fact-checking agencies in Group 1, 6,702 related to the four English fact-checking agencies in Group 2, and 9,128 obtained from the Weibo Community Management Center in Group 3.\nSimilar to existing dataset construction (Shu et al., 2020  ###reference_b31###; Nan et al., 2021  ###reference_b19###), we collected the following information for each piece of news in any group:\n(1) Multi-modal news content, including text, images, and metadata, e.g., timestamps;\n(2) Multi-modal social context, including posts, comments, emojis, user profiles, and other metadata, e.g., like counts of comments. Table 3  ###reference_### presents the detailed statistics of the MCFEND dataset."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Dataset Construction",
            "text": "In this subsection, we present the process of constructing the dataset for each group of the fact-checking agencies. Fig. 2  ###reference_### illustrates the entire process for the dataset construction.\n###figure_2###"
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1. Group 1: Fact-checking Agencies Data Crawling",
            "text": "Fact-checking agencies serve as a common source for labeling fake news detection datasets (Przybyla, 2020  ###reference_b22###; Hu et al., 2022  ###reference_b10###). These agencies are typically operated by government entities, companies, or non-profit organizations, and they employ authoritative experts to assess the authenticity of news pieces originating from diverse sources, such as social platforms, messaging apps, and traditional online news outlets. As discussed in Sec. 1  ###reference_###, including a wider range of fact-checking agencies enhance the diversity of news sources in our dataset.\nTo maximize our coverage of news sources, we conducted web crawling to collect data from all active Chinese fact-checking agencies, encompassing the five Chinese fact-checking agencies identified as active by Duke Reporters, in addition to nine other Chinese fact-checking agencies that were manually verified as active and authoritative. In the case where the labels on some fact-checking agencies, e.g., AFP Fact Check Asia and Factcheck Lab, are presented in the form of images, we utilized an optical character recognition method called Tesseract-OCR to retrieve such labels.888https://github.com/tesseract-ocr/tesseract  ###reference_###"
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2. Group 2: Cross-lingual Identical News Retrieval",
            "text": "To further diversify our news sources, we employ a cross-lingual identical news retrieval method to obtain the corresponding Chinese equivalents of both real and fake English news within several well-known datasets, including FakeNewsNet (Shu et al., 2020  ###reference_b31###), KaggleFakeNews (Risdal, 2016  ###reference_b25###), and FakeNewsCorpus (Pathak and Srihari, 2019  ###reference_b20###). FakeNewsNet consists of two detailed sub-collections sourced from distinct fact-checking organizations, specifically Politifact and Gossipcop. KaggleFakeNews contains news gathered from 244 sources classified as \u201cunreliable or otherwise questionable\u201d by the BS Detector, a browser extension that assesses the reliability of websites by comparing them to a professionally curated list.\nFakeNewsCorpus is a dataset consisting of news related to the 2016 US elections. The news pieces in this dataset are manually annotated by its authors.\nWe consider the BS Detector and the authors of the FakeNewsCorpus as two distinct fact-checking agencies. By incorporating these three datasets, we effectively introduce data from four additional fact-checking agencies, enabling us to collect news from a wider range of English news sources.\nFor each news piece in these datasets, we executed the following steps to identify its corresponding Chinese counterpart:\nStep 1: Translation. We utilized the Baidu Translation API to translate the headlines of the English news into Chinese.999http://fanyi-api.baidu.com/  ###reference_anyi-api.baidu.com/###\nStep 2: Chinese News Retrieval with Google News.101010https://news.google.com/  ###reference_news.google.com/### Google News provides extensive and up-to-date news coverage from sources worldwide. We configured the language and region of interest as \u201cChinese (China)\u201d and employed the translated Chinese news headline as the search query. Search engines typically sort results by relevance. We assumed that the top five returned news pieces were the most relevant Chinese counterparts to the original English news. Subsequently, we crawled the top five returned news pieces.\nStep 3: Cross-lingual News Similarity Calculation. To determine the degree of similarity between the Chinese news retrieved in the previous step and the original English news, we employed the state-of-the-art cross-lingual news similarity calculation system (Xu et al., 2022  ###reference_b40###), which ranked 1st in the SemEval2022 Task 8 challenge (Chen et al., 2022b  ###reference_b5###) with a Pearson correlation coefficient of 0.818 on the official evaluation set. Specifically, we calculated the similarity score between the retrieved Chinese news and the original English news. The Chinese news with the highest similarity score was preserved in our MCFEND dataset. The strong performance of the system effectively ensures the consistency of misleading content across different languages.\nStep 4: Label Assignment. The authenticity label of the original English news is used to label its Chinese counterpart, that is, the Chinese news with the highest similarity score.\nThe method inherently retains human-written news content. In contrast to directly utilizing Chinese news content generated by machine translator, our approach avoids unnatural textual expressions that could potentially introduce noise to the models."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3. Group 3: Weibo News Collection",
            "text": "Group 3 consists solely of news sourced from the Weibo. For this group, we directly utilized news data in the Weibo-21 dataset (Nan et al., 2021  ###reference_b19###), the largest Chinese fake news detection dataset on Weibo."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4. Social Context Collection",
            "text": "Relying solely on news content may be inadequate for detecting fake news, as fake news content is often meticulously crafted to deceive the public. Social platforms offer an invaluable source of supplementary information in the form of social context features (Shu et al., 2020  ###reference_b31###; Hu et al., 2022  ###reference_b10###; Tai et al., 2015  ###reference_b33###; Ma et al., 2018  ###reference_b18###; Shu et al., 2019  ###reference_b30###; Ma and Gao, 2020  ###reference_b17###; Zhang et al., 2021  ###reference_b42###), capturing user interactions and social behaviors within the social platform environment. Thus, to incorporate such important features, we collected social context data, such as posts, comments, user profiles, etc., on the largest social platform in China, Weibo.111111While Weibo serves as the social context source for all collected news pieces, it also acts as an independent news source.\nThe process of collecting social context aligns closely with the approach detailed in (Shu et al., 2020  ###reference_b31###) for gathering social context from Twitter. Firstly, for news pieces that have headlines, we created search queries for associated posts on Weibo using the headlines. For news pieces without headlines, we utilized the Jieba tool to tokenize the textual content of the news and extract the top five keywords, which were then used as search queries.121212https://github.com/fxsjy/jieba  ###reference_github.com/fxsjy/jieba### During this process, we removed special characters from the search queries to eliminate unnecessary noise. All matching posts were considered relevant and included. We then retrieved user responses to these posts, including comments, reposts, and likes. Additionally, upon identifying all users involved in the news propagation process, we collected metadata for these users, such as their usernames and profiles. As shown in Table 3  ###reference_###, we assembled a comprehensive set of relevant social context data, which includes 170,713 posts and 2,102,902 comments from 803,779 distinct users.131313Individual users may engage in the social context of news collected from fact-checking agencies across different groups."
        },
        {
            "section_id": "3.2.5",
            "parent_section_id": "3.2",
            "section_name": "3.2.5. Post-collection Processing",
            "text": "After collecting all news pieces and their corresponding social context, we conducted three post-collection processing steps:\nStep 1: Text Cleaning. To enhance data quality and eliminate unnecessary noise, we conducted text cleaning on text within both news content and social context. This cleaning process involved removing HTML tags, punctuation, white spaces, stop words, and prefix headings.\nStep 2: Deduplication. The raw data contained multiple duplications. As a result, we removed redundant news and social context data to avoid unnecessary repetitions.\nStep 3: Label Mapping. Different fact-checking agencies employ diverse fine-grained labels to express degrees of authenticity (e.g., true, mostly true, and inconclusive). To ensure consistency, we designed a label mapping strategy to standardize the original labels.\nPlease refer to Appendix B  ###reference_### for details of our label mapping strategy."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Experiments",
            "text": "We conducted experiments to evaluate the performance of representative fake news detection methods on our newly proposed MCFEND dataset. Specifically, we aim to answer the following evaluation questions (EQs):\n: Are existing methods, which have demonstrated effectiveness on the existing Weibo datasets, capable of maintaining their performance when being applied to news collected from different sources?\n: Can training with multi-source data enhance the robustness of existing methods in detecting fake news in real-world scenarios, which involve multiple sources?\n: Can training with multi-source data enhance the robustness of existing methods in detecting fake news originating from previously unseen news sources?\nThe detailed experimental setups are provided in Appendix C  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Baselines",
            "text": "To address the above EQs, we carefully selected six baseline models spanning the two widely recognized categories of fake news detection approaches: content-based and social context-based methods (Hu et al., 2022  ###reference_b10###, 2023  ###reference_b11###; A\u00efmeur et al., 2023  ###reference_b2###; Zhu et al., 2024  ###reference_b43###), and constructed the baseline benchmarks. Their implementation details are as follows."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Content-based Methods",
            "text": "Content-based methods rely on solely the textual or visual contents of the news. We adopted two representative types of content-based models, uni-modal models and multi-modal models.\nUni-modal models focus on the textual content of the news. We used BERT (Devlin et al., 2019  ###reference_b8###) and RoBERTa (Liu et al., 2019  ###reference_b16###) as contextualized encoders to encode the textual content.\nThen, the representation of the \u201c[CLS]\u201d special token is used for prediction. The implementation of BERT and RoBERTa in this study is based on their respective Chinese base versions, i.e., BERT-base-Chinese151515https://huggingface.co/bert-base-chinese  ###reference_### and RoBERTa-wwm-Base.161616https://huggingface.co/hfl/chinese-roberta-wwm-ext  ###reference_-wwm-ext###\nMulti-modal models encode both text and images in the input news. We consider two multi-modal baselines: CLIP (Radford et al., 2021  ###reference_b23###) and CAFE (Chen et al., 2022a  ###reference_b6###).\nCLIP (Radford et al., 2021  ###reference_b23###) is a pretrained model for images and text. We input the image and text of the news into CLIP to obtain a joint representation of the visual and textual content.\nThis joint representation is then utilized for predictions.\nWe implement CLIP based on its Chinese version.171717https://github.com/OFA-Sys/Chinese-CLIP  ###reference_### CAFE (Chen et al., 2022a  ###reference_b6###) is an ambiguity-aware fake news detection method.\nSpecifically, it integrates uni-modal features produced by BERT (Devlin et al., 2019  ###reference_b8###) for text and ResNet (He et al., 2016  ###reference_b9###) for images, along with cross-modal correlations.\nIt relies on uni-modal features when cross-modal ambiguity is weak and relies on cross-modal correlations when cross-modal ambiguity is strong.\nCAFE demonstrates superior fake news detection performance on the Twitter (Boididou et al., 2018  ###reference_b3###) and Weibo-16 (Jin et al., 2017a  ###reference_b12###) datasets, respectively.\nIt stands as the state-of-the-art content-based approach to this task."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2. Social Context-based Methods",
            "text": "Social context-based methods are typically categorized into three groups: tree-based, modal fusion-based, and graph-based (Hu et al., 2023  ###reference_b11###). Since our MCFEND dataset contains news from diverse sources and lacks the necessary cross-source user/news interactions to build effective graphs and trees, we included only the modal fusion-based models, excluding the graph-based and tree-based models.\nModal fusion-based models integrate information from both news content and social context. In our study, we considered two representative baseline models for this category: dEFEND (Shu et al., 2019  ###reference_b30###) and BERT-EMO (Zhang et al., 2021  ###reference_b42###).\nThe dEFEND model (Shu et al., 2019  ###reference_b30###) utilizes a sentence-comment co-attention sub-network to exploit news contents and comments in the social context to jointly capture explainable top-k check-worthy sentences and comments for fake news detection.\nOn the other hand, BERT-EMO (Zhang et al., 2021  ###reference_b42###) enhances a BERT-based fake news detector by incorporating dual emotion features that represent both the emotions and the relationship between news and comments within the social context.\nNote that the BERT-EMO model has demonstrated outstanding performance in fake news detection, achieving the highest reported performance on the Weibo-20 dataset (Zhang et al., 2021  ###reference_b42###). Besides, our preliminary experiments on the Weibo-21 dataset (Nan et al., 2021  ###reference_b19###) have also shown that BERT-EMO achieved an impressive F1-score of 0.943, outperforming all other methods.\nThe results show that BERT-EMO as the state-of-the-art social context-based approach in the fake news detection task."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Cross-source Evaluation",
            "text": "To address EQ1, we performed cross-source evaluations on the baseline models.\nSpecifically, we evaluated the performance of baseline models trained exclusively on Weibo data, focusing on their application to diverse news sources.\nOur findings, shown in Table 4  ###reference_###, reveal significant variations in performance across different test groups, underscoring the challenge of applying models trained on a single-source dataset to news originating from varied sources.\nThe BERT-EMO model delivered the highest performance, achieving an accuracy of 0.821 and a macro F1 score of 0.818. However, while it scored a high macro F1 of 0.943 on data from Weibo (Group 3), its effectiveness significantly decreased on data from Group 1 and Group 2, with scores dropping to 0.572 and 0.343, respectively.\nNote that the pattern of performance variance was consistent across all baseline models, including BERT, RoBERTa, CLIP, CAFE, and dEFEND, across different groups.\nThese findings provide a crucial insight in the responses to EQ1: Baseline models, even those considered state-of-the-art and trained on Chinese fake news detection datasets from Weibo, exhibit limited robustness when confronted with fake news from diverse sources in the wild.\nOne contributing factor to such performance decrease may be the significant difference in the content and social context feature, as shown in Fig. 3  ###reference_###, between news sourced from Weibo and news from other sources.\nThis discrepancy also highlights a significant concern regarding the possible overestimation of the effectiveness of current Chinese fake news detection methods, underscoring the necessity for a thorough reevaluation before they are considered for practical application in real-world situations.\nA comparison in Table 5  ###reference_### shows the average decrease in macro F1 score when moving from Weibo-sourced data to other sources, with a smaller decrease indicating greater source robustness.\nOur analysis found that models employing modal fusion approaches, integrating both text and social context, demonstrate stronger resilience against data source variability, with an average decrease of only 0.084.\nIn particular, the modal fusion-based model, dEFEND, exhibited the greatest robustness, with a minimal decrease of 0.032.\nIn contrast, uni-modal and multi-modal approaches saw larger decreases of 0.222 and 0.227, respectively.\nThese results indicate that the model performance is greatly influenced by content pattern differences among sources, suggesting that exploring ways to mitigate these impacts is a valuable direction for future research."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Multi-source Evaluation",
            "text": "To address EQ2, we performed multi-source evaluations on the selected baseline models. Specifically, these baseline systems were trained with the complete train set of the MCFEND dataset, including news from all sources covered in our dataset and their corresponding social contexts.\nThe multi-source evaluation results, depicted in Table 6  ###reference_###, provide crucial insights.\nA comparison with the cross-source evaluation data (refer to Table 4  ###reference_###) reveals significant performance improvements in all baseline models upon integrating multi-source data for training.\nNotably, RoBERTa and CAFE show the most substantial gains, with their macro F1 scores increasing by 0.245 and 0.224, respectively.\nSuch boost in performance can be attributed to the diverse range of fake news features presented by multi-source data, which enhances the models\u2019 ability to discern between fake and real news from different sources, helping to prevent models from overfitting to the specific characteristics of data from a single source like Weibo.\nTo offer a qualitative analysis of the enhancement from using multi-source data in the training process, we take the CAFE model as an example.\nWhen trained with Weibo data exclusively, the CAFE model fails to correctly identify the news pieces (c) and (d) shown in Fig. 1  ###reference_### as fake.\nHowever, when trained with data from all the diverse sources encompassed in the MCFEND train set, the CAFE model exhibits the ability to accurately detect all of the presented fake news in Fig. 1  ###reference_###.\nThese findings address EQ2 by demonstrating that using multi-source data to train fake news detection models significantly improves their performance and robustness in real-world applications. MCFEND dataset could be an invaluable asset for enhancing the detection of Chinese fake news across a variety of sources.\nAdditionally, the overall performance of almost all baseline models, except dEFEND, when they were trained and tested on multi-source data is lower than their performance when they were trained and tested on Weibo data exclusively.\nThis finding addresses the challenge of developing algorithms capable of effectively distinguishing generic fake news features across news from various sources in real-world scenarios.\nCompared among baseline models, the state-of-the-art content-based and social context-based models, CAFE and BERT-EMO, stand out with macro F1 scores of 0.845 and 0.916, respectively.\nThis implies that their relatively lower performance in cross-source evaluation suggests limitations in current training datasets.\nThus, our diverse and comprehensive MCFEND dataset, designed to address these limitations, is crucial for advancing Chinese fake news detection in the wild."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Unseen Source Evaluation",
            "text": "To address EQ3, we conducted an unseen source evaluation to assess the robustness of existing methods in detecting fake news from previously unencountered news sources.\nThis involved training two versions of the BERT-EMO model, which had shown superior performance in both cross-source and multi-source evaluations, on distinct dataset compositions.\nModel A was trained exclusively on data from Group 3 (Weibo), while Model B incorporated data from both Group 1 (various Chinese news sources verified by fact-checking agencies) and Group 3 (Weibo).\nWe then evaluated both models using data from Group 2 (English news sources), which was new to each model.\nThe results, detailed in Table 7  ###reference_###, reveal that Model B, trained on a more diverse dataset, achieved a higher macro F1 score of 0.432, compared to Model A\u2019s score of 0.287.\nThis finding addresses EQ3 by suggesting that leveraging multi-source data enhances the robustness of methods for detecting fake news from unseen sources.\nTherefore, when a new news source emerges in the future, but there is no available data from that platform for model training, a model trained on multi-source data can be expected to more accurately detect fake news from this unseen new source."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Conclusion",
            "text": "In this work, we introduced the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND. Unlike existing Chinese fake news detection datasets that are based on a single news source, i.e., Weibo, MCFEND is constructed on (real and fake) news from multiple sources that were fact-checked by 14 authoritative fact-checking agencies. To test the applicability of existing methods, we conducted a systematic evaluation of six representative fake news detection models, including the state-of-the-art ones, in both cross-source and multi-source scenarios. Our experimental results reveal that models trained exclusively on Weibo data can hardly be applicable in real-world scenarios, where fake news typically originates from diverse sources. We also found that incorporating multi-source data into model training enhances the robustness of existing fake news detection methods. Our proposed MCFEND aims to be a benchmark dataset for Chinese fake news detection in the wild, which advances new effective methods in this research field.\nOur world is increasingly suffering from unrestrained spread of misinformation in many areas. We hope MCFEND can put forward research results that can combat misinformation and help make our world a better one."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7. Acknowledgement",
            "text": "This work was supported by the National Natural Science Foundation of China (No. 62202402), the Guangdong Basic and Applied Basic Research Foundation (No. 2022A1515011583 and No. 2023A1515011562), the Hong Kong RGC Early Career Scheme (No. 22202423), the Germany/Hong Kong Joint Research Scheme sponsored by the Research Grants Council of Hong Kong and the German Academic Exchange Service of Germany (No. G-HKBU203/22), the One-off Tier 2 Start-up Grant (2020/2021) of Hong Kong Baptist University (Ref. RC-OFSGT2/20-21/COMM/002), and the Startup Grant (Tier 1) for New Academics AY2020/21 of Hong Kong Baptist University.\nWe thank Ms. Zihang Shan for her assistance in developing the cross-lingual identical news retrieval model and preparing the data. We also thank Ms. Zixin Tang for her assistance in developing the CLIP baseline and executing the associated experiments."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A The Fact-checking Agencies",
            "text": "Table 8  ###reference_### lists the websites for the 14 fact-checking agencies.\nChina Internet Joint Rumor Refuting Platform\nhttps://www.piyao.org.cn/  ###reference_www.piyao.org.cn/###\nTencent Jiaozhen\nhttps://vp.fact.qq.com/  ###reference_vp.fact.qq.com/###\nChina Daily Factcheck\nhttps://www.chinadaily.com.cn/china/factcheck/  ###reference_eck/###\nTaiwan FactCheck Center\nhttps://tfc-taiwan.org.tw/  ###reference_tfc-taiwan.org.tw/###\nMyGoPen\nhttps://www.mygopen.com/  ###reference_www.mygopen.com/###\nHKBU Factcheck\nhttps://factcheck.hkbu.edu.hk/home/  ###reference_###\nHKU Annie Lab\nhttps://annielab.org/  ###reference_annielab.org/###\nAFP Fact Check Asia\nhttps://factcheck.afp.com/afp-asia/  ###reference_###\nFactcheck Lab\nhttps://www.factchecklab.org/  ###reference_www.factchecklab.org/###\nPolitifact\nhttps://www.politifact.com/  ###reference_www.politifact.com/###\nGossipcop\nhttps://www.gossipcop.com/  ###reference_www.gossipcop.com/###\nBS Detector\nhttps://github.com/selfagency/bs-detector  ###reference_###\nFakeNewsCorpus\nhttps://github.com/architapathak/FakeNewsCorpus  ###reference_orpus###\nWeibo Community Management Center\nhttps://service.account.weibo.com  ###reference_service.account.weibo.com###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Label Mapping Strategy",
            "text": "Various fact-checking agencies use distinct labels to indicate the truthfulness of information, such as \u201crumor\u201d, \u201cmistake\u201d, and \u201cmisleading\u201d.\nTo unify the label space in our dataset, we used a mapping strategy to standardize the original labels, as illustrated in Table 9  ###reference_###.\nWe also provide the English translations of these labels."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Experimental Setup",
            "text": "Consistent with prior studies on detecting fake news, our evaluation framework utilizes accuracy and the macro F1 score as metrics.\nWe divided the MCFEND dataset into training, validation, and test segments following a 7:1.5:1.5 ratio through random division to ensure our experimental results\u2019 reliability.\nThis division was conducted three times, with each cycle running for 100 epochs on a single NVIDIA V100 GPU. We report the mean F1 and accuracy scores along with their standard deviations from the three iterations.\nAll models were trained using the Adam optimizer with a batch size of 32.\nTo increase training efficiency, we applied an early stopping rule, halting the training if no performance improvement was seen after 500 consecutive batches, ensuring model convergence.\nHyper-parameter selection was based on the validation set\u2019s performance.\nA comprehensive summary of the hyper-parameter settings for each model is provided in Table 10  ###reference_###.\nFor parameters not covered here, we suggest consulting the original publications for more details.\nWe standardized the preprocessing of text to a maximum of 256 tokens and images to a uniform size of 224x224 pixels through center-cropping.\nIn scenarios where news items did not have associated images, we used a standard blank white image for training multi-modal models.\n###table_1### ###figure_5### learning rate: 2e-5;\nhidden size: 768; number of layers: 12;\nnumber of attention heads: 12.\nlearning rate: 2e-5;\nhidden size: 768; number of layers: 12;\nnumber of attention heads: 12.\nlearning rate: 2e-5; project_dim: 512.\nlearning rate: 1e-3;\ntext_embedding: fixed BERT embedding;\nimage_embdding: fixed ResNet embedding.\nlearning rate: 2e-5;\ntext_embedding: fixed BERT embedding;\nproject_dim: 200; number of RNN layers: 2.\nlearning rate: 1e-3; \nhidden_size (bidirectional-GRU): 32; \nhidden_size (fully connected layers): 32."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S2.T1.3.1.1\" style=\"font-size:90%;\">Table 1</span>. </span><span class=\"ltx_text\" id=\"S2.T1.4.2\" style=\"font-size:90%;\">Summary of Chinese fake news detection datasets. Please note that the <span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.4.2.1\">MR2</span> dataset comprises two subsets: one from Weibo (Chinese) and one from Twitter (English). The statistics presented in this table specifically pertain to the Weibo (Chinese) subset.</span></figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T1.5.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T1.5.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.1.1.1.1\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T1.5.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S2.T1.5.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.5.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.5.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.1.1.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.1.1.2.1.1.1.1.1\">#News</span></span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" id=\"S2.T1.5.1.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.1.1.3.1\">Feature</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T1.5.1.1.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S2.T1.5.1.1.4.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.5.1.1.4.1.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.5.1.1.4.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.1.1.4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.1.1.4.1.1.1.1.1\">News</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.5.1.1.4.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.1.1.4.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.1.1.4.1.1.2.1.1\">Source</span></span></span>\n</span></span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T1.5.2.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.2.2.1.1\">Text</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T1.5.2.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.2.2.2.1\">Image</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T1.5.2.2.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.5.2.2.3.1\">\n<tr class=\"ltx_tr\" id=\"S2.T1.5.2.2.3.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.2.2.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.2.2.3.1.1.1.1\">Social</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.2.2.3.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.2.2.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.2.2.3.1.2.1.1\">Context</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.5.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.5.3.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.3.1.1.1\">Weibo-16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.5.3.1.2\">5,656</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.5.3.1.3\">\u2713</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.5.3.1.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.5.3.1.5\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.5.3.1.6\">Weibo</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.4.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.4.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.4.2.1.1\">Weibo-Media</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.4.2.2\">5,802</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.4.2.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.4.2.4\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.4.2.5\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.4.2.6\">Weibo</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.5.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.5.3.1.1\">Weibo-20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.3.2\">6,362</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.3.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.3.4\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.3.5\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.5.3.6\">Weibo</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.6.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.6.4.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.6.4.1.1\">Weibo-21</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.6.4.2\">9,128</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.6.4.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.6.4.4\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.6.4.5\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.6.4.6\">Weibo</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.7.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.7.5.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.5.7.5.1.1\">MR2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.7.5.2\">6,976</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.7.5.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.7.5.4\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.7.5.5\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.7.5.6\">Weibo</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.8.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.5.8.6.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S2.T1.5.8.6.1.1\">MCFEND</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.5.8.6.2\">23,974</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.5.8.6.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.5.8.6.4\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.5.8.6.5\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.5.8.6.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.5.8.6.6.1\">\n<tr class=\"ltx_tr\" id=\"S2.T1.5.8.6.6.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.8.6.6.1.1.1\">Multiple</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.8.6.6.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.5.8.6.6.1.2.1\">Sources</td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 1. Summary of Chinese fake news detection datasets. Please note that the MR2 dataset comprises two subsets: one from Weibo (Chinese) and one from Twitter (English). The statistics presented in this table specifically pertain to the Weibo (Chinese) subset."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T2.2.1.1\" style=\"font-size:90%;\">Table 2</span>. </span><span class=\"ltx_text\" id=\"S3.T2.3.2\" style=\"font-size:90%;\">An overview of fact-checking agencies.</span></figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.1.1.1.1\">Group</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T2.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.1.1.2.1\">Fact-checking Agencies</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.1.1\" rowspan=\"9\"><span class=\"ltx_text\" id=\"S3.T2.4.2.1.1.1\">Group 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.1.2\">China Internet Joint Rumor Refuting Platform</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.3.2.1\">Tencent Jiaozhen</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.3.1\">China Daily Factcheck</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.5.4.1\">Taiwan FactCheck Center</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.6.5.1\">MyGoPen</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.7.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.7.6.1\">HKBU Factcheck</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.8.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.8.7.1\">HKU Annie Lab</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.9.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.9.8.1\">AFP Fact Check Asia</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.10.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.10.9.1\">Factcheck Lab</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.11.10.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T2.4.11.10.1.1\">Group 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.11.10.2\">Politifact</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.12.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.12.11.1\">Gossipcop</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.13.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.13.12.1\">BS Detector</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.14.13\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.14.13.1\">FakeNewsCorpus</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.15.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T2.4.15.14.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T2.4.15.14.2\">Weibo Community Management Center</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 2. An overview of fact-checking agencies."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T3.3.1.1\" style=\"font-size:90%;\">Table 3</span>. </span><span class=\"ltx_text\" id=\"S3.T3.4.2\" style=\"font-size:90%;\">Statistics of the <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T3.4.2.1\">MCFEND</span> dataset.</span></figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.5.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.5.1.1.1.1\">Statistics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.5.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.5.1.1.2.1\">Group 1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.5.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.5.1.1.3.1\">Group 2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.5.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.5.1.1.4.1\">Group 3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T3.5.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.5.1.1.5.1\">Overall</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.5.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.2.1.1\">#Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.2.1.2\">8,144</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.2.1.3\">6,702</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.2.1.4\">9,128</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.2.1.5\">23,974</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.3.2.1\">#Fake</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.3.2.2\">7,486</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.3.2.3\">5,741</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.3.2.4\">4,488</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.3.2.5\">17,715</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.4.3.1\">#Real</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.4.3.2\">473</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.4.3.3\">9,61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.4.3.4\">4,640</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.4.3.5\">6,074</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.5.4.1\">#user</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.5.4.2\">235,215</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.5.4.3\">156,862</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.5.4.4\">458,800</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.5.4.5\">803,779</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.6.5.1\">#posts</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.6.5.2\">58,299</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.6.5.3\">41,600</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.6.5.4\">70,814</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.6.5.5\">170,713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.7.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.7.6.1\">#comments</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.7.6.2\">262,342</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.7.6.3\">328,465</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.7.6.4\">1,512,095</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.7.6.5\">2,102,902</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T3.5.8.7.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S3.T3.5.8.7.1.1\">Timeframe</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.8.7.2\">Mar. 2015</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.8.7.3\">Jan. 2015</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.8.7.4\"><span class=\"ltx_text\" id=\"S3.T3.5.8.7.4.1\" style=\"color:#000000;\">Dec. 2014</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.5.8.7.5\">Jan. 2015</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.9.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.9.8.1\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.9.8.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.9.8.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.5.9.8.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.5.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T3.5.10.9.1\">Mar. 2023</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T3.5.10.9.2\">Mar. 2023</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T3.5.10.9.3\">Mar. 2021</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S3.T3.5.10.9.4\">Mar. 2023</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 3. Statistics of the MCFEND dataset."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T4.2.1.1\" style=\"font-size:90%;\">Table 4</span>. </span><span class=\"ltx_text\" id=\"S4.T4.3.2\" style=\"font-size:90%;\">The performance of the baselines in cross-source evaluation.\nThe training data for all baselines in cross-source evaluation is exclusively sourced from Weibo.\nThe overall results are calculated using test data from all groups. The highest number in each group is in bold.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.1.1.2.1\">Test Data</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.1.1.3.1\">Accuracy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.4.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.1.1.4.1\">Macro F1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.2.1.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T4.4.2.1.1.1\">BERT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.2.1.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.2.1.3.1\">0.830\u00b10.018</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.2.1.4.1\">0.521\u00b10.009</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.3.2.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.3.2.2\">0.453\u00b10.027</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.3.2.3\">0.407\u00b10.017</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.4.3.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.4.3.2\">0.817\u00b10.006</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.4.3.3\">0.816\u00b10.006</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.5.4.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.5.4.2\">0.719\u00b10.008</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.5.4.3\">0.673\u00b10.007</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.6.5.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T4.4.6.5.1.1\">RoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.6.5.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.6.5.3\">0.124\u00b10.008</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.6.5.4\">0.123\u00b10.007</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.7.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.7.6.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.7.6.2.1\">0.763\u00b10.038</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.7.6.3.1\">0.539\u00b10.016</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.8.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.8.7.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.8.7.2\">0.883\u00b10.006</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.8.7.3\">0.883\u00b10.006</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.9.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.9.8.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.9.8.2\">0.595\u00b10.011</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.9.8.3\">0.582\u00b10.009</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.10.9.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T4.4.10.9.1.1\">CLIP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.10.9.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.10.9.3\">0.769\u00b10.011</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.10.9.4\">0.503\u00b10.006</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.11.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.11.10.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.11.10.2\">0.500\u00b10.043</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.11.10.3\">0.458\u00b10.030</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.12.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.12.11.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.12.11.2\">0.895\u00b10.012</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.12.11.3\">0.895\u00b10.012</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.13.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.13.12.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.13.12.2\">0.741\u00b10.009</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.13.12.3\">0.711\u00b10.009</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.14.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.14.13.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T4.4.14.13.1.1\">CAFE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.14.13.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.14.13.3\">0.586\u00b10.013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.14.13.4\">0.431\u00b10.006</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.15.14\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.15.14.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.15.14.2\">0.343\u00b10.032</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.15.14.3\">0.335\u00b10.026</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.16.15\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.16.15.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.16.15.2\">0.891\u00b10.001</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.16.15.3\">0.891\u00b10.001</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.17.16\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.17.16.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.17.16.2\">0.635\u00b10.014</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.17.16.3\">0.621\u00b10.013</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.18.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.18.17.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T4.4.18.17.1.1\">dEFEND</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.18.17.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.18.17.3\">0.656\u00b10.031</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.18.17.4\">0.452\u00b10.009</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.19.18\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.19.18.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.19.18.2\">0.511\u00b10.058</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.19.18.3\">0.400\u00b10.026</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.20.19\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.20.19.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.20.19.2\">0.743\u00b10.016</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.20.19.3\">0.743\u00b10.016</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.21.20\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.21.20.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.21.20.2\">0.709\u00b10.002</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.21.20.3\">0.701\u00b10.003</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.22.21\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T4.4.22.21.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T4.4.22.21.1.1\">BERT-EMO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.22.21.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.22.21.3\">0.572\u00b10.043</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.22.21.4\">0.405\u00b10.013</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.23.22\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.23.22.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.23.22.2\">0.343\u00b10.174</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.23.22.3\">0.287\u00b10.104</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.24.23\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.24.23.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.24.23.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.24.23.2.1\">0.943\u00b10.010</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.24.23.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.24.23.3.1\">0.943\u00b10.010</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.25.24\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.4.25.24.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.4.25.24.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.25.24.2.1\">0.821\u00b10.008</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.4.25.24.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.25.24.3.1\">0.818\u00b10.008</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 4. The performance of the baselines in cross-source evaluation.\nThe training data for all baselines in cross-source evaluation is exclusively sourced from Weibo.\nThe overall results are calculated using test data from all groups. The highest number in each group is in bold."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T5.2.1.1\" style=\"font-size:90%;\">Table 5</span>. </span><span class=\"ltx_text\" id=\"S4.T5.3.2\" style=\"font-size:90%;\">The comparison of the average performance decrease of all baseline categories, which represents the difference between overall performance and performance on data sourced from Weibo. A smaller performance decrease indicates greater cross-source performance robustness.</span></figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.4.1.1.1.1\">Baseline Category</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.4.1.1.2.1\">Average Macro F1 Decrease</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.4.2.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T5.4.2.1.1.1\">Uni-modal</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.4.2.1.2\">-0.222</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.4.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.4.3.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T5.4.3.2.1.1\">Multi-modal</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.4.3.2.2\">-0.227</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.4.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.4.4.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T5.4.4.3.1.1\">Modal fusion-based</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.4.4.3.2\">-<span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.4.4.3.2.1\">0.084</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.4.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T5.4.5.4.1\">Average</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T5.4.5.4.2\">-0.107</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 5. The comparison of the average performance decrease of all baseline categories, which represents the difference between overall performance and performance on data sourced from Weibo. A smaller performance decrease indicates greater cross-source performance robustness."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T6.2.1.1\" style=\"font-size:90%;\">Table 6</span>. </span><span class=\"ltx_text\" id=\"S4.T6.3.2\" style=\"font-size:90%;\">The performance of the baselines in multi-source evaluation. The training data for all baselines in multi-source evaluation contains news from all sources. The overall results are calculated using test data from all groups. The highest number in each group is in bold.</span></figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.1.1.2.1\">Test Data</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.1.1.3.1\">Accuracy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.4.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.1.1.4.1\">Macro F1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.2.1.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T6.4.2.1.1.1\">BERT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.2.1.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.2.1.3.1\">0.934\u00b10.004</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.2.1.4\">0.535\u00b10.015</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.3.2.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.3.2.2\">0.858\u00b10.007</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.3.2.3\">0.614\u00b10.052</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.4.3.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.4.3.2\">0.803\u00b10.015</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.4.3.3\">0.801\u00b10.015</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.5.4.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.5.4.2\">0.862\u00b10.005</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.5.4.3\">0.800\u00b10.012</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.6.5.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T6.4.6.5.1.1\">RoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.6.5.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.6.5.3\">0.939\u00b10.002</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.6.5.4\">0.518\u00b10.012</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.7.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.7.6.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.7.6.2.1\">0.860\u00b10.001</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.7.6.3\">0.501\u00b10.013</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.8.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.8.7.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.8.7.2\">0.844\u00b10.011</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.8.7.3\">0.844\u00b10.011</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.9.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.9.8.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.9.8.2\">0.880\u00b10.004</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.9.8.3\">0.827\u00b10.008</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.10.9.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T6.4.10.9.1.1\">CLIP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.10.9.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.10.9.3\">0.8891\u00b10.020</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.10.9.4\">0.5213\u00b10.018</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.11.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.11.10.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.11.10.2\">0.690\u00b10.073</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.11.10.3\">0.565\u00b10.044</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.12.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.12.11.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.12.11.2\">0.8518\u00b10.005</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.12.11.3\">0.851\u00b10.005</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.13.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.13.12.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.13.12.2\">0.8178\u00b10.015</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.13.12.3\">0.7685\u00b10.014</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.14.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.14.13.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T6.4.14.13.1.1\">CAFE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.14.13.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.14.13.3\">0.926\u00b10.008</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.14.13.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.14.13.4.1\">0.563\u00b10.016</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.15.14\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.15.14.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.15.14.2\">0.857\u00b10.010</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.15.14.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.15.14.3.1\">0.675\u00b10.007</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.16.15\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.16.15.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.16.15.2\">0.875\u00b10.008</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.16.15.3\">0.875\u00b10.008</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.17.16\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.17.16.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.17.16.2\">0.887\u00b10.007</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.17.16.3\">0.845\u00b10.007</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.18.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.18.17.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T6.4.18.17.1.1\">dEFEND</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.18.17.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.18.17.3\">0.915\u00b10.005</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.18.17.4\">0.521\u00b10.025</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.19.18\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.19.18.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.19.18.2\">0.755\u00b10.067</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.19.18.3\">0.482\u00b10.065</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.20.19\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.20.19.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.20.19.2\">0.757\u00b10.004</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.20.19.3\">0.756\u00b10.004</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.21.20\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.21.20.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.21.20.2\">0.791\u00b10.007</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.21.20.3\">0.774\u00b10.007</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.22.21\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T6.4.22.21.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T6.4.22.21.1.1\">BERT-EMO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.22.21.2\">Group 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.22.21.3\">0.902\u00b10.004</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.4.22.21.4\">0.514\u00b10.040</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.23.22\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.23.22.1\">Group 2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.23.22.2\">0.827\u00b10.027</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.23.22.3\">0.545\u00b10.052</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.24.23\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.24.23.1\">Group 3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.24.23.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.24.23.2.1\">0.937\u00b10.006</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.4.24.23.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.24.23.3.1\">0.937\u00b10.006</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.4.25.24\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T6.4.25.24.1\">Overall</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T6.4.25.24.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.25.24.2.1\">0.922\u00b10.005</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T6.4.25.24.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.4.25.24.3.1\">0.916\u00b10.005</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 6. The performance of the baselines in multi-source evaluation. The training data for all baselines in multi-source evaluation contains news from all sources. The overall results are calculated using test data from all groups. The highest number in each group is in bold."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T7\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T7.2.1.1\" style=\"font-size:90%;\">Table 7</span>. </span><span class=\"ltx_text\" id=\"S5.T7.3.2\" style=\"font-size:90%;\">Macro F1 Score Comparison on Unseen Sources.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T7.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S5.T7.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.1.2.1\">Train Data</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.1.3.1\">Accuracy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.4.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.1.4.1\">Macro F1 Score</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T7.4.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T7.4.2.1.1\">Model A</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.2.1.2\">Group 3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.2.1.3\">0.343\u00b10.174</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.4.2.1.4\">0.287\u00b10.104</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S5.T7.4.3.2.1\">Model B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.3.2.2\">Groups 1 and 3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.3.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.3.2.3.1\">0.602\u00b10.204</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T7.4.3.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.3.2.4.1\">0.432\u00b10.088</span></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 7. Macro F1 Score Comparison on Unseen Sources."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T8\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A1.T8.2.1.1\" style=\"font-size:90%;\">Table 8</span>. </span><span class=\"ltx_text\" id=\"A1.T8.3.2\" style=\"font-size:90%;\">Fact-Checking Agencies.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T8.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T8.4.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.1.1.1\" style=\"width:195.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"A1.T8.4.1.1.1.1\">Fact-Checking Agency</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A1.T8.4.1.1.2\" style=\"width:195.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"A1.T8.4.1.1.2.1\">URL</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T8.4.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.2.1.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.2.1.1.1\">China Internet Joint Rumor Refuting Platform</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.2.1.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.2.1.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.piyao.org.cn/\" title=\"\">https://www.piyao.org.cn/  ###reference_www.piyao.org.cn/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.3.2.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.3.2.1.1\">Tencent Jiaozhen</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.3.2.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.3.2.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://vp.fact.qq.com/\" title=\"\">https://vp.fact.qq.com/  ###reference_vp.fact.qq.com/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.4.3\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.4.3.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.4.3.1.1\">China Daily Factcheck</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.4.3.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.4.3.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.chinadaily.com.cn/china/factcheck/\" title=\"\">https://www.chinadaily.com.cn/china/factcheck/  ###reference_eck/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.5.4.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.5.4.1.1\">Taiwan FactCheck Center</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.5.4.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.5.4.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://tfc-taiwan.org.tw/\" title=\"\">https://tfc-taiwan.org.tw/  ###reference_tfc-taiwan.org.tw/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.6.5\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.6.5.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.6.5.1.1\">MyGoPen</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.6.5.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.6.5.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.mygopen.com/\" title=\"\">https://www.mygopen.com/  ###reference_www.mygopen.com/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.7.6\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.7.6.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.7.6.1.1\">HKBU Factcheck</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.7.6.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.7.6.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://factcheck.hkbu.edu.hk/home/\" title=\"\">https://factcheck.hkbu.edu.hk/home/  ###reference_###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.8.7\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.8.7.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.8.7.1.1\">HKU Annie Lab</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.8.7.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.8.7.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://annielab.org/\" title=\"\">https://annielab.org/  ###reference_annielab.org/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.9.8\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.9.8.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.9.8.1.1\">AFP Fact Check Asia</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.9.8.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.9.8.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://factcheck.afp.com/afp-asia/\" title=\"\">https://factcheck.afp.com/afp-asia/  ###reference_###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.10.9\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.10.9.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.10.9.1.1\">Factcheck Lab</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.10.9.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.10.9.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.factchecklab.org/\" title=\"\">https://www.factchecklab.org/  ###reference_www.factchecklab.org/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.11.10\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.11.10.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.11.10.1.1\">Politifact</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.11.10.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.11.10.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.politifact.com/\" title=\"\">https://www.politifact.com/  ###reference_www.politifact.com/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.12.11\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.12.11.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.12.11.1.1\">Gossipcop</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.12.11.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.12.11.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.gossipcop.com/\" title=\"\">https://www.gossipcop.com/  ###reference_www.gossipcop.com/###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.13.12\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.13.12.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.13.12.1.1\">BS Detector</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.13.12.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.13.12.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/selfagency/bs-detector\" title=\"\">https://github.com/selfagency/bs-detector  ###reference_###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.14.13\">\n<td class=\"ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.14.13.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.14.13.1.1\">FakeNewsCorpus</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T8.4.14.13.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.14.13.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/architapathak/FakeNewsCorpus\" title=\"\">https://github.com/architapathak/FakeNewsCorpus  ###reference_orpus###</a></p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T8.4.15.14\">\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"A1.T8.4.15.14.1\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.15.14.1.1\">Weibo Community Management Center</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" id=\"A1.T8.4.15.14.2\" style=\"width:195.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T8.4.15.14.2.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://service.account.weibo.com\" title=\"\">https://service.account.weibo.com  ###reference_service.account.weibo.com###</a></p>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 8. Fact-Checking Agencies."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T9\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A3.T9.3.1.1\" style=\"font-size:90%;\">Table 9</span>. </span><span class=\"ltx_text\" id=\"A3.T9.4.2\" style=\"font-size:90%;\">The strategy for mapping the authenticity labels by the fact-checking agencies to our standardized labels.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T9.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T9.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T9.1.1.1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"761\" id=\"A3.T9.1.1.1.g1\" src=\"x3.png\" width=\"830\"/></td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 9. The strategy for mapping the authenticity labels by the fact-checking agencies to our standardized labels."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T10\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"A3.T10.2.1.1\" style=\"font-size:90%;\">Table 10</span>. </span><span class=\"ltx_text\" id=\"A3.T10.3.2\" style=\"font-size:90%;\">Model-wise core hyper-parameter settings adopted in our implementations of the baselines.</span></figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A3.T10.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A3.T10.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A3.T10.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T10.4.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"A3.T10.4.1.1.2\" style=\"width:170.7pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center ltx_align_top\" id=\"A3.T10.4.1.1.2.1\">Hyper-parameters</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T10.4.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A3.T10.4.2.1.1\">BERT</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T10.4.2.1.2\" style=\"width:170.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T10.4.2.1.2.1\">learning rate: 2e-5;\n<br class=\"ltx_break ltx_centering\"/>hidden size: 768; number of layers: 12;\n<br class=\"ltx_break ltx_centering\"/>number of attention heads: 12.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T10.4.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A3.T10.4.3.2.1\">RoBERTa</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T10.4.3.2.2\" style=\"width:170.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T10.4.3.2.2.1\">learning rate: 2e-5;\n<br class=\"ltx_break ltx_centering\"/>hidden size: 768; number of layers: 12;\n<br class=\"ltx_break ltx_centering\"/>number of attention heads: 12.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T10.4.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A3.T10.4.4.3.1\">CLIP</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T10.4.4.3.2\" style=\"width:170.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T10.4.4.3.2.1\">learning rate: 2e-5; project_dim: 512.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T10.4.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A3.T10.4.5.4.1\">CAFE</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T10.4.5.4.2\" style=\"width:170.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T10.4.5.4.2.1\">learning rate: 1e-3;\n<br class=\"ltx_break ltx_centering\"/>text_embedding: fixed BERT embedding;\n<br class=\"ltx_break ltx_centering\"/>image_embdding: fixed ResNet embedding.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T10.4.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"A3.T10.4.6.5.1\">dEFEND</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A3.T10.4.6.5.2\" style=\"width:170.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T10.4.6.5.2.1\">learning rate: 2e-5;\n<br class=\"ltx_break ltx_centering\"/>text_embedding: fixed BERT embedding;\n<br class=\"ltx_break ltx_centering\"/>project_dim: 200; number of RNN layers: 2.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T10.4.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"A3.T10.4.7.6.1\">BERT-EMO</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\" id=\"A3.T10.4.7.6.2\" style=\"width:170.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A3.T10.4.7.6.2.1\">learning rate: 1e-3; \n<br class=\"ltx_break ltx_centering\"/>hidden_size (bidirectional-GRU): 32; \n<br class=\"ltx_break ltx_centering\"/>hidden_size (fully connected layers): 32.</p>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>",
            "capture": "Table 10. Model-wise core hyper-parameter settings adopted in our implementations of the baselines."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.09092v1_figure_1.png",
            "caption": "Figure 1. An example of four pieces of fake news from four different Chinese news sources, including Weibo (a popular social platform), China Times (an online news outlet), Wechat (a messaging app), and Douyin (a social platform). Each piece of fake news showcases different characteristics across various aspects, such as content, topics, publishing methods, linguistic styles, etc."
        },
        "2": {
            "figure_path": "2403.09092v1_figure_2.png",
            "caption": "Figure 2. The process for constructing the MCFEND dataset."
        },
        "3": {
            "figure_path": "2403.09092v1_figure_3.png",
            "caption": "Figure 3. Visualization of textual and social emotion features for news collected from three distinct groups of fact-checking agencies."
        },
        "4": {
            "figure_path": "2403.09092v1_figure_4.png",
            "caption": "Figure 3. Visualization of textual and social emotion features for news collected from three distinct groups of fact-checking agencies."
        },
        "5": {
            "figure_path": "2403.09092v1_figure_5.png",
            "caption": "Table 10. Model-wise core hyper-parameter settings adopted in our implementations of the baselines."
        }
    },
    "references": [
        {
            "1": {
                "title": "Fake News, Disinformation and Misinformation in Social Media: A Review.",
                "author": "Esma A\u00efmeur, Sabrine Amri, and Gilles Brassard. 2023.",
                "venue": "Social Network Analysis and Mining 13, 1 (2023), 30.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Detection and Visualization of Misleading Content on Twitter.",
                "author": "Christina Boididou, Symeon Papadopoulos, Markos Zampoglou, Lazaros Apostolidis, Olga Papadopoulou, and Yiannis Kompatsiaris. 2018.",
                "venue": "International Journal of Multimedia Information Retrieval 7, 1 (2018), 71\u201386.",
                "url": null
            }
        },
        {
            "3": {
                "title": "A Survey on Fake News and Rumour Detection Techniques.",
                "author": "Alessandro Bondielli and Francesco Marcelloni. 2019.",
                "venue": "Information Sciences 497 (2019), 38\u201355.",
                "url": null
            }
        },
        {
            "4": {
                "title": "SemEval-2022 Task 8: Multilingual News Article Similarity. In Proc. of SemEval. 1094\u20131106.",
                "author": "Xi Chen, Ali Zeynali, Chico Camargo, Fabian Fl\u00f6ck, Devin Gaffney, Przemyslaw Grabowicz, Scott Hale, David Jurgens, and Mattia Samory. 2022b.",
                "venue": "",
                "url": null
            }
        },
        {
            "5": {
                "title": "Cross-Modal Ambiguity Learning for Multimodal Fake News Detection. In Proc. of WWW. 2897\u20132905.",
                "author": "Yixuan Chen, Dongsheng Li, Peng Zhang, Jie Sui, Qin Lv, Lu Tun, and Li Shang. 2022a.",
                "venue": "",
                "url": null
            }
        },
        {
            "6": {
                "title": "Ginger Cannot Cure Cancer: Battling FakeHealth News with a Comprehensive Data Repository. In Proc. of ICWSM. 853\u2013862.",
                "author": "Enyan Dai, Yiwei Sun, and Suhang Wang. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "7": {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proc. of NAACL. 4171\u20134186.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "8": {
                "title": "Deep residual learning for image recognition. In Proc. of CVPR. 770\u2013778.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.",
                "venue": "",
                "url": null
            }
        },
        {
            "9": {
                "title": "Deep Learning for Fake News Detection: A Comprehensive Survey.",
                "author": "Linmei Hu, Siqi Wei, Ziwang Zhao, and Bin Wu. 2022.",
                "venue": "AI Open 3 (2022), 133\u2013155.",
                "url": null
            }
        },
        {
            "10": {
                "title": "MR2: A Benchmark for Multimodal Retrieval-Augmented Rumor Detection in Social Media. In Proc. of SIGIR. 2901\u20132912.",
                "author": "Xuming Hu, Zhijiang Guo, Junzhe Chen, Lijie Wen, and Philip S. Yu. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "11": {
                "title": "Multimodal Fusion with Recurrent Neural Networks for Rumor Detection on Microblogs. In Proc. of MM. 795\u2013816.",
                "author": "Zhiwei Jin, Juan Cao, Han Guo, Yongdong Zhang, and Jiebo Luo. 2017a.",
                "venue": "",
                "url": null
            }
        },
        {
            "12": {
                "title": "Multimodal Fusion with Recurrent Neural Networks for Rumor Detection on Microblogs. In Proc. of MM. 795\u2013816.",
                "author": "Zhiwei Jin, Juan Cao, Han Guo, Yongdong Zhang, and Jiebo Luo. 2017b.",
                "venue": "",
                "url": null
            }
        },
        {
            "13": {
                "title": "Improved target-specific stance detection on social media platforms by delving into conversation threads.",
                "author": "Yupeng Li, Haorui He, Shaonan Wang, Francis C.M. Lau, and Yunya Song. 2023a.",
                "venue": "IEEE Transactions on Computational Social Systems (2023).",
                "url": null
            }
        },
        {
            "14": {
                "title": "Contextual Target-Specific Stance Detection on Twitter: Dataset and Method. In Proc. of IEEE ICDM. 359\u2013367.",
                "author": "Yupeng Li, Dacheng Wen, Haorui He, Jianxiong Guo, Xuan Ning, and Francis C. M. Lau. 2023b.",
                "venue": "",
                "url": null
            }
        },
        {
            "15": {
                "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": "arXiv preprint arXiv:1907.11692 (2019).",
                "url": null
            }
        },
        {
            "16": {
                "title": "Debunking Rumors on Twitter with Tree Transformer. In Proc. of COLING. 5455\u20135466.",
                "author": "Jing Ma and Wei Gao. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "17": {
                "title": "Rumor Detection on Twitter with Tree-structured Recursive Neural Networks. In Proc. of ACL. 1980\u20131989.",
                "author": "Jing Ma, Wei Gao, and Kam-Fai Wong. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "18": {
                "title": "MDFEND: Multi-Domain Fake News Detection. In Proc. of CIKM. 3343\u20133347.",
                "author": "Qiong Nan, Juan Cao, Yongchun Zhu, Yanyan Wang, and Jintao Li. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "19": {
                "title": "BREAKING! Presenting Fake News Corpus for Automated Fact Checking. In Proc. of ACL-SRW Workshop. 357\u2013362.",
                "author": "Archita Pathak and Rohini Srihari. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "20": {
                "title": "False News on Social Media: A Data-driven Survey.",
                "author": "Francesco Pierri and Stefano Ceri. 2019.",
                "venue": "ACM Sigmod Record 48, 2 (2019), 18\u201327.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Capturing the Style of Fake News. In Proc. of AAAI. 490\u2013497.",
                "author": "Piotr Przybyla. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "22": {
                "title": "Learning Transferable Visual Models From Natural Language Supervision. In Proc. of ICML. 8748\u20138763.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "23": {
                "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proc. of EMNLP.",
                "author": "Nils Reimers and Iryna Gurevych. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "24": {
                "title": "Getting Real about Fake News.",
                "author": "Megan Risdal. 2016.",
                "venue": "",
                "url": null
            }
        },
        {
            "25": {
                "title": "BuzzFace: A News Veracity Dataset with Facebook User Commentary and Egos. In Proc. of ICWSM. 531\u2013540.",
                "author": "Giovanni Santia and Jake Williams. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "26": {
                "title": "Zoom Out and Observe: News Environment Perception for Fake News Detection. In Proc. of ACL. 4543\u20134556.",
                "author": "Qiang Sheng, Juan Cao, Xueyao Zhang, Rundong Li, Danding Wang, and Yongchun Zhu. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "27": {
                "title": "Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims. In Proc. of ACL. 5468\u20135481.",
                "author": "Qiang Sheng, Juan Cao, Xueyao Zhang, Xirong Li, and Lei Zhong. 2021a.",
                "venue": "",
                "url": null
            }
        },
        {
            "28": {
                "title": "Integrating Pattern- and Fact-based Fake News Detection via Model Preference Learning. In Proc. of CIKM. 1640\u20131650.",
                "author": "Qiang Sheng, Xueyao Zhang, Juan Cao, and Lei Zhong. 2021b.",
                "venue": "",
                "url": null
            }
        },
        {
            "29": {
                "title": "DEFEND: Explainable Fake News Detection. In Proc. of KDD. 395\u2013405.",
                "author": "Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "30": {
                "title": "FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social Media.",
                "author": "Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. 2020.",
                "venue": "Big Data 8, 3 (2020), 171\u2013188.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Early Detection of Fake News with Multi-source Weak Social Supervision. In Proc. of ECML PKDD. 650\u2013666.",
                "author": "Kai Shu, Guoqing Zheng, Yichuan Li, Subhabrata Mukherjee, Ahmed Hassan Awadallah, Scott Ruston, and Huan Liu. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "32": {
                "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. In Proc. of ACL-IJCNLP. 1556\u20131566.",
                "author": "Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015.",
                "venue": "",
                "url": null
            }
        },
        {
            "33": {
                "title": "Visualizing Data Using t-SNE.",
                "author": "Laurens Van der Maaten and Geoffrey Hinton. 2008.",
                "venue": "Journal of Machine Learning Research 9, 11 (2008).",
                "url": null
            }
        },
        {
            "34": {
                "title": "Trustworthy Machine Learning: Robustness, Generalization, and Interpretability. In Proc. of KDD. 5827\u20135828.",
                "author": "Jindong Wang, Haoliang Li, Haohan Wang, Sinno Jialin Pan, and Xing Xie. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "35": {
                "title": "\u201cLiar, Liar Pants on Fire\u201d: A New Benchmark Dataset for Fake News Detection. In Proc. of ACL. 422\u2013426.",
                "author": "William Yang Wang. 2017.",
                "venue": "",
                "url": null
            }
        },
        {
            "36": {
                "title": "EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection. In Proc. of KDD. 849\u2013857.",
                "author": "Yaqing Wang, Fenglong Ma, Zhiwei Jin, Ye Yuan, Guangxu Xun, Kishlay Jha, Lu Su, and Jing Gao. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "37": {
                "title": "Weak Supervision for Fake News Detection via Reinforcement Learning. In Proc. of AAAI. 516\u2013523.",
                "author": "Yaqing Wang, Weifeng Yang, Fenglong Ma, Jin Xu, Bin Zhong, Qiang Deng, and Jing Gao. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "38": {
                "title": "Weibo\u2019s Annual Repot on Fake News.",
                "author": "Weibo. 2023.",
                "venue": "https://weibo.com/1866405545/MoICtozcu?type=repost.",
                "url": null
            }
        },
        {
            "39": {
                "title": "HFL at SemEval-2022 Task 8: A Linguistics-inspired Regression Model with Data Augmentation for Multilingual News Similarity. In Proc. of SemEval. 1114\u20131120.",
                "author": "Zihang Xu, Ziqing Yang, Yiming Cui, and Zhigang Chen. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "40": {
                "title": "CHECKED: Chinese COVID-19 Fake News Dataset.",
                "author": "Chen Yang, Xinyi Zhou, and Reza Zafarani. 2021.",
                "venue": "Social Network Analysis and Mining 11, 1 (2021), 58.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Mining Dual Emotion for Fake News Detection. In Proc. of WWW. 3465\u20133476.",
                "author": "Xueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei Zhong, and Kai Shu. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "42": {
                "title": "FaKnow: A Unified Library for Fake News Detection.",
                "author": "Yiyuan Zhu, Yongjun Li, Jialiang Wang, Ming Gao, and Jiali Wei. 2024.",
                "venue": "arXiv preprint arXiv:2401.16441 (2024).",
                "url": null
            }
        },
        {
            "43": {
                "title": "Exploiting Context for Rumour Detection in Social Media. In Social Informatics. 109\u2013123.",
                "author": "Arkaitz Zubiaga, Maria Liakata, and Rob Procter. 2017.",
                "venue": "",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.09092v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.2.4",
            "3.2.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.3",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.2",
            "4.3",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2403.09092v1",
        "paper_title": "MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection",
        "research_background": "**Motivation:**\nThe motivation behind this paper lies in the increasing prevalence and severe impact of fake news consumed through various online sources like social platforms and news websites. This has driven the urgency to develop effective fake news detection mechanisms. While existing Chinese fake news detection datasets focus primarily on single platforms like Weibo, there is a pressing need to capture the diversity of news sources to improve the robustness and applicability of detection models in real-world scenarios.\n\n**Research Problem:**\nThe research problem this paper addresses is the inadequacy of existing Chinese fake news detection datasets, which are limited to single-source data (primarily from Weibo). This limitation hinders the development of robust and generalizable fake news detection models capable of handling news from multiple and diverse sources. Hence, the paper aims to construct a multi-source benchmark dataset for Chinese fake news detection, which will help improve the performance and generalization of fake news detection models.\n\n**Relevant Prior Work:**\n1. **Chinese Fake News Detection Datasets:**\n   - Hu et al. (2022) and Jin et al. (2017a, b) developed datasets for Chinese fake news detection but are limited to single-source data from Weibo.\n   - Other relevant datasets include works by Zhang et al. (2021), Nan et al. (2021), and Yang et al. (2021), which also focus primarily on Weibo as the data source.\n\n2. **Diversity in News Sources:**\n   - Studies by Pierri and Ceri (2019), Bondielli and Marcelloni (2019), and Li et al. (2023a, b) highlight the significance of diverse news sources, noting that fake news exhibits a range of characteristics based on the source.\n\n3. **Challenges with Single-source Data:**\n   - Research by Wang et al. (2023), Przybyla (2020), and A\u00efmeur et al. (2023) shows that reliance on single-source data for training ML models results in poor generalization and robustness, as evidenced by reduced performance when tested with multi-source data.\n\n4. **ML-based Fake News Detection Models:**\n   - BERT-EMO (Zhang et al., 2021) serves as an example of state-of-the-art models trained on single-source datasets but shows decreased efficacy when applied to multi-source fake news detection.\n\nIn summary, the paper addresses the need for a comprehensive, multi-source dataset for Chinese fake news detection, a gap not covered by prior datasets limited to single sources like Weibo.",
        "methodology": "### MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection\n\n**Methodology:** In this section, we introduce our Chinese multi-source fake news detection dataset, MCFEND. Additionally, we perform data analysis to investigate into the differences between various sources.\n\n#### Components of the Dataset:\n1. **Dataset Collection:** The MCFEND dataset comprises samples collected from multiple sources, including social media platforms, news websites, and fact-checking organizations.\n2. **Labeling Process:** Each piece of news in the dataset is labeled with its veracity status by domain experts, ensuring high-quality annotations.\n\n#### Innovations:\n- **Multi-Source Integration:** Unlike existing datasets that are often sourced from a single platform or medium, MCFEND integrates data from various multimedia sources, providing a more comprehensive and diverse dataset for fake news detection.\n- **Data Analysis:** Detailed analysis is conducted on the dataset to understand the unique characteristics and differences between news from different sources. This may include comparisons of text length, linguistic features, misinformation patterns, and propagation profiles.\n\nIn summary, the proposed method or model, MCFEND, significantly contributes to the field of Chinese fake news detection by providing a multi-source, well-annotated dataset. The inclusion of a data analysis component helps to understand the variances among different sources, making it a valuable tool for researchers and practitioners alike.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\nWe conducted experiments to evaluate the performance of representative fake news detection methods on our newly proposed MCFEND dataset. The primary objectives of these experiments were to address several evaluation questions (EQs):\n\n1. **Evaluation Questions**:\n   - **EQ1**: Are existing methods, which have demonstrated effectiveness on the existing Weibo datasets, capable of maintaining their performance when being applied to news collected from different sources?\n   - **EQ2**: Can training with multi-source data enhance the robustness of existing methods in detecting fake news in real-world scenarios, which involve multiple sources?\n   - **EQ3**: Can training with multi-source data enhance the robustness of existing methods in detecting fake news originating from previously unseen news sources?\n\n2. **Datasets**:\n   - The experiments were conducted on the newly proposed **MCFEND dataset**, which is a multi-source benchmark dataset for Chinese fake news detection.\n\n3. **Baselines**:\n   - We evaluated the performance of **representative fake news detection methods** that have demonstrated effectiveness on existing Weibo datasets.\n\n4. **Evaluation Metrics**:\n   - Performance evaluation metrics were not explicitly specified in the text, likely detailed in Appendix C ###reference_###.\n\n5. **Main Experimental Results**:\n   - The performance of each method on the MCFEND dataset was assessed in light of the stated evaluation questions. Specific numerical results and detailed analysis are available in Appendix C ###reference_### of the paper.\n\nNote: The detailed experimental setups and specific results are provided in Appendix C, which should be referred to for the comprehensive understanding of the experimental process and outcomes."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate whether existing fake news detection methods can maintain their performance when applied to news from multiple diverse sources.",
            "experiment_process": "Cross-source evaluations were performed on baseline models trained exclusively on Weibo data, assessing their application to diverse news sources. Accuracy and macro F1 scores were measured, and performance was compared among various baseline models (BERT, RoBERTa, CLIP, CAFE, dEFEND) on different test groups.",
            "result_discussion": "The baseline models exhibited significant performance drops when applied to news from diverse sources compared to Weibo data. BERT-EMO scored highest with an accuracy of 0.821 and macro F1 of 0.818 but showed a significant decrease in performance on non-Weibo groups. The models' robustness against data source variability was found to be limited, with the dEFEND model showing the greatest robustness. The need to reevaluate current methods for practical applications in diverse real-world scenarios was emphasized.",
            "ablation_id": "2403.09092v1.No1"
        },
        {
            "research_objective": "Determine whether training with multi-source data can enhance the robustness of fake news detection methods in real-world scenarios.",
            "experiment_process": "Baseline models were trained with the entire MCFEND dataset, encompassing news from all included sources and evaluated. Performance improvements were gauged by comparing with previous cross-source evaluation data, with a focus on changes in accuracy and macro F1 scores.",
            "result_discussion": "Training with multi-source data led to significant performance improvements across all baseline models. RoBERTa and CAFE saw substantial gains in macro F1 scores. Using the complete MCFEND dataset improved the models' ability to generalize to multiple sources. However, overall performance on multi-source data was still lower than single-source Weibo data, underscoring the challenge of developing universally effective fake news detection algorithms. MCFEND was highlighted as a valuable tool for enhancing method robustness.",
            "ablation_id": "2403.09092v1.No2"
        },
        {
            "research_objective": "Assess the robustness of fake news detection methods in identifying fake news from previously unseen news sources.",
            "experiment_process": "Two versions of the BERT-EMO model were trained on different compositions: Model A exclusively on Weibo data, and Model B on data from various Chinese news sources and Weibo. Both models were then evaluated using data from an unseen source (English news). Macro F1 scores were used to measure performance.",
            "result_discussion": "Model B, trained on a broader dataset, achieved a higher macro F1 score of 0.432, compared to Model A's score of 0.287. This suggests that multi-source training enhances model robustness against new and unseen news sources, supporting the value of diverse training datasets like MCFEND for future applications where new sources emerge without prior data for model training.",
            "ablation_id": "2403.09092v1.No3"
        }
    ]
}