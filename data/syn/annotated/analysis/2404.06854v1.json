{
    "title": "Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata",
    "abstract": "The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model that performs well in Neural Machine Translation. Two issues prevent its application to general Natural Language Generation (NLG) tasks: frequent Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity names. We introduce Control-DAG, a constrained decoding algorithm for our Directed Acyclic T5 (DA-T5) model which offers lexical, vocabulary and length control. We show that Control-DAG significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets, establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text NLG.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Non-autoregressive (NAR) models for text generation offer the promise of much faster generation than auto-regressive (AR) models. However NAR models have been largely developed for Neural Machine Translation (NMT) Xiao et al. (2022  ###reference_b19###), with other Natural Language Generation (NLG) tasks less well studied. We will show how a NAR model developed for NMT, the Directed Acyclic Transformer (DAT) (Huang et al., 2022  ###reference_b5###), can be used for generation in Task-Oriented Dialogue (TOD) and Data-to-Text (D2T) scenarios.\nDATs as originally developed for NMT perform poorly in NLG on TOD and D2T tasks: they fail to generate specified entity names in up to 40% of responses and frequently (>20%) produce Out-Of-Vocabulary (OOV) words. Practical systems must operate at zero error rate in these aspects to be deployable at scale. Previous NAR study reported similar error patterns Xiao et al. (2022  ###reference_b19###). Unless these shortcomings are addressed, NAR models will not be usable for general NLG.\nWe introduce three constrained decoding procedures for NLG using DATs. Our approach converts Directed Acyclic Graphs (DAG) generated by DAT into Weighted Finite State Automata (WFSA). We then intersect these WFSAs with other automata that are defined to ensure that designated entities (lexical constraints) are generated and OOVs are eliminated (vocabulary constraints). To avoid generating responses that are too short, we employ a Viterbi decoding algorithm to control the target length of the generated text (length constraints).\nWe refer to the decoding procedure that incorporates all these steps as Control-DAG. We evaluate extensively on the Schema Guided Dialogue (SGD) (Rastogi et al., 2020  ###reference_b15###) and the Data Record To Text (DART) datasets (Nan et al., 2021  ###reference_b11###) for NLG in TOD and D2T domains. Our Directed Acyclic T5 model, when decoded with Control-DAG, is free from OOV error, faithfully generates all specified entity names, and achieves marked BLEU and BLEURT gains on both datasets.\nWe use pynini Gorman (2016  ###reference_b2###) for WFSA operations.\nOur contributions are summarized below:\nWe introduce Control-DAG, a constrained decoding algorithm which simultaneously offers lexical, vocabulary, and length controls for Directed Acyclic models, addressing key limitations in NAR text generation.\nWe demonstrate the effectiveness of Control-DAG on two major NLG tasks: Task-Oriented Dialogues and Data-to-Text. To our knowledge, DA-T5 with Control-DAG is the first practical NAR benchmark on the SGD and the DART datasets.111Code: https://github.com/EriChen0615/ControlDAG  ###reference_###\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The Directed Acyclic Transformer (DAT) Huang et al. (2022  ###reference_b5###) performs on par with AR baselines in NMT and has attracted much interests. Shao et al. (2022  ###reference_b17###) developed a Viterbi decoding algorithm for DAT. Ma et al. (2023  ###reference_b9###) introduced a fuzzy alignment objective to improve DAT training. In NLG, PreDAT (Huang et al., 2023  ###reference_b4###) pretrains a DAT for open-domain dialogue, notably with high word error rate reported even after extensive pre-training. Our work highlights the links between DATs and automata, and shows well-studied WFSA algorithms Mohri et al. (2002  ###reference_b10###) can be used in constrained decoding to eliminate OOV errors.\nEnforcing lexical constraints in auto-regressive decoding has been studied extensively. Constrained beam search (CBS) Post and Vilar (2018  ###reference_b13###); Hu et al. (2019  ###reference_b3###); Li et al. (2020  ###reference_b8###) is a widely used family of lexically constrained decoding procedure. We show how CBS can be adapted to NAR Directed Acyclic models."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Constrained Decoding with DA-T5",
            "text": "The architecture of our DA-T5 model follows that of the DAT by Huang et al. (2022  ###reference_b5###). Conceptually, DAT takes an input sequence and generates a DAG with a pre-determined number of DAG vertices. Vertex embeddings are produced first, and then token emission probabilities and state transition probabilities are generated from these vertex embeddings via softmax and self-attention, resp. Each vertex has a token emission distribution. These vertices and transitions define a weighted DAG that contains output string hypotheses. DAT uses a vanilla Transformer to produce vertex embeddings whereas we use T5, hence the name DA-T5.\nIn training DA-T5, we use \u2018glancing training\u2019 Qian et al. (2021  ###reference_b14###) as DAT. In inference, DAGs are generated with DA-T5 and converted to WFSAs. The procedure is simply Moore-to-Mealy Machine conversion (Appendix B.1  ###reference_###). Prior to the conversion, we perform likelihood-based pruning of each vertex, keeping  most likely output tokens and  most likely out-going arcs. This pruning balances coverage against decoding speed, with larger thresholds leading to a more complete WFSA at the cost of slower decoding.\nFor each phrase  that must appear in the generation, we construct a constraint FSA  that accepts and only accepts strings where the phrase  appears at least once, corresponding to the regular expression \u201c\u201d IEEE (2004  ###reference_b6###). We then intersect the WFSA converted from the DAG with all of the constraint FSAs. The resulting WFSA  contains only hypotheses that satisfy all lexical constraints.\nWe build a vocabulary FSA  that accepts and only accepts strings of words from a valid vocabulary; intersection with  prevents OOV errors.  is obtained from three FSAs: a dictionary FSA  that accepts and only accepts English words; a special token FSA  that accepts and only accepts numbers, punctuation, and special tokens; and a dynamic FSA  that accepts and only accepts entity names specified in the input. The final vocabulary FSA  is obtained by unioning the three FSAs and taking the Kleene closure (Eq.1  ###reference_###).\nFor efficiency, we perform a one-time determinization and minimization Mohri et al. (2002  ###reference_b10###) of the union () and store the optimized FSA in memory.\nShao et al. (2022  ###reference_b17###) introduced a Viterbi decoding procedure for DAT that finds the highest scoring hypothesis for each string length. We find this exact Viterbi procedure to be impractical because the number of WFSA states can be large (>30,000) after intersection with the constraint FSAs. We introduce a pruned version of this procedure, Depth-First Search Viterbi (DFS-Viterbi). DFS-Viterbi searches the WFSA with DFS and keeps the best hypotheses of all possible string lengths at each vertex to avoid repeated computation. During DFS, we only explore the minimal set of out-going edges such that their cumulative probability is bigger than a threshold . This pruning is inadmissible but works well in practice. We also introduce an exponential length penalty that penalizes strings shorter than target length  and select the hypothesis with the lowest overall costs. In experiments to follow,  is obtained via simple linear regression.\nIn addition to automata-based methods, we introduce CBS-DAG, a constrained beam search algorithm for our NAR DA-T5. CBS-DAG is straight-forwardly adapted from AR CBS by Hu et al. (2019  ###reference_b3###) (Appendix B.4  ###reference_###)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Constrained Decoding",
            "text": "For hard lexical and vocabulary constraints we build corresponding Finite State Automata (FSA). Intersecting the WFSA with these constraint FSAs produces a WFSA that only contains hypotheses that satisfy all constraints Mohri et al. (2002  ###reference_b10###).\nFor length constraints, we propose a pruned version of DAT Viterbi decoding by Shao et al. (2022  ###reference_b17###) to search for strings with specified length. Appendix B  ###reference_### gives implementation details and complexity analyses. Figure 1  ###reference_### illustrates our Control-DAG system with an example.\nFor each phrase  that must appear in the generation, we construct a constraint FSA  that accepts and only accepts strings where the phrase  appears at least once, corresponding to the regular expression \u201c\u201d IEEE (2004  ###reference_b6###  ###reference_b6###). We then intersect the WFSA converted from the DAG with all of the constraint FSAs. The resulting WFSA  contains only hypotheses that satisfy all lexical constraints.\nWe build a vocabulary FSA  that accepts and only accepts strings of words from a valid vocabulary; intersection with  prevents OOV errors.  is obtained from three FSAs: a dictionary FSA  that accepts and only accepts English words; a special token FSA  that accepts and only accepts numbers, punctuation, and special tokens; and a dynamic FSA  that accepts and only accepts entity names specified in the input. The final vocabulary FSA  is obtained by unioning the three FSAs and taking the Kleene closure (Eq.1  ###reference_###  ###reference_###).\nFor efficiency, we perform a one-time determinization and minimization Mohri et al. (2002  ###reference_b10###  ###reference_b10###) of the union () and store the optimized FSA in memory.\nShao et al. (2022  ###reference_b17###  ###reference_b17###) introduced a Viterbi decoding procedure for DAT that finds the highest scoring hypothesis for each string length. We find this exact Viterbi procedure to be impractical because the number of WFSA states can be large (>30,000) after intersection with the constraint FSAs. We introduce a pruned version of this procedure, Depth-First Search Viterbi (DFS-Viterbi). DFS-Viterbi searches the WFSA with DFS and keeps the best hypotheses of all possible string lengths at each vertex to avoid repeated computation. During DFS, we only explore the minimal set of out-going edges such that their cumulative probability is bigger than a threshold . This pruning is inadmissible but works well in practice. We also introduce an exponential length penalty that penalizes strings shorter than target length  and select the hypothesis with the lowest overall costs. In experiments to follow,  is obtained via simple linear regression.\nIn addition to automata-based methods, we introduce CBS-DAG, a constrained beam search algorithm for our NAR DA-T5. CBS-DAG is straight-forwardly adapted from AR CBS by Hu et al. (2019  ###reference_b3###  ###reference_b3###) (Appendix B.4  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments and Results",
            "text": "We evaluate on the SGD and the DART datasets.\nIn SGD, the aim is to generate natural utterances from dialogue actions (e.g., INFORM(destination=Cambridge)) that contain the specified information. DART is a more general data-to-text task that takes triplets of (SUBJECT, RELATION, OBJECT) to generate natural texts. Hyper-parameters and implementation details are in Appendix A  ###reference_###.\nWe use BLEURT Sellam et al. (2020  ###reference_b16###) and BLEU Papineni et al. (2002  ###reference_b12###) to measure text quality relative to ground truth text. We also report the BLEU Brevity Penalty (BP), as a small BP indicates too short generation.\nFor SGD, we use Slot Error Rate (SER) Kale and Rastogi (2020  ###reference_b7###) to evaluate lexical faithfulness. A slot error occurs when a slot value that should be reproduced exactly (e.g., a phone number) is not in the generated text.\nFor DART, we use subjects/objects whose string values are always in the ground-truth training text as hard lexical constraints and propose Exact Occurrence error Rate (EOR) for evaluation. EOR is the percentage of model responses where at least one of the string values from these subjects/objects is missing.\nFor OOV errors, we define neologism rate (NEO) to be the percentage of model\u2019s responses that contain at least one OOV generation.\nWe emphasize that SER, EOR, and OOV are critical metrics as even a small error rate could lead to an intolerable number of misleading responses for systems deployed at scale.\n\u2018Speed up\u2019 is measured against auto-regressive CBS implemented by Li et al. (2020  ###reference_b8###) with batch size of 1 to reflect a realistic NLG system that operates at zero SER/EOR.\nWe train DA-T5 from scratch by glancing training by Qian et al. (2021  ###reference_b14###) on the SGD and the DART datasets for 30 and 50 epochs, respectively. Auto-regressive T5 is trained following Chen et al. (2023  ###reference_b1###).\nWe use  and  for DAG-to-WFSA conversion on SGD and DART, respectively. For LC, we fit a simple linear regression model on the training set to predict the target token length given the input token length.\nDecoding hyper-parameters are determined on the validation sets."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Non-Autoregressive NLG on SGD",
            "text": "Table 1  ###reference_### reports NLG performance on SGD with auto-regressive T5 decoding in Rows 1-2 with greedy and beam search. Although these systems yield high BLEURT and BLEU, they still\ncommit slot errors (SER=0.12%).\nConstrained Beam Search (CBS) eliminates slot errors by forcing the generation of designated slot values, but with longer decoding times (16:05  22:15) and a degradation in BLEU () and BLEURT () compared to unconstrained beam search. This constraint-quality trade-off is also observed in previous study Post and Vilar (2018  ###reference_b13###); See Appendix D  ###reference_### for CBS failure modes. Auto-regressive T5 is completely free from OOV errors (NEO=0.0).\nTurning to non-autogressive NLG, generation with DA-T5 using common decoding methods (greedy, beam search) leads to very high SER (> 20%) and OOV errors in at least 20% of the generated responses (Rows 4, 5). Although our CBS-DAG (Row 6) eliminates SER by design and enhances quality as measured by BLEURT (+3.8) and BLEU (+3.4), its neologism rate is still unusably high (19.2%).\nWe now discuss the performance of our constrained decoding methods.\nUnconstrained WFSA shortest path decoding (Row 7) is as fast as greedy decoding, showing that DAGs can be efficiently converted to WFSAs. However, unconstrained generation directly from the WFSA frequently leads to\nslot errors (SER=34.8%), OOV errors (NEO=12.2%), and a harsh brevity penalty (BP=0.44).\nThese aspects of text quality can be improved individually by constrained decoding (Rows 8-10): Hard Lexical Constrained decoding eliminates slot errors (SER=0); Vocabulary constraints eliminate OOV errors (NEO=0); and Length constrained decoding leads to better text lengths (BP=1.0).\nControl-DAG (Row 11) combines these methods to achieves zero SER and zero neologism rate while satisfying the length requirement and yielding a speed advantage of x1.7 relative to auto-regressive CBS.\nTable 2  ###reference_### shows the performance of using existing decoding procedures developed for DA-Transformer to decode DA-T5 on the SGD dataset. Control-DAG\nhas the overall best BLEU (22.9) and BLEURT (60.0) ."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose Control-DAG for decoding non-autoregressive Directed Acyclic models with lexical, vocabulary, and length constraints, addressing key limitations in NAR text generation. Constrained decoding is efficiently performed via well-studied Weighted Finite State Automata algorithms. DA-T5 with Control-DAG establishes strong NAR results on the Schema Guided Dialogue and the DART datasets, bridging gaps in NAR research."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgement",
            "text": "Jinghong Chen is supported by the Warwick Postgraduate Studentship from Christ\u2019s College and the Huawei Hisilicon Studentship for the undertaking of the PhD in Engineering at the University of Cambridge.\nWeizhe Lin was supported by a Research Studentship funded by Toyota Motor Europe (RG92562(24020)).\nProf. Bill Byrne holds concurrent appointments as a Professor of Information Engineering at Cambridge University and as an Amazon Scholar. This publication describes work performed at Cambridge University and is not associated with Amazon.\nWe would also like to thank all the reviewers for their knowledgeable reviews."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitation",
            "text": "Given our focus on decoding algorithms, we leave further training and model scaling to future work. It is possible to further improve inference speed by writing the DAG-to-WFSA conversion and the DFS-Viterbi algorithm in the C programming language to reduce overhead from the python interface. In this paper, we demonstrate substantial speed-up can be achieved without these optimizations and leaves further speed-up techniques to future work."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethical Statement",
            "text": "We trained two versions of the DA-T5 model: one on the training set of Schema Guided Dialogue and one on the training set of the DART dataset. These are English datasets and do not contain sensitive personal information or offensive language. Detailed statistics of the SGD and DART datasets can be found in Rastogi et al. (2020  ###reference_b15###) and Nan et al. (2021  ###reference_b11###), respectively. We note that the model may hallucinates information or generates language that appears offensive. Some linguistic phenomena of our DA-T5 models are in Appendix D  ###reference_###. It is vital that developers test DA-T5 fully before deployment.\nAll software packages that our code built on are used as their original intention. Our code is released under the MIT license."
        }
    ],
    "url": "http://arxiv.org/html/2404.06854v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.06854v1",
        "paper_title": "Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata",
        "research_background": "### Motivation:\nThe introduction section outlines the motivation behind the research paper as the need to improve non-autoregressive (NAR) models for various Natural Language Generation (NLG) tasks beyond their primary focus on Neural Machine Translation (NMT). Although NAR models promise faster generation compared to auto-regressive (AR) models, they have largely been developed for NMT and have not been sufficiently explored for other NLG tasks like Task-Oriented Dialogue (TOD) and Data-to-Text (D2T). \n\n### Research Problem:\nThe core research problem addressed in the paper is that Directed Acyclic Transformers (DATs), originally developed for NMT, perform inadequately in TOD and D2T tasks. They frequently fail to generate specified entity names correctly and often produce out-of-vocabulary (OOV) words. Addressing these issues is crucial for making NAR models deployable at scale for such NLG tasks.\n\n### Relevant Prior Work:\nThe prior work relevant to the study includes:\n- **DATs for NMT:** The Directed Acyclic Transformer (DAT) developed for NMT by Huang et al. (2022) has been identified as performing poorly when applied to TOD and D2T tasks.\n- **Error Patterns in NAR Models:** Similar error patterns in NAR models have been observed by Xiao et al. (2022), highlighting the broader issue of NAR models generating OOV words and failing to produce specified entities.\n- **WFSA Operations:** The paper mentions prior tools such as pynini by Gorman (2016), which is used for Weighted Finite State Automata (WFSA) operations, a critical part of the proposed solution.\n\n### Proposed Solution:\nThe paper proposes \"Control-DAG,\" a new constrained decoding procedure for NLG using DATs. This approach converts Directed Acyclic Graphs (DAGs) generated by DAT into Weighted Finite State Automata (WFSAs) and intersects these WFSAs with other automata to ensure lexical constraints, eliminate OOV errors, and control the length of the generated text. The procedure aims to make DATs viable for TOD and D2T tasks by addressing the aforementioned shortcomings.\n\n### Contributions:\n1. Introduction of Control-DAG, a constrained decoding algorithm that incorporates lexical, vocabulary, and length control in Directed Acyclic models, thereby addressing key limitations in NAR text generation.\n2. Demonstration of Control-DAG's effectiveness in TOD and D2T tasks, setting a practical Non-Autoregressive benchmark on the Schema Guided Dialogue (SGD) and Data Record To Text (DART) datasets.",
        "methodology": "### Methodology\n\nThe proposed method, Control-DAG, involves using a non-autoregressive version of the T5 model (DA-T5) for generating sequences. The architecture is adapted from the Directed Acyclic Transformer (DAT) by Huang et al. (2022), but uses T5 instead of a vanilla Transformer.\n\n**Key Components:**\n\n1. **Vertex Embeddings and DAG Generation**:\n    - **Vertex Embeddings**: The input sequence is first mapped to vertex embeddings using the T5 architecture.\n    - **Token Emission and State Transition**: Token emission probabilities and state transition probabilities are computed from these vertex embeddings via softmax and self-attention mechanisms, respectively. Each vertex in the DAG has an associated token emission distribution.\n    - **Weighted DAG Construction**: These vertices and their transitions define a weighted Directed Acyclic Graph (DAG) that encapsulates multiple output string hypotheses.\n\n2. **Training and Inference**:\n    - **Training with Glancing Training**: Similar to the method in DAT, DA-T5 uses 'glancing training' by Qian et al. (2021).\n    - **Inference with WFSA Conversion**: During inference, the generated DAGs are converted into Weighted Finite State Automata (WFSA) using Moore-to-Mealy Machine conversion. This conversion helps facilitate subsequent processing steps.\n\n3. **Likelihood-based Pruning**:\n    - Before converting the DAG to WFSA, vertices are pruned based on their likelihood. The most likely output tokens and out-going arcs are retained to balance decoding speed and coverage.\n\n4. **Constraint FSAs**:\n    - For ensuring certain phrases appear in the output, constraint FSAs (\\(\\mathcal{C}_\\text{phrase}\\)) are constructed and intersected with the WFSA. This prunes the WFSA to meet lexical constraints.\n\n5. **Vocabulary FSA**:\n    - A vocabulary FSA (\\(\\mathcal{V}\\)) is constructed to accept only strings composed of valid vocabulary words, thus preventing OOV errors. This FSA is derived from:\n        - A dictionary FSA (\\(\\mathcal{V}_\\text{dict}\\)) for English words.\n        - A special token FSA (\\(\\mathcal{V}_\\text{spec}\\)) for numbers, punctuation, and special tokens.\n        - A dynamic FSA (\\(\\mathcal{V}_\\text{dyn}\\)) for entity names specified in the input.\n    - The final \\(\\mathcal{V}\\) is obtained by taking the union of the three FSAs and applying the Kleene closure for efficiency.\n\n6. **Optimized WFSA Storage**:\n    - The union of the FSAs is determinized and minimized once for storage optimization, following the methods outlined by Mohri et al. (2002).\n\n7. **DFS-Viterbi Decoding**:\n    - Unlike Shao et al. (2022)'s exact Viterbi decoding, which becomes impractical due to large WFSA states, a pruned version, Depth-First Search Viterbi (DFS-Viterbi), is introduced.\n    - **DFS-Viterbi Features**:\n        - Searches the WFSA using DFS while keeping track of the best hypotheses for all string lengths at each vertex.\n        - Explores a minimal set of out-going edges with cumulative probabilities exceeding a threshold.\n        - Employs an exponential length penalty to penalize strings shorter than a target length \\(L\\), determined via linear regression.\n        - **Selection Criteria**: The hypothesis with the lowest overall cost is selected as the final output.\n\n8. **CBS-DAG**:\n    - A constrained beam search algorithm, CBS-DAG, is introduced as an alternative to automata-based methods. It is adapted straightforwardly from the autoregressive CBS by Hu et al. (2019).\n\n**Innovations**:\n- Integrating T5 into the original DAT framework.\n- Introducing pruning strategies to enhance efficiency.\n- Combining constraint-based finite state automata methods with non-autoregressive sequence generation.\n- DFS-Viterbi for optimized hypothesis selection under resource constraints.\n- CBS-DAG as a novel adaptation for non-autoregressive models.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\n- **SGD (Schema-Guided Dialogue)**:\n  - Objective: Generate natural utterances from dialogue actions (e.g., INFORM(destination=Cambridge)) containing the specified information.\n- **DART**:\n  - Objective: General data-to-text task converting triplets of (SUBJECT, RELATION, OBJECT) into natural text.\n\n#### Evaluation Metrics\n1. **BLEURT** (Sellam et al., 2020)\n2. **BLEU** (Papineni et al., 2002)\n3. **BLEU Brevity Penalty (BP)**: Assesses text length, where a small BP indicates overly short generations.\n4. **Slot Error Rate (SER)** (Kale and Rastogi, 2020) for SGD: Measures lexical faithfulness by identifying slot errors where exact reproduction is required (e.g., phone numbers).\n5. **Exact Occurrence error Rate (EOR)** for DART: Percentage of model responses missing string values from subjects/objects that are always in ground-truth training text.\n6. **Neologism Rate (NEO)**: Percentage of responses containing out-of-vocabulary (OOV) generations.\n7. **Speed up**: Compared against auto-regressive CBS (Li et al., 2020) with batch size of 1.\n\n#### Training Details\n- **DA-T5 Model**:\n  - Trained from scratch using glancing training (Qian et al., 2021) for 30 epochs on SGD and 50 epochs on DART.\n  - Auto-regressive T5 trained following Chen et al. (2023).\n- **Decoding Details**:\n  - DAG-to-WFSA conversion specific to each dataset.\n  - Linear regression model for predicting target token length from input token length.\n\n#### Results\n- **SER/EOR/OOV Metrics**:\n  - These are critical as small error rates can lead to a significant number of misleading responses in large-scale deployments.\n- **Performance**:\n  - Improved speed compared to auto-regressive CBS, highlighting efficiency in practical NLG systems operating at zero SER/EOR.\n\nThe experiment setup demonstrates the utility of Control-DAG for efficient and high-quality non-autoregressive generation in data-to-text tasks. Specific quantitative results for BLEURT, BLEU, SER, EOR, and NEO, as well as speed-up ratios, are detailed in tables referenced in the full paper."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of Control-DAG on non-autoregressive natural language generation (NLG) tasks, specifically on the Schema Guided Dialogue (SGD) dataset, and to compare it with other decoding methods.",
            "experiment_process": "The experiments were conducted on the SGD dataset with different decoding methods. Rows 1-2 used auto-regressive T5 decoding with greedy and beam search. Constraints such as Constrained Beam Search (CBS) were tested. Rows 4-10 tested various non-autoregressive methods using DA-T5. The specific methods tested included greedy decoding, beam search, CBS-DAG, WFSA shortest path decoding, and individual constraints like Hard Lexical Constrained decoding, Vocabulary constraints, and Length constrained decoding. The performance was evaluated using metrics like BLEURT, BLEU, slot error rate (SER), and neologism rate (NEO).",
            "result_discussion": "Control-DAG (Row 11) achieved the best overall performance with zero slot errors (SER=0) and zero neologism rate (NEO=0), while also satisfying length requirements. It also provided a speed advantage of 1.7x relative to auto-regressive CBS. This method had the highest BLEU (22.9) and BLEURT (60.0) scores among the non-autoregressive methods tested.",
            "ablation_id": "2404.06854v1.No1"
        },
        {
            "research_objective": "To validate the performance of Control-DAG on the DART dataset and to compare its effectiveness for different NLG tasks.",
            "experiment_process": "The experiments extended the use of Control-DAG to the DART dataset, comparing its performance against other decoding methods. The vocabulary complexity and other characteristics of DART were considered, and the process involved less aggressive pruning (top-5) compared to SGD (top-3). A simple procedure of searching the training data for subjects/objects exactly reproduced and using them as lexical constraints was also tested to evaluate its effectiveness.",
            "result_discussion": "Control-DAG reaffirmed its superior performance on the DART dataset, showing the best results while maintaining a speed advantage. The hard lexical constraints method boosted performance by +4.7 BLEURT and +3.6 BLEU (Row 8). This demonstrated that hard lexical constraints are effective and easy to apply for less lexically constrained NLG tasks such as DART.",
            "ablation_id": "2404.06854v1.No2"
        }
    ]
}