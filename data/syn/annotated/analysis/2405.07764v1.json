{
    "title": "LGDE: Local Graph-based Dictionary Expansion",
    "abstract": "Expanding a dictionary of pre-selected keywords is crucial for tasks in information retrieval, such as database query and online data collection. Here we propose Local Graph-based Dictionary Expansion (LGDE), a method that uses tools from manifold learning and network science for the data-driven discovery of keywords starting from a seed dictionary. At the heart of LGDE lies the creation of a word similarity graph derived from word embeddings and the application of local community detection based on graph diffusion to discover semantic neighbourhoods of pre-defined seed keywords.\nThe diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings and can capture word similarities based on paths of semantic association.\nWe validate our method on a corpus of hate speech-related posts from Reddit and Gab and show that LGDE enriches the list of keywords and achieves significantly better performance than threshold methods based on direct word similarities. We further demonstrate the potential of our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on data collected and analysed by domain experts by expanding a conspiracy-related dictionary.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Dictionary expansion aims to expand a set of pre-selected keywords by adding related terms that can enhance original queries in keyword-based information retrieval tasks. Designing a dictionary without in-depth knowledge of the vocabulary in the domain of interest is prone to inaccurate, non-specific or incomplete results. Therefore, expert-generated seed dictionaries are typically expanded with domain-specific keywords for diverse applications, such as patent searches (Lee et al., 2014  ###reference_b22###), queries of bibliometric databases (Yin et al., 2020  ###reference_b41###) and online forums (Gharibshah et al., 2022  ###reference_b16###), query expansion for more effective web searches (Roy et al., 2016  ###reference_b31###; Kuzi et al., 2016  ###reference_b21###), or collecting topic-specific content from social media platforms (Bruns et al., 2020  ###reference_b6###; Zeng and Sch\u00e4fer, 2021  ###reference_b43###; Klinger et al., 2022  ###reference_b20###; van Atteveldt et al., 2022  ###reference_b3###). Dictionary expansion is particularly relevant, and challenging, in domains with evolving semantics where word choice and language style are highly specialised and diverge from general language usage or are constantly in flux to ensure exclusive, community-internal communication, to adjust to ongoing events or the emergence of topics and cultural changes, or to avoid legal prosecution (Heft et al., 2023  ###reference_b17###).\nWhen retrieving information around a certain topic, the challenge becomes to find a \u2018good\u2019 dictionary that leads to a corpus containing most documents associated with the topic (high recall) and with few irrelevant documents (high precision). New approaches for data-driven dictionary expansion have leveraged word embedding models to find semantically similar words to pre-selected keywords (Roy et al., 2016  ###reference_b31###; Amsler, 2020  ###reference_b2###; Gharibshah et al., 2022  ###reference_b16###; van Atteveldt et al., 2022  ###reference_b3###; Stoll et al., 2023  ###reference_b36###). While systems based on Large Language Models (LLMs) could also be used for dictionary expansion (Jagerman et al., 2023  ###reference_b18###; Wang et al., 2023  ###reference_b40###; Lei et al., 2024  ###reference_b23###), their application is prohibited in certain domains like hate speech or conspiracy-related communication due to strict moderation filters.\nIn this work, we build on the idea of data-driven dictionaries but rather than focusing only on words most directly similar to pre-selected keywords, we propose Local Graph-based Dictionary Expansion (LGDE), a method that incorporates tools from manifold learning and network science to explore a graph of semantic similarities built from a domain-specific word level representation. LGDE expands a pre-selected set of keywords by adding words from their corresponding semantic communities, as determined through fast local community detection in the semantic network (Yu et al., 2020  ###reference_b42###).\nThrough a graph-based manifold representation, LGDE thus captures the local nonlinear geometry of domain-specific word embeddings around seed keywords, and then exploits graph diffusion to find local semantic communities that naturally include multi-step word associations.\nTo evaluate our method, we consider the task of expanding a dictionary of pre-selected keywords from a human-coded hate speech dataset from the social media platforms Reddit111https://reddit.com  ###reference_reddit.com### and Gab222https://gab.com  ###reference_gab.com### (Qian et al., 2019  ###reference_b29###). As compared to approaches based on direct word similarities, LGDE leads to a better-expanded dictionary in terms of  scores with discovered words significantly more likely to appear in hate speech-related communication. To further showcase the potential of LGDE in a real-world use case, we analyze conspiracy-related posts from the message forum 4chan333https://4chan.org  ###reference_4chan.org### collected through an expert-selected dictionary representative of two conspiracy theories (\u2018Great Replacement\u2019 and \u2018New World Order\u2019). In this case, LGDE shows a quantitative advantage in discovering additional relevant words that would be missed without the chain of word associations"
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Problem definition",
            "text": "Let us consider a pre-selected list of  keywords , denoted the seed dictionary, which are known to be relevant to a certain topic.\nThese initial terms are usually derived from expert knowledge, literature research or existing keyword lists (Gharibshah et al., 2022  ###reference_b16###; Heft et al., 2023  ###reference_b17###). Let us further assume that we have access to a domain-specific corpus of  documents  related to the topic of interest, and each keyword in  is contained in at least one document.We can then formulate the dictionary expansion problem: expand the seed dictionary  by  new words from the domain-specific corpus  to obtain a data-driven expanded dictionary\n such that\nthe newly discovered keywords make\n \u2018more representative\u2019 of the topics of interest as measured by evaluation metrics such as the  score that balances precision and recall."
        },
        {
            "section_id": "1.2",
            "parent_section_id": "1",
            "section_name": "Related work",
            "text": "Keyword extraction and query expansion are related tasks but not the same as dictionary expansion. The former refers to the extraction of representative keywords from text without an initial seed dictionary (Firoozeh et al., 2020  ###reference_b14###), whereas the latter relies on user input on query words or phrases, possibly expanding the current query term or suggesting additional terms (Sch\u00fctze et al., 2008  ###reference_b35###). These tasks have been studied mostly in the context of information retrieval in search engines and often involve relevance user feedback on retrieved documents (Zheng et al., 2020  ###reference_b44###).\nHere, we use semantic relationships captured in the latent representation space and focus on generating relevant keyword or query terms based on curated seed keywords, without explicit user feedback. Early statistical approaches were based on ranking candidate keywords using TF-IDF or TextRank (Mihalcea and Tarau, 2004  ###reference_b25###) or analysing word co-occurrences directly (Yin et al., 2020  ###reference_b41###).\nPromising work has also leveraged pre-trained word embeddings to expand a seed dictionary by most similar words (Amsler, 2020  ###reference_b2###; Gharibshah et al., 2022  ###reference_b16###; van Atteveldt et al., 2022  ###reference_b3###; Stoll et al., 2023  ###reference_b36###). Prior work also suggests that global word embeddings may underperform in tasks that benefit from local properties Diaz et al. (2016  ###reference_b11###). Farmanbar et al. (2020  ###reference_b13###) explore domain-specific query terms but focus on important challenges in end-to-end production pipelines using direct cosine similarities. Tsai and Wang (2014  ###reference_b37###) and Gharibshah et al. (2022  ###reference_b16###) use similar methods adapted to custom domains. The latter are close to our domain-specific setting but do not employ the properties of the semantic network."
        },
        {
            "section_id": "1.3",
            "parent_section_id": "1",
            "section_name": "Motivation",
            "text": "A data-driven augmented dictionary can be constructed by adding words from a domain-specific vocabulary  of word embedding vectors that are most similar to the keywords in the seed dictionary  according to the cosine similarity , given by  for two words . For a threshold , the thresholding-based expanded dictionary  is defined as\nChoosing the parameter  appropriately can enrich the seed dictionary by considering direct word associations. To further improve the quality of the expanded dictionary, one can fine-tune the word embeddings in  on a domain-specific corpus  to better capture contextual semantics.\nAn issue not addressed by thresholding approaches is that direct similarities can be uninformative in noisy, latent space representations, such as word embeddings. This can lead to relatively unspecific word associations in text. One way to circumvent this limitation is to construct geometric graphs that capture the local manifold and explore it via diffusion. This allows us to consider chains of word associations as paths on the graph. Indeed, a seed keyword  could be similar to a word  (with ) which is in turn similar to another word  (with ), yet we might have low direct similarity , reminiscent of cosine similarity not fulfilling the standard triangle inequality (Schubert, 2021  ###reference_b34###). A method based only on direct similarities would then exclude the word  from the data-driven dictionary , although the chain of strong word associations arguably makes  a sensible candidate for dictionary expansion. Similar problems may also occur when adding the  most similar words for each seed keyword. LGDE uses tools from manifold learning and network scienceto account for such chains of word associations to better capture the local nonlinear geometry of seed keywords, (see Figure 1  ###reference_### for an illustration).\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "LGDE consists of three steps. In the first step, we derive vectors for the words in our corpus using a fine-tuned word embedding model. In the second step, we compute a similarity graph from the word vectors that captures the local semantic similarities of the corpus. In the third step, we use local community detection based on graph diffusion to obtain semantic communities of each seed keyword as candidates for dictionary expansion. We detail these steps in the following subsections and also describe a validation strategy for our method."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Fine-tuned word representations",
            "text": "As a starting point, we use GloVe as base word embeddings, generated from general-purpose corpora like Wikipedia 2014 and Gigaword 5  (Pennington et al., 2014  ###reference_b26###). The base GloVe embeddings are available in different dimensions . It is well-known that word embeddings are dependent on the corpus they are trained on. Given that terms often adopt new or additional meanings across domains or over time\n(e.g., \u201cApple\u201d may refer to a concept in technology or business domains that came into existence only in the 1970s),\nwe use Mittens  (Dingwall and Potts, 2018  ###reference_b12###) to fine-tune GloVe representations to better represent the semantic relationships in our use-case domains. For a set of  words in our domain-specific corpus , the word embeddings , , are computed from a retrofitting model that optimises the Mittens cost function\nwhere  is the standard GloVe cost function (Pennington et al., 2014  ###reference_b26###) based on the word co-occurrences in ,  is the index set of words for which pre-trained vector representations  are available, and  is the hyperparameter that determines the extent of fine-tuning. A small  favors greater adaptation of the word representations  whereas a large  favors remaining closer to the base embeddings . Setting  means that the word vectors are essentially computed from scratch on the the new corpus. Although the value  was the default used by Dingwall and Potts (2018  ###reference_b12###), we find that a larger value of  can improve the quality of embeddings for a small domain-specific corpus .\nBy training the Mittens model, we compute fine-tuned -dimensional word vectors , for , and we assume . While Schnabel et al. (2015  ###reference_b33###) have shown that the length of the word vectors may encode term frequency, we normalize the word vectors to unit length, as the length carries no semantic meaning in our case.\nRegarding our choice of GloVe and Mittens, it is worth noting that BERT-based (Devlin et al., 2019  ###reference_b10###) models rely on subword tokenization and, typically, word embeddings are extracted by summing or averaging the subword token embeddings, a heuristic that often degrades the word-level semantic network. For instance, a Semeval task on diachronic semantics (Schlechtweg et al., 2020  ###reference_b32###) showed that static or type-based embeddings outperformed contextualized embeddings such as BERT or ELMo (Peters et al., 2018  ###reference_b27###). It is also worth pointing out that BERT-style models are designed for supervised end-to-end fine-tuning, and not for extracting intermediate embedding layers. Previous studies on obtaining representations from BERT-like models (Vuli\u0107 et al., 2020  ###reference_b39###) have shown the various issues with representations produced by averaging hidden layers. These are highly task- and domain-dependent and there are no techniques to select a single layer that is reliably better (Bommasani et al., 2020  ###reference_b5###). Work on hidden layer selection for good word-level representations may be an interesting direction but is not the focus of this work."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Semantic network construction",
            "text": "The analysis of the semantic space  is often restricted to the computation of pairwise cosine similarities between words. To explore the full semantic space of our vocabulary  with size , we construct an undirected, weighted semantic similarity graph , where the nodes correspond to the words in . The weighted edges  are computed from the  matrix of normalised cosine distances:\nwhere  is the cosine similarity and  is the element-wise normalisation with the max-norm so that all elements of  are normalised between . We also define the matrix of normalised cosine similarities (Altuncu et al., 2019  ###reference_b1###):\nWe would like to obtain a semantic network with edges weighted by the similarity  but  is a dense matrix that contains many small values corresponding to negligible word similarities. To uncover a robust semantic network that only contains the most relevant chains of semantic similarities, we first obtain the undirected and unweighted Continuous -Nearest Neighbors (CkNN) graph from the distance matrix  (Berry and Sauer, 2019  ###reference_b4###). The  adjacency matrix , of the CkNN graph is given by:\nwhere  denote the -th nearest neighbours of , respectively, and  controls the graph density.\nIn contrast with a -Nearest Neighbors (kNN) graph, which does not account for inhomogeneities in the data as it connects a node to all of its  nearest neighbours, the CkNN construction corrects for different densities and has been shown to approximate consistently the geometry of complex manifolds embedded in a Euclidean space (Berry and Sauer, 2019  ###reference_b4###). Note that  is equivalent to using Euclidean distances of normalised word vectors when . In that case, and assuming , we have:\nwhich follows directly from:\nwhere  is a constant. Moreover, empirical studies have shown that CkNN with  and adequate choice of  outperforms other graph constructions for downstream tasks such as data clustering (Liu and Barahona, 2020  ###reference_b24###) and classification (Qian et al., 2021  ###reference_b30###).\nFinally, we can define the weighted semantic similarity network  with adjacency matrix\nwhere  denotes the element-wise (Hadamard) product. Therefore, the edge weights of the semantic network  are given by the normalised semantic similarity , and its backbone is the sparse CkNN graph  that preserves the topological and local geometric features of the semantic space ."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Semantic community detection",
            "text": "The constructed graph  encodes the semantic information contained in our domain-specific corpus  at a word-level representation with the inter-word weighted edges  capturing relevant semantic similarities between words. Hence, paths in the graph can be interpreted as chains of word associations. Moreover, the keywords in the seed dictionary  are nodes in the graph and we can study their context and related words by computing their local semantic community structure. To do so, we use the severability method for fast local community detection (Yu et al., 2020  ###reference_b42###) and determine the semantic community  for each seed keyword . Severability is a community detection algorithm that detects local graph communities by exploiting a discrete-time random walk with transition probability matrix , where  is the probability of the random walk jumping from word  to  given by:\nThe semantic community  of  is then the subset  (with ) that maximises the severability quality function  for the time scale :\nwhere the mixing term  measures the mixing of the random walker within  over time  and the retention term  quantifies the probability of the random walker not escaping  by time  (see Yu et al. (2020  ###reference_b42###) for details). As  increases, we need a larger-sized community  to trap the random walk and achieve high retention , but simultaneously, increasing the size of  makes mixing more difficult and leads to a reduced .\nTo indicate the dependency of the semantic community on  and the CkNN parameter  we write .\nImportantly, severability captures chains of word associations (through paths in the random walk) and allows for overlapping semantic communities (potentially capturing polysemy, if present).\nThe result of the LGDE method is then the extended dictionary  defined as the union of (overlapping) local semantic communities:\nBy construction , and we can expect that the size of  generally grows with increasing \nsuch that in the set-theoretic limit, we have . Importantly, our extended dictionary can include words that are connected to a seed keyword  via a chain of strong word associations."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Evaluation of expanded dictionaries",
            "text": "Consider a domain-specific corpus of  documents  with ground-truth (human-coded) labels  such that  if document  has a certain topic of interest (true) and  otherwise (false). To assess the quality of a data-driven dictionary  we can evaluate the performance of a simple document classifier  associated with , where we define  if there exists a keyword in  that appears in the document  and  otherwise. To evaluate the trade-off between precision and recall of the dictionary  on the potentially unbalanced benchmark data , we compute the macro  score of its associated classifier  and denote this number by . Similarly,  denotes the macro precision and  the macro recall. This evaluation strategy can also be used to train the hyperparameters  in the case of the dictionary  from Eq. (1  ###reference_###) or  in the case of the LGDE dictionary  from Eq. (9  ###reference_###) on a train split of the benchmark data.\nIt is also possible to evaluate the contribution of a single keyword  to the performance of the dictionary . Let us consider the probability  that the word  appears in a true document and  that it appears in a false document. Then we can define the likelihood ratio (LR) (van der Helm and Hische, 1979  ###reference_b38###) of word  as\nwhich is larger than 1 if  is more likely to appear in true documents than in false ones. Words with larger LR can thus be considered to be more representative of true documents. The median LR for all words  denoted by  can be used to summarise the individual contributions of keywords to the performance of the dictionary.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Hate speech data",
            "text": "We use a benchmark dataset of 56,099 manually annotated hate speech posts  collected from Reddit and Gab444The microblogging platform Gab is known for its far-right content (Zeng and Sch\u00e4fer, 2021  ###reference_b43###). by Qian et al. (2019  ###reference_b29###),555Data available at: https://github.com/jing-qian/A-Benchmark-Dataset-for-Learning-to-Intervene-in-Online-Hate-Speech  ###reference_ataset-for-Learning-to-Intervene-in-Online-Hate-Speech### of which 19,873 (35.4%) are hate speech666Facebook\u2019s definition of hate speech was used, see Qian et al. (2019  ###reference_b29###).\nWe split the data into train data (75%) and test data (25%) using stratification. We follow Qian et al. (2019  ###reference_b29###) to choose our seed dictionary  as the five most frequent keywords in the benchmark dataset\u2014\u201cni**er\u201d, \u201cfa**ot\u201d, \u201cret**d\u201d, \u201cret***ed\u201d and \u201cc**t\u201d."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Experimental setup",
            "text": "As outlined in Section 2.1  ###reference_###, we first compute fine-tuned word embeddings , filtering out stopwords, infrequent misspellings and very rare words, such that . We found that choosing a relatively large value  produces better-quality word embeddings for various dimensions , where embeddings for out-of-vocabulary (OOV) words are learnt while retaining the quality of pre-existing word embeddings. From the semantic space  we compute the  matrix of normalised cosine similarities  (4  ###reference_###).\nFor each dimension , we then compare the expansion of the seed dictionary  using the thresholding-based expanded dictionary  from Eq. (1  ###reference_###) (derived from ) with the LGDE dictionary  from Eq. (9  ###reference_###). To control for the size of the different dictionary expansion methods, we require that an expanded dictionary should include at least 30 keywords to facilitate proper comparison but no more than 50 for presentation purposes. Under these constraints, we fine-tune the hyperparameters  of  and  of  on the train split of the benchmark data (see Figure 3  ###reference_### and Table 6  ###reference_### in the Appendix for additional information and optimal hyperparameters). We find that LGDE consistently outperforms thresholding across different sizes of the expanded dictionary\n(Figure 2  ###reference_###). We then evaluate the macro F1-scores of the optimised dictionaries on the test split. Finally, we evaluate the individual contributions of keywords in the discovered dictionaries using the likelihood ratio (LR), see Eq. (10  ###reference_###), comparing the optimal LGDE dictionary with the thresholding-based dictionary of the closest size."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "Table 1  ###reference_### shows that the optimal LGDE dictionaries  outperform the optimal thresholding-based dictionaries  for all dimensions of the word embeddings,with the overall best dictionary achieved with LGDE at dimension . The thresholding-based dictionaries  do not improve upon the performance of the bare seed dictionary  because of the inclusion of many non-relevant terms.\nA qualitative assessment of the discovered words\n(Table 2  ###reference_###) shows that LGDE discovers more relevant keywords that lead to hate speech documents, whereas thresholding often produces expected offensive, but largely standard, words without bringing in new semantic context. Some examples of relevant keywords only found by LGDE include \u201cbe**er\u201d (a racist term for Hispanic men), \u201ch*g\u201d (a misogynist term for older women), \u201ctr**ny\u201d (a transphobic term for a transgender person) and \u201cgo**m\u201d (an antisemitic term for a Jewish person). These derogatory terms, including neologisms and online slang, are part of an informal jargon and are potentially difficult to anticipate beforehand without being part of the online community under investigation. To discover these terms with the thresholding-based approach requires choosing smaller than optimal values of  and comes with the price of adding many irrelevant terms to  such that the overall  performance is reduced. For example, discovering the term \u201ctr**ny\u201d at dimension  requires  such that  with ; at dimension  it requires  such that  with .\nTable 3  ###reference_### shows that the median LR for words only discovered by LGDE is significantly higher than the median LR for words only discovered by thresholding, i.e, , and the result is statistically significant for all dimensions (, Mann-Whitney U test). This matches our qualitative assessment that LGDE discovers words more representative of hate speech-related content."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Application to conspiracy-related content on 4chan",
            "text": "As a further illustration in a real-world use case, we apply LGDE to the problem of collecting conspiracy-related 4chan posts. All content on 4chan is ephemeral and together with complete user anonymity and the absence of content moderation (De Zeeuw and Tuters, 2020  ###reference_b9###) a highly vernacular user culture has developed, which can be partly characterised by its racist and misogynous content. Detecting conspiracy-related content, defined as \u201cthe full public communication on and about (alleged) conspiracy theories in various communication venues, including their narrations, counter-narrations, and debunking, as well as neutral observation forms\u201d (Heft et al., 2023  ###reference_b17###, p. 3), can be challenging in this environment as participants use slang and insider humour to explicitly distinguish themselves from a perceived out-group (De Zeeuw et al., 2020  ###reference_b8###). Therefore, common words used in public debate or scientific literature to describe specific conspiracy theories might deviate from the vocabulary used by 4chan users. Furthermore, the vocabulary used to describe specific conspiracy theories might change over time, as well as the conspirational narratives themselves, when new events lead to adaptations of conspiracy theories or when new conspiracy theories emerge and are included in the existing canon (Garry et al., 2021  ###reference_b15###; Heft et al., 2023  ###reference_b17###).\nUsing only a literature-based dictionary to retrieve conspiracy-related posts from 4chan is thus insufficient to collect a longitudinal dataset of relevant user comments posted to this platform, rendering a sophisticated method of dictionary expansion necessary. Starting from an expert-selected seed dictionary, we show that LGDE discovers new conspiracy-related words that would be missed without a graph-based perspective."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data",
            "text": "We assemble an initial seed dictionary  to be representative of two conspiracy theories (\u2018Great Replacement\u2019 and \u2018New World Order\u2019) with 215 keywords including \u201cwhite genocide\u201d, \u201cIlluminati\u201d etc. based on the RPC-Lex dictionary (Puschmann et al., 2022  ###reference_b28###) and other relevant literature (full list in Table 7  ###reference_###). Using the fouRplebsAPI (Buehling, 2022  ###reference_b7###), we collect all English language posts published in 22 sample weeks (2 weeks in each year from 2011 to 2021) on 4chan\u2019s political discussion board /pol/777https://boards.4chan.org/pol/  ###reference_boards.4chan.org/pol/### leading to a corpus  with  unique documents. Since many conspiracy-related keywords, such as \u201cgreat replacement\u201d, are multi-word phrases, we pre-process the input to include hyphenated terms and noun phrases.\nFor evaluation and to determine optimal hyper-parameters for dictionary expansion with thresholding and LGDE we prepared human-coded benchmark data. As training data, we take a sample of 500 documents from , which was labelled according to the majority vote of three independent human coders (trained student assistants), and we find that 65 documents (13.0%) are conspiracy-related.\nWe also collected test data independent of  by first sampling a large number of random posts from 4chan and then oversampling conspiracy-related documents. The test data consists of 225 documents of which 69 are conspiracy-related (34.5%) according to the majority vote of the three independent human coders."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experimental setup",
            "text": "We restrict our analysis to the 5000 most frequent words in  (excluding stop words but including our seed keywords) denoted by . We then compute fine-tuned word embeddings  from\nour domain-specific corpus , starting from pre-trained 100-dimensional GloVe base embeddings, and we use the default value  as our corpus is reasonably large (Dingwall and Potts, 2018  ###reference_b12###). Furthermore, we define the effective seed dictionary  as the 109 seed keywords that actually appear in the corpus.\nNext, we compare the expansion of the seed dictionary  using the thresholding-based expanded dictionary  to the LGDE dictionary \nboth with a maximum of 150 discovered keywords. We perform hyperparameter tuning to obtain the optimal dictionaries but only evaluate the performance of the discovered words  and , as our human-coded train data was collected using the seed dictionary  (see Table 6  ###reference_### in the Appendix for optimal hyperparameters).\nTo assess the discovered words, three domain experts independently carried out blind annotation of whether discovered terms obtained by both methods (ordered randomly) are suitable to be used as a keyword to search for conspiracy-related content on 4chan as defined above."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Discussion",
            "text": "In this work, we have proposed the LGDE framework for data-driven discovery of new keywords that are semantically similar to a pre-defined seed dictionary. Using tools from manifold learning and network science allows us to capture the complex nonlinear geometry of word embeddings and to not only find most similar words but also chains of word associations.\nOn the task of expanding a seed dictionary of the most frequent hate speech-related keywords, we found that LGDE performs significantly better than a simple thresholding-based approach. In particular, LGDE can take advantage of higher dimensional word embeddings (with richer information in principle) as indicated by higher -scores and likelihood ratios for -dimensional word embeddings. In contrast, the thresholding-based approach performs worse as the dimensionality is increased. This suggests that LGDE as a manifold learning method better captures the complex nonlinear geometry of high dimensional word embedding spaces, whereas a thresholding-based approach suffers more from the curse of dimensionality. Moreover, in a real-world data collection use case from communication science on a corpus of conspiracy-related 4chan posts, LGDE outperforms thresholding in expanding a curated list of conspiracy-related keywords by platform-specific keywords. Across tasks, the terms provided by LGDE contained a larger variety of formal and informal language, resulting in a heterogeneous set of keywords that represent the neologisms and informal register specific to the corpora under study. This makes LGDE especially informative in cases where researchers cannot assume a comprehensive knowledge of the lexical variety of the object of study.\nStudies suggest that the space of word embeddings is at least close to a manifold, e.g., a \u2018pinched\u2019 manifold  (Jakubowski et al., 2020  ###reference_b19###). The construction of a CkNN graph from the word vector embeddings can capture the geometry of complex non-linear manifolds in a manner that is consistent with the geometry of the original space. In particular, Berry and Sauer (2019  ###reference_b4###) show that the CkNN graph is consistent in the sense that its unnormalised graph Laplacian, which determines the properties of graph diffusions, converges to the Laplace-Beltrami operator in the limit of large data. This preservation of diffusion properties further justifies the subsequent use of the severability method for local community detection (Yu et al., 2020  ###reference_b42###), which is also based on graph diffusion."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Limitations and future work",
            "text": "We list some limitations in the current work on which we would like to expand in future research. While the experiments employ only English language data, our method is general and the application could be useful for similar data in other languages. It would also be interesting to see the adaptation and evolution of terminology in other domains, e.g., in the scientific literature in relation with the emergence of new sub-disciplines or research areas. Although qualitative assessment is invaluable, the process of manual annotation can be slow and costly and, in particular, labelling hate speech- or conspiracy-related content can pose severe mental health risks to human annotators. In future work, we would like to use LGDE as part of mixed methods approaches  (Puschmann et al., 2022  ###reference_b28###) applied to other specialised domains. Specifically, it would be interesting to evaluate the applicability of LGDE to specialised word disambiguation tasks since we observe preliminary evidence of polysemy being captured through overlapping semantic communities (Yu et al., 2020  ###reference_b42###)."
        }
    ],
    "url": "http://arxiv.org/html/2405.07764v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1",
            "1.2",
            "1.3"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.07764v1",
        "paper_title": "LGDE: Local Graph-based Dictionary Expansion",
        "research_background": "**Motivation:**\nThe main motivation behind this paper is to improve the effectiveness of dictionary expansion for keyword-based information retrieval tasks, particularly in domains with evolving semantics where language use is specialized and constantly changing. Current methods often focus on finding words that are directly similar to pre-selected keywords, but this might not capture the full range of relevant terms, especially in highly specialized domains where vocabulary and semantics can be complex and fluid.\n\n**Research Problem:**\nThe research problem addressed in this paper is how to design a more effective dictionary expansion approach that can capture a broader set of relevant terms while maintaining high precision and recall in information retrieval tasks. Specifically, how can tools from manifold learning and network science be leveraged to better explore semantic similarities and community structures around pre-selected keywords in a domain-specific context?\n\n**Relevant Prior Work:**\n1. **Expert-generated vs Data-driven Approaches:** Traditionally, dictionaries are expanded with domain-specific keywords derived from expert knowledge, but this can be error-prone and incomplete (Lee et al., 2014; Yin et al., 2020; Gharibshah et al., 2022; Roy et al., 2016; Kuzi et al., 2016; Bruns et al., 2020; Zeng and Sch\u00e4fer, 2021; Klinger et al., 2022; van Atteveldt et al., 2022; Heft et al., 2023).\n\n2. **Word Embedding Models:** Recent data-driven approaches leverage word embedding models to find semantically similar words to pre-selected keywords (Roy et al., 2016; Amsler, 2020; Gharibshah et al., 2022; van Atteveldt et al., 2022; Stoll et al., 2023).\n\n3. **Large Language Models (LLMs):** While LLMs could potentially be applied for dictionary expansion, their use is limited in certain domains such as hate speech or conspiracy theories due to moderation filters (Jagerman et al., 2023; Wang et al., 2023; Lei et al., 2024).\n\n4. **Graph-based Methods:** The approach proposed in this paper builds on the idea of using semantic similarities but introduces a novel method called Local Graph-based Dictionary Expansion (LGDE). This method explores a graph of semantic similarities using manifold learning and local community detection (Yu et al., 2020), aiming to extend keyword sets by finding related terms within semantic communities.\n\nThrough these references and prior approaches, the paper situates its contribution as a more nuanced method for dictionary expansion, capable of uncovering intricate and contextually relevant word associations through graph diffusion and local community detection in semantic networks.",
        "methodology": "LGDE: Local Graph-based Dictionary Expansion\n\n**Methodology:** LGDE consists of three steps.\n\n1. **Deriving Word Vectors:** \n   - In the first step, we derive vectors for the words in our corpus using a fine-tuned word embedding model.\n\n2. **Computing a Similarity Graph:**\n   - In the second step, we compute a similarity graph from the word vectors that captures the local semantic similarities of the corpus.\n\n3. **Local Community Detection:**\n   - In the third step, we use local community detection based on graph diffusion to obtain semantic communities for each seed keyword as candidates for dictionary expansion.\n\nWe detail these steps in the following subsections and also describe a validation strategy for our method.",
        "main_experiment_and_results": "In the main experiment of \"LGDE: Local Graph-based Dictionary Expansion,\" the setup, datasets, baselines, evaluation metrics, and main experimental results are as follows:\n\n### Main Experiment Setup:\n\n1. **Dataset:**\n   - A benchmark dataset consisting of 56,099 manually annotated hate speech posts collected from Reddit and Gab (as reported by Qian et al., 2019).\n   - The dataset includes 19,873 hate speech posts, making up 35.4% of the total.\n\n2. **Data Split:**\n   - The dataset is divided into training and testing sets using stratification, with 75% of the data for training and 25% for testing.\n\n3. **Seed Dictionary:**\n   - The initial seed dictionary is comprised of the five most frequent keywords in the benchmark dataset: \u201cni**er,\u201d \u201cfa**ot,\u201d \u201cret**d,\u201d \u201cret***ed,\u201d and \u201cc**t.\u201d\n\n### Main Experimental Results:\n\nThe results of this main experiment would typically cover the performance of the model using specific evaluation metrics and comparisons to baselines. However, the detailed performance metrics and comparisons are not provided in the excerpt. Normally, the performance would be measured using metrics such as precision, recall, F1-score, or other relevant evaluation criteria."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare the performance of the LGDE method with a threshold-based method in expanding the seed dictionary and to evaluate its effectiveness on a corpus of hate speech-related posts from Reddit and Gab.",
            "experiment_process": "The word embeddings were computed after filtering out stopwords, infrequent misspellings, and very rare words. A similarity matrix was derived from these embeddings. The seed dictionary expansion was carried out using both the thresholding-based method and the LGDE method, ensuring that the expanded dictionaries included between 30 and 50 keywords. Hyperparameters for both methods were fine-tuned using the training split of benchmark data. Macro F1-scores of the optimised dictionaries were then evaluated on the test split. Individual contributions of the keywords in the discovered dictionaries were assessed using the likelihood ratio (LR).",
            "result_discussion": "LGDE consistently outperforms the thresholding method across different sizes of the expanded dictionary, as shown in the comparative results. The effectiveness of the LGDE dictionary was highlighted by better macro F1-scores and more relevant keyword contributions.",
            "ablation_id": "2405.07764v1.No1"
        },
        {
            "research_objective": "To extend the seed dictionary for identifying conspiracy-related content on 4chan and to compare the effectiveness of the LGDE method with a threshold-based method.",
            "experiment_process": "The analysis was restricted to the 5000 most frequent words in the domain-specific corpus after excluding stopwords. Fine-tuned word embeddings were generated from 100-dimensional GloVe base embeddings. An effective seed dictionary containing 109 keywords present in the corpus was used. The expansion was carried out to a maximum of 150 discovered keywords using both the thresholding-based method and the LGDE method. Hyperparameters were tuned for optimal performance. The discovered keywords were independently annotated by three domain experts to evaluate their suitability for identifying conspiracy-related content on 4chan.",
            "result_discussion": "The evaluation carried out by domain experts suggests that the LGDE method more effectively identified suitable keywords for conspiracy-related content compared to the thresholding-based method, indicating LGDE's superior performance in real-world applications.",
            "ablation_id": "2405.07764v1.No2"
        }
    ]
}