{
    "title": "Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP",
    "abstract": "Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the scale and diversity of training data and language supervision play a key role in unlocking the compositional generalization abilities of vision-language models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of large pre-trained models has significantly advanced the field of machine learning. Innovations such as GPT-3 [1  ###reference_b1###], Chinchilla [2  ###reference_b2###], PaLM [3  ###reference_b3###], and CLIP [4  ###reference_b4###] have broadened the horizons of generalization and underscored their exceptional capacity for zero-shot inference. The Out-of-Distribution (OoD) generalization of models like CLIP has been explored, revealing two differing perspectives on its origin: one attributing it to dataset diversity[5  ###reference_b5###], the other to language supervision[6  ###reference_b6###].\nMost of the previous work studied the CLIP generalization under a certain type of out-of-distribution data, namely, distribution shifts [7  ###reference_b7###, 8  ###reference_b8###, 5  ###reference_b5###]. However, there are other types of OoD generalization, including spurious correlation [9  ###reference_b9###], and compositional generalization [10  ###reference_b10###]. One has to note that each of these OoD generalization categories has a unique nature that should be studied separately.\nThis paper focuses on the compositional generalization, the ability of models to generalize new combinations of known concepts. Despite some shortcomings, it has been shown that Vision-Language Models (VLMs) can compose concepts in the single-object setting[11  ###reference_b11###]. We explore if the compositional nature of VLMs impacts their compositional OoD generalization, hypothesizing that joint vision-language representation learning has enhanced CLIP\u2019s decomposability between objects and attributes in images containing single objects.\nA significant challenge in evaluating OoD generalization is the unknown training distribution, as seen in models like CLIP where the training dataset has not been released. A novel benchmark design is proposed to assess CLIP models, involving a new compositional OoD dataset of unconventional attribute-object pairs distinct from the CLIP\u2019s training data called Imagenet-AO. We evaluate various CLIP models on this Imagenet-AO dataset to determine their performance and analyze contributing factors to the performance, offering insights into enhancing CLIP\u2019s generalization abilities. Our contributions include crafting an unseen attribute-object pair image test dataset called Imagenet-AO, providing a controlled benchmarking setting for various CLIP models using Imagenet-AO, and identifying the importance of compositional diversity in training captions for CLIP to demonstrate decomposable representation and basic compositional generalization.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related works",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Robustness to Natural Distribution Shift",
            "text": "In certain applications, the test samples may exhibit different styles, colors, or contrasts compared to the training data. OoD generalization under such distribution shifts have extensively been studied, and it has been argued that training on diverse datasets is the most effective factor in increasing the robustness [12  ###reference_b12###, 8  ###reference_b8###], while combining various data modalities did not enhance the performance [5  ###reference_b5###]."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Compositional Generalization of CLIP",
            "text": "Compositional generalization, generalizing to unfamiliar compositions of familiar elements, poses challenges for models like CLIP. This includes associating attributes with objects, understanding object relationships, and extrapolating to unfamiliar concept combinations. Possible solutions to this problem include utilization of image scene graphs and augmentation framework for contrastive learning [13  ###reference_b13###], leveraging LLMs to generate sentence-level descriptions for each compositional class [14  ###reference_b14###], and fine-tuning the vocabulary for attributes and objects on seen classes, then recomposing the learned vocabulary in new combinations for the novel classes [15  ###reference_b15###]. The emergence of concept representations within CLIP was studied in [16  ###reference_b16###]. In [17  ###reference_b17###], the authors examine VLMs struggles with relation, attribution, and order understanding. They propose a novel training procedure to improve these aspects.\nThis work differs from the mentioned studies by investigating and comparing the power of CLIP\u2019s compositional generalization in a single-object setting, including attribute-object compositions, and creating a dataset with combinations of objects and unusual attributes."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "CLIP Object-Attribute Compositional Generalization",
            "text": "Compositional OoD generalization refers to a model\u2019s ability to handle novel combinations of familiar concepts. This is critical in contexts like attribute-object images, where the goal is perceiving new compositions of objects and attributes.\nDecomposable image representations that assign separate embedding dimensions to objects and attributes facilitate this generalization. Such representation makes meaningful construction of known concepts in the embedding space feasible.\nWe hypothesize that large and diverse datasets reduce the dependency between attributes and objects, promoting a more decomposable understanding of images.\nBased on these insights, we posit that decomposability is the key to the CLIP model\u2019s unseen composition generalization. This claim is supported by two main arguments:\nLarge and diverse datasets reduce entanglement between object and attribute tokens. In other words, they help to promote a more decomposable text representation (see sec. 5.2  ###reference_###).\nText representation decomposability is induced in the image encoding, due to implicit maximization of the mutual information. We elaborate on this claim in what comes next.\nWhy decomposability may arise in contrastive learning?\nCLIP training maximizes the mutual information between text and image encodings. We claim that decomposability in the text representation, induces decomposability in the image encoding. To see this, let , and  be the text embeddings for the objects and attributes, respectively. Let , and  be the corresponding image embeddings. Assuming a decomposable text embedding means , i.e. . Now by minimizing the contrastive loss, the mutual information  is maximized. By letting , and , we have:\nThe latter term makes  and  dependent random variables, otherwise if , the expected KL divergence would be minimum (or zero), which is against maximizing the mutual information.\nNote that however,  does not ideally depend on both  and , otherwise the two distributions in the KL divergence in the first term become similar, which is also against maximizing the mutual information. Putting these together,  mostly depends on  if the mutual information is maximized. Using a symmetric argument,  mostly depends on . Finally, because , we conclude that  and  tend to become independent. Therefore, maximizing  decomposes  if  is already decomposed.\n###figure_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "ImageNet-AO: Dataset Desgin",
            "text": "To effectively assess the compositional generalization capabilities of models, we created a unique dataset of rare compositions, ensuring these were not present in the models\u2019 training data. This dataset was produced by creating compositional images via a text-to-image model, using an Attribute+Object template. Our process is as follows:\nSelecting objects or nouns:\nWe extracted class names from the ImageNet dataset, using these as objects (or nouns) in our structure to create a link between the generated images and ImageNet classes. This allows for comparison of model performances on the familiar ImageNet validation set. We aimed for a diverse set of class names to enhance the complexity of the generated images.\nSelecting attributes or adjectives:\nThe next step involved choosing 30 distinct adjectives that were relevant and could create unique combinations with the selected nouns, enhancing the diversity of our compositional images.\nSelecting unseen (attribute, object) pairs:\nWe combined the 30 adjectives with a pool of 1000 nouns, resulting in 30000 distinct pairs. These were given to the text-to-image model to generate corresponding images. To ensure these combinations were not present in the CLIP training set, we conducted a thorough search and removed any that were found.\nGenerating images for (attribute, object) pairs:\nThe selected combinations were given to a text-to-image model for the image generation. Among various models, the Microsoft model powered by DALL-E proved to be the most powerful. However, it had limitations and some prompts were blocked for unknown reasons.\nValidating the generated images:\nLastly, human supervision was used to validate the generated images, with images not closely aligning with their prompts removed. After this process, around 12000 combinations remained, for which we successfully generated around 50000 accurate, high-quality images. An illustrative example of the diversified dataset generated through this process can be observed in Figure 1  ###reference_###. This figure showcases a selection of images that exhibit various degrees of alignment with their corresponding prompts, highlighting the effectiveness of the validation procedure."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we examine the effects of language supervision on compositional Out-of-Distribution (OoD) performance. We explore links between the training dataset characteristics, and CLIP OoD generalization. Specifically, we assess our hypothesis regarding the role of the training data quality and quantity in disentangling the object and attributes, and its consequences in compositional OoD generalization. In a nutshell, we found that CLIPs whose training sets consist of more diverse caption compositions would exhibit this property more than other CLIP models."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "CLIP Models Comparison",
            "text": "We assessed CLIP model performance in zero-shot classification tasks using an evaluation method similar to that of [18  ###reference_b18###] and [4  ###reference_b4###] on ImageNet-AO dataset. We provided the model with images and captions, then calculated their cosine similarities to estimate the caption relevance to the image content. The models trained on the LAION 400m, LAION 2B, and DataComp 12.8B datasets showed similar performances on ImageNet-AO compared to the model trained on the OpenAI dataset. This indicates the potential efficacy of these datasets in training CLIP models for specific evaluated composition types. While larger training datasets typically resulted in enhanced accuracy, the CLIP model trained on YFCC15m displayed lower performance than the CC12m model, despite the former\u2019s larger dataset size. Additionally, experiments showed that models trained on Commonpool data filtered by LAION or CLIP scores outperformed the model trained on the full unfiltered Commonpool set, although the latter contained more data. This implies that various other factors can play a role in influencing the model behavior. To be more precise, the subsequent subsection discusses one of these factors that can significantly impact the model performance.\nTo visualize the comparative performance of these CLIP models trained on different datasets, refer to Figure 2  ###reference_###a."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Comparison with Supervised Models",
            "text": "In this experiment, we investigated the impact of language supervision on CLIP models compared to supervised models under compositional OoD settings. We did not intend a direct comparison, but rather to explore if CLIP\u2019s language supervision improves the OoD accuracy. We assumed the object names as the class labels and evaluated the supervised models\u2019 accuracy on ImageNet-AO. For CLIP, we generated captions using only object names, removing adjectives, to align the evaluations.\nFigure 2  ###reference_###b shows CLIP models trained on OpenAI, LAION, and DataComp datasets consistently outperform supervised models on the OoD accuracy. This suggests that language supervision during CLIP training positively impacts the model representation decomposability, enabling generalization to detect unseen compositions."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study examines the generalization of CLIPs to new object and attribute compositions. We created a benchmark dataset of compositional images and found that CLIPs training data quality is crucial for the compositional generalization. We showed that models trained on more diverse caption compositions perform better, and language supervision during training improves OoD generalization. The study highlights the importance of dataset diversity and decomposability in enhancing vision-language models\u2019 compositional generalization capabilities."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix.",
            "text": "###figure_4### Figure 4  ###reference_### shows the performance of different models on the our benchmark. The models are trained on different datasets or have special backbone, as follows:\nFigure 4  ###reference_###.a:shows the performance of CLIP models trained on the OpenAI dataset [4  ###reference_b4###].\nFigure 4  ###reference_###.b: shows the performance of CLIP models trained on the LAION dataset [19  ###reference_b19###] with 400 million or 2 billion image-text pairs.\nFigure 4  ###reference_###.c: shows the performance of CLIP models trained on Yahoo-Flickr Creative Commons dataset with 15 million image-text pairs [20  ###reference_b20###].\nFigure 4  ###reference_###.d: shows the performance of models trained on CC12M dataset [21  ###reference_b21###] with 12 million image-text pairs.\nFigure 4  ###reference_###.e: shows the performance of the CoCa model [22  ###reference_b22###] trained on the LAION dataset.\nFigure 4  ###reference_###.f: shows the performance of CLIP models trained on the Datacomp dataset [23  ###reference_b23###].\nFigure 4  ###reference_###.g: shows the performance of ConvNeXt CLIP models [24  ###reference_b24###] trained on the LAION dataset.\nFigure 4  ###reference_###.h:shows the performance of CLIP models trained on the Common Pool dataset [23  ###reference_b23###].\nFigure 4  ###reference_###.i: shows the performance of CLIP models with a Roberta encoder [25  ###reference_b25###] trained on the LAION dataset .\nFigure 4  ###reference_###.j: shows the performance of CLIP models introduced by [26  ###reference_b26###] trained on the LAION dataset."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Models performance on various datasets</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T1.1\" style=\"width:496.9pt;height:328.7pt;vertical-align:-0.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-78.8pt,52.0pt) scale(0.75914,0.75914) ;\">\n<p class=\"ltx_p\" id=\"A1.T1.1.1\"><span class=\"ltx_text\" id=\"A1.T1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"A1.T1.1.1.1.1\" style=\"width:654.5pt;height:433pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"A1.T1.1.1.1.1.1\"><span class=\"ltx_text\" id=\"A1.T1.1.1.1.1.1.1\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T1.1.1.1.1.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.1.1\">Model</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.2.1\">ImageNet</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.3.1\">ImageNet-v2</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.4.1\">Imagenet-sketch</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.5.1\">ImageNet-R</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.6.1\">ImageNet-A</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1.1.1.1.1.7.1\">Imagenet-AO</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.2.1\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.1.1.1.1.1.2.1.1\">vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T1.1.1.1.1.1.1.1.2.1.2\">88.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T1.1.1.1.1.1.1.1.2.1.3\">80.11</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T1.1.1.1.1.1.1.1.2.1.4\">65.31</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T1.1.1.1.1.1.1.1.2.1.5\">66.44</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T1.1.1.1.1.1.1.1.2.1.6\">75.013</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T1.1.1.1.1.1.1.1.2.1.7\">61.45</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.3.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.3.2.1\">vit_large_patch14_clip_336.openai_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.3.2.2\">88.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.3.2.3\">80.33</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.3.2.4\">63.79</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.3.2.5\">65.64</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.3.2.6\">77.64</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.3.2.7\">61.87</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.4.3\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.4.3.1\">vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.4.3.2\">88.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.4.3.3\">79.24</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.4.3.4\">65.77</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.4.3.5\">66.56</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.4.3.6\">69.91</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.4.3.7\">62.28</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.5.4\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.5.4.1\">vit_large_patch14_clip_336.laion2b_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.5.4.2\">88.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.5.4.3\">78.87</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.5.4.4\">59.74</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.5.4.5\">59.74</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.5.4.6\">68.84</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.5.4.7\">59.6</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.6.5\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.6.5.1\">vit_large_patch14_clip_224.openai_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.6.5.2\">88.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.6.5.3\">79.07</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.6.5.4\">61.83</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.6.5.5\">61.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.6.5.6\">71.12</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.6.5.7\">61.37</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.7.6\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.7.6.1\">vit_large_patch14_clip_224.openai_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.7.6.2\">87.9</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.7.6.3\">79.26</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.7.6.4\">62.52</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.7.6.5\">63.47</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.7.6.6\">70.85</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.7.6.7\">61.39</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.8.7\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.8.7.1\">vit_large_patch14_clip_336.laion2b_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.8.7.2\">87.9</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.8.7.3\">78.35</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.8.7.4\">63.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.8.7.5\">63.91</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.8.7.6\">61.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.8.7.7\">59.94</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.9.8\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.9.8.1\">vit_huge_patch14_clip_224.laion2b_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.9.8.2\">87.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.9.8.3\">79.06</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.9.8.4\">67.94</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.9.8.5\">68.05</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.9.8.6\">64.76</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.9.8.7\">62.72</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.10.9\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.10.9.1\">vit_large_patch14_clip_224.laion2b_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.10.9.2\">87.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.10.9.3\">77.16</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.10.9.4\">63.49</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.10.9.5\">63.08</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.10.9.6\">52.36</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.10.9.7\">60.24</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.11.10\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.11.10.1\">vit_base_patch16_clip_384.laion2b_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.11.10.2\">87.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.11.10.3\">77.77</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.11.10.4\">53.09</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.11.10.5\">49.45</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.11.10.6\">58.48</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.11.10.7\">56.41</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.12.11\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.12.11.1\">vit_base_patch16_clip_384.openai_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.12.11.2\">87</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.12.11.3\">77.32</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.12.11.4\">50.54</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.12.11.5\">48.28</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.12.11.6\">57.76</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.12.11.7\">55.15</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.13.12\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.13.12.1\">vit_base_patch16_clip_384.laion2b_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.13.12.2\">86.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.13.12.3\">77.51</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.13.12.4\">56.42</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.13.12.5\">53.03</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.13.12.6\">54.09</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.13.12.7\">57.27</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.14.13\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.14.13.1\">vit_base_patch16_clip_384.openai_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.14.13.2\">86.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.14.13.3\">76.44</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.14.13.4\">52.52</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.14.13.5\">49.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.14.13.6\">54.26</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.14.13.7\">54.91</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.15.14\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.15.14.1\">vit_base_patch16_clip_224.laion2b_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.15.14.2\">86.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.15.14.3\">75.53</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.15.14.4\">52.09</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.15.14.5\">49.17</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.15.14.6\">46.88</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.15.14.7\">55.9</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.16.15\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.16.15.1\">vit_base_patch16_clip_224.openai_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.16.15.2\">85.9</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.16.15.3\">74.79</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.16.15.4\">49.51</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.16.15.5\">46.91</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.16.15.6\">46.66</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.16.15.7\">54.46</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.17.16\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.17.16.1\">vit_base_patch32_clip_448.laion2b_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.17.16.2\">85.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.17.16.3\">75.55</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.17.16.4\">47.74</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.17.16.5\">44.78</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.17.16.6\">50.92</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.17.16.7\">53.57</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.18.17\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.18.17.1\">vit_base_patch16_clip_224.laion2b_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.18.17.2\">85.5</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.18.17.3\">74.92</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.18.17.4\">55.53</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.18.17.5\">52.03</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.18.17.6\">40.74</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.18.17.7\">56.22</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.19.18\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.19.18.1\">vit_base_patch32_clip_384.laion2b_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.19.18.2\">85.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.19.18.3\">75.08</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.19.18.4\">48.36</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.19.18.5\">45.29</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.19.18.6\">46.61</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.19.18.7\">53.35</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.20.19\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.20.19.1\">vit_base_patch16_clip_224.openai_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.20.19.2\">85.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.20.19.3\">74.43</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.20.19.4\">51.53</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.20.19.5\">48.47</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.20.19.6\">43.54</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.20.19.7\">54.1</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.21.20\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.21.20.1\">vit_base_patch32_clip_384.openai_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.21.20.2\">85.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.21.20.3\">74.22</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.21.20.4\">45.96</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.21.20.5\">42.92</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.21.20.6\">42.413</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.21.20.7\">52.23</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.22.21\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.22.21.1\">vit_base_patch32_clip_224.laion2b_ft_in12k_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.22.21.2\">83.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.22.21.3\">70.36</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.22.21.4\">46.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.22.21.5\">42.12</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.22.21.6\">28.58</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.22.21.7\">52.55</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.23.22\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1.1.1.1.23.22.1\">vit_base_patch32_clip_224.laion2b_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.23.22.2\">82.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.23.22.3\">69.26</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.23.22.4\">49.52</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.23.22.5\">43.9</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.23.22.6\">21.81</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T1.1.1.1.1.1.1.1.23.22.7\">51.51</span></span>\n<span class=\"ltx_tr\" id=\"A1.T1.1.1.1.1.1.1.1.24.23\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T1.1.1.1.1.1.1.1.24.23.1\">vit_base_patch32_clip_224.openai_ft_in1k</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T1.1.1.1.1.1.1.1.24.23.2\">81.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T1.1.1.1.1.1.1.1.24.23.3\">68.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T1.1.1.1.1.1.1.1.24.23.4\">44.82</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T1.1.1.1.1.1.1.1.24.23.5\">40.04</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T1.1.1.1.1.1.1.1.24.23.6\">20.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T1.1.1.1.1.1.1.1.24.23.7\">50.09</span></span>\n</span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n</figure>",
            "capture": "Table 1: Models performance on various datasets"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Models performance on text-to-image Retrieval task</figcaption>\n<p class=\"ltx_p ltx_align_center\" id=\"A1.T2.1\"><span class=\"ltx_text\" id=\"A1.T2.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"A1.T2.1.1.1\" style=\"width:387.7pt;height:864pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"A1.T2.1.1.1.1\"><span class=\"ltx_text\" id=\"A1.T2.1.1.1.1.1\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T2.1.1.1.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T2.1.1.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.1.1.1.1.1.1.1\">Model</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T2.1.1.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.1.1.1.1.1.2.1\">R@1</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T2.1.1.1.1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.1.1.1.1.1.3.1\">R@5</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T2.1.1.1.1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.1.1.1.1.1.4.1\">R@10</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.2.1\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.1.1.1.2.1.1\">RN50_openai</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.1.1.1.1.1.2.1.2\">0.1628</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.1.1.1.1.1.2.1.3\">0.4022</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.1.1.1.1.1.2.1.4\">0.5318</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.3.2\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.3.2.1\">RN50_yfcc15m</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.3.2.2\">0.0359</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.3.2.3\">0.0995</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.3.2.4\">0.1484</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.4.3\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.4.3.1\">RN50_cc12m</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.4.3.2\">0.0627</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.4.3.3\">0.1823</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.4.3.4\">0.2673</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.5.4\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.5.4.1\">RN50-quickgelu_openai</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.5.4.2\">0.1628</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.5.4.3\">0.4022</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.5.4.4\">0.5318</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.6.5\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.6.5.1\">RN50-quickgelu_yfcc15m</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.6.5.2\">0.0394</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.6.5.3\">0.1076</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.6.5.4\">0.1569</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.7.6\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.7.6.1\">RN50-quickgelu_cc12m</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.7.6.2\">0.0687</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.7.6.3\">0.1918</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.7.6.4\">0.2774</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.8.7\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.8.7.1\">RN101_openai</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.8.7.2\">0.1856</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.8.7.3\">0.4349</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.8.7.4\">0.5708</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.9.8\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.9.8.1\">RN101_yfcc15m</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.9.8.2\">0.0404</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.9.8.3\">0.1170</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.9.8.4\">0.1670</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.10.9\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.10.9.1\">RN101-quickgelu_openai</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.10.9.2\">0.1856</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.10.9.3\">0.4349</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.10.9.4\">0.5708</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.11.10\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.11.10.1\">RN101-quickgelu_yfcc15m</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.11.10.2\">0.0431</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.11.10.3\">0.1233</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.11.10.4\">0.1767</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.12.11\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.12.11.1\">ViT-B-32_openai</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.12.11.2\">0.2011</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.12.11.3\">0.4674</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.12.11.4\">0.6020</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.13.12\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.13.12.1\">ViT-B-32_laion400m_e31</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.13.12.2\">0.2161</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.13.12.3\">0.4818</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.13.12.4\">0.6109</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.14.13\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.14.13.1\">ViT-B-32_laion400m_e32</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.14.13.2\">0.2158</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.14.13.3\">0.4803</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.14.13.4\">0.6097</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.15.14\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.15.14.1\">ViT-B-32_laion2b_e16</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.15.14.2\">0.2748</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.15.14.3\">0.5700</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.15.14.4\">0.7058</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.16.15\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.16.15.1\">ViT-B-32_laion2b_s34b_b79k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.16.15.2\">0.2849</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.16.15.3\">0.5751</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.16.15.4\">0.7107</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.17.16\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.17.16.1\">ViT-B-32_datacomp_m_s128m_b4k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.17.16.2\">0.0587</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.17.16.3\">0.1612</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.17.16.4\">0.2321</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.18.17\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.18.17.1\">ViT-B-32_datacomp_s_s13m_b4k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.18.17.2\">0.0033</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.18.17.3\">0.0122</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.18.17.4\">0.0206</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.19.18\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.19.18.1\">ViT-B-32-quickgelu_openai</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.19.18.2\">0.2011</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.19.18.3\">0.4674</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.19.18.4\">0.6020</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.20.19\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.20.19.1\">ViT-B-32-quickgelu_laion400m_e31</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.20.19.2\">0.2437</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.20.19.3\">0.5136</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.20.19.4\">0.6468</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.21.20\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.21.20.1\">ViT-B-32-quickgelu_laion400m_e32</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.21.20.2\">0.2416</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.21.20.3\">0.5158</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.21.20.4\">0.6474</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.22.21\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.22.21.1\">ViT-B-16_openai</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.22.21.2\">0.2313</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.22.21.3\">0.5105</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.22.21.4\">0.6533</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.23.22\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.23.22.1\">ViT-B-16_laion400m_e31</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.23.22.2\">0.2727</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.23.22.3\">0.5654</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.23.22.4\">0.6943</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.24.23\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.24.23.1\">ViT-B-16_laion400m_e32</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.24.23.2\">0.2754</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.24.23.3\">0.5637</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.24.23.4\">0.6921</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.25.24\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.25.24.1\">ViT-B-16_laion2b_s32b_b82k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.25.24.2\">0.3006</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.25.24.3\">0.5964</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.25.24.4\">0.7242</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.26.25\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.26.25.1\">ViT-B-16_datacomp_l_s1b_b8k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.26.25.2\">0.2393</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.26.25.3\">0.5218</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.26.25.4\">0.6520</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.27.26\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.27.26.1\">ViT-B-16_commonpool_l_clip_s1b_b8k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.27.26.2\">0.2189</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.27.26.3\">0.4822</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.27.26.4\">0.6154</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.28.27\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.28.27.1\">ViT-B-16_commonpool_l_laion_s1b_b8k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.28.27.2\">0.2059</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.28.27.3\">0.4582</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.28.27.4\">0.5891</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.29.28\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.29.28.1\">ViT-B-16_commonpool_l_image_s1b_b8k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.29.28.2\">0.1847</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.29.28.3\">0.4265</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.29.28.4\">0.5591</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.30.29\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.30.29.1\">ViT-B-16_commonpool_l_text_s1b_b8k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.30.29.2\">0.1904</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.30.29.3\">0.4278</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.30.29.4\">0.5590</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.31.30\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.31.30.1\">ViT-B-16_commonpool_l_basic_s1b_b8k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.31.30.2\">0.1683</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.31.30.3\">0.3932</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.31.30.4\">0.5173</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.32.31\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.32.31.1\">ViT-B-16_commonpool_l_s1b_b8k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.32.31.2\">0.1307</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.32.31.3\">0.3173</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.32.31.4\">0.4264</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.33.32\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.33.32.1\">ViT-L-14_openai</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.33.32.2\">0.2818</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.33.32.3\">0.5864</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.33.32.4\">0.7243</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.34.33\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.34.33.1\">ViT-L-14_laion400m_e31</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.34.33.2\">0.3305</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.34.33.3\">0.6298</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.34.33.4\">0.7548</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.35.34\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.35.34.1\">ViT-L-14_laion400m_e32</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.35.34.2\">0.3310</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.35.34.3\">0.6304</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.35.34.4\">0.7543</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.36.35\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.36.35.1\">ViT-L-14_laion2b_s32b_b79k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.36.35.2\">0.3790</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.36.35.3\">0.6980</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.36.35.4\">0.8108</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.37.36\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.37.36.1\">ViT-L-14_datacomp_xl_s13b_b90k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.37.36.2\">0.4010</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.37.36.3\">0.7028</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.37.36.4\">0.8166</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.38.37\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.38.37.1\">ViT-L-14_commonpool_xl_clip_s13b_b90k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.38.37.2\">0.3897</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.38.37.3\">0.7004</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.38.37.4\">0.8154</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.39.38\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.39.38.1\">ViT-L-14_commonpool_xl_laion_s13b_b90k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.39.38.2\">0.3657</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.39.38.3\">0.6777</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.39.38.4\">0.7994</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.40.39\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.40.39.1\">ViT-L-14_commonpool_xl_s13b_b90k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.40.39.2\">0.2775</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.40.39.3\">0.5752</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.40.39.4\">0.7154</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.41.40\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.41.40.1\">ViT-H-14_laion2b_s32b_b79k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.41.40.2\">0.3659</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.41.40.3\">0.6652</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.41.40.4\">0.7785</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.42.41\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.42.41.1\">ViT-g-14_laion2b_s12b_b42k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.42.41.2\">0.3628</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.42.41.3\">0.6562</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.42.41.4\">0.7720</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.43.42\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.43.42.1\">ViT-g-14_laion2b_s34b_b88k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.43.42.2\">0.3653</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.43.42.3\">0.6542</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.43.42.4\">0.7726</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.44.43\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.44.43.1\">ViT-bigG-14_laion2b_s39b_b160k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.44.43.2\">0.3757</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.44.43.3\">0.6711</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.44.43.4\">0.7893</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.45.44\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.45.44.1\">coca_ViT-B-32_laion2b_s13b_b90k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.45.44.2\">0.2628</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.45.44.3\">0.5498</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.45.44.4\">0.6874</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.46.45\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.46.45.1\">coca_ViT-B-32_mscoco_finetuned_laion2b_s13b_b90k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.46.45.2\">0.0006</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.46.45.3\">0.0019</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.46.45.4\">0.0040</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.47.46\">\n<span class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.1.1.1.1.47.46.1\">coca_ViT-L-14_laion2b_s13b_b90k</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.47.46.2\">0.3362</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.47.46.3\">0.6381</span>\n<span class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.1.1.1.1.47.46.4\">0.7613</span></span>\n<span class=\"ltx_tr\" id=\"A1.T2.1.1.1.1.1.1.48.47\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T2.1.1.1.1.1.1.48.47.1\">coca_ViT-L-14_mscoco_finetuned_laion2b_s13b_b90k</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T2.1.1.1.1.1.1.48.47.2\">0.3626</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T2.1.1.1.1.1.1.48.47.3\">0.6652</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T2.1.1.1.1.1.1.48.47.4\">0.7823</span></span>\n</span>\n</span></span></span>\n</span></span></span></p>\n</figure>",
            "capture": "Table 2: Models performance on text-to-image Retrieval task"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.18525v1_figure_1.png",
            "caption": "Figure 1: Examples of images from Imagenet-AO dataset. This dataset is created by combining attributes and objects that do not appear in the CLIP training sets, specifically designed for benchmarking OoD generalization purposes. More examples are in Figure 8."
        },
        "2": {
            "figure_path": "2403.18525v1_figure_2.png",
            "caption": "Figure 2:  a) Comparing effective OoD generalization of CLIP models with diverse backbones and training sets in a zero shot setting, where no fine-tuning is performed on the target task. The in-distribution (ID) test set is the ImageNet validation split, with the labels being the object names, while the out-of-distribution test set is our designed compositional dataset, with labels being attribute-object pairs. Noticeably, there is a large gap between the performance of CLIPs that are trained on small datasets, e.g. CC15m and YFCC12m, and that of the CLIPs trained on gigantic datasets such as LAION and OpenAI. b) Comparing OoD generalization of the models trained with a supervised loss vs. CLIPs. ID and OoD test sets are the same as before, with the labels being the object names in both ID and OoD test sets, as the adjectives are not among the labels of the pre-trained supervised models. Despite being competitive on ID accuracy, the supervised models fall short of the OoD accuracy of the CLIP models."
        },
        "3": {
            "figure_path": "2403.18525v1_figure_3.png",
            "caption": "Figure 3: Normalized Mutual Information between attributes and objects in different CLIP training sets based on ImageNet-AO compositions"
        },
        "4": {
            "figure_path": "2403.18525v1_figure_4.png",
            "caption": "Figure 4: Evaluation OoD generalization of different CLIP models trained using various datasets. The evaluation involved testing these models on both in-distribution and out-of-distribution test sets."
        },
        "5": {
            "figure_path": "2403.18525v1_figure_5.png",
            "caption": "Figure 5: Evaluation of the CLIP models on Imagenet objects"
        },
        "6": {
            "figure_path": "2403.18525v1_figure_6.png",
            "caption": "Figure 6: Comparison of OOD Accuracy in Various Few-Shot Settings for Different CLIP Models. This plot illustrates the out-of-distribution (OOD) accuracy performance across diverse few-shot scenarios, with the x-axis representing the number of samples used for fine-tuning, and the y-axis depicting the OOD accuracy."
        },
        "7": {
            "figure_path": "2403.18525v1_figure_7.png",
            "caption": "Figure 7: OOD Accuracy vs. ID Accuracy for Different CLIP Models Fine-Tuned on ImageNet."
        },
        "8": {
            "figure_path": "2403.18525v1_figure_8.png",
            "caption": "Figure 8:  Examples of images from Imagenet-AO dataset."
        }
    },
    "references": [
        {
            "1": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al.",
                "venue": "Advances in neural information processing systems,\n33:1877\u20131901, 2020.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Training compute-optimal large language models.",
                "author": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al.",
                "venue": "arXiv preprint arXiv:2203.15556, 2022.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Palm: Scaling language modeling with pathways.",
                "author": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al.",
                "venue": "arXiv preprint arXiv:2204.02311, 2022.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Learning transferable visual models from natural language\nsupervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al.",
                "venue": "In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Data determines distributional robustness in contrastive language\nimage pre-training (clip).",
                "author": "Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar,\nAchal Dave, and Ludwig Schmidt.",
                "venue": "In International Conference on Machine Learning, pages\n6216\u20136234. PMLR, 2022.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Is a caption worth a thousand images? a controlled study for\nrepresentation learning.",
                "author": "Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, and Tatsunori\nHashimoto.",
                "venue": "arXiv preprint arXiv:2207.07635, 2022.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Adaptive risk minimization: Learning to adapt to domain shift.",
                "author": "Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine,\nand Chelsea Finn.",
                "venue": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Processing Systems,\nvolume 34, pages 23664\u201323678. Curran Associates, Inc., 2021.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Measuring robustness to natural distribution shifts in image\nclassification.",
                "author": "Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,\nand Ludwig Schmidt.",
                "venue": "Advances in Neural Information Processing Systems,\n33:18583\u201318599, 2020.",
                "url": null
            }
        },
        {
            "9": {
                "title": "What is a spurious correlation?",
                "author": "Brian D Haig.",
                "venue": "Understanding Statistics: Statistical Issues in Psychology,\nEducation, and the Social Sciences, 2(2):125\u2013132, 2003.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Learning to generalize to new compositions in image understanding.",
                "author": "Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson, and Gal Chechik.",
                "venue": "arXiv preprint arXiv:1608.07639, 2016.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Does clip bind concepts? probing compositionality in large image\nmodels, 2023.",
                "author": "Martha Lewis, Nihal V. Nayak, Peilin Yu, Qinan Yu, Jack Merullo, Stephen H.\nBach, and Ellie Pavlick.",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "Do imagenet classifiers generalize to imagenet?",
                "author": "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.",
                "venue": "In International conference on machine learning, pages\n5389\u20135400. PMLR, 2019.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Coarse-to-fine contrastive learning in image-text-graph space for\nimproved vision-language compositionality.",
                "author": "Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei\nDu, and Yu Chen.",
                "venue": "arXiv preprint arXiv:2305.13812, 2023.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Prompting language-informed distribution for compositional zero-shot\nlearning.",
                "author": "Wentao Bao, Lichang Chen, Heng Huang, and Yu Kong.",
                "venue": "arXiv preprint arXiv:2305.14428, 2023.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Learning to compose soft prompts for compositional zero-shot\nlearning.",
                "author": "Nihal V. Nayak, Peilin Yu, and Stephen Bach.",
                "venue": "In The Eleventh International Conference on Learning\nRepresentations, 2023.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Do vision-language pretrained models learn primitive concepts?",
                "author": "Tian Yun, Usha Bhalla, Ellie Pavlick, and Chen Sun.",
                "venue": "arXiv preprint arXiv:2203.17271, 2022.",
                "url": null
            }
        },
        {
            "17": {
                "title": "When and why vision-language models behave like bag-of-words models,\nand what to do about it?",
                "author": "Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James\nZou.",
                "venue": "arXiv preprint arXiv:2210.01936, 2022.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Openclip, July 2021.",
                "author": "Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas\nCarlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John\nMiller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.",
                "venue": "If you use this software, please cite it as below.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Laion-5b: An open large-scale dataset for training next generation\nimage-text models.",
                "author": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross\nWightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell\nWortsman, et al.",
                "venue": "arXiv preprint arXiv:2210.08402, 2022.",
                "url": null
            }
        },
        {
            "20": {
                "title": "YFCC100M: The new data in multimedia research.",
                "author": "Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,\nDouglas Poland, Damian Borth, and Li-Jia Li.",
                "venue": "Communications of the ACM, 59(2):64\u201373, 2016.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Conceptual 12M: Pushing web-scale image-text pre-training to\nrecognize long-tail visual concepts.",
                "author": "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.",
                "venue": "In CVPR, 2021.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Coca: Contrastive captioners are image-text foundation models.",
                "author": "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and\nYonghui Wu.",
                "venue": "arXiv preprint arXiv:2205.01917, 2022.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Datacomp: In search of the next generation of multimodal datasets.",
                "author": "Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios\nSmyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu\nZhang, et al.",
                "venue": "arXiv preprint arXiv:2304.14108, 2023.",
                "url": null
            }
        },
        {
            "24": {
                "title": "A convnet for the 2020s.",
                "author": "Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,\nand Saining Xie.",
                "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11976\u201311986, 2022.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Roberta: A robustly optimized bert pretraining approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.",
                "venue": "arXiv preprint arXiv:1907.11692, 2019.",
                "url": null
            }
        },
        {
            "26": {
                "title": "An inverse scaling law for clip training.",
                "author": "Xianhang Li, Zeyu Wang, and Cihang Xie.",
                "venue": "arXiv preprint arXiv:2305.07017, 2023.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.18525v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.18525v1",
        "paper_title": "Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP",
        "research_background": "The motivation of the paper stems from the significant advancements in the field of machine learning due to large pre-trained models like GPT-3, Chinchilla, PaLM, and CLIP, particularly highlighting their capacity for zero-shot inference. The authors are motivated by the need to understand and enhance the Out-of-Distribution (OoD) generalization of these models, with a specific focus on compositional generalization\u2014defined as the model\u2019s ability to generalize to new combinations of known concepts.\n\nThe research problem the paper addresses is the gap in understanding how Vision-Language Models (VLMs), such as CLIP, perform on compositional OoD generalization. Existing research has primarily focused on other types of OoD generalization like distribution shifts, spurious correlations, and tested the idea that joint vision-language representation learning contributes to the decomposability between objects and attributes in single-object images.\n\nRelevant prior work includes:\n1. Exploration of OoD generalization in models like CLIP, attributing it to either dataset diversity or language supervision.\n2. Research into how models respond to different types of distribution shifts and spurious correlations.\n3. Studies which examine generalization in Vision-Language Models under a single-object setting, showcasing their competence in composing concepts.\n4. Gaps in research where the unique nature of each OoD generalization category has not been examined individually.\n\nThe paper contributes to the field by:\n1. Creating a new compositional OoD dataset named Imagenet-AO, containing unconventional attribute-object pairs not found in CLIP\u2019s training data.\n2. Offering a controlled benchmarking method for assessing various CLIP models using the Imagenet-AO dataset.\n3. Identifying the importance of incorporating compositional diversity in training captions to boost CLIP\u2019s decomposable representations and basic compositional generalization.\n\nBy addressing these aspects, the paper aims to provide deeper insights into the compositional generalization abilities of CLIP and suggest ways to improve its performance in unseen attribute-object pair scenarios.",
        "methodology": "### Methodology: \n\nThe paper delves into compositional out-of-domain (OoD) generalization, emphasizing its importance in the context of determining new combinations of familiar attributes and objects within attribute-object images.\n\nThe authors argue that decomposeable image representations, which independently ascribe embedding dimensions to objects and attributes, are essential for enhancing generalization. This clear division allows for the viable construction of familiar concepts within the embedding space, making it feasible to perceive novel compositions.\n\n### Key Hypotheses and Insights:\n\n1. **Large and Diverse Datasets:** \n   - Reduce dependence between object and attribute tokens.\n   - Promote more decomposable text representations, facilitating clearer segmentation of different components in the embedding space.\n\n2. **Induced Decomposability in Image Encoding:**\n   - The CLIP model leverages large and varied datasets to naturally induce a decomposable structure within both text and image encodings.\n   - The implicit maximization of mutual information between text and image encodings during training supports this claim.\n\n### Why Decomposability Arises in Contrastive Learning:\n\n- **Mutual Information Maximization:** \n   - CLIP training aims to maximize the mutual information between text and image encodings, inherently promoting a decomposable representation in each domain.\n   - If text embeddings for objects (\\( u \\)) and attributes (\\( v \\)) are decomposable (\\( z(u, v) = z_u + z_v \\)), minimizing contrastive loss maximizes mutual information \\( I(x; z) \\).\n\n### Detailed Argument:\n\n- Let:\n  - \\( z_u \\), \\( z_v \\) represent text embeddings for objects and attributes\n  - \\( x_u \\), \\( x_v \\) denote the corresponding image embeddings.\n\n- Assuming decomposability:\n  - If \\( z(u, v) = z_u + z_v \\), the mutual information \\( I(x; z) \\) can be maximized.\n  - Define \\( p_x \\) and \\( p_x,z \\), along with two other terms representing KL divergence.\n\n- Analyzing Dependencies:\n   - \\( z_u \\) and \\( z_v \\) result in dependencies when mutual information is maximized. \n   - With ensuring mutual information maximization, the expected KL divergence will not be at its lowest (not zero), suggesting dependence.\n   - To maintain high mutual information, \\( x \\) primarily relies on \\( x_u + x_v \\).\n\n### Conclusion:\n\n- **Decomposition in Mutual Information:**\n   - \\( I(z_u; x) \\) must depend on both \\( z_u \\) and \\( z_v \\) to maintain this balance, but independently.\n   - Resultantly, \\( z(x) = x_u + x_v \\) becomes independent, thereby decomposing image embeddings when text embeddings are also decomposed.\n\nBy leveraging the structural properties of embedding decomposability and the role of mutual information in contrastive learning, the CLIP model is hypothesized to achieve compositional OoD generalization effectively.",
        "main_experiment_and_results": "In the main experiment, we aimed to assess the compositional generalization capabilities of models by creating a unique dataset composed of rare attribute-object combinations that were not present in the models' training datasets. Here is a detailed breakdown of the experiment setup and results:\n\n### Experiment Setup\n\n**Dataset:**\n1. *Object Selection*: Class names were extracted from the ImageNet dataset and used as objects (nouns) to maintain a link between generated images and ImageNet classes. This facilitated performance comparisons on the familiar ImageNet validation set.\n2. *Attribute Selection*: A total of 30 distinct adjectives were selected to form unique combinations with the chosen nouns, which increased the diversity of the compositional images.\n3. *Pair Selection*: By combining 30 adjectives with a pool of 1000 nouns, we created 30000 distinct attribute-object pairs. These pairs were given to a text-to-image model to generate the corresponding images. Any pair already present in the CLIP training set was excluded.\n4. *Image Generation*: The combinations were fed to a text-to-image model, with the Microsoft model powered by DALL-E being used primarily, despite some limitations and blocked prompts.\n5. *Image Validation*: Human supervision filtered out images that didn't closely align with their prompts. After validation, about 12000 combinations remained, for which approximately 50000 high-quality images were successfully generated.\n\n### Baselines and Evaluation Metrics\n\nIn the baseline comparison, the performance of models was measured against traditional validation sets from ImageNet, highlighting any significant gaps or improvements in handling the synthesized compositional images.\n\n**Baselines:**\n1. Pre-trained CLIP model performance on standard ImageNet compositions.\n2. Performance of generative models in creating accurate text-to-image compositions.\n\n**Evaluation Metrics:**\n1. *Accuracy*: How accurately the model identified objects and attributes in the compositional images.\n2. *Generalization*: The model's capacity to generalize from known image pairs to previously unseen combinations.\n3. *Human Validation*: Qualitative measures to ensure the images matched the intended attribute-object pairs.\n\n### Main Experimental Results\n\nThe experiment yielded the following results:\n- The generated compositional dataset effectively exhibited rare attribute-object combinations, with high-quality images validated by human supervisors, showcasing model capabilities to handle unfamiliar data.\n- When evaluated on the newly created dataset, models demonstrated varied degrees of success in compositional generalization. The results highlighted strengths and weaknesses in model performance, pointing to the pivotal role of language in understanding and generating image compositions that involve rare attribute-object pairs.\n\nBy using diverse and previously unseen combinations, the experiment underscored the potential for enhanced generalization in models, driven by innovative methods to curate and validate datasets outside the realm of their conventional training data."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "We assessed the performance of CLIP models in zero-shot classification tasks using images with novel compositions of attribute-object pairs. The goal was to determine the effectiveness of different training datasets in enhancing the model's ability to classify these novel compositions.",
            "experiment_process": "We used the ImageNet-AO dataset and calculated cosine similarities between given images and their corresponding captions to estimate caption relevance. The models trained on datasets including LAION-400M, LAION-2B, and DataComp 12.8B were compared to the OpenAI dataset-trained model. Larger datasets generally resulted in better performance, but the YFCC15M-trained model performed worse than the CC12M-trained model despite its larger size. Additionally, models trained on Commonpool data filtered by LAION or CLIP scores outperformed those trained on unfiltered Commonpool data.",
            "result_discussion": "The study found that large and diverse datasets typically enhance CLIP model performance on novel compositions. However, the performance on the YFCC15M dataset suggests that factors other than dataset size, such as data quality and relevance, significantly impact model performance. The results imply the importance of using high-quality datasets curated based on specific scoring methods.",
            "ablation_id": "2403.18525v1.No1"
        },
        {
            "research_objective": "We hypothesize that training with datasets containing diverse and creative samples with less dependency between objects and attributes is crucial for models to learn decomposable representations.",
            "experiment_process": "We measured the normalized mutual information (NMI) between object class and attributes based on the captions in ImageNet-AO to evaluate the decomposability of CLIP training data. A lower NMI signifies better disentanglement of attributes and objects. We analyzed datasets used for CLIP training to assess the level of decomposability.",
            "result_discussion": "The LAION-400M dataset exhibited lower NMI values compared to the CC12M dataset, and similarly, CC12M showed lower NMI values compared to YFCC15M. This indicates that datasets with lower NMI values better support decomposable representation learning, aligning with improved compositional OoD generalization observed in other experiments.",
            "ablation_id": "2403.18525v1.No2"
        },
        {
            "research_objective": "We investigated whether language supervision in CLIP models improves performance in compositional OoD scenarios compared to supervised models.",
            "experiment_process": "We evaluated supervised models using object names as class labels on ImageNet-AO, and aligned CLIP's evaluation by generating captions containing only object names. This setup was aimed at comparing the impact of language supervision on model performance.",
            "result_discussion": "CLIP models trained on OpenAI, LAION, and DataComp datasets consistently outperformed supervised models in OoD accuracy. This suggests that language supervision during training enhances the model's representation decomposability, enabling better generalization to detect unseen attribute-object compositions.",
            "ablation_id": "2403.18525v1.No3"
        }
    ]
}