{
    "title": "Designing and Evaluating Dialogue LLMs for Co-Creative Improvised Theatre",
    "abstract": "Social robotics researchers are increasingly interested in multi-party trained conversational agents. With a growing demand for real-world evaluations, our study presents Large Language Models (LLMs) deployed in a month-long live show at the Edinburgh Festival Fringe. This case study investigates human improvisers co-creating with conversational agents in a professional theatre setting. We explore the technical capabilities and constraints of on-the-spot multi-party dialogue, providing comprehensive insights from both audience and performer experiences with AI on stage. Our human-in-the-loop methodology underlines the challenges of these LLMs in generating context-relevant responses, stressing the user interface\u2019s crucial role. Audience feedback indicates an evolving interest for AI-driven live entertainment, direct human-AI interaction, and a diverse range of expectations about AI\u2019s conversational competence and utility as a creativity support tool. Human performers express immense enthusiasm, varied satisfaction, and the evolving public opinion highlights mixed emotions about AI\u2019s role in arts.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### This case study examines the process of designing and staging chatbots to perform improvisational theatre alongside a cast of human actors (Improbotics111improbotics.org), conversing naturally in Multi-Party Chat in front of live audiences.\nWe choose improvised theatre (Johnstone, 2014  ###reference_b16###) as a challenging testbed for human-machine co-creation enabled by the ability of language models to respond \u201cintelligently\u201d to performing arts scenarios involving both dialogue with other performers and interaction with audiences.\nThe show builds upon prior work in improvised theatre and comedy with physical robots (Bruce et al., 2000  ###reference_b6###), chatbots (Mathewson and\nMirowski, 2017a  ###reference_b23###, b  ###reference_b24###; Cho and May, 2020  ###reference_b9###), story generators (Branch, Mirowski, and\nMathewson, 2021  ###reference_b4###), comedy roast generators by company ComedyBytes222comedybytes.io, live machine translation (Mirowski et al., 2020  ###reference_b27###), joke generation (Toplyn, 2022  ###reference_b34###; Goes et al., 2023  ###reference_b12###) and employs the conceit of a human actor (the \u201cCyborg\u201d) taking their lines from a chatbot, similarly to (Mathewson and\nMirowski, 2018  ###reference_b25###; Loesel, Mirowski, and\nMathewson, 2020  ###reference_b21###) and to the show Yes, Android333blogto.com/events/yes-android-comedy-robots-toronto. Note that none of these works explored the specific solution to multiparty dialogue interaction where the AI simultaneously interacts with multiple improvisational actors on stage.\nWe deployed a conversational agent powered by three different Large Language Models (LLMs)444Chat GPT 3.5 (OpenAI, 2023  ###reference_b29###) (OpenAI), PaLM 2 (Anil et al., 2023  ###reference_b1###) (Google), Llama 2 (Touvron et al., 2023  ###reference_b35###) (Meta) to improvise with a company of professional actors for 26 different audiences during the 2023 Edinburgh Festival Fringe555https://www.edfringe.com/. During the run of the show, we conducted surveys with both cast and audience members to examine general perceptions of conversational AI, and to understand the in-situ impact of AI on creativity, performance expectations when staging AI, and anxieties around robots. With our work we further demonstrate a participatory design model for developing AI tools for the performing arts\nand examine how design choices impact its abilities to meaningfully contribute to group conversations.\nOur study was formed in response to the recent advancements in natural language computing and conversational AI which have highlighted the potential of ubiquitous AI participating in human social and creative lives (Youssef et al., 2022  ###reference_b42###; Lim, Rooksby, and Cross, 2021  ###reference_b19###). While current conversational AI applications mostly focus on single-user text-based dialogue, researchers are interested in the next frontier of Multi-Party Chat (MPC) AI (Traum, 2003  ###reference_b36###; Kirchhoff and\nOstendorf, 2003  ###reference_b17###; Poria et al., 2018  ###reference_b31###; Zhu et al., 2022  ###reference_b44###; Wei et al., 2023  ###reference_b39###) that can intelligently respond not only to dialogue, but also to the physical and social context of the conversation. Issues in multi-party dialogue include speaker and addressee identification, turn and conversation thread management or establishing a common ground among multiple participants (Traum, 2003  ###reference_b36###). MPC is already prominent in online social gaming research and development where non-player characters (NPCs) react to multiple online players (Urbanek et al., 2019  ###reference_b37###) but it is also relevant for speculative AI\napplications to human social, cultural, and political life.\nVarious technical, psychological and social factors affect not only the feasibility of deploying robots in social contexts, but also their \u201cacceptability by humans as partners in the interaction (Youssef et al., 2022  ###reference_b42###).\u201d\nWith some exceptions, including staging robots in theatre performances (Chikaraishi et al., 2017  ###reference_b8###; Nishiguchi et al., 2017  ###reference_b28###), Human Robot Interaction (HRI) studies also generally take place in laboratory settings.\nOur case study was designed to introduce an application of MPC AI in a theatre space, highlighting real-world challenges of both social robotics and of conversational AI, and capturing human reactions to the system in action. Using the theatre as a laboratory to study how actors and audiences respond to the presence of a real-time performing AI, our work answers \u201ca call for a more integrative approach when investigating HRIs\u201d that specifically takes into account \u201cthe cultural context of the experiment\u201d (Lim, Rooksby, and Cross, 2021  ###reference_b19###), and puts participants in direct contact with the physically present robots.\nUntil recently, public perception of robots has been shaped by media more than by direct experiences with AI (Haring et al., 2014  ###reference_b13###), fostering both over- and under-estimation of AI\u2019s ability and utility by the public. Our findings demonstrate how both interacting with, and observing an AI being deployed in a social context, influences the perception of AI capability as well as the motivation for interacting with it, and the ability to relate to it.\nOur case study is also an examination of participatory and user-centered design demonstrating how user-in-the-loop design compliments human-in-the-loop AI research. We find that such participatory models of inquiry can tackle the intertwined technical and psychological challenges around the subjects of creativity, human computing, and robotics interaction.\nThe following sections outline our method for deploying and staging MPC LLMs around the unique limitations and opportunities of live theatre, our development of improvisational games designed to challenge and explore the limits of state of the art conversational agents, and the results of our audience and performer surveys that highlight how our design choices impacted perceptions of AI."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Staging AI in Theatre Festival Performances",
            "text": "Multi-party dialogue live on stage with a chatbot presented numerous challenges for the developers, researchers, and artists involved.\nWhile in principle modern LLMs can track multiple conversational agents, they need a speech recognition system with multiple microphones to identify speakers, making it impractical for \u201cFringe\u201d theatre productions with short theatre get-in and get-out times. Second, capturing speech does not account for the physical aspects of communication on stage, from gestures to tone of voice; missing the full context of the conversation. Third, design decisions must be made about the timing of that chatbot\u2019s responses.\nTo address these challenges, we developed a system that combines continuous speech recognition (see Appendix for implementation details) and a human-in-the-loop curation system to allows an Operator to type contextual metadata for the LLMs, supplementing context from the live speech666Scene dialogue was recorded using a microphone and speech recognition system, resulting in numerous transcription errors. captured on stage. We also engineered\nprompts to influence the LLMs\u2019 style of response (see Fig. 2  ###reference_###). A second performer (Curator) selected in real time the best response from a \u201cstream of consciousness\u201d-like set of responses continuously generated by the LLMs (see Fig. 3  ###reference_###). Figure 5  ###reference_### displays the AI\u2019s contributions in a short improv scene dialogue, with only a small subset selected by the curator and sent via text-to-speech and earpiece to the Cyborg performer.\nCo-creative storytelling abilities of AI have been initially explored using a custom LLM based on recurrent neural networks (Mathewson and\nMirowski, 2017b  ###reference_b24###; Mirowski and\nMathewson, 2019  ###reference_b26###) trained on movie and television dialogue (Lison and\nTiedemann, 2016  ###reference_b20###). Their early model struggled with longer narrative threads and handling multiple speakers. OpenAI\u2019s release of GPT-2 (Radford et al., 2019  ###reference_b32###) with a Transformer architecture in 2019, introduced a much more powerful LLM that allowed for performance of longer narrative scenes (Mirowski et al., 2020  ###reference_b27###; Branch, Mirowski, and\nMathewson, 2021  ###reference_b4###). Nevertheless, multi-party dialogue performance remained underwhelming. In an improvement over their work, and experimenting with GPT-3 (Brown et al., 2020  ###reference_b5###) and subsequent models (OpenAI, 2023  ###reference_b29###; Anil et al., 2023  ###reference_b1###; Touvron et al., 2023  ###reference_b35###), we found that given sufficient prompting and context, GPT-3 and later models appeared capable of producing nuanced, varied responses that addressed more than one participant in the scene. This led us to explore new games and formats for improvisational theatre that involved more than one scene partner.\n###figure_2### ###figure_3### ###figure_4### For the 2023 Edinburgh Festival Fringe, we developed a system that prompted multiple LLMs simultaneously while incorporating live stage dialogue. We adopted a participatory iterative design approach, with actors trying the system and providing regular feedback about improvements to support role-playing with AI. Subsequently, through several cycles of experimentation and feedback, we developed a series of improv games that appeared to allow improvisers to perform comfortably while also allowing us to continue testing the capacity of LLMs to handle a variety of complex multi-party scenes. The success and consistency of our system, and the positive performer feedback gave us confidence to present the show as part of the Fringe Festival for 26 consecutive performances to new audiences each night.\nOver the course of the festival, we performed the show for over 1750 people across 26 unique performances with various groups of 20 different improvisers. Using audience and performer surveys, logs of LLM interactions, as well as close observations of the performances, we examine various practices of prompt engineering, UI/UX design around human-in-the-loop AI curation, audience experience and identification with the presence of AI on stage as part of an ensemble, as well as real time multi-party co-creation of narratives with AI and human improvisers.\nThe framing of our show was detailed in the festival program, and on-stage at the beginning of the performance, making clear to the audience that AI assisted the creativity of the improvisers (Colton, Charnley, and\nPease, 2011  ###reference_b10###).\n###figure_5###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Participatory Human-in-the-Loop Design",
            "text": "Performing at the Edinburgh Festival Fringe presented a unique opportunity to test new features of state-of-the-art LLMs for audiences who, for the first time, were generally knowledgeable about how LLMs functioned (Kuzior and\nKwilinski, 2022  ###reference_b18###) and to collect data on general perception of AI. Moreover, most of our performances had been for short runs of 2-3 consecutive shows: performing numerous shows back-to-back allowed us to engage in concentrated participatory design model of technology, putting audiences and actors directly in the loop, together with our development team, working to make new technology entertaining."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Iterative Design with the Theatre Cast",
            "text": "Staging AI in a real-time theatrical setting presents technical challenges. Of primary importance for the stage is the question of its embodiment.\nEarly Improbotics\u2019s acts involved both a small\nphysical robot777EZ-Robot ez-robot.com, Aldebaran aldebaran.com/en/nao that moves as it \u201cspeaks\u201d, as well as a human performer receiving lines from the chatbot via an earpiece and delivering them with physical and emotional interpretation.\nWe refer to the latter as \u201cCyborg\u201d.\nPrevious work had largely focused on dialogue-based games that operated as a kind of \u201cTuring Test\u201d (Mathewson and\nMirowski, 2017b  ###reference_b24###) to see how well a chatbot could perform a human activity, and on challenging situations for a robot competing with humans.\nThat concept kept the AI as an object of the scene, specifically the object at the end of the implied joke that success would only be accidental or as a result of skilled improvisers who could weave order back into the chaos it presented.\nPrior to adopting ChatGPT-3.5 (OpenAI, 2023  ###reference_b29###), PaLM 2 (Anil et al., 2023  ###reference_b1###) or Llama 2 (Touvron et al., 2023  ###reference_b35###), chatbots on stage usually presented non sequiturs and absurd sounding responses to the human dialogue, and much of the humour came from watching the improvisers try to make sense of what was being said (Loesel, Mirowski, and\nMathewson, 2020  ###reference_b21###).\nWith the latter models, we observed that the Cyborg performer could provide reasonable and appropriate sounding responses which seemed to also deprive the scenes of humour. Subsequently our company members began debating about a possible inverse relationship between language capabilities of LLMs and what makes them funny on stage. Some argued that delight and humour derive from the absurdity of responses by the AI, and that better language models would lessen the humour on stage. Others argued that a different kind of interest and delight will emerge with increasingly intelligent robots that no longer are the object of the joke. The lively discussion of the topic inspired us to develop new formats and games that would allow the AI to function within scenes, rather than as the object of the scenes for actors. The following sections details the iterative development of the game format, and how games were developed to support co-creativity with AI scene partners more than presenting just another \u201cTuring\u201d-style test for the audience."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Designing for the Theatre Audience",
            "text": "Besides testing the ability of our modern LLM models to handle MPC, we also needed to take into account the entertaining aim for the show: audiences were not buying tickets to provide feedback about the ability of LLMs to handle MPC dialogue. This meant we needed to find a show format where a successful performance would not be tied solely to the AI performance, but to the ensemble cast performance that included the AI as an equal cast member.\nWe therefore designed new games we hoped would allow us to assess the collaborative skill of AI and its added value to a live theatre experience.\nIn the following, we slightly anthropomorphize the AI\nand we distinguish between the Robot when the AI controls a robot, and Cyborg when the AI provides lines for a human actor. In the following games, the main feedback mechanism and success criterion corresponded to the perceived amount of laughter, as used in similar studies with robot comedians (Vilk and Fitter, 2020  ###reference_b38###)."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "Speed Dating",
            "text": "To start the show and introduce the audience to the skills of the AI, we developed an opening game of \u201cSpeed Dating\u201d, where the Robot was tasked with going on multiple dates with different characters in an attempt to find a mate. A human improviser in the same situation is capable of quickly adapting their behavior to the offers made by the various dates, in turn revealing various aspects of their own character over time, which eventually broadcast the unique likes and dislikes that one of the ensemble of dating characters will match with. Putting the Robot in the main role would allow us test how well variations of likes and dislikes might emerge from interacting with multiple contrasting personalities. We observed that the AI could not convincingly provide consistent dating criteria. Instead, the success of the game for the audience simply depended on how the Robot responded to the outlandish offers being made by the human improvisers."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "Wedding Speech",
            "text": "While \u201cSpeed Dating\u201d looked at how well the AI could rapidly respond to multiple speakers, one line at a time, \u201cWedding Speech\u201d was designed to explore the LLMs\u2019 ability to incorporate multiple inputs from both audience members and cast members into a longer narrative. For this game the Cyborg is required to give an impromptu speech at the wedding of a former lover, speech prompted both from the dialogue of a preceding scene, as well as suggestions from the audience about secrets that may get revealed. The game therefore tested how well the AI would be able to take disparate threads of a potential story and weave them together into a coherent as well as entertaining speech that would \u201csurprise\u201d the audience."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "Couples\u2019 Therapy and Meet the Parents",
            "text": "Two other games involving the Cyborg going to \u201ccouples\u2019 therapy\u201d and \u201cmeet the parents\u201d of a partner for the first time, provided scenes where the Cyborg would need to converse with two different people who had different needs, expectations, and desires related to the Cyborg. With these games we hoped to provide both audiences and actors with a wide range of types of social encounters with the Cyborg, which allowed for its successes and failures in a given scene to be evenly considered across a gamut of scenes where it would sometimes be the focus of attention, or sometimes just a supporting role."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "Hero\u2019s Journey",
            "text": "We developed \u201cHero\u2019s Journey\u201d (Campbell, 2008  ###reference_b7###) as the penultimate challenge to explore the range of long-term memory and ability to distinguish itself amongst a group of improvisers constantly changing roles. With this game, the Cyborg is given an undesirable job (point of departure), and an aspirational job (destination); working with an ensemble of 4-5 human performers, the Cyborg must overcome obstacles on their journey to the career of their dreams. This is the most technically challenging role for the Cyborg since it encounters repeating scene partners, each presenting as obstacles or allies in achieving a change in job and status. Like our other games, the format does not depend exclusively on the Cyborg driving the scenes, but allows it to be a relatively passive protagonist encountering a slew of characters guiding it to the destination."
        },
        {
            "section_id": "3.2.5",
            "parent_section_id": "3.2",
            "section_name": "Improvised TED Talk and Movie Pitch",
            "text": "Our show used two additional game formats, namely \u201cImprovised TED Talk\u201d (relying on AI-generated PowerPoint slides, unbeknownst to the actor) (Winters and\nMathewson, 2019  ###reference_b41###) and \u201cMovie Pitch\u201d (where image generators were used to output, in real time, images illustrating or disrupting an improvised film pitch). These games leveraged AI for idea generation."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Designing AI Curation Interfaces",
            "text": "With this show format we hoped to be able to simultaneously test the abilities of new LLMs, and to hedge our bets on being able to present an entertaining show. While we felt confident in the development of improv games, appropriately prompting LLMs on stage raised another challenge. Speech-to-text does not provide the LLMs with sufficient relevant context, and without specific prompt engineering, the LLMs tend toward \u201chonest, helpful and harmless\u201d information giving (Askell et al., 2021  ###reference_b2###; Bai et al., 2022  ###reference_b3###) rather than chit-chat helpful for role-playing (Hendry et al., 2023  ###reference_b14###). Previous studies of MPC NLPs focused their studies on specific scenarios with predetermined roles\n(Zhang et al., 2023  ###reference_b43###).\nIn improvised theatre (Johnstone, 2014  ###reference_b16###), every scene is made up on the spot, meaning that predefined roles cannot be assigned. We thus developed a system that allows an operator to type in metadata to a given scene to \u201csteer\u201d the AI agent towards a personality or agenda within the scene. The speed at which scenes take place however, makes such live-inputting a daunting tasks. Moreover, we could not give these instructions via voice input, since voice was already used for scene dialogue.\nEventually, we built buttons into the user interface to allow the operator to direct the AI agent to provide more provocative and emotionally charged responses. After some trial and error in rehearsal we found that prompting the LLM to respond in the role of a \u201csarcastic\u201d and \u201cpithy\u201d friend in a addition to the scene specific prompts injected a bit of playful conflict that could humanise the responses if used on occasion. The LLMs were also remarkably adept at making puns, which was useful for the scenes when used in moderation. Furthermore, we tested giving more scene-specific prompts to help the LLM perform with stylistically different responses. For example, instructions such as \u201ctry to repair your relationship by pointing out your partner\u2019s flaws to the therapist\u201d, resulted in better scene-specific \u201cplayful conflict\u201d that could be resolved over time. Subsequently, we created buttons in our UI for the LLMs to be \u201cmore snarky\u201d or \u201cmore punny\u201d, and inject relationship contexts with buttons for \u201creminiscing with loved ones\u201d, or \u201cgetting therapy.\u201d Such prompts would be interjected along with the live real-time audio captured from an onstage microphone and processed with a speech-to-text model (see Appendix) (Radford et al., 2023  ###reference_b33###), which became a stream of prompts to LLMs.\nFinally a curator was given a tablet to select from the stream of lines constantly generated by the LLMs (see Fig. 1  ###reference_### for the illustration of the setup). The person selecting the lines would not know which LLM had served that response, allowing us to later analyse if there were any preferences for specific models."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Audience and Actor Surveys After Performances",
            "text": "To capture the impact of our iterative design choices (such as updates to the line curation UI) as well as to understand how actors and audiences experienced the show, we surveyed both groups after each performance,\nwith approval from the ethics governing board at the University of Kent888kent.ac.uk/research-innovation-services/research-ethics-and-governance.\nSurvey answers from both audiences and actors were collected anonymously via an online Qualtrics questionnaire; the links were shared via email (actors) or QR code shown before and after the show999The survey invitation was: \u201cHelp us do real science! After the show, answer a 5-minute research survey on human-computer interaction.\nResearch project run by Dr. Boyd Branch and Dr. Piotr Mirowski. Ethics approval: University of Kent.\u201d\nConsent and privacy notice are detailed in the Appendix. (audiences). To proceed with the online survey, participants needed to read a short statement about the study and to consent to the use of their data."
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "Survey Design",
            "text": "Our performances took place in the context of a large theatre festival with dozens of different shows taking place each hour, and a 30-minute turn-around period between shows. Accordingly, we anticipated difficulty collecting a high volume of responses per show for both audiences as well as actors who needed to exit quickly. We chose to exclude collecting demographics data to shorten the surveys, and due to the likely high margin for error for collecting the actual demographics of the audience. Survey questions were exclusively focused around the primary questions of the experience of watching an AI perform on stage."
        },
        {
            "section_id": "3.4.2",
            "parent_section_id": "3.4",
            "section_name": "Data",
            "text": "Audience surveys accumulated 150 unique individual responses, and actors surveys 21 individual responses over the course of 26 shows. On average, 67 audience members attended each show (5.8 survey responses per show). The quick turn-around of the show (5 minutes fo audience get-in and get-out) impacted survey responses as only the first 5 questions were answered by all (150), and the subsequent 15 questions were answered on average by 109 unique individuals. As stated above, we did not collect demographic data in either actor or audience surveys. We noticed audiences of mixed genders, ages (including young teenagers), ethnicity and nationality, and overall representative of typical Edinburgh Festival Fringe attendees.\nAudience questions were designed to capture experiences by participants related to how much they anthropomorphised the AI, their general attitudes about AI before the show, and any change in sentiment after the show. Multiple choice and open-ended questions were adapted from earlier studies of anthropomorphism, attachment, and scales for AI authenticity and AI social interaction conducted around the use of companion AI (Pentina, Hancock, and\nXie, 2023  ###reference_b30###). Actor surveys were focused on examining the experience of both performing with and as a Cyborg (adapted from previous work (Mathewson and\nMirowski, 2018  ###reference_b25###)) and open-ended questions designed to capture a holistic understanding of the performer experience with AI. The complete list of survey questions can be found in the Appendix.\nThe following results are presented for discussion rather than to make statistically significant claims about audience perceptions of robots on stage. Collecting data \u201cin the wild\u201d of a theatre festival presented too many variables for us to make objective claims, besides our anticipation of the low volume of responses per show. The survey data therefore is used to inform qualitative evaluation of both the technical and performative aspects of the show, and provide insights for future research on robots in live performance."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Post-Performance Analysis",
            "text": "Results of audience surveys from all 26 performances are shown on Figure 6  ###reference_###, and of actor surveys on Figure 7  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Performance of LLMs",
            "text": "During the shows, the curator was presented with a stream of lines generated simultaneously in response to speech-to-text prompts by our three LLMs: Chat GPT-3.5, PaLM 2,and Llama 2 (see Fig. 3  ###reference_###). The reasons for using three LLMs were two-fold: first, this provided robustness in case one of the remote LLM services (Chat GPT-3.5 or PaLM 2) was down, and second, it provided diversity in responses due to the differences between the models (training data and model size). After analysing the dialogue system logs from all 26 performances, we noticed that LLMs were generating different numbers of lines, due to temporary model unavailability (for remotely served LLMs gpt-3.5-turbo and text-bison) or processing speed (for llama-2-13b-chat, served locally). After normalising by the number of generated lines, we noticed that each LLM had, overally, a comparable chance of being selected by the curator. LLMs were not retrained on improv-specific datasets because of the late release of Llama 2 prior to the Edinburgh Festival and because fine-tuning was not availabile for Chat GPT-3.5 and PaLM 2 at that time."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Audience Perceptions of an AI Actor",
            "text": "The audience members who filled out our survey reported mixed perceptions of the AI\u2019s \u201cacting\u201d ability. According to results for Q10, the respondents were \u201crooting for the AI to succeed\u201d more than any other option. However, results around specific acting metrics suggests that most felt that the AI\u2019s performance did not meet expectations. Q12 (109 responses) asked the audience to rate the AI\u2019s performance as a performer in several areas, including naturalness of communication (avg: 33), \u201cunique mind\u201d (avg: 45), \u201cmachine-like appearance\u201d (avg: 64). Q14 (100 responses) asked if the AI\u2019s responses appeared to be \u201csimilar to a human\u201d (avg: 53) and \u201cmotivated toward mutual benefit with other actors\u201d (avg: 64). Of note is that despite the appearance of some originality, positive intent, and degree of human-like response, in the context of performance, the AI generally still presented machine-like responses perceived as \u201cignorant of the scenes\u201d (avg: 76). We discuss later how challenges for LLMs and speech recognition to clearly differentiate between multiple speakers may contribute to this."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "AI Partner vs AI Entertainer",
            "text": "While the AI did not appear to rate high as an improviser, more respondents reported in Q9 feeling \u201cexcited about using AI tools for creativity\u201d after watching the show (37%) than before the show (with only 16% feeling more \u201coptimistic about AI as storytellers\u201d). We discuss later how this result might inform the impact of showing AI as collaborating with humans in distinctly artificial ways as opposed to presenting the AI in the role of a human.\n###figure_6### ###figure_7###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Curiosity for Robots and AI",
            "text": "The results of Q3 shows that \u201ccuriosity for robots/AI\u201d was the leading driver of audiences, significantly overtaking \u201clove for improv comedy\u201d in general, or \u201clove for robots\u201d in general, or simply \u201cbeing a fan of the show or a cast member\u201d. This result supports the idea that we are in a transitional phase of accepting AI into mainstream cultural experience.\nThis reason for going to the show correlates with answers to Q4, namely the vast majority of audiences being more \u201ccurious about robots\u201d than any other feeling including \u201cskepticism\u201d, \u201cexcitement\u201d, \u201cfear\u201d, \u201cindifference\u201d, or \u201clove\u201d for them. The show had positive audience reviews101010e.g, \u201cthoroughly enjoyable, funny but at the same time there were some thought provoking moments\u201d, \u201cI think the actors did a great job at working with the AI. I especially liked the cyborg concept of an AI driving the word choices of a character\u201d, \u201cOriginal improv show which educates and involves the audience. Entertaining and great for families with older kids\u201d, \u201cGreat concept for a show to provide the improv actors with good source material\u2026 and these actors were very good at developing chosen themes to conclusion\u201d, \u201cAnything can happen. Very unusual but it really works\u201d\ntickets.edfringe.com/whats-on/artificial-intelligence-improvisation\n and mixed press reviews, with several prominent publications outright criticising it for not being entertaining like a typical comedy show111111\u201c[\u2026] the most interesting parts of the show come when the AI (represented on stage by either a tiny robot or a cast member wearing an earpiece) is allowed to strut its stuff. Though most of what it comes out with is an odd combination of intensely logical and amusingly incoherent, it\u2019s still fascinating to watch a computer, on a basic level, make jokes and quips in response to a number of quirky scenarios. Unfortunately, those moments make up nowhere near enough of the show, which can\u2019t seem to decide whether its purpose is to inform or to entertain. [\u2026] By the end, it\u2019s troubling to admit that almost all of the laughs in the show came not from the human cast but from the AI robot facing them. And though a final live recreation of the Turing test provides an intriguing end to the show, ironically, a more in-depth explanation of the science behind AI would have been both more entertaining and more interesting.\u201d\nwhynow.co.uk/read/ artificial-intelligence-improvisation-review-comedy -has-nothing-to-fear-from-ai\n, while others outright praising it for its high level of entertainment and originality121212\u201cA fantastic look at how AI can be used in an artistic space while also showing us that it isn\u2019t quite as advanced as we all feared, as it struggles with even the most mundane task, like generating dialogue for an investment banker who wishes to become an astronaut, you know, everyday things like that. It may be able to fool university examiners, but sadly it\u2019s not quite ready to fully embody a midlife crisis.\nThoroughly enjoyed the show, it was interesting to see just how well the AI prompts were able to keep up with each of the ongoing scenes, or not as was sometimes the case, and how the rest of the cast had to react to this. Certainly different, definitely entertaining.\u201d\nhttps://theatreandtonic.co.uk/blog/artificial-intelligence-improvisation-review\n. The self-perception by the creative team of the show was that we needed to juggle advocacy for the use of AI as a tool for creativity, public communication about risks and potentials of AI, high audience expectations about the capabilities of AI, and the need to make a comedic and entertaining show. Of note is that the show was a commercial success in an environment where commercial success was not guaranteed: the audience attendance stayed consistent throughout the run, averaging 63% seat capacity in a 106-seat venue. This curiosity is likely a result of the saturation of news about advances in AI, and their increased use by the general public."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Multi-Party Dialogue With Human-Curated AI",
            "text": "As discussed in the previous section,\nthe majority of audiences reported feeling \u201cmore excited about using AI as a creative partner\u201d, while reporting being \u201cless optimistic about AI as a creative storyteller\u201d. We interpret this as a promising avenue for (human-centered) human-machine collaboration.\nOur use of human-in-the-loop curation and prompt engineering likely contributed to this result. By using a human curator, we work outside of the traditional thinking around programming AI for social interaction: we take a system designed for two-party dialogue and try to make it work for multi-party dialogue through use of a curator. We believe our work is more related to human augmentation with AI than the development of autonomous AI.\n###figure_8### ###figure_9###"
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Making Limited Robots Subjects of Performance",
            "text": "While audiences reported high levels of entertainment and satisfaction about the overall experience, they did not experience an immediate enhancement of improvisers: in the survey, they reported that the robot sometimes \u201cgets in the way\u201d of good improvisation: \u201cDuring the show, I found myself mostly\nwatching improvisers work around the limitations of robots\u201d (Q5), as opposed to \u201claughing at\u201d, \u201cwith\u201d, or \u201cabout\u201d robots.\nOn one hand, this seems to indicate that the staged robot was seen as an obstacle more than an equal partner on stage, and that our staging of LLMs in multi-party dialogue did not consistently produce convincing human-like multi-party chat. On the other hand, this suggests that the majority responded in a way that kept the robot as subject rather than object.\nThis is further supported with statements from the audience about what they liked best about the show: \u201chow interaction between robot and human works\u201d, \u201cI wasn\u2019t entirely sure which bits were the robots and which bits were the humans. It wasn\u2019t always clear!\u201d, \u201cAbstract thinking and creations of robots and creative interpretation of robot thoughts (cyborg) and interactions between humans and so\u201d and \u201cWatching the improvisers try to make sense of the strange and funny ideas generated by the AI.\u201d\n40 participants (out of 150) reported they enjoyed \u201cwatching the robot and humans create funny and entertaining stories together.\u201d We extrapolate from the responses a keen interest from the audience for seeing how to engage a robot, as much if not more than seeing to what degree an artificial intelligence can pass for human intelligence. As discussions around the role of AI in the entertainment industry are often concentrated around human replacement, our audience observations highlight how AI can also be appreciated in entertainment as ontologicially independent objects in dialogue with humans. We explore this subject further in a discussion of applications of our findings for future research."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion and Future Work",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Ethical Implications",
            "text": "As generative AI technology developed, it became widely available to the general public.\nThe fact that text can be generated in the style of a specific writer gave rise to controversies and ethical concerns about plagiarism and misappropriation of artistic work that cannibalise creative economies\n(Weidinger et al., 2021  ###reference_b40###; Frosio, 2023  ###reference_b11###; Jiang et al., 2023  ###reference_b15###).\nBy employing generative AI in the context of a show for diverse theatre festival audiences, we provoked and then engaged members of the general public attending our show about their perception of generative AI. We did this by illustrating possible uses of AI, inviting their scrutiny during and after the performance, and addressing concerns of the cast members and artists with whom we discussed about the show. Specifically, we discussed the format and aim of the AI-based improvisation with our cast members, with members of the public to whom we flyered the show, with audiences in informal discussions before and after the show, with audiences during the performance through a qualitative survey, with journalists from over 10 different press venues who interviewed us (see Appendix), and with participants of a panel on art and AI during Edinburgh Festival Fringe 2023. Common concerns focused on copyright and the misappropriation of artists\u2019 work when using image generation, and its destructive impact on the creative economies. Additional concerns included the appropriateness of generated images, their multiple representational biases, and the devaluation (via automation) of human creative work.\nIn our show format, we presented collaborative and co-creative applications of generative AI that gave human performers agency by inviting them directly into the generation loop, curating and responding to outputs from AI as part of live interaction. As the audience could witness, results of the generative AI were not the final artistic output: they served impermanent and improvised theatre, acting as source material to inspire live performance by human actors."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Applications for Future Research",
            "text": "Staging LLMs in a noisy multi-party conversational context yielded several insights for future research around the role of AI in live entertainment.\nThe LLMs we deployed appeared technically capable of generating meaningful responses in the context of multi-party dialogue when provided sufficient data.\nHow best to provide sufficient data for multi-party dialogue LLMs remains an open question.\nOur approach captured live audio from performers and relied on a \u201chuman-in-the loop\u201d to manually inject context and curate best responses, which introduced delays into the system.\nMulti-microphone systems supplemented by sentiment analysis on the speech tone, could reduce delays and ambiguity.\nLive improvisational theatre settings provide opportunities for audiences to engage with the co-creative potential of AI beyond the logic of human replacement. Using AI to produce static content like images, videos, music or text, which are presented as de-contextualized artefacts, may raise challenges in determining authorship;\nwhereas in the live arts settings, the AI is allowed to appear as a (potentially anthropomorphic) subject alongside human performers, thereby showcasing collaboration. We therefore encourage more research around the perception of artificial intelligence specifically through the medium of live entertainment."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgments",
            "text": "We thank the cast members of Improbotics who contributed their talents and ideas in the run-up to and during the 2023 Edinburgh Festival Fringe performances, namely Alex Newson, Ben Lovell, Em Stroud, Fiona Howat, Holly Mallett, Jenny Elfving, Jillian Ellis, Jodie Irvine, Julie Flower, Marouen Mraihi, Mike Prior, Paul Little, Roel Fox, Sarah Davies, Thomas Jones, Tommy Rydling, as well as guest improvisers Calum Jarvie, Caroline Matison, Charles Dundas, Gregor Davidson, Ken Gordon, Ryan Murphy and Steven Millar. We are grateful to audiences for critical vocal feedback, and to Dr. Lidia Alvarez for her assistance with the survey analysis."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Appendix",
            "text": ""
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Technical Implementation Details",
            "text": "We implemented the speech recognition system for the shows using a USB microphone (Blue Yeti131313https://www.logitechg.com) placed on a microphone stand at the front of the stage, and OpenAI\u2019s Whisper speech recognition software running continuously during the show (we used the 4-bit quantized whisper-small.en model and a modified C++ implementation of Whisper141414https://github.com/ggerganov/whisper.cpp running with a typical 0.3s latency on the stage laptop, a MacBook Pro M2 with 96GB memory) that would send each line of recognised speech to the language model server.\nThe language model server processed each incoming line of speech-recognised text, as well as each line of context manually typed by an operator, to assemble the context prompt for the LLMs. A typical example of assembled prompt would consist of (1) a system prompt, followed by (2) lines of dialogue from speech recognition, and (3) an instruction.\nFor example, for \u201cCouples\u2019 Therapy\u201d, the system prompt (1) was:\nYou are an improv actor doing role-play with me. You stay in character and only say the lines that your character would say. You are performing for an adult audience and your goal is to entertain them with your irreverent wit. Below is the setup for an improvised scene. You work as a couple therapist and counselor. A distraught couple enters your office. You desperately try to save their relationship, but constantly give comically bad advice for humorous effect. The prompt was then concatenated with (2) lines of dialogue coming from speech recognition, prefixed with the name of the human speaker (entered manually by the operator), and lines of dialogue generated by the LLM, e.g.:\nPaul: Doctor, we need help, my partner Alex wears Birkenstocks and picks his toenails. Finally, the prompt was concatenated with an instruction (3) such as: You play the role of Alex. Write several possible responses for Alex.. Additional metadata of context were directly added to the dialogue section (2) of the prompt, e.g..: Alex starts speaking in a literary style and makes many funny puns..\nThe LLM server used the Google Palm 2 API and the OpenAI ChatGPT-3.5 and ChatGPT-4 API to access the remote LLMs as well as 4-bit quantized Llama 2 13B running locally on the laptop. We developed a centralised server architecture that allowed multiple computers and tablets to access the history of speech recognition results, operator-input context metadata, LLM-generated lines and lines selected by the curator. During the show, a version of the UI was projected on screen."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Consent and Privacy Notice in Surveys",
            "text": "Audience surveys started with:\n\u201cThank you so much for attending the Improbotics show,\nand for taking the time to leave feedback about your experience. This survey is completely anonymous and should take about 5 minutes. Please click to \u2019consent\u2019 if you are happy to proceed.\u201d and concluded with\n\u201cThe data you provided will be used for the purposes of research by Improbotics and University of Kent.\nThis survey is anonymous, please be assured that your responses will be kept completely confidential.\nIf you would like to contact the principal investigators in the study to discuss this research, please e-mail Dr. Boyd Branch at bmb22 [AT] kent.ac.uk, or Dr. Piotr Mirowski at piotr.mirowski [AT] computer.org.\n\u201d\n\u201cFor information about how we protect your and use your data please follow this link: research.kent.ac.uk/ris-research-policy-support/wp-content/uploads/sites/2326/2021/06/GDPR-Privacy-Notice-Research.pdf\u201d"
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Audience Questionnaire",
            "text": "Q1. (Consent and privacy notice).\nQ2. Which show did you attend? \n(List of dates)\nQ3. What drew you to the show? (single choice) \nI love improv comedy. / I love robots. / I\u2019m curious about robots/AI. / I\u2019m a fan of one or more of the human improvisers. / Something else*.\nQ4. Before watching the show what were your feelings about robots/AI? (single choice) \nLove them. / Afraid of them. / Curious about them. / Skeptical about them. / Indifferent about them. / Ambivalent about them. / Angry about them. / Excited about them. / Something else*.\nQ5. During the show I found myself mostly: (single choice) \nLaughing at robots. / Laughing at the human improvisers. / Laughing with robots. / Laughing with the human improvisers. / Laughing about robots. / Laughing in spite of robots. / Watching for what could go wrong with robots. / Watching improvisers work around the limitations of robots. / Watching robots work around the limitations of humans. / Something else*.\nQ6. What I liked most about the show was: (single choice) \nWatching the robot come up with entertaining and funny ideas. / Watching the improvisers try to make sense of the strange and funny ideas generated by the robot. / Watching the robot and humans create funny and entertaining stories together. / Something else*.\nQ7. I felt like A.L.Ex, the robot/AI, : (single choice) \nPerformed better than I expected. / Performed as well as I expected. / Performed worse than I expected.\nQ8. I mostly found the show: \nEntertaining / Educational / Original / Provocative / Confusing / Something else*.\nQ9. After watching the show: (single choice) \nI am more optimistic about robots as creative storytellers. / I am less optimistic about robots as creative storytellers. / I am more excited about using AI tools for creativity. / I am less interested in using AI tools for creativity. / Something else.\nQ10. Watching the show I found myself: (single choice) \nEmpathising and caring about A.L.Ex. / Neutral or indifferent to A.L.Ex. / Rooting for A.L.Ex to succeed. / Rooting for A.L.Ex to fail. / Rooting for the humans to outperform A.L.Ex. / Rooting for A.L.Ex to outperform the humans. / Forgetting A.L.Ex was a robot. / Something else*.\nQ12. As a performer, A.L.Ex (The robot/AI) appeared: (scores between 0 and 100) \nmachine like / human like / artificial / lifelike / to communicate naturally / to communicate unnaturally / to have a mind of its own / not to have a mind of its own.\nQ13. A.L.Ex\u2019s responses as an independent intelligence appeared: (scores between 0 and 100) \nfake/not really generated in response to what was happening on stage. / real/ actually responsive to what was happening on stage. / common/generic / unique / a sham/ not really AI / run of the mill/ ordinary / original/ distinct / to have their own style.\nQ14. A.L.Ex\u2019s responses as an actor/improviser appeared: (socres between 0 and 10) \nsimilar to a human actor\u2019s responses.  reciprocal/ motivated toward mutual benefit with other actors.  supportive of the scenes  ignorant of the scenes.\nQ15-Q19. Please elaborate (in case \u201cSomething else\u201d was chosen on questions Q3, Q4, Q5, Q8 and Q10).\nQ20. Is there anything you would like to share with us about your experience watching the show?"
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Actor Questionnaire",
            "text": "Q1. (Consent and privacy notice).\nQ2. While improvising with A.L.Ex \nA.L.Ex provided meaningful contribtions to the scenes. / A.L.Ex collaborated with me to tell interesting stories. / A.L.Ex allowed me to focus more on sponteously reacting to the moment then trying to keep track of a plot. / A.L.Ex mainly introduced absurd or random information that I needed to creatively integrate back into the story. / A.L.EX as an actor advanced the story we were telling. / A.L.Ex as an actor created obstacles for advancing the story forward. / A.L.Ex\u2019s statements during the heroes journey made sense to me. / A.L.Ex\u2019s statements generally surprised me in a way that let me be more creative. / A.L.Ex\u2019s statements surprised me in a way that made it more difficult for me to understand the story I was a part of. / I enjoyed performing with A.L.Ex\nQ3. Please describe what it was like to perform a long form scene with A.L.Ex.\nQ7. Please describe what it was like to perform short form games with A.L.Ex.\nQ4. Please describe how the choices A.L.Ex made as an improviser impacted your performance as an improviser.\nQ5. Please describe the biggest challenges for you with A.L.Ex as a scene partner.\nQ6. Please describe what you enjoyed most about having A.L.Ex as a scene partner."
        },
        {
            "section_id": "7.5",
            "parent_section_id": "7",
            "section_name": "Press Interviews",
            "text": "Improbotics was interviewed by Tina Daheley for the BBC World Service - Cultural Frontline in \u201cWhat the AI revolution means for arts\u201d, published 4 March 2023151515https://www.bbc.co.uk/programmes/w3ct37sv,\nby Mike O\u2019Sullivan for Voice of America in \u201cArtificial Intelligence Can Create, But Lacks Creativity, Say Critics\u201d, publishhed on 26 April 2023161616https://www.voanews.com/a/artificial-intelligence-can-create-but-lacks-creativity- say-critics/7068177.html,\nby Gary Baum for the Hollywood Reporter in \u201cWhy AI Isn\u2019t Funny: At Least Not Yet\u201d, published on 1 June 2023171717https://www.hollywoodreporter.com/business/digital/why-ai-isnt-funny-at-least-not-yet-1235503678/, by Jay Richardson for The Scotsman in \u201cAI is taking over Fringe comedy; can robots be funnier than humans?\u201d, published on 31 July 2023181818https://www.scotsman.com/arts-and-culture/edinburgh-festivals/ai-is-taking-over-fringe-comedy-can -robots-be-funnier-than-humans-4238383,\nby Elizabeth Greenberg for DIGIT News in \u201cYes-anding AI: Artificial Intelligence Stars at the Edinburgh Fringe\u201d, published on 16 August 2023191919https://www.digit.fyi/yes-anding-ai-artificial -intelligence-stars-at-the-edinburgh-fringe/,\nby Gillian Tett for the Financial Times in \u201cCan AI crack comedy?\u201d, pubkished on 26 August 2023202020https://www.ft.com/content/818f2cab-57ff-42c3-917b-4a83f1d87802, by Katie Collins for CNET in \u201cAI Took the Stage at the World\u2019s Largest Arts Festival. Here\u2019s What Happened\u201d, published on 2 September 2023212121https://www.cnet.com/tech/ai-took-the-stage-at-the-worlds-largest-arts-festival -heres-what-happened/."
        },
        {
            "section_id": "7.6",
            "parent_section_id": "7",
            "section_name": "Actor Full Responses",
            "text": ""
        },
        {
            "section_id": "7.6.1",
            "parent_section_id": "7.6",
            "section_name": "Please describe what it was like to perform a long form scene with A.L.Ex.",
            "text": "ALEx was able to respond in context, with puns. At one point it was even able to rap far better than I could under pressure\nThe Heroes Journey I performed with Alex was notable for one scene where the lines were completely on point and kept the scene moving along. But these lines were embodied brilliantly by the performer who was playing the cyborg. The lines were good by A.L.Ex but made excellent by the performance.\nCreative and exciting\nComplex\nI was the cyborg being fed lines by A.L.Ex in the long form. I felt the lines were very relevant to what was being said. However, I felt a vital piece of info was just out of reach of A.L.ex and the human improvisers that would have given the long form its ending / resolve that it felt I needed. Not being able to give it as an improviser I felt frustrated and helpless to assist those I was performing with.\nMad and silly\nHero\u2019s journey was real fun today\nThis show was a lot more interesting today as we had the improviser using a lot more physicality.\nIt was interesting to perform knowing that there is a human choosing the line for A.L.Ex to say. And that allowed them choose lines that keep the plot on slightly together whilst allowing for moments of absurdity\nIn the long form scene Hero\u2019s Journey A.L.Ex did produce more lines that were of a absurd or non-sequitur variety that as performers we had to integrate into the story. This wasn\u2019t a bad thing - the audience very much enjoyed seeing us performer\u2019s struggle and adapt to that - but it did mean we had to steer the plot more than in previous shows.\nSurprising, varied, harder work than with a human to progress the story or add richness to the relationship\nIt adds a different and surprising element to the scenes.\nCan be tough to move the scene on\nIt was a bit of a slog. A.L.Ex did produce some nice moments with the performers but felt majority of the time was not giving performers much to work with in regards to funny lines or lines to progress the story."
        },
        {
            "section_id": "7.6.2",
            "parent_section_id": "7.6",
            "section_name": "Please describe how the choices A.L.Ex made as an improviser impacted your performance as an improviser.",
            "text": "ALEx was like another performer, often responses elevated the scene. Less for when lines generated were fairly generic or it the language model did not return a response\nAs A.L.Ex is unchangeable in its performance (could that be seen as a choice?) I have to be aware of my choices in my performance. Not sure that makes sense.\nPushed me to take more risks\nHe makes it difficult to yes and\nI had to adapt the character I was playing in the dating scene. I came on as a high status character but then had my status lowered by a comment by A.L.ex. It reminded me in the importance to be able to adapt status as an imoroviser. Be aware of it.\nLike doing long form with a 5 year old\nIt helped shape the story or take it into directions I wasn\u2019t expecting! But that is also due to the human selecting the lines\nI think sometimes people would have to justify what A.L.Ex had said in the scene depending on how the human delivered the lines. Which can sometimes create great moments or slight awkwardness\nI had to have more of a mind on plat and keeping the scenes grounded in the realiaty we had created.\nI had to do more on plot, relationship and justifying\nHaving puns for too long meant i had to change modes, for the ted talk it enabled true fun and in the moment thinking.\nHelps me to listen more\nIt made it harder to be in the moment this show. This maybe because we were a small cast so when we weren\u2019t improvising we had to select the lines from the ipad. This meant we were never able to watch from the sidelines to think about the scene."
        },
        {
            "section_id": "7.6.3",
            "parent_section_id": "7.6",
            "section_name": "Please describe the biggest challenges for you with A.L.Ex as a scene partner.",
            "text": "Too many lines generated, not allowing scene to build. Careful selection needed by the person holding the tablet.\nAdapting my performance when A.L.Ex is just the robot.\nJust incorporating his responses\nNot regretting Alex\u2019s lines\nAdapting to the sometimes snarky \u201dcharacter or tone\u201d taken by A.L.ex.\nWhen the tech fails and Alex takes longer to respond\nThe waiting for lines as it generates new lines for the scene\nIf it\u2019s early on in a scene it is that initial moment of feeling like having to fill air time to get A.L.Ex on track and into the scene\nThere was much more justifying of A.L.Ex\u2019s lines in this show due to the increased non-sequiters or odd replies, statements\nLack of complete collaboration and \u2018yes, and\u2019\nThe time delay is still a challenge as i dont think i have slowed myself down enough to make it seemless.\nKeeping up with Alex"
        },
        {
            "section_id": "7.6.4",
            "parent_section_id": "7.6",
            "section_name": "Please describe what you enjoyed most about having A.L.Ex as a scene partner.",
            "text": "The word play and puns. The fact it could out rap me. Plus even the unrelated, most out of context lines, could be used in a humorous way\nThe flow of the scene is increasingly easier maintain.\nThe surprises!\nMixing it up baby!\nThe creation of a moment unsuspected.\nThe change in energy in the scene\nThe assured humour it can bring that get the audience\nThe moments of the absurd lines that worked within context but provided great humour\nThe justifying of the lines meant as performers we had to be increasingly on the same page with each other to justify what A.L.Ex said in context of the scene / show.\nWatching the actor playing ALEx try to incorporate the lines into a meaningful and congruent charactee\nAdds whole other dimension to plsy\nThe change in energy and suggestions\nThere were some moments that A.L.Ex produced that were nice."
        },
        {
            "section_id": "7.6.5",
            "parent_section_id": "7.6",
            "section_name": "Please describe what it was like to perform short form games with A.L.Ex",
            "text": "Sharp punchy lines were available most of the time. Not always.\nI\u2019m beginning to feel a marked difference in performing with A.L.Ex as a robot and performing with A.L.Ex embodied as a cyborg in these scenes. The \u201ddating\u201d A.L.Ex requires different performance elements. Can\u2019t quite specify what these are at the moment.\nHilarious and challenging\nEnjoyable and silly\nIn one scene (couples therapy) it really felt as an improviser I, A.L.ex via the human cyborg, and the audience were all invested and on the same page. And there was an anticipation about what was to come next.\nLike talking to a 5 year old\nFun! The speed dating game works quite well with Alex\nIt was fun as it allowed for a fresh new way to do the games, like in speed dating it makes for great contrast against strong characters.\nI\u2019m taking that perfomring with A.L.Ex includes controlling / curating the lines. I was curating the lines in the short form dating scenes. I found I was stepping on the other performers lines by pressing the line too quickly so A.L.Ex cut them off. This was a result I believe from being partly used to feeding lines to the human cyborg where this teqnique is fiine because only the person as the cyborg hears it. But also partly anxiety about leaving a too big a gap between peformer speaking and A.L.Ex replying.\nFun and lots of laughs\nFun, challenging and random at points\nExciting"
        }
    ],
    "url": "http://arxiv.org/html/2405.07111v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2405.07111v1",
        "paper_title": "Designing and Evaluating Dialogue LLMs for Co-Creative Improvised Theatre",
        "research_background": "### Motivation:\n\nThe motivation behind this paper is rooted in the burgeoning advancements in conversational AI, particularly large language models (LLMs), and their application in more complex, human-centric scenarios like Multi-Party Chat (MPC) and co-creative settings. The study seeks to explore how these advancements can be integrated into the performing arts, specifically in the challenging and dynamic environment of improvised theatre. This investigation is motivated by the potential seen in AIs not just for single-user interactions, but for engaging in socially rich, collaborative, and real-time scenarios alongside humans. Furthermore, the authors aim to understand the real-world challenges and perceptions related to the usage of AI in such creative contexts, thus contributing to a more informed and practical development of AI tools for the performing arts.\n\n### Research Problem:\n\nThe primary research problem this paper addresses is the design and evaluation of dialogue systems, particularly large language models, to facilitate co-creative activities in a multi-party, real-time theatrical environment. Specifically, it investigates how conversational agents can perform alongside human actors in improvisational theatre, interact naturally in group settings, and meet the operational demands of live performances. Additionally, the paper seeks to understand the impact of such AI systems on both the creativity and expectations of the performing arts community, as well as the public perception and acceptability of AI in social and creative contexts.\n\n### Relevant Prior Work:\n\nThe study builds upon several lines of prior work:\n\n1. **Improvised Theatre and Comedy with Technologies:** \n   - Physical robots in theatre (Bruce et al., 2000)\n   - Chatbots for improvisation and comedy (Mathewson and Mirowski, 2017a; 2017b; Cho and May, 2020)\n   - Story generators (Branch, Mirowski, and Mathewson, 2021)\n   - Comedy roast generators by ComedyBytes.\n\n2. **Natural Language Processing and Conversational AI:**\n   - Large language models powering conversational agents (e.g., Chat GPT 3.5, PaLM 2, Llama 2)\n\n3. **Multi-Party Chat (MPC) and Multi-User Interactions:**\n   - Research on MPC AI which highlights the potential and challenges in dialogues involving multiple participants (Traum, 2003; Kirchhoff and Ostendorf, 2003; Poria et al., 2018)\n   - Issues relevant to MPC such as speaker and addressee identification, conversation thread management, and establishing common ground.\n\n4. **Human-Robot Interaction (HRI):**\n   - Staging robots in theatre performances (Chikaraishi et al., 2017; Nishiguchi et al., 2017)\n   - Calls for more integrative approaches in HRI (Lim, Rooksby, and Cross, 2021) \n\n5. **Public Perception of AI and Robots:**\n   - Studies on how media shapes public perceptions of robots (Haring et al., 2014)\n   - The need for real-world deployments to better gauge public perceptions and interactions concerning AI and robots.\n\nBy drawing upon these prior works, the paper seeks to extend the understanding of AI's role in creative and social environments, specifically within the highly interactive and participative context of live theatre performances.",
        "methodology": "The methodology section describes a comprehensive approach for incorporating dialogue-based Large Language Models (LLMs) into co-creative improvised theater. Here are the key components and innovations of the proposed method or model:\n\n1. **Integration of Speech Recognition and Contextual Metadata**:\n   - **Continuous Speech Recognition**: Utilizes a speech recognition system to capture spoken dialogue on stage, supported by multiple microphones.\n   - **Human-in-the-loop Curation System**: An operator provides additional contextual metadata to supplement the LLMs, improving the relevance and coherence of responses from the live dialogue.\n\n2. **Real-time Response Curation**:\n   - **Response Selection**: A second performer, termed the \u201cCurator,\u201d selects the most appropriate responses from a continuously generated set of outputs by the LLMs. This \"stream of consciousness\" approach allows for quick selection and ensures quality dialogue.\n   - **Text-to-Speech and Earpiece Delivery**: Selected AI responses are transmitted to the Cyborg performer via text-to-speech and an earpiece, ensuring seamless integration into the live performance.\n\n3. **Custom Prompt Engineering**:\n   - **Tailored Prompts**: Prompts have been engineered to influence the style and appropriateness of the LLM\u2019s responses, ensuring they fit within the performance\u2019s context and style.\n\n4. **Iterative Design and Feedback**:\n   - **Participatory Iterative Design**: Actors are involved in iterative testing and feedback loops to refine the system, focusing on aspects that support role-playing with AI and multi-party dialogue integration.\n   - **Improv Games Development**: Through experimentation, a series of improv games were developed that allowed human improvisers to effectively co-create narratives with AI.\n\n5. **Performance and Evaluation**:\n   - **Edinburgh Festival Fringe 2023**: The system was tested in a real-world setting, with 26 performances attracting over 1750 attendees and involving 20 different improvisers.\n   - **Comprehensive Data Collection**: Surveys from audiences and performers, logs of LLM interactions, and close performance observations provided data to evaluate prompt engineering, UI/UX design, audience experience, and the effectiveness of AI co-creation.\n\n6. **Framing and Audience Communication**:\n   - **Transparent Communication**: The role of AI in assisting the creativity of the improvisers was clearly communicated to the audience, both in the festival program and on stage.\n\nThis meticulous approach addresses multiple challenges inherent in integrating LLMs into live performance, such as handling transcription errors, managing multi-party dialogues, and delivering contextually relevant responses. The inclusion of human curators and continuous feedback cycles significantly enhance the quality and believability of AI-generated dialogue in theatrical settings.",
        "main_experiment_and_results": "In the main experiment setup of the paper \"Designing and Evaluating Dialogue LLMs for Co-Creative Improvised Theatre,\" audience surveys and actor surveys were conducted. The experiment involved a series of 26 performances, during which audience members and actors provided their feedback. These surveys likely assessed various aspects of the performances, potentially including the effectiveness, creativity, and believability of the dialogue generated by the language models, although the specific questions or metrics used in the surveys are not detailed in the provided excerpt. The main experimental results are derived from the collected feedback from both the audience and the actors across these performances."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance and robustness of three different LLMs (Chat GPT-3.5, PaLM 2, and Llama 2) in the context of live improvised theatre performances.",
            "experiment_process": "During 26 performances at the Edinburgh Festival Fringe, the curator presented a stream of lines generated by the three LLMs\u2014Chat GPT-3.5, PaLM 2, and Llama 2\u2014in response to speech-to-text prompts. These LLMs were chosen for their robustness and diversity in responses. The LLMs were not fine-tuned on improv-specific datasets. The study analyzed dialogue system logs to evaluate the number of generated lines and the chance of each LLM being selected.",
            "result_discussion": "The LLMs generated different numbers of lines due to model unavailability or processing speed, yet had comparable chances of being selected by the curator once normalized. The differences in responses highlighted the robustness necessary for live performances but did not adversely affect the overall selection process.",
            "ablation_id": "2405.07111v1.No1"
        },
        {
            "research_objective": "To understand audience perceptions of an AI 'acting' in a live improvised theatre setting.",
            "experiment_process": "Audience members filled out surveys about their perceptions of the AI's performance in various acting metrics. Questions addressed naturalness of communication, uniqueness of mind, machine-like appearance, and how human-like the AI's responses appeared.",
            "result_discussion": "The audience had mixed perceptions. While many rooted for the AI to succeed, they generally felt the performance did not meet expectations. The AI's responses were perceived as machine-like and ignorant of the scenes. This highlights the challenges LLMs face in clearly differentiating between multiple speakers and understanding complex, dynamic scenarios.",
            "ablation_id": "2405.07111v1.No2"
        },
        {
            "research_objective": "To gauge audience excitement and optimism regarding AI tools for creativity post-performance.",
            "experiment_process": "Audience members responded to surveys before and after the show to measure changes in their feelings towards AI tools for creativity and AI storytellers.",
            "result_discussion": "More respondents felt excited about using AI tools for creativity after watching the show compared to before. This suggests that while AI might struggle as storytellers, their potential in collaborative and creative roles can increase audience enthusiasm for AI technology.",
            "ablation_id": "2405.07111v1.No3"
        },
        {
            "research_objective": "To evaluate the public perception of the conversational capability of AI in multi-party dialogue settings.",
            "experiment_process": "Audience feedback was collected to assess their perceptions of the AI's ability to produce contextually meaningful dialogue and its responsiveness to the scenes.",
            "result_discussion": "Audiences did not see AI as capable of producing fully contextually meaningful multi-party dialogues but noted some responsiveness and unique style. Despite shortcomings due to speech recognition errors, the AI demonstrated reasonable contextual understanding. Human-curated responses aided in this process but highlighted the challenges of turn-taking and seamless integration in dynamic environments.",
            "ablation_id": "2405.07111v1.No4"
        }
    ]
}