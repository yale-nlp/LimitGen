{
    "title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning",
    "abstract": "Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision.\nTo address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making.\nSpecifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment.\nAs the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities.\nOur approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN.\n\n\n\nKeywords:\u2009Vision-and-Language Navigation, Cross-modal Alignment, Cross-modal Contrastive Learning",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Vision-and-Language navigation (VLN) has attracted increasing interest from natural language processing, computer vision and robotics communities due to its potential of applications such as embodied robots (Gu et al., 2022  ###reference_b10###). The VLN task requires the agent to navigate to a predetermined destination, guided by natural language instruction and real-time visual observations. As depicted in Figure 1  ###reference_###, a comprehensive navigation instruction typically comprises a sequence of action, direction and landmark clues. The agent needs to proficiently model its historical experiences and accurately relate the landmark clues to the environmental observations in order to successfully navigate.\n###figure_1### Similar to traditional Vision-Language task, the general model architecture of VLN agent adopts a multi-modal late-fusion paradigm (Du et al., 2023  ###reference_b8###). Under this paradigm, individual modalities are encoded by their respective encoders and then passed through a fusion module, as illustrated in Figure 2  ###reference_###. In Vision-Language task, aligning unimodal representations before fusion has been extensively validated as it contributes to cross-modal modeling ability in the fusion stage and semantic comprehension ability of unimodal encoders (Li et al., 2021  ###reference_b19###, 2022b  ###reference_b21###). However, pre-fusion alignment has not yet been explored in the context of VLN tasks, resulting in non-negligible disparities among correlated modalities before fusion.\nTherefore, we propose to align corresponding unimodal representations and bridge the modality gaps prior to the cross-modal fusion stage in VLN task. This idea is challenging for two reasons. Firstly, there are limited training signals available for aligning unimodal representations. Training signals about navigation, such as navigation progress and relative spatial position, are used for supervising post-fusion representations in previous works (Ma et al., 2019a  ###reference_b26###; Chen et al., 2021  ###reference_b3###). Secondly, VLN is a sequential decision process distinct from traditional Vision-Language tasks (Hong et al., 2021  ###reference_b14###).The navigation history condenses visited panorama and performed actions to retain past information, while observation at each time step contains detailed landmark semantics corresponding to the instruction. Consequently, how to effectively leverage the different characteristics of navigation history and real-time observations to enhance the cross-modal alignment remains an open question.\n###figure_2### To address these problems, we propose a model-agnostic Dual-levEL AligNment (DELAN) framework that utilizes cross-modal contrastive learning. As the training signals are limited, we propose the self-supervised contrastive learning approach, relying on the contrast between positive and negative pairs within a batch as the training signal. For the diverse characteristics of unimodal representations, we partition the cross-modal pre-fusion alignment into two separate levels: instruction-history level alignment and landmark-observation level alignment, according to their strong semantic correlations. Specifically, history modality records visual experiences along the described navigation trajectory. The landmarks, such as \u201ccouch\u201d and \u201ccloset\u201d, appearing in the real-time observations constitute the textual foundation of language instructions. Furthermore, we restructure the original instruction into a dual-level instruction-landmark format to explicitly formulate the dual-level semantic correspondences with history and observation. For instruction-history alignment, we employ a contrastive loss on the history tokens and the instruction part of dual-level instruction across both global and local representations. For landmark-observation alignment, we use a contrastive loss on the observations and the landmark part of dual-level instruction at each time step across only local representations.\nWe summarize our contributions as follows: (1) We are the first to introduce a cross-modal contrastive learning framework to improve the pre-fusion alignment in VLN task; (2) Inspired by the characteristics of different VLN modalities, we propose to conduct the dual-level alignment before fusion, i.e. instruction-history level alignment and landmark-observation level alignment, and develop self-supervised learning strategies respectively. (3) We validate our framework across various VLN benchmarks, demonstrating the effectiveness and consistency of our DELAN framework.111Code is available at https://github.com/mengfeidu/DELAN  ###reference_###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Vision-and-Language Navigation",
            "text": "In recent years, training an instruction-following navigator has attracted growing research interest.\nEarlier studies mainly employ a sequence-to-sequence LSTM framework in VLN tasks Fried et al. (2018  ###reference_b9###); Tan et al. (2019  ###reference_b33###).\nOwing to the success of transformer Vaswani et al. (2017b  ###reference_b37###) in Vison-Language tasks (Tan and Bansal, 2019  ###reference_b34###; Lu et al., 2019  ###reference_b25###; Li et al., 2020  ###reference_b20###), transformer-based models are widespread in the VLN field.\nVLNBERT Hong et al. (2021  ###reference_b14###) devises a recurrent unit in the cross-modal transformer to model the navigation history.\nHAMT Chen et al. (2021  ###reference_b3###) proposes to encode all past observations and actions with hierarchical transformer.\nDUET (Chen et al., 2022  ###reference_b4###) utilizes a graph transformer to encode global history for efficient planning.\nBesides the improvement of model architecture, various other methods are proposed to enhance the navigation performance, including training strategies Wang et al. (2019  ###reference_b39###); Zhang et al. (2021  ###reference_b45###); Liang et al. (2022  ###reference_b22###) and auxiliary tasks Ma et al. (2019b  ###reference_b27###); Zhu et al. (2020  ###reference_b47###).\nDespite the progress above, aligning navigation modalities remains a formidable challenge.\nOAAM Qi et al. (2020  ###reference_b30###) and RelGraph Hong et al. (2020a  ###reference_b12###) utilize separate soft-attention modules to implicitly align visual representations with different attribute words.\nFGR2R Hong et al. (2020b  ###reference_b13###) grounds the observation to a certain sub-instruction using human annotations.\nDDL (Cheng et al., 2022  ###reference_b5###) provides token-level supervision by labeling landmark and action words relating to observation at each time step.\nHowever, these approaches only focus on the cross-modal fusion module, neglecting the modality gaps among unimodal representations that are commonly appeared in vision-language tasks Li et al. (2021  ###reference_b19###).\nIn contrast, our work presents a self-supervised framework to align these modalities before fusion stage by contrastive learning to promote cross-modal reasoning.\n###figure_3###"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Cross-modal Contrastive Learning",
            "text": "Cross-modal contrastive learning has proven its effectiveness in modeling vision-language relationships and has gained prominence as a popular strategy in VL tasks. CLIP Radford et al. (2021  ###reference_b31###) firstly employs cross-modal contrastive learning on large-scale image-text pairs. FILIP Yao et al. (2021  ###reference_b44###) utilizes token-wise maximum similarity to achieve the fine-grained alignment between visual and textual tokens. TACo Yang et al. (2021  ###reference_b42###) proposes a token-aware contrastive loss and a cascade sampling method for aligning video and text. X-CLIP Ma et al. (2022  ###reference_b28###) proposes attention method to realize multi-grained contrast between text and video.\nCITL Liang et al. (2022  ###reference_b22###) first introduces the contrastive learning into VLN domain but only considers the unimodal contrast.\nInspired by these works, we propose to apply contrastive learning to bridge the modality gaps. Furthermore, to handle the sparse training signals for pre-fusion alignment, we apply self-supervised learning strategies to realize our dual-level alignment framework."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Preliminaries",
            "text": "Given a similarity vector/matrix  of two instances from distinct modalities, we need to reduce it to scalar value for later contrastive loss calculation. Attention mechanism is introduced in the aggregation process, allowing each element to attend to multiple contrasting elements and automatically balance their influences (Ma et al., 2022  ###reference_b28###). Concretely, we first perform attention operations on row and column to reduce values in each dimension.\nwhere  represents all elements in the dimension,  represents element-wise product,  and .\nLikewise, we second perform attention operations on column and row to get a scale value respectively.\nwhere .\nFinally, we average two scale values to acquire similarity score of two instances and define above process as reduce function .\nUnder a general formulation of cross-modal contrastive learning Radford et al. (2021  ###reference_b31###), we consider  pairs  from two modalities  and  in each training batch.\nWith similarity scores  among instances  and , we can compute both -to- and -to- contrastive losses.\nThe total contrastive loss can be summed from above losses and defined as contrastive loss function  as:\nwhere  is temperature parameter."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Problem Formulation",
            "text": "Given the instruction , the agent receives a panorama observation  of its surrounding environment at each step . The panorama observation consists of  single view images with corresponding relative angles, i.e. , where  is the visual feature of the -th view and  is the relative angle with respect to the current view (subscript  is omitted for simplicity). At each viewpoint, the agent has  navigable candidate viewpoints . All observations  and performed actions  before time step  form the history , where action  denotes the turned angles at step . The navigation is finished when the agent makes STOP decision. The task is considered accomplished when the agent stops near the target location (3m)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Base Agent Model",
            "text": "Current VLN agents Chen et al. (2021  ###reference_b3###, 2022  ###reference_b4###) mainly adopts multi-modal late-fusion learning paradigm Du et al. (2023  ###reference_b8###). To encode each modality, they usually employ standard BERT Devlin et al. (2018  ###reference_b6###) for textual instructions and vision transformer (ViT) Dosovitskiy et al. (2021  ###reference_b7###) for visual observations. As for history modality, they encode historical observations and performed actions during navigation, enabling agents to remember the trajectory. Specially, graph-based methods Chen et al. (2022  ###reference_b4###) maintain a topological map to record the historical experiences.\nAbove three unimodal encoding processes can be formulated as follows:\nwhere . These unimodal embeddings are further fed to a cross-modal transformer to predict the action probability over the candidate viewpoints . The candidate viewpoint with largest probability will be chosen as next move location."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Contrastive Learning Base",
            "text": "Given a similarity vector/matrix  of two instances from distinct modalities, we need to reduce it to scalar value for later contrastive loss calculation. Attention mechanism is introduced in the aggregation process, allowing each element to attend to multiple contrasting elements and automatically balance their influences (Ma et al., 2022  ###reference_b28###  ###reference_b28###). Concretely, we first perform attention operations on row and column to reduce values in each dimension.\nwhere  represents all elements in the dimension,  represents element-wise product,  and .\nLikewise, we second perform attention operations on column and row to get a scale value respectively.\nwhere .\nFinally, we average two scale values to acquire similarity score of two instances and define above process as reduce function .\nUnder a general formulation of cross-modal contrastive learning Radford et al. (2021  ###reference_b31###  ###reference_b31###), we consider  pairs  from two modalities  and  in each training batch.\nWith similarity scores  among instances  and , we can compute both -to- and -to- contrastive losses.\nThe total contrastive loss can be summed from above losses and defined as contrastive loss function  as:\nwhere  is temperature parameter."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Methods",
            "text": "In this section, we elaborate each component of our dual-level alignment (DELAN) framework. We explain the dual-level instruction construction in section 4.1  ###reference_###. Then we introduce the instruction-history level alignment in section 4.2  ###reference_###, and the landmark-observation level alignment in section 4.3  ###reference_###, both by self-supervised contrastive learning. Finally, we present the complete training strategy in section 4.4  ###reference_###. The pipeline of our framework is illustrated as Figure 3  ###reference_### and Figure 4  ###reference_###.\n###figure_4### helps global alignment. Given global instruction representation  and global trajectory representation , the similarity between instruction and trajectory can be computed by matrix multiplication as:\nwhere  represents for the instruction-trajectory similarity score.\nassists in emphasizing the most important words and downplaying words with low navigation information, like prepositions and conjunctions. For given local word representation  and global trajectory representation , we also use matrix multiplication to get the similarity between word and trajectory:\nwhere  represents for the word-trajectory similarity score vector.\nfacilitates generalizing viewpoints which closely follow the instruction. We can compute the similarity between instruction and viewpoint by global instruction representation  and local viewpoint representation (equal to history representation) , which can be formulated as follows:\nwhere  represents for the instruction-viewpoint similarity score vector.\ncontributes to monitoring the navigation progress, as each viewpoint embedding can implicitly align with its corresponding sub-instruction word embeddings. The similarity between word and viewpoint can also be obtained from local word representation  and local viewpoint representation  as:\nwhere  represents for word-viewpoint similarity score matrix.\nWe employ the reduce function  in equation 4  ###reference_### to derive scalar values from similarity score vectors or matrices, based on attention mechanisms. Given  instruction-history pairs  where  represents in-batch -th instruction instance and  represents in-batch -th history instance, the score  measures the semantic similarity between two instances and can be averaged from various similarity scores:\nFinally, the instruction-history level contrastive loss can be calculated from equation 5  ###reference_### and equation 11  ###reference_###:\nhelps align landmark tokens with specific view images of observation at each time step. The similarity matrix can be obtained from local landmark representation  and local representation observation  at time step  as follows:\nwhere  is the landmark-observation similarity score matrix at step .\nWe also employ the reduce function  in equation 4  ###reference_### to dynamically score the importance of each element in similarity matrix and aggregate them.\nAnalogously, given  landmark-observation pairs  where  represents in-batch -th landmark instance and  represents in-batch -th observation instance at time step , the similarity score  measures the semantic similarity between the landmark and observation :\nFinally, the landmark-observation level contrastive loss can be computed from averaged similarity scores along the trajectory according to equation 5  ###reference_### and equation 14  ###reference_###. The formula is:\nNote that  is the trajectory length."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Dual-level Instruction Construction",
            "text": "A navigation instruction (e.g. \"Walk to the right past the TV.\") comprises a series of action, orientation and landmark words.\nSince the navigation history has strong semantic correlation with the entire instruction, while the real-time observations are closely related to the landmark words in instructions, it is significant to explicitly represent such dual-level semantic correspondences.\nTherefore, an additional landmark part is introduced for alignment with observation while the original instruction part is reserved for alignment with history.\nSpecifically, we concatenate original instruction with the extracted landmark words to form a dual-level instruction. We employ a standard pre-trained BERT model Vaswani et al. (2017a  ###reference_b36###) fine-tuned on part-of-speech task to extract all nouns in the instruction as landmark words.\nThe instruction level text guides the overall navigation and the landmark level text helps focus on key details in observation. After this we can drive the instruction part align to history modality and the landmark part align to observation modality respectively.\nFormally, we extract  landmark words  from original instructions. These landmark words are appended to the original instructions, subsequently fed into the text encoder to get textual embeddings, i.e.\nwhere  is the instruction word matrix,  is the landmark word matrix. The representation of navigation history and real-time observations are computed based on equation 1  ###reference_###, resulting in history embeddings matrix  and observation embeddings matrix  at time step . Note that  is the total number of navigation steps."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Instruction-History Alignment",
            "text": "In VLN task, it\u2019s crucial to model historical experiences which helps agent make informed decision based on past information. The history modality exhibits a strong semantic correlation with instruction. The instruction comprehensively describes the navigation trajectory while the history records the experience along this trajectory. The instruction token sequence and history token sequence both adhere to chronological order and are synchronized in time. Current unimodal representation of history is solely derived from past observation and action Chen et al. (2021  ###reference_b3###); Cheng et al. (2022  ###reference_b5###) but lack adequate supervision from textual contents before the fusion stage.\nHence, we propose to align the history modality with the instruction part of dual-level instruction. Since they both represent the trajectory and are synchronized in time, exhibiting an element-wise correspondence, we implement the contrastive loss across global representation and local representation.\nConcretely, we split the instruction-history level alignment into instruction-trajectory alignment, word-trajectory alignment, instruction-viewpoint alignment and word-viewpoint alignment, each providing different benefits.\nAdditionally, we introduce the global trajectory representation  which obtained through mean pooling of all history embeddings during navigation and choose the textual [CLS] embedding  as the global instruction representation.\nhelps global alignment. Given global instruction representation  and global trajectory representation , the similarity between instruction and trajectory can be computed by matrix multiplication as:\nwhere  represents for the instruction-trajectory similarity score.\nassists in emphasizing the most important words and downplaying words with low navigation information, like prepositions and conjunctions. For given local word representation  and global trajectory representation , we also use matrix multiplication to get the similarity between word and trajectory:\nwhere  represents for the word-trajectory similarity score vector.\nfacilitates generalizing viewpoints which closely follow the instruction. We can compute the similarity between instruction and viewpoint by global instruction representation  and local viewpoint representation (equal to history representation) , which can be formulated as follows:\nwhere  represents for the instruction-viewpoint similarity score vector.\ncontributes to monitoring the navigation progress, as each viewpoint embedding can implicitly align with its corresponding sub-instruction word embeddings. The similarity between word and viewpoint can also be obtained from local word representation  and local viewpoint representation  as:\nwhere  represents for word-viewpoint similarity score matrix.\nWe employ the reduce function  in equation 4  ###reference_###  ###reference_### to derive scalar values from similarity score vectors or matrices, based on attention mechanisms. Given  instruction-history pairs  where  represents in-batch -th instruction instance and  represents in-batch -th history instance, the score  measures the semantic similarity between two instances and can be averaged from various similarity scores:\nFinally, the instruction-history level contrastive loss can be calculated from equation 5  ###reference_###  ###reference_### and equation 11  ###reference_###  ###reference_###:"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Landmark-Observation Alignment",
            "text": "The panoramic observation at each time step consists of  view images together with relative directions, providing a detailed spatial layout.\nSuch observation exhibits strong semantic correlation with landmarks in instructions, as the navigation can be typically succeeded by following landmark entities in the environment based on landmark references in instructions. The pre-fusion alignment between landmark and observation enhance the interaction between related representations.\nConsequently, we align the observation modality with the landmark part of dual-level instructions. The observation and landmarks exhibit an element-wise correspondence, as landmarks only present in some of the view images and real-time observation contains only a subset of the landmarks mentioned in instruction. Therefore, we implement the contrastive loss only across local representation here.\nhelps align landmark tokens with specific view images of observation at each time step. The similarity matrix can be obtained from local landmark representation  and local representation observation  at time step  as follows:\nwhere  is the landmark-observation similarity score matrix at step .\nWe also employ the reduce function  in equation 4  ###reference_###  ###reference_### to dynamically score the importance of each element in similarity matrix and aggregate them.\nAnalogously, given  landmark-observation pairs  where  represents in-batch -th landmark instance and  represents in-batch -th observation instance at time step , the similarity score  measures the semantic similarity between the landmark and observation :\nFinally, the landmark-observation level contrastive loss can be computed from averaged similarity scores along the trajectory according to equation 5  ###reference_###  ###reference_### and equation 14  ###reference_###  ###reference_###. The formula is:\nNote that  is the trajectory length."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Training Strategy",
            "text": "Two typical training strategies in VLN task are Imitation Learning (IL) and Reinforcement Learning (RL). In IL phase, the agent learns by following teacher action :\n,\nwhere  is predicted action probability. In RL phase, the agent samples the action to explore the environment and learns from the reward:\n,\nwhere  is the sampled action according to predicted action probability  and  is the advantage in A2C algorithm Mnih et al. (2016  ###reference_b29###).\nOverall, by combining contrastive loss on instruction-history level and landmark-observation level, the total loss is calculated as:\nwhere , ,  and  are weighting parameters that balance different losses."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experiments",
            "text": "We evaluated our method on three representative VLN tasks (four datasets): VLN with fine-grained instructions R2R (Anderson et al., 2018b  ###reference_b2###) and RxR (Ku et al., 2020  ###reference_b17###); long-horizon VLN R4R (Jain et al., 2019  ###reference_b16###); and vision-and-dialogue navigation\nCVDN (Thomason et al., 2020  ###reference_b35###).\nR2R is the basic VLN dataset which contains step-by-step navigation instructions. It includes 90 scans with 22k human-annotated instruction-trajectory pairs. The dataset is divided into train, val seen, val unseen and test unseen splits,consisting of 61, 56, 11 and 18 scans respectively. Scans in the unseen splits are excluded from the training set.\nRxR is a multilingual (English, Hindi and Telugu) VLN dataset. Comparing to R2R, RxR is 10x larger with longer and more various paths.\nR4R extends R2R dataset by concatenating two adjacent trajectories and their instructions. It encourages the agent to navigate as human plan instead of going to the destination directly.\nCVDN requires an agent to navigate based on multi-turn question-answering dialogues. The lengths of instructions and trajectories are much longer than R2R.\nFor R2R, we use standard evaluation metrics following previous work Anderson et al. (2018a  ###reference_b1###, b  ###reference_b2###): Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), Success rate weighted by Path Length (SPL).\nMetrics that measure the path fidelity between the predicted path and target path are introduced for R4R and RxR, including normalized Dynamic Time Warping (nDTW), Success weighted by normalized Dynamic Time Warping (SDTW) (Ilharco et al., 2019  ###reference_b15###) and Coverage weighted by Length Score (CLS) (Jain et al., 2019  ###reference_b16###).\nFor CVDN, we use Goal Progress (GP) in meters (Thomason et al., 2020  ###reference_b35###) as the primary metric. GP measures the difference between completed distance and left distance to the goal, so the higher the better. Note that bold metrics in tables indicate the best results.\nThe experiments are conducted on a NVIDIA 3090 GPU.\nFor contrastive loss, we set  and  in equation  16  ###reference_### while keeping temperature parameter . We also introduce the memory bank Wu et al. (2018  ###reference_b41###) to expand number of negative samples and fix the size to 480.\nOur baseline models consist of HAMT (Chen et al., 2021  ###reference_b3###) and DUET (Chen et al., 2022  ###reference_b4###).\nDifferent from the original implementation (Chen et al., 2022  ###reference_b4###), we utilize CLIP-ViT-B-16 (Li et al., 2022a  ###reference_b18###) as vision encoder for DUET. The batch size is set to 4 for DUET and 8 for HAMT. The optimizer is AdamW (Loshchilov and Hutter, 2017  ###reference_b24###). The remaining training schedules remain consistent with the baselines. Inference is performed using a greedy search approach, following a single-run setting (Wang et al., 2019  ###reference_b39###).\nNote that models marked with \u2217 in table denote our re-implementation.\nTable 1  ###reference_### compares the performances of different agents on R2R benchmark.\nOur method outperforms baselines over all dataset splits, achieving 62.69% SPL (+1.7%) on the test split.\nTable 2  ###reference_### shows results compared with state-of-the-art methods on RxR dataset. Our method gets significant improvements (+1.1% on SPL and +1.0% on SR) on test split compared with the backbones.\nAll above demonstrate the effectiveness and generalization of our framework.\nTable 3  ###reference_### shows navigation results on R4R dataset. DELAN performs consistently better than the baseline HAMT (+0.9% on SR), especially on these path fidelity related metrics (+3.3% on CLS, +3.7% on nDTW and +2.6% on sDTW). This demonstrates its strong instruction following ability in long-horizon navigation scenario. DELAN can model relation between instruction and history to monitor progress, and closely follow the abundant landmarks in long instruction to navigate.\nThe graph-based baseline DUET naturally generates more sub-trajectories for back-and-forth exploration in the predicted path, resulting in reduced path fidelity metrics (CLS, nDTW and sDTW). However, our method can still improve almost all of its navigation metrics (especially +1.1% on SR), which explains the universality of our framework.\nThe results in Table 4  ###reference_### demonstrate that our method enhances the goal progress of previous models, increasing HAMT\u2019s performance by 0.27 meters and DUET\u2019s by 0.23 meters in test environments. This highlights the effectiveness of our proposed framework in a dialog setting.\nWe first evaluate the necessity of employing alignment with dual levels rather than single level, as shown in Table 5  ###reference_###. Line 1 represents our DELAN framework with dual-level alignment and corresponding dual-level instruction. Line 2 depicts the use of single-level alignment, which aligns only the original instruction with concatenated visual-related components (history and observation). The results indicate that explicit alignment at dual levels significantly enhance navigation performance compared to directly aligning vision and language modalities.\nIn the DELAN framework, we adopt a method of separating landmark words from instructions to facilitate explicit modality alignment. Table 6  ###reference_### presents performance under three approaches with increasing degrees of separation between instructions and landmarks. Line 1 illustrates the strategy employed in DELAN, allowing instructions and landmarks attending to each other using the shared text encoder. Line 2 represents a strategy where instructions and landmarks are encoded separately but with the shared text encoder. Line 3 represents a strategy utilizing two independent encoders to encode instructions and landmarks respectively, ensuring complete instruction and landmark separation before the fusion stage. The results presented in Table 6  ###reference_### highlight the superiority of the separation approach in DELAN. Since individual landmark words still hold contextual relevance, allowing them to attend to instruction words makes practical sense.\nWe study the effectiveness of each component of DELAN over the R2R val unseen split. As shown in Table 7  ###reference_###, both instruction-history (Instr-His) and landmark-observation (Land-Ob) alignment contributes to the excellent performance of DELAN, especially on metric NE and SR.\nLine 2 to 5 in Table 7  ###reference_### assess the influence of four components of the instruction-history level contrastive learning, i.e. instruction-trajectory (IT), word-trajectory (WT), instruction-viewpoint (IV), and word-viewpoint (WV). Removing the word-trajectory alignment significantly increases the trajectory length, indicating a decrease in the overall understanding of navigation progress. The elimination of instruction-viewpoint alignment degrade the success rate most, showing the importance of aligning history viewpoints with instructions. Each component has a positive effect on the performance. These results demonstrate that DELAN is not only effective but also reasonable."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Experimental Setup",
            "text": "We evaluated our method on three representative VLN tasks (four datasets): VLN with fine-grained instructions R2R (Anderson et al., 2018b  ###reference_b2###  ###reference_b2###) and RxR (Ku et al., 2020  ###reference_b17###  ###reference_b17###); long-horizon VLN R4R (Jain et al., 2019  ###reference_b16###  ###reference_b16###); and vision-and-dialogue navigation\nCVDN (Thomason et al., 2020  ###reference_b35###  ###reference_b35###).\nR2R is the basic VLN dataset which contains step-by-step navigation instructions. It includes 90 scans with 22k human-annotated instruction-trajectory pairs. The dataset is divided into train, val seen, val unseen and test unseen splits,consisting of 61, 56, 11 and 18 scans respectively. Scans in the unseen splits are excluded from the training set.\nRxR is a multilingual (English, Hindi and Telugu) VLN dataset. Comparing to R2R, RxR is 10x larger with longer and more various paths.\nR4R extends R2R dataset by concatenating two adjacent trajectories and their instructions. It encourages the agent to navigate as human plan instead of going to the destination directly.\nCVDN requires an agent to navigate based on multi-turn question-answering dialogues. The lengths of instructions and trajectories are much longer than R2R.\nFor R2R, we use standard evaluation metrics following previous work Anderson et al. (2018a  ###reference_b1###  ###reference_b1###, b  ###reference_b2###  ###reference_b2###): Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), Success rate weighted by Path Length (SPL).\nMetrics that measure the path fidelity between the predicted path and target path are introduced for R4R and RxR, including normalized Dynamic Time Warping (nDTW), Success weighted by normalized Dynamic Time Warping (SDTW) (Ilharco et al., 2019  ###reference_b15###  ###reference_b15###) and Coverage weighted by Length Score (CLS) (Jain et al., 2019  ###reference_b16###  ###reference_b16###).\nFor CVDN, we use Goal Progress (GP) in meters (Thomason et al., 2020  ###reference_b35###  ###reference_b35###) as the primary metric. GP measures the difference between completed distance and left distance to the goal, so the higher the better. Note that bold metrics in tables indicate the best results.\nThe experiments are conducted on a NVIDIA 3090 GPU.\nFor contrastive loss, we set  and  in equation  16  ###reference_###  ###reference_### while keeping temperature parameter . We also introduce the memory bank Wu et al. (2018  ###reference_b41###  ###reference_b41###) to expand number of negative samples and fix the size to 480.\nOur baseline models consist of HAMT (Chen et al., 2021  ###reference_b3###  ###reference_b3###) and DUET (Chen et al., 2022  ###reference_b4###  ###reference_b4###).\nDifferent from the original implementation (Chen et al., 2022  ###reference_b4###  ###reference_b4###), we utilize CLIP-ViT-B-16 (Li et al., 2022a  ###reference_b18###  ###reference_b18###) as vision encoder for DUET. The batch size is set to 4 for DUET and 8 for HAMT. The optimizer is AdamW (Loshchilov and Hutter, 2017  ###reference_b24###  ###reference_b24###). The remaining training schedules remain consistent with the baselines. Inference is performed using a greedy search approach, following a single-run setting (Wang et al., 2019  ###reference_b39###  ###reference_b39###).\nNote that models marked with \u2217 in table denote our re-implementation."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Main Results",
            "text": "Table 1  ###reference_###  ###reference_### compares the performances of different agents on R2R benchmark.\nOur method outperforms baselines over all dataset splits, achieving 62.69% SPL (+1.7%) on the test split.\nTable 2  ###reference_###  ###reference_### shows results compared with state-of-the-art methods on RxR dataset. Our method gets significant improvements (+1.1% on SPL and +1.0% on SR) on test split compared with the backbones.\nAll above demonstrate the effectiveness and generalization of our framework.\nTable 3  ###reference_###  ###reference_### shows navigation results on R4R dataset. DELAN performs consistently better than the baseline HAMT (+0.9% on SR), especially on these path fidelity related metrics (+3.3% on CLS, +3.7% on nDTW and +2.6% on sDTW). This demonstrates its strong instruction following ability in long-horizon navigation scenario. DELAN can model relation between instruction and history to monitor progress, and closely follow the abundant landmarks in long instruction to navigate.\nThe graph-based baseline DUET naturally generates more sub-trajectories for back-and-forth exploration in the predicted path, resulting in reduced path fidelity metrics (CLS, nDTW and sDTW). However, our method can still improve almost all of its navigation metrics (especially +1.1% on SR), which explains the universality of our framework.\nThe results in Table 4  ###reference_###  ###reference_### demonstrate that our method enhances the goal progress of previous models, increasing HAMT\u2019s performance by 0.27 meters and DUET\u2019s by 0.23 meters in test environments. This highlights the effectiveness of our proposed framework in a dialog setting."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   Further Analysis",
            "text": "We further analyze the agent\u2019s behavior in four specific navigation skills described in Yang et al. (2023  ###reference_b43###). These skills are Stop, Turn, Object and Room, indicating the agent\u2019s ability in stopping, changing direction, object seeking, and room finding, respectively. A higher score means a stronger ability. As shown in Table 8  ###reference_###, our DELAN framework significantly enhances the performance across all skills (average +1.5). We attribute the notable promotion in object seeking (+3.6) to our proposed landmark-observation level alignment, which enforces the agent to recognize landmarks appeared in panoramic observations. Meanwhile, the ability of agents to stop and change directions has also been improved, indicating that under the DELAN framework, the agent has a better judgment of instruction execution and navigation progress."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "5.5.   Qualitative Results",
            "text": "We visualize the trajectories of DUET (Chen et al., 2022  ###reference_b4###) and DUET+DELAN in Figure 5  ###reference_###. Due to the lack of the pre-fusion alignment, DUET has the unsatisfactory instruction-following capacity. For example, it fails to follow the instruction \"turn slight right\" or misses important landmark \"painting placed on the wall\", leading to either incorrect endpoints (upper) or unnecessary exploration trajectories (lower). In contrast, our proposed DELAN framework could promote the instruction comprehension and help the agent make correct decisions, resulting in a more precise and efficient navigation.\n###figure_5###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "This paper highlights the significance of pre-fusion alignment in the VLN task and introduces a dual-level alignment (DELAN) framework, focusing on the instruction-history and landmark-observation levels. For adaptation to the dual-level alignment, we reconstruct a dual-level instruction through concatenating original instruction and landmarks. We employ different self-supervised contrastive learning strategies to realize the dual-level alignment respectively. Experiments across various VLN benchmarks demonstrate the effectiveness, generalization and universality of our approach. We hope this work can inspire further explorations on modality alignment in VLN and related fields."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Acknowledgement",
            "text": "This work is supported by National Natural Science Foundation of China (No.71991471, No.6217020551) and Science and Technology Commission of Shanghai Municipality Grant (No.21DZ1201402)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.01994v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.01994v1",
        "paper_title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning",
        "research_background": "### Motivation\n\nThe motivation behind this paper is driven by the significant challenges and opportunities in the field of Vision-and-Language Navigation (VLN). VLN has garnered substantial interest due to its applicability in various domains like embodied robotics, where agents navigate environments based on natural language instructions and real-time visual data. Despite advancements in this area, a crucial challenge remains in aligning unimodal representations before fusion to better utilize cross-modal modeling and unimodal comprehension abilities. The authors identify this pre-fusion alignment as unexplored territory within VLN, where current models suffer from disparities among correlated modalities prior to their fusion stage.\n\n### Research Problem\n\nThe main research problem addressed in this paper is how to effectively align unimodal representations in VLN tasks before the cross-modal fusion stage. This is challenging due to two primary reasons: \n1. Limited training signals are available for aligning unimodal representations in VLN, which are conventionally used for supervising post-fusion representations.\n2. The sequential decision-making nature of the VLN task adds complexity as it requires integrating navigation history and real-time observations, each having distinct characteristics.\n\nThe aim is to develop a framework that can align these unimodal representations pre-fusion, thereby improving the cross-modal interaction and resulting in better navigation performance.\n\n### Relevant Prior Work\n\n1. **Multi-modal Late-fusion Paradigm in VLN**: Existing VLN agents typically use a multi-modal late-fusion approach where individual modality encoders process inputs separately before combining them through a fusion module (Du et al., 2023).\n\n2. **Pre-fusion Alignment in Vision-Language Tasks**: Aligning unimodal representations prior to fusion has been extensively studied in the broader Vision-Language tasks, demonstrating significant improvements in cross-modal modeling and semantic understanding (Li et al., 2021, 2022b). However, such pre-fusion alignments have not been explored specifically within the VLN context.\n\n3. **Navigation Supervision Signals**: Prior works incorporate training signals for navigation progress and spatial position to supervise post-fusion representations. However, these works do not address pre-fusion alignment issues (Ma et al., 2019a; Chen et al., 2021).\n\n4. **Sequential Nature of VLN Tasks**: The nature of VLN as a sequential decision-making process sets it apart from other traditional Vision-Language tasks, requiring a nuanced approach to handle both navigation history and real-time observations effectively (Hong et al., 2021).\n\nBy positioning their work against this backdrop, the authors highlight the novelty and the need for their proposed Dual-levEL AligNment (DELAN) framework, which leverages cross-modal contrastive learning to address the identified gaps and challenges in pre-fusion alignment within VLN tasks.",
        "methodology": "The proposed method or model, termed **DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning**, is described in the methodology section as follows:\n\n### Overview\nThe methodology aims to effectively align instances from distinct modalities using a dual-level approach with cross-modal contrastive learning. The procedure involves:\n\n1. **Similarity Calculation**: A similarity vector or matrix is computed between two instances from different modalities.\n2. **Reduction to Scalar Value**: The similarity vector/matrix is reduced to a scalar value for later contrastive loss calculation using an attention mechanism.\n\n### Key Components\n1. **Attention Mechanism**:\n    - **Purpose**: To allow each element to attend to multiple contrasting elements and automatically balance their influences.\n    - **Process**:\n        - **First Attention Operation**:\n            - **Row-wise and Column-wise**: Attention operations are performed on rows and columns to reduce values in each dimension.\n            - **Mathematical Representation**: For a dimension \\( d \\), the elements are aggregated using attention weights:\n            \\[ sim'_{d} = generateAttentionRow(att_{d}) \\otimes sim_{d}, \\quad \\text{and} \\quad att_{d} = softmax(sim_{d}) \\]\n        - **Second Attention Operation**:\n            - **Column-wise and Row-wise**: Attention operations are applied again to obtain a scalar value from the reduced dimensional data:\n            \\[ sim'_{d} = generateAttentionCol(att_{d}) \\otimes sim_{d} \\]\n            - **Result**: Two scalar values are thus obtained.\n    - **Averaging**: The two scalar values are averaged to acquire a final similarity score between the instances.\n\n2. **Cross-Modal Contrastive Learning**:\n    - **General Formulation**: Based on Radford et al. (2021), it involves considering \\( P \\) pairs \\( (x_M, x_T) \\) from modalities \\( M \\) and \\( T \\) within each training batch.\n    - **Similarity Scores**: With similarity scores \\( s_{ij} \\) among instances \\( i \\) and \\( j \\), both \\( M \\)-to-\\( T \\) and \\( T \\)-to-\\( M \\) contrastive losses are calculated.\n    - **Contrastive Loss Function**:\n        - **Definition**: The total contrastive loss is a summation of the individual modality contrastive losses and is represented as:\n        \\[ \\mathcal{L} = \\mathcal{L}_{M \\rightarrow T} + \\mathcal{L}_{T \\rightarrow M} \\]\n        - **Temperature Parameter**: The loss calculation includes a temperature parameter \\( \\tau \\).\n\n### Innovations\n- **Dual-Level Attention**: By introducing a two-tier attention mechanism applied to both rows and columns, the methodology ensures robust feature aggregation from both dimensions.\n- **Cross-Modal Aggregation**: The method improves the alignment between different modalities by leveraging a cross-modal contrastive loss that enforces learning representations that are closer for matched pairs and further apart for non-matched pairs.\n- **Similarity Score Calculation**: The averaging of two scalar values obtained via attention operations fosters a balanced similarity score that is less sensitive to individual peaks in the similarity vectors or matrices.\n\nThe combination of advanced attention-based reduction techniques and cross-modal contrastive learning methodologies enables DELAN to effectively align visual and language data, contributing to improved performance in vision-and-language navigation tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### VLN Tasks and Datasets:\nWe evaluated our method on three representative Vision-and-Language Navigation (VLN) tasks across four datasets:\n1. **R2R (Room-to-Room)**: A basic VLN dataset with step-by-step navigation instructions. It contains 90 scans with 22k human-annotated instruction-trajectory pairs and is divided into train, val seen, val unseen, and test unseen splits with 61, 56, 11, and 18 scans respectively.\n2. **RxR (Room-across-Room)**: A multilingual VLN dataset in English, Hindi, and Telugu, 10x larger than R2R with longer and more varied paths.\n3. **R4R (Room-for-Room)**: Extends R2R by concatenating two adjacent trajectories and their instructions to simulate a more human-like navigation plan.\n4. **CVDN (Continuous Vision-and-Dialog Navigation)**: Involves navigating based on multi-turn question-answering dialogues, with longer instructions and trajectories compared to R2R.\n\n#### Evaluation Metrics:\n- **R2R Metrics**: Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), Success Rate weighted by Path Length (SPL).\n- **R4R and RxR Metrics**: normalized Dynamic Time Warping (nDTW), Success weighted by normalized Dynamic Time Warping (SDTW), Coverage weighted by Length Score (CLS).\n- **CVDN Metric**: Goal Progress (GP) in meters, measuring the distance completed towards the goal.\n\n#### Baseline Models:\n- **HAMT (Chen et al., 2021)**\n- **DUET (Chen et al., 2022)**: For this method, we used CLIP-ViT-B-16 as the vision encoder. The training schedule and inference were performed consistently with the baseline methods, using AdamW as the optimizer, with batch sizes of 4 for DUET and 8 for HAMT.\n\n#### Results:\n- **R2R**: DELAN outperformed all baselines, achieving a 62.69% SPL (+1.7%) on the test split.\n- **RxR**: DELAN showed significant improvements on the test split (+1.1% SPL and +1.0% SR) compared to the base models.\n- **R4R**: DELAN surpassed HAMT with a 0.9% increase in SR, and higher path fidelity metrics: +3.3% on CLS, +3.7% on nDTW, and +2.6% on sDTW. This indicates strong instruction-following ability in long-horizon navigation scenarios. It outperformed DUET by generating fewer sub-trajectories, showing improvements in almost all navigation metrics, particularly SR (+1.1%).\n- **CVDN**: DELAN improved HAMT\u2019s goal progress by 0.27 meters and DUET\u2019s by 0.23 meters, showcasing the framework's effectiveness in a dialog setting.\n\nOverall, the results demonstrate the effectiveness and generalization capabilities of the DELAN framework across various VLN tasks and datasets."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the necessity and effectiveness of employing dual-level alignment rather than single-level alignment in the DELAN framework for enhanced navigation performance.",
            "experiment_process": "The study compared two scenarios of alignment: Line 1 used dual-level alignment with corresponding dual-level instruction, and Line 2 used single-level alignment, which aligns only the original instruction with concatenated visual-related components (history and observation). Additionally, three approaches with varying degrees of separation between instructions and landmarks were evaluated: Line 1 used a strategy allowing instructions and landmarks to attend to each other using a shared text encoder, Line 2 encoded instructions and landmarks separately with a shared text encoder, and Line 3 used two independent encoders to completely separate instructions and landmarks before the fusion stage. Performance was assessed on multiple VLN benchmarks including R2R.",
            "result_discussion": "The results indicated that explicit alignment at dual levels significantly enhanced navigation performance. The strategy used in DELAN, where landmark words still hold contextual relevance and attend to instruction words, proved superior. This demonstrates that individual components of dual-level alignment substantially contribute to improved navigation performance.",
            "ablation_id": "2404.01994v1.No1"
        },
        {
            "research_objective": "To study the effectiveness of each component of DELAN over the R2R validation unseen split, particularly focusing on how different elements of the instruction-history level contrastive learning affect performance.",
            "experiment_process": "The influence of four components of the instruction-history level contrastive learning was assessed: instruction-trajectory (IT), word-trajectory (WT), instruction-viewpoint (IV), and word-viewpoint (WV). Performance was analyzed by removing each component one at a time and observing its effect on key metrics such as NE (Navigation Error) and SR (Success Rate).",
            "result_discussion": "Eliminating word-trajectory alignment significantly increased the trajectory length, indicating a decrease in the overall understanding of navigation progress. Removing instruction-viewpoint alignment most severely degraded the success rate. This underscores the importance of aligning history viewpoints with instructions. Each component positively affected performance, demonstrating that the comprehensive inclusion of these elements is integral to the DELAN framework's success.",
            "ablation_id": "2404.01994v1.No2"
        }
    ]
}