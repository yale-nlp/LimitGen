{
    "title": "\\faRulerRuler: What\u2019s the Real Context Size of Your Long-Context Language Models?",
    "abstract": "The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the \u201cneedle\u201d) from long distractor texts (the \u201chaystack\u201d), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark Ruler with flexible configurations for customized sequence length and task complexity. Ruler expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, Ruler introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate ten long-context LMs with 13 representative tasks in Ruler. Despite achieving nearly perfect accuracy in the vanilla NIAH test, all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only four models (GPT-4, Command-R, Yi-34B, and Mixtral) can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement  as we increase input length and task complexity. We open source Ruler to spur comprehensive evaluation of long-context LMs. \u2020\u2020* Authors contributed equally.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent advancements in AI system engineering (Dao et al., 2022  ###reference_b14###; Jacobs et al., 2023  ###reference_b28###; Fu et al., 2024  ###reference_b21###) and language model designs (Chen et al., 2023  ###reference_b9###; Xiong et al., 2023  ###reference_b75###) have enabled efficient scaling up of context length for language models (Liu et al., 2024a  ###reference_b39###; Young et al., 2024  ###reference_b78###). Previous works (AI21, 2024  ###reference_b1###; X.AI, 2024  ###reference_b72###; Reid et al., 2024  ###reference_b53###; Anthropic, 2024  ###reference_b4###) commonly adopt synthetic tasks, such as passkey retrieval (Mohtashami & Jaggi, 2023  ###reference_b44###) and needle-in-a-haystack (Kamradt, 2023  ###reference_b32###) to evaluate long-context LMs. However, these evaluations are used inconsistently across works and reveal merely the retrieval capability, failing to gauge other forms of long-context understanding.\nIn this work, we propose Ruler, a new benchmark to evaluate long-context modeling capabilities for language models.\nRuler contains four task categories to test behaviors (Ribeiro et al., 2020  ###reference_b54###) beyond simple retrieval from context:\nRetrieval: we extend the needle-in-a-haystack (Kamradt, 2023  ###reference_b32###, NIAH) test to evaluate retrieval capability with diverse types and quantities of needles.\nMulti-hop Tracing: we propose variable tracking, a minimal proxy task for coreference chain resolution to check the behavior of tracing entities with multi-hop connections.\nAggregation: we propose common/frequent words extraction, proxy tasks for summarization to test the ability to aggregate relevant information that spans long-range context.\nQuestion Answering: we add distracting information to the input of existing short-context QA datasets to evaluate question answering capability at various context sizes.\nCompared to existing realistic benchmarks (Table 1  ###reference_###), Ruler consists solely of synthetic tasks, which offer the flexibility to control sequence length and task complexity.\nThe synthetic input in Ruler reduces reliance on parametric knowledge, which interferes with the utilization of long-context input in realistic tasks (Shaham et al., 2023  ###reference_b55###; Bai et al., 2023  ###reference_b6###).\nUsing Ruler, we benchmark GPT-4  (OpenAI: Josh Achiam et al., 2023  ###reference_b47###) and nine open-source models with context length ranging from 4k to 128k. Despite achieving nearly perfect performance on the vanilla NIAH test, all models exhibit large degradation on more complex tasks in Ruler as sequence length increases. While all models claim context size of 32k tokens or greater, our results indicate that only four of them can effectively handle sequence length of 32K by exceeding a qualitative threshold. Moreover, almost all models fall below the threshold before reaching the claimed context lengths.\nTo obtain fine-grained model comparisons, we aggregate performance from 4k to 128k with two weighted average scores where the weights simulate the length distribution of real-world use cases. The top models - GPT-4, Command-R (Cohere, 2024  ###reference_b12###), Yi-34B (Young et al., 2024  ###reference_b78###), and Mixtral (Jiang et al., 2024  ###reference_b30###), consistently outperform other models regardless of the chosen weighting scheme.\nWe further analyze Yi-34B, which claims context length of 200K and achieves the 2nd place on Ruler among open-source models. Our results demonstrate large degradation in Yi\u2019s performance as we increase input length and task complexity. At large context sizes, Yi-34B often returns incomplete answers and fails to precisely locate the relevant information. Furthermore, we observe two behaviors emerging with the scaling of context size across multiple models: the increased reliance on parametric knowledge and the increased tendency to copy from context for non-retrieval tasks. Our additional ablations demonstrate that training on longer sequences does not always lead to better performance on Ruler, and that larger model sizes positively correlate with better long-context capabilities. Finally, we show that non-Transformer architectures, such as RWKV and Mamba, still lag behind Transformer by large margins on Ruler.\nOur contributions are as follows:\nWe propose a new benchmark Ruler for evaluating long-context language models via synthetic tasks with flexible configurations.\nWe introduce new task categories, specifically multi-hop tracing and aggregation, to test behaviors other than retrieval from long context.\nWe evaluate ten long-context LMs using Ruler and perform analysis across models and task complexities.\nWe open source Ruler to spur future research in long-context language models.111https://github.com/hsiehjackson/RULER  ###reference_###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Numerous long-context language models have been introduced lately owing to the progress in engineering, architectural, and algorithmic designs. Flash attention (Dao et al., 2022  ###reference_b14###; Dao, 2023  ###reference_b13###) and Ring attention (Liu et al., 2023  ###reference_b38###) significantly reduce the memory footprint required for processing long context. Various sparse attention mechanisms (Child et al., 2019  ###reference_b11###; Jaszczur et al., 2021  ###reference_b29###) such as shifted sparse attention (Chen et al., 2024  ###reference_b10###), dilated attention (Ding et al., 2023  ###reference_b15###), and attention sinks (Han et al., 2023  ###reference_b25###; Xiao et al., 2024b  ###reference_b74###) were employed to enable efficient context scaling. Novel position embedding methods were proposed to improve length extrapolation in Transformers (Vaswani et al., 2017  ###reference_b68###), including ALiBi (Press et al., 2022  ###reference_b51###), xPOS (Sun et al., 2023b  ###reference_b59###), and RoPE (Su et al., 2023  ###reference_b56###) variants (Chen et al., 2023  ###reference_b9###; Xiong et al., 2023  ###reference_b75###; Peng et al., 2024  ###reference_b49###; Liu et al., 2024b  ###reference_b40###; Ding et al., 2024  ###reference_b16###; Zhu et al., 2024  ###reference_b81###). Another line of research focuses on reducing context size. This can be achieved by caching previous context using recurrence mechanism (Zhang et al., 2024a  ###reference_b79###; Bulatov et al., 2023  ###reference_b7###; Martins et al., 2022  ###reference_b42###; Wu et al., 2022  ###reference_b71###), or preserving only the salient information within long context via retrieval (Xu et al., 2024  ###reference_b76###; Mohtashami & Jaggi, 2023  ###reference_b44###; Wang et al., 2024  ###reference_b69###; Tworkowski et al., 2024  ###reference_b66###; Xiao et al., 2024a  ###reference_b73###) or compression (Jiang et al., 2023  ###reference_b31###). Finally, novel architectures (Gu et al., 2022  ###reference_b24###; Fu et al., 2023a  ###reference_b19###; Poli et al., 2023  ###reference_b50###; Fu et al., 2023b  ###reference_b20###; Sun et al., 2023a  ###reference_b58###) such as Mamba (Gu & Dao, 2023  ###reference_b23###) and RWKV (Peng et al., 2023  ###reference_b48###) have also been proposed to efficiently handle long-context input.\nOur work is closely related to other works on benchmarking long-context language models. ZeroSCROLLS (Shaham et al., 2023  ###reference_b55###) covers ten realistic natural language tasks, such as long-document QA and (query-based) summarization. L-Eval (An et al., 2024  ###reference_b2###) also uses realistic data, which was filtered manually to ensure quality. LongBench (Bai et al., 2023  ###reference_b6###) contains tasks in a bilingual setting. InfiniteBench (Zhang et al., 2024b  ###reference_b80###) includes tasks with length greater than 100K tokens. LTM (Castillo et al., 2024  ###reference_b8###) targets the evaluation of long-term conversations. To isolate the effect of parametric knowledge, previous works (Dong et al., 2023  ###reference_b17###; Li et al., 2023b  ###reference_b37###) also propose to use documents posted online later than a certain cutoff date, or leverage extremely low-resource materials (Tanzer et al., 2024  ###reference_b60###).\nCompared to realistic benchmarks, synthetic tasks are more flexible to control the setup (e.g., sequence length and task complexity) and less affected by parametric knowledge. Recent works have mostly focused on retrieval-based synthetic tasks(Kamradt, 2023  ###reference_b32###; Mohtashami & Jaggi, 2023  ###reference_b44###; Li et al., 2023a  ###reference_b36###; Liu et al., 2024c  ###reference_b41###), with a few on other types of long-context usage, including various types of reasoning (Tay et al., 2021  ###reference_b61###) and long-range discourse modeling (Sun et al., 2022  ###reference_b57###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The Ruler Benchmark",
            "text": "Ruler comprises tasks across four categories: retrieval, multi-hop tracing, aggregation, and question answering with\nall tasks configurable for varying length and complexity (see Table 2  ###reference_###).\nOne of the special magic numbers for long-context is: 12345. \u2026\u2026\nWhat is the special magic number for long-context mentioned in the provided text?\nAnswer: 12345\nOne of the special magic numbers for long-context is: 12345.\nOne of the special magic numbers for large-model is: 54321.\nWhat is the special magic number for long-context mentioned in the provided text?\nAnswer: 12345\nOne of the special magic numbers for long-context is: 12345.\nOne of the special magic numbers for long-context is: 54321.\nWhat are all the special magic numbers for long-context mentioned in the provided text?\nAnswer: 12345 54321\nOne of the special magic numbers for long-context is: 12345.\nOne of the special magic numbers for large-model is: 54321.\nWhat are all the special magic numbers for long-context and large-model mentioned in the provided text?\nAnswer: 12345 54321\nVAR X1 = 12345 \u2026\u2026 VAR Y1 = 54321 \u2026\u2026\nVAR X2 = X1 \u2026\u2026 VAR Y2 = Y1 \u2026\u2026\nVAR X3 = X2 \u2026\u2026 VAR Y3 = Y2 \u2026\u2026\nFind all variables that are assigned the value 12345.\nAnswer: X1 X2 X3\naaa bbb ccc aaa ddd eee ccc fff ggg\nhhh iii iii \u2026\u2026\nWhat are the 10 most common words in the above list?\nAnswer: aaa ccc iii \u2026\u2026\naaa bbb ccc aaa ddd eee ccc fff ggg aaa hhh aaa ccc iii iii \u2026\u2026\nWhat are the 3 most frequently appeared words in the above coded text?\nAnswer: aaa ccc iii\nDocument 2: \u2026\u2026 bbb \u2026\u2026\nQuestion: question\nAnswer: bbb"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Retrieval: Needle-in-a-haystack (NIAH)",
            "text": "Recent works (Reid et al., 2024  ###reference_b53###; Anthropic, 2023  ###reference_b3###) commonly employ the needle-in-a-haystack (Kamradt, 2023  ###reference_b32###, NIAH) test to evaluate long-context modeling capability. The NIAH test is reminiscent of the extensively studied (Hopfield, 1982  ###reference_b26###; Graves et al., 2014  ###reference_b22###; Olsson et al., 2022  ###reference_b46###; Arora et al., 2024  ###reference_b5###) associative recall tasks, in which relevant information needs to be retrieved from context given a sufficient query. In Ruler, we include multiple retrieval-based tasks, extending the vanilla NIAH test to evaluate models based on three criteria. Concretely, the retrieval capability should be (1) agnostic to the type of the \u201cneedle\u201d and the \u201chaystack\u201d, (2) strong enough to disregard hard distractors, and (3) of high recall when multiple items need to be retrieved. Based on these criteria, we develop four NIAH tasks. The \u201cneedle\u201d in each of these tasks is a key-value pair inserted into the \u201chaystack\u201d (long distractor texts). The query is located at the end of the sequence and serves as a cue for matching the keys in the context and subsequently retrieving the associated values.\nSingle NIAH (S-NIAH):  This is the vanilla NIAH test where a single \u201cneedle\u201d222Similar to Liu et al. (2024a  ###reference_b39###), we use \u201cthe special magic number for XXX is: YYY\u201d as the needle due to its extendability instead of the sentence about San Francisco proposed by Kamradt (2023  ###reference_b32###). needs to be retrieved from the \u201chaystack\u201d.\nThe query/key/value can take the form of words, numbers (7 digits), or UUIDs (32 digits). The \u201chaystack\u201d can be repeated noise sentences333 Following Mohtashami & Jaggi (2023  ###reference_b44###), we use \u201cThe grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.\u201d as noise sentences. or Paul Graham essays (Kamradt, 2023  ###reference_b32###).\nMulti-keys NIAH (MK-NIAH):  Multiple \u201cneedles\u201d are inserted into the \u201chaystack\u201d, and only one of them needs to be retrieved. The additional \u201cneedles\u201d are hard distractors. The most challenging setting is a version where the \u201chaystack\u201d is filled with distractor needles.\nMulti-values NIAH (MV-NIAH):  Multiple \u201cneedles\u201d sharing the same key are inserted into the \u201chaystack\u201d. All values associated with the same key need to be retrieved.\nMulti-queries NIAH (MQ-NIAH):  Multiple \u201cneedles\u201d are inserted into the \u201chaystack\u201d. All \u201cneedles\u201d with distinct keys need to be retrieved. This is the same multi-query associative recall task setup used by Arora et al. (2024  ###reference_b5###). Together with MV-NIAH, these two tasks evaluate the retrieval capability without missing any critical information."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Multi-hop Tracing: Variable Tracking (VT)",
            "text": "Effective discourse comprehension (van Dijk & Kintsch, 1983  ###reference_b67###) is contingent upon successful recognition of newly mentioned entities and establishing the chain of references co-referring to the same entity (Karttunen, 1969  ###reference_b33###) throughout the long context. We develop a new task variable tracking to emulate a minimal coreference chain resolution (Ng, 2010  ###reference_b45###) task. This task checks the behavior of tracking relevant co-occurrence patterns and drawing skipped connections within long input. Specifically, a variable  is initialized with a value , followed by a linear chain of variable name binding statements (e.g., ), which are inserted at various positions of the input. The objective is to return all variable names pointing to the same value . The task complexity can be increased by adding more hops (i.e., the times of name binding) or more chains, similar to adding hard distractors in MK-NIAH."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Aggregation: Common Words (CWE) and Frequent Words Extraction (FWE)",
            "text": "In Ruler, we introduce a new category as a proxy for summarization tasks where relevant information constitutes much larger portion of the context, and the target output depends on accurate aggregation of the relevant input.\nConcretely, we construct an input sequence by sampling words from a pre-defined (synthetic) word list. In the common word extraction task (CWE), words are sampled from discrete uniform distributions, with the number of common words fixed while the number of uncommon words increases with the sequence length. In the frequent words extraction task (FWE), words are sampled from Zeta distribution.444We draw inspiration from Zipf\u2019s Law (Kingsley Zipf, 1932  ###reference_b34###). Let  be the total number of words, which is determined by the context size, the frequency of the -th ranked word (the -th most frequently appeared word) is , where  is the Zeta function. We set the top-ranked word to noise. Figure 1  ###reference_### shows an illustration of word frequency in the constructed input. A model needs to return the top- frequent words in the context. In CWE,  equals to the number of common words. In FWE, we set  to 3, as increasing  leads to poor performance even at small context sizes for most models. The task complexity can be adjusted by varying the number of common words or the parameter of Zeta distribution.\n###figure_1### ###figure_2###"
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Question Answering (QA)",
            "text": "The majority of existing QA datasets (Rajpurkar et al., 2018  ###reference_b52###; Yang et al., 2018  ###reference_b77###; Trivedi et al., 2022  ###reference_b65###) are designed to answer questions based on short passages. These datasets can be extended to simulate long-context input by adding distracting information. In this task category, we insert the golden paragraphs (i.e., the paragraphs that contain answers) into paragraphs randomly sampled from the same dataset. This category is a real-world adaptation (Ivgi et al., 2023  ###reference_b27###) of NIAH, where the question serves as the query, the golden paragraphs are the \u201cneedles\u201d, and the distracting paragraphs form the \u201chaystack\u201d."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments & Results",
            "text": "We select 10 long-context LLMs, including 9 open-source models and one closed-source model (GPT-4), covering diverse model sizes (6B to 8x7B with MoE architecture) and claimed context lengths (32k to 1M). Complete information about these models is included in Appendix A  ###reference_###. We evaluate all models using vLLM (Kwon et al., 2023  ###reference_b35###), an LLM serving system with efficient KV cache memory management. For all models, we run the inference in BFloat16 on 8 NVIDIA A100 GPUs with greedy decoding.\nWe test all models on 13 tasks ranging diverse complexities from the four categories of Ruler.\nThe test configurations have been selected (shown in Appendix B  ###reference_###) based on a task correlational study described in Appendix C  ###reference_###.\nFor each task, we evaluate each model with 500 examples generated for each length from the series (4k, 8k, 16k, 32k, 64k, 128k), while complying with each model\u2019s necessary chat template.555See Appendix D  ###reference_### for model and tasks templates details.\nTo prevent the model from refusing to answer a query or generating explanations, we append the task input with an answer prefix and check the presence of the target output with recall-based accuracy.\nWe notice large performance degradation in all models as we increase input length in Ruler. To determine the maximum context size a model can effectively handle, we grade each model with a fixed threshold, passing which indicates satisfactory performance at the length of evaluation. We use the performance of Llama2-7b model at the 4K context length as the threshold.\nWe report in Table 3  ###reference_### the maximum length exceeding the threshold as the \u201ceffective length\u201d along with the \u201cclaimed length\u201d.\nWhile the threshold-based grading reveals the discrepancy between claimed and effective length, it lacks details for fine-grained model comparisons. As such, we use a weighted average score to aggregate model performance across various context sizes. We rank models under two weighting schemes: wAvg. (inc) and wAvg. (dec) where the weight linearly increases and decreases with sequence length respectively. Ideally, the weight for each length should be determined by the length distribution of model usage, here we choose the two schemes to simulate the scenarios where longer sequences (inc) or shorter sequences (dec) dominate the distribution.\nWe include the results of ten long-context LMs in comparison with the Llama2-7B baseline in Table 3  ###reference_###.666Performance of base models and breakdown by task categories can be found in Appendix F  ###reference_###.\nThe performance at a certain length is the average of all 13 tasks in Ruler. While these models all claim effective context of 32K tokens or greater, none of them maintains performance above the Llama2-7B baseline at their claimed length, except for Mixtral, which achieves moderate performance on length doubling the claimed 32K context size. Despite achieving nearly perfect performance on the\npasskey retrieval and the vanilla NIAH task (shown in Appendix E  ###reference_###), all models exhibit large degradation in RULER as sequence length increases. The best performant model on Ruler is GPT-4, which has the highest performance at length of 4k and demonstrates the least but non-marginal degradation (15.4) when extending the context to 128K. The top three ranked open-source models, Command-R, Yi-34B and Mixtral, all use a large base frequency in RoPE and are larger in parameter size than other models.\nDespite having been trained with context size of 1M, the LWM performs worse than Llama2-7B even at 4K. However, it shows smaller degradation with the increase of context size, therefore achieves higher rank than Mistral-7B when longer sequences receive larger weight (wAvg. inc). This result suggests a trade-off in evaluation between absolute performance on short sequences and the relative degradation with the scaling of context size."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Model Analysis",
            "text": "###figure_11### ###figure_12### ###figure_13### ###figure_14### Do models trained with larger context sizes perform better on Ruler? We evaluate the suite of LargeWorldModels (Liu et al., 2024a  ###reference_b39###, LWM) of equal parameter size and trained up to various context lengths. Figure 4  ###reference_### (left & middle-left) shows that larger context sizes overall lead to better performance, but the ranking can be inconsistent for long sequences. For instance, the model trained with 1M context size (LWM-1M) is worse than the one with 512K at length of 256K, likely due to insufficient training for adjusting to the new base frequency in RoPE. Moreover, we observe abrupt performance drops when models need to extrapolate to unseen lengths (e.g., LMW-128K given input of 256K), and almost linear degradation with input length on log scale within the max training context size.\nThe top models in our main results are much larger than other models. To ablate the effect of model size, we evaluate Yi-34B-200k, Yi-9B-200k, and Yi-6B-200k, all trained up to the same context length using the same data blend. Figure 4  ###reference_### (middle-right) shows that the 34B model is significantly better than the 6B model on Ruler for both performance at length of 4K and the relative degradation, suggesting the benefit of scaling model sizes for better long-context modeling.\nWe evaluate the effective context length for two models with non-Transformer architectures: RWKV-v5 (Peng et al., 2023  ###reference_b48###) and Mamba-2.8B-slimpj (Gu & Dao, 2023  ###reference_b23###). We find that both models demonstrate significant degradation when extending context size to 8K, and both underperform the Transformer baseline Llama2-7B by large margins up till the length of 4K, beyond which Llama2 shows poor length extrapolation performance (Figure 4  ###reference_### right)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present Ruler, a synthetic benchmark for evaluating long-context language models.\nRuler contains diverse task categories, retrieval, multi-hop tracing, aggregation and question answering, providing a flexible and comprehensive evaluation of LLM\u2019s long-context capabilities.\nWe benchmark ten long-context LMs using Ruler with context sizes ranging from 4K to 128K. Despite achieving perfect results in the widely used needle-in-a-haystack test, all models fail to maintain their performance in other tasks of Ruler as we increase input length. We observe common failure modes at large context sizes, including the failure to ignore distractors and ineffective utilization of long context (e.g., simply copy from context or use parametric knowledge instead). We show that Ruler is challenging for even the top-ranked open-source models as we increase task complexity. Our analysis further reveals the large potential for improvement on Ruler and the benefit of scaling model sizes in achieving better long context capabilities."
        }
    ],
    "url": "http://arxiv.org/html/2404.06654v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4",
            "5",
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2404.06654v2",
        "paper_title": "\\faRulerRuler: What\u2019s the Real Context Size of Your Long-Context Language Models?",
        "research_background": "**Paper's Motivation:**\nThe motivation behind this paper stems from the recent advancements in AI system engineering and language model designs, which have enabled the scaling up of context lengths in language models. Despite these advancements, current evaluations using synthetic tasks like passkey retrieval and needle-in-a-haystack are inconsistent and primarily measure retrieval capabilities, failing to provide a comprehensive assessment of long-context understanding. This paper aims to address this gap by proposing a new benchmark that evaluates broader capabilities of language models beyond simple retrieval.\n\n**Research Problem:**\nThe research problem this paper tackles is the inadequate and inconsistent evaluation of long-context language models. Existing evaluations focus mainly on retrieval tasks, neglecting other crucial aspects of long-context understanding such as multi-hop reasoning, aggregation, and context-dependent question answering. This limitation in existing benchmarks necessitates the development of a more comprehensive and controlled evaluation method to better gauge the capabilities of long-context language models.\n\n**Relevant Prior Work:**\n1. **Technological Advancements:**\n   - **System and Model Designs:** Dao et al. (2022), Jacobs et al. (2023), Fu et al. (2024), Chen et al. (2023), Xiong et al. (2023) have contributed to the techniques enabling the scaling of context lengths in language models.\n   - **Efficient Scaling:** Liu et al. (2024a) and Young et al. (2024a) have worked on scaling context lengths efficiently.\n\n2. **Current Evaluative Benchmarks:**\n   - **Synthetic Tasks:** AI21 (2024), X.AI (2024), Reid et al. (2024), Anthropic (2024) have utilized tasks such as passkey retrieval (Mohtashami & Jaggi, 2023) and needle-in-a-haystack (Kamradt, 2023) to evaluate these models.\n   - **Issues in Evaluation:** These methods are criticized for their inconsistency and limited scope of measuring only retrieval capabilities.\n\n3. **Foundational Insights:**\n   - **Evaluation Metrics in AI:** Ribeiro et al. (2020) have discussed behaviors that need to be tested beyond simple retrieval.\n   - **Synthetic Inputs:** The use of synthetic inputs as discussed by Shaham et al. (2023) and Bai et al. (2023) allows controlled evaluation without interference from parametric knowledge.",
        "methodology": "### Methodology: \n\n**Ruler** is an evaluative framework designed to analyze the effective context size of long-context language models. It comprises tasks across four main categories:\n\n1. **Retrieval**: Assessing the model\u2019s ability to locate and extract specific pieces of information from a body of text.\n2. **Multi-hop Tracing**: Evaluating the model's capability to follow a sequence of logical steps or connections across the text.\n3. **Aggregation**: Testing the model's proficiency in synthesizing information from various parts of the text to form a coherent response.\n4. **Question Answering**: Determining the model\u2019s effectiveness in answering complex questions based on the information spread throughout the text. \n\nEach task within these categories is designed to be configurable with varying lengths and complexity, enabling a thorough assessment of the model's capabilities under different scenarios. For detailed configurations, refer to Table 2 in the original document.\n\nAdditionally, a significant aspect of evaluating long-context language models in Ruler is the introduction of \"special magic numbers\". These numbers serve as unique identifiers or markers which are crucial in tracking the model\u2019s ability to remember and utilize information over extended contexts. The special magic number for long-context mentioned in the provided text is **12345**. Another number associated with models, particularly large-models, is **54321**.\n\n### Key Components and Innovations:\n\n- **Configurable Tasks**: The ability to tailor each task's length and complexity provides a nuanced way to measure a model's context use.\n- **Special Magic Numbers**: Using specific magic numbers (e.g., 12345 and 54321) to test and trace the model\u2019s long-term memory capabilities.\n- **Assessment across Diverse Categories**: Including varied task types such as retrieval, multi-hop tracing, aggregation, and question answering ensures a comprehensive evaluation.\n- **Real-Life Context Evaluation**: The framework simulates real-world information usage and synthesis, providing practical insights into the model's efficiency and accuracy over long contexts.\n\nThese components and innovations position Ruler as a sophisticated tool for dissecting and understanding the real context size capabilities of long-context language models.",
        "main_experiment_and_results": "In the main experiment, the setup and results are detailed as follows:\n\n### Main Experiment Setup\n1. **Models Evaluated**: The study involves 10 long-context Language Learning Models (LLMs), including 9 open-source models and one closed-source model (GPT-4). The models span a range of sizes from 6 billion to 8x7 billion parameters with mixtures of experts (MoE) architectures, and their claimed context lengths range from 32,000 to 1,000,000 tokens. Detailed information about these models can be found in Appendix A.\n\n2. **Inference System**: Evaluation is conducted using vLLM, a system noted for efficient KV cache memory management. Inference for all models is performed in BFloat16 on 8 NVIDIA A100 GPUs with greedy decoding.\n\n3. **Tasks and Context Lengths**: The models are tested on 13 tasks representing diverse complexities across four categories of the Ruler benchmark. Evaluation for each task uses 500 examples for each context length in the series (4k, 8k, 16k, 32k, 64k, 128k), as specified in Appendix B. The model templates and task configurations are provided in Appendix D.\n\n4. **Evaluation Metrics**: Performance is measured by recall-based accuracy with a focus on identifying the presence of the target output. The models\u2019 abilities to handle various input lengths are graded against a fixed threshold, defined by the performance of the Llama2-7b model at a 4K context length.\n\n5. **Performance Aggregation**: Besides threshold-based grading, model performances are aggregated using weighted average scores (wAvg) under two schemes: wAvg. (inc) where weights increase with sequence length and wAvg. (dec) where weights decrease with sequence length. \n\n### Main Experimental Results\n1. **Threshold-Based Grading**: The results show a substantial discrepancy between claimed and effective context lengths. Models claiming effective context lengths of 32,000 tokens or more failed to maintain performance above the Llama2-7B baseline at their claimed lengths. Mixtral is the exception, demonstrating moderate performance at double its claimed 32,000 context size.\n\n2. **Weighted Average Scores**: No model maintains performance above the Llama2-7B baseline at their claimed lengths. However, weighted average ranking elucidates differences among the models depending on sequence length emphasis:\n   - **GPT-4**: Emerges as the best-performing model in the Ruler benchmark, showing the highest performance at the 4K length and the smallest degradation (15.4%) at a 128K context length.\n   - **Top Open-Source Models**: Command-R, Yi-34B, and Mixtral perform notably well, attributed to their large base frequencies in RoPE and their significant parameter sizes.\n\n3. **Notable Observations**:\n   - **Mixtral**: Achieves nearly perfect performance on specific tasks such as passkey retrieval and the vanilla NIAH task (details in Appendix E).\n   - **LWM Model**: Despite being trained for a 1M context size, it performs worse than Llama2-7B at lengths up to 4K but shows less degradation over longer contexts, achieving higher ranks when longer sequence weights (wAvg. inc) is considered.\n  \nThese findings highlight a performance trade-off between short-sequence effectiveness and long-sequence degradation, crucial for evaluating long-context LLMs."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Determine the maximum context size that various long-context language models (LLMs) can effectively handle and understand the degradation in performance as context length increases.",
            "experiment_process": "Ten long-context LLMs, including GPT-4 and nine open-source models with context lengths ranging from 32k to 1M tokens, were evaluated using Ruler on 13 tasks from four categories (Retrieval, Multi-hop Tracing, Aggregation, Question Answering). Each model was tested with 500 examples for each specified length (4k, 8k, 16k, 32k, 64k, 128k) and performance was benchmarked using recall-based accuracy. A threshold-based grading system was used to determine the 'effective length' at which each model could maintain satisfactory performance, with weighted average scores aggregating performance across different context sizes for finer comparisons.",
            "result_discussion": "Significant performance degradation was observed in all models as context length increased. Only four models (GPT-4, Command-R, Yi-34B, and Mixtral) maintained satisfactory performance at 32K context length. The analysis indicated that large models and those trained with longer sequences showed better long-context capabilities. Non-Transformer architectures like RWKV and Mamba lagged behind Transformer models significantly.",
            "ablation_id": "2404.06654v2.No1"
        },
        {
            "research_objective": "Examine the degradation patterns and failure modes in the Yi-34B-200k model when presented with complex tasks and increased input lengths.",
            "experiment_process": "Yi-34B-200k was evaluated on complex tasks with input lengths up to 256K. Performance metrics analyzed included response accuracy in retrieving needles of different forms, handling multiple distracting needles, and retrieving multiple items from long context. Specific degradation patterns were observed through error analysis, focusing on failure modes in multi-hop tracing, common/frequent words extraction, and question answering scenarios.",
            "result_discussion": "Yi-34B-200k showed performance degradation with various complex tasks and increased input lengths. Key failure modes included struggling with UUID retrieval, failing to ignore distractors, outputting duplicated answers, and copying context verbatim in variable tracking and word extraction tasks. In question answering tasks, degradation was noted due to hallucination and reduced reliance on contextual information.",
            "ablation_id": "2404.06654v2.No2"
        },
        {
            "research_objective": "Investigate whether training larger context sizes improve performance on Ruler and analyze the impact of model size and architecture on long-context capabilities.",
            "experiment_process": "Various models within the LargeWorldModels suite, trained up to different context lengths (up to 1M), were evaluated to determine the effect of training context size. Additionally, ablation studies on model sizes (Yi-34B-200k, Yi-9B-200k, and Yi-6B-200k) and non-Transformer architectures (RWKV-v5, Mamba-2.8B-slimpj) were conducted, focusing on effective context length and degradation patterns.",
            "result_discussion": "Models trained with larger context sizes generally performed better but showed inconsistent ranking for longer sequences. Larger models, such as Yi-34B, significantly outperformed smaller counterparts (Yi-6B). Non-Transformer architectures underperformed compared to Transformer models, especially beyond 4K context length.",
            "ablation_id": "2404.06654v2.No3"
        }
    ]
}