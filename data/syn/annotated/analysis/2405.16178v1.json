{
    "title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection",
    "abstract": "Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts.\nHowever, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency.\nIn this paper, we propose a novel paradigm named Sparse RAG, which seeks to cut computation costs through sparsity.\nSpecifically, Sparse RAG encodes retrieved documents in parallel, which eliminates latency introduced by long-range attention of retrieved documents.\nThen, LLMs selectively decode the output by only attending to highly relevant caches auto-regressively, which are chosen via prompting LLMs with special control tokens.\nIt is notable that Sparse RAG combines the assessment of each individual document and the generation of the response into a single process.\nThe designed sparse mechanism in a RAG system can facilitate the reduction of the number of documents loaded during decoding for accelerating the inference of the RAG system.\nAdditionally, filtering out undesirable contexts enhances the model\u2019s focus on relevant context, inherently improving its generation quality.\nEvaluation results of two datasets show that Sparse RAG can strike an optimal balance between generation quality and computational efficiency, demonstrating its generalizability across both short- and long-form generation tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have attracted increasing attention and exhibited impressive abilities to understand instructions and generate fluent outputs in natural language [4  ###reference_b4###, 27  ###reference_b27###, 35  ###reference_b35###, 34  ###reference_b34###].\nNevertheless, LLMs inevitably manifest hallucinations [13  ###reference_b13###] due to their struggle with factual errors and inability to secure the accuracy of generated text solely by the parametric knowledge they encapsulate [39  ###reference_b39###, 26  ###reference_b26###].\nFeeding the source of truth to LLMs in the format of retrieved context segments [29  ###reference_b29###] alleviates this problem. The technique is widely known as Retrieval-Augmented Generation (RAG) [20  ###reference_b20###, 21  ###reference_b21###, 8  ###reference_b8###].\nAlthough the RAG framework is empirically shown to be effective, it can be expensive to scale up. This is because it requires prepending relevant documents retrieved from an external knowledge corpus to the queries [8  ###reference_b8###]. As a result, the input length grows linearly in the number of documents, causing a dramatic increase in latency when using a standard Transformer whose the latency scales quadratically with the input length.\nSome prior works such as Fusion-in-Decoder (FiD) [12  ###reference_b12###] and Parallel Context Windows (PCW) [28  ###reference_b28###] have proposed to alleviate this issue. Yet these methods fail to strike an optimal balance between generation quality and computational efficiency.\nFiD was originally designed for the encoder-decoder architecture, and thus is not compatible with currently prevalent decoder-only architectures without significant changes. While\nPCW can be applied to decoder-only LLMs, it only speeds up the model pre-filling and still incurs high latency since the whole context window cache is still being attended to when decoding each token.\nMoreover, the heavy reliance of generation on the retrieved knowledge raises significant concerns about the model\u2019s behavior and performance in scenarios where retrieval may fail or return inaccurate results [33  ###reference_b33###].\nA typical approach for mitigating this issue is to rely on an external classifier to rank or filter the documents before prepending them to the input [36  ###reference_b36###], but this process requires extra model calls which adds new complexity to inference.\n###figure_1### In light of the issues above, we propose a novel paradigm named Sparse RAG.\nIt generally operationalizes by massive pre-filling where the key-value cache was generated through a single forward pass of the input tokens and selective decoding where the output is generated by attending to highly relevant tokens auto-regressively.\nPrevious works where the length of the retrieved contexts during pre-filling are equal to that during decoding are called dense-RAG in this paper.\nDifferent from that, Sparse RAG allows the pre-filled context to be significantly larger than the decoding context, where partial low-quality context is dynamically dropped according to their relevance to the input query. Furthermore, Sparse RAG combines the assessment of each individual context and the generation of the response into a single process.\nSpecial control tokens are used to prompt LLMs to assess each retrieved context for picking high-quality ones.\nThen only the key-value caches of those picked contexts are loaded into LLMs for decoding with another control tokens.\nThe design of such a sparse mechanism in RAG system brings two unique advantages.\nFirstly, by reducing the number of key-value cache loads during the decoding process, LLMs can achieve lower latency as they are typically constrained by memory usage.\nBesides, filtering out undesirable contexts enhances the model\u2019s focus on relevant context, inherently improving its generation quality.\nTo demonstrate the effectiveness and efficiency of the proposed method, we evaluate on two datasets of PopQA [24  ###reference_b24###], and Biography [25  ###reference_b25###].\nExperimental results show that Sparse RAG can achieve the similar or better performance while keep much better latency compared with standard dense-RAG or PCW-RAG, demonstrating its generalizability across both short- and long-form generation tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "RAG is a family of techniques for generating output structures while using retrieved nearest-neighbor structures as a reference. It typically involves two stages: retrieval and generation.\nRetrieval finds most similar contexts based on BM25 or learned embedding, where the context could be represented as token embedding [16  ###reference_b16###, 37  ###reference_b37###], dense embedding [5  ###reference_b5###] or raw text [9  ###reference_b9###, 12  ###reference_b12###, 19  ###reference_b19###].\nOnce those contexts are retrieved, different architectures are leveraged to incorporate them into the model.\nPopular approaches includes concatenation [12  ###reference_b12###, 19  ###reference_b19###] and cross-attention [3  ###reference_b3###, 18  ###reference_b18###].\nIn recent years, the LLM architecture evolves towards a single decoder only model with significant larger sizes. To this end, concatenation of raw text [19  ###reference_b19###] is becoming popular for being simple and practical.\nMany advanced approaches have been developed on top of it in recent years.\nSelf-RAG [2  ###reference_b2###] is proposed to selectively retrieve knowledge and introduce a critic model to decide whether to retrieve.\nYoran et al. [38  ###reference_b38###] designed an NLI model to identify the irrelevant context and improve robustness.\nSAIL [23  ###reference_b23###] is tuned on instructions to insert retrieved documents before instructions.\nToolformer [30  ###reference_b30###] is pre-trained for calling APIs such as Wikipedia.\nJiang et al. [14  ###reference_b14###] actively anticipate future content and decide when and what to retrieve in long-form generation.\nCRAG [36  ###reference_b36###] makes the attempt to explore and design corrective strategies for RAG to improve its robustness of generation. Specifically, an external T5 model is trained and used to determine the usefulness of the retrieved context.\nGenerally, these approaches target on exploiting retrieval as a useful tool to augment generation or whether retrieval is necessary.\nThe efficiency of LLM inference is a widely explored research area, where different categories of approaches have been studied. Some works are towards architecture level acceleration such as efficient attention [31  ###reference_b31###], Mixture of Expert [6  ###reference_b6###], Transformer-alternative architectures [7  ###reference_b7###], etc. While some other works are algorithm-level acceleration like quantization [22  ###reference_b22###] or speculative decoding [17  ###reference_b17###].\nNevertheless, most previous works are not RAG specific.\nRecently, RAG Cache [15  ###reference_b15###] was proposed as a multilevel dynamic caching system tailored for RAG from the system perspective. Another stream of the work try to process each document individually so that no cross document attention computation is required, such as FiD [12  ###reference_b12###] or PCW [28  ###reference_b28###].\nFiD encoded each retrieved passage independently from other passages and decoded by attending over the concatenation of the resulting representations of all the retrieved passages.\nPCW carved a long context into chunks (\u201cwindows\u201d), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows.\nComparison with previous works that are the most relevant to our work is illustrated in Table 1  ###reference_###.\nThis work aims to strike an optimal balance between generation quality and computational efficiency.\nIt is notable that the extra classifier in CRAG requires maintaining extra model with more complex serving infrastructure. When there is  contexts retrieved. There is totally  model runs.\nOur work also relies an classification process to refine the retrieved documents. However, our approach relies on the \u201cinternal\u201d classification process that is aligned with the generation process. The total model run in our case is ."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Sparse RAG",
            "text": "Sparse RAG is designed for the decoder-only model architecture, which is the default case for most popular LLMs.\nFigure 1  ###reference_### presents an overview of Sparse RAG at inference, which designs the document assessment strategies to improve the robustness of generation.\nThe key hypothesis of our approach is that the RAG task and per context assessment is a similar tasks and the model could do both tasks in one shot.\nWe found simple and effective approaches to enable them.\nFor typical RAG data, one question-answer pair can be mapped to multiple retrieved contexts using either BM25 or existing standalone retriever. However, there are cases where no golden label regarding the quality of every retrieved context is available.\nTo collect these missing labels, we leveraged two off-the-shelf LLMs [1  ###reference_b1###, 34  ###reference_b34###]\u2013PALM2 and Gemini\u2013to get the assessment for each context. We observe empirically that a second round of prompting for critique, especially using a different model from the initial round, ensures the best quality labels. We provide our prompts in Table 9  ###reference_### in the Appendix. We compare different model combinations for labeling to human ground truth labels in Section 4  ###reference_###.\nThe LLM is trained on a mixture of two types of tasks, \u201cRate\u201d assessment and answer generation. Specifically, we format the inputs and outputs of the two task types as\nAssessment: {Question}{Context}{Control_Assessment}{Rate}\nGeneration: {Question}{Context_1}\u2026{Context_N}{Control_Generation}{Answer}\nwhere Rate and Answer are targets for the generative tasks while everything before them are inputs. Control_Assessment and Control_Generation are fixed tokens to ensure the LLM can differentiate the two tasks.\nSince each context is rated independently in PCA task, we introduce this independence in the primary RAG task as well so two tasks could reuse KV cache during inference time.\nThus for the generation task, we enforce no cross attention between different two contexts following the Parallel Context Windows [28  ###reference_b28###] to process the input for training.\nSpecifically, we modify two things in the standard LM training process. First, we\nchange the attention masks to be block-wise, and restrict that context_i and context_j will not attend to each other. Question, Context_i and Control_Generation still follow the default attention mechanism, i.e. in the causal attention case, the latter could attend to the previous ones.\nSecond, we adjust the position encoding to \u201cmimic\u201d the situation that Context_i is not visible to context_j that each context will have its own incremental positional ids. However, the position id of Control_Generation is designed to back to normal, where its position considers all the length of contexts.\nSimilar to the training process, when pre-filling the KV cache, each retrieved context is treated independently. Then these KV cache will be used to score the input, which is the concatenation of the token of Control_Assessment and the token of \u201cGood\u201d. The score is the probability of this tokens as the next tokens.\nThe generation uses a filtered KV cache, where only  out of  KV cache will finally be loaded. We use a simple threshold based filtering approach: we drop the context when its score is less than . Once those caches are loaded, Control_Generation will be used to prompt the model to generate the answers."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training Process",
            "text": "Our work is based on the assumption that a certain amount of RAG training data is accessible, which allows us to effectively tailor and adapt existing LLMs to our specific needs.\nIn the training phase, we integrate an additional Per Context Assessment (PCA) task into the training mixture.\nThis PCA task is included alongside the primary RAG task to enhance the model\u2019s ability to assess and respond accurately within various contexts.\nBy incorporating the PCA task, we aim to improve the overall performance and contextual understanding of the LLMs, ensuring they are more adept at handling a diverse range of scenarios presented during their use.\nFor typical RAG data, one question-answer pair can be mapped to multiple retrieved contexts using either BM25 or existing standalone retriever. However, there are cases where no golden label regarding the quality of every retrieved context is available.\nTo collect these missing labels, we leveraged two off-the-shelf LLMs [1  ###reference_b1###  ###reference_b1###, 34  ###reference_b34###  ###reference_b34###]\u2013PALM2 and Gemini\u2013to get the assessment for each context. We observe empirically that a second round of prompting for critique, especially using a different model from the initial round, ensures the best quality labels. We provide our prompts in Table 9  ###reference_###  ###reference_### in the Appendix. We compare different model combinations for labeling to human ground truth labels in Section 4  ###reference_###  ###reference_###.\nThe LLM is trained on a mixture of two types of tasks, \u201cRate\u201d assessment and answer generation. Specifically, we format the inputs and outputs of the two task types as\nAssessment: {Question}{Context}{Control_Assessment}{Rate}\nGeneration: {Question}{Context_1}\u2026{Context_N}{Control_Generation}{Answer}\nwhere Rate and Answer are targets for the generative tasks while everything before them are inputs. Control_Assessment and Control_Generation are fixed tokens to ensure the LLM can differentiate the two tasks.\nSince each context is rated independently in PCA task, we introduce this independence in the primary RAG task as well so two tasks could reuse KV cache during inference time.\nThus for the generation task, we enforce no cross attention between different two contexts following the Parallel Context Windows [28  ###reference_b28###  ###reference_b28###] to process the input for training.\nSpecifically, we modify two things in the standard LM training process. First, we\nchange the attention masks to be block-wise, and restrict that context_i and context_j will not attend to each other. Question, Context_i and Control_Generation still follow the default attention mechanism, i.e. in the causal attention case, the latter could attend to the previous ones.\nSecond, we adjust the position encoding to \u201cmimic\u201d the situation that Context_i is not visible to context_j that each context will have its own incremental positional ids. However, the position id of Control_Generation is designed to back to normal, where its position considers all the length of contexts."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Inference Process",
            "text": "Given the question and retrieved contexts, Sparse RAG handles the assessment task and generation task in one single pass.\nSimilar to the training process, when pre-filling the KV cache, each retrieved context is treated independently. Then these KV cache will be used to score the input, which is the concatenation of the token of Control_Assessment and the token of \u201cGood\u201d. The score is the probability of this tokens as the next tokens.\nThe generation uses a filtered KV cache, where only  out of  KV cache will finally be loaded. We use a simple threshold based filtering approach: we drop the context when its score is less than . Once those caches are loaded, Control_Generation will be used to prompt the model to generate the answers."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Evaluation of Per Context Assessment",
            "text": ""
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "LLM rater comparisons",
            "text": "We tested several different LLM-based automatic labeling methods\u2013different combinations of models and prompts\u2013for creating training data for the classifier in Sparse RAG. We compared several of these auto-rater approaches by creating a ground-truth relevance dataset using human labeling.\nThe auto-rater comparison using the revised human labels as the ground-truth is shown in Table 2  ###reference_###. We find that combining two different models in two rounds\u2013initial prompting and critique\u2013provides the labels that are most closely aligned with the human labels. We hypothesize that the different representations learned by two different models are able to capture the most nuance in the input sequences, leading to better relevance judgements. We also observe that Gemini Ultra appears slightly less effective at critiquing model outputs than PALM2 XL."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluation of Sparse RAG",
            "text": "is a large-scale open-domain question answering (QA) dataset, consisting of 14k entity-centric QA pairs. Each question is created by converting a knowledge tuple retrieved from Wikidata using a template. We follows the setup from  [36  ###reference_b36###] and use Contriver [11  ###reference_b11###] to retrieve the related contexts.\nSince PopQA is lacking the per-context assessment label, we adopted the \u201cGemini + PALM2\u201d combination to get training labels. We split the dataset into training, validation and test set with 8:1:1 ratio.\nSince the answer is usually short, we report Exact Match (EM) and F1 scores.\n[40  ###reference_b40###] is a human-annotated benchmark for query-based multi-domain meeting summarization task, which consists of 1,808 query-summary pairs over 232 meetings in multiple domains. To adopt it in the RAG domain, we divide each conversation into different contexts where the average context contains 300 words, and filter the data point where the conversation containing ground truth label is split into multiple contexts.\nNote that this dataset has human labeled per-context assessment that we could leverage during training.\nEventually we get  training data,  validation and  test set. The target of this data is longer and we report RougeLSum and F1 scores.\nWe evaluated the standard RAG [20  ###reference_b20###] where an LM generates output given the query prepended with all the top ranked documents using the same retriever as\nin our system.\nWe applied the Parallel Context Windows [28  ###reference_b28###] to the RAG process, where no cross attention is applied between documents.\nIt is the same with RAG except that an external T5-XXL classifier is trained using heuristic labels [36  ###reference_b36###]. This classifier is used to process all the document and decide the rank. Note to facilitate the fair comparison, we did not adopt the \"web search\" feature of this paper."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Benchmarks and Metrics",
            "text": "is a large-scale open-domain question answering (QA) dataset, consisting of 14k entity-centric QA pairs. Each question is created by converting a knowledge tuple retrieved from Wikidata using a template. We follows the setup from  [36  ###reference_b36###  ###reference_b36###] and use Contriver [11  ###reference_b11###  ###reference_b11###] to retrieve the related contexts.\nSince PopQA is lacking the per-context assessment label, we adopted the \u201cGemini + PALM2\u201d combination to get training labels. We split the dataset into training, validation and test set with 8:1:1 ratio.\nSince the answer is usually short, we report Exact Match (EM) and F1 scores.\n[40  ###reference_b40###  ###reference_b40###] is a human-annotated benchmark for query-based multi-domain meeting summarization task, which consists of 1,808 query-summary pairs over 232 meetings in multiple domains. To adopt it in the RAG domain, we divide each conversation into different contexts where the average context contains 300 words, and filter the data point where the conversation containing ground truth label is split into multiple contexts.\nNote that this dataset has human labeled per-context assessment that we could leverage during training.\nEventually we get  training data,  validation and  test set. The target of this data is longer and we report RougeLSum and F1 scores."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Baselines",
            "text": "We evaluated the standard RAG [20  ###reference_b20###  ###reference_b20###] where an LM generates output given the query prepended with all the top ranked documents using the same retriever as\nin our system.\nWe applied the Parallel Context Windows [28  ###reference_b28###  ###reference_b28###] to the RAG process, where no cross attention is applied between documents.\nIt is the same with RAG except that an external T5-XXL classifier is trained using heuristic labels [36  ###reference_b36###  ###reference_b36###]. This classifier is used to process all the document and decide the rank. Note to facilitate the fair comparison, we did not adopt the \"web search\" feature of this paper."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Experimental Configuration",
            "text": "The base LLMs used in the paper Gemini [34  ###reference_b34###].\nAlthough our approach could be applied to different training stage of the model, we apply LoRA tuning [10  ###reference_b10###] to enforce alignment on top of the foundation LLMs due to its low resource requirement and wide usage. Note that the same LoRA tunning is applied to Sparse RAG and all baselines. In all our experiment, we apply LoRA in self-attention and use the default rank as 4. By default, we use the XXS size which could be running on-device.\nDuring training, we use  Tensor Processing Units (TPU) V3 chips for PopQA while use  Units for .\nThe batch size is .\nWe use the Adafactor optimizer [32  ###reference_b32###] with a learning rate of . The training dropout rate is .\nWe leverage the metrics of validation set to pick the best checkpoint.\nDuring inference, the temperature is set to . Unless specifically noted, we use sampling decoding with sample number  for our experiments."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Inference Setup and Metrics",
            "text": "Evaluation of Sparse RAG was conducted on a Samsung S21 Ultra, utilizing the device\u2019s CPU to assess real-world performance on a relatively mid-tier smartphone compared to the latest flagship models. Inference configuration consisted of fixed token lengths for queries, contexts and generated responses. This setup allows for evaluating the system\u2019s efficiency and effectiveness under resource constraints typical of mobile devices, providing insights into its practical applicability for on-device question-answering tasks.\nSpecifically, the overall inference process considers two stages.\nPrefill stage:\nFor baseline RAG Model, we measure the total time taken to process all input tokens (question and all contexts).\nFor PCW RAG and Sparse RAG models: We take advantage of these models\u2019 ability to cache question processing. We first measure the time to process the question alone. Then, we measure the time to process different contexts with the pre-processed kv-cached question. We use Encoding Speed (ES) (tokens per second (t/s)) to measure the efficiency of prefill stage.\nDecoding stage:\nTo assess decoding speed comprehensively, we generate output sequences using the same question but varying the amount of relevant context data considered, ranging from the top 1 most relevant document to the top . For each context size, we produce output sequences of different lengths. This systematic approach allows us to evaluate the impact of both context size and response length on decoding speed. We use Decoding Speed (DS) (tokens per second (t/s)) to measure the efficiency of decoding stage."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "We report the results with both short form answer (PopQA) and long form answer (QMSum) in Table 3  ###reference_###, where both quality and latency metrics are shared. \u201cK\u201d is the number of chosen memories.\nNotably, our proposed approach achieves the best quality while being the most efficient during inference compared to other \u201cDense RAG\u201d approaches.\nIt can be seen that Sparse RAG shares the same pre-filling efficiency with PCW, due to the parallel context encoding. However Sparse RAG can achieve significant better decoding efficiency than standard RAG and PCW RAG.\nIn detail, out of 20 retrieved contexts, the Sparse RAG has average of  context for PopQA and  context for QMSum. This leads to almost double or even triple decoding speed.\nMeanwhile, Sparse RAG achieves higher quality metrics than the dense counterparts, which shows that Sparse RAG effectively filtered noisy and irrelevant contexts.\nOn the other hand, it is interesting to compare Corrective-RAG(CRAG) with Sparse RAG on PopQA task since Corrective-RAG trained their own external classifier with T5 XXL based on PopQA data. Results suggested that our approach, albeit using an \u201cin-place\u201d classifier, is outperforming the CRAG with external classifier. Note the encoding and decoding speed of CRAG is not comparable because it involves multiple model running from the classifier."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Analysis",
            "text": ""
        },
        {
            "section_id": "5.6.1",
            "parent_section_id": "5.6",
            "section_name": "5.6.1 Impact of Confidence Threshold",
            "text": "To show how the metrics changes with different thresholds, the performance of Sparse RAG corresponding to different thresholds is illustrated in Table 4  ###reference_###.\nAs the threshold gradually increases, the system will filter out more contexts, which would always reduce the latency during inference.\nIn terms of generation quality, it can be seen that the performance of Sparse RAG was significantly improved as thresholds increased at the beginning, which shows the effectiveness of filtering out irrelevant contexts.\nThen, the performance was stable and dropped slightly.\nThe reason might be that some truly relevant contexts are accidentally filtered out."
        },
        {
            "section_id": "5.6.2",
            "parent_section_id": "5.6",
            "section_name": "5.6.2 Silver Labels vs LLM Labels",
            "text": "In Corrective RAG, the T5 model was trained with silver labels that comes from the title matching [36  ###reference_b36###]. We collect the same silver labels to replace the LLM labels and train the model with this new dataset. The comparison result is shown in Table 5  ###reference_###.\nFrom these results, we can observe that the quality of the labels generated by LLMs is higher than that of the silver labels from Yan et al. [36  ###reference_b36###].\nWe hypothesize that the superior quality of the LLM-generated labels is attributable to our methodology, which involved a two-round process of soliciting responses from two different LLMs.\nBy engaging two distinct models, we likely enhanced the robustness and accuracy of the labels through a form of cross-validation, thereby mitigating potential biases or errors that might arise from relying on a single LLM."
        },
        {
            "section_id": "5.6.3",
            "parent_section_id": "5.6",
            "section_name": "5.6.3 Ablation of Prefill Documents",
            "text": "One question one may have is that the better performance may rely on the \u201cmassive\u201d prefilling but in practical situation, we can retrieve smaller amount of context which have better quality.\nTo this end, we compare different numbers of the prefill document: 10 vs 20 for PopQA, and show the results in Table 6  ###reference_###.\nEven in the setting lower prefilled case, the previous conclusion of Sparse RAG remains. It achieves better quality with higher speed of decoding.\nIt makes sense that the gap between dense RAG and sparse RAG is relative smaller at 10 prefill documents. This is because the headroom is smaller as the top 10 documents naturally has less to filter and make it less \u201csparse\u201d. Yet our approach still achieves best trade-off, showing the capability to generalize with different numbers of input documents."
        },
        {
            "section_id": "5.6.4",
            "parent_section_id": "5.6",
            "section_name": "5.6.4 Inference Efficiency Ablations",
            "text": "Focusing on the decoding stage, we delve into the performance differences when varying the number of retrieved memories (i.e., top-K documents) and the length of the generated response. This analysis provides valuable insights into the computational overhead associated with incorporating additional context and generating more extensive outputs, ultimately shedding light on the practical trade-offs for real-time question-answering on resource-constrained devices.\nAs illustrated in Fig 2(b)  ###reference_sf2###, we observed that the baseline method requires over 50%(varies from 52% to over 55% depending on number of output tokens) more time to generate outputs of varying lengths compared to the Sparse RAG approach. This significant disparity in generation speed is primarily attributed to the increased computational burden incurred by the baseline method when considering a larger number of memories. As shown in Fig 2(a)  ###reference_sf1###, this heightened demand for computational resources results in a notable slowdown in terms of tokens per second (t/s). This observation underscores the efficiency advantages offered by Sparse RAG, especially in scenarios where a substantial amount of context needs to be considered during the decoding process.\n###figure_2### ###figure_3###"
        },
        {
            "section_id": "5.6.5",
            "parent_section_id": "5.6",
            "section_name": "5.6.5 Ablation on Foundation Model Size",
            "text": "We applied Sparse RAG to different sizes of LLMs, specifically testing it on Gemini XS and Gemini XXS.\nThe results of these experiments are presented in Table 7  ###reference_###.\nThe findings demonstrate that Sparse RAG is compatible with various foundation models, effectively adapting to different model sizes.\nNotably, with a reduced amount of decoding caches, Sparse RAG is capable of achieving the highest quality results.\nThis indicates that Sparse RAG maintains its efficiency and effectiveness across different foundation models, making it a versatile approach for various LLM configurations."
        },
        {
            "section_id": "5.6.6",
            "parent_section_id": "5.6",
            "section_name": "5.6.6 Using Golden Context Label during inference",
            "text": "Since QMSum provides golden per-context labels, we leverage these labels during inference to evaluate the upper bound performance under the condition of perfect per-context assessment.\nBy utilizing the highest quality labels available, we aim to determine the best possible outcomes that our model can achieve.\nThe results of this experiment, which highlight the performance ceiling under ideal labeling conditions, are presented in Table 8  ###reference_###.\nThis approach allows us to understand the maximum potential of our model when supplied with optimal input data, thereby offering insights into its ultimate capabilities."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents Sparse RAG to address the challenges of increased input length and latency.\nThrough a novel approach of massive pre-filling and selective decoding, Sparse-RAG efficiently manages the key-value cache of retrieved documents, allowing the LLMs to focus on highly relevant tokens.\nThis selective attention mechanism not only reduces the computational burden during inference but also enhances the generation quality by filtering out irrelevant contexts.\nThe evaluation on two datasets validates Sparse RAG\u2019s ability to achieve a balanced trade-off between high-quality generation and computational efficiency, proving its versatility and effectiveness for both short- and long-form content generation tasks.\nThis innovative paradigm showcases the potential for improving LLM performance in various applications by optimizing context management and inference processes."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledge",
            "text": "We would like to thanks Yu-hui Chen, Yuqi Li, Qifei Wang, Zonglin Li, Zhong Meng, Alex Chen, Jingbin Wang, Abhanshu Sharma, Alec Go, Jesper Andersen, David Racz, DeLesley Hutchins, Luke Sernau, David Munday, and Ewa Dominowska for their suggestions."
        }
    ],
    "url": "http://arxiv.org/html/2405.16178v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6",
            "5.6.1",
            "5.6.2",
            "5.6.3",
            "5.6.4",
            "5.6.5",
            "5.6.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.6.3",
            "5.6.4",
            "5.6.5"
        ]
    },
    "research_context": {
        "paper_id": "2405.16178v1",
        "paper_title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection",
        "research_background": "### Motivation:\nLarge language models (LLMs) have demonstrated impressive capabilities in understanding instructions and generating natural language outputs. However, they are prone to hallucinations due to factual inaccuracies and limitations in their parametric knowledge. Retrieval-Augmented Generation (RAG) mitigates this problem by feeding LLMs with retrieved context segments. Despite its effectiveness, the RAG framework is costly in terms of computational efficiency and latency, especially as the input length grows linearly with the number of documents retrieved. There is a need for an approach that balances generation quality with computational efficiency and reduces reliance on retrieval accuracy.\n\n### Research Problem:\nThe primary research problem addressed by this paper is to improve the efficiency of the Retrieval-Augmented Generation (RAG) framework while maintaining or enhancing generation quality. Specifically, the paper aims to reduce the latency associated with the extensive context input required by RAG and address issues related to high memory usage and the potential failure of the retrieval system to return high-quality documents.\n\n### Relevant Prior Work:\n1. Large Language Models:\n   - Demonstrated abilities: impressive understanding and generation of natural language [4, 27, 35, 34].\n   - Issues: Prone to hallucinations and factual inaccuracies [13, 39, 26].\n\n2. Retrieval-Augmented Generation (RAG):\n   - Alleviates factual errors by using retrieved context segments [29].\n   - Drawback: Expensive scaling due to increased input length and associated latency [8, 20, 21].\n\n3. Methods for Improving Efficiency:\n   - Fusion-in-Decoder (FiD): Faster but not compatible with decoder-only architectures [12].\n   - Parallel Context Windows (PCW): Applicable to decoder-only models but still incurs high latency [28].\n\n4. External Classifiers for Document Ranking:\n   - Mitigation approach: Uses classifiers to filter documents before encoding, but adds complexity and requires extra model calls [36].\n",
        "methodology": "### Methodology: Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection\n\nThe proposed method, Sparse RAG, is specifically designed for a decoder-only model architecture, commonly used in large language models (LLMs). Sparse RAG aims to enhance the efficiency and robustness of the Retrieval-Augmented Generation (RAG) task during inference by integrating document assessment strategies.\n\n#### Key Hypothesis and Approach\nSparse RAG operates on the hypothesis that RAG tasks and per-context assessments are sufficiently similar, allowing the model to handle both tasks in a single pass. This approach hinges on leveraging simple yet effective methods to enable these dual functions.\n\n#### Data Preparation\nFor typical RAG data, each question-answer pair is associated with multiple retrieved contexts via methods like BM25 or existing standalone retrievers. In cases where quality labels for each retrieved context are absent, the methodology employs two off-the-shelf LLMs, specifically PALM2 and Gemini, to generate assessments. Empirical observations suggest that a second round of critique from a different model significantly improves label quality. The prompts used in this labeling process are provided in the Appendix (Table 9).\n\n#### Task Formulation\nThe LLM is trained on a mixture of two types of tasks:\n1. **Assessment Task (Rate Assessment)**\n   - Format: `{Question}{Context}{Control_Assessment}{Rate}`\n   - Role: Provides quality ratings for each context.\n2. **Generation Task (Answer Generation)**\n   - Format: `{Question}{Context_1}\u2026{Context_N}{Control_Generation}{Answer}`\n   - Role: Generates answers based on provided contexts.\n\nIn these tasks, \"Rate\" and \"Answer\" are the target outputs, while the components before them serve as inputs. Control_Assessment and Control_Generation are fixed tokens ensuring the LLM can distinguish between assessment and generation tasks.\n\n#### Training and Independence of Contexts\nGiven that each context is rated independently in the Per Context Assessment (PCA), this independence is introduced to the primary RAG task. This ensures both tasks can share the Key-Value (KV) cache during inference, preventing cross-attention between different contexts and using Parallel Context Windows.\n\n#### Modifications in Training Process\nTwo main modifications are introduced into the standard LM (Language Model) training process:\n1. **Attention Masks:**\n   - Attention masks are adjusted to be block-wise, ensuring Context_i does not attend to Context_j. Standard attention mechanisms apply to Question, Context_i, and Control_Generation.\n2. **Position Encoding:**\n   - Positional encoding is adjusted so each context has its own incremental position identifiers, mimicking independent visibility. Control_Generation\u2019s position considers the entire length of all contexts.\n\n#### Inference Optimization via KV Cache\nDuring inference, each retrieved context is treated independently when pre-filling the KV cache. These caches are then scored using tokens of Control_Assessment and the token \u201cGood's\u201d probability as the next token. A threshold-based filtering approach is applied to these scores, dropping contexts below a specified threshold. The filtered KV cache is subsequently used to generate answers, prompted by Control_Generation.\n\n### Summary\nThe Sparse RAG method innovates by:\n- Combining Rate Assessment and Answer Generation into a single model pass.\n- Utilizing existing LLMs for high-quality context labeling.\n- Employing Parallel Context Windows to maintain context independence.\n- Optimizing inference through selective KV caching and position encoding adjustments.\n\nBy implementing these strategies, Sparse RAG enhances retrieval-augmented generation efficiency, ensuring robust and quality-aware answer generation.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets:**\n\n1. **PopQA**: This is a large-scale open-domain question answering (QA) dataset consisting of 14k entity-centric QA pairs. Each question is created by converting a knowledge tuple retrieved from Wikidata using a template. The dataset is split into training, validation, and test sets with an 8:1:1 ratio.\n   \n2. **Data from [40]**: A human-annotated benchmark for query-based multi-domain meeting summarization, comprising 1,808 query-summary pairs over 232 meetings in multiple domains. For use in the Retrieval-Augmented Generation (RAG) setup, each conversation was divided into contexts averaging 300 words, filtering data points where the conversation containing the ground truth label was split into multiple contexts. This dataset is also split into training, validation, and test sets.\n   \n**Baselines:**\n\n1. **Standard RAG [20]**: In this setting, a Language Model (LM) generates output given the query prepended with all the top-ranked documents using the same retriever as in our system.\n   \n2. **Parallel Context Windows [28]**: Similar to standard RAG but with no cross attention applied between documents. \n\n3. **\"Gemini + PALM2\u201d Combination**: Particularly for PopQA, due to the lack of per-context assessment labels, this combination is used to get training labels.\n\n4. **Use of T5-XXL Classifier [36]**: An external T5-XXL classifier is trained using heuristic labels to rank documents. \n\n**Evaluation Metrics:**\n\n1. **PopQA**:\n   - **Exact Match (EM)**\n   - **F1 Score**\n   \n2. **Meeting Summarization dataset from [40]**:\n   - **RougeLSum**\n   - **F1 Score**\n   \n**Main Experimental Results:**\n\n- **PopQA Results**: Reporting exact match (EM) and F1 scores to evaluate the quality of short answers.\n- **Meeting Summarization Results**: Reporting RougeLSum and F1 scores due to the longer target responses.\n\nNote: The experimental results would include a detailed comparison of these metrics across different methods/approaches but are specifically centered on the described datasets and evaluation criteria.\n"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare the performance of Sparse RAG with varying numbers of prefill documents and evaluate its ability to generalize with different amounts of input context.",
            "experiment_process": "Different numbers of the prefill document are compared: 10 vs 20 for PopQA, and the results are shown in Table 6. The setup involves comparing the decoding speed and quality between dense RAG and sparse RAG under these conditions.",
            "result_discussion": "Even with a lower number of prefill documents, Sparse RAG maintains better quality and higher decoding speed compared to dense RAG. The performance gap between dense RAG and sparse RAG is smaller with 10 prefill documents because there are fewer documents to filter, making it less 'sparse'. However, Sparse RAG still achieves the best trade-off, demonstrating its ability to generalize with different numbers of input documents.",
            "ablation_id": "2405.16178v1.No1"
        },
        {
            "research_objective": "To investigate the computational overhead and efficiency of Sparse RAG during the decoding stage, especially concerning the number of retrieved memories and the length of generated responses.",
            "experiment_process": "The performance differences were examined by varying the number of top-K retrieved documents and the output lengths. The computational time and tokens per second (t/s) were measured and compared between the baseline method and Sparse RAG, as illustrated in Fig 2(b) and Fig 2(a).",
            "result_discussion": "The baseline method requires over 50% more time (varying from 52% to over 55% depending on the number of output tokens) to generate responses compared to Sparse RAG. This is due to the increased computational burden of handling a larger number of memories in the baseline method. Sparse RAG shows significant efficiency advantages, particularly in scenarios that require considering substantial context during decoding.",
            "ablation_id": "2405.16178v1.No2"
        },
        {
            "research_objective": "To assess the compatibility and effectiveness of Sparse RAG with different sizes of large language models (LLMs).",
            "experiment_process": "Sparse RAG was applied to different sizes of LLMs, specifically Gemini XS and Gemini XXS. The outcomes of these experiments are presented in Table 7, focusing on the quality and efficiency with reduced decoding caches.",
            "result_discussion": "Sparse RAG is compatible with various foundation models, adapting effectively to different model sizes. With fewer decoding caches, Sparse RAG achieves the highest quality results, indicating that it maintains efficiency and effectiveness across different LLM configurations. This makes Sparse RAG a versatile approach for various LLM sizes.",
            "ablation_id": "2405.16178v1.No3"
        }
    ]
}