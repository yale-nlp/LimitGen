{
    "title": "Math Multiple Choice Question Generation via Human-Large Language Model Collaboration",
    "abstract": "Multiple choice questions (MCQs) are a popular method for evaluating students\u2019 knowledge due to their efficiency in administration and grading. Crafting high-quality math MCQs is a labor-intensive process that requires educators to formulate precise stems and plausible distractors. Recent advances in large language models (LLMs) have sparked interest in automating MCQ creation, but challenges persist in ensuring mathematical accuracy and addressing student errors. This paper introduces a prototype tool designed to facilitate collaboration between LLMs and educators for streamlining the math MCQ generation process. We conduct a pilot study involving math educators to investigate how the tool can help them simplify the process of crafting high-quality math MCQs. We found that while LLMs can generate well-formulated question stems, their ability to generate distractors that capture common student errors and misconceptions is limited. Nevertheless, a human-AI collaboration has the potential to enhance the efficiency and effectiveness of MCQ generation.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Multiple choice questions (MCQs) are widely used to evaluate students\u2019 knowledge since they enable quick and accurate administration and grading [2  ###reference_b2###, 6  ###reference_b6###, 9  ###reference_b9###]. MCQs are constructed in a specific format. The stem refers to the statement on the problem setup and context, followed by a question that needs to be answered. Among the options, the correct one can be referred to as the key, while incorrect ones can be referred to as distractors. As the name implies, distractors in MCQs are typically formulated to align with common errors among students. These distractors are chosen because students either i) lack the necessary comprehension of the knowledge components (KCs) or concepts/skills tested in the question to accurately identify the key as the correct answer or ii) exhibit misconceptions that make them think a specific distractor is correct.\nWhile MCQs offer many advantages in student knowledge assessment, manually crafting high-quality MCQs, especially in math-related domains, is a demanding and labor-intensive process [5  ###reference_b5###].\nThere are three main tasks in this process: First, educators need to formulate a question stem that effectively encapsulates the KCs they aim to test. Second, educators need to anticipate common errors and/or misconceptions among students and create corresponding distractors. Third, educators need to provide feedback to students who select distractors that can help them identify their errors and lead them to the correct answer, to expedite their learning process.\nThe emergence of large language models (LLMs) has raised hopes for making MCQ creation more scalable by automating the process. Specifically, few-shot, in-context learning is promising for generating math MCQs since LLMs can follow instructions based on contextual information conveyed by a few examples. While automated question generation for open-ended questions has shown notable success, generating plausible distractors within MCQs presents a different challenge: distractors should be based on anticipated student errors/misconceptions [12  ###reference_b12###], whereas LLMs have not necessarily learned this information during training. Moreover, math MCQs are challenging since they require mathematical reasoning, which means that distractors cannot be generated using a knowledge graph [13  ###reference_b13###] or paraphrasing tool [8  ###reference_b8###]. Consequently, math educators need to take an important role in guiding LLMs in math MCQ generation: LLMs are responsible for scaling up the process while humans use their expertise efficiently. Therefore, we raise following are two core research questions (RQs) that help identify opportunities to generate math MCQs through collaboration between LLMs and human educators: 1) RQ1: Can LLMs generate valid MCQs, especially distractors and feedback corresponding to common student errors/misconceptions? 2) RQ2: What are the key design elements in a system where human math educators and LLMs collaborate on MCQ generation?"
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Contributions",
            "text": "In this paper, we introduce a prototype tool called the Human Enhanced Distractor Generation Engine(HEDGE) for math MCQ creation, which leverages the expertise of educators by asking them to edit LLM-generated MCQs in a two-step process. In the first step, we prompt the LLM to generate stem, key, and explanation in an MCQ, and ask educators to evaluate and edit the output to make sure it is mathematically correct and relevant to the intended KC. In the second step, we prompt the LLM to generate a set of possible errors/misconceptions and the corresponding distractors and feedback, and ask educators to evaluate and edit the output to make sure they correspond to valid distractors to the generated question stem. In a pilot study, we recruit four former/current math teachers to evaluate our tool on generating math MCQs related to five pre-defined KCs. Results show that educators considered 70% of the generated stem, key, and explanation generated by GPT-4 as valid. However, they only considered 37% of the generated misconception, distractor, and feedback valid, which reveals significant limitations of LLMs in capturing anticipated common errors/misconceptions among real students. This observation underscores the necessity of involving humans in the process of generating math MCQs and leveraging real math educators\u2019 expertise on common errors among students."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Human Enhanced Distractor Generation Engine",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Overview",
            "text": "###figure_1### HEDGE is our prototype for math MCQ generation that generates math MCQ for a given mathematical KC, as illustrated in Figure 1  ###reference_###. These KCs are categorized into three levels of granularity: coarse, medium, and fine-grained. For instance, KCs can cover either a broad topic such as \u201cbasic arithmetic\u201d or a specific topic like \u201cIdentify that a problem needs to be solved using addition.\u201d HEDGE is designed to utilize LLMs within OpenAI. The provided example is generated using ChatGPT. We take a two-step approach for MCQ generation: 1) generate the question step and answer key, and an explanation, and 2) generate a list of possible misconceptions, corresponding distractors, and feedback messages. We implement both steps using by prompting LLMs with an in-context example of these tasks.\nThe in-context example shows the KC converting ratios to fractions, employing a real-life scenario in which Kate and Isaac share yogurt in a  ratio. The objective is to calculate the fraction representing Kate\u2019s share, . In this context, we list three common misconceptions. First, a student mistakenly thinks that the ratio  could be directly converted into the fraction . Second, a student mistakenly calculates the difference between Kate\u2019s and Issac\u2019s share. Third, a student mistakenly think the goal is to calculate Issac\u2019s share. These misconceptions, along with the corresponding feedback on how to resolve them, are included as part of the in-context example.\nNow, we explore a scenario where an educator creates MCQs using our tool based on the concept of basic arithmetic, specifically focusing on mental addition. In the first step, given the target KC, along with an in-context example consisting of the concept, stem, key, and explanation, the LLM generates the following stem: \u201cSally has 5 apples. She gives 2 apples to her friend. How many apples does Sally have left?\u201d However, this stem mistakenly embodies the KC of subtraction rather than addition. Therefore, the educator edits the generated results to align it with the intended KC of addition. In the second step, using the adjusted stem, key, and explanation, as well as incorporating in-context examples with distractors, misconceptions, and feedback, the LLM generates distractors along with corresponding misconceptions and feedback. Figure 1  ###reference_### illustrates option , which contains a misconception related to subtraction instead of addition, accompanied by feedback designed to correct this error. Additionally, the educator has the option to edit option  to address any misconceptions associated with multiplication."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "User Interface",
            "text": "We develop HEDGE interface, as illustrated in Figure 2  ###reference_###. This interface is built using React and employs Firestore as its database for data storage. The interface comprises three components: a Sidebar, a Preview, and a Generation.\nThe educator generates MCQs using the Generation component as discussed in Section 2.1  ###reference_###. Here, after prompting LLMs using the edited stem, key, and explanation, we add a rating step to assess the overall quality of misconceptions, distractors, and feedback that the educator rates based on a 5-point Likert scale.\nOnce the educator completes the distractor editing process, the Preview component displays a fully structured MCQ, with the answer options randomized. We store any metadata that isn\u2019t visually represented within the image. Following the completion of distractor editing, the Sidebar component is refreshed. The educator can click on the stem to view the generated image along with the answer sheet or create a new MCQ.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Pilot Study",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "We perform a pilot study to assess the usability of HEDGE in generating MCQs. In this study, we select pre-defined KCs and instruct participants to utilize these KCs to simulate a scenario where an educator is crafting MCQs. We select the KCs and the in-context example from a large education company\u2019s content repository, categorized under the label \u201cNumber,\u201d encompasses various subtopics, such as \u201cBasic Arithmetic,\u201d \u201cFractions,\u201d and \u201cRounding and Estimating.\u201d We choose five KCs, as shown in Table 2  ###reference_###, from the KCs that incorporate mathematical expressions, such as fractions, powers, and surds. We utilize GPT-4 as LLM for the study and set the parameters to temperature  and top_p value  to balance creativity and consistency of the generated MCQs. After completing the study, participants are asked to complete an exit survey. The survey includes open-ended questions and ratings on their satisfaction with the quality of LLM-generated responses and the usability of the tool using a 5-point Likert scale."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Participants",
            "text": "We recruit four participants for the study, comprising one male and three females, all recruited through Upwork [14  ###reference_b14###]. Among them, two currently work as middle/high school math teachers, while the other two currently work as tutors, with prior experience as former math teachers. All participants are selected based on their qualifications and expertise in mathematics education. Each participant was tasked with creating five MCQs using the HEDGE, employing the five KCs specified in Table 2  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In , educator corrected grammar error of \u201cshe have\u201d to \u201cshe has.\u201d No other grammar errors occurred in the study besides this one, underscoring the capability of LLMs to consistently produce grammatically correct sentences.\nRegarding 5th KC, GPT-4 shows a lack of knowledge on the distinction between simplified and non-simplified surd. The followings are invalid stems generated by GPT-4: 1) . If  is a simplified surd, what is its non-simplified form? 2) . Express the simplified surd  in a non-simplified form. 3) . A simplified surd is . How can it be represented in non-simplified form?\nThis invalid stem has misled a participant to edit a stem to convey KC as simplifying surd, which is the opposite of non-simplifying surd ().\nIn , GPT-4 generated a key of $4750, erroneously calculating the car price after one year instead of two years. However, in the other three cases within the same KC, GPT-4 calculated correctly, showing its math problem-solving skills.\nAmong 60 distractors, educators identified 22 responses as valid, including two cases that are actually invalid.\nThese cases have valid misconception and distractor and educators has made adjustments to the feedback to enhance its clarity. For example, one of the distractors for  is . The feedback generated by GPT-4 is as follows: \u201cYou seem to have compared only the numerators of the fractions. However, when checking for equivalent fractions, both the numerator and denominator need to be considered. The fraction  is not equivalent to .\u201d The educator removed the redundant final sentence and introduced \u201cRemember, equivalent fractions require both the numerator and denominator to be proportional.\u201d, which helps students better understand the importance of considering both the numerator and denominator when comparing fractions for equivalence. This adjustment emphasizes that the equivalence between fractions relies on maintaining proportionality between the numerator and denominator. While GPT-4 provides valid explanations, it sometimes fail to include critical insights that are necessary for students\u2019 improvement.\nThese cases are often due to a mismatch between the misconception and the distractor. In , the misconception \u201cThe student mistakenly believed that the car depreciates by a constant amount each year, not a percentage.\u201d did not match the distractor . Additionally, there are cases when, even if the distractor is valid, it may not effectively encapsulate student misconceptions. In , the educator updated the distractor from  to , making it a more attractive distractor for those who confuse factors for multiples.\nAs in Case 4, invalid cases are often due to a mismatch between the misconception and the distractor. In , the misconception \u201cThe student may believe that all square roots are in their simplest form.\u201d did not match the distractor \u201c.\u201d The educator updated the misconception as \u201cThe student may have confused square roots with cube roots.\u201d providing a more accurate misconception for the distractor. Additionally, there are cases when, even if the misconception is valid, it may not likely be the misconception why the student selects the distractor. In , the educator updated the misconception of distractor \u201c\u201d from \u201cThe student might think that only the numbers less than 18 can be the factors of 18.\u201d to \u201cThe student might think that any even number can be a factor of an even number.\u201d, making it more accurate for addressing the student\u2019s misconception.\nThese cases were when educators adopted distractors and edited wrong misconceptions and feedback. For example, in the case of ,  is a valid distractor as the student could simply multiply  and . However, the misconception and feedback generated by GPT-4 did not align with the distractor; therefore the educator had to edit it accordingly.\nIn Cases 4, 5, and 6, LLMs revealed inconsistent mathematical reasoning when analyzing misconceptions, distractors, and feedback for a given stem. The inconsistency underscores a necessity for human educators to manually align distractors and their underlying misconceptions and corresponding feedback in many cases.\nThese cases were when misconceptions had poor quality or were wrong, resulting in inadequate distractors and feedback. Two of the distractors generated for  by GPT-4 shows both poor quality and wrong misconceptions. While the misconception in the first distractor is valid, stating that \u201cThe student may not divide both the numerator and denominator by the same number,\u201d the distractor itself, represented by , and its associated feedback lack coherence and fail to align with this misconception. Meanwhile, the misconception in the second distractor () lacks coherence, as expressed in the following manner: \u201cThe student may confuse the concept of equivalent fractions with simplifying fractions.\u201d These results reveal that LLMs often fail to anticipate valid misconceptions and errors that are common among students, making human educators\u2019 involvement crucial in the creation of math MCQs.\nOn a 5-point Likert scale, the participants gave an average rating of 4. This rating aligns with the open-ended responses regarding most of the generated stem, key, and explanation valid. However, two participants addressed the tool\u2019s limitation in terms of the level of question difficulty. One participant points out that the questions appear to be at a low Bloom\u2019s Taxonomy level. For example, \u201cIf  is read as \u2018a cubed\u2019, how is  read?\u201d While it\u2019s important for students to grasp the verbal representation of these terms, educators often place greater emphasis on whether students understand the equivalent expressions and concepts associated with them. The other participant points out that the Depth of Knowledge (DOK) levels predominantly focused on Level 1 (Recall) and Level 2 (Skill or Concept). We can prompt LLMs to generate questions at various Bloom\u2019s or DOK levels to enhance the question difficulty and promote deeper understanding [3  ###reference_b3###]. Moreover, we can invite educators to craft in-context examples with higher Bloom\u2019s or DOK levels.\nOn a 5-point Likert scale, the participants gave an average rating of 2.5. This rating aligns with the open-ended responses regarding most of the generated misconceptions, distractors, and feedback that do not reflect what students typically make in the classroom based on the participant\u2019s teaching experience. The responses again point to the observation that LLMs do not understand errors that student are likely to make. One participant suggest providing a \u201cbank\u201d of misconceptions that educators could refer to. We can prompt LLMs to generate multiple misconceptions and engage educators in ranking these misconceptions based on their alignment with actual student errors.\nOn a 5-point Likert scale, the participants gave an average rating of 4 for comfort level with generating MCQs using HEDGE while giving an average rating of 3.25 for the effectiveness of generating high-quality MCQs. Participants are enthusiastic about the tool\u2019s potential for simplifying the process of generating MCQs but are nevertheless skeptical about LLMs\u2019 capability to generate valid distractors. We will need to enhance the tool by making improvements in the quality of generated distractors to align more closely with educators\u2019 expectations."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Stem, Key, and Explanation",
            "text": "Table 3  ###reference_### shows the stems produced by participants utilizing HEDGE. In the \u201cFine-grained KC\u201d column, the original stem is indicated in italics, while the stems modified by each participant denoted as , , , and , respectively. In what follows, we label each MCQ in the format of , where  denotes the index of the fine-grained KC and  denotes index of the participant.\nOut of 20 sets of stem, key, and explanation generated by the LLM, participants deemed 14 sets of them as valid. Among these valid sets, two added more details in their explanations, while the remaining sets were adopted without any need for edits. For example, italicized details were added in the explanation for : \u201cThe fraction  simplifies to  because both the numerator and the denominator can be divided by a common factor of 3. 3 divided by 3 is 1, and 9 divided by 3 is 3. Hence,  is an equivalent fraction to .\u201d The other case was to make the question setting more realistic: In , the educator edited the initial price of the car worth $5000 to $35000. This adjustment reveals the limitations of LLMs in accurately representing real-life problem scenario. We now analyze the cases that participants deemed invalid.\nIn , educator corrected grammar error of \u201cshe have\u201d to \u201cshe has.\u201d No other grammar errors occurred in the study besides this one, underscoring the capability of LLMs to consistently produce grammatically correct sentences.\nRegarding 5th KC, GPT-4 shows a lack of knowledge on the distinction between simplified and non-simplified surd. The followings are invalid stems generated by GPT-4: 1) . If  is a simplified surd, what is its non-simplified form? 2) . Express the simplified surd  in a non-simplified form. 3) . A simplified surd is . How can it be represented in non-simplified form?\nThis invalid stem has misled a participant to edit a stem to convey KC as simplifying surd, which is the opposite of non-simplifying surd ().\nIn , GPT-4 generated a key of $4750, erroneously calculating the car price after one year instead of two years. However, in the other three cases within the same KC, GPT-4 calculated correctly, showing its math problem-solving skills."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Takeaways from the Survey",
            "text": "After the study, participants were asked to fill out the survey, asking the experience using HEDGE. We categorize result into two: Quality of LLM-generated responses and Tool Usability.\nOn a 5-point Likert scale, the participants gave an average rating of 4. This rating aligns with the open-ended responses regarding most of the generated stem, key, and explanation valid. However, two participants addressed the tool\u2019s limitation in terms of the level of question difficulty. One participant points out that the questions appear to be at a low Bloom\u2019s Taxonomy level. For example, \u201cIf  is read as \u2018a cubed\u2019, how is  read?\u201d While it\u2019s important for students to grasp the verbal representation of these terms, educators often place greater emphasis on whether students understand the equivalent expressions and concepts associated with them. The other participant points out that the Depth of Knowledge (DOK) levels predominantly focused on Level 1 (Recall) and Level 2 (Skill or Concept). We can prompt LLMs to generate questions at various Bloom\u2019s or DOK levels to enhance the question difficulty and promote deeper understanding [3  ###reference_b3###  ###reference_b3###]. Moreover, we can invite educators to craft in-context examples with higher Bloom\u2019s or DOK levels.\nOn a 5-point Likert scale, the participants gave an average rating of 2.5. This rating aligns with the open-ended responses regarding most of the generated misconceptions, distractors, and feedback that do not reflect what students typically make in the classroom based on the participant\u2019s teaching experience. The responses again point to the observation that LLMs do not understand errors that student are likely to make. One participant suggest providing a \u201cbank\u201d of misconceptions that educators could refer to. We can prompt LLMs to generate multiple misconceptions and engage educators in ranking these misconceptions based on their alignment with actual student errors.\nOn a 5-point Likert scale, the participants gave an average rating of 4 for comfort level with generating MCQs using HEDGE while giving an average rating of 3.25 for the effectiveness of generating high-quality MCQs. Participants are enthusiastic about the tool\u2019s potential for simplifying the process of generating MCQs but are nevertheless skeptical about LLMs\u2019 capability to generate valid distractors. We will need to enhance the tool by making improvements in the quality of generated distractors to align more closely with educators\u2019 expectations."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Quality of LLM-generated responses.",
            "text": "On a 5-point Likert scale, the participants gave an average rating of 4. This rating aligns with the open-ended responses regarding most of the generated stem, key, and explanation valid. However, two participants addressed the tool\u2019s limitation in terms of the level of question difficulty. One participant points out that the questions appear to be at a low Bloom\u2019s Taxonomy level. For example, \u201cIf  is read as \u2018a cubed\u2019, how is  read?\u201d While it\u2019s important for students to grasp the verbal representation of these terms, educators often place greater emphasis on whether students understand the equivalent expressions and concepts associated with them. The other participant points out that the Depth of Knowledge (DOK) levels predominantly focused on Level 1 (Recall) and Level 2 (Skill or Concept). We can prompt LLMs to generate questions at various Bloom\u2019s or DOK levels to enhance the question difficulty and promote deeper understanding [3  ###reference_b3###  ###reference_b3###  ###reference_b3###]. Moreover, we can invite educators to craft in-context examples with higher Bloom\u2019s or DOK levels.\nOn a 5-point Likert scale, the participants gave an average rating of 2.5. This rating aligns with the open-ended responses regarding most of the generated misconceptions, distractors, and feedback that do not reflect what students typically make in the classroom based on the participant\u2019s teaching experience. The responses again point to the observation that LLMs do not understand errors that student are likely to make. One participant suggest providing a \u201cbank\u201d of misconceptions that educators could refer to. We can prompt LLMs to generate multiple misconceptions and engage educators in ranking these misconceptions based on their alignment with actual student errors."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Tool Usability",
            "text": "On a 5-point Likert scale, the participants gave an average rating of 4 for comfort level with generating MCQs using HEDGE while giving an average rating of 3.25 for the effectiveness of generating high-quality MCQs. Participants are enthusiastic about the tool\u2019s potential for simplifying the process of generating MCQs but are nevertheless skeptical about LLMs\u2019 capability to generate valid distractors. We will need to enhance the tool by making improvements in the quality of generated distractors to align more closely with educators\u2019 expectations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "In this paper, we conducted a pilot study using a prototype tool HEDGE to explore the opportunity for collaboration between LLMs and humans in generating math MCQs. We identified that while LLMs can generate valid stems, keys, and explanations, they are currently limited in capturing anticipated student errors, which is reflected in invalid misconceptions, distractors, and feedback.\nThis study opens up many avenues for future work. First, we can extend the prompt with more in-context examples. Currently, we use only one in-context example; using multiple in-context examples can help guide LLMs to help capture valid misconceptions for the target stem. As mentioned in our survey takeaways, we can also add in-context examples with different Bloom\u2019s taxonomy and difficulty levels to enhance the diversity of the generated questions.\nWe could also use techniques for optimally selecting these in-context examples [10  ###reference_b10###].\nSecond, we can change the interface to choose generated distractors from a bank that contains more than three distractors.\nWhen building the bank, we can employ a -nearest neighbor approach that gauges question similarity and leverage LLMs to generate distractors [4  ###reference_b4###].\nEducators will have less burden on thinking of anticipated misconceptions and even benefit from discovering misconceptions they might have overlooked. Third, improving this tool as a platform so that educators can share their misconceptions will result in a constantly-expanding error bank, which will benefit future MCQ generation.\nFourth, we can provide educators choice to create personalized questions by adding a module to customize the named entity or topic (e.g., sports, popular culture) in an MCQ to stimulate student interest and make the question more culturally relevant [15  ###reference_b15###].\nLastly, we can extend the area of Human-LLM collaboration to other domains beyond math MCQ generation, such as feedback generation [11  ###reference_b11###], question generation [7  ###reference_b7###], and programming education [1  ###reference_b1###]."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "We thank Schmidt Futures and the NSF (under grants IIS-2118706 and IIS-2237676) for partially supporting this work."
        }
    ],
    "url": "http://arxiv.org/html/2405.00864v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.1",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.1",
            "4.3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "3.2",
            "3.1",
            "4.1",
            "4.2",
            "4.3",
            "4.3.1",
            "4.3.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.00864v1",
        "paper_title": "Math Multiple Choice Question Generation via Human-Large Language Model Collaboration",
        "research_background": "### Motivation\n\nThe paper is motivated by the challenges and laborious nature of manually creating high-quality multiple choice questions (MCQs), especially in the domain of mathematics. MCQs are widely favored for student assessments because they allow for quick and accurate grading. However, the creation process, which involves formulating the question stem, generating relevant distractors that reflect common student errors, and providing feedback, is demanding for educators. With the advent of large language models (LLMs), there's an opportunity to make the creation of MCQs more scalable and efficient by automating parts of this process. The integration of LLMs promises to lessen the burden on educators, provided that the models can generate appropriate distractors and feedback based on student misconceptions, an area where LLMs might fall short due to the need for deep mathematical reasoning.\n\n### Research Problem\n\nThe fundamental research problem the paper addresses is whether collaboration between LLMs and human educators can effectively generate valid math MCQs, focusing particularly on the creation of plausible distractors that reflect common student misconceptions and providing corrective feedback. This problem is broken down into two core research questions:\n\n1. **Can LLMs generate valid MCQs, particularly with appropriate distractors and feedback corresponding to common student errors/misconceptions?**\n2. **What are the key design elements in a system where human math educators and LLMs collaborate on MCQ generation?**\n\n### Relevant Prior Work\n\nThe paper builds on several streams of prior work:\n\n1. **Use of MCQs in Student Assessment:** MCQs are a popular method for student evaluation due to their ease of administration and grading [2, 6, 9]. They typically include a stem, a key (correct answer), and distractors (incorrect answers that reflect common errors).\n   \n2. **Challenges in MCQ Creation:** Manually crafting high-quality MCQs is particularly labor-intensive in math domains [5], requiring educators to identify key knowledge components and generate effective distractors that anticipate student misconceptions.\n   \n3. **Large Language Models (LLMs):** The emergence of LLMs has generated optimism for scalable MCQ creation via few-shot, in-context learning. While LLMs have been successful in generating open-ended questions, the challenge remains in generating distractors aligned with student errors/misconceptions [12].\n\n4. **Limitations of Existing Tools:** Unlike question generation for other domains, math MCQs require mathematical reasoning, and thus, knowledge graphs [13] and paraphrasing tools [8] are insufficient for generating plausible math distractors.\n\nBy addressing these areas, the research aims to leverage the strengths of both LLMs and human expertise to improve the efficiency and quality of math MCQ generation.",
        "methodology": "The methodology for the proposed math multiple-choice question (MCQ) generation model, HEDGE, revolves around a collaborative approach leveraging the capabilities of large language models (LLMs) hosted by OpenAI, particularly ChatGPT. This model is designed to generate MCQs tailored to various levels of mathematical knowledge components (KCs), ensuring a range from broad topics to very specific subtopics.\n\nKey Components and Workflow:\n\n1. **Knowledge Components (KCs)**: HEDGE addresses KCs across three granularity levels:\n   - **Coarse-grained**: High-level topics such as \"basic arithmetic.\"\n   - **Medium-grained**: Detailed topics within the broad category.\n   - **Fine-grained**: Highly specific concepts, e.g., \"Identify that a problem needs to be solved using addition.\"\n\n2. **Two-step MCQ Generation Process**:\n   - **Step 1**: Generate the question stem, the correct answer (key), and an explanation. \n   - **Step 2**: Generate a set of possible misconceptions, distractor options, and feedback messages tailored to each misconception.\n\n3. **In-Context Learning**: Both steps utilize in-context examples to guide LLM outputs. An example is provided for converting ratios to fractions using real-life scenarios, which includes:\n   - **Sample Stem**: Involves a ratio problem where participants must calculate the fraction of a share.\n   - **Misconceptions and Feedback**: Common misunderstandings and corrective feedback, such as incorrect direct conversion of ratios to fractions, subtraction errors, and confusion about whose share to calculate.\n\n4. **Manual Educator Adjustments**:\n   - **Step 1 Output Refinement**: Educators review and edit the LLM-generated stems to ensure correctness and alignment with the targeted KC. \n   - **Step 2 Adjustment**: Using the refined stem and key along with in-context examples of distractors, the LLM produces distractor options along with misconceptions and feedback. Educators then review and can further edit these outputs to address additional misconceptions.\n\nExample Scenario:\n- **Coarse-grained KC Target**: Basic arithmetic focusing on mental addition.\n- **Initial LLM Generation**: Example question stem: \"Sally has 5 apples. She gives 2 apples to her friend. How many apples does Sally have left?\" mistakenly aligned with subtraction.\n- **Educator's Role**: Corrects the stem to fit addition.\n- **Further LLM Utilization**: Generates distractors, identifies misconceptions, and associates feedback, which are then subjected to additional educator refinements to correct any non-alignment with the intended KC.\n\nInnovations:\n- **Interactive Model-Educator Collaboration**: Combines AI-driven generation with human expert adjustment to improve accuracy and relevance.\n- **Customizable Feedback and Misconception Handling**: Provides tailored feedback for each distractor, ensuring thorough understanding and correction of student errors.\n- **Multi-granularity KC Coverage**: Flexibly handles diverse question complexities from basic arithmetic to nuanced mathematical concepts.\n\nOverall, HEDGE exemplifies an advanced collaborative AI tool designed to enhance educational content creation effectively, providing both robust automated generation and necessary human oversight.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n- **Purpose:** The main experiment aims to evaluate the usability of HEDGE in generating multiple-choice questions (MCQs) using human collaboration with a Large Language Model (LLM).\n  \n- **Dataset and Context:** \n  - **Pre-defined Knowledge Components (KCs):** Selected from a large education company's content repository under the category \"Number,\" which includes subtopics like \"Basic Arithmetic,\" \"Fractions,\" and \"Rounding and Estimating.\"\n  - **Specific KCs:** Five KCs, detailed in Table 2 (reference excluded for this description), that involve mathematical expressions such as fractions, powers, and surds.\n\n- **LLM Used:** GPT-4.\n  - **Parameters:** Adjusted settings for temperature and top_p value to balance creativity and consistency in the generated MCQs.\n\n- **Participants' Task:** Participants are instructed to use the pre-selected KCs to craft MCQs, simulating an educator's task.\n\n- **Evaluation Method:** \n  - **Exit Survey:** Conducted post-experiment, including:\n    - **Open-Ended Questions:** To gather qualitative feedback.\n    - **Ratings:** Participants rate their satisfaction with the quality of LLM-generated responses and the overall usability of the tool on a 5-point Likert scale.\n\n**Main Experimental Results:**\n\nUnfortunately, the provided text does not include specific results from the main experiment. For comprehensive understanding, additional information on survey outcomes, satisfaction scores, or any quantitative/qualitative findings would be necessary to elaborate on the main results."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The study aims to evaluate the usability of HEDGE, a tool designed for human-Large Language Model (LLM) collaboration in generating high-quality math Multiple Choice Questions (MCQs), focusing on the quality of the generated stems, keys, explanations, and distractors.",
            "experiment_process": "The researchers performed a pilot study involving four math educators who used HEDGE to generate MCQs for predefined Knowledge Components (KCs). The KCs and in-context examples were sourced from a large education company's content repository, covering subtopics like Basic Arithmetic, Fractions, and Rounding. The participants, two middle/high school teachers and two tutors, each created five MCQs using the tool. They used GPT-4, with parameters balanced for creativity and consistency. An exit survey evaluated the quality and usability of the tool via open-ended questions and a 5-point Likert scale.",
            "result_discussion": "Participants identified 14 out of 20 sets of stems, keys, and explanations as valid. They acknowledged the tool's ability to generate grammatically correct sentences, except for minor errors. However, the tool struggled with mathematical consistency and creating valid distractors. GPT-4 occasionally generated invalid stems and misaligned misconceptions and distractors. Educators frequently edited these elements for coherence. On a Likert scale, participants rated the tool's quality of generated responses at 4 but highlighted issues with low Bloom\u2019s Taxonomy levels and Depth of Knowledge (DOK). They rated distractor validity and alignment at 2.5, pointing out GPT-4's failure to predict actual student errors. Educators suggested enhancements, including a 'bank' of misconceptions and variability in question complexity.",
            "ablation_id": "2405.00864v1.No1"
        },
        {
            "research_objective": "The goal is to assess the capability of GPT-4 in generating valid distractors, misconceptions, and corresponding feedback for math MCQs, highlighting areas where human intervention is critical.",
            "experiment_process": "Educators used HEDGE to generate 60 distractors for 20 stems, analyzing the validity of the misconceptions, distractors, and feedback provided by GPT-4. Participants provided adjustments and modifications to align generated content with common student errors. Invalid distractors and misconceptions were documented, and feedback was refined based on participants' teaching experience.",
            "result_discussion": "Out of 60 distractors, 22 were identified as valid, although two supposedly valid cases were deemed incorrect upon deeper analysis. GPT-4 often mismatched misconceptions with distractors, requiring educators to align them manually. Examples included misconceptions about mathematical operations (e.g., simplifying fractions). The tool revealed inconsistencies in mathematical reasoning, underlining the necessity for human oversight in aligning misconceptions, distractors, and feedback. Suggestions for improvement included creating a misconception bank and generating multiple misconceptions for ranking to better align with student errors.",
            "ablation_id": "2405.00864v1.No2"
        },
        {
            "research_objective": "To measure user satisfaction and effectiveness of HEDGE in generating MCQs, with a specific focus on the tool's usability and practical application.",
            "experiment_process": "Participants completed a survey rating their satisfaction with the generated responses and the tool's usability on a 5-point Likert scale. The survey was divided into two parts: Quality of LLM-generated responses and Tool Usability, with ratings and open-ended feedback collected for both aspects.",
            "result_discussion": "Participants gave a rating of 4 for overall satisfaction with the generated responses, noting strengths in the validity of stems, keys, and explanations. However, issues with question difficulty levels were noted, suggesting the need for more complex questions. The average rating for misconceptions, distractors, and feedback effectiveness was 2.5, indicating the tool's challenges in generating valid and coherent distractors. Participants were generally comfortable using HEDGE (rating of 4) but were skeptical about the LLM\u2019s ability to generate high-quality MCQs (rating of 3.25). They recommended improvements in distractor quality to better meet educators\u2019 needs.",
            "ablation_id": "2405.00864v1.No3"
        }
    ]
}