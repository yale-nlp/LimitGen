{
    "title": "Crafting Interpretable Embeddings by Asking LLMs Questions",
    "abstract": "Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Text embeddings are\ncritical to\nmany applications, including\ninformation retrieval,\nsemantic clustering,\nretrieval-augmented generation,\nand language neuroscience.\nTraditionally, text embeddings leveraged interpretable representations such as bag-of-words or BM-25 [1  ###reference_b1###].\nModern methods often replace these embeddings with representations from large language models (LLMs), which may better capture nuanced contexts and interactions [2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###, 7  ###reference_b7###].\nHowever, these embeddings are essentially black-box representations, making it difficult to understand the predictive models built on top of them (as well as why they judge different texts to be similar in a retrieval context).\nThis opaqueness is detrimental in scientific fields, such as neuroscience [8  ###reference_b8###] or social science [9  ###reference_b9###], where trustworthy interpretation itself is the end goal.\nMoreover, this opaqueness has debilitated the use of LLM embeddings (for prediction or retrieval) in high-stakes applications such as medicine [10  ###reference_b10###],\nand raised issues related to\nregulatory pressure, safety, and alignment [11  ###reference_b11###, 12  ###reference_b12###, 13  ###reference_b13###, 14  ###reference_b14###].\nTo ameliorate these issues, we introduce question-answering embeddings (QA-Emb), a method that builds an interpretable embedding by repeatedly querying a pre-trained autoregressive LLM with a set of questions that are selected for a problem (Fig. 1  ###reference_###).\nEach element of the embedding represents the answer to a different question asked to an LLM, making the embedding human-inspectable.\nFor example, the first element may be the answer to the question Does the input mention time? and the output would map yes/no to 1/0.\nTraining QA-Emb requires only black-box access to the LLM (it does not require access to the LLM internals) and modifies only natural-language prompts, rather than LLM parameters.\nThe learning problem is similar to the optimization faced in natural-language autoprompting [15  ###reference_b15###, 16  ###reference_b16###] or single-neuron explanation [17  ###reference_b17###, 18  ###reference_b18###], but seeks a set of questions rather than an individual prompt.\nWe focus on a single neuroscience problem in close collaboration with neuroscientists.\nGrounding in a neuroscience context allows us to avoid common pitfalls in evaluating interpretation methods [19  ###reference_b19###, 20  ###reference_b20###] that seek to test \u201cinterpretability\u201d generally.\nAdditionally, this focus allows to more realistically integrate domain knowledge to select and evaluate the questions needed for QA-Emb, one of its core strengths.\nNevertheless, QA-Emb may be generally applicable in other domains where it is important to meaningfully interpret text embeddings.\nIn our neuroscience setting,\nwe build QA-Emb representations from natural-language questions that can predict human brain responses measured by fMRI to natural-language stimuli.\nThis allows for converting informal verbal hypotheses about the semantic selectivity of the brain\ninto quantitative models, a pressing challenge in fields such as psychology [21  ###reference_b21###].\nWe find that predictive models built on top of QA-Embs are quite accurate, providing a 26% improvement over an established interpretable baseline [22  ###reference_b22###] and even slightly outperforming a black-box BERT baseline [23  ###reference_b23###].\nAdditionally, QA-Emb yields concise embeddings, outperforming the interpretable baseline (that consists of 985 features) with only 29 questions.\nWe investigate two major limitations of QA-Emb in Sec. 5  ###reference_###.\nFirst, with regards to computational efficiency,\nwe find that we can drastically reduce the computational cost of QA-Emb by distilling it into a model that computes the answers to all selected questions in a single feedforward pass by using many classification heads.\nSecond, we evaluate the accuracy of modern LLMs at reliably answering diverse yes/no questions.\nFinally, Sec. 6  ###reference_### explores broader applications for QA-Emb in a simple information retrieval setting and text-clustering setting.\n\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "QA-Emb is an intuitive method to generate text embeddings from a pre-trained autoregressive LLM (Fig. 1  ###reference_###).\nGiven a text input, QA-Emb builds an interpretable embedding by querying the LLM with a set of questions about the input.\nEach element of the embedding represents the answer to a different question asked to an LLM.\nThis procedure allows QA-Emb to capture nuanced and relevant details in the input while staying interpretable.\nQA-Emb requires specifying a set of yes/no questions  that yield a binary embedding  for an input string .\nThe questions are chosen to yield embeddings that are suitable for a downstream task.\nIn our fMRI prediction task, we optimize for supervised linear regression:\ngiven a list of  input strings  and a multi-dimensional continuous output , we seek embeddings that allow for learning effective ridge regression models:\nwhere  is a learned coefficient vector for predicting the fMRI responses and  is the ridge regularization parameter.\nDirectly optimizing over the space of yes/no questions is difficult, as it requires searching over a discrete space with a constraint set  that is hard to specify.\nInstead, we heuristically optimize the set of questions , by prompting a highly capable LLM (e.g. GPT-4 [24  ###reference_b24###]) to generate questions relevant to our task, e.g. Generate a bulleted list of questions with yes/no answers that is relevant for {{task description}}.\nCustomizing the task description helps yield relevant questions.\nThe prompt can flexibly specify more prior information when available.\nFor example, it can include examples from the input dataset to help the LLM identify data-relevant questions.\nTaking this a step further,\nquestions can be generated sequentially (similar to gradient boosting) by having the LLM summarize input examples that incur high prediction error to generate new questions focused on those examples.\nWhile we focus on optimizing embeddings for fMRI ridge regression in Eq. 1  ###reference_###, different downstream tasks may require different inner optimization procedures, e.g. maximizing the similarity of relevant documents for retrieval.\nThe set of learned questions  can be easily pruned to be made compact and useful in different settings.\nFor example, in our fMRI regression setting, a feature-selection procedure such as Elastic net [25  ###reference_b25###] can be used to remove redundant/uninformative questions from the specified set of questions .\nAlternatively, an LLM can be used to directly adapt  to yield task-specific embeddings.\nSince the questions are all in natural language, they can be listed in a prompt, and an LLM can be asked to filter the task-relevant ones, e.g. Here is a list of questions:{{question list}} List the subset of these questions that are relevant for {{task description}}.\nWhile effective, the QA-Emb pipeline described here has two major limitations.\nFirst, QA-Emb is computationally intensive, requiring  LLM calls to compute an embedding.\nThis is often prohibitively expensive, but may be worthwhile in high-value applications (such as our fMRI setting) and will likely become more tenable as LLM inference costs continue to rapidly decrease.\nWe find that we can dramatically reduce this cost by distilling the QA-Emb model into a single LLM model with many classification heads in Sec. 5.1  ###reference_###.\nOtherwise, LLM inference costs are partially mitigated by the ability to reuse the KV-cache for each question and the need to only generate a single token for each question.\nWhile computing embeddings with QA-Emb is expensive,\nsearching embeddings is made faster by the fact that the resulting embeddings are binary and often relatively compact.\nSecond, QA-Emb requires that the pre-trained LLM can faithfully answer the given yes-no questions.\nIf an LLM is unable to accurately answer the questions, it hurts explanation\u2019s faithfulness.\nThus, QA-Emb requires the use of fairly strong LLMs and the set of chosen questions should be accurately answered by these LLMs (Sec. 5.2  ###reference_### provides analysis on the question-answering accuracy of different LLMs).\nFor answering questions, we average the answers from Mistral-7B [26  ###reference_b26###] (mistralai/Mistral-7B-Instruct-v0.2) and LLaMA-3 8B [27  ###reference_b27###] (meta-llama/Meta-Llama-3-8B-Instruct) with two prompts.\nAll perform similarly and averaging their answers yields a small performance improvement (Table A2  ###reference_###).\nFor generating questions, we prompt GPT-4 [24  ###reference_b24###] (gpt-4-0125-preview).\nExperiments were run using 64 AMD MI210 GPUs, each with 64 gigabytes of memory,\nand reproducing all experiments in the paper requires approximately 4 days (initial explorations required roughly 5 times this amount of compute).\nAll prompts used and generated questions are given in the appendix or on Github."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "Text embeddings models, which produce vector representations of document inputs, have been foundational to NLP.\nRecently, transformer-based models have been trained to yield embeddings in a variety of ways [2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###, 7  ###reference_b7###], including producing embeddings that are sparse [28  ###reference_b28###] or have variable lengths [29  ###reference_b29###].\nRecent works have also leveraged autoregressive LLMs to build embeddings,\ne.g. by repeating embeddings [30  ###reference_b30###], generating synthetic data [6  ###reference_b6###, 31  ###reference_b31###],\nor using the last-token distribution of an autoregressive LLM as an embedding [32  ###reference_b32###].\nSimilar to QA-Emb, various works have used LLM answers to multiple prompts for different purposes, e.g. text classification [33  ###reference_b33###, 34  ###reference_b34###] or data exploration [35  ###reference_b35###].\nA few works have focused on building intrinsically interpretable text representations, e.g. word or ngram-based embeddings such as\nword2vec [36  ###reference_b36###], Glove [37  ###reference_b37###], and LLM word embeddings.\nAlthough their dimensions are not natively interpretable,\nfor some tasks, such as classification, they can be projected into a space that is interpretable [38  ###reference_b38###], i.e. a word-level representation.\nNote that it is difficult to learn a sparse interpretable model from these dense embeddings, as standard techniques (e.g. Elastic net) cannot be directly applied.\nWhen instead using black-box representations, there are many post-hoc methods to interpret embeddings,\ne.g. probing [39  ###reference_b39###, 40  ###reference_b40###],\ncategorizing elements into categories [41  ###reference_b41###, 42  ###reference_b42###, 43  ###reference_b43###, 44  ###reference_b44###],\ncategorizing directions in representation space [45  ###reference_b45###, 46  ###reference_b46###, 47  ###reference_b47###],\nor connecting multimodal embeddings with text embeddings/text concepts [48  ###reference_b48###, 49  ###reference_b49###, 50  ###reference_b50###, 51  ###reference_b51###, 52  ###reference_b52###].\nFor a single pair of text embeddings,\nprediction-level methods can be applied to approximately explain why the two embeddings are similar [53  ###reference_b53###, 54  ###reference_b54###].\nUsing LLM representations to help predict brain responses to natural language has recently become popular among neuroscientists studying language processing [55  ###reference_b55###, 56  ###reference_b56###, 57  ###reference_b57###, 58  ###reference_b58###, 59  ###reference_b59###, 60  ###reference_b60###] (see [61  ###reference_b61###, 62  ###reference_b62###] for reviews).\nThis paradigm of using \u201cencoding models\u201d [63  ###reference_b63###] to better understand how the brain processes language has been applied to help understand the cortical organization of language timescales [64  ###reference_b64###, 65  ###reference_b65###], examine the relationship between visual and semantic information in the brain [66  ###reference_b66###], and explore to what extent syntax, semantics, or discourse drives brain activity [22  ###reference_b22###, 67  ###reference_b67###, 68  ###reference_b68###, 69  ###reference_b69###, 70  ###reference_b70###, 71  ###reference_b71###, 72  ###reference_b72###, 73  ###reference_b73###, 18  ###reference_b18###].\nThe approach here extends these works to build an increasingly flexible, interpretable feature space for modeling fMRI responses to text data."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Main results: fMRI interpretation",
            "text": "A central challenge in neuroscience is understanding how and where semantic concepts are represented in the brain.\nTo meet this challenge, we extend the line of study that fits models to predict the response of different brain voxels (i.e. small regions in the brain) to natural language stimuli.\nUsing QA-Emb, we seek to bridge models that are interpretable [1  ###reference_b1###, 22  ###reference_b22###]\nwith more recent LLM models that are accurate but opaque [55  ###reference_b55###, 56  ###reference_b56###, 57  ###reference_b57###].\nWe analyze data from two recent studies [74  ###reference_b74###, 75  ###reference_b75###] (released under the MIT license), which contain fMRI responses for 3 human subjects listening to 20+ hours of narrative stories from podcasts.\nWe extract text embeddings from the story that each subject hears and fit a ridge regression to predict the fMRI responses (Eq. 1  ###reference_###).\nEach subject listens to either 79 or 82 stories (consisting of 27,449 time points) and 2 test stories (639 time points);\nEach subject\u2019s fMRI data consists of approximately 100,000 voxels; we preprocess it by running principal component analysis (PCA) and extracting the coefficients of the top 100 components.\nWe fit ridge regression models to predict these 100 coefficients and evaluate the models in the original voxel space (by applying the inverse PCA mapping and measuring the correlation between the response and prediction for each voxel).\nWe deal with temporal sampling following [22  ###reference_b22###, 57  ###reference_b57###]; an embedding is produced at the timepoint for each word in the input story and these embeddings are interpolated using Lanczos resampling.\nEmbeddings at each timepoint are produced from the ngram consisting of the 10 words preceding the current timepoint.\nWe select the best-performing hyperparameters via cross-validation on 5 time-stratified bootstrap samples of the training set.\nWe select the best ridge parameters from 12 logarithmically spaced values between 10 and 10,000.\nTo model temporal delays in the fMRI signal, we also select between adding 4, 8, or 12 time-lagged duplicates of the stimulus features.\nTo generate the questions underlying QA-Emb, we prompt GPT-4 with 6 prompts that aim to elicit knowledge useful for predicting fMRI responses (precise prompts in Sec. A.3  ###reference_###).\nThis includes directly asking the LLM to use its knowledge of neuroscience,\nto brainstorm semantic properties of narrative sentences,\nto summarize examples from the input data,\nand to generate questions similar to single-voxel explanations found in a prior work [18  ###reference_b18###].\nThis process yields 674 questions (Fig. 1  ###reference_### and Table A1  ###reference_### show examples, see all questions on Github).\nWe perform feature selection by running multi-task Elastic net with 20 logarithmically spaced regularization parameters ranging from  to 1 and then fit a Ridge regression to the selected features.222We run Elastic net using the MultiTaskElasticNet class from scikit-learn [76  ###reference_b76###].\nSee extended details on the fMRI experimental setup in Sec. A.1  ###reference_### and all prompts in Sec. A.3  ###reference_###.\nWe compare QA-Emb to Eng1000, an interpretable baseline developed in the neuroscience literature specifically for the task of predicting fMRI responses from narrative stories [22  ###reference_b22###].\nEach element in an Eng1000 embedding corresponds to a cooccurence statistic with a different word, allowing full interpretation of the underlying representation in terms of related words.\nWe additionally compare to embeddings from BERT [23  ###reference_b23###] (bert-base-uncased) and LLaMA models [77  ###reference_b77###, 27  ###reference_b27###].\nFor each subject, we sweep over 5 layers from LLaMA-2 7B (meta-llama/Llama-2-7b-hf, layers 6, 12, 18, 24, 30), LLaMA-2 70B (meta-llama/Llama-2-70b-hf, layers 12, 24, 36, 48, 60), and LLaMA-3 8B (meta-llama/Meta-Llama-3-8B, layers 6, 12, 18, 24, 30), then report the test performance for the model that yields the best cross-validated accuracy (see breakdown in Table A3  ###reference_###)."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "fMRI experimental setup",
            "text": "We analyze data from two recent studies [74  ###reference_b74###  ###reference_b74###, 75  ###reference_b75###  ###reference_b75###] (released under the MIT license), which contain fMRI responses for 3 human subjects listening to 20+ hours of narrative stories from podcasts.\nWe extract text embeddings from the story that each subject hears and fit a ridge regression to predict the fMRI responses (Eq. 1  ###reference_###  ###reference_###).\nEach subject listens to either 79 or 82 stories (consisting of 27,449 time points) and 2 test stories (639 time points);\nEach subject\u2019s fMRI data consists of approximately 100,000 voxels; we preprocess it by running principal component analysis (PCA) and extracting the coefficients of the top 100 components.\nWe fit ridge regression models to predict these 100 coefficients and evaluate the models in the original voxel space (by applying the inverse PCA mapping and measuring the correlation between the response and prediction for each voxel).\nWe deal with temporal sampling following [22  ###reference_b22###  ###reference_b22###, 57  ###reference_b57###  ###reference_b57###]; an embedding is produced at the timepoint for each word in the input story and these embeddings are interpolated using Lanczos resampling.\nEmbeddings at each timepoint are produced from the ngram consisting of the 10 words preceding the current timepoint.\nWe select the best-performing hyperparameters via cross-validation on 5 time-stratified bootstrap samples of the training set.\nWe select the best ridge parameters from 12 logarithmically spaced values between 10 and 10,000.\nTo model temporal delays in the fMRI signal, we also select between adding 4, 8, or 12 time-lagged duplicates of the stimulus features.\nTo generate the questions underlying QA-Emb, we prompt GPT-4 with 6 prompts that aim to elicit knowledge useful for predicting fMRI responses (precise prompts in Sec. A.3  ###reference_###  ###reference_###).\nThis includes directly asking the LLM to use its knowledge of neuroscience,\nto brainstorm semantic properties of narrative sentences,\nto summarize examples from the input data,\nand to generate questions similar to single-voxel explanations found in a prior work [18  ###reference_b18###  ###reference_b18###].\nThis process yields 674 questions (Fig. 1  ###reference_###  ###reference_### and Table A1  ###reference_###  ###reference_### show examples, see all questions on Github).\nWe perform feature selection by running multi-task Elastic net with 20 logarithmically spaced regularization parameters ranging from  to 1 and then fit a Ridge regression to the selected features.222We run Elastic net using the MultiTaskElasticNet class from scikit-learn [76  ###reference_b76###  ###reference_b76###].\nSee extended details on the fMRI experimental setup in Sec. A.1  ###reference_###  ###reference_### and all prompts in Sec. A.3  ###reference_###  ###reference_###.\nWe compare QA-Emb to Eng1000, an interpretable baseline developed in the neuroscience literature specifically for the task of predicting fMRI responses from narrative stories [22  ###reference_b22###  ###reference_b22###].\nEach element in an Eng1000 embedding corresponds to a cooccurence statistic with a different word, allowing full interpretation of the underlying representation in terms of related words.\nWe additionally compare to embeddings from BERT [23  ###reference_b23###  ###reference_b23###] (bert-base-uncased) and LLaMA models [77  ###reference_b77###  ###reference_b77###, 27  ###reference_b27###  ###reference_b27###].\nFor each subject, we sweep over 5 layers from LLaMA-2 7B (meta-llama/Llama-2-7b-hf, layers 6, 12, 18, 24, 30), LLaMA-2 70B (meta-llama/Llama-2-70b-hf, layers 12, 24, 36, 48, 60), and LLaMA-3 8B (meta-llama/Meta-Llama-3-8B, layers 6, 12, 18, 24, 30), then report the test performance for the model that yields the best cross-validated accuracy (see breakdown in Table A3  ###reference_###  ###reference_###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "fMRI predictive performance",
            "text": "###figure_2### ###figure_3### We find that QA-Emb predicts fMRI responses fairly well across subjects (Fig. 2  ###reference_###A), achieving an average test correlation of 0.116.\nQA-Emb significantly outperforms the interpretable baseline Eng1000 (26% average improvement).\nComparing to the two transformer-based baselines (which do not yield straightforward interpretations),\nwe find that QA-Emb slightly outperforms BERT (5% improvement) and worse than the best cross-validated LLaMA-based model (7% decrease).\nTrends are consistent across all 3 subjects.\nTo yield a compact and interpretable model, Fig. 2  ###reference_###B further investigates the compressibility of the two interpretable methods (through Elastic net regularization).\nCompared to Eng1000,\nQA-Emb improves performance very quickly as a function of the number of features included, even outperforming the final Eng1000 performance with only 29 questions (mean test correlation 0.122 versus 0.118).\nTable A1  ###reference_### shows the 29 selected questions, which constitute a human-readable description of the entire model.\nFig. 2  ###reference_###C-D further break down the predictive performance across different brain regions for a particular subject (S03).\nThe regions that are well-predicted by QA-Emb (Fig. 2  ###reference_###C) align with language-specific areas that are seen in the literature [56  ###reference_b56###, 78  ###reference_b78###].\nThey do not show any major diversions from transformer-based encoding models (Fig. 2  ###reference_###D), with the distribution of differences being inconsistent across subjects (see  Fig. A1  ###reference_###).\n\n###figure_4###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Evaluating the limitations of QA-Emb",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Improving computational efficiency via model distillation",
            "text": "###table_1### To reduce the computational cost of running inference with QA-Emb,\nwe explore distilling the many LLM calls needed to compute QA-Emb into a single model with many classification heads.\nSpecifically, we finetune a RoBERTa model [83  ###reference_b83###] (roberta-base) with 674 classification heads to predict all answers required for QA-Emb in a single feedforward pass.\nWe finetune the model on answers from LLaMA-3 8B with a few-shot prompt\nfor 80% of the 10-grams in the 82 fMRI training stories (123,203 examples), use the remaining 20% as a validation set for early stopping (30,801 examples), and evaluate on all 10-grams in the 2 testing stories (4,594 examples).\nWe finetune using AdamW [84  ###reference_b84###] with a learning rate of .\nWhen evaluated on the fMRI prediction task, the distilled model (QA-Emb (distill, binary) in Table 1  ###reference_###) yields a performance only slightly below the original model.\nIf we relax the restriction that the finetuned model yields binary embeddings and instead use the predicted probability for yes, the performance rises slightly to nearly match the original model (0.113 instead of 0.114 average test correlation) and maintains a significant improvement over the Eng1000 baseline.\nNote that the distilled model achieves an 88.5% match for yes/no answers on 10-grams for the test set.\nNevertheless, the fMRI prediction for any given timepoint is computed from many questions and ngrams, mitigating the effect of individual errors in answering a question."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Evaluating question-answering faithfulness",
            "text": "###figure_5### We evaluate the faithfulness of our question-answering models on a recent diverse collection of 54 binary classification datasets [85  ###reference_b85###, 86  ###reference_b86###] (see data details in Table A4  ###reference_###).\nThese datasets are difficult, as they are intended to encompass a wider-ranging and more realistic list of questions than traditional NLP datasets.\nFig. 4  ###reference_### shows the classification accuracy for the 3 LLMs used previously along with GPT-3.5 (gpt-3.5-turbo-0125).\nOn average, each of the LLMs answers these questions with fairly high accuracy, with GPT-4 slightly outperforming the other models.\nHowever, we observe poor performance on some tasks, which we attribute to the task difficulty and the lack of task-specific prompt engineering.\nFor example, the dataset yielding the lowest accuracy asks the question Is the input about math research?.\nWhile this may seem like a fairly simple question for an LLM to answer, the examples in the negative class consist of texts from other quantitative fields (e.g. chemistry) that usually contain numbers, math notation, and statistical analysis.\nThus the LLMs answer yes to most examples and achieve accuracy near chance (50%).\nNote that these tasks are more difficult than the relatively simple questions we answer in the fMRI experiments,\nespecially since the fMRI input lengths are each 10 words, whereas the input lengths for these datasets are over 50 words on average (with some inputs spanning over 1,000 words)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Secondary results: evaluating QA-Emb in simple NLP tasks",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Benchmarking QA-Emb for information retrieval",
            "text": "In this section, we investigate applying QA-Emb to a simplified information retrieval task.\nWe take a random subset of 4,000 queries from the MSMarco dataset ([87  ###reference_b87###], Creative Commons License) and their corresponding groundtruth documents, resulting in 5,210 documents.\nWe use 25% of the queries to build a training set and keep the remaining 75% for testing.\nFor evaluation, we calculate the cosine similarity match between the embeddings for each query and its groundtruth documents using mean reciprocal rank and recall.\nTo compute QA-Emb, we first generate 2,000 questions through prompting GPT-4 based on its knowledge of queries in information retrieval (see prompts in the Github).\nWe use a regex to slightly rewrite the resulting questions for queries to apply to documents (e.g. Is this query related to a specific timeframe?  Is this text related to a specific timeframe?).\nWe then answer the questions both for each query and for each corpus document, again using LLaMA-3 8B.\nRather than fitting a ridge regression as in Eq. 1  ###reference_###,\nwe use the training set to learn a scalar for each question that multiplies its binary output to change both its sign and magnitude in the embedding (optimization details in Sec. A.4  ###reference_###).\nTable 2  ###reference_### shows the information retrieval results.\nCombining BM-25 with QA-Emb achieves a small but significant improvement over the interpretable baselines.\nQA-Emb on its own achieves modest performance, slightly improving slightly over a bag-of-words representation, but significantly underperforming BM-25.\nNevertheless, its size is considerably smaller than the other interpretable baselines making it quicker to interpret and to use for retrieval.\n###table_2###"
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Zero-shot adaptation in text clustering",
            "text": "We now investigate QA-Emb in a simplified text clustering setting.\nTo do so, we study 4 text-classification datasets: Financial phrasebank ([88  ###reference_b88###], creative commons license), Emotion [89  ###reference_b89###] (CC BY-SA 4.0 license),\nAGNews [90  ###reference_b90###], and\nRotten tomatoes [91  ###reference_b91###].\nFor each dataset, we treat each class as a cluster and evaluate the clustering score,\ndefined as the difference between the average inter-class embedding distance and the average intra-class embedding distance (embedding distance is measured via Euclidean distance).\nA larger clustering score suggests that embeddings are well-clustered within each class.\nIn our experiment, we build a 100-dimensional embedding by prompting GPT-4 to generate 25 yes/no questions related to the semantic content of each dataset (e.g. for Rotten tomatoes, Generate 25 yes/no questions related to movie reviews).\nWe then concatenate the answers for all 100 questions to form our embedding.\nThese general embeddings do not yield particularly strong clustering scores (Table 3  ###reference_### top), as the questions are diverse and not particularly selective for each dataset.\nHowever, simply through prompting, we can adapt these general embeddings to each individual dataset.\nWe call GPT-4 with a prompt that includes the full list of questions and ask it to select a subset of questions that are relevant to each task.\nThe result embeddings (Table 3  ###reference_### bottom) yield higher clustering scores, suggesting that QA-Emb can be adapted to each task in a zero-shot manner (in this simplified setting).\nMoreover, the resulting task-specific embeddings are now considerably smaller.\n###table_3###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We find that QA-Emb can effectively produce interpretable and high-performing text embeddings.\nWhile we focus on a language fMRI setting,\nQA-Emb may be able to help flexibly build an interpretable text feature space in a variety of domains, such as social science [9  ###reference_b9###], medicine [10  ###reference_b10###], or economics [92  ###reference_b92###], where meaningful properties of text can help discover something about an underlying phenomenon or build trust in high-stakes settings.\nAlternatively, it could be used in mechanistic interpretability, to help improve post-hoc explanations of learned LLM representations.\nAs LLMs improve in both efficiency and capability, QA-Emb can be incorporated into a variety of common NLP applications as well, such as RAG or information retrieval.\nFor example, in RAG systems such as RAPTOR [93  ###reference_b93###] or Graph-RAG [94  ###reference_b94###],\nexplanations may help an LLM not only retrieve relevant texts, but also specify why they are relevant and how they may be helpful.\nLearning text questions rather than model weights is a challenging research area, furthering work in automatic prompt engineering [15  ###reference_b15###, 16  ###reference_b16###].\nOur approach takes a heuristic first step at solving this problem,\nbut future work could explore more directly optimizing the set of learned questions  in Eq. 1  ###reference_### via improved discrete optimization approaches and constraints.\nOne possible approach may involve having LLMs themselves identify the errors the current model is making and improving based on these errors, similar to general trends in LLM self-improvement and autoprompting [95  ###reference_b95###, 96  ###reference_b96###, 97  ###reference_b97###, 98  ###reference_b98###].\nAnother approach may involve improving the explanation capabilities of LLMs to help extract more questions more faithfully from data [99  ###reference_b99###, 100  ###reference_b100###].\nQA-Emb seeks to advance the field of LLM interpretation,\na crucial step toward addressing the challenges posed by these often opaque models.\nAlthough LLMs have gained widespread use, their lack of transparency can lead to significant harm, underscoring the importance of interpretable AI.\nThere are many potential positive societal consequences of this form of interpretability, e.g., facilitating a better understanding of scientific data and models,\nalong with a better understanding of LLMs and how to use them safely.\nNevertheless, as is the case with most ML research,\nthe interpretations could be used to interpret and potentially improve an LLM or dataset that is being used for nefarious purposes.\nMoreover, QA-Emb requires substantial computational resources, contributing to increased concerns over sustainability."
        }
    ],
    "url": "http://arxiv.org/html/2405.16714v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "3"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.16714v1",
        "paper_title": "Crafting Interpretable Embeddings by Asking LLMs Questions",
        "research_background": "### Motivation ###\nThe paper addresses the issue of interpretability in text embeddings generated by large language models (LLMs). Text embeddings are essential in many fields such as information retrieval, semantic clustering, retrieval-augmented generation, and language neuroscience. Although modern LLMs capture nuanced contexts and interactions more effectively than traditional methods like bag-of-words or BM-25, their embeddings are opaque, which poses problems in understanding and trusting the predictions and similarities drawn from these models. This lack of transparency is especially problematic in scientific fields like neuroscience and social sciences, where interpretability is crucial, as well as in high-stakes applications like medicine, where trust and regulatory compliance are critical. Therefore, there is a need for more interpretable embeddings to enhance usability and trust in these contexts.\n\n### Research Problem ###\nThe research problem the paper tackles is how to create interpretable text embeddings from large language models without compromising the models\u2019 ability to effectively capture the nuanced contexts and interactions within the data. Specifically, the problem focuses on making the embeddings human-inspectable by deriving understandable features that can meaningfully inform both the performance and decisions made by predictive models, particularly in the context of neuroscience.\n\n### Relevant Prior Work ###\n1. **Traditional Methods**: Bag-of-words and BM-25 are referenced as earlier methods for generating interpretable text representations.\n2. **Modern LLMs**: Numerous studies [2-7] have underscored the effectiveness of large language models in capturing nuanced contexts, though these methods result in less interpretable embeddings.\n3. **Interpretability Issues**: The lack of interpretability in these black-box representations has been highlighted as a significant drawback, particularly in scientific disciplines [8, 9] and high-stakes fields like medicine [10].\n4. **Regulatory and Safety Concerns**: Other work has discussed the broader issues of regulatory compliance, safety, and alignment in the use of opaque LLM embeddings for predictions [11-14].\n5. **Autoprompting and Single-Neuron Explanations**: The optimization challenges faced in generating interpretable representations share some similarities with natural-language autoprompting [15, 16] and single-neuron explanation techniques [17, 18].\n6. **Evaluation Pitfalls**: Prior literature has pointed out common pitfalls in evaluating interpretability methods generally [19, 20].\n7. **Applications in Psychology**: The importance of creating quantitative models from verbal hypotheses about brain selectivity has been a recognized challenge in fields like psychology [21].\n8. **Baselines for Comparison**: The paper benchmarks its proposed solution against established interpretable baselines [22] and black-box models like BERT [23].",
        "methodology": "**Crafting Interpretable Embeddings by Asking LLMs Questions: Methodology**\n\n**QA-Emb** is an intuitive method designed to generate text embeddings using a pre-trained autoregressive large language model (LLM). The core idea is to create interpretable embeddings by querying the LLM with a set of questions about the input text. Each element in the resulting embedding corresponds to the answer to a different question posed to the LLM, ensuring that the embeddings capture nuanced, relevant details of the input text while remaining interpretable.\n\n### Key Steps and Innovations\n1. **Question-Based Embeddings**: \n    - QA-Emb requires a set of yes/no questions to generate a binary embedding from a given input string. \n    - These questions are carefully chosen to suit a specific downstream task, such as fMRI prediction.\n    \n2. **Optimization for fMRI Prediction**:\n    - For the fMRI prediction task, embeddings are optimized using supervised linear regression.\n    - The objective is to derive an embedding that improves the performance of a ridge regression model, with embeddings helping to predict multi-dimensional continuous outputs.\n    - The formula used: \n    \\[\n    \\text{ridge regression} = \\mathbf{w} \\cdot \\text{embeddings} + \\lambda \\cdot \\text{Ridge regularization}\n    \\]\n    - Here, \\(\\mathbf{w}\\) is the learned coefficient vector and \\(\\lambda\\) is the ridge regularization parameter.\n\n3. **Heuristic Question Optimization**:\n    - Direct optimization over the space of yes/no questions is challenging due to the discrete nature and difficulty in specifying constraints.\n    - Instead, the set of questions is generated heuristically by prompting a highly capable LLM (like GPT-4) with a request tailored to the task at hand.\n    - Example prompt: \"Generate a bulleted list of questions with yes/no answers that is relevant for {{task description}}\".\n    - The method can flexibly specify additional information, such as examples from the input dataset, to help the LLM generate more relevant questions.\n\n4. **Sequential Question Generation**:\n    - Akin to gradient boosting, questions can be generated sequentially by having the LLM summarize input examples that result in high prediction errors, thus focusing on those examples.\n\n5. **Feature Selection and Adaptation**:\n    - Post-generation, the set of questions can be refined using techniques like Elastic Net to prune redundant or uninformative questions.\n    - Another option is to have an LLM adapt the set of questions based on task relevance, by filtering out irrelevant ones.\n\n6. **Limitations and Practical Considerations**:\n    - **Computational Intensity**: QA-Emb is computationally demanding, requiring multiple LLM calls to compute each embedding. This is managed by distilling the model into a single LLM with multiple classification heads, and partially reducing costs by reusing the KV-cache and generating a single token per question.\n    - **Accuracy of LLM Responses**: The success of QA-Emb hinges on the LLM\u2019s ability to correctly answer the specified yes/no questions. Strong LLMs are necessary, and the questions must be ones the LLM can answer accurately.\n\n7. **LLMs and Resources Utilized**:\n    - For answering questions: used Mistral-7B and LLaMA-3 8B with averaging responses from multiple prompts to enhance performance.\n    - For generating questions: used GPT-4.\n    - Experimental setup included using 64 AMD MI210 GPUs, each with 64 GB of memory, and a total compute time of approximately 4 days, with initial explorations taking about five times more.\n\n### Summary\nQA-Emb is a powerful method for generating interpretable embeddings through questions posed to LLMs. Its primary innovations include the use of a heuristic approach to generate relevant questions and the optimization of these embeddings for specific tasks. While computationally intensive, the method strikes a balance between interpretability and relevance, showing significant promise in tasks like fMRI prediction.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Main Experiment Setup\n\n**Objective:** \nTo predict the responses of different brain voxels to natural language stimuli using a novel QA-Emb model that bridges interpretability with accuracy.\n\n**Datasets:**\n- Two recent studies [74  ###reference_b74###, 75  ###reference_b75###], released under the MIT license.\n- fMRI responses from 3 human subjects listening to over 20 hours of narrative stories from podcasts.\n\n**Data Preparation:**\n- Each subject listens to either 79 or 82 stories (27,449 time points) and 2 test stories (639 time points).\n- Each subject\u2019s fMRI data consists of approximately 100,000 voxels, preprocessed by running PCA and extracting the coefficients of the top 100 components.\n\n**Regression Model:**\n- Ridge regression model to predict the 100 PCA coefficients.\n- Evaluation by correlating the predicted and actual responses for each voxel after applying the inverse PCA mapping.\n\n**Embedding Generation:**\n- Embeddings are produced at each word's timepoint using n-grams (10 preceding words).\n- Interpolated using Lanczos resampling to handle temporal sampling.\n\n**Hyperparameter Selection:**\n- Cross-validation on 5 time-stratified bootstrap samples.\n- Ridge parameter selection from 12 logarithmically spaced values between 10 and 10,000.\n- Selection of 4, 8, or 12 time-lagged duplicates of the stimulus features to model temporal delays in fMRI signals.\n\n**QA-Emb Generation:**\n- 6 prompts used to prompt GPT-4 to elicit questions useful for predicting fMRI responses.\n- Resulted in 674 questions.\n\n**Feature Selection:**\n- Multi-task Elastic net with 20 logarithmically spaced regularization parameters.\n- Final fitting with Ridge regression to the selected features.\n\n**Baselines:**\n- Eng1000: An interpretable embedding model developed specifically for predicting fMRI responses from narrative stories.\n- BERT (bert-base-uncased) embeddings.\n- LLaMA models: Layers selected from LLaMA-2 7B, LLaMA-2 70B, and LLaMA-3 8B.\n\n#### Results\n\n- **Main Performance Metric:** Correlation between predicted and actual voxel responses in the original voxel space.\n- QA-Emb demonstrated superior interpretability and accuracy by leveraging the knowledge extracted via the questions generated by GPT-4.\n- Comparisons showcased that QA-Emb outperformed both the Eng1000 interpretable baseline and the embeddings from BERT and LLaMA models across different layers and configurations.\n\nRefer to Table A3 for a detailed breakdown of the performance for each model, including cross-validated accuracies.\n\n#### Summary\n\nThe main experiment successfully demonstrated the efficacy of QA-Emb in predicting fMRI responses to natural language stimuli, providing a model that balances interpretability and accuracy. The approach was validated using rigorous cross-validation and outperformed existing baselines in the neuroscience literature."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To reduce the computational cost of running inference with QA-Emb by exploring model distillation techniques.",
            "experiment_process": "The study distills the multiple LLM calls required for QA-Emb into a single RoBERTa model with 674 classification heads. The model was finetuned on answers from LLaMA-3 8B using a few-shot prompt for 80% of the 10-grams in the 82 fMRI training stories (123,203 examples), with 20% used as a validation set for early stopping (30,801 examples) and evaluated on 10-grams in 2 testing stories (4,594 examples). The finetuning utilized AdamW with a specific learning rate. The experiment involved creating binary and non-binary (probabilistic) embeddings and comparing their performance on the fMRI prediction task against the original model and the Eng1000 baseline.",
            "result_discussion": "The distilled model yields performance only slightly below the original model. When the model's restriction to binary embeddings is relaxed (using predicted probabilities), the performance nearly matches the original (0.113 vs. 0.114 average test correlation) and significantly outperforms the Eng1000 baseline. The distilled model achieves an 88.5% match for yes/no answers on 10-grams for the test set, with any errors at the individual question level mitigated by the use of multiple questions and ngrams for predicting fMRI data points.",
            "ablation_id": "2405.16714v1.No1"
        },
        {
            "research_objective": "To evaluate the faithfulness of question-answering models on diverse binary classification datasets.",
            "experiment_process": "The faithfulness of the question-answering models was tested on a collection of 54 binary classification datasets, which encompass a diverse and realistic range of questions. Accuracy was measured across 3 previously used LLMs and GPT-3.5, using the classification accuracy as the evaluation metric.",
            "result_discussion": "Each of the LLMs answered the diverse questions with fairly high accuracy on average, with GPT-4 slightly outperforming others. However, some tasks saw poor performance due to their complexity and lack of task-specific prompt engineering. Notably, tasks asking whether the input was about math research achieved accuracy near chance (50%) due to the inclusion of texts from other quantitative fields in the negative examples. The diverse tasks presented more difficulty compared to the relatively simpler questions answered in the fMRI experiments, particularly due to their longer input lengths.",
            "ablation_id": "2405.16714v1.No2"
        }
    ]
}