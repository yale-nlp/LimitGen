{
    "title": "PolygloToxicityPrompts : Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models Warning: this paper discusses content that some may find toxic, obscene, or undesirable.",
    "abstract": "Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual toxicity evaluations. However, existing toxicity benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages.\nWe address this by introducing PolygloToxicityPrompts (PTP), the first large-scale multilingual toxicity evaluation benchmark of 425K naturally occurring prompts spanning 17 languages.\nWe overcome the scarcity of naturally occurring toxicity in web-text and ensure coverage across languages with varying resources by automatically scraping over 100M web-text documents.\nUsing PTP, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs. Notably, we find that toxicity increases as language resources decrease or model size increases. Although instruction- and preference-tuning reduce toxicity, the choice of preference-tuning method does not have any significant impact.\nOur findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) are increasingly being deployed in global contexts (Pichai & Hassabis, 2023  ###reference_b54###; Forbes, 2024  ###reference_b23###). Naturally, this has led to rapid advances in the multilingual capabilities of LLMs (Scao et al., 2022  ###reference_b60###; \u00dcst\u00fcn et al., 2024  ###reference_b77###; Yuan et al., 2023  ###reference_b85###). However, current toxicity evaluation benchmarks and safety alignment methods (Christiano et al., 2017  ###reference_b10###; Lee et al., 2024  ###reference_b41###) overwhelmingly focus on the English language, leading to significantly less safe responses in non-English languages (Wang et al., 2023  ###reference_b78###; Kotha et al., 2024  ###reference_b38###; Yong et al., 2023  ###reference_b82###). The lack of a standard multilingual benchmark for evaluating toxicity poses significant challenges to non-English users and the development of safer multilingual models.\nWe introduce PolygloToxicityPrompts (PTP),111We provide our dataset, code, and a leaderboard: https://huggingface.co/spaces/ToxicityPrompts/PTP  ###reference_pts/PTP### the first large-scale multilingual benchmark for evaluating neural toxic degeneration, defined as the propensity of LLMs to generate toxic text given a prompt (Gehman et al., 2020  ###reference_b26###). We create PTP by scraping over 100M documents from web-text corpora to collect naturally occurring toxic prompts. This results in 425K prompts in 17 languages ranging from non-toxic to highly-toxic prompts scored with Perspective API.222https://perspectiveapi.com/  ###reference_perspectiveapi.com/###\nPolygloToxicityPrompts provides three key improvements for multilingual toxicity evaluation, surfacing more toxic generations from LLMs than existing toxicity benchmarks (Figure 1  ###reference_###).\nFirst, PTP covers 17 languages while existing toxic degeneration work predominantly focuses on English (Gehman et al., 2020  ###reference_b26###; Lin et al., 2023a  ###reference_b43###).\nSecond, existing multilingual toxicity evaluation testbeds such as \u00dcst\u00fcn et al. (2024  ###reference_b77###) and RTP-LX (de Wynter et al., 2024  ###reference_b14###) are translations of RealToxicityPrompts (RTP; Gehman et al., 2020  ###reference_b26###), which can lack cultural nuances of toxicity and introduce deviations in toxicity, leading to under-estimated toxic degeneration (Sharou & Specia, 2022  ###reference_b62###; Costa-juss\u00e0 et al., 2023  ###reference_b11###).\nThird, PTP\u2019s naturally occurring prompts are more representative of real-world inputs than recent works on jailbreaking (Deng et al., 2023  ###reference_b15###; Wei et al., 2024  ###reference_b79###) and adversarial prompt generation (Zou et al., 2023  ###reference_b91###; Huang et al., 2023  ###reference_b30###), which lead to unnatural and often gibberish prompts.\n###figure_1### We evaluate 62 LLMs on PolygloToxicityPrompts to study the impact of prompt language, model size, alignment methods, and input prompt toxicity on toxicity.\nWe find significant toxicity in multilingual models, especially as the availability of language resources decreases. We observe that toxicity increases with model size within a model family for base LLMs. Furthermore, while instruction and preference-tuning reduce toxicity in models, the choice of preference-tuning method does not impact toxicity. Finally, we find that (un)safety and toxicity are related, but distinct aspects of LLMs that require their own solutions.\nOverall, our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research, notably, the need for multilingual toxicity mitigation and further investigations into the impact of model hyperparameters on toxicity. Our evaluation benchmark will advance efforts toward combating the critical issue of neural toxic degeneration."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Early works on evaluation datasets for studying biases and toxicity in models were created using templates or scraping web-text corpora. Sheng et al. (2019  ###reference_b63###); Nangia et al. (2020  ###reference_b50###); Nadeem et al. (2021  ###reference_b49###) use templated prompts to study social biases in pretrained language models. However, templates are focused on specific contexts such as demographic identities and not necessarily realistic. Thus, Gehman et al. (2020  ###reference_b26###) create RealToxicityPrompts by crawling English web-text for naturally occurring input prompts to evaluate toxicity in a sentence completion setting.\nMore recently, there has been a shift towards examining toxicity in input-response settings. Si et al. (2022  ###reference_b65###); Baheti et al. (2021  ###reference_b4###) use generations from dialogue models like DialoGPT (Zhang et al., 2020  ###reference_b87###) to study toxic degenerations in chatbots. Furthermore, the advent of instruction-tuned LLMs has led to studies of toxicity in real-world user-AI conversations. Zheng et al. (2024  ###reference_b89###) and Lin et al. (2023a  ###reference_b43###) collect user-AI interactions with automatic and manual toxicity annotations respectively to tackle a different toxic data distribution\u2014namely instructions. However, most of these approaches are limited to English.\nMultilingual dataset curation for evaluating toxicity has utilized both manual and automated translation techniques. Recent work on AI safety evaluation (Wang et al., 2023  ###reference_b78###; Yong et al., 2023  ###reference_b82###; Deng et al., 2023  ###reference_b15###) create multilingual safety benchmarks by translating monolingual benchmarks into other languages. They observe that LLMs are primarily safeguarded for English, leading to significantly unsafe generations in other languages, especially as availability of languages decreases. While these works are aimed towards the broader area of safety, the absence of a standard multilingual toxicity evaluation benchmark has also led researchers to translate prompts from RealToxicityPrompts into other languages, either automatically (\u00dcst\u00fcn et al., 2024  ###reference_b77###) or using human annotations (de Wynter et al., 2024  ###reference_b14###). However, manual translations are expensive, not scalable, and can introduce cultural biases, whereas automated translations can introduce deviations in toxicity due to incorrect translations and hallucinations (Specia et al., 2021  ###reference_b67###; Sharou & Specia, 2022  ###reference_b62###; Team et al., 2022  ###reference_b72###; Costa-juss\u00e0 et al., 2023  ###reference_b11###).\nBesides human-generated or naturally occurring data, a wealth of recent work has explored using machine-generated approaches to curate datasets and methods for evaluating the toxicity and safety of LLMs. Hartvigsen et al. (2022  ###reference_b28###) and Kim et al. (2022  ###reference_b37###) generate adversarial prompts about minority groups using classifier-guided decoding and conversations with a toxic partner respectively. Extensive research has studied red teaming (Perez et al., 2022  ###reference_b53###; Chao et al., 2023  ###reference_b9###; Mazeika et al., 2024  ###reference_b47###) and jailbreaking (Liu et al., 2023  ###reference_b46###; Wei et al., 2024  ###reference_b79###; Yu et al., 2023  ###reference_b84###; Deng et al., 2023  ###reference_b15###) to identify safety failures in LLMs and elicit harmful outputs. Furthermore, adversarial attack methods have also been shown to be effective against models without requiring substantial prompt engineering (Shin et al., 2020  ###reference_b64###; Zou et al., 2023  ###reference_b91###; Huang et al., 2023  ###reference_b30###; Jones et al., 2023  ###reference_b35###). However, such methods involve extensive prompt engineering, often leading to unnatural and non-representative prompts or model-specific artifacts (Das et al., 2024  ###reference_b13###). Furthermore, the extent to which these methods work in non-English languages remains to be studied.\nWhile the literature on toxicity evaluation has grown rapidly, their predominant focus on English highlights the need for multilingual benchmarks on naturally occurring toxic input prompts. We address this gap with PolygloToxicityPrompts, a collection of 425K naturally occurring prompts across 17 languages for evaluating toxicity."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "PolygloToxicityPrompts",
            "text": "We create PolygloToxicityPrompts, a large-scale multilingual testbed to evaluate toxic degeneration in LLMs. It consists of 425K prompts extracted from web-text corpora paired with toxicity scores from Perspective API. All 17 languages supported by Perspective API are represented in our testbed, namely: Arabic (ar), Chinese (zh), Czech (cs), Dutch (nl), English (en), French (fr), German (de), Hindi (hi), Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Polish (pl), Portuguese (pt), Russian (ru), Spanish (es), and Swedish (sv).\nWhile the extraction of toxic content from web-text may appear straightforward, we encountered several challenges associated with the scarcity of multilingual toxicity. The mC4 corpus (Xue et al., 2021  ###reference_b81###) filters toxicity by removing pages containing bad words.4  ###reference_te4### As a result, we observe less than  toxicity rate out of 5M samples for ar, cs, fr, ko, id, it, nl, pl, and sv. However, consistent with previous findings (Zhou et al., 2021  ###reference_b90###; Dodge et al., 2021  ###reference_b19###), we note that filtered datasets still exhibit toxicity, and observe higher toxicity rates for other languages.\nTo attain a larger sample of toxic content for languages with low toxicity rates, we create synthetic high-toxicity data. Specifically, we translate toxic samples from the mC4 and The Pile corpora into target languages using the NLLB-3B model (Team et al., 2022  ###reference_b72###). We use this process to create  70K translated prompts across 9 languages, which amounts to only  of our dataset. Contrary to prior works, we observe a Pearson correlation of 0.725 () between the toxicity scores of the original and translated samples across all languages, suggesting that low amounts of translated data are not necessarily an issue.555We discuss limitations with translating data in the Ethics Statement  ###reference_###.\nWe also create , a stratified sample of 5K prompts per language from PolygloToxicityPrompts to benchmark models in limited computational resources.\n###figure_2### Given a toxicity evaluator  and a language model ,  predicts the toxicity score for a text, and  provides a completion  for a prompt . We sample  completions per prompt and follow Gehman et al. (2020  ###reference_b26###); \u00dcst\u00fcn et al. (2024  ###reference_b77###) to measure a model\u2019s toxicity over a set of prompts . Specifically, we compute Expected Maximum Toxicity, that is, what is the expected toxicity of a model\u2019s worst-case generations?, as\nEMT  and the Empirical Probability (EP), that is, how frequently does a model generate toxicity, as the probability that a model generates at least one completion with toxicity score  in  completions. We additionally compute Average Toxicity, that is, what is the model\u2019s overall toxicity?, as AT .\nWe utilize  to benchmark LLMs due to the breadth of considered models and computational constraints. We use the Toxicity score from Perspective API as our toxicity evaluator ,  completions, temperature , top_p , and a maximum generation length of  tokens for our experiments. We use Microsoft Azure\u2019s OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM (Kwon et al., 2023  ###reference_b39###) for decoder-only models, and Huggingface\u2019s TGI666https://github.com/huggingface/text-generation-inference  ###reference_tion-inference### for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Operationalizing and Evaluating Toxicity",
            "text": "We define toxicity as \u201ca rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion\u201d (Wulczyn et al., 2017  ###reference_b80###; Borkan et al., 2019  ###reference_b8###). We use Perspective API,2  ###reference_te2### an industry-standard toxicity detection tool because it supports our 17 languages.\nSpecifically, we use the Toxicity score from Perspective API, computed using the UTC (Unified Toxic Content Classification) framework (Lees et al., 2022  ###reference_b42###), composed of a Charformer-based transformer (Tay et al., 2022  ###reference_b69###). UTC is a Seq2Seq architecture pretrained with the mC4 corpus (Xue et al., 2021  ###reference_b81###) and Perspective Pretraining Corpus (PPC). Additionally, Perspective API utilizes a single-language CNN (Lecun et al., 1998  ###reference_b40###) distilled from multilingual BERT models (Devlin et al., 2019  ###reference_b17###) for German and Portuguese."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dataset Creation",
            "text": "We construct our dataset by scraping over 100M documents from the mC4 (Xue et al., 2021  ###reference_b81###) and The Pile (Gao et al., 2020  ###reference_b25###) corpora as they contain multilingual texts from a variety of domains. We also leverage Pile Curse,333https://huggingface.co/datasets/tomekkorbak/pile-curse-full  ###reference_k/pile-curse-full### a subset of The Pile scored using the bad words 444https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words  ###reference_aughty-Obscene-and-Otherwise-Bad-Words### list for our English split. We then extract Toxicity scores with Perspective API for all scraped documents. To obtain a stratified range of prompt toxicity, we sample 6250 documents from 4 equal-width toxicity levels (). We then split collected documents in half to form prompts and continuations, both of which are scored for toxicity. We provide preprocessing details, dataset statistics, and metadata analysis in Appendix A  ###reference_###.\nThe final dataset includes 25K naturally occurring prompts for each language, for a total of 425K prompts across 17 languages. Figures 10(a)  ###reference_.sf1### and 10(b)  ###reference_.sf2### show the prompt toxicity and length distributions of our collected prompts for all languages. We create our prompts using documents instead of sentences (Gehman et al., 2020  ###reference_b26###). Thus, our prompts are much longer than RealToxicityPrompts, with an average length of approximately 400 GPT-4 tokens.\nWhile the extraction of toxic content from web-text may appear straightforward, we encountered several challenges associated with the scarcity of multilingual toxicity. The mC4 corpus (Xue et al., 2021  ###reference_b81###  ###reference_b81###) filters toxicity by removing pages containing bad words.4  ###reference_te4###  ###reference_te4### As a result, we observe less than  toxicity rate out of 5M samples for ar, cs, fr, ko, id, it, nl, pl, and sv. However, consistent with previous findings (Zhou et al., 2021  ###reference_b90###  ###reference_b90###; Dodge et al., 2021  ###reference_b19###  ###reference_b19###), we note that filtered datasets still exhibit toxicity, and observe higher toxicity rates for other languages.\nTo attain a larger sample of toxic content for languages with low toxicity rates, we create synthetic high-toxicity data. Specifically, we translate toxic samples from the mC4 and The Pile corpora into target languages using the NLLB-3B model (Team et al., 2022  ###reference_b72###  ###reference_b72###). We use this process to create  70K translated prompts across 9 languages, which amounts to only  of our dataset. Contrary to prior works, we observe a Pearson correlation of 0.725 () between the toxicity scores of the original and translated samples across all languages, suggesting that low amounts of translated data are not necessarily an issue.555We discuss limitations with translating data in the Ethics Statement  ###reference_###  ###reference_###.\nWe also create , a stratified sample of 5K prompts per language from PolygloToxicityPrompts to benchmark models in limited computational resources.\n###figure_3###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Benchmarking Large Language Models",
            "text": "We benchmark a large variety of models () spanning different sizes and multilingual capabilities. We follow the taxonomy proposed by Albalak et al. (2024  ###reference_b1###) and include LLMs trained purely with the language modeling objective (base) such as Llama2 (Touvron et al., 2023b  ###reference_b75###), Pythia (Biderman et al., 2023  ###reference_b7###), LLMs fine-tuned to follow instructions (instruct) such as Mistral-Instruct (Jiang et al., 2023  ###reference_b34###), and LLMs aligned with preference-tuning/alignment methods (preference) such as GPT-3.5-Turbo (Ouyang et al., 2022  ###reference_b52###) and Zephyr (Tunstall et al., 2023  ###reference_b76###). In the subsequent section (Section 4  ###reference_###), we explore a variety of research questions that require specific functionalities and thus use the appropriate subset of models for our analyses. We also note that the LLMs we benchmark are, to the best of our knowledge, the neural networks that are trained and possibly instruction and/or preference-tuned, without any possible safeguards or guardrails that may have been added onto the public interfaces of such LLMs, such as safety classifiers applied to the input/output of LLMs."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Benchmarking Setup",
            "text": "Given a toxicity evaluator  and a language model ,  predicts the toxicity score for a text, and  provides a completion  for a prompt . We sample  completions per prompt and follow Gehman et al. (2020  ###reference_b26###  ###reference_b26###); \u00dcst\u00fcn et al. (2024  ###reference_b77###  ###reference_b77###) to measure a model\u2019s toxicity over a set of prompts . Specifically, we compute Expected Maximum Toxicity, that is, what is the expected toxicity of a model\u2019s worst-case generations?, as\nEMT  and the Empirical Probability (EP), that is, how frequently does a model generate toxicity, as the probability that a model generates at least one completion with toxicity score  in  completions. We additionally compute Average Toxicity, that is, what is the model\u2019s overall toxicity?, as AT .\nWe utilize  to benchmark LLMs due to the breadth of considered models and computational constraints. We use the Toxicity score from Perspective API as our toxicity evaluator ,  completions, temperature , top_p , and a maximum generation length of  tokens for our experiments. We use Microsoft Azure\u2019s OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM (Kwon et al., 2023  ###reference_b39###  ###reference_b39###) for decoder-only models, and Huggingface\u2019s TGI666https://github.com/huggingface/text-generation-inference  ###reference_tion-inference###  ###reference_tion-inference### for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Research Questions",
            "text": "To investigate multilingual toxic degeneration in a large suite of models, we obtain and score continuations for the 5K prompts per language contained in  (due to computational resource limitations). We find similar trends across all evaluation metrics and thus report only Average Toxicity for brevity.\nTable 1  ###reference_### previews our findings for the models with the lowest and highest Average Toxicity. We provide results for all models with languages categorized based on Joshi et al. (2020  ###reference_b36###)777Since all considered languages belong to categories 3 and above, we compare relative resource availability, that is, categories 3, 4 and 5 are referred as low-, medium- and high-resource respectively. in Table LABEL:tab:secondary_results.\nNext, we explore specific patterns concerning prompt language, model size, alignment methods, and prompt toxicity below. Finally, we also compare toxicity and safety detectors using Perspective API and Llama Guard Inan et al. (2023  ###reference_b32###) respectively.\nWe investigate the distribution of continuation toxicity for base LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the continuation toxicity for base LLMs (, ).\nPrior work has shown limited evidence of the dependence of model toxicity on size. For instance, Touvron et al. (2023a  ###reference_b74###; b  ###reference_b75###) find that toxicity increases with model size, whereas\nGehman et al. (2020  ###reference_b26###); Hoffmann et al. (2022  ###reference_b29###) find that larger models are not necessarily more toxic. We hypothesize that toxicity might depend on model size within a model family only, and investigate this further with the Pythia suite.\nThe Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on toxicity using the English split of our dataset. Figure 4  ###reference_### shows an overall increase in toxicity with an increase in model size, which plateaus near  parameters (effect size of the difference between  and  is small, Cohen\u2019s , ).\n###figure_4### ###figure_5### This is consistent with prior works (Touvron et al., 2023a  ###reference_b74###; b  ###reference_b75###). More specifically, we find that the toxicity levels in  Pythia models are comparatively higher than the smallest  model (Cohen\u2019s , ).\nThis implies that toxicity is a long-tail phenomenon that large enough models ( parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better (Tirumala et al., 2022  ###reference_b73###).\nTo investigate the impact of model size on toxicity for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2\u2019s pretraining data) as shown in Figure 6  ###reference_###.\nWe observe different trends in both model families when scaling from  to  \u2014 for Llama 2-Chat models, AT first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces toxicity for Tulu 2 models as they are scaled to  parameters. However, such differences are small (Cohen\u2019s  for all combinations with  models).\nThere seems to be no conclusive answer as to whether model size affects toxicity in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results.\nThus, future work is needed to investigate the specific effects of model sizes on toxic degeneration in safety-aligned models.\nWe first compare toxicity levels aggregated over base, instruct, and preference models (Figure 6  ###reference_###).\nWe find that, on average, base models have the highest toxicity (AT; significantly different from instruct and preference models; Cohen\u2019s  and , respectively, ).\nFurthermore, we find that instruct and preference models barely differ in toxicity (Cohen\u2019s , ), though preference-tuned models have slightly lower toxicity on average.\nTo study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite888https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430  ###reference_ualAI/archangel-65bd45029fa020161b052430### of Llama models (Touvron et al., 2023a  ###reference_b74###) and TinyLLama999https://huggingface.co/collections/abideen/tinyllama-alignment-65a2a99c8ac0602820a22a46  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46### (Zhang et al., 2024  ###reference_b86###) models.\nInterestingly, we do not observe a considerable difference in the average toxicity exhibited by models trained with different alignment methods (Cohen\u2019s ) (Figure 7  ###reference_###). Moreover, this trend remains at different scales of , , and , suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model toxicity.\n###figure_6### To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B (Team et al., 2024  ###reference_b70###) variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0.1,101010https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1  ###reference_r-7b-gemma-v0.1### trained on AI preferences (Figure 8  ###reference_###). We observe that AI feedback is better than human feedback for en, whereas human feedback shows lower toxicity levels for non-English languages. We emphasize toxicity results on the en split since both models were trained using English-only preference data, likely making multilingual prompts out-of-distribution. Furthermore, zephyr-7b-gemma-v0.1 is aligned using DPO which has been found to reduce multilingual capabilities (Ivison et al., 2023  ###reference_b33###), likely leading to higher toxicity for non-English languages.\nWhile this suggests that AI feedback reduces model toxicity, we hypothesize that the operationalization of toxicity might play a role.\nAI feedback relies on LLMs\u2019 definition of toxic content, which likely aligns better with Perspective API\u2019s perception of toxicity rather than human perceptions, which are more nuanced and subjective (Sap et al., 2022  ###reference_b59###).\nFurthermore, curating datasets using models can result in the under-representation of more veiled toxicity (Han & Tsvetkov, 2020  ###reference_b27###) and general data and topical skews (Das et al., 2024  ###reference_b13###).\nWe examine the extent to which different model categories mirror input toxicity.\nWe find that the continuation toxicity of base models is most strongly correlated with input toxicity (, ).\nSurprisingly, preference models have a higher correlation between input and continuation toxicity (, ), compared to instruct models (, ).\nWe find that this is due in large to low-toxicity prompts, for which preference models mimic the input (low) toxicity in continuations better () than for high-toxicity prompts ().\ninstruct models also show a stronger correlation between prompt and continuation toxicity for low-toxicity prompts () than for high-toxicity ones (). This indicates that preference models better match input toxicity than instruct models, but predominantly in low-toxicity inputs, suggesting that preference models are better safeguarded against high-toxicity inputs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "How does Model Size impact Average Toxicity?",
            "text": "Prior work has shown that undesirable content generation can increase with model size and possibly pretraining dataset size (Bender et al., 2021  ###reference_b6###; Tal et al., 2022  ###reference_b68###; Smith et al., 2022  ###reference_b66###; Touvron et al., 2023a  ###reference_b74###). We conduct a similar investigation on the impact of model size on toxicity. We first study these trends in base models such as Llama 2 (Touvron et al., 2023b  ###reference_b75###) and Pythia (Biderman et al., 2023  ###reference_b7###), and later examine models with additional tuning (instruct, preference) such as Tulu 2 (Ivison et al., 2023  ###reference_b33###).\nWe investigate the distribution of continuation toxicity for base LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the continuation toxicity for base LLMs (, ).\nPrior work has shown limited evidence of the dependence of model toxicity on size. For instance, Touvron et al. (2023a  ###reference_b74###  ###reference_b74###; b  ###reference_b75###  ###reference_b75###) find that toxicity increases with model size, whereas\nGehman et al. (2020  ###reference_b26###  ###reference_b26###); Hoffmann et al. (2022  ###reference_b29###  ###reference_b29###) find that larger models are not necessarily more toxic. We hypothesize that toxicity might depend on model size within a model family only, and investigate this further with the Pythia suite.\nThe Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on toxicity using the English split of our dataset. Figure 4  ###reference_###  ###reference_### shows an overall increase in toxicity with an increase in model size, which plateaus near  parameters (effect size of the difference between  and  is small, Cohen\u2019s , ).\n###figure_9### ###figure_10### This is consistent with prior works (Touvron et al., 2023a  ###reference_b74###  ###reference_b74###; b  ###reference_b75###  ###reference_b75###). More specifically, we find that the toxicity levels in  Pythia models are comparatively higher than the smallest  model (Cohen\u2019s , ).\nThis implies that toxicity is a long-tail phenomenon that large enough models ( parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better (Tirumala et al., 2022  ###reference_b73###  ###reference_b73###).\nTo investigate the impact of model size on toxicity for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2\u2019s pretraining data) as shown in Figure 6  ###reference_###  ###reference_###.\nWe observe different trends in both model families when scaling from  to  \u2014 for Llama 2-Chat models, AT first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces toxicity for Tulu 2 models as they are scaled to  parameters. However, such differences are small (Cohen\u2019s  for all combinations with  models).\nThere seems to be no conclusive answer as to whether model size affects toxicity in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results.\nThus, future work is needed to investigate the specific effects of model sizes on toxic degeneration in safety-aligned models."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "How do Alignment Methods impact Average Toxicity?",
            "text": "###figure_11### While prior work has shown that safety alignment leads to reduced toxicity levels in models (Touvron et al., 2023b  ###reference_b75###), the impact of different alignment methods on toxicity is yet to be studied.\nWe investigate the impact of instruction-tuning and preference-tuning using different alignment methods, namely PPO (Schulman et al., 2017  ###reference_b61###), DPO (Rafailov et al., 2024  ###reference_b56###), KTO (Ethayarajh et al., 2024  ###reference_b22###), and IPO (Azar et al., 2023  ###reference_b3###) on toxicity. For preference-tuned models, we also study the effect of the method used to create preference data for preference-tuning or alignment.\nWe first compare toxicity levels aggregated over base, instruct, and preference models (Figure 6  ###reference_###  ###reference_###).\nWe find that, on average, base models have the highest toxicity (AT; significantly different from instruct and preference models; Cohen\u2019s  and , respectively, ).\nFurthermore, we find that instruct and preference models barely differ in toxicity (Cohen\u2019s , ), though preference-tuned models have slightly lower toxicity on average.\nTo study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite888https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430  ###reference_ualAI/archangel-65bd45029fa020161b052430###  ###reference_ualAI/archangel-65bd45029fa020161b052430### of Llama models (Touvron et al., 2023a  ###reference_b74###  ###reference_b74###) and TinyLLama999https://huggingface.co/collections/abideen/tinyllama-alignment-65a2a99c8ac0602820a22a46  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46###  ###reference_/tinyllama-alignment-65a2a99c8ac0602820a22a46### (Zhang et al., 2024  ###reference_b86###  ###reference_b86###) models.\nInterestingly, we do not observe a considerable difference in the average toxicity exhibited by models trained with different alignment methods (Cohen\u2019s ) (Figure 7  ###reference_###  ###reference_###). Moreover, this trend remains at different scales of , , and , suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model toxicity.\n###figure_12### To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B (Team et al., 2024  ###reference_b70###  ###reference_b70###) variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0.1,101010https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1  ###reference_r-7b-gemma-v0.1###  ###reference_r-7b-gemma-v0.1### trained on AI preferences (Figure 8  ###reference_###  ###reference_###). We observe that AI feedback is better than human feedback for en, whereas human feedback shows lower toxicity levels for non-English languages. We emphasize toxicity results on the en split since both models were trained using English-only preference data, likely making multilingual prompts out-of-distribution. Furthermore, zephyr-7b-gemma-v0.1 is aligned using DPO which has been found to reduce multilingual capabilities (Ivison et al., 2023  ###reference_b33###  ###reference_b33###), likely leading to higher toxicity for non-English languages.\nWhile this suggests that AI feedback reduces model toxicity, we hypothesize that the operationalization of toxicity might play a role.\nAI feedback relies on LLMs\u2019 definition of toxic content, which likely aligns better with Perspective API\u2019s perception of toxicity rather than human perceptions, which are more nuanced and subjective (Sap et al., 2022  ###reference_b59###  ###reference_b59###).\nFurthermore, curating datasets using models can result in the under-representation of more veiled toxicity (Han & Tsvetkov, 2020  ###reference_b27###  ###reference_b27###) and general data and topical skews (Das et al., 2024  ###reference_b13###  ###reference_b13###)."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Comparing Toxicity and Safety Detectors: Perspective API vs. Llama Guard",
            "text": "Recent work has seen rapid growth in studies on safety evaluation and safeguarding techniques (Ganguli et al., 2022  ###reference_b24###; Mazeika et al., 2024  ###reference_b47###). For instance, Inan et al. (2023  ###reference_b32###) develop Llama Guard, a Llama 2 model to classify safety risks in LLM inputs and responses. However, the extent to which toxicity and safety overlap is unclear. To fill this gap, we compare Perspective API, a toxicity detector, and Llama Guard, a safety detector.\nSince Llama Guard only supports English, we compute scores for all models on the English split of  following the instructions in its model card.111111https://huggingface.co/meta-llama/LlamaGuard-7b  ###reference_rd-7b### We find that Perspective API toxicity scores are generally well-aligned with Llama Guard scores ().\nHowever, Llama Guard and Perspective API still capture distinct concepts. To analyze the differences between both evaluation methods, we examine the prompts and generations where the metrics differ the most (Table 5  ###reference_### in Appendix E  ###reference_###). We observe that Perspective API is better at detecting explicit toxicity, hate speech, and derogative language and provides extensive support for non-English languages. However, Llama Guard can identify subtle unsafe generations and extend to other axes of AI safety.\nOur findings suggest that LLM safety detectors may not be equipped to capture the full spectrum of toxicity."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "How does Prompt Toxicity impact Continuation Toxicity?",
            "text": "We investigate the relationship between input prompt toxicity and continuation toxicity at greater granularity, that is, without aggregating as in Average Toxicity. Intuitively, we expect a model\u2019s propensity to generate toxic text to be proportional to the toxicity of the input prompt. Empirically, we find a Pearson correlation of  () between prompt toxicity and continuation toxicity.\nWe also find that continuation toxicity spans the entire toxicity range, regardless of input toxicity score, indicating that non-toxic prompts can yield toxic continuations and vice-versa, corroborating Gehman et al. (2020  ###reference_b26###). Furthermore, we investigate the correlations between prompt and continuation toxicity across languages and model families in Appendix B  ###reference_###.\nWe examine the extent to which different model categories mirror input toxicity.\nWe find that the continuation toxicity of base models is most strongly correlated with input toxicity (, ).\nSurprisingly, preference models have a higher correlation between input and continuation toxicity (, ), compared to instruct models (, ).\nWe find that this is due in large to low-toxicity prompts, for which preference models mimic the input (low) toxicity in continuations better () than for high-toxicity prompts ().\ninstruct models also show a stronger correlation between prompt and continuation toxicity for low-toxicity prompts () than for high-toxicity ones (). This indicates that preference models better match input toxicity than instruct models, but predominantly in low-toxicity inputs, suggesting that preference models are better safeguarded against high-toxicity inputs."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "How do different Data Sources elicit Average Toxicity?",
            "text": "Finally, we study the ability of different data sources to elicit toxicity from LLMs. Specifically, we compare Average Toxicity when generating continuations for naturally occurring prompts from PTP, RTP-LX (de Wynter et al., 2024  ###reference_b14###), and an automatically translated sample of user-LLM interactions from WildChat (Zhao et al., 2024  ###reference_b88###).121212We provide details about RTP-LX and WildChat in Appendix C  ###reference_###.\n###figure_13### Figure 9  ###reference_### shows that PTP consistently draws out higher Average Toxicity. While RTP-LX is comprised of naturally occurring prompts in English and their culturally-aware translations to other languages, we find that PTP is still able to capture more toxicity, likely due to longer prompt lengths, corroborating Anil et al. (2024  ###reference_b2###). Furthermore, we hypothesize that preference-tuning makes models less vulnerable to what users input into LLMs as opposed to naturally occurring toxicity, leading to higher toxicity levels elicited by PTP compared to WildChat."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We present PolygloToxicityPrompts, the first large-scale multilingual benchmark of 425K naturally occurring prompts across 17 languages for evaluating toxic degenerations in LLMs. We benchmark 62 LLMs to study the impact of factors like prompt language, prompt toxicity, model size, instruction- and preference-tuning, and alignment methods on toxicity. We also compare toxicity and safety detectors to emphasize that toxicity and safety are related but distinct aspects. Overall, our findings highlight crucial gaps in current research around the need for multilingual safeguarding and emphasize further empirical and theoretical investigations of how toxic degeneration is affected by prompt language, model size, and alignment methods."
        }
    ],
    "url": "http://arxiv.org/html/2405.09373v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "3.4",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.5",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2405.09373v2",
        "paper_title": "PolygloToxicityPrompts : Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models Warning: this paper discusses content that some may find toxic, obscene, or undesirable.",
        "research_background": "### Motivation\nThe motivation for this paper stems from the increasing deployment of Large Language Models (LLMs) in global contexts (Pichai & Hassabis, 2023; Forbes, 2024). As LLMs advance in their multilingual capabilities (Scao et al., 2022; \u00dcst\u00fcn et al., 2024; Yuan et al., 2023), there is a growing need to ensure that these models are safe and capable of generating non-toxic responses across various languages. However, existing toxicity evaluation benchmarks and safety alignment methods mainly focus on English, leaving significant gaps in safety for non-English languages (Wang et al., 2023; Kotha et al., 2024; Yong et al., 2023). This deficiency poses challenges for non-English users and the development of safer multilingual models.\n\n### Research Problem\nThe research problem addressed by this paper is the lack of a standard multilingual benchmark for evaluating toxicity in LLMs. Without such a benchmark, it is difficult to measure and mitigate the generation of toxic content by LLMs across different languages, thereby jeopardizing the safety and suitability of these models for global use. This paper introduces PolygloToxicityPrompts (PTP), a novel large-scale multilingual benchmark designed to evaluate neural toxic degeneration \u2014 the tendency of LLMs to generate toxic text in response to prompts.\n\n### Relevant Prior Work\n1. **Current Multilingual Capabilities of LLMs:**\n   - The capabilities of LLMs in handling multiple languages have seen rapid advancements (Scao et al., 2022; \u00dcst\u00fcn et al., 2024; Yuan et al., 2023).\n\n2. **Existing Toxicity Evaluation Benchmarks and Safety Alignment Methods:**\n   - Most of these methods predominantly address the English language (Christiano et al., 2017; Lee et al., 2024), leading to less safe model outputs in non-English languages (Wang et al., 2023; Kotha et al., 2024; Yong et al., 2023).\n\n3. **Existing Toxicity Datasets:**\n   - Current datasets, including the RealToxicityPrompts (RTP) (Gehman et al., 2020), lack coverage of non-English languages and may miss cultural nuances, resulting in under-estimated toxic degeneration (Sharou & Specia, 2022; Costa-juss\u00e0 et al., 2023).\n   - Works like \u00dcst\u00fcn et al. (2024) and RTP-LX (de Wynter et al., 2024) are translations of RTP, which do not fully capture linguistic and cultural variations in toxicity.\n\n4. **Evaluation of Prompts:**\n   - Efforts like jailbreaking (Deng et al., 2023; Wei et al., 2024) and adversarial prompt generation (Zou et al., 2023; Huang et al., 2023) often result in unnatural prompts that do not accurately represent real-world scenarios.\n\n### Contributions of the Paper\nThe contributions of this paper include:\n1. **Introduction of a New Benchmark:**\n   - PolygloToxicityPrompts (PTP), which includes a dataset of 425K naturally occurring toxic prompts across 17 languages, more accurately reflecting real-world usage and covering a range of toxicity levels.\n\n2. **Evaluation and Findings:**\n   - The paper evaluates 62 LLMs on PTP to examine the impact of various factors such as prompt language, model size, alignment methods, and the inherent toxicity of prompts.\n   - It uncovers significant findings such as increased toxicity in multilingual models with fewer language resource availabilities, higher toxicity with larger model sizes within a family of models, and the effectiveness of instruction and preference-tuning methods in reducing toxicity.",
        "methodology": "### Methodology Summary\n\nPolygloToxicityPrompts proposes a comprehensive method to evaluate toxic degeneration in Large Language Models (LLMs) across multiple languages. Below are the key components and innovations of the proposed methodology:\n\n1. **Multilingual Testbed Creation**:\n   - **Dataset**: The testbed consists of 425,000 prompts extracted from various web-text corpora, each paired with toxicity scores derived from the Perspective API.\n   - **Language Support**: The dataset spans 17 languages, including Arabic, Chinese, Czech, Dutch, English, French, German, Hindi, Indonesian, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, and Swedish.\n   \n2. **Addressing Scarcity Issues**:\n   - **Multilingual Toxicity**: Challenges were encountered due to the low prevalence of toxic content in some languages within web-text corpora like mC4, especially for languages such as Arabic, Czech, French, Korean, Indonesian, Italian, Dutch, Polish, and Swedish.\n   - **Synthetic Data**: To bolster datasets with scarce toxic content, the team translated toxic samples from the mC4 and The Pile corpora into target languages using the NLLB-3B model, creating 70,000 additional toxic prompts. Despite this synthetic data only representing a small portion of the overall dataset, a high Pearson correlation (0.725) between original and translated toxicity scores was observed, indicating the effectiveness and reliability of this approach.\n   \n3. **Benchmarking Subset**:\n   - **Stratified Sampling**: A representative subset of 5,000 prompts per language from PolygloToxicityPrompts is used to facilitate model benchmarking, especially when computational resources are limited.\n\n### Evaluation Metrics\n\nThe methodology evaluates model toxicity using the following metrics:\n\n1. **Expected Maximum Toxicity (EMT)**: Assesses the expected toxicity of the model\u2019s most toxic generation for a prompt.\n   \\[\n   \\text{EMT} = \\mathbb{E}\\left[\\max_{\\mathbf{c} \\in \\mathcal{C}} T(\\mathbf{c})\\right]\n   \\]\n   \n2. **Empirical Probability (EP)**: Measures the frequency at which a model generates at least one toxic completion across multiple generations for a prompt.\n   \\[\n   \\text{EP} = \\mathbb{P}\\left(\\exists \\mathbf{c} \\in \\mathcal{C} : T(\\mathbf{c}) \\geq t\\right)\n   \\]\n   \n3. **Average Toxicity (AT)**: Captures the overall average toxicity of the model\u2019s generated completions.\n   \\[\n   \\text{AT} = \\mathbb{E}\\left[T(\\mathbf{c})\\right]\n   \\]\n\n### Experimental Setup\n\n1. **Model and API Usage**:\n   - The Perspective API is used to evaluate toxicity scores.\n   - Language models involved in the experiments include GPT-3.5-Turbo (via Microsoft Azure's OpenAI API without safety settings), vLLM for decoder-only models, and Huggingface\u2019s TGI for encoder-decoder models.\n\n2. **Configuration**:\n   - **Generations per Prompt**: Each prompt receives 25 generated completions.\n   - **Hyperparameters**: Temperature is set to 0.7, top_p to 0.9, with a maximum token generation length of 100 tokens.\n\n3. **Prompt Templates**: Only the required prompt templates, as stated in the model cards, are utilized without additional instructions to assess the models' raw performance on the generated toxic content.\n\nThis methodology provides a broad, systematic approach to evaluate the toxic degeneration of LLMs across diverse languages, addressing both resource constraints and the challenge of multilingual toxicity evaluation.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets\nThe key dataset used in the main experiment consists of 5,000 prompts per language. These prompts are used to investigate multilingual toxic degeneration across various large language models (LLMs).\n\n#### Baselines and Models\n1. **Base LLMs**: Models trained only with the language modeling objective.\n2. **Safeguarded LLMs**: Includes Llama 2-Chat and Tulu 2-DPO models for English and other related languages.\n3. **Preference-Tuned Models**: Gemma 7B variants and zephyr-7b-gemma-v0.1.\n\n#### Evaluation Metrics\n- **Average Toxicity (AT)**: The primary evaluation metric, reporting the average toxicity score for continuations produced by the models.\n- Toxicity is measured using tools such as Perspective API and Llama Guard.\n\n### Main Experimental Results\n\n1. **Base LLMs**:\n   - There is a slight correlation between the number of parameters in base LLMs and continuation toxicity. This is consistent across the Pythia suite, where toxicity increases with model size but plateaus after a certain point.\n   - Large enough models (parameter count) tend to capture and demonstrate long-tail toxic phenomena better.\n\n2. **Safeguarded LLMs**:\n   - For Llama 2-Chat models, AT first decreases and then increases with model size.\n   - For Tulu 2 models, DPO alignment first increases and then reduces toxicity as the model size increases.\n   - Differences in toxicity levels between model sizes for safeguarded LLMs are marginal.\n\n3. **Preference Models vs. Instruct Models**:\n   - Aggregate comparison shows base models exhibit the highest toxicity, significantly different from instruct and preference models.\n   - No significant difference in toxicity levels between instruct and preference models, although preference models show slightly lower toxicity.\n\n4. **Input vs. Continuation Toxicity Correlation**:\n   - Base models show the strongest correlation between input and continuation toxicity.\n   - Preference models better match input toxicity, particularly in low-toxicity inputs, compared to instruct models.\n\n   These findings indicate variances in how model size, alignment methods, and feedback types affect the toxicity of generated content in multilingual contexts. Future work is suggested to explore in greater depth the specific impact of these factors on safe language model deployment."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs using PolygloToxicityPrompts.",
            "experiment_process": "The study uses a large suite of diverse LLMs, including base, instruct, and preference models like Llama2, Pythia, and GPT-3.5-Turbo. Expected Maximum Toxicity (EMT), Empirical Probability (EP), and Average Toxicity (AT) are calculated using completions from sampled prompts. Toxicity is measured using the Perspective API, with a specific setup including temperature, top_p, and a maximum generation length of tokens. The experiments are conducted on Microsoft Azure's OpenAI API and Huggingface's TGI for different model families.",
            "result_discussion": "The study finds that toxicity generally increases with model size and decreases slightly with instruction and preference-tuning. The choice of preference-tuning method does not significantly impact toxicity levels. The distribution of toxicity highlights the susceptibility of models to more toxic generations as language resources decrease.",
            "ablation_id": "2405.09373v2.No1"
        },
        {
            "research_objective": "Evaluate how different prompt languages affect average toxicity in multilingual LLMs.",
            "experiment_process": "The study benchmarks multilingual LLMs like GPT-3.5-Turbo, Aya101, and Bloomz, evaluating their Average Toxicity (AT) for 5,000 prompts per language from the PolygloToxicityPrompts dataset. The models are assessed across high, medium, and low-resource languages to understand their translation and response behaviors.",
            "result_discussion": "The findings show that models exhibit the lowest AT in Russian and Dutch, but higher AT in Hindi and Czech, especially for languages with fewer resources in their pretraining corpora. GPT-3.5-Turbo has generally higher AT levels across languages, whereas Bloomz models exhibit lower toxicity, potentially due to shorter and less coherent completions.",
            "ablation_id": "2405.09373v2.No2"
        },
        {
            "research_objective": "Understand the correlation between prompt toxicity and continuation toxicity in different LLMs.",
            "experiment_process": "The study measures the correlation between input prompt toxicity and continuation toxicity across various models, focusing on base, instruction-tuned, and preference-tuned models. Pearson correlation coefficients are calculated to determine the relationship between input and output toxicity levels.",
            "result_discussion": "Continuation toxicity strongly correlates with input toxicity, especially in base models. Preference models exhibit a higher correlation for low-toxicity prompts but perform less impressively with high-toxicity prompts compared to instruct models. This indicates that preference models might be better at safeguarding against high-toxicity inputs.",
            "ablation_id": "2405.09373v2.No3"
        },
        {
            "research_objective": "Examine the impact of model size on average toxicity in both base and safeguarded LLMs.",
            "experiment_process": "The Pythia suite of models, varying in size but consistent in pretraining data, is used for a controlled investigation. Experiments are set up to track Average Toxicity (AT) as model size increases, and results are plotted to observe trends and inflection points.",
            "result_discussion": "Findings suggest that toxicity tends to increase with model size but plateaus at a certain parameter count. For safeguarded models like Llama 2-Chat and Tulu 2-DPO, trends show varying impacts of model scaling on toxicity levels. Discrepancies in smaller models underline the need for further research.",
            "ablation_id": "2405.09373v2.No4"
        },
        {
            "research_objective": "Investigate the impact of different alignment methods on average toxicity in LLMs.",
            "experiment_process": "Models trained with different alignment methods, such as PPO, DPO, KTO, and IPO, are benchmarked for their toxicity levels. The comparison includes evaluating models like Gemma 7B versions, trained on human and AI preferences, across various languages and alignment techniques.",
            "result_discussion": "The results show little difference in average toxicity among models trained with different alignment methods, though AI feedback tends to lower toxicity in English completions. Human feedback better reduces toxicity in non-English languages, indicating the need for tailored alignment methods.",
            "ablation_id": "2405.09373v2.No5"
        },
        {
            "research_objective": "Assess how different data sources elicit average toxicity from LLMs.",
            "experiment_process": "The study compares Average Toxicity (AT) using naturally occurring prompts from PTP, RTP-LX, and WildChat datasets. It evaluates how these sources influence elicited toxic content in LLM generations, considering factors like prompt length and naturally occurring vs. curated data.",
            "result_discussion": "PTP elicits higher average toxicity compared to RTP-LX and WildChat. The longer prompts in PTP and more naturally occurring toxic content likely contribute to this observation. Preference-tuning seems to make models less vulnerable to user-generated inputs than naturally occurring prompts.",
            "ablation_id": "2405.09373v2.No6"
        }
    ]
}