{
    "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules",
    "abstract": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key\u2014provided by the LLM to the verifier\u2014to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks\u2014one of which has been internally implemented at OpenAI\u2014and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have emerged in recent years as a disruptive technology to generate human-like text and other media [59  ###reference_b59###, 44  ###reference_b44###, 2  ###reference_b2###]. While this reality enhances productivity in many sectors, the mismatch between ownership and generation of content could lead to several unwanted outcomes:\nExacerbating misinformation. The ability of LLMs to generate a large amount of text in parallel can be easily leveraged to exacerbate the spread of misinformation [70  ###reference_b70###, 66  ###reference_b66###], which can be enabled by deploying automated bots on social media platforms [55  ###reference_b55###]. It may also facilitate fraud and deception by pretending to be humans interacting with their relatives and acquaintances.\nFacade of AI-assisted education. LLMs impose challenges to education because students may use powerful LLMs to write essays for themselves [57  ###reference_b57###, 41  ###reference_b41###]. This deprives students of opportunities to practice their own skills and creates inequalities among students depending on the capabilities of the LLMs they access.\nData pollution. Another potential concern is that the internet will soon consist of more LLM-generated text than human-written text. If LLM-generated text is indiscriminately mixed with human-written text for training, it becomes difficult to create high-quality data for developing next-generation LLMs [48  ###reference_b48###, 54  ###reference_b54###, 17  ###reference_b17###].\nAn initial effort is to leverage specific patterns of LLM-generated text to distinguish it from human-generated text [25  ###reference_b25###, 71  ###reference_b71###, 42  ###reference_b42###]. However, this approach has become increasingly ineffective as models such as ChatGPT-4, Claude 3, Gemini 1.5 Pro, and many others have reached a level that makes it significantly difficult, if not impossible, to distinguish their generated text from human-written text [65  ###reference_b65###]. A more viable approach is to watermark text by embedding a signal into the LLM-generated text in a manner that allows the watermark to be provably detected [15  ###reference_b15###, 1  ###reference_b1###]. The property of provable detection is crucial because it allows the verifier to identify LLM-generated text for malicious purposes without relying on assumptions about the text, which may not always hold. Moreover, a reasonable watermarking scheme should be approximately unbiased, meaning it does not significantly distort the meaning or style of the original text.\nThe necessity of watermarking LLM-generated text was highlighted in the Biden administration\u2019s October 2023 executive order, which incorporated proposals for watermarking LLM-generated text and other AI-generated content [10  ###reference_b10###]. As part of the executive order, the U.S. Department of Commerce will help develop standards to watermark LLM-generated content. Accordingly, OpenAI, Google, Meta, and other tech giants have pledged to develop watermarking systems [9  ###reference_b9###].\nThis reality has made it imperative for researchers to develop watermarking methods for LLM-generated text. Within a year since 2023, numerous watermarks have been proposed [34  ###reference_b34###, 22  ###reference_b22###, 36  ###reference_b36###, 29  ###reference_b29###, 67  ###reference_b67###, 75  ###reference_b75###, 74  ###reference_b74###, 39  ###reference_b39###, 24  ###reference_b24###]. A common feature of these methods is leveraging the probabilistic nature of LLMs. In essence, these methods incorporate pseudorandomness into the text generation process of LLMs, and the coupling between the LLM-generated text and the pseudorandomness serves as a signal that can be used for detecting the watermark. The constructed signal becomes pronounced for detection only when the pseudorandom numbers are known, making it practically difficult for one to remove the watermark without access to the pseudorandom numbers [15  ###reference_b15###].\nTo understand how watermarks work for LLMs in more detail, we must first introduce the concept of \u201ctokenization\u201d in LLM text processing. Tokenization involves breaking down the text into smaller units called tokens, which are informally known as sub-words. These tokens can be words, parts of words, or even punctuation marks. For example, the sentence \u201cHello, world!\u201d when tokenized, might be split into four tokens: [\u201cHello\u201d, \u201c,\u201d, \u201c world\u201d, \u201c!\u201d]111Refer to the website https://platform.openai.com/tokenizer  ###reference_### for user-customized examples of tokenization.. An LLM generates each token sequentially by sampling from a probability distribution conditioned on prior tokens, among other things. Typically, the size of the token vocabulary is of the order of  and varies with language models. Letting  denote the vocabulary of all tokens, for example,  for the OPT-1.3B model [73  ###reference_b73###],  for the LLaMA series models [59  ###reference_b59###], and  for GPT-2 and GPT-3.5 series models [49  ###reference_b49###, 11  ###reference_b11###].\nAfter generating text in the form of a token sequence, , the (unwatermarked) LLM generates the next token  according to a multinomial distribution  on the vocabulary , satisfying222As a convention, throughout this paper three subscripts appear: , and , which correspond to step , token , and token  at step , respectively.\nWe call  the next-token prediction (NTP) distribution at step , and it depends on all prior generated tokens, the user-supplied prompt, as well as system prompts that are hidden from users [62  ###reference_b62###, 49  ###reference_b49###, 11  ###reference_b11###].\nIn contrast, a watermarked LLM generates the next token that is jointly determined by a pseudorandom variable and the NTP distribution. Let  denote the pseudorandom variable at step , which is available only to the verifier. Formally, the watermarked LLM samples a token according to the rule [29  ###reference_b29###, 67  ###reference_b67###]\nwhere  is a (deterministic) decoder function. To achieve approximate unbiasedness, we require that the probability distribution of  over the randomness333Strictly speaking,  has no randomness. However, modern cryptographic theories ensure that  behaves very much like a random variable. See more details in Section 2  ###reference_###. embodied in  is close to , conditional on all previous tokens,  [29  ###reference_b29###, 36  ###reference_b36###]. In this regard, an unbiased watermark roughly corresponds to a sampling method from multinomial distributions.\nWith the generated text now indistinguishable from that of the unwatermarked LLM, it seems at first glance hopeless to obtain provable detectability for watermarked text [52  ###reference_b52###, 72  ###reference_b72###]. Interestingly, detection can be made possible by carefully designing the decoder  to impose a coupling relationship between the token and pseudorandom variable, even without knowing the NTP predictions [15  ###reference_b15###].444We cannot use the probabilities \u2019s as the verifier in general does not have access to the LLM that generates the LLM. Moreover, the prompt for generating the text is unavailable to the verifier, hence the verifier cannot obtain  even having access to the LLM. In the following, we elaborate on this point by considering perhaps the simplest example of a watermark that achieves both unbiasedness and provable detectability.\nEnvision an LLM that involves only two tokens, 0 and 1\u2014that is, . Let  denote the NTP distribution at step , and let  be i.i.d. copies of the standard uniform random variable . Set the decoder as follows:\nThis watermark is unbiased. Intuitively, if  is large, then  is more likely to be 1 instead of 0, and vice versa. This intuition suggests using the following statistic for detecting the watermark:\nwhich measures the correlation between the tokens and pseudorandom variables. When the watermark is present, this statistic tends to be larger in distribution than when the watermark is absent. One can conclude that a watermark is detected if this statistic is above a certain threshold. Despite being intuitive, however, this statistic is ad hoc, and one cannot rule out the possibility of a better detection rule.\nTo better understand practical watermarking schemes, we turn to real-world LLMs where the size of the token vocabulary is large. As a recap, at the core of an unbiased watermark is a sampling method for multinomial distributions. Perhaps the two most common sampling methods are the Gumbel-max trick [40  ###reference_b40###, 31  ###reference_b31###] and the inverse transform [19  ###reference_b19###]. Interestingly, these two sampling methods correspond precisely to two important and representative watermarks [36  ###reference_b36###, 1  ###reference_b1###].\nLet  consist of  i.i.d. copies of . A version of the Gumbel-max trick [26  ###reference_b26###, 46  ###reference_b46###, 40  ###reference_b40###, 31  ###reference_b31###] states that\nfollows the NTP distribution . Recognizing this fact, Scott Aaronson proposed the following decoder [1  ###reference_b1###]:\nwhich is, by definition, unbiased [22  ###reference_b22###, 47  ###reference_b47###, 75  ###reference_b75###]. This watermark has recently been implemented internally at OpenAI [1  ###reference_b1###]. Aaronson suggested declaring the presence of the watermark if the following statistic is above a certain threshold: . The intuition is that, when the watermark is employed, (3  ###reference_###) implies that a token  is more likely to be selected when its associated pseudorandom number  is large. In contrast, when the text is written by a human,  would not be larger than other entries in the distribution at step . Despite being intuitive, it is worth mentioning that there are countless detection rules capable of capturing this distribution shift. In particular, it is not clear whether this detection rule is optimal in any sense.\nIt is well-known that any univariate distribution can be sampled by applying the inverse cumulative distribution function (CDF) to . Given an NTP distribution  and  that maps all tokens in  to a permutation of , consider the multinomial distribution with probability mass  at  for . The CDF of this distribution takes the form\nTaking as input , the generalized inverse of this CDF is defined as\nwhich by construction follows the multinomial distribution  after applying the permutation . Making use of this fact, [36  ###reference_b36###] proposed the inverse transform watermark with the following decoder:\nwhere the pseudorandom variable  and the permutation  is uniformly at random. By definition, this watermark is unbiased. To detect the watermark, [36  ###reference_b36###] proposed detection rules that, roughly speaking, examine the absolute difference between  and the rank of the generated token, , and sum the differences across the token sequence (see details in Section 4  ###reference_###). When the text is watermarked, the difference turns out to be small because the involved two random variables are highly dependent. Similar to the Gumbel-max watermark, the detection rules proposed by [36  ###reference_b36###] could potentially benefit from a more principled derivation to enhance their performance."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "This Paper",
            "text": "From a statistical viewpoint, a watermark scheme\u2019s performance at the detection phase is evaluated by two competing criteria. The first is how likely it would mistakenly detect human-written text as LLM-generated text, and the second is the probability that it would mistakenly classify LLM-generated text as human-written counterpart. This perspective relates watermarks to hypothesis testing, which formally calls the two aforementioned errors Type I error and Type II error, respectively. In this regard, the effectiveness of the Gumbel-max watermark and inverse transform watermark, even including the baby watermark, is not clear yet, though [22  ###reference_b22###, 47  ###reference_b47###] compared their efficiency through empirical experiments. For example, even though they come with detection rules that seem intuitive, it is not clear if they are statistically optimal in the sense of having the optimal trade-off between Type I and Type II errors. If they are not optimal, it is of interest to find a better detection rule.\nIn general, we wish to have a general and flexible framework that can guide the development of watermarks through optimizing its detection phase and assessing watermarks in a principled manner. Specifically, the challenge lies in how we can provably control the Type I error for any given watermark, considering that the NTP distributions, which are not accessible to the verifier, vary from token to token. Once Type I error control is achieved, the next question is how to evaluate the Type II error, preferably through a closed-form expression, which also hinges on the unknown NTP distributions. Having known both the Type I and Type II errors, the final step involves comparing watermark detection rules to ultimately identify the optimal detection rule based on our knowledge of the LLM.\nIn this paper, we address these questions and challenges in a unified way by making the following contributions.\nA Statistical framework for watermarks. A major contribution of this paper is a general statistical framework for developing statistically sound watermark detection rules through precisely evaluating the Type I and Type II errors from the hypothesis testing viewpoint. Under this framework, Type I error is controlled by leveraging a pivotal statistic that is distributionally independent of the NTP distributions under the null. This framework is accompanied by a technique for evaluating the asymptotic Type II error using large deviation theory, and moreover, relies on the notion of class-dependent efficiency to tackle the challenge of unknown and varying NTP distributions. Finally, this framework formulates the problem of finding the most powerful detection rule as a minimax optimization program.\nThis framework is formally developed in Section 2  ###reference_###.\nApplication to the Gumbel-max watermark. We apply our framework to Aaronson\u2019s Gumbel-max watermark in Section 3  ###reference_###. Our main finding is that Aaronson\u2019s detection rule is suboptimal in the sense that its class-dependent efficiency is relatively low. Moreover, by maximizing the class-dependent efficiency, we obtain the optimal detection rule, which admits a simple analytical expression. This optimal detection rule is shown to outperform existing rules in numerical experiments. Underlying these results is a technique that can reduce the optimality problem to a convex geometry problem for the Gumbel-max watermark, which is a contribution of independent interest to future research on watermarks.\nApplication to the inverse transform watermark. Next, we apply our framework to the inverse transform watermark in Section 4  ###reference_###. Our main finding is twofold. First, we overcome a significant challenge in applying our framework to analyze this watermark by deriving an asymptotic distribution when the text is watermarked. Second, we obtain the optimal detection rule for the inverse transform watermark in a closed-form expression by maximizing its class-dependent efficiency. Our numerical experiments corroborate its efficiency."
        },
        {
            "section_id": "1.2",
            "parent_section_id": "1",
            "section_name": "Related Work",
            "text": "The most closely related work to ours is the Gumbel-max watermark [1  ###reference_b1###] and the inverse transform watermark [36  ###reference_b36###]. In [75  ###reference_b75###, 22  ###reference_b22###], the authors introduced unbiased watermarks that are robust to probability perturbations or multi-bit processing. Other unbiased watermarks in this fast-growing line of research include [15  ###reference_b15###, 67  ###reference_b67###, 29  ###reference_b29###]. In addition, a popular example is the red-green list watermark, which splits the vocabulary into red-green lists based on hash values of previous n-grams and slightly increases the probability of green tokens embedding the watermark [33  ###reference_b33###, 34  ###reference_b34###, 74  ###reference_b74###, 39  ###reference_b39###, 12  ###reference_b12###]. In the detection phase, a high frequency of green tokens suggests the text is LLM-generated. This type of watermark is biased since the NTP distributions have been altered, thereby leading to a performance degradation of the LLM.\nIn contrast, there is much less work on the theoretical front. One exception is [30  ###reference_b30###], which conceptualized watermark detection as a problem of composite dependence testing, with the aim of understanding the corresponding statistical limit, such as the minimax Type II error and the most powerful test achieving it. A significant difference between [30  ###reference_b30###] and our work is that the former assumes the NTP distributions remain unchanged from predicting the first token to the last token. This is a very restrictive assumption from a practical perspective, as we observe from experiments that the NTP distributions indeed vary substantially.\nResearch on watermarking text has been conducted well before the advent of modern language models. This body of research focuses on watermarking text by modifying it to introduce specific patterns that are unlikely to be noticeable to readers. This includes synonym substitution [58  ###reference_b58###], syntactic restructuring [5  ###reference_b5###], and linguistic steganography [16  ###reference_b16###]. A common weakness of these approaches lies in their biasedness and vulnerability to attacks that aim to remove watermarks [13  ###reference_b13###, 76  ###reference_b76###].\nIn a different direction, many methods have been proposed to detect text generated by LLMs\u2014often not watermarked\u2014from human-written counterparts. A common feature of these methods is to examine the complete context, linguistic patterns, and other potentially revealing markers in the given text to assess whether it is likely LLM-generated. The simplest method is to build a classifier using synthetic and human text data, which is adopted by some commercial detection platforms [25  ###reference_b25###, 71  ###reference_b71###]. Another category is training-free and leverages the inherent stylistic differences between human and machine writing without specific training data, using techniques such as log probability curvature [42  ###reference_b42###, 7  ###reference_b7###], divergent n-gram analysis [69  ###reference_b69###], and intrinsic dimension estimation [60  ###reference_b60###]. However, [65  ###reference_b65###] find that most post-hoc detection methods are neither accurate nor reliable and suffer from a significant bias towards classifying the output as human-written rather than detecting LLM-generated text. Furthermore, these methods have proven fragile to adversarial attacks and biased against non-native English writers [35  ###reference_b35###, 52  ###reference_b52###, 37  ###reference_b37###]."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "A Statistical Framework for Watermark Detection",
            "text": "In this section, we formally introduce our framework that enables statistical analysis of watermarks. Our focus is on the development of effective techniques for assessing the statistical efficiency of the watermarks accompanying this framework. Henceforth, in this paper, we write  and  for the text and associated pseudorandom variables, respectively.\nMore importantly, the notion of class-dependent efficiency serves as a concrete approach to identifying the optimal score function that achieves the largest possible value of -efficiency rate. Following from (13  ###reference_###), formally, this amounts to solving the following optimization problem:\nBy viewing  as a new score function, finding the optimal  is reduced to solving the following minimax optimization program:888To highly the minimax nature of this formulation, we use  and  in place of  and , respectively.\nThe function  is convex in the score function  for any fixed , but is generally not concave in  when  is fixed. Therefore, this minimax optimization problem is generally not convex-concave, making it challenging to solve (14  ###reference_###) numerically [38  ###reference_b38###, 51  ###reference_b51###].\nInterestingly, we will show in Sections 3  ###reference_### and 4  ###reference_### that, for the Gumbel-max and inverse transform watermarks, the minimax optimization problem possesses certain structural properties that enable us to analytically identify global solutions to (14  ###reference_###).\nThe choice of the distribution class  is crucial since it follows from (13  ###reference_###) that  if . While it might be plausible to assume a fraction of NTP distributions are in a \u201cnice\u201d class, some might be outside. The following result extends Proposition 2.1  ###reference_prop1### to this practical scenario.\nAssume that at least -fraction of  are in  with the rest being in ,999We do not need to know which are in the  fraction. where  and . Then, for any , we have\nWhen there is no prior at all about the LLMs, the associated class is the -dimensional simplex, which contains all possible NTP distributions and we denote by . This distribution class includes singular distributions\u2014a distribution  such that  for some token \u2014for which the pivotal statistic has the same distribution under the null and alternative in (9  ###reference_###). Formally, this gives , that is, the class-dependent efficiency of  is zero. Taking , Proposition 2.1  ###reference_prop1### shows\nThis result implies that when only a fraction of NTP distributions are known to some extent, the Type II error is still governed by  up to a multiplicative constant . The comparison between choices of the score functions  can still be reduced to comparing the value of .\n###figure_1### In addition, the discussion above reveals that any detection rule would become powerless if nothing is known about the NTP distributions. This justifies the necessity of imposing structural assumptions on the distribution class. For , we call\nthe -regular distribution class. Accordingly, an NTP distribution  is called -regular if . This excludes singular distributions. Interestingly, -regularity is closely related to the Shannon entropy. For any -regular NTP distribution , its Shannon entropy satisfies\nwhere the first inequality follows because  for . This shows that -regularity imposes a lower bound on how much information the NTP distribution offers.\nIn practice, most NTP distributions are -regular for a proper value of .\nSee Figure 1  ###reference_### for an empirical investigation where 88.65% of the NTP distributions are 0.0001-regular, 81.36% are 0.001-regular, and 70.28% are 0.01-regular.\nThis distribution class offers a way to measure the performance of a detection rule using the class-dependent efficiency in (13  ###reference_###). For example, the -efficiency rate of the baby watermark using the statistic (2  ###reference_###) is\nA proof of this fact is deferred to Appendix B.7  ###reference_###.\nIn practice, -regularity is closely related to the temperature parameter in LLMs [3  ###reference_b3###], which is used to divide the raw output of a model\u2019s last layer by a temperature parameter before applying the softmax function. A high temperature leads to more uniform probabilities (encouraging exploration), while a low temperature makes the distribution sharper, emphasizing the most likely outcomes (encouraging exploitation)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Working Hypotheses",
            "text": "The problem of determining whether a watermark is present or not in the text can be formulated as a hypothesis testing problem [30  ###reference_b30###, 14  ###reference_b14###, 33  ###reference_b33###, 67  ###reference_b67###, 24  ###reference_b24###]:\nIn addition to the text, this hypothesis testing problem also uses the pseudorandom variables as data. A unified way to represent  based on existing constructions [67  ###reference_b67###, 33  ###reference_b33###, 34  ###reference_b34###, 47  ###reference_b47###, 22  ###reference_b22###] is to take the form\nwhere  denotes a secret key that will be passed to the verifier. The (deterministic) hash function  maps any token sequence and a key to a pseudorandom number that will be used to generate the next token. More precisely, the watermarked LLM generates the next token according to the rule\nfor some (deterministic) decoder . Recall that  denotes the probability distribution of the next token generated by the (unwatermarked) LLM.\nWith these notations in place, a watermark can be formally presented by the tuple . To lay a solid footing of watermarks on statistical hypothesis testing, we need two working hypotheses.\nIn the watermarked LLM, the pseudorandom variables  constructed above are i.i.d. copies of a random variable. Furthermore,  is (statistically) independent of .\nWorking Hypothesis 2.1  ###reference_hypo1### is grounded purely in cryptographic considerations. In cryptography, there are well-established approaches to efficiently constructing and computing the pseudorandom number as a function of text and the secret key [8  ###reference_b8###, 45  ###reference_b45###, 56  ###reference_b56###, 53  ###reference_b53###, 32  ###reference_b32###]. The pseudorandom number is very sensitive to the key, making it computationally indistinguishable from its truly random counterpart without knowledge of the key. Specifically, although  is completely determined by the prior text  (and the secret key), a run of the hash function  could effectively introduce fresh randomness, thereby making  statistically independent of . Note that, during detection, the communication cost is merely to pass the secret key to the verifier as the prior tokens are publicly available.\nHereafter, we regard  as a random variable, which allows us to formally define the unbiasedness of a watermark.\nWe say a watermark is unbiased if, for any multinomial distribution ,  follows . That is, for any NTP distribution  and token ,\nwhere the expectation is taken over the randomness embodied in . Together with the joint independence of \u2019s across the sequence of tokens, unbiasedness holds at every step conditional on prior tokens.\nOur next working hypothesis concerns the joint distribution of  and  when the text is written by a human.\nLet  be a sequence of tokens generated by a human who has no knowledge of the secret key. Then, the human-generated token  and  are (statistically) independent conditional on , for all .\nTo clear up any confusion, we remark that the verifier uses the human-generated tokens  to compute the random variables  in (5  ###reference_###). In particular, Working Hypothesis 2.1  ###reference_hypo1### remains valid for human-generated text, due to the construction of the hash function .\nIn particular, this working hypothesis shows that, for human-written text, the token is not generated according to (6  ###reference_###). The rationale of this working hypothesis is that how a human writes text is intrinsically random and cannot be captured by pseudorandomness. Therefore, the human-generated token  has nothing to do with .\nMoreover, one can also argue for this working hypothesis by recognizing that it is practically impossible for a human to generate text such that  and  are dependent because the secret key is not available. This is even the case for a different LLM without having the secret key because it is computationally infeasible to replicate the pseudorandomness without the key.\nThe two working hypotheses have been adopted by the literature, albeit not as explicit as our treatment. For example, [67  ###reference_b67###, 75  ###reference_b75###] assumed conditions on the pseudorandom hash function so that the first working hypothesis is valid.555In particular, they assume  i.i.d. for any possible . Other works directly impose distributional assumptions on some summary statistics to effectively satisfy the two working hypotheses [33  ###reference_b33###, 34  ###reference_b34###, 74  ###reference_b74###, 22  ###reference_b22###]. As a departure from these works, [36  ###reference_b36###] considered a hash function of the form . Consequently, the independence between the pseudorandom number and prior tokens follows by construction.\nHowever, it is worthwhile mentioning that this form of the hash function hampers computational efficiency in detection due to the necessity of iterating through the entire text sequence multiple times."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Pivotal Statistics",
            "text": "Understanding the hypothesis testing problem (4  ###reference_###) requires analyzing the differences between the joint distribution of  under Working Hypotheses 2.1  ###reference_hypo1### and 2.2  ###reference_hypo2###. This can be seen by first decomposing the joint probability density of \ninto a product of conditional probabilities:666As an abuse of notation,  is considered the probability density function when applied to a continuous variable, and probability mass function when applied to a discrete variable.\nUnder , Working Hypotheses 2.1  ###reference_hypo1### and 2.2  ###reference_hypo2### show that  and  are independent given . Hence,777Here, we conceptualize a human as an LLM, using a multinomial distribution to model the selection of the next token based on prior tokens.\nFor example, the display above is equal to  for the baby watermark defined in (1  ###reference_###).\nUnder ,  and  are dependent given . By Working Hypothesis 2.1  ###reference_hypo1### and the decoding construction (6  ###reference_###), we have\nbecause the decoder  is deterministic. For example, the baby watermark satisfies  if , or . It is 0 otherwise.\nBy the Neyman\u2013Pearson lemma, the most powerful test is based on the likelihood ratio:\nAlthough the likelihood ratio appears simple, taking only two values, we encounter a significant challenge in distinguishing between the two cases mentioned above. This challenge arises because the NTP distribution  is unknown and, worse, can vary with . This nuisance parameter remains unknown even if the verifier has complete access to the LLM, as the prompt used for generating the text is usually not available to the verifier.\nTo address this challenge, we seek a pivotal statistic  such that its distribution is the same for any NTP distribution  under the null. Such a pivot allows us to construct test statistics for watermark detection with known distributions and consequently obtain detection rules with provable Type I error control, though at the price of information loss compared to using the full data,  and . Formally, the original testing problem (4  ###reference_###) is reduced to the following:\nwhere  denotes the (known) distribution of  when the text is human-written, and  denotes the (unknown) distribution of  conditional on  when the text is generated by the watermarked LLM. As is clear,  is determined by the NTP distribution .\nAs a caveat, the choice of  satisfies pivotality but is useless since the alternative distribution is the same as the null distribution. A useful choice of , while being pivotal, should allow the selected token to \u201cpull\u201d the alternative distribution toward the same direction for any NTP distribution .\nFor example,  is a good choice for the baby watermark since the dependence between  and  (see (1  ###reference_###) in Section 1  ###reference_###) tends to make  larger. In general, it is a case-by-case approach to find a reasonable pivot for a given watermark (see Sections 3  ###reference_### and 4  ###reference_###).\nTo test (9  ###reference_###), it is natural to use the sum of \u2019s across the token sequence as a test statistic. To enhance the flexibility, we consider a score function  that applies to the pivot . This leads to the following rejection rule for the hypothesis testing problem:\nThat is, we reject that the text is written by a human if  is above . The threshold  is chosen to ensure significance level at : . As the null distribution of  is known, an estimator of  when the text is sufficiently long is\nwhere  and  indicate that  is used to take the expectation and variance. As can be seen, the underlying NTP distributions \u2019s are not involved in this detection rule."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Class-Dependent Efficiency",
            "text": "Once the Type I error is controlled, we seek to evaluate the Type II error and use it as a measure to ascertain which choice of score function is more desired than others. If the NTP distributions  were known and remained the same with respect to varying , the optimal score function would simply be given by the log-likelihood ratio, according to the Neyman\u2013Pearson lemma. However, this is not the case. We cope with this challenge by assuming that the NTP distributions belong to a distribution class, denoted as . The flexibility of using a distribution class lies in that one can choose a small class when much is known about the distributional properties of the LLM, and choose a large class if little is known.\nGiven a distribution class , we can evaluate the Type II error of the test statistic  over the least-favorable NTP distributions in . This gives rise to a notion of class-dependent efficiency. For any score function  and NTP distribution , we define the following moment-generating function (MGF):\nwhere  indicates that the expectation is taken over the randomness embodied in  in (9  ###reference_###). Assuming that the NTP distributions are all in , the following result delineates the Type II error in the large sample limit. We defer its proof to Appendix A  ###reference_###, which relies on techniques in large deviation theory [18  ###reference_b18###, 61  ###reference_b61###].\nAssume  for all . For any  satisfying , the Type II error of the detection rule  defined in (10  ###reference_###) obeys\nwhere  is given by\nWe call  the -dependent efficiency rate of , or simply the -efficiency rate.\nUnder a certain regularity condition, (12  ###reference_###) is tight in the sense that there exists  in  such that, if  for all , then\nfor any positive  and sufficiently large . The regularity condition is satisfied by both the Gumbel-max and inverse transform watermarks and is detailed in the remark following the proof of Theorem 2.1  ###reference_thm1### in Appendix A  ###reference_###.\nProposition 2.1  ###reference_thm1### shows the Type II error decays exponentially as long as the exponential component  is positive. The larger the value of  is, the more efficient the detection rule  is. Put differently, this class-dependent measure of efficiency reduces comparing watermark detection rules to the rate of class-dependent efficiency.\nThis efficiency rate presents a significant departure from the Bahadur efficiency [6  ###reference_b6###], though closely related. The Bahadur efficiency quantifies the rate at which the significance level (-value) of a test statistic approaches zero with increasing sample size. It emphasizes the control of Type I errors for simple hypothesis testing with i.i.d. data. In contrast, our metric  centers on quantifying the least decline of Type II errors to zero when the underlying NTP distribution  is picked up from the distribution class .\nIn short, it focuses on composite hypothesis testing where the observed data is not identically distributed.\nMore importantly, the notion of class-dependent efficiency serves as a concrete approach to identifying the optimal score function that achieves the largest possible value of -efficiency rate. Following from (13  ###reference_###  ###reference_###), formally, this amounts to solving the following optimization problem:\nBy viewing  as a new score function, finding the optimal  is reduced to solving the following minimax optimization program:888To highly the minimax nature of this formulation, we use  and  in place of  and , respectively.\nThe function  is convex in the score function  for any fixed , but is generally not concave in  when  is fixed. Therefore, this minimax optimization problem is generally not convex-concave, making it challenging to solve (14  ###reference_###  ###reference_###) numerically [38  ###reference_b38###  ###reference_b38###, 51  ###reference_b51###  ###reference_b51###].\nInterestingly, we will show in Sections 3  ###reference_###  ###reference_### and 4  ###reference_###  ###reference_### that, for the Gumbel-max and inverse transform watermarks, the minimax optimization problem possesses certain structural properties that enable us to analytically identify global solutions to (14  ###reference_###  ###reference_###).\nThe choice of the distribution class  is crucial since it follows from (13  ###reference_###  ###reference_###) that  if . While it might be plausible to assume a fraction of NTP distributions are in a \u201cnice\u201d class, some might be outside. The following result extends Proposition 2.1  ###reference_prop1###  ###reference_prop1### to this practical scenario.\nAssume that at least -fraction of  are in  with the rest being in ,999We do not need to know which are in the  fraction. where  and . Then, for any , we have\nWhen there is no prior at all about the LLMs, the associated class is the -dimensional simplex, which contains all possible NTP distributions and we denote by . This distribution class includes singular distributions\u2014a distribution  such that  for some token \u2014for which the pivotal statistic has the same distribution under the null and alternative in (9  ###reference_###  ###reference_###). Formally, this gives , that is, the class-dependent efficiency of  is zero. Taking , Proposition 2.1  ###reference_prop1###  ###reference_prop1### shows\nThis result implies that when only a fraction of NTP distributions are known to some extent, the Type II error is still governed by  up to a multiplicative constant . The comparison between choices of the score functions  can still be reduced to comparing the value of .\n###figure_2### In addition, the discussion above reveals that any detection rule would become powerless if nothing is known about the NTP distributions. This justifies the necessity of imposing structural assumptions on the distribution class. For , we call\nthe -regular distribution class. Accordingly, an NTP distribution  is called -regular if . This excludes singular distributions. Interestingly, -regularity is closely related to the Shannon entropy. For any -regular NTP distribution , its Shannon entropy satisfies\nwhere the first inequality follows because  for . This shows that -regularity imposes a lower bound on how much information the NTP distribution offers.\nIn practice, most NTP distributions are -regular for a proper value of .\nSee Figure 1  ###reference_###  ###reference_### for an empirical investigation where 88.65% of the NTP distributions are 0.0001-regular, 81.36% are 0.001-regular, and 70.28% are 0.01-regular.\nThis distribution class offers a way to measure the performance of a detection rule using the class-dependent efficiency in (13  ###reference_###  ###reference_###). For example, the -efficiency rate of the baby watermark using the statistic (2  ###reference_###  ###reference_###) is\nA proof of this fact is deferred to Appendix B.7  ###reference_###  ###reference_###.\nIn practice, -regularity is closely related to the temperature parameter in LLMs [3  ###reference_b3###  ###reference_b3###], which is used to divide the raw output of a model\u2019s last layer by a temperature parameter before applying the softmax function. A high temperature leads to more uniform probabilities (encouraging exploration), while a low temperature makes the distribution sharper, emphasizing the most likely outcomes (encouraging exploitation)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Application to the Gumbel-max Watermark",
            "text": "In this section, we apply the framework to the Gumbel-max watermark [1  ###reference_b1###]. Recall that the Gumbel-max decoder can also be written as\nAbove,  represents  i.i.d. replicates of the standard uniform random variable . As seen from (15  ###reference_###), the Gumbel-max trick ensures that this decoder is unbiased for sampling from the NTP distribution . In implementation,  can be computed using a pseudorandom hash function  that depends only on, for example, the last five tokens  in addition to the secrete key [1  ###reference_b1###].\nAs a deviation from the main focus, we are tempted to ask whether other forms of Gumbel-max-style decoders remain unbiased. Consider an unbiased decoder that takes the form\nfor some function . When the vocabulary size , writing , unbiasedness requires101010In (16  ###reference_###), it evaluates to  since . But we regard (16  ###reference_###) as an identity for any  and  such that .\nWhen there are more than two tokens in the vocabulary, a natural requirement is that, by \u201cgluing\u201d two tokens into a new token, the decoder remains unbiased for the new vocabulary that has been reduced by one in size. This new token is sampled whenever either of the two original tokens is sampled, which occurs with probability . This would be true if we have\nwhere  denotes equality in distribution and .\nBoth (16  ###reference_###) and (17  ###reference_###) are satisfied by Aaronson\u2019s choice  for any  and  such that . Interestingly, our following theorem shows that this choice is essentially unique. Its proof is deferred to Appendix B.2  ###reference_###.\nSuppose  satisfies both (16  ###reference_###) and (17  ###reference_###). Then  is unbiased if and only if\nfor a strictly increasing function .\nOur framework starts by identifying a pivot for the hypothesis testing problem (4  ###reference_###). The random number , which corresponds to the selected token  at step , is a good candidate. This is because  follows the standard uniform distribution under  while being stochastically larger under , as ensured by Working Hypotheses 2.1  ###reference_hypo1### and 2.2  ###reference_hypo2###. Indeed,  is used in [1  ###reference_b1###] for its detection rule.\nWe write . The distribution of this pivot under  is a mixture of Beta distributions, which is a classic result in the literature.\nUnder , the distribution of  given  obeys\n###figure_3### The fact that this distribution is stochastically larger than  can be gleaned from the following inequality:\nAlso, see Figure 2  ###reference_### for an illustration: there is a higher accumulation of probability mass around one under  compared to the distribution observed under .\nFrom this observation, any score function  aware of the distributional differences could be used for detecting the Gumbel-max watermarks.\nFor example, [1  ###reference_b1###] uses , while [36  ###reference_b36###, 22  ###reference_b22###] propose .\nAll of these score functions are increasing functions that assign large values to points near one.\nWhile all these choices control the Type I error, a crucial question is on what basis we compare score functions and which is the optimal one.\nTo proceed under the framework, we consider the class-dependent efficiency with distribution class  for . As one of the main findings of this paper, we obtain the score function with the highest -efficiency rate. Its proof constitutes the subject of Section 3.2  ###reference_###. Below,  denotes the greatest integer that is less than or equal to .\nLet  be defined as\nThis function gives the optimal -efficiency rates in the sense that\nfor any measurable function . Moreover,  is attained at the following least-favorable NTP distribution in :\nTo clear off any confusion, the second conclusion amounts to saying that  is a saddle point solution to the minimax problem (14  ###reference_###). Explicitly,  maximizes  in (14  ###reference_###) if and only if  can be obtained by permuting the coordinates of .\n###figure_4### The significance of Theorem 3.2  ###reference_thm2### lies partly in the closed-form nature of the optimal score function, thereby making it easy to use in detecting LLM-generated text. The left panel of Figure 3  ###reference_### shows  for some values of . By the Neyman\u2013Pearson lemma, the optimal score function and least-favorable distribution are related via . The left panel of Figure 4  ###reference_### shows the efficiency  as a function of . This function is not smooth when  for integer  because the support of  jumps at these values.\nAs an interesting fact, the least-favorable distribution  and its permuted counterparts form all the vertices of  (see Lemma 3.4  ###reference_lem4### in Section 3.2  ###reference_###). Moreover, the vertices are the closest NTP distributions in  to singular distributions. Roughly speaking, the closer to singular distributions, the more difficult it is to test apart the two hypotheses in (9  ###reference_###). To see this, note that the testing problem becomes most difficult when the NTP distribution is singular, which makes the null and alternative distributions of the pivot indistinguishable.\nWhile it is now known that  is optimal in the sense of -efficiency, it remains interesting to analyze the relative efficiency of other existing rules. In addition to the aforementioned  and , we consider  for a threshold , say, .111111According to the -efficiency defined in (25  ###reference_###),  achieves the largest -efficiency when . See Lemma C.7  ###reference_lem7### for details. This indicator function is included as it is perhaps the simplest score function detecting the distributional differences between  and .\nBy making use of Theorem 2.1  ###reference_thm1### within our framework, we compare these four detection rules in terms of -efficiency in the theorem below.\n###figure_5### There exists an absolute constant  such that the following two statements hold:\nWhen ,  has higher -efficiency than both  and , that is,\nWhen ,  has higher -efficiency than both  and , that is,\nThe constant  is the unique root of the equation  for . Numerical evaluation shows . A proof of Theorem 3.3  ###reference_thm3### can be found in Appendix B.5  ###reference_###.\nFigure 4  ###reference_### illustrates Theorem 3.3  ###reference_thm3### by presenting the comparisons under our statistical framework. Notably, for small values of ,  can be close to 1. In this regime,  is close to  in distribution, making the detection efficiency relatively low regardless of the rule used. Conversely, a large value of  implies all the token probabilities tend to be small, leading to a significant discrepancy between  and . Accordingly, all detection rules have efficiency rates tending to infinity.\nWhile  achieves the highest -efficiency rate across the entire range of , the relative performance of  and  depends on the value of . From an empirical perspective,  is observed to outperform  in common settings of LLMs [36  ###reference_b36###, 22  ###reference_b22###]. This is because the largest token probability in NTP distributions is, by and large, close to 1. To appreciate this fact, we experiment on ChatGPT-3.5-turbo with twenty prompts (see Appendix D.3  ###reference_### for details) and track the largest token probabilities across the generation of token sequences. The results are presented in Figure 1  ###reference_###, which shows that, for example, 56.85% of the NTP distributions have the largest token probability above . This is the regime where  is superior to , as predicted by our Theorem 3.3  ###reference_thm3###.\nNevertheless,  can be large when the LLM generation is highly stochastic.\nIn this regime,  slightly outperforms , but the gap soon diminishes to zero as  tends to 1, as seen from Figure 4  ###reference_###. Similarly,  has a diminishing suboptimality compared with the optimal  in the sense that . We prove the limiting behaviors of these score functions in Appendix B.6  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Main Results",
            "text": "Our framework starts by identifying a pivot for the hypothesis testing problem (4  ###reference_###  ###reference_###). The random number , which corresponds to the selected token  at step , is a good candidate. This is because  follows the standard uniform distribution under  while being stochastically larger under , as ensured by Working Hypotheses 2.1  ###reference_hypo1###  ###reference_hypo1### and 2.2  ###reference_hypo2###  ###reference_hypo2###. Indeed,  is used in [1  ###reference_b1###  ###reference_b1###] for its detection rule.\nWe write . The distribution of this pivot under  is a mixture of Beta distributions, which is a classic result in the literature.\nUnder , the distribution of  given  obeys\n###figure_6### The fact that this distribution is stochastically larger than  can be gleaned from the following inequality:\nAlso, see Figure 2  ###reference_###  ###reference_### for an illustration: there is a higher accumulation of probability mass around one under  compared to the distribution observed under .\nFrom this observation, any score function  aware of the distributional differences could be used for detecting the Gumbel-max watermarks.\nFor example, [1  ###reference_b1###  ###reference_b1###] uses , while [36  ###reference_b36###  ###reference_b36###, 22  ###reference_b22###  ###reference_b22###] propose .\nAll of these score functions are increasing functions that assign large values to points near one.\nWhile all these choices control the Type I error, a crucial question is on what basis we compare score functions and which is the optimal one.\nTo proceed under the framework, we consider the class-dependent efficiency with distribution class  for . As one of the main findings of this paper, we obtain the score function with the highest -efficiency rate. Its proof constitutes the subject of Section 3.2  ###reference_###  ###reference_###. Below,  denotes the greatest integer that is less than or equal to .\nLet  be defined as\nThis function gives the optimal -efficiency rates in the sense that\nfor any measurable function . Moreover,  is attained at the following least-favorable NTP distribution in :\nTo clear off any confusion, the second conclusion amounts to saying that  is a saddle point solution to the minimax problem (14  ###reference_###  ###reference_###). Explicitly,  maximizes  in (14  ###reference_###  ###reference_###) if and only if  can be obtained by permuting the coordinates of .\n###figure_7### The significance of Theorem 3.2  ###reference_thm2###  ###reference_thm2### lies partly in the closed-form nature of the optimal score function, thereby making it easy to use in detecting LLM-generated text. The left panel of Figure 3  ###reference_###  ###reference_### shows  for some values of . By the Neyman\u2013Pearson lemma, the optimal score function and least-favorable distribution are related via . The left panel of Figure 4  ###reference_###  ###reference_### shows the efficiency  as a function of . This function is not smooth when  for integer  because the support of  jumps at these values.\nAs an interesting fact, the least-favorable distribution  and its permuted counterparts form all the vertices of  (see Lemma 3.4  ###reference_lem4###  ###reference_lem4### in Section 3.2  ###reference_###  ###reference_###). Moreover, the vertices are the closest NTP distributions in  to singular distributions. Roughly speaking, the closer to singular distributions, the more difficult it is to test apart the two hypotheses in (9  ###reference_###  ###reference_###). To see this, note that the testing problem becomes most difficult when the NTP distribution is singular, which makes the null and alternative distributions of the pivot indistinguishable.\nWhile it is now known that  is optimal in the sense of -efficiency, it remains interesting to analyze the relative efficiency of other existing rules. In addition to the aforementioned  and , we consider  for a threshold , say, .111111According to the -efficiency defined in (25  ###reference_###  ###reference_###),  achieves the largest -efficiency when . See Lemma C.7  ###reference_lem7###  ###reference_lem7### for details. This indicator function is included as it is perhaps the simplest score function detecting the distributional differences between  and .\nBy making use of Theorem 2.1  ###reference_thm1###  ###reference_thm1### within our framework, we compare these four detection rules in terms of -efficiency in the theorem below.\n###figure_8### There exists an absolute constant  such that the following two statements hold:\nWhen ,  has higher -efficiency than both  and , that is,\nWhen ,  has higher -efficiency than both  and , that is,\nThe constant  is the unique root of the equation  for . Numerical evaluation shows . A proof of Theorem 3.3  ###reference_thm3###  ###reference_thm3### can be found in Appendix B.5  ###reference_###  ###reference_###.\nFigure 4  ###reference_###  ###reference_### illustrates Theorem 3.3  ###reference_thm3###  ###reference_thm3### by presenting the comparisons under our statistical framework. Notably, for small values of ,  can be close to 1. In this regime,  is close to  in distribution, making the detection efficiency relatively low regardless of the rule used. Conversely, a large value of  implies all the token probabilities tend to be small, leading to a significant discrepancy between  and . Accordingly, all detection rules have efficiency rates tending to infinity.\nWhile  achieves the highest -efficiency rate across the entire range of , the relative performance of  and  depends on the value of . From an empirical perspective,  is observed to outperform  in common settings of LLMs [36  ###reference_b36###  ###reference_b36###, 22  ###reference_b22###  ###reference_b22###]. This is because the largest token probability in NTP distributions is, by and large, close to 1. To appreciate this fact, we experiment on ChatGPT-3.5-turbo with twenty prompts (see Appendix D.3  ###reference_###  ###reference_### for details) and track the largest token probabilities across the generation of token sequences. The results are presented in Figure 1  ###reference_###  ###reference_###, which shows that, for example, 56.85% of the NTP distributions have the largest token probability above . This is the regime where  is superior to , as predicted by our Theorem 3.3  ###reference_thm3###  ###reference_thm3###.\nNevertheless,  can be large when the LLM generation is highly stochastic.\nIn this regime,  slightly outperforms , but the gap soon diminishes to zero as  tends to 1, as seen from Figure 4  ###reference_###  ###reference_###. Similarly,  has a diminishing suboptimality compared with the optimal  in the sense that . We prove the limiting behaviors of these score functions in Appendix B.6  ###reference_###  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Proof of Theorem 3.2",
            "text": "The proof of Theorem 3.2  ###reference_thm2### relies on the minimax formulation (14  ###reference_###) in our framework to solve for the optimal score function. As mentioned earlier, however, this task is generally difficult since  is not convex-concave. We circumvent this obstacle by introducing a simple yet remarkable result that characterizes the MGF (11  ###reference_###) associated with the Gumbel-max watermark.\nFor any non-decreasing function , the functional121212We consider a slightly different version of the MGF by setting  and taking as input  in (11  ###reference_###).\nis convex in .\nThis convexity lemma is the key to the proof of Theorem 3.2  ###reference_thm2### and is a contribution of independent interest to future work on LLM watermarks. Roughly speaking, it helps reduce the \u201cmax\u201d part in the minimax problem (14  ###reference_###) to the problem of identifying vertices of the distribution class.\nTechnically speaking, the role of the convexity lemma in the proof of Theorem 3.2  ###reference_thm2### is through the following lemma.\nFor any non-decreasing function , we have\nA direct consequence of Lemma 3.3  ###reference_lem3### is that\nNow we prove Lemmas 3.2  ###reference_lem2### and 3.3  ###reference_lem3### in order.\nBy Lemma 3.1  ###reference_lem1###, the CDF of  with NTP distribution  takes the form\nThis function relates to  since\nFirst, we show that  is a convex function for any given . This is demonstrated by showing that the Hessian matrix of  is positive semidefinite for any  within the interior of its domain :\nNote the Hessian is diagonal with all non-negative entries, confirming the convexity of .\nSecond, we examine the functional form of  through integration by parts, which yields:\nThis equation expresses  as a nonnegative weighted sum of  evaluated over .\nGiven the established convexity of , and considering that a nonnegative weighted sum of convex functions remains convex, it follows directly that  is convex. This completes the proof of Lemma 3.2  ###reference_lem2###.\n\u220e\nOur proof is based on a fundamental principle in convex analysis: the supremum\nof a convex function over a compact convex set in an Euclidean space is necessarily attained at an extreme point of the convex set [28  ###reference_b28###].\nFix a non-decreasing function . The mapping  is shown to be convex in  by Lemma 3.3  ###reference_lem3###. Additionally,  is convex. Thus the supremum of the convex function  must be necessarily attained at an extreme point of the convex constraint set . By Lemma 3.4  ###reference_lem4###, the extreme points of the set  equal to all possible permutations of .\nThe set of extreme points of , denoted by , is given by\nwhere  denote the permuted NTP distribution whose th coordinate is .\nNote there is a permutation invariance in the objective:  holds for every permutation  on  and every vector . As a consequence, we can explicitly compute the supremum of  on the convex set  as follows:\nThis proves equation (20  ###reference_###). The remaining statement in Lemma 3.3  ###reference_lem3### then follows.\n\u220e\nWe conclude this section by proving Theorem 3.2  ###reference_thm2###.\nThe second part of this theorem follows directly from the convexity lemma (Lemma 3.2  ###reference_lem2###) and Lemma 3.4  ###reference_lem4###.\nTo prove the first part, we work on the minimax formulation (14  ###reference_###) and begin by recognizing that\nwhere second equality is due to the Donsker\u2013Varadhan representation [20  ###reference_b20###] and  denotes the Kullback\u2013Leibler divergence. Note that Lemma 3.3  ###reference_lem3### can not be applied here because the minimization function  is not necessarily non-decreasing.\nDue to the uniqueness of the Donsker\u2013Varadhan representation,  is strictly larger than  unless we take the log-likelihood ratio\nwhich is non-decreasing in . In this case, we get\nwhere the second equality follows from Lemma 3.3  ###reference_lem3### and the third equality is ensured by, again, the Donsker\u2013Varadhan representation.\nTaken together, (21  ###reference_###) and (22  ###reference_###) show that  is the unique score function that solves the minimax problem (14  ###reference_###) with .\u220e"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Application to the Inverse Transform Watermark",
            "text": "In this section, we apply the framework to the\ninverse transform watermark [36  ###reference_b36###]. Without loss of generality, below we take .\nRecall that its decoder is defined as\nwhere  with  and  being sampled uniformly at random from all permutations on . Following the strategy in [47  ###reference_b47###, 36  ###reference_b36###], \u2019s are jointly independent across the token sequence. For a given permutation ,  above denotes the CDF under permutation:\nBy construction, the inverse transform watermark is unbiased for sampling from the NTP distribution . To see the unbiasedness directly, for any token , note that  if and only if\nwhich occurs with probability  since the interval above has length .\nTo continue under our framework, we are faced with a difficulty in characterizing the distribution of the  under the alternative. Although it is clear from Figure 5  ###reference_### that  tends to be small under the alternative, the complex dependence of \u2019s distribution on the unknown NTP  under  renders the evaluation of class-dependent efficiency in the framework generally intractable.\nFormally, this technical challenge can be elucidated by Lemma 4.1  ###reference_lem1### and its following remark.\nUnder , the CDF of  is\nUnder , the CDF of  is\nwhere  collects all permutations on , ,  and  represents the length of an interval.\nThe first part of Lemma 4.1  ###reference_lem1### in particular shows that  is a pivotal statistic under . The second part of Lemma 4.1  ###reference_lem1### provides the explicit CDF of  under  conditional on .\nNonetheless, Lemma 4.1  ###reference_lem1### also shows that the distribution formula for  is by no means simple under the alternative hypothesis : the dependence on  is intricate. The intricacy arises due to the nature of , whose definition involves all permutations  of , which also complicates the distribution formula for  where . Such complexity in the distribution formula poses significant technical challenges in evaluating the effectiveness of any test procedure under the alternative hypotheses .\nConsidering these intricacies, our goal of deriving an optimal score function  based on  seems challenging at first glance.\nFortunately, we discover a surprising result that simplifies the understanding of the distribution of  under the alternative , which facilitates later derivation of the optimal score function . Roughly speaking, the conditional distribution of  given  under  primarily depends on the largest values among \u2019s coordinates, especially when the vocabulary size  is large.\nFor illustration, histograms in Figure 5  ###reference_### compare  under  for different  and  scenarios where . Despite differences in  and , each sharing the same top five coordinates but differing elsewhere, the conditional distribution of  remains remarkably similar.\nWe now mathematically formalize this empirical observation under a special case where there is one token that takes predominantly high probability in . This scenario, being the most basic case in theory, offers practical insights as empirically, many LLMs often concentrate most of \u2019s probability mass on a single token [23  ###reference_b23###]. Through our analysis, we derive the limiting distribution of  given  when the vocabulary size . Formally, this means that we are effectively examining a sequence of , which is indexed by , and investigate the limiting conditional distribution of  given  as we move along that sequence. Yet when the context makes it clear, we suppress the dependence of  on the index  for notation simplicity.\nIn below, we always use  to refer to the -th largest coordinates of  for every vector  and integer  between  and .\nUnder ,\nUnder , assuming that  and  hold, then\nTheorem 4.1  ###reference_thm1### formalizes this observed phenomenon, providing a simplified distribution characterization of the statistics  under , assuming that the token probability  is highly concentrated at a single token, under the large vocabulary size limit . The limiting distribution is determined by a single scalar  that corresponds to the top token probability.\nNotably, in this limit , the statistics  has a different support under  and under .\nBy exploiting this distinction, we proceed to identify the optimal score  under our framework, which achieves infinite power in the limit  in distinguishing the hypotheses  and .\nFollowing our previous discussions, we focus on scenarios where an LLM\u2019s probability distribution, , is primarily concentrated on a single token.\nWe model  using a belief class , a subset of the -regular class . Unlike , which only restricts the highest token probability to , the belief class  also sets a limit on the second highest token probability, using a threshold :131313For simplicity, we omit the notation indicating the dependence of  on  when the context makes this clear.\nIn our subsequent asymptotic analysis , we assume that , from which we can utilize Theorem 4.1  ###reference_thm1### to simplify our distributional characterization of .\nThis leads to the definition of a limit efficiency measure, , whose definition is based on our efficiency measure  under our framework in Section 2  ###reference_###:\nwhere  denotes the large vocabulary size limit where  and . In plain words,  quantifies the efficiency of any score function  in the limit of large vocabulary size , assuming that the distribution of the underlying LLM predominantly focuses on a single token.\nWe are ready to describe the optimal score function  that maximizes this efficiency measure .\nThis optimal procedure is formally stated in Theorem 4.2  ###reference_thm2###.\nFix . Let  denote\nThen,\nTheorem 4.2  ###reference_thm2### identifies the score function  that achieves infinite power in distinguishing  from  in the large vocabulary limit , assuming that the NTP distribution  focuses on a single token, formally described by  with . The right panel of Figure 3  ###reference_### shows  for different values of .\nOur derivation of  is based on calculating a log-likelihood ratio between the hardest alternative within  and the null, using the asymptotic distribution formula described in Theorem 4.1  ###reference_thm1###. The log-likelihood ratio  achieves infinite power due to  having distinct supports under  and  in the limit, as demonstrated in Theorem 4.1  ###reference_thm1###.\nFinally, the truncation in (26  ###reference_###) is mainly for technical reasons, stemming from a proof artifact."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Proof of Theorem 4.2",
            "text": "Fix .\nTo prove Theorem 4.2  ###reference_thm2###,\nour goal is to establish the limit:\nOur strategy involves establishing an effective lower bound for , as detailed\nin Lemma 4.2  ###reference_lem2###. This involves introducing a function\n for each , defined as:\nBy Theorem 4.1  ###reference_thm1###, this corresponds to the limiting\nCDF of  given , assuming that .\nFor any function  that is non-decreasing, Lipschitz-continuous on , there is the lower bound:\nThe proof of Lemma 4.2  ###reference_lem2### is deferred to\nSection 4.2.1  ###reference_.SSS1###.\nNow we finish the proof of Theorem 4.2  ###reference_thm2###. Let  denote the RHS of inequality (27  ###reference_###).\nAlso, let  denote the probability measure associated\nwith the CDF .\nBy Donsker\u2013Varadhan representation [20  ###reference_b20###], we deduce:\nwhere the last identity holds because\nthe two probability measures  and  have different support, with  supported on  and  on  for .\nThis supremum  is obtained for  as the log-likelihood ratio:\nDespite , we are not able to use Lemma 4.2  ###reference_lem2###\nto conclude that . This is because the function ,\ndespite being non-increasing, is neither uniformly bounded nor Lipschitz-continuous on ,\nthus not meeting the requirements of Lemma 4.2  ###reference_lem2###.\nNonetheless, after truncating  to , the function  is\nnon-decreasing, Lipschitz-continuous, and uniformly bounded on  for any\n. This allows us to use Lemma 4.2  ###reference_lem2###\nto conclude that for every :\nApplying the limit  on both sides, and\nleveraging Lebesgue\u2019s dominated convergence theorem along with Fatou\u2019s lemma, we deduce:\nThis yields Theorem 4.2  ###reference_thm2### as desired."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Proof of Lemma 4.2",
            "text": "We are interested in lower bounding . Recall that its definition is given by:\nOur lower bound strategy relies on first connecting  to an intermediate quantity:\nwhere . This connection is given in Lemma 4.3  ###reference_lem3###.\nFor every function  Lipschitz-continuous on , we have:\nLemma 4.3  ###reference_lem3### implies that maximizing  is related to the following minimax optimization:\nThis minimax formulation is closely related to our original minimax formulation (14  ###reference_###), yet it replaces the -dimensional variable  with a scalar  after taking asymptotics (Theorem 4.1  ###reference_thm1###).\nBy definition, we have\nAfter we substitute this inequality into (28  ###reference_###),\nwe obtain a lower bound:\nwhere the second identity holds because  by definition.\nWe wish to swap the order of  and  in (29  ###reference_###) to reach the conclusion of Lemma 4.3  ###reference_lem3###. Lemma 4.4  ###reference_lem4### justifies such a swap as valid, whose proof is deferred to Appendix C.4  ###reference_###.\nFor any Lipschitz-continuous function  on , we have\nBy swapping  and  in (29  ###reference_###), we obtain\nThis completes the proof of Lemma 4.3  ###reference_lem3###.\n\u220e\nLemma 4.3  ###reference_lem3### reduces the problem of finding a lower bound on  to evaluating . To do so, we first derive an explicit expression for .\nFor any Lipschitz-continuous  on , we have\nThe proof of Lemma 4.5  ###reference_lem5### is deferred to Appendix C.4  ###reference_###. Here we give some intuition behind its proof. Theorem 4.1  ###reference_thm1### states that as , for any sequence of  indexed by , the CDF of  under  converges to , while its CDF under  given  converges to . This results in a convergence statement that holds for any sequence of , and for any continuous function  uniformly bounded on :\nLemma 4.5  ###reference_lem5### strengthens this by ensuring such convergence can be made uniform across the sequence:\nThe caveat is that the above, uniform limit applies only to functions  that are Lipschitz-continuous and uniformly bounded on , as required in Lemma 4.5  ###reference_lem5###.\nFinally, with Lemma 4.5  ###reference_lem5### we are able to derive an explicit\nexpression of the supremum .\nFor any non-increasing and Lipschitz-continuous  on :\nUsing integration by parts, we first obtain the following equation:\nAs a consequence, the above integral\u2019s value is non-increasing as  increases because  is non-decreasing in  for any  within the interval .\nBy Lemma 4.5  ###reference_lem5###, this further implies that  is\nnon-decreasing in . Thus, . The conclusion of Lemma 4.6  ###reference_lem6### then follows.\n\u220e\nWe now finish the proof of Lemma 4.2  ###reference_lem2###. By Lemma 4.3  ###reference_lem3### and Lemma 4.6  ###reference_lem6###, we obtain\nThis yields the lower bound as desired."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "This section highlights our framework\u2019s effectiveness through synthetic and real-data experiments, showcasing the practical utility of our proposed methods for detecting watermarks."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Synthetic Studies",
            "text": "In our simulation, we generate a vocabulary  of size  and assess Type I and II errors in watermark detection methods for generated token sequences. These methods include , , ,  for Gumbel-max watermark and ,  for the inverse transform watermark.\n###figure_10### Our initial investigation is on Type I error control for finite token sequences, recognizing that\nthe guarantees for Type I error control follow from the central limit theorem and hold asymptotically when the sequence length . For a given text length , we generate  samples of unwatermarked word tokens sequences. We uniformly sample each unwatermarked token from the vocabulary  in our experiments. Although different sampling strategies could be employed, due to the pivotal property of our test statistics, we anticipate that they would lead to similar Type I error rates.\nThroughout  repeated experiments, we compute the average Type I error, with the findings presented in the leftmost column of Figure 6  ###reference_###. These results reveal that empirically, the Type I errors generally align with the nominal level of , fluctuating between  and . This performance confirms our theoretical expectations regarding Type I error control across most scenarios. The only exception is the score function , achieving close to the nominal  level only for token sequences over 300 in length, where asymptotic effects emerge.\nNext, we examine the effectiveness of these methods in controlling Type II errors on watermarked sequences. Performing this examination involves specifying how each  is generated under .\nIn our simulation, we model LLM\u2019s NTP () distributions as spike distributions: we set its largest probability as , where  is i.i.d. sampled from  with ,141414This small number  is used to avoid  merging with . and , and uniformly distribute the remaining probabilities so that  for . The choice of this setup is based on our theoretical development. We also tested alternative  values of , , and , but given the results are similar, we will include those details in the appendix.\nThroughout  repeated experiments, we compute the average Type II error and present the results in the rightmost column of Figure 6  ###reference_###.\nDesigned using the optimality criterion through our framework, both  for Gumbel-max and  for inverse transform outperform other watermark detection techniques in reducing Type II errors.\nAdditionally, empirical performance rankings are consistent with our theoretical prediction in Theorem 3.3  ###reference_thm3###:  has lower Type II error, followed by , with  having the highest Type II error."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Real-World Examples",
            "text": "###figure_11### In this section, we conduct an empirical analysis of watermark detection methods for text sequences from large token generation models such as OPT-1.3B [73  ###reference_b73###] and Sheared-LLaMA-2.7B [68  ###reference_b68###]. Specifically, we evaluate Type I errors on the unwatermarked texts directly from large token generation models and Type II errors with texts from the same models, but with watermarks incorporated. This experimental setup essentially follows the approach described in the existing literature [33  ###reference_b33###] and is further detailed in Appendix D.2  ###reference_### for completeness.\nWe present our numerical results with the OPT-1.3B models in Figure 7  ###reference_### (similar results for Sheared-LLaMA-2.7B are documented in Appendix D.2  ###reference_###). Notably, both the average Type I error (leftmost column of Figure 7  ###reference_###) and Type II error (rightmost column of Figure 7  ###reference_###) curves are computed based on  repeated experiments. Though the setup (mainly the way how each  is generated) is slightly different from our simulation studies, the conclusions are quite similar.\nFirst, in most scenarios, all the detection methods maintain Type I errors within  to , aligning well with the expected  level, particularly in texts over  tokens long, as observed in our numerical experiments. Second, our methods,  for Gumbel-max and  for inverse transform, excel in minimizing Type II errors over other score functions. For Gumbel-max watermarks, the performance rankings follows our Theorem 3.3  ###reference_thm3###\u2019s predictions:  records a lower Type II error, then , with  the highest type II error."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this paper, we have introduced a statistical framework for reasoning about watermarks for LLMs from a hypothesis testing viewpoint. Our framework offers techniques and concepts for analyzing the asymptotic test efficiency of watermark detection by evaluating Type I and Type II errors of the problem of watermark detection. Specifically, to provide a formal approach to comparing different detection rules, we have introduced class-dependent efficiency, which further reduces the comparison problem to a minimax optimization problem. We have applied this framework to two representative unbiased watermarks, namely, the Gumbel-max watermark and the inverse transform watermark. Our results include the derivation of optimal detection rules, which are shown to have competitive or superior performance compared to existing detection rules.\nOur work opens up several research directions in this emerging line of watermarks for LLMs. Perhaps the most immediate question is how to choose  for the distribution class  in a manner adaptive to the LLM and specific problem instance. Another approach is to extend Proposition 2.1  ###reference_prop1### to multiple distribution classes in recognition of the heterogeneity of NTP distributions across the token sequence. This can be done, for example, by adaptively clustering the empirical distributions of  into several clusters and using these to represent the distribution classes. Moreover, it would be a welcome advance to offer guidelines on how to choose pivotal statistics, which perhaps would allow the framework to maximize its utility since in this paper we basically fix the choice of pivotal statistics at the beginning. Finally, given that an unbiased watermark is probabilistically similar to sampling from a multinomial distribution, it would be of interest to consider other sampling schemes such as the alias method [63  ###reference_b63###, 64  ###reference_b64###] to develop new categories of watermarks.\nMore broadly, we wish to remark on several possible extensions of our framework for improving watermark detection and design. Our framework would be enhanced if different watermarks with their own detection rules could be compared on a fair and comprehensive basis. Different watermarks might work best in different regimes and, therefore, directly comparing their class-dependent efficiency rates would be very sensitive to the choice of the distribution class. Another direction to extend our framework is to take distribution classes that are adaptive to NTP distributions in the real-world deployment of LLMs. This adaptivity has the potential to make the minimax formulation (14  ###reference_###) better capture the empirical performance. A possible approach is to model the empirical distribution of the NTP distributions  using Zipf\u2019s law [77  ###reference_b77###] or other laws. A challenge here, however, might arise from the fact that the distribution class is no longer permutation invariant. Finally, an interesting extension is to have varying score functions across the token sequence, as opposed to having the same  in the test statistic (10  ###reference_###). This flexibility might enhance the power of watermark detection by taking into account the heterogeneity of the NTP distributions from token to token in generation."
        }
    ],
    "url": "http://arxiv.org/html/2404.01245v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1",
            "1.2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "2",
            "3",
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2404.01245v1",
        "paper_title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules",
        "research_background": "### Paper's Motivation\n\nThe motivation behind this paper stems from several pressing concerns related to the use of large language models (LLMs):\n1. **Misinformation:** The ability of LLMs to generate vast amounts of text quickly could exacerbate the spread of misinformation, especially through automated bots on social media platforms.\n2. **Fraud and Deception:** LLMs can also facilitate fraud and deception by impersonating humans online.\n3. **Challenges in Education:** Students might use LLMs to write essays, depriving them of practice and creating inequalities based on their access to these models.\n4. **Data Pollution:** As more LLM-generated text floods the internet, training future LLMs could become challenging due to the mixture of human and machine-generated content.\n\n### Research Problem\n\nThe core research problem addressed in the paper is the development of effective watermarking methods to distinguish LLM-generated text from human-written text. This is crucial because existing pattern-based approaches to detect LLM-generated content have become increasingly ineffective as newer models produce text nearly indistinguishable from human-written text. The paper aims to create watermarking techniques that allow for **provable detection**, maintain **unbiasedness** (i.e., they do not significantly distort the text), and are robust against unauthorized removal.\n\n### Relevant Prior Work\n\n1. **Pattern-based Detection Ineffectiveness:** Earlier efforts in distinguishing LLM-generated content based on specific patterns have been rendered ineffective due to the high quality of text generated by advanced models like ChatGPT-4 and others [65].\n  \n2. **Watermarking Techniques:** Several watermarking methods have been proposed in the literature, leveraging the probabilistic nature of LLMs. These methods incorporate pseudorandomness into the text generation process, with the watermark being detectable only when the pseudorandom numbers are known [34, 22, 36, 29, 67, 75, 74, 39, 24, 15].\n\n3. **Tokenization and Sampling Methods:** The common ground for unbiased watermarking involves sampling methods for multinomial distributions. The methods discussed include the Gumbel-max trick and the inverse transform, which are foundational for creating watermarks in LLMs [40, 31, 19].\n\n4. **Existing Watermark Schemes:** Notable schemes include Aaronson's watermark implemented at OpenAI, employing the Gumbel-max trick [1], and another method using the inverse transform proposed by [36]. Both have intuitive detection rules, although there\u2019s room for more principled derivations to enhance performance.\n\nBy synthesizing and building upon these prior works, the paper aspires to establish a statistical framework for watermarking in LLMs that ensures high detection efficiency and robustness, while maintaining the naturalness of the generated text.",
        "methodology": "**Methodology:**\n\nIn this section, the authors introduce a comprehensive framework designed for the statistical analysis of watermarks in Large Language Models (LLMs). The primary goal is to develop techniques that effectively assess the statistical efficiency of these watermarks. The text and associated pseudorandom variables are represented by \\( x \\) and \\( a \\), respectively.\n\nThe framework emphasizes the concept of class-dependent efficiency, which helps identify the optimal score function that maximizes the \\(\\alpha\\)-efficiency rate. This involves solving an optimization problem that, when observing \\( S(x) \\) as a new score function, translates into a minimax optimization problem. Here, \\( \\varphi \\) and \\( \\epsilon \\) replace some original variables to highlight the minimax essence of the problem.\n\nThe function \\( f \\) is convex in the score function \\( s \\) for any fixed \\( \\pi \\), but generally not concave in \\( \\pi \\) when \\( s \\) is fixed. Consequently, this makes the minimax optimization problem challenging to solve as it is typically not convex-concave.\n\nThe choice of the distribution class \\( \\mathcal{G} \\) is critical. If the distributions fall within a 'nice' class \\( \\mathcal{G}_0 \\), efficiency can be achieved. However, in practical scenarios, not all NTP distributions adhere to this class. The paper generalizes Proposition 2.1 to account for scenarios where only a fraction of distributions belong to \\( \\mathcal{G}_0 \\), with the remainder in \\( \\mathcal{G}_1 \\).\n\nThe framework assumes a mixed distribution scenario where at least a \\(\\rho\\)-fraction of distributions are in \\( \\mathcal{G}_0 \\) and the rest in \\( \\mathcal{G}_1 \\). This scenario leads to potential Type II errors governed by the \\(\\alpha\\)-efficiency rate up to a constant.\n\nWhen there is no prior knowledge about the LLMs, the associated distribution class is the \\(d\\)-dimensional simplex, denoted by \\( \\Delta^d \\). This broad class includes singular distributions, where the pivotal statistic has a null and alternative distribution equivalence, resulting in zero class-dependent efficiency.\n\nThe practical aspect of imposing structural assumptions on the distribution class is highlighted, with \\(\\gamma\\)-regular distribution class being a key component. An NTP distribution is termed \\(\\gamma\\)-regular if it avoids singular distributions, and \\(\\gamma\\)-regularity correlates with a lower bound on the Shannon entropy, indicating the amount of information the NTP distribution possesses.\n\nThe document concludes by linking \\(\\gamma\\)-regularity to the temperature parameter in LLMs, which influences the sharpness and exploration characteristics of the model's output probabilities.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Framework Application\nThe primary focus of the main experiment is the application of the proposed statistical framework to the Gumbel-max watermark. The Gumbel-max decoder implemented is unbiased for sampling from the Next Token Probability (NTP) distribution.\n\n#### Implementation Details\n- The decoder uses pseudorandom hash function \\( H \\) depending on the last five tokens and a secret key.\n- The experiment introduces a pivot for hypothesis testing, where the random number \\( U_{t,J_{t}} \\) corresponding to the selected token at step \\( t \\) is identified as a suitable pivot due to its properties under the null and alternative hypotheses.\n\n#### Evaluation Metrics\n- **Score Functions:** These include \\( g(x) = x \\), \\( S_{1-\\alpha}(x) = (x-\\alpha)1\\{x>\\alpha\\} \\), and additional score functions that are increasing and assign larger values to points nearer to one.\n- **Class-dependent efficiency (\\(\\epsilon_{C}\\))**: This metric is used to compare score functions.\n- **Optimal \\( \\epsilon_{C}\\)-efficiency rate:** This is derived to find the best-performing score function under different conditions.\n\n#### Main Experimental Results\n\n1. **Optimal Score Function:**\n   - Derived the optimal score function within the proposed framework as \\( g^*(x) \\).\n   - Theorem 3.2 establishes that \\( g^*(x) \\) yields the highest \\(\\epsilon_{C}\\)-efficiency rate for detecting the Gumbel-max watermarks.\n\n3. **Empirical Comparison:**\n   - Experiments with ChatGPT-3.5-turbo showed that in real-world scenarios, a significant portion of the largest token probabilities in NTP distributions are above 0.5.\n   - In this empirical setting, \\( S_{1-\\alpha}(x) \\) outperformed \\( g(x) = x \\) in common Large Language Model (LLM) setups, aligning with the theoretical predictions (Theorem 3.3).\n\nThe results cement the derived score function \\( g^*(x) \\) as the optimal detector for Gumbel-max watermarks across different scenarios, demonstrating its superior performance in maintaining high detection efficiency under various conditions."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To apply the statistical framework for watermark detection to the Gumbel-max watermark implemented internally at OpenAI and determine the optimal detection rule.",
            "experiment_process": "The Gumbel-max watermark uses a decoder defined by replicates of the standard uniform random variable, computed using a pseudorandom hash function. The study examines multiple score functions, including existing ones and an optimal one derived within the statistical framework. The class-dependent efficiency is used to compare these score functions across different Next-Token Prediction (NTP) distributions. The efficiency rates were analyzed theoretically and compared numerically.",
            "result_discussion": "The study finds that the Nedecoder function provides the highest class-dependent efficiency rate. The optimal score function achieves superior performance compared to alternative detection rules for the Gumbel-max watermark. Specifically, the study reveals that the choice of score function significantly impacts detection efficiency, with the proposed optimal score function consistently outperforming others across a range of scenarios. The findings suggest practical implications for enhancing watermark detection in real-world applications.",
            "ablation_id": "2404.01245v1.No1"
        },
        {
            "research_objective": "To apply the statistical framework for watermark detection to the Inverse Transform watermark and derive the optimal score function for detection.",
            "experiment_process": "The Inverse Transform watermark uses a decoder based on uniform permutations and constructs token sequences with independent pseudorandom variables. The study examines the distribution of pseudorandom variables under both the null and alternative hypotheses. A detailed analysis was conducted to handle the complexity introduced by permutations and compute the class-dependent efficiency. The research also focused on scenarios where the probability distribution over tokens is concentrated on a single token\u2014a common occurrence in large language models.",
            "result_discussion": "The study successfully simplifies the distribution characterization for the Inverse Transform watermark, particularly when token probabilities are highly concentrated. This characterization leads to the identification of an optimal score function achieving infinite power in distinguishing hypotheses in the large vocabulary limit. The research highlights that the proposed score function significantly enhances watermark detection efficiency in practical LLM scenarios, especially where one token has a much higher probability than others.",
            "ablation_id": "2404.01245v1.No2"
        }
    ]
}