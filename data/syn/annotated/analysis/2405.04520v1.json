{
    "title": "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts",
    "abstract": "Large language models (LLMs) have manifested strong ability to generate codes for productive activities.\nHowever, current benchmarks for code synthesis, such as HumanEval, MBPP, and DS-1000, are predominantly oriented towards introductory tasks on algorithm and data science, insufficiently satisfying challenging requirements prevalent in real-world coding.\nTo fill this gap, we propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks.\nNCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains.\nNoting the extraordinary difficulty in creating testing cases for real-world queries, we also introduce a semi-automated pipeline to enhance the efficiency of test case construction.\nComparing with manual solutions, it achieves an efficiency increase of more than 4 times.\nOur systematic experiments on 39 LLMs find that performance gaps on NCB between models with close HumanEval scores could still be significant, indicating a lack of focus on practical code synthesis scenarios or over-specified optimization on HumanEval.\nOn the other hand, even the best-performing GPT-4 is still far from satisfying on NCB.\nThe evaluation toolkit and development set are available at\nhttps://github.com/THUDM/NaturalCodeBench.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) pre-trained on extensive open code repositories [13  ###reference_b13###; 45  ###reference_b45###; 33  ###reference_b33###; 14  ###reference_b14###] have demonstrated impressive performance on code synthesis and even achieve performance comparable to average human level in coding competitions [35  ###reference_b35###].\nUnlike open text generation, which often underscores human preferences as noted by [47  ###reference_b47###], code synthesis prioritizes accuracy and the fulfillment of user intent, essential for practical production and application.\nAs a result, evaluating code synthesis presents unique challenges in the era of LLMs.\nTraditional evaluation metrics by token matching [48  ###reference_b48###; 36  ###reference_b36###; 50  ###reference_b50###] show a weak correlation with human judgement [21  ###reference_b21###] and overlook functional correctness of the generated code 20  ###reference_b20###; 56  ###reference_b56###.\nRecently, execution-based evaluation has gained increasing popularity, where code generated by models is tested through unit tests to verify its functional correctness.\nIt leads to the development of several benchmarks, including HumanEval [13  ###reference_b13###], MBPP [7  ###reference_b7###], MBXP [6  ###reference_b6###], CodeContests [35  ###reference_b35###], and DS-1000 [32  ###reference_b32###].\nNotwithstanding their commendable reliability and accuracy, these benchmarks fall short to sufficiently capture the wide range of needs and complexity found in real-world engineering applications.\nThey are primarily limited to well-defined coding problems in algorithm, program basics, or data science.\nFor example, as shown in Figure 1  ###reference_###, a problem from HumanEval [13  ###reference_b13###] tests the implementation of a basic function has_close_elements and takes floating-point arguments as inputs.\nHowever, in practical applications, user engineering requirements can be much more complex and varied.\nIn Figure 1  ###reference_###, we showcase an example adapted from a real user query, where the user asks to read and parse XML files given certain tags.\nDifficult and costly though it is, curating a benchmark composed of such problems is meaningful for evaluating the real user experience of LLM code synthesis.\nContributions.\nIn light of the challenge, we introduce NaturalCodeBench (NCB), a challenging application-driven dataset for code synthesis evaluation.\nNCB is dedicated to creating a reliable evaluation environment that is more aligned with real-world applications.\nWe leverage an CodeGeeX [70  ###reference_b70###] online services to collect real and diverse application-related user queries.\nAfter filtering and reprocessing, 402 high-quality Python and Java problems are compiled, covering 6 domains including software, front-end, system administration, and artificial intelligence, highlighting practical scenarios.\nBeyond basic data structures like lists and numbers, the test inputs for NCB problems include versatile file types and other complex structures, making it more challenging.\nThe challenging nature of NCB necessitates significant human labor in its annotation process\nTo improve construction efficiency, we tailor a semi-automated annotation pipeline to curate high-quality, testable, and useful queries with corresponding test cases.\nSpecifically, we employ GPT-4 [45  ###reference_b45###] to generate reference solutions followed by manual correction.\nSubsequently, GPT-4, guided by the problem descriptions and reference solutions, generates multiple test cases, which are also refined with manual correction, for each problem.\nConsequently, the annotators are only required to correct any errors, substantially reducing the time and manpower required.\nComparative experiments reveal that our semi-automated pipeline can quadruple the construction speed of the evaluation framework, as evidenced by tests involving programming experts with or without the pipeline.\nBased on NCB, we conduct extensive experiments on a variety range of LLMs, encompassing 39 APIs or open models.\nThe results indicate that although certain LLMs demonstrate comparable performance on established benchmarks like HumanEval, they exhibit significant performance disparities when evaluated using NCB.\nIt suggests that there may be inadequate focus on optimizing LLMs for practical coding applications, or have conducted over-specified optimization on HumanEval-style problems.\nMore importantly, even the best-performing GPT-4 only reaches about a pass rate of 53%, demonstrating a large room for LLMs to improve their coding skills to face real-world coding challenges.\nTo facilitate community research, we pack up the whole NCB testing environment into a docker image and make its development set publicly available.\nTo sum up our contributions:\nWe propose NaturalCodeBench, a benchmark that aligns with real-world applications, comprising 402 problems in Python and Java across 6 domains. We open source 140 problems (70 Python, 70 Java) as the development set of NCB for research purposes, but keep the 262 problems of the test set closed to avoid contamination.\nWe introduce a semi-automated pipeline for the construction of code synthesis benchmarks, which significantly reduces time and manpower costs without compromising the quality of test cases. Comparative experiments reveal that our\nsemi-automated pipeline can quadruple the construction speed of the evaluation framework\nWe systematically benchmark the code generation capabilities of 39 LLMs using NCB. Besides quantitative evaluation, we carry out a deep insight into the present stage of development in LLMs for code generation, and outline potential pathways for future progress.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Benchmark Construction",
            "text": "The overview of NCB is shown in Figure 2  ###reference_###. The pipeline of constructing NCB consists of four steps: 1) collecting and filtering high-quality problems from online services (Section 2.1  ###reference_###) 2) constructing a complete evaluation framework through a semi-automated pipeline (Section 2.2  ###reference_###) 3) designing prompts to align different models (Section 2.3  ###reference_###) 4) translating all problems and instructions to produce bilingual versions (Section 2.4  ###reference_###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Selection",
            "text": "Collecting Real-World Problems.\nTo establish a meaningful and practical benchmark, we centered on collecting real-world code problems frequently encountered by users. To achieve this, the seed problems of NCB are cleaned from the queries in coding online services. A part of users have granted permission for their data to be utilized exclusively for research purposes. We have strictly adhered to this directive by collecting only the relevant data from these consenting users and have implemented robust de-identification measures to eliminate any possibility of information leakage. We collect a varied collection of queries, spanning multiple programming languages, problem types, and levels of complexity. This diversity ensures that our benchmark accurately reflects a broad range of code issues users encountering in practice. We specifically concentrated on queries related to Python and Java, chosen for their widespread use in different domains.\nFiltering Testable Problems. While it\u2019s possible to source inexhaustible queries from online services, many of these queries posed by users are either of low value or challenging to test the solution of these queries. For instance, some users may only seek basic clarifications on a built-in function, while others may not clearly articulate their objectives. To sieve out unsuitable queries for our testing, we\u2019ve implemented a two-step filtering process. Initially, we employ GPT-3.5 to filter out low-quality queries, which saves on labour. This is achieved by adding specific criteria in the instruction, instructing GPT-3.5 to abandon those problems that cannot meet all specified requirements. These criteria are as follows: 1) Each query must involve at least one task, where the user requests the model\u2019s assistance in solving one or more problems. 2) Each query should be associated with several input-output pairs, ensuring that a given input correspond to a singular, definitive output. 3) The query must not contain any elements of randomness or uncertainty. The specifics of the instruction are detailed in (Appendix A  ###reference_###). Following this automated pre-screening, we conduct a manual review to further refine the selection, adhering to the outlined criteria. This process yields a final set of 201 unique Python and 201 unique Java problems. It is noteworthy that over 80% of the initial queries failed to meet our stringent requirements."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Semi-automated Pipeline",
            "text": "In this section, we will introduce our semi-automated pipeline. To generate structurally complex and accurate test cases by GPT-4, it is first necessary to determine the arguments and return values of functions, as well as the names of objects. Therefore, a completely accurate reference solution is required initially. We generate a solution using GPT-4, then manually correct all errors. After this, based on the problem description and the reference solution, we instruct GPT-4 to generate multiple test cases. These are then reviewed by programming experts who correct errors and supplement any deficiencies in the generated test cases.\nGenerating and Rewriting Reference Solution.\nGPT-4 is instructed to generate a solution for each problem in NCB. It is important to note that while GPT-4 is highly capable, it is not infallible. Therefore, each solution generated by GPT-4 is meticulously examined by expert programmers to ensure correctness. In cases where the generated code contains errors, the expert programmers rewrite the code to rectify these issues. This process ensures the quality of the reference solutions. Even though we did not use the reference solution in NCB for evaluation, we provided them to facilitate the generation of test cases and future research.\nBuild High-Coverage and Corner Evaluation.\nWe employ GPT-4 to generate evaluation codes for each problem. We construct a prompt using 1) the description of the problem for GPT-4 to inspect; 2) the reference solution to demonstrate the names and formats in the code; 3) an instruction to encourage GPT-4 to come up with effective test cases. Specifically, each prompt start with an instruction that ask GPT-4 to produce ten test cases based on the description of problem and the reference solution. Then, we present both the description of problem and its reference solution. We finalize the prompt with a initial segment of the evaluation code to assist GPT-4 in accurately generating the desired code format.\nOur objective is to harness GPT-4\u2019s advanced comprehension and analytical abilities to learn valid format in the code and essential functionalities of the reference solution to enable the generation of superior test cases that are adept at uncovering latent errors in code.\nA complete and effective test should seek to identify potential bugs at different locations in the code, while also finding inputs that might trigger errors in the code. High coverage ensures that each test case examines more code and branches, thereby facilitating the discovery of concealed errors. Meanwhile, it is often observed that corner values in a problem\u2019s input are most prone to trigger code errors. Consequently, our instruction will cause some of the test cases generated by GPT-4 to have higher coverage, while the other part will be some corner values contained in the problem, so as to obtain more effective test cases.\nSubsequently, expert programmers review and correct any test cases with formatting and answer errors. To ensure that the final evaluation framework is error-free."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Alignment Between Different Models",
            "text": "In contrast to the problem format in Humaneval, the majority of problems in our benchmark are composed in natural language by actual users. Consequently, there is no predetermined naming convention for functions or classes created by models. This divergence can lead to inconsistencies between the names generated by LLMs and those referenced in test cases. To address this issue of name misalignment, we present a representative test case that includes the designated function or class name and its usage within the test. We then instruct the LLMs to adhere to the naming convention specified in the provided test case when generating solutions. It is important to note that the test cases utilized for solution generation are not employed in subsequent testing phases. The details of the instruction is showed in Appendix A  ###reference_###."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Building Bilingual Benchmark",
            "text": "The majority of the questions we collected from online services are in Chinese, which is not fair for the LLMs that are primarily designed for English. Therefore, we translate all the problems, resulting in both Chinese and English versions."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Dataset Statistics",
            "text": "We provide more detailed statistics in Table 2  ###reference_###. NCB comprises a total of 402 problems collected from online services, with 201 problems in Python and 201 in Java, spanning across 6 domains: Database, Artificial Intelligence, Data Science, Algorithm and Data Structure, Front-End, Software Engineering, and System Administration. This diversity also leads to complex input data types in NCB, which are classified into 9 categories: number (int/float/boolean), string, list (array), dict, tensor (matrix), data frame (table), plain text file, image, and special format file. The first four are the most common and simplest data types. Since a boolean can be represented by 1 and 0, we consider it as a type of number. Matrix and list are two similar types of data, but they are categorized separately due to differences in their usage scenarios.\nDue to the current popularity of deep learning, tensor has become a very common data format. Therefore, we have designated a separate category for tensor and have included matrix within this category. The last three are all file types, differentiated by their processing methods. The content of a plain text file is text and can be directly read. Figures require processing of each pixel value. A special format file refers to files that require specific methods for processing, such as PDF and DOCX.\nEach problem within the dataset has been carefully curated with a set of test cases to assess the correctness of solutions. On average, there are 9.3 test cases associated with each problem. These cases are strategically designed, with about 60% focused on enhancing statement and branch coverage, and the remaining 40% dedicated to evaluating the robustness of solutions against corner values. The average word count for each problem in the NCB is 78.3.\nCompared with Other Benchmark.\nTable 1  ###reference_### compares NCB to other benchmarks. It is noteworthy that our benchmark offers a substantial supplement to current benchmarks in terms of both problem and data types. Unlike Humaneval and MBPP, which consist of 96.9% and 89.5% algorithmic and basic programming problems respectively, our benchmark features a more balanced distribution across each domain.\nIn addition, NCB include more data types. Furthermore, NCB focuses on assessing the model\u2019s ability to handle multiple file formats, a type of data that is both very commonly used in daily life and relatively challenging to process. We note that the problems involving files have fewer test cases, since GPT-4 still struggles to fully generate various types of file . This is also more challenging for human annotators to design compared to simpler data types.\nOn the other hand, NCB is also limited by its size due to the high costs of problems collection and the construction of the evaluation framework. We are continuously working on expanding our benchmark.\n###table_1###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setup",
            "text": "We conducted comprehensive evaluations of 33 popular state-of-the-art models. For proprietary models, our focus was on OpenAI\u2019s GPT-4-Turbo-0125, GPT-4-Turbo-1106, GPT-4, GPT-3.5-Turbo, Anthropic\u2019s Claude-2, ZhipuAI\u2019s CodeGeeX3. In the case of open-source models, we performed evaluations using the vLLM [31  ###reference_b31###] and FastChat [69  ###reference_b69###] framework. Our evaluation primarily utilizes pass@k [13  ###reference_b13###] as the metric to accurately assess the functional correctness of code generated by these models. For k equal to 1, we employ greedy-search decoding. For random sampling, we demonstrate the best pass@k results of the best-performing models with each LLM family for each , where the sampling temperature is set to 0.2 and topp to 0.9.\nOur semi-automated pipeline is capable of reducing the time required for benchmark construction without compromising the quality of test cases. This paper primarily focuses on evaluating the efficiency of benchmark construction and the quality of test cases. Specifically, we adopt code coverage [26  ###reference_b26###], a widely used metric for assessing the effectiveness of testing, as the criterion for evaluating the quality of test cases. We invite five programming experts, each tasked with constructing the same five problems. Initially, we ask each expert to manually write a standard solution and 5 test cases. Subsequently, for the same problems, they complete the writing of standard solutions and test cases using the semi-automated pipeline. As it is challenging to ensure identical test case coverage, we require that the test cases written under both methods should not have a code coverage of less than 80%. Then, for the sake of convenient comparison, we calculate the scores for each construction method in a straightforward manner, which is outlined as follows:"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results of LLMs",
            "text": "Table 3  ###reference_### and Table 6  ###reference_### shows the pass@1 results on the test set and dev set of NCB, respectively. Considering the high consistency of results, we primarily analyze the results on the test set. As expected, OpenAI\u2019s GPT-4 achieves the highest score of 52.8%. The performance of GPT-4-Turbo is very close to that of GPT-4, differing only by 1.3% , with GPT-4-Turbo performing better in Java but showing a larger difference in Python. Among the open-source models, DeepSeek-Coder-33B-Instruct performs the best, reaching a score of 43.0%. However, the 9.8% score gap with GPT-4 remains significant. On the other hand, it surpasses the 40.7% achieved by GPT-3.5, exceeding it by 2.3%. In summary, the performance of state-of-the-art open-source models is now between GPT-3.5 and GPT-4, yet the majority of open-source models still do not match the performance of GPT-3.5.\nWhen compared to a perfect score of 100%, it is observed that even the best-performing model, GPT-4, still falls significantly short. This is in contrast to its performance in HumanEval, where it has approached 90%.\nComparing the performance of models in Chinese and English versions, it is evident that the vast majority of models perform better in English. This holds true even for the top models, GPT-4 and GPT-4-Turbo, which outperform their average scores in Chinese by 1.1% and 3.9%, respectively.\nFurthermore, Table 3  ###reference_### systematically presents the performance of various open-source models at different scales. Models smaller than 10B scored between 0.0% and 23.9%, models between 10B and 30B scored between 3.9% and 35.1%, models between 30B and 60B scored between 21.8% and 43.0%, and models larger than 60B scored between 27.9% and 33.2%. It is evident that the scale of the model still has a significant impact on performance. Larger models generally outperform smaller models, indicating that increasing scale can indeed enhance a model\u2019s capabilities. However, this is not to say that scale is everything; more refined data and training strategies can also significantly impact a model\u2019s performance. Some smaller models, such as DeepSeek-Coder-6.7B-Instruct, can outperform those larger than 30B by approximately 2.8% and those larger than 60B by approximately 1.9%.\nTable 5  ###reference_### shows the pass@k results of best-performing LLMs with each LLM family on NCB, where . We found that under random sampling, the scores of some models increased significantly. For instance, Codellama-70B-Instruct, unlike its performance on pass@1, clearly outperformed GPT-3.5 on both Pass@10 and Pass@50.\nWe compared the Python scores on the test set of NCB with the performances of models on Humaneval, as shown in the Figure 1  ###reference_###. Most models are located in the upper triangular area of the graph, with many models scoring high on Humaneval but exhibiting relatively lower performance on NCB."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Results of Semi-automated Construction",
            "text": "In Table  4  ###reference_###, we can observe that the coverage of hand-written test cases is almost identical to that of test cases constructed through a semi-automatic pipeline, yet the time required for the former significantly exceeds the time needed for constructing test cases via the semi-automatic pipeline. Specifically, test cases can be constructed via the semi-automated pipeline in just 40 minutes, whereas manual writing requires 175.9 minutes, a difference of more than 4x. Consequently, the scores obtained for test cases constructed using the semi-automated pipeline are far higher than those for manually written test cases, with an average difference of 37.6. In summary, constructing test cases through the semi-automatic framework can achieve significantly higher efficiency without substantial loss in quality compared to manual writing."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "LLMs for code.\nSignificant advancements in LLMs (57  ###reference_b57###, 18  ###reference_b18###, 11  ###reference_b11###) are transforming everyday life, particularly in the field of coding, driven by the vast amount of openly available codebases and the push to enhance productivity among developers. Code-specific LLMs have proven their ability to perform various tasks such as code generation (13  ###reference_b13###, 27  ###reference_b27###, 35  ###reference_b35###), program repair (29  ###reference_b29###, 58  ###reference_b58###, 60  ###reference_b60###, 61  ###reference_b61###), automated testing (16  ###reference_b16###, 17  ###reference_b17###, 39  ###reference_b39###, 59  ###reference_b59###, 63  ###reference_b63###), code translation (52  ###reference_b52###, 53  ###reference_b53###) and code summarization (1  ###reference_b1###, 40  ###reference_b40###). Notably, prominent LLMs including CODEX [13  ###reference_b13###], CodeGen [44  ###reference_b44###], INCODER [22  ###reference_b22###], and PolyCoder [62  ###reference_b62###] have been developed and rigorously tested, particularly in code generation. This area, often referred to as the ultimate goal in computer science research since the early days of AI in the 1950s, involves the model producing code snippets from natural language explanations of the required functionality. The landscape of code LLMs is currently experiencing a surge, with new models being introduced regularly. This includes both proprietary ones (42  ###reference_b42###, 45  ###reference_b45###) and open-source ones (36  ###reference_b36###, 44  ###reference_b44###, 55  ###reference_b55###, 33  ###reference_b33###, 3  ###reference_b3###, 54  ###reference_b54###), marking a trend of frequent releases in this domain.\nCode Synthesis Benchmarks.\nAs the capabilities of models advance, researchers are developing more challenging and versatile benchmarks for code generation.\nInitially, the earlier focus was on domain-specific languages [67  ###reference_b67###], while the subsequent effort launched a Text-to-SQL benchmark to evaluate the capacity for generating comprehensive SQL programs [66  ###reference_b66###].\nA investigation [65  ###reference_b65###] assesses the ability to compose brief yet broadly applicable Python snippets.\nMore recent studies (25  ###reference_b25###, 35  ###reference_b35###) have tested models\u2019 proficiency in solving competitive programming challenges using Python.\nA leading and extensively researched benchmark in this domain is HumanEval [13  ###reference_b13###], which features 164 Python function signatures accompanied by docstrings and corresponding test cases for validating correctness.\nAdditionally, each problem in HumanEval includes a reference solution. The MBPP [7  ###reference_b7###] dataset, another Python-centric collection, was developed by having participants contribute 974 programming challenges.\nEach challenge encompasses a problem description (i.e., docstring), a function signature, and three test cases. There are also benchmarks for other programming languages, such as HumanEval-X [70  ###reference_b70###] for C++, JavaScript, and Go, CodeContests [35  ###reference_b35###] for C++ and Java, and MultiPL-E [12  ###reference_b12###], which expands HumanEval and MBPP to 18 languages.\nMore recent efforts have introduced benchmarks that more closely mirror real-world coding scenarios that require interactive coding.\nFor example, AgentBench [38  ###reference_b38###] introduces interactive tasks regarding unix shell and MySQL.\nSWE-Bench [30  ###reference_b30###] compiles GitHub issues, their associated codebases, and tests, to gauge LLMs\u2019 effectiveness in practical software engineering tasks."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose NaturalCodeBench for evaluating the code generating ability of LLMs. Our benchmark comprises a total of 402 problems selected from coding online services, and it supports automatic evaluation of code generated by LLMs. We have also proposed a semi-automated pipeline for efficiently constructing the entire benchmark, achieving an efficiency gain of more than 4x compared to manual construction. We hope that NCB can provide a fair environment for the comparison between models, and our pipline can also provide inspiration to other complex tasks or domains where evaluation costs are high."
        }
    ],
    "url": "http://arxiv.org/html/2405.04520v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.04520v1",
        "paper_title": "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts",
        "research_background": "**Motivation:**\nThe paper is motivated by the observed discrepancy between the performance of large language models (LLMs) on code synthesis benchmarks and their effectiveness in real-world coding applications. Despite the impressive results on established benchmarks such as HumanEval, the applicability of these models in diverse, real-world engineering contexts remains questionable. Traditional evaluation metrics, such as token matching, show weak correlations with human judgement and often overlook the functional correctness of generated code. Additionally, current benchmarks primarily focus on basic algorithmic problems, which do not capture the full complexity and variety of practical engineering tasks.\n\n**Research Problem:**\nThe research problem tackled in this paper is the development of a more representative and challenging code synthesis benchmark that aligns closely with real-world applications. The existing benchmarks do not adequately reflect the wide-ranging needs of the software engineering industry. Therefore, the paper aims to create a benchmark that includes complex and varied user queries, simulating real-world scenarios to evaluate the practical effectiveness of LLMs in code synthesis.\n\n**Relevant Prior Work:**\n- **LLM Pre-training on Code Repositories:** Several works have demonstrated the efficacy of LLMs pre-trained on extensive open code repositories, achieving performance comparable to average human levels in coding competitions ([13, 33, 35, 45]).\n- **Human Preferences vs. Functional Correctness in Evaluation:** Unlike open text generation, where human preferences play a significant role ([47]), code synthesis evaluation emphasizes accuracy and user intent fulfillment, which are critical for practical application.\n- **Current Evaluation Techniques:** Traditional token matching metrics ([36, 48, 50]) have shown weak correlations with human judgments ([21]) and often fail to account for the functional correctness of code ([20, 56]).\n- **Execution-based Evaluation:** Recently, execution-based evaluation, which involves testing generated code through unit tests, has gained traction. This has led to the creation of benchmarks such as HumanEval ([13]), MBPP ([7]), MBXP ([6]), CodeContests ([35]), and DS-1000 ([32]).\n\nBased on these insights and challenges, the paper proposes the NaturalCodeBench (NCB) to provide a more reliable and realistic evaluation framework for LLMs in code synthesis.",
        "methodology": "The proposed method or model described in the methodology section of \"NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts\" is referred to as NCB. The methodology outlines a four-step pipeline for constructing NCB, described as follows:\n\n1. **Collecting and Filtering High-Quality Problems**: This step involves gathering coding problems from online services and ensuring they meet high-quality standards (detailed in Section 2.1).\n\n2. **Constructing a Complete Evaluation Framework**: A semi-automated pipeline is developed to create a robust framework for evaluating coding performance (discussed in Section 2.2).\n\n3. **Designing Prompts to Align Different Models**: This step focuses on creating prompts that can be used to standardize the inputs across various coding models (explored in Section 2.3).\n\n4. **Translating All Problems and Instructions**: To produce bilingual versions of all problems and instructions, translations are carried out, making the evaluation accessible in multiple languages (explained in Section 2.4).",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Experiment Setup:\n1. **Models Evaluated**: \n    - **Proprietary Models**: OpenAI\u2019s GPT-4-Turbo-0125, GPT-4-Turbo-1106, GPT-4, GPT-3.5-Turbo; Anthropic\u2019s Claude-2; ZhipuAI\u2019s CodeGeeX3.\n    - **Open-Source Models**: Evaluations conducted using the vLLM and FastChat frameworks.\n  \n2. **Evaluation Metric**:\n    - **Pass@k**: Utilized to accurately assess the functional correctness of code generated by the models. For \\( k = 1 \\), greedy-search decoding is used. For random sampling, the best pass@k results are demonstrated with each Language Learning Model (LLM) family, using a sampling temperature of 0.2 and topp set to 0.9.\n\n3. **Evaluation Approach**:\n    - The evaluation pipeline includes a semi-automated process to streamline benchmark construction without compromising quality. \n\n#### Result Metrics:\n1. **Code Coverage**:\n    - Code coverage, a standard metric for testing effectiveness, is used to evaluate the quality of test cases. This ensures comprehensive assessment of the correctness and thoroughness of generated code.\n\n#### Procedure for Code Coverage Evaluation:\n1. **Expert Involvement**:\n    - Five programming experts each constructed the same five problems.\n    - Initial Task: Write a standard solution and 5 test cases manually.\n    - Secondary Task: Use a semi-automated pipeline to write standard solutions and test cases.\n    - Code Coverage Requirement: Test cases from both the manual and semi-automated methods should have a minimum coverage of 80%.\n\n2. **Comparison Approach**:\n    - To facilitate straightforward comparison, the scores for each method of construction are calculated, although specific details on scoring methodology are not provided in the extracted text.\n\n### Main Experimental Results:\n- **Efficiency and Quality**:\n    - The semi-automated pipeline is highlighted for its efficiency in benchmark construction.\n    - There is an emphasis on maintaining test case quality, reflected in using code coverage to ensure test cases are sufficiently thorough.\n\nThe specific numerical results, detailed findings, and comparisons of model performances (e.g., which models had the highest pass@k scores) are not provided in the above text and would need to be referenced directly from the paper for a comprehensive understanding of the main experimental outcomes."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To examine the performance differences of Large Language Models (LLMs) between the HumanEval and NaturalCodeBench (NCB) benchmarks to determine if NCB reflects the coding abilities of LLMs as HumanEval does.",
            "experiment_process": "The performance of 39 LLMs was evaluated on both the HumanEval and NCB benchmarks. A rank order comparison for all tested LLMs was conducted, with the differences in rank order noted. Performances on both benchmarks were plotted in a scatter diagram to visualize correlations.",
            "result_discussion": "Performances on both benchmarks generally grew linearly proportional, with most differences in rank order around 0, indicating that NCB reflects LLMs' coding abilities as HumanEval does. However, certain models like Phi, Deepseek-Chat, and WizardCoder consistently ranked higher on HumanEval compared to NCB, possibly due to the increased difficulty and natural user prompts in NCB or potential over-optimization for HumanEval-style problems.",
            "ablation_id": "2405.04520v1.No1"
        },
        {
            "research_objective": "To assess the efficiency and quality of a semi-automated pipeline for constructing test cases compared to traditional manual writing methods.",
            "experiment_process": "A comparison was made between hand-written test cases and those constructed via a semi-automated pipeline. The evaluation included the time required for each method and the coverage of test cases. The time required for construction and the scores obtained from the test cases were compared.",
            "result_discussion": "The semi-automated pipeline constructed test cases in just 40 minutes compared to 175.9 minutes for manual writing, achieving more than 4 times the efficiency. Despite the difference in time, the coverage of test cases was almost identical. Scores from the semi-automated pipeline were significantly higher, with an average difference of 37.6. Thus, the semi-automated method achieved higher efficiency without a substantial loss in quality.",
            "ablation_id": "2405.04520v1.No2"
        }
    ]
}