{
    "title": "Modeling Orthographic Variation Improves NLP Performance for Nigerian Pidgin",
    "abstract": "Nigerian Pidgin is an English-derived contact language and is traditionally an oral language, spoken by approximately 100 million people. No orthographic standard has yet been adopted, and thus the few available Pidgin datasets that exist are characterised by noise in the form of orthographic variations. This contributes to under-performance of models in critical NLP tasks.\nThe current work is the first to describe various types of orthographic variations commonly found in Nigerian Pidgin texts, and model this orthographic variation.\nThe variations identified in the dataset form the basis of a phonetic-theoretic framework for word editing, which is used to generate orthographic variations to augment training data. We test the effect of this data augmentation on two critical NLP tasks: machine translation and sentiment analysis.\nThe proposed variation generation framework augments the training data with new orthographic variants which are relevant for the test set but did not occur in the training set originally. Our results demonstrate the positive effect of augmenting the training data with a combination of real texts from other corpora as well as synthesized orthographic variation, resulting in performance improvements of 2.1 points in sentiment analysis and 1.4 BLEU points in translation to English.\n\n\n\nKeywords:\u2009Nigerian Pidgin, orthographic variation, sentiment analysis, machine translation",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Models developed for a variety of NLP tasks can give high-quality performance for resource-rich languages, such as English and French. However, for under-resourced languages such as many African and South-East Asian languages, NLP models show poor performance due to a lack of high-quality, sufficiently sized, publicly available datasets.111The Masakhane community aims to address this by strengthening and spurring NLP research in African languages; see https://www.masakhane.io  ###reference_www.masakhane.io###.\nIn such cases, models might be particularly sensitive to noise in the data.\nOne source of \u201cnoise\u201d is orthographic variation \u2013 that is, variations in the spelling of words in the data. Orthographic variation can be detrimental to the performance of NLP models, which are typically trained on curated datasets and tend to break when faced with noisy data (Bergmanis et al., 2020  ###reference_b7###).\nThe issue of orthographic variation is especially present for languages that do not have a standardized and normalized orthography yet, which is the case for many creoles and pidgins.\nThe current work addresses the orthographic variation in one such language, namely Nigerian Pidgin.\nThis language has 100 million speakers, but is still largely absent from NLP research.\nNigerian Pidgin is a predominantly spoken language, without a normalized orthography in place Mensah et al. (2021  ###reference_b22###). Consequently, written Nigerian Pidgin texts are characterized by a large proportion of orthographic variations.\nThese diverse orthographic variations contribute to a significant under-performance in critical tasks, such as sentiment analysis and machine translation.\nWe address this by synthesizing the orthographic variation at the phonological level, and subsequently training language models on data augmented with these variations.\nOur contributions are as follows:\nWe are the first to provide an analysis of the various types of orthographic variations that occur in a variety of Nigerian Pidgin texts (the Bible, magazine texts and transcriptions of spoken conversations). These orthographic variations are found to be of a phonetic nature.\nWe propose a phonetic-theoretic framework for word editing, which can be used to generate orthographic variations to augment training data for language models.\nWe show performance improvement on two NLP tasks (machine translation and sentiment analysis) when including the augmented training data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related work",
            "text": "Pidgins and creole languages have various unique features (e.g., grammar, morphology, lexicon) that make them interesting to study for various linguistic topics. Nevertheless, linguistic research and resources on these languages are limited Lent et al. (2021  ###reference_b15###). Nigerian Pidgin is no exception to this, although it has received more attention in recent years.\nA few datasets now exist for Nigerian Pidgin, focusing on newspaper text Ogueji and Ahia (2019  ###reference_b27###); Ndubuisi-Obi et al. (2019  ###reference_b24###), text from several magazines written by a religious society Agi\u0107 and Vuli\u0107 (2019  ###reference_b5###), and task-specific data for named entity recognition Oyewusi et al. (2021  ###reference_b30###); Adelani et al. (2021  ###reference_b2###), sentiment analysis Oyewusi et al. (2020  ###reference_b29###), speech recognition Ajisafe et al. (2021  ###reference_b6###), and transcribed spoken data (Caron et al., 2019  ###reference_b9###).\nLin et al. (2023  ###reference_b18###) enriched the existing available parallel and monolingual Pidgin datasets to generate a high-quality fully parallel corpus of Nigerian Pidgin text across ten resources and five domains.\nAn orthography is a set of rules or conventions that is used to represent language in a standardized system of writing.\nWhen a language does not have a commonly used orthography, writers make decisions about orthography based on various criteria. The most dominant criterion is phonology: writers try to match the pronunciation of the word in the writing system, given (language-specific) assumptions about grapheme-to-phoneme mapping (Eskander et al., 2013  ###reference_b13###). Often, these assumptions come from other languages known to the writer; in the case of Nigerian Pidgin, this might be English or local Nigerian languages.\nDeuber and Hinrichs (2007  ###reference_b10###) present an analysis of orthographic choices in Nigerian Pidgin computer mediated communication. They report that for Pidgin lexical items that have English origins and mean the same as the origin, the English spelling is commonly adopted for Pidgin. In such cases, non-English spellings are used mainly for the symbolic purpose of indicating distance from English. For example, English \u2018thing\u2019 is often written as Pidgin thing, although a non-English spelling (tin) is also possible. For Pidgin lexical items which have roots in English, but have developed distinct meanings in Pidgin, writers are more likely to adopt non-English spellings. For example, \u2018done\u2019 has adopted a new meaning in Pidgin and is thus usually written as don. For Pidgin lexical items of non-English origin (e.g., pikin meaning \u2018child\u2019), writers tend to adopt the phonemic orthographies of local African languages.\nEskander et al. (2013  ###reference_b13###) address the orthographic variation found in Egyptian Arabic by normalizing the input data (i.e., transforming variations into a conventionalized orthography). Their results showed an error reduction of 69.2% over a baseline on an unseen test set. While the approach seems promising, a normalization approach is not efficient for languages that lack a standardized orthography since (i) it is not always clear which word form is the standard one and should be taken as the base word, and (ii) new and unknown variations will likely be encountered in new datasets, which makes the process not scaleable. Rather, we propose to train a model to be able to deal with orthographic variation directly, without the preprocessing step of normalization. We do so by employing data augmentation techniques.\nData augmentation is a method used to increase the amount of training data for NLP systems by adding slightly modified copies of already existing data or newly created synthetic data.\nIn particular, it has been shown to improve performance when a limited amount of labeled samples are available (see Feng et al., 2021  ###reference_b14###; Li et al., 2022  ###reference_b17###, for overviews).\nDifferent data augmentation methods exist; most relevant to the current work are noising-based methods such as inserting and changing words. These methods not only expand the amount of training data but also improve model robustness.\nBergmanis et al. (2020  ###reference_b7###) used an augmentation approach to improve machine translation systems\u2019 performance when faced with orthographic variations (such as unintentional misspellings and deliberate spelling alternations) of Latvian, Estonian and Lithuanian words.\nTheir results show that, when tested on noisy data, systems trained using adversarial examples performed almost as well as when translating clean data, achieving an improvement of 2-3 BLEU points over the baseline. The current study follows a similar approach, but applies it to two NLP tasks for Nigerian Pidgin."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Nigerian Pidgin orthography",
            "text": "Nigerian Pidgin (commonly referred to simply as \u2018Pidgin\u2019 and otherwise known as \u2018Naija\u2019) is an English-based contact language that developed as a result of European contact with West African languages. Pidgin incorporates syntax and vocabulary primarily from English, Portuguese and Nigeria\u2019s indigenous languages, as well as new vocabulary that is unique to Nigerian Pidgin.\nLike many other pidgin and creole languages, Nigerian Pidgin is a predominantly spoken language. Attempts to write the language go back to the late 18th century (Ofulue and Esizimetor, 2010  ###reference_b26###); nevertheless, it is considered to be fairly young as a written language, since it is still in the process of orthographic standardization and normalization. In 2009, the Naij\u00e1 Langwej Akedemi (NLA) proposed a harmonized orthography; more recently, Mensah et al. (2021  ###reference_b22###) published a new proposal for a working orthography of Nigerian Pidgin.\nNevertheless, these orthographies have not yet been adopted by non-linguist Pidgin speakers. Rather, they have developed their own ways of representing their unstandardized language. A gap has thus developed between experts and users, and this may widen in the coming years (Deuber and Hinrichs, 2007  ###reference_b10###), given that orthographic standardization on the basis of linguists\u2019 proposals is not in sight, and that Nigerian Pidgin is not being taught in school. Instead, Nigerian Pidgin speakers are taught in English in school, which likely also influences how these speakers write in Pidgin.\nNigerian Pidgin typically uses a Latin-based alphabet similar to English. Crucially, orthographies tend to be phonetically based with varying degrees of anglicized spellings (see Esizimetor, 2009  ###reference_b12###; Ojarikre, 2013  ###reference_b28###, for related discussions of various Pidgin orthography proposals); that is, words are typically spelled and written as pronounced according to the sound patterns of Nigerian Pidgin. However, as we will see in Section 3.1  ###reference_###, variations can still occur in phonetic-based orthographies, resulting in both inter-textual (between texts written by different authors) and intra-textual (within texts written by single authors) orthographic variation. Such inconsistencies in the orthography increase data sparseness and noise (Lewis, 2010  ###reference_b16###), which affect language models.\nIn the approach outlined in this paper, we generate \u201cadversarial\u201d training data to be able to train the model to deal with orthographic variation. This effort requires a deeper understanding of the types of variations that occur. Section 3.1  ###reference_### presents a qualitative analysis of common orthographic variations in Nigerian Pidgin.\nThe remaining subsections then present our approach to create synthetic data with more orthographic variation.\n###figure_1### Most orthographies stem from changes at the phonetic level.\nCorrectly identifying the character corresponding to the phonetic sound is a crucial step toward accurate word synthesis, as direct substitution based on subwords itself, like  often results in variations that do not exist.\nTo stimulate the process of creating variation, our framework edits the lexicon by considering acoustic features.\nWe first access the sound of a word by using the phonemization tool \u2018phonemizer\u2018 Bernard and Titeux (2021  ###reference_b8###), which transcribes written words into a series of phonemes that are consistent with English pronunciation rules \u2013 we found that this works well also for Pidgin words.\nGiven a word , appearing at i-th of a sentence, we obtain the transcription for the Pidgin word, denoted as .\nIn Figure 1  ###reference_###, this is illustrated with the word \u2019anything\u2019, which is transcribed as /\\textipaeniTIN\u2019/.\nWe adopt GIZA++ Och and Ney (2003  ###reference_b25###) to align the acoustic symbols and the corresponding characters of each word in the given corpus.333\nWe trained the aligner for 10 iterations using the IBM4 model. We utilized the character and phoneme as the training examples. Due to errors related to missed one-to-many and many-to-one alignments, we first conducted a preprocessing step to merge certain symbols that belong together as a unit, such as /\\textipao:/ on the phonetic side, or \u2018th\u2019, \u2018ng\u2019 on the spelling side, as these would have to be aligned to single symbols such as /\\textipaT/ or /\\textipat/ and /\\textipaN/ or /\\textipan/, respectively, see also Figure 1  ###reference_### for an example.\nThe aligned pairs of word and phoneme sequence serve as input for Step 3.\nTo generate orthographic variation candidates of words found in our Pidgin dataset, we created a set of variation rules. These rules were based on the qualitative analysis of the data, as shown in Table 1  ###reference_###.\nWhen multiple rules could apply to a single word (e.g., in the case of \u2018anything\u2019 four rules could apply; see Table 1  ###reference_###), we synthesized variations with different combinations of the rules, see Table 3  ###reference_###.\nWe note that the rules overgenerate in some cases; that is, they might in some cases strongly change the pronunciation of a word, e.g., \u2018anything\u2019 would result in the alternative spelling \u2018onytin\u2019 after the application of several rewriting rules. This variant is however very implausible to occur in Pidgin writing, as the resulting pronunciation in this case would deviate quite a lot from the original pronunciation. This is addressed in Step 4.\nAs a result of Steps 1-3, we now have a set of generated variation candidates for each seed word, including some variants that may be implausible as their pronunciation might not fit well with the original words\u2019 pronunciation.\nTo filter out such poor orthographic variation candidates, we automatically transcribe each generated variant to its phonetic transcription using again the \u2018phonemizer\u2019 tool Bernard and Titeux (2021  ###reference_b8###). We then measure the distance between the seed word\u2019s pronunciation and the candidate\u2019s pronunciation using Phonologically Weighted Levenshtein Distance (PWLD) Abdullah et al. (2021  ###reference_b1###). The PWLD metric assigns a lower distance to similar phonemes and a larger distance to dissimilar ones by introducing a weight term. The weight term defines how similar phonetic sounds  and  are in the acoustic space.\nThe original weight distance matrix was initially learned from English; we edited it slightly to fit permissible pronunciation variations in Pidgin for the generated candidates.\nSpecifically, we annotated 400 synthesized variants as \u2018good\u2019 or \u2018bad\u2019 variations, and adjusted the distance feature based on these labels.\nThe PWLD metric allows us to rank the generated variation candidate  according to their similarity in pronunciation to the original word.\nNote that the distance is measured at the level of the word\u2019s transcription () and the variation candidate\u2019s transcription ().\nWe sample the variations by the normalized probability using inverse-distance weighting Lu and Wong (2008  ###reference_b21###).\nThe probability to certain variation  is normalized by the summation of the inverse distances respective to all variation candidates, defined as444we write the distance as  for simplicity. It is equivalent to . This also holds for  in Formula 3.:\nwhere  denotes the summation of inverse PLWD scores with regards to all candidates:\nThe normalization over distance ensures that a higher probability is assigned to candidates with lower distances. This allows frequently selected variants that are pronounced similarly to the original word, while preventing dissimilar ones from being selected."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Synthesizing variation via phonological distance",
            "text": "The key to synthesizing variants of the words found in the Pidgin dataset is to consider the phonetics of these words. This is because the orthographic variations of Pidgin words tend to originate from those words being written as they are pronounced according to the sound patterns of Nigerian Pidgin.\nBased on this insight, we designed an approach that considers the phonemes of the words to generate variants that sound near-identical, but are spelled differently.\nGenerating orthographic variations could be done in various ways. One option is to convert phoneme sequences into spelling variants generatively.\nHowever, phoneme-to-grapheme models usually also rely on already having a model of acceptable word spellings, which is not available in our setting.\nWe therefore take a different approach:\nwe observe what variation exists at the level of orthography, then generate rules based on the orthographic forms, and check the generated variants via pronunciation distance (relying on a grapheme-to-phoneme conversion tool for English).\nThese variation rules were based on the qualitative analysis of the dataset described in Section 3.1  ###reference_###.\nIn what follows we describe how the framework generates variations. Figure 1  ###reference_### depicts the pipeline for this process. Note that, although this framework was designed for Nigerian Pidgin, it likely can be adapted for other Pidgin-based languages as well, as long as these languages also exploit the phonetic writing system of their lexifier.\nMost orthographies stem from changes at the phonetic level.\nCorrectly identifying the character corresponding to the phonetic sound is a crucial step toward accurate word synthesis, as direct substitution based on subwords itself, like  often results in variations that do not exist.\nTo stimulate the process of creating variation, our framework edits the lexicon by considering acoustic features.\nWe first access the sound of a word by using the phonemization tool \u2018phonemizer\u2018 Bernard and Titeux (2021  ###reference_b8###  ###reference_b8###), which transcribes written words into a series of phonemes that are consistent with English pronunciation rules \u2013 we found that this works well also for Pidgin words.\nGiven a word , appearing at i-th of a sentence, we obtain the transcription for the Pidgin word, denoted as .\nIn Figure 1  ###reference_###  ###reference_###, this is illustrated with the word \u2019anything\u2019, which is transcribed as /\\textipaeniTIN\u2019/.\nWe adopt GIZA++ Och and Ney (2003  ###reference_b25###  ###reference_b25###) to align the acoustic symbols and the corresponding characters of each word in the given corpus.333\nWe trained the aligner for 10 iterations using the IBM4 model. We utilized the character and phoneme as the training examples. Due to errors related to missed one-to-many and many-to-one alignments, we first conducted a preprocessing step to merge certain symbols that belong together as a unit, such as /\\textipao:/ on the phonetic side, or \u2018th\u2019, \u2018ng\u2019 on the spelling side, as these would have to be aligned to single symbols such as /\\textipaT/ or /\\textipat/ and /\\textipaN/ or /\\textipan/, respectively, see also Figure 1  ###reference_###  ###reference_### for an example.\nThe aligned pairs of word and phoneme sequence serve as input for Step 3.\nTo generate orthographic variation candidates of words found in our Pidgin dataset, we created a set of variation rules. These rules were based on the qualitative analysis of the data, as shown in Table 1  ###reference_###  ###reference_###.\nWhen multiple rules could apply to a single word (e.g., in the case of \u2018anything\u2019 four rules could apply; see Table 1  ###reference_###  ###reference_###), we synthesized variations with different combinations of the rules, see Table 3  ###reference_###  ###reference_###.\nWe note that the rules overgenerate in some cases; that is, they might in some cases strongly change the pronunciation of a word, e.g., \u2018anything\u2019 would result in the alternative spelling \u2018onytin\u2019 after the application of several rewriting rules. This variant is however very implausible to occur in Pidgin writing, as the resulting pronunciation in this case would deviate quite a lot from the original pronunciation. This is addressed in Step 4.\nAs a result of Steps 1-3, we now have a set of generated variation candidates for each seed word, including some variants that may be implausible as their pronunciation might not fit well with the original words\u2019 pronunciation.\nTo filter out such poor orthographic variation candidates, we automatically transcribe each generated variant to its phonetic transcription using again the \u2018phonemizer\u2019 tool Bernard and Titeux (2021  ###reference_b8###  ###reference_b8###). We then measure the distance between the seed word\u2019s pronunciation and the candidate\u2019s pronunciation using Phonologically Weighted Levenshtein Distance (PWLD) Abdullah et al. (2021  ###reference_b1###  ###reference_b1###). The PWLD metric assigns a lower distance to similar phonemes and a larger distance to dissimilar ones by introducing a weight term. The weight term defines how similar phonetic sounds  and  are in the acoustic space.\nThe original weight distance matrix was initially learned from English; we edited it slightly to fit permissible pronunciation variations in Pidgin for the generated candidates.\nSpecifically, we annotated 400 synthesized variants as \u2018good\u2019 or \u2018bad\u2019 variations, and adjusted the distance feature based on these labels.\nThe PWLD metric allows us to rank the generated variation candidate  according to their similarity in pronunciation to the original word.\nNote that the distance is measured at the level of the word\u2019s transcription () and the variation candidate\u2019s transcription ().\nWe sample the variations by the normalized probability using inverse-distance weighting Lu and Wong (2008  ###reference_b21###  ###reference_b21###).\nThe probability to certain variation  is normalized by the summation of the inverse distances respective to all variation candidates, defined as444we write the distance as  for simplicity. It is equivalent to . This also holds for  in Formula 3.:\nwhere  denotes the summation of inverse PLWD scores with regards to all candidates:\nThe normalization over distance ensures that a higher probability is assigned to candidates with lower distances. This allows frequently selected variants that are pronounced similarly to the original word, while preventing dissimilar ones from being selected."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Orthographic variation augmentation",
            "text": "We introduce an orthographic variation augmentation approach, which adds sentences with synthesized spellings to the training data.\nConsider a corpus with a total of  samples, denoted as . We generate the corresponding variation-enhanced versions  by randomly selecting  sentences,  and greedily generating a spelling variant for all words in that sentence. This results in  variation-augmented sentences forming the augmented data .\nThis augmented data complements the training data, creating . Table 4  ###reference_### presents an example of our approach to generating variations in the context of sentences.\nIt is worth noting that a single word can have different generated variants in  as these variants are sampled from the distance-normalized probability distribution.\nOur proposed framework is not restricted to a particular task but is applicable to any NLP task that involves Pidgin data.\nIn the next two sections, we explore the utilization of augmented data in two critical NLP tasks: sentiment analysis and machine translation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Sentiment analysis experiment",
            "text": "To determine the optimal size of augmented samples  for the sentiment analysis task, we adopt RoBERTa and test various sizes of extra data points within a range from =50 to =8,000.\nFigure 2  ###reference_### presents the results in comparison to the fine-tuned baseline ( size=0).\nFigure 2  ###reference_### shows that appending 100 augmented samples leads to a substantial performance boost of  points over the baseline.\nHowever, as more augmented samples are introduced, we see a gradual decline in performance to .\nWe attribute the observed fluctuations to the explanation that, while additional generated variations enrich the diversity of the training data, a relatively small test set does not present such high variability of the real variation distribution, and thus the additional augmentation introduces disproportionally much noise.\nFigure 3  ###reference_### shows the potential reason for the fluctuation: an increasing number of new variants is introduced as more augmented data is appended to the training dataset.\nNevertheless, even with 8K samples, the improvement in performance compared to the baseline is close to  point in F1 score (as shown in Figure 2  ###reference_###).\nIn sum, we find that controlling the size of the synthesized data influences the level of generalization achieved, but regardless of the size , adding orthographic variants to the dataset improved the baseline model performance on NaijaSenti.\n###figure_2### We further explore training stability by examining the cross-entropy of the model\u2019s prediction , in the context of sentiment analysis.\nTable 7  ###reference_### demonstrates the cross-entropy in the training epochs, as a crucial metric under cross-validation to gauge the effectiveness of our classification model.\nCross-entropy informs us on how well the model\u2019s predictions match the ground-truth class labels.\nMeasuring cross-entropy is done as an alternative to accuracy, which is less applicable to smaller datasets.\nThe results in Table 7  ###reference_### show that models trained using samples with orthographic variations are characterised by lower cross-entropy compared to the counterpart models that are not trained with appended orthographic variation samples.\nThis indicates that our orthographic variation augmentation approach leads to more accurate sentiment analysis results.\n###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Dataset, network and training details",
            "text": "Table 5  ###reference_### describes the datasets used in our experiments.\nFor the task of sentiment analysis, we use the NaijaSenti dataset (Muhammad et al., 2022  ###reference_b23###), which comprises a three-way data split (6.7K/0.6K/1.2K).\nWe use RoBERTa Liu et al. (2019  ###reference_b19###) and BERT Devlin et al. (2019  ###reference_b11###) in base versions. We employed AdamW optimization Loshchilov and Hutter (2019  ###reference_b20###) with a learning rate of 0.0001, and we found that a smaller learning rate helped prevent overfitting in the downstream task.\nWe compare three models.\nThe first model, PgOnly, is trained on NaijaSenti for 5 epochs with a mini-batch of 32.\nThe second model is a fine-tuned model (FT), which is trained on English and fine-tuned on Nigerian Pidgin. Finally, the third model is an orthographically augmented model (Orth-Augm) which, like FT, is trained on English and fine-tuned on Nigerian Pidgin, but the fine-tuning now also includes orthographic variations, which are appended to the training data.\nOur orthographic variation augmentation approach samples K=100 variants. We re-ran the model 6 times with different random seeds and present averaged results.\nAll models are evaluated using the F1 score."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Main results",
            "text": "Table 6  ###reference_### presents the results on the sentiment analysis task.\nBoth BERT and RoBERTa models, when trained with orthographic variation augmented data, demonstrated improvements in F1 scores, gaining  and  points over the fine-tuned model (FT), respectively.\nWe notice the higher improvement between FT and our approach when using RoBERTa in comparison to BERT.\nEven though RoBERTa leverages more pre-training data over BERT, the model still shows more improvements when trained with the augmented variations.\nThe model variant PgOnly is only trained on NaijaSenti and has not seen any English data. PgOnly shows significant gaps compared to models with fine-tuned and variation-augmented training.\nThis result emphasizes the importance of fine-tuning language models and how such models can be further improved with our generated variations."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Ablation study",
            "text": "This section provides further results on the augmentation effectiveness and shows the advantage of our augmentation approach.\nTo determine the optimal size of augmented samples  for the sentiment analysis task, we adopt RoBERTa and test various sizes of extra data points within a range from =50 to =8,000.\nFigure 2  ###reference_###  ###reference_### presents the results in comparison to the fine-tuned baseline ( size=0).\nFigure 2  ###reference_###  ###reference_### shows that appending 100 augmented samples leads to a substantial performance boost of  points over the baseline.\nHowever, as more augmented samples are introduced, we see a gradual decline in performance to .\nWe attribute the observed fluctuations to the explanation that, while additional generated variations enrich the diversity of the training data, a relatively small test set does not present such high variability of the real variation distribution, and thus the additional augmentation introduces disproportionally much noise.\nFigure 3  ###reference_###  ###reference_### shows the potential reason for the fluctuation: an increasing number of new variants is introduced as more augmented data is appended to the training dataset.\nNevertheless, even with 8K samples, the improvement in performance compared to the baseline is close to  point in F1 score (as shown in Figure 2  ###reference_###  ###reference_###).\nIn sum, we find that controlling the size of the synthesized data influences the level of generalization achieved, but regardless of the size , adding orthographic variants to the dataset improved the baseline model performance on NaijaSenti.\n###figure_4### We further explore training stability by examining the cross-entropy of the model\u2019s prediction , in the context of sentiment analysis.\nTable 7  ###reference_###  ###reference_### demonstrates the cross-entropy in the training epochs, as a crucial metric under cross-validation to gauge the effectiveness of our classification model.\nCross-entropy informs us on how well the model\u2019s predictions match the ground-truth class labels.\nMeasuring cross-entropy is done as an alternative to accuracy, which is less applicable to smaller datasets.\nThe results in Table 7  ###reference_###  ###reference_### show that models trained using samples with orthographic variations are characterised by lower cross-entropy compared to the counterpart models that are not trained with appended orthographic variation samples.\nThis indicates that our orthographic variation augmentation approach leads to more accurate sentiment analysis results.\n###figure_5###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Machine translation experiment",
            "text": "Given that orthographic variation is even more likely to occur between texts than within texts, we test the effect of orthographic variation augmentation on a test set from an unseen corpus. Specifically, we investigate the cross domain transfer of a model trained on the JW300 (with and without orthographic variations) and tested on the Naija Treebank test set.\nFigure 4  ###reference_### shows that our augmentation approach improves the performance of the machine translation model, on the unseen Naija Treebank during training, leading to an zero-shot generalization along with more augmented samples.\nIn fact, the results show that the higher the , the higher the model performance over the baseline  where no variation-enhanced sentences are used, further supporting the effectiveness of the variation augmentation.\n###figure_6### ###figure_7### The data augmentation approach introduces new word variants to the training data; naturally, the number of new variants introduced increases when the size of the augmented data increases.\nTo understand how these new variants affect the generalization of the model, we correlate the number of new variants for various  sizes with the performance improvement in the domain shifting setting.\nTable 9  ###reference_### shows that with a higher number of new variants, the results also show a performance improvement on the Naija Treebank\u2019s test set.\nWe attribute this phenomenon to two factors: (1) certain newly created variations are absent in the current test split but are indeed found in real Pidgin corpora, as exemplified by \u2018everytin\u2019; (2) in other cases, such as \u2018piple\u2019 and \u2018pipl\u2019, these variants feature minor lexical alterations compared to the variant \u2018pipol\u2019 that is actually found in the dataset.\nEven though certain generated variants, such as \u2018piple\u2019 and \u2018pipl\u2019, are classified as new variants, parts of these created variants are indeed present in the original text (e.g. \u2018pip\u2019 in the three variants \u2018piple\u2019, \u2018pipl\u2019, and \u2018pipol\u2019) and the model might capture nearly identical semantics due to their lexical overlap.\nThis could lead to the model being able to generalize to previously unseen tokens when conducting inference on the test splits."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Dataset, networks and training details",
            "text": "We leverage T5 Base Raffel et al. (2020  ###reference_b31###) for the JW300 translation benchmark, which consists of 29K training samples. We perform bi-directional translation for the parallel English-Pidgin datasets, which are the Bible, JW300 (Agi\u0107 and Vuli\u0107, 2019  ###reference_b5###) and the Naija Treebank (Caron et al., 2019  ###reference_b9###).\nAll model variants are evaluated on the test set using BLEU scores.\nWe fine-tuned the T5-base model with additional real data samples (DataAug) for 20 epochs with a batch size of 64.\nWe configured the sequence length to be 196 characters and used the AdamW optimizer Loshchilov and Hutter (2019  ###reference_b20###) with a learning rate of 0.0001.\nWe appended the orthographic variation samples as augmentation, using =20,000 samples. Depending on the model type (only JW300 or a combination of JW300 and the Bible or Treebank), the augmentation samples were drawn from either a single source or from multiple sources."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Main results",
            "text": "Table 8  ###reference_### shows that our orthographic variation augmentation approach leads to improvements in BLEU scores in both translation directions.\nThe results show that this also surpasses the standard data augmentation technique in performance.\nFor DataAug (including real samples of additional datasets), the model\u2019s performance benefits from injecting more accurately labeled real training samples, leading to an improvement in BLEU points.\nWe observe further improvement by using our orthographic variant generation approach, suggesting that the augmented samples are an effective augmentation approach to\nenrich the dataset."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Ablation study",
            "text": "Given that orthographic variation is even more likely to occur between texts than within texts, we test the effect of orthographic variation augmentation on a test set from an unseen corpus. Specifically, we investigate the cross domain transfer of a model trained on the JW300 (with and without orthographic variations) and tested on the Naija Treebank test set.\nFigure 4  ###reference_###  ###reference_### shows that our augmentation approach improves the performance of the machine translation model, on the unseen Naija Treebank during training, leading to an zero-shot generalization along with more augmented samples.\nIn fact, the results show that the higher the , the higher the model performance over the baseline  where no variation-enhanced sentences are used, further supporting the effectiveness of the variation augmentation.\n###figure_8### ###figure_9### The data augmentation approach introduces new word variants to the training data; naturally, the number of new variants introduced increases when the size of the augmented data increases.\nTo understand how these new variants affect the generalization of the model, we correlate the number of new variants for various  sizes with the performance improvement in the domain shifting setting.\nTable 9  ###reference_###  ###reference_### shows that with a higher number of new variants, the results also show a performance improvement on the Naija Treebank\u2019s test set.\nWe attribute this phenomenon to two factors: (1) certain newly created variations are absent in the current test split but are indeed found in real Pidgin corpora, as exemplified by \u2018everytin\u2019; (2) in other cases, such as \u2018piple\u2019 and \u2018pipl\u2019, these variants feature minor lexical alterations compared to the variant \u2018pipol\u2019 that is actually found in the dataset.\nEven though certain generated variants, such as \u2018piple\u2019 and \u2018pipl\u2019, are classified as new variants, parts of these created variants are indeed present in the original text (e.g. \u2018pip\u2019 in the three variants \u2018piple\u2019, \u2018pipl\u2019, and \u2018pipol\u2019) and the model might capture nearly identical semantics due to their lexical overlap.\nThis could lead to the model being able to generalize to previously unseen tokens when conducting inference on the test splits."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Discussion",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   Overgeneration",
            "text": "Although Nigerian Pidgin does not have a standard orthography, there is still a degree of plausibility in the generations that can be found in natural language. Automatically generating orthographic variations can thus lead to implausible variations, which, in turn, can lead to a decrease in generalization. A manual analysis of the data indicated that many of the implausible variations generated by our approach were synthesized when multiple rules were applied on the seed word, and when the pronunciation of a word was strongly affected. For instance, \u2018anything\u2019 became \u2018onytin\u2019 by applying three different rules. Such candidates are less plausible variations than one that has higher phonological similarity (also see Appendix B  ###reference_### on generating irrelevant words). In our framework, we address the issue of overgeneration by sampling the variations through the phonological weighted Levenshtein distance."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Generalization to unseen domains",
            "text": "The dataset used to identify common orthographic variations consisted of texts from the bible, a religious magazine (JW300), and Nigerian Pidgin conversations (Naija Treebank). The variations observed within these sources appeared to be more author-specific than domain-specific \u2013 that is, the degree and variety of variations often involve specific authors instead of domains. An example of this can be seen in Table 2  ###reference_###: the Bible and JW300 are from the same domain, but the types of variations and the frequencies are very different.\n\nThe experiments reported in Sections 4 and 5 show that the proposed method can be applied across domains. For sentiment analysis and machine translation tasks, we created separate frameworks on two distinct domains; social media and religion, respectively. Our results showed positive correlations when the increase of the model performance and augmenting the variation-enhance examples (also see Tables 6  ###reference_### and 8  ###reference_###). Our proposed approach thus appears to be domain-agnostic and can be generalized to unseen domains."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "6.3.   Code-switching",
            "text": "Nigerian Pidgin is an English-lexified language that draws from other local languages as well. Code-switching between Pidgin, English, and local languages is motivated by factors such as formality, setting, interpersonal relations and audience (Agbo and Plag, 2020  ###reference_b4###).\nFor example, in a Hausa community, there might be more code-switching with Hausa words, but in a mixed community of Hausa and Yoruba speakers, there might be more code-switching with (Nigerian-)English words.\nTable 4  ###reference_### presents an example of how English and Pidgin words co-occur in one uttereance: \u2018E come later dey serve as pioneer\u2019 translates to English as \u2018Later, he began serving as a pioneer.\u2019 In this sentence, the speaker mixes Pidgin words (e.g., \u2018dey\u2019) with (Nigerian-)English words (e.g., \u2018serve as pioneer\u2019). This blend of English and Nigerian Pidgin is a form of code-switching commonly observed within the text (although it could be argued that there might not be a Pidgin equivalent of \u201cpioneer\u201d available in the lexicon).\n\nOur current work proposes a framework to describe various types of orthographic variations commonly found in Nigerian Pidgin texts, and model this orthographic variation. As a result, words like \u2018they\u2019 in the dataset might be generated as \u2018dey\u2019. Through this process, the original text is enhanced with more orthographic variations, which potentially creates the illusion of a higher rate of code-switching than is originally present in the data. \nHowever, note that this view on code-switching relies solely on orthographic choices made by the transcriber, whereas code-switching also pertains to vocabulary items that are not dependent on orthographic choices (such as the Pidgin word \u2018wetin\u2019 meaning what). We refer the reader to Agbo (2022  ###reference_b3###) for more information on code-switching in Nigerian Pidgin."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "In this paper, we address the issue of orthographic variation in a mostly spoken language that does not have a standardized orthography in place: Nigerian Pidgin (Naija).\nWe provide an analysis of the types of orthographic variations commonly found in Nigerian Pidgin writing.\nBased on this analysis, we propose a novel phonological-based word synthesizing framework to augment the corpus with orthographic variations.\nWe examine the concept of synthesized variation on two main tasks: sentiment analysis and machine translation. The results demonstrate the effectiveness of adding synthesized orthographic variation to the dataset instead of collecting new samples."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Acknowledgement",
            "text": "This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 Project-ID 232722074 \u2013 SFB 1102 Information Density and Linguistic Encoding."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Limitations",
            "text": "We acknowledge the inherent limitations in our work.\nDespite the effectiveness of our orthographic variation generation framework, we still observed the overgeneration of variations that are less likely to occur.\nWe face a challenge in precisely quantifying the correlation between the model\u2019s performance and the extent of overgeneration. The absence of such measurement hinders a comprehensive assessment of the impact of overgeneration on model performance.\nMoreover, our evaluation of NaijaSenti is constrained to an in-domain context due to the lack of an out-of-domain corpus that represents stronger or novel variations, potentially resulting in an underestimation of the model\u2019s capabilities.\nAdditionally, the exploration of alternative data-driven sampling methods is a important option; however, it requires additional non-annotated Pidgin data, which is currently unavailable.\nLastly, our research focuses on Nigerian Pidgin, which is an English-lexified language; this allowed us to exploit the availability of SOTA English-based tools. It is possible that our observations do not generalize well to other languages that are either not English-lexified (due to a less resources being available for other languages), or display more differences compared to their lexifier."
        }
    ],
    "url": "http://arxiv.org/html/2404.18264v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.18264v1",
        "paper_title": "Modeling Orthographic Variation Improves NLP Performance for Nigerian Pidgin",
        "research_background": "### Motivation\n\nThe paper is motivated by the poor performance of NLP models on under-resourced languages, particularly many African and South-East Asian languages, due to a lack of high-quality, sufficiently sized datasets. Nigerian Pidgin, despite having approximately 100 million speakers, is largely absent from NLP research and faces significant challenges due to orthographic variations \u2013 differences in spelling that are not yet standardized. These variations introduce noise into datasets, adversely affecting the performance of NLP tasks like sentiment analysis and machine translation.\n\n### Research Problem\n\nThe research problem tackled by this paper is the detrimental impact of orthographic variation on the performance of NLP models when applied to Nigerian Pidgin, a language with no standardized orthography and significant spelling variations. The fundamental aim is to mitigate the negative effects of these variations to improve the performance of NLP models on Nigerian Pidgin data.\n\n### Relevant Prior Work\n\n- **Masakhane Community**: This initiative aims to bolster NLP research in African languages, addressing the lack of resources and the subsequent poor performance of NLP models for these languages.\n- **Noise Sensitivity in NLP Models**: Bergmanis et al. (2020) highlighted how sensitive NLP models are to noise in the data, including orthographic variations, which cause significant degradation in model performance.\n- **Orthographic Challenges in Creoles and Pidgins**: Similar challenges exist in other languages without standardized orthography. Mensah et al. (2021) discussed the lack of normalized orthographies in many languages, exacerbating the problem of orthographic variation.\n\nThe paper leverages these insights to understand and address the specific challenges posed by orthographic variation in Nigerian Pidgin, proposing a novel framework for augmenting training data by synthesizing orthographic variations at the phonological level.",
        "methodology": "### Modeling Orthographic Variation Improves NLP Performance for Nigerian Pidgin\n\n#### Methodology:\n\nNigerian Pidgin, also known as \u2018Naija\u2019, is an English-based contact language with influences from Portuguese and Nigeria\u2019s indigenous languages. As a predominantly spoken language, written forms of Nigerian Pidgin have not yet been standardized, leading to significant orthographic variation. The orthographies proposed by linguistic bodies such as the Naij\u00e1 Langwej Akedemi (NLA) and Mensah et al. have not been widely adopted by non-linguist speakers. Hence, native speakers generate their written forms which are phonetically based but inconsistent, leading to data sparseness and noise in language models.\n\nTo address this orthographic inconsistency, the paper proposes generating \"adversarial\" training data to train models capable of handling these variations. The methodology involves several key steps:\n\n1. **Identification and Analysis of Variations**: The paper begins with a qualitative analysis of common orthographic variations in Nigerian Pidgin to understand the types of variations that occur.\n\n2. **Phonetic Transcription**: The sound of words is accessed using the phonemization tool \u2018phonemizer\u2019, which transcribes written words into phonemes consistent with English pronunciation rules. This is shown to work well for Pidgin words.\n\n3. **Alignment with GIZA++**: The acoustic symbols are aligned with corresponding characters using GIZA++ trained over iterations with the IBM4 model. Preprocessing is done to merge symbols that should be treated as units (e.g., \u2018th\u2019, \u2018ng\u2019).\n\n4. **Rule-Based Generation of Variants**: From the aligned data, variation rules based on the qualitative analysis are generated. These rules are applied to words to produce orthographic variants.\n\n5. **Filtering Implausible Variants**: Variants that are phonetically implausible are filtered out using Phonologically Weighted Levenshtein Distance (PWLD). This metric measures the similarity in pronunciation between the original word and its variants, assigning lower distances to more similar phonemes.\n\n6. **Ranking and Normalization**: Orthographic variants are ranked based on their PWLD scores. Higher probabilities are assigned to variants with lower distances by normalizing over the inverse distances to ensure more plausible variants are selected.\n\n#### Key Components and Innovations:\n- **Phonemizer for Phonetic Transcription**: Leverages an existing tool to accurately transcribe written words into phonemes, useful for phonetic-based spelling variations.\n- **GIZA++ for Alignment**: Utilizes a robust alignment tool to map acoustic symbols to characters, essential for understanding how phonetic changes manifest in writing.\n- **Qualitative and Rule-Based Approach**: Combines qualitative analysis with rule-based generation, allowing for the systematic creation of orthographic variants.\n- **Phonologically Weighted Levenshtein Distance (PWLD)**: Innovatively applies this metric to filter out implausible variants based on phonetic similarity, refining the synthetic data generation process.\n- **Inverse-Distance Weighting for Selection**: Uses normalized probabilities to ensure frequently selected variants maintain a phonetic resemblance to the originals, preventing the selection of dissimilar variants.\n\nThis methodology aims to address the challenge of orthographic variation in Nigerian Pidgin by creating synthetic data that reflect realistic variations, ultimately improving NLP model performance.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Models and Datasets\n- **Model Used**: RoBERTa is adopted for the sentiment analysis task.\n- **Dataset**: The primary dataset is NaijaSenti, which is used for evaluating the performance.\n\n#### Augmentation and Configuration\n- **Baseline Comparison**: The baseline model is a fine-tuned version of RoBERTa without any augmented samples.\n- **Evaluation Configuration**: The model\u2019s performance with different sizes of augmented samples is compared to the baseline.\n\n#### Evaluation Metrics\n- **Primary Metric**: F1 Score is used to measure the performance improvement.\n- **Stability Metric**: Cross-entropy is evaluated to gauge the effectiveness and stability of the model\u2019s predictions during training epochs.\n\n### Main Experimental Results\n\n- **Stability Analysis**: \n  - **Cross-Entropy Results**: Models trained with orthographic variation samples exhibit lower cross-entropy compared to models without such samples, indicating that the augmented data results in more accurate sentiment analysis.\n\n### Summary\n- **Augmentation Effectiveness**: The inclusion of orthographic variants improves the baseline model performance on NaijaSenti, demonstrating the efficacy of the augmentation approach in enhancing the model\u2019s generalization.\n- **Overall Conclusion**: All augmented datasets lead to performance gains over the baseline."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the optimal size of augmented samples for the sentiment analysis task using RoBERTa and to evaluate the effect of orthographic variation augmentation on model performance.",
            "experiment_process": "The experiment tests various sizes of extra data points within a range from 50 to 8,000 augmented samples for the sentiment analysis task. The model used is RoBERTa, comparing performance against a fine-tuned baseline where size=0. F1 score and cross-entropy are used as evaluation metrics. Cross-entropy is measured to gauge the effectiveness of the classification model under cross-validation.",
            "result_discussion": "Appending 100 augmented samples leads to a substantial performance boost over the baseline. However, performance declines as more augmented samples are introduced, due to excessive noise introduced by the additional variations. Despite this, even with 8,000 samples, there is still an improved performance compared to the baseline. Models trained with orthographic variation samples exhibit lower cross-entropy, indicating more accurate sentiment analysis results.",
            "ablation_id": "2404.18264v1.No1"
        },
        {
            "research_objective": "To test the effect of orthographic variation augmentation on cross-domain transfer by evaluating a model trained on JW300 and tested on the Naija Treebank test set.",
            "experiment_process": "The experiment investigates cross-domain transfer, with the model trained on JW300 (with and without orthographic variations) and tested on the Naija Treebank test set. The performance on the unseen Naija Treebank corpus during training is compared. The number of new word variants introduced in the training data increases with the size of the augmented data, and their impact on model generalization is analyzed.",
            "result_discussion": "The augmentation approach improves machine translation model performance on the unseen Naija Treebank test set, showing zero-shot generalization with more augmented samples. The higher the number of new variants introduced, the better the performance improvement in the domain-shifting setting. Certain newly created variations absent in the test split are found in real Pidgin corpora, and minor lexical alterations between variants help in capturing nearly identical semantics, aiding model generalization to unseen tokens.",
            "ablation_id": "2404.18264v1.No2"
        }
    ]
}