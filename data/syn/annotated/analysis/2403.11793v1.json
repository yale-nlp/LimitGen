{
    "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus",
    "abstract": "The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Recent Large Language Models (LLMs) have demonstrated performance levels close to that of humans, but experimental results showed that they lacked planning ability through thought or reasoning (Bubeck et al., 2023  ###reference_b6###). Consequently, a key question in recent language model research is: Can LLMs think? To address this question, new benchmarks for measuring reasoning abilities such as MathVista (Lu et al., 2024  ###reference_b25###), Bonagard-Logo (Nie et al., 2020  ###reference_b33###), and Raven (Zhang et al., 2019  ###reference_b59###) have been proposed. Among them, the Abstract and Reasoning Corpus (ARC) (Chollet, 2019  ###reference_b8###) emerged to be one of the representative benchmarks for assessing reasoning abilities. As shown in Fig. 1  ###reference_### below, each task in ARC consists of 2\u20135 example pairs and a problem input grid. The goal is to infer rules from given example pairs and apply them to the problem input grid. Input and output grid size can vary from a minimum of  to a maximum of , with each grid having up to 10 different colors.\n###figure_1### ARC remains an unsolved challenge despite its seemingly simple content and evaluation methods. It demands a high level of abstraction and multiple reasoning steps, reasons why conventional deep learning techniques have not achieved success. The best-performing models to date have only achieved an accuracy of 30% (Lab42, 2024  ###reference_b22###), while LLMs have shown an accuracy of around 10% (Mirchandani et al., 2023  ###reference_b31###). Compared to the average human accuracy of 80% (Johnson et al., 2021  ###reference_b20###), these results suggest significant differences in reasoning and abstraction capabilities between humans and LLMs. However, in-depth research into how LLMs reason and how their reasoning differs from humans is lacking. This has led to calls for a shift from a results-focused evaluation to a more nuanced analysis of the process (Chang et al., 2024  ###reference_b7###; Huang and Chang, 2023  ###reference_b18###; Xue et al., 2023  ###reference_b57###), indicating a need for a new perspective that evaluates reasoning abilities based on the process rather than just the outcome.\nTo overcome the limitations of result-oriented analysis in artificial intelligence, this study embraces the Language of Thought Hypothesis (LoTH) (Fodor, 1975  ###reference_b12###), which is predominantly applied in the philosophy of mind to explain human reasoning. According to LoTH, effective reasoning encompasses three essential characteristics: logical coherence, the ability to follow basic logical principles; compositionality, the capability to construct complex ideas from simpler components; and productivity, the capacity to formulate an indefinite number of thoughts or solutions using a finite set of elements.\n###figure_2### While there have been studies questioning LLM\u2019s reasoning abilities (Bubeck et al., 2023  ###reference_b6###; Valmeekam et al., 2024  ###reference_b42###), a detailed analysis is lacking. Therefore, this study aims to utilize ARC to examine their reasoning skills from three perspectives of LoTH. To achieve this goal, we designed three separate experiments as follows:\nLogical Coherence: Using prompting techniques, LLMs are set to solve ARC tasks. By analyzing the types of ARC tasks it can solve and the process of solving them, we aim to determine whether LLMs are capable of logical reasoning and whether its logic is consistent.\nCompositionality:\nARC tasks are known to be solvable through the application of step-by-step functions (Hodel, 2023a  ###reference_b16###). With such functions provided, we aim to ascertain whether LLMs can identify the combinations of functions that are needed to solve a task. This process can be broken down into two parts: understanding how LLMs manipulate the problem input and determining whether they can achieve the desired results through multiple steps of manipulation.\nProductivity: We tested if LLMs can create new input-output pairs for ARC tasks. We selected tasks with multiple inputs leading to the same output, devised prompts for inverse transformation, and assessed LLMs\u2019 ability to generate varied inputs based on these prompts.\nAs a result, we have confirmed that the current level of LLM possesses a basic understanding of images and is capable of simple types of compositional object manipulations. However, compared to human reasoning abilities, LLM lags in three areas: 1) It is weak in understanding aspects such as objectness in images. 2) Its logical reasoning abilities, especially in a step-by-step manner, are weak. 3) It struggles with understanding and generating unseen representations.\nFinally, this study summarizes and presents recent trends proposed to address the weaknesses in abstraction abilities and reasoning capabilities. Analyzing the reasoning abilities of LLMs according to the components of human reasoning and discussing how to enhance each component represents a differentiated approach from previous research. It offers a fresh perspective for measuring and advancing the reasoning capabilities of LLMs in the future."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Preliminaries",
            "text": "This section aims to explain why we chose the LoTH perspective and ARC before starting a detailed evaluation of LLM\u2019s reasoning capabilities. First, we will look at existing definitions of reasoning abilities and show why LoTH is useful in the perspective of measuring intelligence in Section 2.1  ###reference_###. Then, by looking at ARC benchmarks and how they vary in Section 2.2  ###reference_###, we highlight the need for using both the building blocks of meaning and the rules of combining them to tackle ARC tasks. This approach matches the LoTH perspective, showing that ARC is a good benchmark for studying LLM through the point of view of human reasoning."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Assessing Reasoning Ability of LLMs",
            "text": "Efforts to evaluate LLMs\u2019 capabilities continue, underscoring strengths in image and text generation. Especially, analysis confirms LLMs possess elements of a World Model (Gurnee and Tegmark, 2023  ###reference_b14###), indicating potential in inference tasks. Despite these capabilities, challenges in reasoning are noted (Valmeekam et al., 2024  ###reference_b42###), with errors such as distortion and incomplete reasoning highlighted (Li et al., 2024  ###reference_b24###). Research suggests these reasoning abilities can improve through methodological adjustments. Furthermore, studies indicate that complex compositionality remains challenging (Dziri et al., 2023  ###reference_b11###).\nThe divergent claims about the abilities of LLMs stem from result-centric measurement methods. Turing was the first figure to shift the approach to inference towards consequential direction (Turing, 1950  ###reference_b41###). Subsequently, Wiener (Wiener, 1950  ###reference_b52###), McCulloch and Pitts (McCulloch and Pitts, 1943  ###reference_b29###), and Rosenblatt (Rosenblatt, 1958  ###reference_b37###) shifted to studying methods for measuring performance rather than focusing on the process. Recently, Chollet attempted to quantify inference abilities from a consequential perspective (Chollet, 2019  ###reference_b8###). However, these studies all focus on what reasoning can achieve using a result-oriented approach, without specifying the elements that constitute reasoning ability. West et al. (West et al., 2024  ###reference_b51###) raised concerns about evaluating the reasoning ability of LLMs from a consequentialist perspective, as the generation capability of LLMs may not necessarily depend on comprehension abilities.\nTherefore, a new perspective is needed to evaluate AI\u2019s inference processes; Language of Thought Hypothesis (LoTH) enhances discussions by integrating reasoning components with quantitative metrics. LoTH posits that inference involves manipulating mental representations, a view that dominates the philosophy of mind due to its explanatory power over logical coherence, compositionality, and productivity observed in human cognition. These mental representations are believed to have a compositional syntax and combinatorial semantics. Our study compares with prior works and evaluates LLMs\u2019 inference capabilities through the LoTH, marking progress by assessing aspects like logical coherence, compositionality, and productivity.\nExploring deeper into the three perspectives of LoTH offers strong justification for improving reasoning capabilities. These principles help in developing the ability to process information and solve tasks similar to human reasoning. Logical coherence ensures LLMs can reason without contradictions, compositionality allows LLMs to adapt known knowledge to new scenarios, and productivity enhances LLMs\u2019 capacity to generate results based on given rules. Thus, adopting these perspectives of LoTH aids LLMs in achieving more human-like reasoning, enabling them to address complex problems with innovative and valid results."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Advantages of using ARC as Reasoning Benchmark",
            "text": "In exploring benchmarks suitable for evaluating inference abilities through the lens of the Language of Thought Hypothesis (LoTH), the Abstraction and Reasoning Corpus (ARC) emerges as a compelling candidate. ARC tasks, necessitating combinatorial syntax and compositional semantics, align with LoTH\u2019s emphasis on logical coherence, compositionality, and productivity, validating its role as an appropriate benchmark for reasoning abilities."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1. Comparison Between ARC and Its Variants",
            "text": "When comparing ARC with its variant benchmarks, it becomes evident that ARC poses problems that require a combination to solve. Despite its simple rules, ARC remains a benchmark with relatively low accuracy. LLM achieves an accuracy of 15% (Qiu et al., 2024  ###reference_b35###), while non-LLM AI models reach a maximum of 31% (Lab42, 2024  ###reference_b22###), which significantly differs from the human average accuracy of 80% (Johnson et al., 2021  ###reference_b20###). The excessively high complexity of abstraction and reasoning is a major challenge of ARC. Consequently, various attempts to lower the difficulty of ARC by securing intermediate stages have naturally emerged. Those variants mainly fall into two categories: one focuses on reducing the complexity of abstraction by simplifying input images, while the other aims to reduce the complexity of reasoning dimensions by decreasing the number of steps needed to solve. Below are examples of variant benchmarks.\n1D-ARC (Xu et al., 2023b  ###reference_b56###) is a benchmark that reduces the dimensionality of ARC from 2D to 1D, aiming to simplify the complexity of the ARC. This transformation simultaneously maintains the core prior knowledge of ARC while reducing the dimensionality, thus lowering the complexity of the existing ARC task for enhanced research feasibility. Most importantly, the one-dimensionality of 1D-ARC effectively tackles the challenge of object cohesion. The unresolved object cohesion, regarded as one of the fundamental aspects of human cognition, poses a challenge in solving ARC tasks. Due to its successful handling of object cohesion, LLMs exhibited high accuracy in solving 1D-ARC tasks.\n###figure_3### MC-LARC (Shin et al., 2023  ###reference_b39###) adopts a multiple-choice format, featuring a rule sentence derived from ARC tasks alongside similar, but incorrect rule sentences. Solvers are tasked with selecting the accurate rule from five textual options, using the ARC examples as a reference. Effective resolution of MC-LARC requires solvers to semantically link these examples to the text options and distinguish the correct choice from the false ones. The pivotal shift from ARC to MC-LARC is transitioning from generative tasks to multiple-choice tasks.\n###figure_4### Mini-ARC (Kim et al., 2022  ###reference_b21###) is a benchmark dataset limiting the grid size to 5  5. It simplifies the ARC task by maintaining the core prior knowledge of ARC. It is eliminating the unnecessary process of varying input/output grid sizes. However, unlike the previously introduced 1D-ARC and MC-LARC, Mini-ARC remains similar to ARC in that it retains the characteristics of a 2D generative task. As a result, Mini-ARC remains challenging similar to ARC.\n###figure_5### GPT-4 showed strong performance in MC-LARC, with approximately 75% (Shin et al., 2023  ###reference_b39###), and in 1D-ARC, with about 90% (Xu et al., 2023b  ###reference_b56###). However, it exhibited lower performance in Mini-ARC, around 15% (Qiu et al., 2024  ###reference_b35###). As shown in Fig. 6  ###reference_###, MC-LARC and 1D-ARC reduced the difficulty on a reasoning step level, whereas Mini-ARC differs by reducing image complexity as a benchmark. The significant decrease in difficulty of benchmarks that reduced step-by-step reasoning complexity supports the idea that ARC tasks requires a combination of sequential transformations. The main difference between MC-LARC, 1D-ARC, Mini-ARC, and ARC lies in whether specific functions must be combined to solve the task. 1D-ARC is designed around relatively straightforward transformations, thereby obviating the necessity for devising complex sequences of transformation combinations. Similarly, MC-LARC provides multiple choices, easily solvable by comparing options without the need for multiple stages of inference. However, Mini-ARC presents a unique challenge. Although the grid size is fixed at , the steps required to solve the task as complex as those in the standard ARC. Therefore, the performance difference between MC-LARC, 1D-ARC, and Mini-ARC implies the necessity of combinatorial syntax to combine functions to solve ARC.\n###figure_6###"
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2. Improvements on ARC Accuracy By Adding Object Information",
            "text": "ARC is a benchmark that requires extracting a large amount of semantic information from given images. Two prior research findings support this: 1) the improvement observed in studies providing additional information to solve ARC, and 2) comparative research results with other benchmarks. In one experiment, incorporating object information represented as graphs led to a notable increase in accuracy, with results nearly doublin  (Xu et al., 2023b  ###reference_b56###). This significant increase in accuracy underscores the importance of semantic information as a critical factor in resolving ARC tasks. Further research indicate that ARC contains more abstract information compared to other benchmarks, as illustrated in Table 1  ###reference_###. Chollet, who proposed this benchmark, also argued that traditional feature extraction methods alone would not be sufficient to solve ARC, as it requires complex shapes and even additional information about transformation processes (Chollet, 2019  ###reference_b8###). These observations confirm the importance of employing strategies that can interpret the abstract content essential to solving ARC challenges.\nIn summary, the variation in LLM accuracy with different ARC variations supports the need for combinatorial syntax in solving ARC, while the changes in accuracy with object information highlight the necessity of compositional semantics. These results demonstrate that ARC effectively represents the LoTH perspective, thereby validating the rationale for using it in this study."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Evaluating the Inferential Capabilities of LLMs Using the ARC Benchmark",
            "text": "To evaluate whether LLMs possess inferential capabilities, one could compare these capabilities to human reasoning. As explained in Section 2.1  ###reference_###, according to the Language of Thought Hypothesis (LoTH), human reasoning can be broadly divided into three main components: Logical Coherence (Section 3.1  ###reference_###), Compositionality (Section 3.2  ###reference_###), and Productivity (Section 3.3  ###reference_###). To examine each aspect of reasoning capabilities in LLMs, we utilized ARC, aiming to assess both the validity as a benchmark for evaluating reasoning capabilities and the inferential abilities of LLMs themselves.\nWe conducted an analysis to identify problems that LLMs solve well and those they struggle with. Table 3  ###reference_### presents the accuracy of LLMs across problem difficulty levels classified by humans. The classification was based on the existing categorization relying on perceived difficulty by humans (Borsky, 2021  ###reference_b5###). As a result, we discovered a tendency where problems perceived as difficult by humans align closely with those challenging for LLMs. Difficult problems shared two commonalities: 1) they required lengthy inference processes to solve, and 2) they involved considering multiple simultaneous problems to extract information about changes. An example from Fig. 10  ###reference_### illustrates this point: a task classified as \u2018Entry\u2019, only requires a single step of coloring, while a task classified as \u2018Hard\u2019, requires three steps: recognizing each object, identifying the priority of each object, and merging each object considering their priority. \u2018Easy\u2019 and \u2018Medium\u2019 are tasks that require relatively more complex steps than \u2018Entry\u2019 and fewer steps than \u2018Hard\u2019. Considering these observations, it can be inferred that artificial intelligence possesses simple forms of visual logic, but cannot handle complex combinations of logic.\n###figure_7### ###table_1### Among the tasks where LLM has correctly answered, the process in some cases is flawed. Fig. 11  ###reference_### illustrates various cases. To solve the task, one needs to 1) identify  objects within the input grid, 2) count the number of black squares inside each object, and 3) extract the object with the largest number of black squares. However, CoT, LtM, and ToT attempted to solve this task in the wrong ways. For CoT, objects appearing in the input grid were sorted first, then the object in the middle was selected. Even though CoT got the answer right, the way of sorting the object was not understandable. For LtM and ToT techniques, it was understood that a specific object needed to be selected from the given input grid to solve the task. However, they mistakenly recognized objects from the test input grid. These solutions share a commonality in that they fail to explain a logically consistent rule between the provided examples of different training inputs and outputs. In other words, regardless of prompting techniques such as CoT, LtM, or ToT, LLMs have yet to demonstrate logical consistency in discovering a single rule that applies consistently across the examples provided to solve the task.\n###figure_8### If LLMs have a consistent logical structure, they should produce consistent results for the same type of task. To verify this consistency, we examined whether LLMs could solve two tasks presented in Fig. 12  ###reference_###. Those tasks can be classified as the same type because they both require a fixed output grid size and necessitate the repetitive generation of a specific pattern. To solve these tasks, appropriate functions such as rotation and flipping are used to modify and combine the input grids. However, while all three prompt techniques could solve the first task, none could solve the second one. Considering that pattern recognition and application are relatively strong points for LLMs, the inability to consistently solve tasks at a human-like level suggests that LLMs lack a consistent logical structure.\n###figure_9### ###figure_10### To begin with, we analyzed whether LLM understands the functionality of the DSLs by examining its ability to accurately generate outputs that perform the specified transformations when given the DSLs and ARC grids as input. The result shows that LLM can perform the transformations on a  grid without errors. This result demonstrates that LLM understands the DSLs provided as input in this experiment and knows how the state changes when applying a certain DSL.\n###figure_11### To determine whether LLMs can find the transformations to solve ARC tasks with combinations of DSLs, we evaluated whether the state transformations are necessary for generating the correct grid. The result indicates that while the given DSLs are insufficient to achieve the solution, they seem capable of resolving the task through subsequent steps. Furthermore, when asked to predict the next step, LLM shows a tendency to select the same DSL repeatedly, with a 75% rate of selecting the same DSL that was selected just before. This tendency indicates that LLM understands the functionality of the combined DSLs, as it can compare the resulting grid after passing through the combined DSLs with the correct grid. However, it cannot predict which DSL combinations are necessary to achieve the desired outcome.\nDirect replication of other grids in the same task was a recurring issue, even with prompts designed to prevent such outcomes. As shown in Fig. 17  ###reference_###, LLMs often simply copy the input from given example pairs to create new inputs. This error stemmed from LLMs\u2019 failure to deduce meaningful rules from provided examples.\nThere was another limitation in considering small steps to generate inputs from the given output. As a result, it was challenging to determine whether generated examples could be solved by specific rules of the task, leading to frequent occurrences of generated examples that could not be solved by rules of the task. Fig. 17  ###reference_### is the example. In the case of an input where all the vertices of the square have been erased, the color of the vertices cannot be determined, so the given output cannot be inferred.\n###figure_12###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Capability of LLMs 1: Logical Coherence",
            "text": "In Section 3.1  ###reference_###, we aim to evaluate LLMs\u2019 logical coherence, a first fundamental aspect of the LoTH. Logical coherence is the ability to understand a given logic and apply it consistently across different contexts. This concept is crucial in human cognitive processes as it facilitates the construction of sentence structures based on consistent logic, which is essential for solving various tasks. Such an ability is particularly relevant to the rule inference required in ARC tasks, where the challenge is to identify common logical patterns among given examples and use them to deduce the most logically coherent answer.\nWe conducted an analysis to identify problems that LLMs solve well and those they struggle with. Table 3  ###reference_###  ###reference_### presents the accuracy of LLMs across problem difficulty levels classified by humans. The classification was based on the existing categorization relying on perceived difficulty by humans (Borsky, 2021  ###reference_b5###  ###reference_b5###). As a result, we discovered a tendency where problems perceived as difficult by humans align closely with those challenging for LLMs. Difficult problems shared two commonalities: 1) they required lengthy inference processes to solve, and 2) they involved considering multiple simultaneous problems to extract information about changes. An example from Fig. 10  ###reference_###  ###reference_### illustrates this point: a task classified as \u2018Entry\u2019, only requires a single step of coloring, while a task classified as \u2018Hard\u2019, requires three steps: recognizing each object, identifying the priority of each object, and merging each object considering their priority. \u2018Easy\u2019 and \u2018Medium\u2019 are tasks that require relatively more complex steps than \u2018Entry\u2019 and fewer steps than \u2018Hard\u2019. Considering these observations, it can be inferred that artificial intelligence possesses simple forms of visual logic, but cannot handle complex combinations of logic.\n###figure_13### ###table_2### Among the tasks where LLM has correctly answered, the process in some cases is flawed. Fig. 11  ###reference_###  ###reference_### illustrates various cases. To solve the task, one needs to 1) identify  objects within the input grid, 2) count the number of black squares inside each object, and 3) extract the object with the largest number of black squares. However, CoT, LtM, and ToT attempted to solve this task in the wrong ways. For CoT, objects appearing in the input grid were sorted first, then the object in the middle was selected. Even though CoT got the answer right, the way of sorting the object was not understandable. For LtM and ToT techniques, it was understood that a specific object needed to be selected from the given input grid to solve the task. However, they mistakenly recognized objects from the test input grid. These solutions share a commonality in that they fail to explain a logically consistent rule between the provided examples of different training inputs and outputs. In other words, regardless of prompting techniques such as CoT, LtM, or ToT, LLMs have yet to demonstrate logical consistency in discovering a single rule that applies consistently across the examples provided to solve the task.\n###figure_14### If LLMs have a consistent logical structure, they should produce consistent results for the same type of task. To verify this consistency, we examined whether LLMs could solve two tasks presented in Fig. 12  ###reference_###  ###reference_###. Those tasks can be classified as the same type because they both require a fixed output grid size and necessitate the repetitive generation of a specific pattern. To solve these tasks, appropriate functions such as rotation and flipping are used to modify and combine the input grids. However, while all three prompt techniques could solve the first task, none could solve the second one. Considering that pattern recognition and application are relatively strong points for LLMs, the inability to consistently solve tasks at a human-like level suggests that LLMs lack a consistent logical structure.\n###figure_15### ###figure_16###"
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1. Experiment",
            "text": "The perceived deficiency in LLMs\u2019 logical reasoning has been a recurrent critique, with direct attempts at solving ARC tasks yielding success rates below 10% (Mirchandani et al., 2023  ###reference_b31###). To mitigate this, enhancements in LLMs\u2019 logical reasoning are being pursued through prompting techniques like Chain of Thought (CoT) (Wei et al., 2022  ###reference_b50###), Least to Most (LtM) (Zhou et al., 2023  ###reference_b61###), and Tree of Thought (ToT) (Yao et al., 2023  ###reference_b58###).\nThese strategies have been shown to effectively leverage LLMs\u2019 reasoning capabilities (Wang et al., 2019  ###reference_b43###), and has the advantage of allowing for a more transparent analysis for humans, as they involve a step-by-step reasoning process. Therefore, in this experiment, we assess the impact of these prompting strategies on LLMs\u2019 logical coherence by solving ARC tasks. We utilized advanced models, namely GPT-4 and GPT-4-32k, to test their logical reasoning with CoT, LtM, and ToT prompts.\n###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21###"
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2. Method",
            "text": "Simple prompts often lead LLMs to generate basic or inappropriate responses, even for straightforward question-answer tasks. To enhance LLM\u2019s inference abilities, advanced prompting techniques such as Chain of Thought (CoT), Least to Most (LtM), and Tree of Thought (ToT) were introduced.\nCoT includes examples of problem-solving processes in the prompts, encouraging LLMs to engage in causal reasoning and sequential problem-solving (Wei et al., 2022  ###reference_b50###). LtM guides LLMs through a step-by-step solution process, enhancing their ability to tackle complex logic problems efficiently (Zhou et al., 2023  ###reference_b61###). ToT builds on LtM by organizing solution steps into a tree structure, allowing error correction at intermediate stages by exploring different solutions (Xu et al., 2023b  ###reference_b56###).\nEach method has its own pros and cons. CoT lacks transparency in its process, as it presents examples all at once. LtM, while offering clearer process analysis, doesn\u2019t evaluate steps separately, risking error accumulation. ToT provides both transparency and error reduction by considering multiple paths but is slower due to its complex evaluation process.\nIn our experiment, we applied all three techniques to ARC tasks, adapting them to include 2\u2013-5 example pairs, test inputs, and general instruction for solving ARC. These instructions outline the thought process for solving representative problems. CoT\u2019s prompts are based on human-written problem-solving instructions. LtM and ToT, however, prompt LLM to generate its own problem-solving steps. For LtM, we posed an initial question to segment the problem into manageable steps111We ask a starting question: \u201dIf these examples follow the same rule, how would you break down the question to solve it?\u201d, and for ToT, we implemented a similar approach but allowed for multiple proposals, incorporating additional coding to manage and evaluate these proposals in a tree structure."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3. Results",
            "text": "Table 2  ###reference_### displays the performance of LtM, CoT, and ToT when applied to 100 random tasks from the evaluation set. 222Task IDs and used prompts are available at https://bit.ly/Prompt-ARC  ###reference_bit.ly/Prompt-ARC###.\nWe conducted five iterations, reporting the percentage of correctly answered questions for each. Accuracy outside parentheses indicates correct answers only, while inside parentheses indicates both correct answers and processes, as assessed by humans.\nCoT demonstrated 10.6% accuracy, while LtM and ToT exhibited around 6%.\nConsidering both correct answers and processes, accuracy fell to 2\u20134%, revealing that solution processes often lacked correct reasoning, regardless of prompting technique. This suggests LLMs\u2019 logical reasoning capabilities diverge from human reasoning.\n###table_3### Further analysis from three perspectives was conducted on the LLMs\u2019 responses to ARC tasks to probe their inference mechanisms. We evaluated the tasks LLMs solved and failed, assessed cases with correct answers but flawed processes, and scrutinized the logical coherence when solving the same task type.\nWe conducted an analysis to identify problems that LLMs solve well and those they struggle with. Table 3  ###reference_###  ###reference_###  ###reference_### presents the accuracy of LLMs across problem difficulty levels classified by humans. The classification was based on the existing categorization relying on perceived difficulty by humans (Borsky, 2021  ###reference_b5###  ###reference_b5###  ###reference_b5###). As a result, we discovered a tendency where problems perceived as difficult by humans align closely with those challenging for LLMs. Difficult problems shared two commonalities: 1) they required lengthy inference processes to solve, and 2) they involved considering multiple simultaneous problems to extract information about changes. An example from Fig. 10  ###reference_###  ###reference_###  ###reference_### illustrates this point: a task classified as \u2018Entry\u2019, only requires a single step of coloring, while a task classified as \u2018Hard\u2019, requires three steps: recognizing each object, identifying the priority of each object, and merging each object considering their priority. \u2018Easy\u2019 and \u2018Medium\u2019 are tasks that require relatively more complex steps than \u2018Entry\u2019 and fewer steps than \u2018Hard\u2019. Considering these observations, it can be inferred that artificial intelligence possesses simple forms of visual logic, but cannot handle complex combinations of logic.\n###figure_22### ###table_4### Among the tasks where LLM has correctly answered, the process in some cases is flawed. Fig. 11  ###reference_###  ###reference_###  ###reference_### illustrates various cases. To solve the task, one needs to 1) identify  objects within the input grid, 2) count the number of black squares inside each object, and 3) extract the object with the largest number of black squares. However, CoT, LtM, and ToT attempted to solve this task in the wrong ways. For CoT, objects appearing in the input grid were sorted first, then the object in the middle was selected. Even though CoT got the answer right, the way of sorting the object was not understandable. For LtM and ToT techniques, it was understood that a specific object needed to be selected from the given input grid to solve the task. However, they mistakenly recognized objects from the test input grid. These solutions share a commonality in that they fail to explain a logically consistent rule between the provided examples of different training inputs and outputs. In other words, regardless of prompting techniques such as CoT, LtM, or ToT, LLMs have yet to demonstrate logical consistency in discovering a single rule that applies consistently across the examples provided to solve the task.\n###figure_23### If LLMs have a consistent logical structure, they should produce consistent results for the same type of task. To verify this consistency, we examined whether LLMs could solve two tasks presented in Fig. 12  ###reference_###  ###reference_###  ###reference_###. Those tasks can be classified as the same type because they both require a fixed output grid size and necessitate the repetitive generation of a specific pattern. To solve these tasks, appropriate functions such as rotation and flipping are used to modify and combine the input grids. However, while all three prompt techniques could solve the first task, none could solve the second one. Considering that pattern recognition and application are relatively strong points for LLMs, the inability to consistently solve tasks at a human-like level suggests that LLMs lack a consistent logical structure.\n###figure_24### ###figure_25###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Capability of LLMs 2: Compositionality",
            "text": "In Section 3.2  ###reference_###, we investigate compositionality, the second concept of LoTH. Compositionality refers to the ability to generate complex linguistic expressions using simpler ones. This characteristic allows individuals to effectively tackle more complex tasks by breaking sub-tasks down into simpler steps, supporting the notion that humans can solve more complex tasks when faced with them. Strong compositionality enables the resolution of complex tasks and facilitates transparent descriptions of the process, which is also an important aspect from the perspective of LLMs. This section tests compositionality by treating ARC tasks as stepwise compositions of simpler functions.\nTo begin with, we analyzed whether LLM understands the functionality of the DSLs by examining its ability to accurately generate outputs that perform the specified transformations when given the DSLs and ARC grids as input. The result shows that LLM can perform the transformations on a  grid without errors. This result demonstrates that LLM understands the DSLs provided as input in this experiment and knows how the state changes when applying a certain DSL.\n###figure_26### To determine whether LLMs can find the transformations to solve ARC tasks with combinations of DSLs, we evaluated whether the state transformations are necessary for generating the correct grid. The result indicates that while the given DSLs are insufficient to achieve the solution, they seem capable of resolving the task through subsequent steps. Furthermore, when asked to predict the next step, LLM shows a tendency to select the same DSL repeatedly, with a 75% rate of selecting the same DSL that was selected just before. This tendency indicates that LLM understands the functionality of the combined DSLs, as it can compare the resulting grid after passing through the combined DSLs with the correct grid. However, it cannot predict which DSL combinations are necessary to achieve the desired outcome."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1. Experiment",
            "text": "We test the compositional ability of LLMs through ARC. Compositionality consists of two steps: 1) understanding the meaning of the given expressions and 2) combining appropriately the expressions to obtain the desired output. Therefore, we verify whether LLMs understand the meaning of the functions provided for ARC tasks and whether they can combine the functions appropriately to produce the desired results. The result of this experiment indicates that while LLMs sufficiently understand the functions and their relationship with images, their ability to decompose and combine functions to achieve the desired outcome is weak.\n###figure_27###"
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2. Method",
            "text": "This experiment investigates the compositionality of LLMs by providing basic Domain-Specific Languages (DSLs) representing logical concepts to generate solutions for given ARC tasks. In the context of ARC, DSLs refer to functions that can solve ARC tasks. Chollet has demonstrated that humans can solve ARC using a web interface equipped with basic functions (Chollet, 2019  ###reference_b8###). However, attempts to solve ARC using DSLs through computer algorithms, such as those in prior research have only achieved about 2\u201330% accuracy (Hodel, 2023b  ###reference_b17###; Wind, 2020  ###reference_b53###). This indicates that ARC tasks can be solved through combinations of DSLs, and while humans can find suitable combinations, computers seem unable to do so, which ultimately relates to the absence of compositional ability.\nTo analyze whether appropriate DSL is selected, we used the ToT prompting technique to generate results step by step. ToT consists of two stages: 1) generating multiple proposals for the next step and 2) selecting the most promising one. In the proposal phase, LLM receives four inputs: the current state of the ARC task, Python-formatted DSLs, natural language descriptions and examples of DSL usage, and definitions of parameters required by the DSLs. Since object information plays a significant role in ARC tasks (Xu et al., 2023a  ###reference_b55###), using the Push-and-Pull clustering algorithm, additional object information based on the current state was provided. (Park et al., 2023  ###reference_b34###). The form of the ARC grid is a 2D array, where object information comes in the form of (x, y) coordinates representing the position of points in the 2D array. The input DSL consists of a total of eight types: \u2018rotate\u2019, \u2018flip\u2019, \u2018coloring object\u2019, \u2018rotate object\u2019, \u2018color object\u2019, \u2018move object\u2019, \u2018flip object\u2019, \u2018draw a line diagonally from a given point to the end of the grid\u2019, \u2018draw a line between two points\u2019, and \u2018color a single point\u2019. In the evaluation phase, LLM selects four suitable DSLs to create candidates. Each of the four candidates is scored from 0.001 to 20 points, and the two candidates with the highest scores are applied to generate the next state using a separate simulator. The output is then used as an input prompt recursively provided to LLM for five iterations. The grid obtained from the final output is compared to the correct answer for evaluation."
        },
        {
            "section_id": "3.2.3",
            "parent_section_id": "3.2",
            "section_name": "3.2.3. Results",
            "text": "The experiment results are as follows: out of 99 tasks, LLM was not able to solve any tasks. The fact that humans can solve 80% of ARC tasks by combining similar DSLs implies a significant difference in compositional ability between humans and LLMs (Johnson et al., 2021  ###reference_b20###). To understand the reasons for this difference, we analyzed whether 1) LLM can understand the functionality of the DSLs, and 2) LLM can find the proper combination of DSLs to solve the task.\nTo begin with, we analyzed whether LLM understands the functionality of the DSLs by examining its ability to accurately generate outputs that perform the specified transformations when given the DSLs and ARC grids as input. The result shows that LLM can perform the transformations on a  grid without errors. This result demonstrates that LLM understands the DSLs provided as input in this experiment and knows how the state changes when applying a certain DSL.\n###figure_28### To determine whether LLMs can find the transformations to solve ARC tasks with combinations of DSLs, we evaluated whether the state transformations are necessary for generating the correct grid. The result indicates that while the given DSLs are insufficient to achieve the solution, they seem capable of resolving the task through subsequent steps. Furthermore, when asked to predict the next step, LLM shows a tendency to select the same DSL repeatedly, with a 75% rate of selecting the same DSL that was selected just before. This tendency indicates that LLM understands the functionality of the combined DSLs, as it can compare the resulting grid after passing through the combined DSLs with the correct grid. However, it cannot predict which DSL combinations are necessary to achieve the desired outcome."
        },
        {
            "section_id": "3.2.4",
            "parent_section_id": "3.2",
            "section_name": "3.2.4. Conclusion",
            "text": "In Section 3.2  ###reference_###, we conducted experiments to evaluate the compositionality of LLMs in solving ARC tasks using DSLs. The results lead to the following two conclusions. First, LLM contains an ability to understand inputs including DSLs and grids. Second, the ability to analyze the combination of steps to decompose the task into smaller sub-tasks and achieve the desired result is weak. This indicates that while LLMs have a good understanding ability, they cannot still perform step-by-step planning.\nThe result of the experiment with a specific DSL for addressing ARC tasks also shows that the compositionality of LLMs is insufficient. Over the DSL used in this experiment, there are other DSLs designed to evaluate the compositional capabilities of LLMs such as Hodel (Hodel, 2023b  ###reference_b17###) and Wind (a.k.a. icecuber) (Wind, 2020  ###reference_b53###). However, even if LLMs could fully understand and use these more detailed DSLs, LLMs will be evaluated as lacking in compositional abilities due to the described limitations(the tendency to repeat the same DSL, the inability to find the right combination of DSLs for the answer). Furthermore, if the compositional abilities of the current LLM validator, which evaluates steps, are found lacking, then for LLMs to autonomously combine DSLs to solve ARC problems would be extremely challenging."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3. Capability of LLMs 3: Productivity",
            "text": "In Section 3.3  ###reference_###, we investigate the third concept of LoTH, productivity. Productivity refers to the ability to generate unseen representations based on observed data. This characteristic allows humans to imagine different situations even from a single phenomenon, thus allowing for efficient learning without the need to learn from data inefficiently each time. Similarly, when equipped with this ability, LLMs are expected to excel in unseen tasks, making it one of the key functions of essential reasoning. The ability to generate new pairs within a limited set of rules could be helpful while solving ARC tasks, highlighting the need for productivity. In this section, we will test productivity by evaluating the validity of created examples based on given example pairs of ARC tasks.\nDirect replication of other grids in the same task was a recurring issue, even with prompts designed to prevent such outcomes. As shown in Fig. 17  ###reference_###  ###reference_###, LLMs often simply copy the input from given example pairs to create new inputs. This error stemmed from LLMs\u2019 failure to deduce meaningful rules from provided examples.\nThere was another limitation in considering small steps to generate inputs from the given output. As a result, it was challenging to determine whether generated examples could be solved by specific rules of the task, leading to frequent occurrences of generated examples that could not be solved by rules of the task. Fig. 17  ###reference_###  ###reference_### is the example. In the case of an input where all the vertices of the square have been erased, the color of the vertices cannot be determined, so the given output cannot be inferred.\n###figure_29###"
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1. Experiment",
            "text": "We conduct experiments using ARC tasks to understand how well LLMs can generate new expressions based on inherent logical concepts. Productivity involves two main steps: inferring specific rules for generating images from example images and natural language expressions and using those rules to generate new, unseen images. However, solving ARC tasks, experimented on in the previous sections so far, is unsuitable for confirming these two processes. For precise evaluation, we propose a new experiment: Given an ARC task and a basic rule shared with similar ARC tasks, can LLMs generate valid examples of the given task? If LLMs can understand a relationship between the given ARC task and the abstract rule, they should be able to infer specific rules for the given task and generate new valid examples. Through this, we want to see if LLMs can imitate human thinking productivity.\n###figure_30###"
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2. Method",
            "text": "To precisely evaluate whether LLMs can infer their own generation rules given ARC examples and create new tasks by appropriately applying these rules, we rigorously controlled the prompts. LLMs receive two types of prompts: example pairs included in the ARC task and descriptions of abstract rules applicable to similar ARC tasks. However, in this case, one example pair was used as the basis for generation, and the remaining examples were used for inferring specific rules for the task. Based on the category of ConceptARC (Moskvichev et al., 2023  ###reference_b32###), which organizes a subset of ARC tasks into 16 distinct categories according to human classification criteria, we developed abstract rules. For each category within ConceptARC, we crafted a corresponding abstract rule, ensuring that tasks within the same category adhere to the identical abstract rule. An example of this abstract rule is shown at the top of the Inverse Transformation Prompt panel in Fig. 15  ###reference_###.\nWe proposed the Inverse Transformation Prompt (ITP), a prompting technique for this experiment. ITP instructs LLMs to generate multiple valid examples using the ARC task and its abstract rules. Fig. 15  ###reference_### demonstrates the process by which LLMs generate new examples given the ARC task and the corresponding ITP. LLMs generate multiple inputs that can form pairs with the output from one example of the task. This example, as mentioned earlier, is selected to serve as the basis for generation and is not included in ITP. If LLMs understood the specific rules corresponding to the ARC task given through ITP, the new example pairs generated by LLMs would be suitable as examples of the task.\nITP is based on many-to-one corresponding to elicit two advantages. First, the method of generating only input is more data-efficient than the method of generating both input and output because the output of the existing ARC task can be used as is. Since all tasks in ARC have example pairs, reusing these examples can be said to make full use of the given data. ITP allows for the reuse of a single ARC task multiple times, making it data-efficient. In particular, using ITP can further increase data efficiency by allowing one ARC task to be reused multiple times by changing the order of examples. Secondly, ITP increases the likelihood of generating valid responses. Through simple simulations, we have seen that inferring inputs from output tends to be more likely to generate valid results than inferring outputs from input. Because generating input from output is subject to relatively less stringent constraints, there is often a wide range of acceptable outcomes.\nIn the process of creating ITP, we encounter two challenges. First, according to the ConceptARC category, there could be multiple solutions within one category. Fig. 15(a)  ###reference_.sf1### illustrates that there are various types of tasks with the same category. Abstract rules given in the same sentences for each category may not be sufficient to cover various types of tasks. Second, there were ARC tasks that made not possible to infer multiple inputs from a single output (Fig. 15(b)  ###reference_.sf2###). In such cases, there was only one valid input. Although we tried to take these cases into account while writing the ITP, these challenges nevertheless harmed the experimental results.\n###figure_31### ###figure_32### Before analyzing the experimental results, it was necessary to redefine the evaluation metric as the object was changed from solving ARC tasks to generating valid examples. As explained earlier, for one example of a particular ARC task, we generated valid inputs that could be paired with the output of that example. To properly generate inputs, LLM must understand the specific rules of the given ARC task through its ITP and apply the understood rules to the output for generating valid inputs. In this experiment, we evaluated whether all generated inputs were valid for each ARC task. This evaluation metric can check both whether the LLM understands the correct rules and whether it generates valid examples based on the rules it understands. Therefore, this experiment systematically evaluates the logical and valid demo pair generation capability of LLMs, contributing to our understanding of their ability to generate new representations."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3. Results",
            "text": "Based on 160 ARC tasks classified by ConceptARC, we evaluated the validity of a total of 2,913 generated examples. The valid generation ratio on average was approximately 17.1%, while the rest were invalid. As we mentioned before, the validity of results was determined by human judgment regarding whether the generated task adhered to the given rule. Results in Table 4  ###reference_### indicate that LLMs demonstrate a certain level of performance in generating examples consistent with the rule. However, since the criteria for validating the generated results as valid or invalid are weak, there is a limitation that results cannot be used before data post-processing even if an infinite number of results can be created. We analyzed the generated inputs to investigate why LLMs were unable to generate valid inputs and identified the following two sources of error: LLMs struggled with rule inference and were weak in step-by-step generation.\n###table_5### Direct replication of other grids in the same task was a recurring issue, even with prompts designed to prevent such outcomes. As shown in Fig. 17  ###reference_###  ###reference_###  ###reference_###, LLMs often simply copy the input from given example pairs to create new inputs. This error stemmed from LLMs\u2019 failure to deduce meaningful rules from provided examples.\nThere was another limitation in considering small steps to generate inputs from the given output. As a result, it was challenging to determine whether generated examples could be solved by specific rules of the task, leading to frequent occurrences of generated examples that could not be solved by rules of the task. Fig. 17  ###reference_###  ###reference_###  ###reference_### is the example. In the case of an input where all the vertices of the square have been erased, the color of the vertices cannot be determined, so the given output cannot be inferred.\n###figure_33###"
        },
        {
            "section_id": "3.3.4",
            "parent_section_id": "3.3",
            "section_name": "3.3.4. Conclusion",
            "text": "In Section 3.3  ###reference_###, we conducted experiments to confirm the productivity of LLMs by assessing whether they can understand given ARC tasks in abstracted representations and generate valid new examples based on abstracted rules. Although it is known that LLMs have great strengths in creating creative works, our experimental results reveal that LLMs are weak in understanding rules and producing creations that adhere to those rules. These results indicate that the process by which LLMs generate outputs is closer to mimicking human-generated results and achieving human-level generation abilities for LLMs is challenging.\nFurther research is needed to understand the specific mechanisms behind LLMs\u2019 generation and how to elevate it to the human level. The experiments proposed in this study only examined whether LLMs can generate the given ARC tasks with simple rules and examples. However, it is difficult to determine whether the generated tasks were created following a human-like generation process or if they simply appear valid. Similarly, there is a need to analyze whether the process of incorrect generation resembles that of humans or not."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Discussion",
            "text": "Through the three experiments in Section 3  ###reference_###, we have observed that LLMs demonstrate strengths in understanding and manipulating both image and text inputs. However, they still exhibit weaknesses in logical inference, sequential planning based on understanding, and generating unseen images according to predefined rules. We will conclude by introducing the current research directions aimed at further enhancing LLMs\u2019 ability and outlining the goals after solving ARC."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. What Should LLMs Possess to Solve ARC?",
            "text": "Based on the experimental results of Section 3  ###reference_###, it is evident that LLMs still cannot solve ARC effectively. This is attributed to the deficiencies in logical coherence, compositionality, and productivity. How can we improve the inference capabilities of LLMs? In this section, we explore directions to enhance LLMs from the perspectives of abstraction knowledge and reasoning."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1. Abstract Knowledge",
            "text": "To solve ARC, the first challenge is the ability to extract the implicit information contained within ARC. Xu et al. argued that object-based representation is crucial for solving ARC and proposed ARGA (Xu et al., 2023a  ###reference_b55###), which converts given example grids into graphs. Their follow-up study (Xu et al., 2023b  ###reference_b56###) involved LLM solving ARC tasks using information obtained from ARGA and showed notable performance for object-based ARC tasks. However, these studies have a fundamental weakness in that they cannot be applied to ARC tasks without objects. Since only about 40% of ARC tasks contain object concepts (Xu et al., 2023a  ###reference_b55###), this method cannot be applied to more than half of the tasks. Wang et al. on the other hand, improved the abstraction ability of LLMs to some extent with a graph-form dataset consisting of 221K textual descriptions, called AbsPyramid (Wang et al., 2023a  ###reference_b48###), and also proposed a framework called AbsInstruct (Wang et al., 2024a  ###reference_b47###) utilizing this dataset. Attempting to structure sentences can be an effective abstraction method for natural language, but its effectiveness cannot be seen in tasks that do not contain sentences."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2. Reasoning",
            "text": "Another challenge for LLMs in the context of ARC is the vast search space. One method gaining attention to address this is to enable LLMs to generate DSLs themselves. Rajani et al. introduced CAGE (Rajani et al., 2019  ###reference_b36###), which prompts LLMs to generate explanations before generating answers. Subsequently, Wang et al. (Wang et al., 2024b  ###reference_b45###) reported improved results by having LLMs generate DSLs based on hypotheses they set themselves. Additionally, active research is underway on prompting techniques applying algorithmic approaches. Zhou et al. (Zhou et al., 2022  ###reference_b62###) demonstrated enhanced inference performance in LLMs by applying in-context learning. Follow-up research is actively being conducted following CoT and ToT. For example, CoT-SC (Wang et al., 2023b  ###reference_b46###) is a study that selects results through voting from multiple instances of CoT, GoT (Besta et al., 2023  ###reference_b4###) secures flexibility by enabling the generation of graph-like thought nodes, and XoT (Ding et al., 2023  ###reference_b10###) uses the thought tree while Monte Carlo tree search and refines the tree with reinforcement learning. However, these attempts are closer to additional learning for LLMs, and more researches are needed to ascertain whether fundamental improvements in LLMs\u2019 reasoning abilities are achievable."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Limitation of ARC",
            "text": "Does solving ARC signify the completion of human-like AI? To answer this question, two doubts need to be appropriately addressed: 1) Will the ARC solver possess human-level problem-solving abilities? and 2) Will that solver think like humans to solve ARC? It\u2019s not easy to imagine how the ARC solver operates without human-level reasoning. At this point, what we can assume is that the model will have the three properties of LoTH, and the model could be capable of several types of reasoning included in ARC. With this hypothesis, we attempt to address the following questions."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1. Will the Model Possess Human-Level Problem-Solving Abilities?",
            "text": "Being capable of reasoning does not necessarily equate to having human-level problem-solving abilities. In other words, even if a model can reason to a level that can solve ARC, it may not have human-level problem-solving capabilities. Various tasks that humans face are generally more complex than ARC and involve various other cognitive factors besides reasoning. Therefore, even models that can solve ARC may have the following limitations compared to human-level problem-solving abilities.\nFirst, with the current ARC criteria, it\u2019s still unknown whether the model that solved it can solve more complex types of tasks. This is because ARC tasks focus on just reasoning and are therefore presented in a relatively simple environment. Whether the reasoning ability learned through ARC would also work in more complex environments has not been revealed. Second, solving ARC does not imply the presence of other components of intelligence beyond reasoning. While reasoning is undoubtedly a core aspect of cognitive processes, it is not the entirety of intelligence. There is research shows that solving human-level complex tasks requires various cognitive abilities (Gardner, 2011  ###reference_b13###)."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2. Will the Model Think Like Humans?",
            "text": "Even if we assume that the ARC solver can reason in terms of LoTH, we cannot guarantee whether this solver\u2019s process is human-like for the following two reasons. Firstly, the current ARC provides a performance measure that rewards only for solving a task. It\u2019s important to recognize that such a measure might instigate a wrong purpose, leading to what is known as the King Midas problem (Russell and Norvig, 1995  ###reference_b38###). This problem emphasizes the risk of AI achieving its given objective too literally, leading to unintended negative consequences, underscoring the importance of aligning AI\u2019s goals with human values and the broader context. The policy of rewarding only the results, excluding the solution process, makes it difficult to evaluate whether the solution process is similar to human reasoning. Therefore, models trained on current ARC likely differ in how they solve tasks compared to humans. The second reason is that directly comparing the reasoning processes of humans and language models is challenging. The process by which humans solve ARC tasks has not been investigated, making it unclear how the solving process differs between humans and artificial intelligence. Furthermore, there is a lack of metrics for comparing the solving processes, making direct comparisons difficult."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Future Direction After Solving ARC",
            "text": "To summarize, solving ARC tasks does not directly imply achieving human-level artificial intelligence. Moreover, there is a challenge in comparing task-solving approaches with those of humans. Thus, we suggest three alternatives to more accurately measure human-level inference abilities."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1. Using Different Benchmarks",
            "text": "One limitation of ARC is its simple environment. SQA3D (Ma et al., 2023  ###reference_b26###), for instance, addresses inference tasks in a 3D domain by extending them into question-answering tasks using simulators like ScanNet (Dai et al., 2017  ###reference_b9###). Additionally, benchmarks such as TGIF-QA (Jang et al., 2017  ###reference_b19###), MovieQA (Tapaswi et al., 2016  ###reference_b40###), TVQA (Lei et al., 2018  ###reference_b23###), and STAR (Wu et al., 2021  ###reference_b54###), which append question-answering to videos, have been proposed. Such benchmarks mimicking real-world inference scenarios could serve as supplements to measure complex abstractions not covered by ARC."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2. Quantification of ARC Task-Solving Processes",
            "text": "Chollet, the creator of ARC, argued that ARC maximizes generality while minimizing prior and experience (Chollet, 2019  ###reference_b8###), but these components have not been quantitatively evaluated. As a result, the quantitative assessment of factors such as the generality achieved by models solving ARC, the level of prior knowledge, and the components of prior knowledge remains elusive. One possible way to quantitatively evaluate the process of solving ARC tasks is to quantify the model\u2019s achievement of prior, experience, and generality."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3. Adding Evaluation Methods to Compare Task-Solving Processes with Human Approaches",
            "text": "Recent ARC research has focused on finding ways for AI to solve tasks. However, there are doubts about how similar these solutions are to those of humans. The initial paper by Johnson et al. (Johnson et al., 2021  ###reference_b20###) analyzed human ARC solutions. Subsequently, LARC (Acquaviva et al., 2022  ###reference_b2###) was proposed to analyze how tasks are solved through the language-based explanation of human solutions. Tools for facilitating the collection of human data are also continuously being developed. Kim et al. (Kim et al., 2022  ###reference_b21###), for instance, have analyzed how tasks are solved through O2ARC. It is suggested to not only calculate simple correctness for each ARC task but also to add similarity with human data to the evaluation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Conclusions",
            "text": "In this study, we addressed the limitation of existing research, which predominantly analyzed LLM\u2019s inference ability from a deterministic perspective, by introducing the LoTH perspective to ensure a fair evaluation of the process as well. By comparing and analyzing arguments across various domains of inference, we confirmed that logicality, compositionality, and productivity are quantifiable components to evaluate inference ability. Next, we proposed three experiments using the ARC dataset to quantitatively evaluate these three components from the LoTH perspective. The results showed that although current LLMs exhibit outstanding performance, they lack logicality, compositionality, and productivity in their processes, suggesting that they are closer to probabilistic mimicry rather than possessing autonomous inference abilities. Finally, we explored meaningful research directions for LLM to acquire inference capabilities from the LoTH perspective, as well as alternative approaches beyond ARC. This attempt to quantitatively evaluate the inference process from the LoTH perspective through experimental methods represents a differentiated approach not present in previous research. It contributes by providing a new perspective on how to handle inference capabilities in the field of computer science, going beyond LLMs."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S2.T1.2.1.1\" style=\"font-size:90%;\">Table 1</span>. </span><span class=\"ltx_text\" id=\"S2.T1.3.2\" style=\"font-size:90%;\">Alignment of Abstract Visual Reasoning tasks with its taxonomy\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Miko\u0142aj and Jacek, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib30\" title=\"\">2023</a>)</cite>. The tasks and their corresponding benchmarks are cataloged under the following four dimensions of the taxonomy: input shapes, hidden rules, target tasks, and specific challenges.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S2.T1.4\" style=\"width:433.6pt;height:231.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-91.3pt,48.8pt) scale(0.703740749041318,0.703740749041318) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.4.1\">\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.1\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" id=\"S2.T1.4.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S2.T1.4.1.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.1.2.1\">Input Shapes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S2.T1.4.1.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.1.3.1\">Hidden Rules</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S2.T1.4.1.1.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.1.4.1\">Target Tasks</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S2.T1.4.1.1.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.1.5.1\">Specific Challenges</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.2.1\" rowspan=\"-2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.1.1\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.2.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text\" id=\"S2.T1.4.1.2.2.1\"></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.2.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.4.1.2.2.2.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.2.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.2.2.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.2.2.1.1.1.1\">Geometric</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.2.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.2.2.1.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.2.2.1.2.1.1\">Shapes</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.2.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.2.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text\" id=\"S2.T1.4.1.2.3.1\"></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.3.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.4.1.2.3.2.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.3.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.3.2.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.3.2.1.1.1.1\">Abstract</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.3.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.3.2.1.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.3.2.1.2.1.1\">Shapes</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.3.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.2.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text\" id=\"S2.T1.4.1.2.4.1\"></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.4.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.4.1.2.4.2.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.4.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.4.2.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.4.2.1.1.1.1\">Explicit</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.4.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.4.2.1.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.4.2.1.2.1.1\">Rules</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.4.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.2.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text\" id=\"S2.T1.4.1.2.5.1\"></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.5.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.4.1.2.5.2.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.5.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.5.2.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.5.2.1.1.1.1\">Abstract</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.5.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.5.2.1.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.5.2.1.2.1.1\">Rules</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.5.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.2.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.6.1\">Classify</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.2.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.7.1\">Generate</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.2.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text\" id=\"S2.T1.4.1.2.8.1\"></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.8.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.4.1.2.8.2.1\">\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.8.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.8.2.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.8.2.1.1.1.1\">Domain</span></span></span>\n<span class=\"ltx_tr\" id=\"S2.T1.4.1.2.8.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.2.8.2.1.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.8.2.1.2.1.1\">Transfer</span></span></span>\n</span></span> <span class=\"ltx_text\" id=\"S2.T1.4.1.2.8.3\"></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.2.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.2.9.1\">Extrapolate</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.3\" style=\"background-color:#FFFF00;\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.3.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.3.1.1\" style=\"background-color:#FFFF00;\">ARC<span class=\"ltx_text ltx_font_medium\" id=\"S2.T1.4.1.3.1.1.1\">\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Chollet, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib8\" title=\"\">2019</a>)</cite></span></span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.4.1.3.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.3.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.1.3.3.1\" style=\"background-color:#FFFF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.4.1.3.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.3.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.1.3.5.1\" style=\"background-color:#FFFF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.4.1.3.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.3.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.1.3.7.1\" style=\"background-color:#FFFF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.3.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.1.3.8.1\" style=\"background-color:#FFFF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.3.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.1.3.9.1\" style=\"background-color:#FFFF00;\">\u2713</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.4.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.4.1.1\">Mini-ARC</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib21\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.4.1.4.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.4.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.4.1.4.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.4.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.4.1.4.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.4.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.4.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.4.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.5.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.5.1.1\">1D-ARC</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib56\" title=\"\">2023b</a>)</cite>\n</td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.5.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.4.1.5.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.5.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.4.1.5.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.5.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.4.1.5.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.5.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.5.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.6.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.6.1.1\">MC-LARC</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Shin et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib39\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.6.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.4.1.6.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.6.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.4.1.6.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.6.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.6.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.6.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.6.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.7.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.7.1.1\">Sandia</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Matzen et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib28\" title=\"\">2010</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.7.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.7.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.7.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.7.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.4.1.7.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S2.T1.4.1.7.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.4.1.7.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T1.4.1.7.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.8.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.8.1.1\">Synthetic</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang and Su, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib44\" title=\"\">2015</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.8.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.8.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.8.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.8.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.8.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.8.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.8.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.8.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.9.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.9.1.1\">G-set</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma\u0144dziuk and \u017bychowski, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib27\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.9.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.9.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.9.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.9.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.9.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.9.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.9.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.9.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.10.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.10.1.1\">RAVEN</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib59\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.10.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.10.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.10.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.10.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.10.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.10.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.10.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.10.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.11.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.11.1.1\">PGM</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrett et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib3\" title=\"\">2018</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.11.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.11.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.11.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.11.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.11.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.11.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.11.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.11.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.12.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.12.1.1\">Hill et al.</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Hill et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib15\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.12.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.12.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.12.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.12.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.12.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.12.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.12.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.12.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.13.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.13.1.1\">G1-set</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma\u0144dziuk and \u017bychowski, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib27\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.13.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.13.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.13.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.13.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.13.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.13.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.13.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.13.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.14.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.14.1.1\">S1-set</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma\u0144dziuk and \u017bychowski, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib27\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.14.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.14.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.14.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.14.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.14.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.14.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.14.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.14.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.15.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.15.1.1\">MNS</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib60\" title=\"\">2020</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.15.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.15.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.15.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.15.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.15.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.15.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.15.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.15.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.16\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.1.16.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.16.1.1\">VAEC</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Webb et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib49\" title=\"\">2020</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.16.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.16.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.16.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.16.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.16.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.4.1.16.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td\" id=\"S2.T1.4.1.16.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.1.16.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.17\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S2.T1.4.1.17.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.1.17.1.1\">DOPT</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Webb et\u00a0al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib49\" title=\"\">2020</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.4.1.17.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_r\" id=\"S2.T1.4.1.17.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.4.1.17.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_r\" id=\"S2.T1.4.1.17.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.4.1.17.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_r\" id=\"S2.T1.4.1.17.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_border_bb\" id=\"S2.T1.4.1.17.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.4.1.17.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\u2713</td>\n</tr>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1. Alignment of Abstract Visual Reasoning tasks with its taxonomy\u00a0(Miko\u0142aj and Jacek, 2023). The tasks and their corresponding benchmarks are cataloged under the following four dimensions of the taxonomy: input shapes, hidden rules, target tasks, and specific challenges."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2. </span>Averaged performance of each prompting technique. The accuracy is based on solving 100 random ARC tasks with CoT, LtM, and ToT prompts, each repeated five times. The accuracy outside the parentheses refers to the accuracy when only the results are correct, while the accuracy inside the parentheses indicates the accuracy when both the results and the process are correct.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T2.4\">\n<tr class=\"ltx_tr\" id=\"S3.T2.4.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.1.1.1\" style=\"font-size:90%;\">Iteration</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.1.2.1\" style=\"font-size:90%;\">CoT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.1.3.1\" style=\"font-size:90%;\">LtM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.1.4.1\" style=\"font-size:90%;\">ToT</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.1\"><span class=\"ltx_text\" id=\"S3.T2.4.2.1.1\" style=\"font-size:90%;\">1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.2\"><span class=\"ltx_text\" id=\"S3.T2.4.2.2.1\" style=\"font-size:90%;\">11% (3%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.3\"><span class=\"ltx_text\" id=\"S3.T2.4.2.3.1\" style=\"font-size:90%;\">6% (4%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.2.4\"><span class=\"ltx_text\" id=\"S3.T2.4.2.4.1\" style=\"font-size:90%;\">7% (3%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.3.1\"><span class=\"ltx_text\" id=\"S3.T2.4.3.1.1\" style=\"font-size:90%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.3.2\"><span class=\"ltx_text\" id=\"S3.T2.4.3.2.1\" style=\"font-size:90%;\">10% (2%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.3.3\"><span class=\"ltx_text\" id=\"S3.T2.4.3.3.1\" style=\"font-size:90%;\">7% (4%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.3.4\"><span class=\"ltx_text\" id=\"S3.T2.4.3.4.1\" style=\"font-size:90%;\">5% (1%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.1\"><span class=\"ltx_text\" id=\"S3.T2.4.4.1.1\" style=\"font-size:90%;\">3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.2\"><span class=\"ltx_text\" id=\"S3.T2.4.4.2.1\" style=\"font-size:90%;\">10% (5%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.3\"><span class=\"ltx_text\" id=\"S3.T2.4.4.3.1\" style=\"font-size:90%;\">6% (3%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.4\"><span class=\"ltx_text\" id=\"S3.T2.4.4.4.1\" style=\"font-size:90%;\">7% (2%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.5.1\"><span class=\"ltx_text\" id=\"S3.T2.4.5.1.1\" style=\"font-size:90%;\">4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.5.2\"><span class=\"ltx_text\" id=\"S3.T2.4.5.2.1\" style=\"font-size:90%;\">10% (4%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.5.3\"><span class=\"ltx_text\" id=\"S3.T2.4.5.3.1\" style=\"font-size:90%;\">4% (2%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.5.4\"><span class=\"ltx_text\" id=\"S3.T2.4.5.4.1\" style=\"font-size:90%;\">7% (4%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.6.1\"><span class=\"ltx_text\" id=\"S3.T2.4.6.1.1\" style=\"font-size:90%;\">5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.6.2\"><span class=\"ltx_text\" id=\"S3.T2.4.6.2.1\" style=\"font-size:90%;\">12% (6%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.6.3\"><span class=\"ltx_text\" id=\"S3.T2.4.6.3.1\" style=\"font-size:90%;\">5% (2%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.6.4\"><span class=\"ltx_text\" id=\"S3.T2.4.6.4.1\" style=\"font-size:90%;\">6% (2%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.4.7.1\"><span class=\"ltx_text\" id=\"S3.T2.4.7.1.1\" style=\"font-size:90%;\">Average</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.4.7.2\"><span class=\"ltx_text\" id=\"S3.T2.4.7.2.1\" style=\"font-size:90%;\">10.6% (4.0%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.4.7.3\"><span class=\"ltx_text\" id=\"S3.T2.4.7.3.1\" style=\"font-size:90%;\">5.6% (3.0%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.4.7.4\"><span class=\"ltx_text\" id=\"S3.T2.4.7.4.1\" style=\"font-size:90%;\">6.4% (2.4%)</span></td>\n</tr>\n</table>\n</figure>",
            "capture": "Table 2. Averaged performance of each prompting technique. The accuracy is based on solving 100 random ARC tasks with CoT, LtM, and ToT prompts, each repeated five times. The accuracy outside the parentheses refers to the accuracy when only the results are correct, while the accuracy inside the parentheses indicates the accuracy when both the results and the process are correct."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3. </span>Analyzing LLMs\u2019 reasoning capabilities by task difficulty, following prior categorization\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Borsky, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11793v1#bib.bib5\" title=\"\">2021</a>)</cite>. The number of ARC tasks corresponding to each category is listed in the table, and the experiment was performed five times for each task.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T3.8\">\n<tr class=\"ltx_tr\" id=\"S3.T3.8.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S3.T3.8.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.8.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.8.1.2.1\" style=\"font-size:90%;\">Entry</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.8.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.8.1.3.1\" style=\"font-size:90%;\">Easy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.8.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.8.1.4.1\" style=\"font-size:90%;\">Medium</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.8.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.8.1.5.1\" style=\"font-size:90%;\">Hard</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.2.1\"><span class=\"ltx_text\" id=\"S3.T3.8.2.1.1\" style=\"font-size:90%;\">Tasks</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.2.2\"><span class=\"ltx_text\" id=\"S3.T3.8.2.2.1\" style=\"font-size:90%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.2.3\"><span class=\"ltx_text\" id=\"S3.T3.8.2.3.1\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.2.4\"><span class=\"ltx_text\" id=\"S3.T3.8.2.4.1\" style=\"font-size:90%;\">46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.2.5\"><span class=\"ltx_text\" id=\"S3.T3.8.2.5.1\" style=\"font-size:90%;\">14</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.3.1\"><span class=\"ltx_text\" id=\"S3.T3.8.3.1.1\" style=\"font-size:90%;\">Trials</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.3.2\"><span class=\"ltx_text\" id=\"S3.T3.8.3.2.1\" style=\"font-size:90%;\">10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.3.3\"><span class=\"ltx_text\" id=\"S3.T3.8.3.3.1\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.3.4\"><span class=\"ltx_text\" id=\"S3.T3.8.3.4.1\" style=\"font-size:90%;\">230</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.3.5\"><span class=\"ltx_text\" id=\"S3.T3.8.3.5.1\" style=\"font-size:90%;\">70</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.4.1\"><span class=\"ltx_text\" id=\"S3.T3.8.4.1.1\" style=\"font-size:90%;\">CoT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.4.2\"><span class=\"ltx_text\" id=\"S3.T3.8.4.2.1\" style=\"font-size:90%;\">100.00%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.4.3\"><span class=\"ltx_text\" id=\"S3.T3.8.4.3.1\" style=\"font-size:90%;\">30.00%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.4.4\"><span class=\"ltx_text\" id=\"S3.T3.8.4.4.1\" style=\"font-size:90%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.8.4.5\"><span class=\"ltx_text\" id=\"S3.T3.8.4.5.1\" style=\"font-size:90%;\">0.00%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.5.1\"><span class=\"ltx_text\" id=\"S3.T3.8.5.1.1\" style=\"font-size:90%;\">LtM</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.5.2\"><span class=\"ltx_text\" id=\"S3.T3.8.5.2.1\" style=\"font-size:90%;\">20.00%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.5.3\"><span class=\"ltx_text\" id=\"S3.T3.8.5.3.1\" style=\"font-size:90%;\">19.00%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.5.4\"><span class=\"ltx_text\" id=\"S3.T3.8.5.4.1\" style=\"font-size:90%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.5.5\"><span class=\"ltx_text\" id=\"S3.T3.8.5.5.1\" style=\"font-size:90%;\">2.85%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.6.1\"><span class=\"ltx_text\" id=\"S3.T3.8.6.1.1\" style=\"font-size:90%;\">ToT</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.6.2\"><span class=\"ltx_text\" id=\"S3.T3.8.6.2.1\" style=\"font-size:90%;\">50.00%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.6.3\"><span class=\"ltx_text\" id=\"S3.T3.8.6.3.1\" style=\"font-size:90%;\">22.00%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.6.4\"><span class=\"ltx_text\" id=\"S3.T3.8.6.4.1\" style=\"font-size:90%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.8.6.5\"><span class=\"ltx_text\" id=\"S3.T3.8.6.5.1\" style=\"font-size:90%;\">0.00%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.8.7.1\"><span class=\"ltx_text\" id=\"S3.T3.8.7.1.1\" style=\"font-size:90%;\">Average</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.8.7.2\"><span class=\"ltx_text\" id=\"S3.T3.8.7.2.1\" style=\"font-size:90%;\">56.67%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.8.7.3\"><span class=\"ltx_text\" id=\"S3.T3.8.7.3.1\" style=\"font-size:90%;\">23.67%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.8.7.4\"><span class=\"ltx_text\" id=\"S3.T3.8.7.4.1\" style=\"font-size:90%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.8.7.5\"><span class=\"ltx_text\" id=\"S3.T3.8.7.5.1\" style=\"font-size:90%;\">0.95%</span></td>\n</tr>\n</table>\n</figure>",
            "capture": "Table 3. Analyzing LLMs\u2019 reasoning capabilities by task difficulty, following prior categorization\u00a0(Borsky, 2021). The number of ARC tasks corresponding to each category is listed in the table, and the experiment was performed five times for each task."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T4.2.1.1\" style=\"font-size:90%;\">Table 4</span>. </span><span class=\"ltx_text\" id=\"S3.T4.3.2\" style=\"font-size:90%;\">The ratio of valid examples among examples generated for each category of ConceptARC.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T4.4\">\n<tr class=\"ltx_tr\" id=\"S3.T4.4.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.4.1.1\" style=\"padding-bottom:2.15277pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.4.1.1.1\">Category</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S3.T4.4.1.2\" style=\"padding-bottom:2.15277pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.4.1.2.1\">Generated Examples</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S3.T4.4.1.3\" style=\"padding-bottom:2.15277pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.4.1.3.1\">Valid Examples</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S3.T4.4.1.4\" style=\"padding-bottom:2.15277pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.4.1.4.1\">Validity</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T4.4.2.1\">Above Below</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.4.2.2\">158</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.4.2.3\">34</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.4.2.4\">21.52%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.3.1\">Center</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.3.2\">236</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.3.3\">35</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.3.4\">14.83%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.4.1\">Clean Up</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.4.2\">183</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.4.3\">83</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.4.4\">45.36%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.5.1\">Complete Shape</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.5.2\">147</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.5.3\">37</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.5.4\">25.17%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.6.1\">Copy</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.6.2\">153</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.6.3\">4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.6.4\">2.61%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.7.1\">Count</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.7.2\">202</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.7.3\">29</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.7.4\">14.36%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.8.1\">Extend To Boundary</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.8.2\">167</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.8.3\">8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.8.4\">4.79%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.9.1\">Extract Objects</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.9.2\">176</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.9.3\">21</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.9.4\">11.93%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.10.1\">Filled Not Filled</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.10.2\">203</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.10.3\">29</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.10.4\">14.29%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.11.1\">Horizontal Vertical</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.11.2\">114</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.11.3\">7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.11.4\">6.14%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.12.1\">Inside Outside</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.12.2\">191</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.12.3\">24</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.12.4\">12.57%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.13.1\">Move To Boundary</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.13.2\">165</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.13.3\">12</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.13.4\">7.27%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.14.1\">Order</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.14.2\">162</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.14.3\">26</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.14.4\">16.05%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.15.1\">Same Different</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.15.2\">246</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.15.3\">76</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.15.4\">30.89%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.16.1\">Top Bottom 2D</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.16.2\">255</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.16.3\">59</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.16.4\">23.14%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.17\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T4.4.17.1\">Top Bottom 3D</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.17.2\">215</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.17.3\">25</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T4.4.17.4\">11.63%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T4.4.18.1\">Total</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.4.18.2\">2,913</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.4.18.3\">509</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T4.4.18.4\">17.12%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.19\">\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S3.T4.4.19.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S3.T4.4.19.2\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S3.T4.4.19.3\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S3.T4.4.19.4\"></td>\n</tr>\n</table>\n</figure>",
            "capture": "Table 4. The ratio of valid examples among examples generated for each category of ConceptARC."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.11793v1_figure_1.png",
            "caption": "Figure 1. Three different ARC tasks (Chollet, 2019). Each task involves example pairs of input and output grids that exemplify the required transformation. Participants are asked to predict the output grid based on the last input grid they receive. ARC is a simple benchmark that can be solved using only four types of prior knowledge: objectness, goal-directedness, arithmetic, and geometric topology. Despite the small amount of prior knowledge required to solve the problem, it presents a high level of reasoning difficulty. These characteristics enable ARC to function as a benchmark that fairly measures reasoning abilities."
        },
        "2": {
            "figure_path": "2403.11793v1_figure_2.png",
            "caption": "Figure 2. Three concepts of the Language of Thought Hypothesis (LoTH)."
        },
        "3": {
            "figure_path": "2403.11793v1_figure_3.png",
            "caption": "Figure 3. 1D-ARC (Xu et al., 2023b). (See https://github.com/khalil-research/1D-ARC for all tasks)"
        },
        "4": {
            "figure_path": "2403.11793v1_figure_4.png",
            "caption": "Figure 4. MC-LARC (Shin et al., 2023). (See https://mc-larc.github.io for all tasks)"
        },
        "5": {
            "figure_path": "2403.11793v1_figure_5.png",
            "caption": "Figure 5. Mini-ARC (Kim et al., 2022). (See https://bit.ly/ARC-GIST for all tasks)"
        },
        "6": {
            "figure_path": "2403.11793v1_figure_6.png",
            "caption": "Figure 6. Required reasoning step based on the complexity of input image in ARC, Mini-ARC, MC-LARC, and 1D-ARC. Mini-ARC and 1D-ARC simplify the task by reducing image size or dimensionality, thus lessening the reasoning needed. Meanwhile, MC-LARC changes the task format, decreasing reasoning steps but keeping the image complexity."
        },
        "7": {
            "figure_path": "2403.11793v1_figure_7.png",
            "caption": "(a) Chain of Thought"
        },
        "8": {
            "figure_path": "2403.11793v1_figure_8.png",
            "caption": "(b) Least to Most"
        },
        "9": {
            "figure_path": "2403.11793v1_figure_9.png",
            "caption": "(c) Tree of Thoughts"
        },
        "10": {
            "figure_path": "2403.11793v1_figure_10.png",
            "caption": "Figure 8. Three types of prompts are shown on the left. Although all prompts are described as a 2D array of grids, we visualized them on the right for clarity. In default, all three techniques use prompts with two main components: a sample task and a target task. However, LtM and ToT use a different combination of the target task and its decomposition command. This deviation occurs because CoT strictly follows the given sub-task, while LtM and CoT decompose the task on their own."
        },
        "11": {
            "figure_path": "2403.11793v1_figure_11.png",
            "caption": "Figure 9. Grey blocks illustrate prompt sets delivered to the LLM, including the sample task, target task, and LLM\u2019s prior responses, as shown in Fig. 8. Green blocks denote the final answer. CoT relies on a single grey block, indicating that the LLM strictly follows the provided sub-tasks. Conversely, LtM and ToT prompt the LLM to generate and address sub-tasks sequentially, represented by decomposed results (red) and intermediate responses (blue). ToT further distinguishes itself from LtM by evaluating multiple suggestions for sub-task handling and selecting the most effective one through a voting mechanism."
        },
        "12": {
            "figure_path": "2403.11793v1_figure_12.png",
            "caption": "Figure 10. Showcases of ARC tasks organized by human-perceived difficulty levels. These tasks illustrate the spectrum of complexity that humans use to rate problems, ranging from single-step \u2018Entry\u2019 level tasks to multi-step \u2018Hard\u2019 challenges. The difficulty classification reflects both the depth of inference required and the number of logical operations needed to reach a solution, paralleling the varying success rates of LLMs in tackling these tasks."
        },
        "13": {
            "figure_path": "2403.11793v1_figure_13.png",
            "caption": "Figure 11. Presenting instances where LLMs arrive at the correct answer but follow incorrect reasoning processes, highlighting the challenge of consistent logical rule application across different ARC task examples. The task is to identify a unique 5\u00d75555\\times 55 \u00d7 5 object within a grid, based on the number of black squares. The \u2018Correct\u2019 process shows the LLM successfully identifying the unique object. \u2018Incorrect 1\u2019 and \u2018Incorrect 2\u2019 represent failed reasoning processes, with the former depicting arbitrary selection and the latter showing a misidentification of objects."
        },
        "14": {
            "figure_path": "2403.11793v1_figure_14.png",
            "caption": "(a) Solved task"
        },
        "15": {
            "figure_path": "2403.11793v1_figure_15.png",
            "caption": "(b) Failed task"
        },
        "16": {
            "figure_path": "2403.11793v1_figure_16.png",
            "caption": "Figure 13. An example of the single step in an experimental process. LLM solver observes the current state and chooses the DSL from the DSL list. Then, LLM validator evaluates the selected DSLs by score. Based on the score, the top two states are sent to the next step."
        },
        "17": {
            "figure_path": "2403.11793v1_figure_17.png",
            "caption": "Figure 14. Since LLMs correctly understand the innated rules in DSLs, LLM generates valid outputs with required actions such as coloring, rotating, drawing a line, and flipping when we give information about DSL, current state, and object."
        },
        "18": {
            "figure_path": "2403.11793v1_figure_18.png",
            "caption": "Figure 15. The overall process of example generation with the Inverse Transformation Prompt (ITP). With ITP and one example of the task, LLMs generate input candidates of the output of the given example. If these generated inputs are valid, pairs created by these inputs and the given output can become new examples."
        },
        "19": {
            "figure_path": "2403.11793v1_figure_19.png",
            "caption": "(a) Even within the same category, tasks can showcase varied objectives and complexities. The left task eliminates vertically striped objects, while the right recolors objects based on orientation."
        },
        "20": {
            "figure_path": "2403.11793v1_figure_20.png",
            "caption": "(b) Depending on the task, there may be multiple or a unique input for an output. The left shows a task of completing a square with various inputs, and the right combines specific shapes, leading to a unique input."
        },
        "21": {
            "figure_path": "2403.11793v1_figure_21.png",
            "caption": "Figure 17. Two examples of the wrong generations for the task of completing the square shape. (a) LLM creates this input from the output of another example. (b) It is impossible to infer the color of the corners of the square based on this input."
        }
    },
    "references": [
        {
            "1": {
                "title": "Communicating Natural Programs to Humans and Machines. In NeurIPS.",
                "author": "Samuel Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong, Gabrielle Ecanow, Maxwell Nye, Michael Tessler, and Joshua B. Tenenbaum. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "2": {
                "title": "Measuring Abstract Reasoning in Neural Networks. In ICML.",
                "author": "David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "3": {
                "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models.",
                "author": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2023.",
                "venue": "arXiv:2308.09687 (2023).",
                "url": null
            }
        },
        {
            "4": {
                "title": "ARC-Game.",
                "author": "Alexey Borsky. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "5": {
                "title": "Sparks of Artificial General Intelligence: Early Experiments with GPT-4.",
                "author": "S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023.",
                "venue": "arXiv:2303.12712 (2023).",
                "url": null
            }
        },
        {
            "6": {
                "title": "A Survey on Evaluation of Large Language Models.",
                "author": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2024.",
                "venue": "ACM Transactions on Intelligent Systems and Technology (2024).",
                "url": null
            }
        },
        {
            "7": {
                "title": "On the Measure of Intelligence.",
                "author": "Fran\u00e7ois Chollet. 2019.",
                "venue": "arXiv:1911.01547 (2019).",
                "url": null
            }
        },
        {
            "8": {
                "title": "ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes. In CVPR. 5828\u20135839.",
                "author": "Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. 2017.",
                "venue": "",
                "url": null
            }
        },
        {
            "9": {
                "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation.",
                "author": "Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. 2023.",
                "venue": "arXiv:2311.04254 (2023).",
                "url": null
            }
        },
        {
            "10": {
                "title": "Faith and Fate: Limits of Transformers on Compositionality. In NeurIPS.",
                "author": "Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "11": {
                "title": "The Language of Thought. Vol. 5.",
                "author": "Jerry A Fodor. 1975.",
                "venue": "Harvard University Press.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Frames of Mind: The Theory of Multiple Intelligences.",
                "author": "Howard E Gardner. 2011.",
                "venue": "Basic Books.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Language Models Represent Space and Time.",
                "author": "Wes Gurnee and Max Tegmark. 2023.",
                "venue": "arXiv:2310.02207 (2023).",
                "url": null
            }
        },
        {
            "14": {
                "title": "Learning to Make Analogies by Contrasting Abstract Relational Structure. In ICLR.",
                "author": "Felix Hill, Adam Santoro, David GT Barrett, Ari S Morcos, and Timothy Lillicrap. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "15": {
                "title": "ARC: Where Do We Stand Today?",
                "author": "Michael Hodel. 2023a.",
                "venue": "",
                "url": null
            }
        },
        {
            "16": {
                "title": "Domain Specific Language for the Abstraction and Reasoning Corpus.",
                "author": "Michael Hodel. 2023b.",
                "venue": "",
                "url": null
            }
        },
        {
            "17": {
                "title": "Towards Reasoning in Large Language Models: A Survey. In ACL.",
                "author": "Jie Huang and Kevin Chen-Chuan Chang. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "18": {
                "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering. In CVPR. 2758\u20132766.",
                "author": "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017.",
                "venue": "",
                "url": null
            }
        },
        {
            "19": {
                "title": "Fast and Flexible: Human Program Induction in Abstract Reasoning Tasks. In CogSci.",
                "author": "Aysja Johnson, Wai Keen Vong, Brenden M. Lake, and Todd M. Gureckis. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "20": {
                "title": "Playgrounds for Abstraction and Reasoning. In NeurIPS Workshop on nCSI.",
                "author": "Subin Kim, Prin Phunyaphibarn, Donghyun Ahn, and Sundong Kim. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "21": {
                "title": "ARCathon Leaderboard.",
                "author": "Lab42. 2024.",
                "venue": "",
                "url": null
            }
        },
        {
            "22": {
                "title": "TVQA: Localized, Compositional Video Question Answering. In EMNLP.",
                "author": "Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. 2018.",
                "venue": "",
                "url": null
            }
        },
        {
            "23": {
                "title": "Understanding and Patching Compositional Reasoning in LLMs.",
                "author": "Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, and Ying Wei. 2024.",
                "venue": "arXiv:2402.14328 (2024).",
                "url": null
            }
        },
        {
            "24": {
                "title": "MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. In ICLR.",
                "author": "Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2024.",
                "venue": "",
                "url": null
            }
        },
        {
            "25": {
                "title": "SQA3D: Situated Question Answering in 3D Scenes. In ICLR.",
                "author": "Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "26": {
                "title": "DeepIQ: A Human-Inspired AI System for Solving IQ Test Problems. In IJCNN.",
                "author": "Jacek Ma\u0144dziuk and Adam \u017bychowski. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "27": {
                "title": "Recreating Raven\u2019s: Software for Systematically Generating Large Numbers of Raven-Like Matrix Problems With Normed Properties.",
                "author": "Laura E Matzen, Zachary O Benz, Kevin R Dixon, Jamie Posey, James K Kroger, and Ann E Speed. 2010.",
                "venue": "Behavior Research Methods 42, 2 (2010), 525\u2013541.",
                "url": null
            }
        },
        {
            "28": {
                "title": "A Logical Calculus of the Ideas Immanent in Nervous Activity.",
                "author": "Warren S McCulloch and Walter Pitts. 1943.",
                "venue": "The Bulletin of Mathematical Biophysics 5 (1943), 115\u2013133.",
                "url": null
            }
        },
        {
            "29": {
                "title": "A Review of Emerging Research Directions in Abstract Visual Reasoning.",
                "author": "Ma\u0142ki\u0144ski Miko\u0142aj and Ma\u0144d\u0301ziuk Jacek. 2023.",
                "venue": "Information Fusion 91 (2023), 713\u2013736.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Large Language Models as General Pattern Machines.",
                "author": "Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023.",
                "venue": "arXiv:2307.04721 (2023).",
                "url": null
            }
        },
        {
            "31": {
                "title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain.",
                "author": "Arseny Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. 2023.",
                "venue": "Transactions on Machine Learning Research (2023).",
                "url": null
            }
        },
        {
            "32": {
                "title": "Bongard-Logo: A New Benchmark for Human-Level Concept Learning and Reasoning. In NeurIPS.",
                "author": "Weili Nie, Zhiding Yu, Lei Mao, Ankit B Patel, Yuke Zhu, and Anima Anandkumar. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "33": {
                "title": "Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer.",
                "author": "Jaehyun Park, Jaegyun Im, Sanha Hwang, Mintaek Lim, Sabina Ualibekova, Sejin Kim, and Sundong Kim. 2023.",
                "venue": "ICML Workshop on ILHF (2023).",
                "url": null
            }
        },
        {
            "34": {
                "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement. In ICLR.",
                "author": "Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. 2024.",
                "venue": "",
                "url": null
            }
        },
        {
            "35": {
                "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning. In ACL.",
                "author": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "36": {
                "title": "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.",
                "author": "Frank Rosenblatt. 1958.",
                "venue": "Psychological Review 65, 6 (1958), 386\u2013408.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Artificial Intelligence: A Modern Approach.",
                "author": "Stuart J Russell and Peter Norvig. 1995.",
                "venue": "Pearson.",
                "url": null
            }
        },
        {
            "38": {
                "title": "MC-LARC Benchmark to Measure LLM Reasoning Capability. In Korea Software Congress.",
                "author": "Donghyeon Shin, Sanha Hwang, Seokki Lee, Yunho Kim, and Sundong Kim. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "39": {
                "title": "MovieQA: Understanding Stories in Movies through Question-Answering. In CVPR. 4631\u20134640.",
                "author": "Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016.",
                "venue": "",
                "url": null
            }
        },
        {
            "40": {
                "title": "Computing Machinery and Intelligence.",
                "author": "Alan Turing. 1950.",
                "venue": "Mind 59, 236 (1950), 433\u2013460.",
                "url": null
            }
        },
        {
            "41": {
                "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change. In NeurIPS.",
                "author": "Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2024.",
                "venue": "",
                "url": null
            }
        },
        {
            "42": {
                "title": "Do Multi-Hop Readers Dream of Reasoning Chains?. In EMNLP Workshop on MRQA.",
                "author": "Haoyu Wang, Mo Yu, Xiaoxiao Guo, Rajarshi Das, Wenhan Xiong, and Tian Gao. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "43": {
                "title": "Automatic Generation of Raven\u2019s Progressive Matrices. In IJCAI.",
                "author": "Ke Wang and Zhendong Su. 2015.",
                "venue": "",
                "url": null
            }
        },
        {
            "44": {
                "title": "Hypothesis Search: Inductive Reasoning with Language Models. In ICLR.",
                "author": "Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman. 2024b.",
                "venue": "",
                "url": null
            }
        },
        {
            "45": {
                "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models. In ICLR.",
                "author": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b.",
                "venue": "",
                "url": null
            }
        },
        {
            "46": {
                "title": "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation.",
                "author": "Zhaowei Wang, Wei Fan, Qing Zong, Hongming Zhang, Sehyun Choi, Tianqing Fang, Xin Liu, Yangqiu Song, Ginny Y Wong, and Simon See. 2024a.",
                "venue": "arXiv:2402.10646 (2024).",
                "url": null
            }
        },
        {
            "47": {
                "title": "AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph.",
                "author": "Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang, Sehyun Choi, Xin Liu, and Yangqiu Song. 2023a.",
                "venue": "arXiv:2311.09174 (2023).",
                "url": null
            }
        },
        {
            "48": {
                "title": "Learning Representations That Support Extrapolation. In ICML.",
                "author": "Taylor Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall O\u2019Reilly, and Jonathan Cohen. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "49": {
                "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In NeurIPS.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022.",
                "venue": "",
                "url": null
            }
        },
        {
            "50": {
                "title": "The Generative AI Paradox: \u201cWhat It Can Create, It May Not Understand\u201d. In ICLR.",
                "author": "Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, Benjamin Newman, Pang Wei Koh, Allyson Ettinger, and Yejin Choi. 2024.",
                "venue": "",
                "url": null
            }
        },
        {
            "51": {
                "title": "Cybernetics.",
                "author": "Norbert Wiener. 1950.",
                "venue": "Bulletin of the American Academy of Arts and Sciences 3, 7 (1950), 2\u20134.",
                "url": null
            }
        },
        {
            "52": {
                "title": "ARC-Solution.",
                "author": "Johan Sokrates Wind. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "53": {
                "title": "STAR: A Benchmark for Situated Reasoning in Real-World Videos. In NeurIPS.",
                "author": "Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. 2021.",
                "venue": "",
                "url": null
            }
        },
        {
            "54": {
                "title": "Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus. In AAAI.",
                "author": "Yudong Xu, Elias B. Khalil, and Scott Sanner. 2023a.",
                "venue": "",
                "url": null
            }
        },
        {
            "55": {
                "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations.",
                "author": "Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, and Elias B Khalil. 2023b.",
                "venue": "Transactions on Machine Learning Research (2023).",
                "url": null
            }
        },
        {
            "56": {
                "title": "Phy-Q as a Measure for Physical Reasoning Intelligence.",
                "author": "Cheng Xue, Vimukthini Pinto, Chathura Gamage, Ekaterina Nikonova, Peng Zhang, and Jochen Renz. 2023.",
                "venue": "Nature Machine Intelligence 5, 1 (2023), 83\u201393.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In NeurIPS.",
                "author": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "58": {
                "title": "RAVEN: A Dataset for Relational and Analogical Visual Reasoning. In CVPR.",
                "author": "Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. 2019.",
                "venue": "",
                "url": null
            }
        },
        {
            "59": {
                "title": "Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning. In AAAI.",
                "author": "Wenhe Zhang, Chi Zhang, Yixin Zhu, and Song-Chun Zhu. 2020.",
                "venue": "",
                "url": null
            }
        },
        {
            "60": {
                "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. In ICLR.",
                "author": "Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023.",
                "venue": "",
                "url": null
            }
        },
        {
            "61": {
                "title": "Teaching Algorithmic Reasoning via In-Context Learning.",
                "author": "Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. 2022.",
                "venue": "arXiv:2211.09066 (2022).",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.11793v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.3",
            "3.3.1",
            "3.3.2"
        ],
        "main_experiment_and_results_sections": [
            "3.1.3",
            "3.1.4",
            "3.2.3",
            "3.2.4",
            "3.3.3",
            "3.3.4",
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.11793v1",
        "paper_title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus",
        "research_background": "### Motivation and Research Problem\n\nRecent advancements in Large Language Models (LLMs) have showcased close-to-human performance levels across various tasks. However, existing studies indicate that these models still lack true planning and reasoning abilities (Bubeck et al., 2023). This gap provokes a fundamental research question: Can LLMs actually think? Despite the development of multiple benchmarks to assess reasoning skills, such as MathVista (Lu et al., 2024), Bonagard-Logo (Nie et al., 2020), and Raven (Zhang et al., 2019), the Abstract and Reasoning Corpus (ARC) (Chollet, 2019) has emerged as a particularly challenging and representative benchmark. Unlike other datasets, ARC requires a high level of abstraction and multiple reasoning steps, areas where traditional deep learning techniques and current LLMs struggle to succeed. The best-performing models have achieved only a meager accuracy of 30% (Lab42, 2024), while LLMs lag even further behind at around 10% (Mirchandani et al., 2023), compared to an average human accuracy of 80% (Johnson et al., 2021). These statistics highlight significant differences in reasoning and abstraction capabilities between humans and LLMs.\n\nExisting literature often focuses on the results, neglecting the intricacies of the reasoning process, prompting researchers to advocate for a more process-oriented analysis (Chang et al., 2024; Huang and Chang, 2023; Xue et al., 2023). This study aims to fill this research gap by adopting the Language of Thought Hypothesis (LoTH) (Fodor, 1975) to assess the reasoning skills of LLMs. LoTH, a dominant theory in the philosophy of mind, suggests that effective reasoning involves logical coherence, compositionality, and productivity. This study sets out to examine the ARC benchmark through the lens of these three components, aiming to gain deeper insights into the reasoning abilities of LLMs and how they compare to human capabilities.\n\n### Relevant Prior Work\n\n1. **LLMs' Performance and Limitations:**\n   - **Bubeck et al. (2023)** highlighted the lack of planning and reasoning abilities in current LLMs.\n   - The study by **Lab42 (2024)** reported an accuracy of 30% for the best-performing models on ARC, while **Mirchandani et al. (2023)** found that LLMs only reached around 10% accuracy.\n   - By comparison, **Johnson et al. (2021)** noted that average human accuracy for similar tasks stands at approximately 80%.\n\n2. **Reasoning Benchmarks:**\n   - Multiple new benchmarks like **MathVista (Lu et al., 2024)**, **Bonagard-Logo (Nie et al., 2020)**, and **Raven (Zhang et al., 2019)** have been developed to evaluate reasoning abilities.\n   - The **Abstract and Reasoning Corpus (Chollet, 2019)** remains one of the prominent benchmarks for assessing reasoning capabilities, posing a significant challenge due to its need for high levels of abstraction and multiple reasoning steps.\n\n3. **Need for Process-oriented Analysis:**\n   - Calls for shifting focus from results to process-oriented analysis have been made by researchers like **Chang et al. (2024)**, **Huang and Chang (2023)**, and **Xue et al. (2023)**.\n   - The Language of Thought Hypothesis (LoTH) by **Fodor (1975)** offers a framework to evaluate reasoning abilities based on logical coherence, compositionality, and productivity.\n\n4. **Previous Studies Questioning LLMs' Reasoning Abilities:**\n   - Studies by **Bubeck et al. (2023)** and **Valmeekam et al. (2024)** raise doubts about the reasoning capabilities of LLMs but lack detailed, in-depth analysis.\n\nBy incorporating these insights, the study aims to evaluate LLMs on the ARC benchmark through logical coherence, compositionality, and productivity, thereby advancing the understanding of LLMs' reasoning abilities relative to human cognition.",
        "methodology": "The methodology section of the paper on the reasoning abilities of large language models (LLMs) provides a comprehensive evaluation framework to determine whether LLMs possess inferential capabilities comparable to human reasoning. The evaluation is structured around three main components of human reasoning according to the Language of Thought Hypothesis (LoTH): Logical Coherence, Compositionality, and Productivity. The study uses the Abstraction and Reasoning Corpus (ARC) to assess these aspects and to test the validity of ARC as a benchmark for reasoning abilities in LLMs.\n\n### Key Components and Innovations:\n\n1. **Analysis of Problem-Solving Capabilities:**\n   - The study categorizes problems based on difficulty as perceived by humans, using these categories to evaluate LLM performance. The classification revealed that problems deemed difficult by humans were also challenging for LLMs. Difficult problems typically require complex inference processes and multitasking to identify changes.\n   - Tasks are broken into steps to analyze their complexity. For example, an 'Entry' level task involves a single step, while a 'Hard' task involves multiple steps such as object recognition, prioritization, and merging.\n\n2. **Evaluation of Different Prompting Techniques:**\n   - The study explores various prompting techniques including Chain-of-Thought (CoT), Look to Model (LtM), and Tree of Thought (ToT). These methods are analyzed to determine their ability to solve tasks logically and consistently.\n   - Despite occasional correct answers, the methods often lacked a logical consistency in rule application. Examples such as counting objects with the highest number of black squares revealed flaws in the process, as CoT sorted objects incorrectly and selected arbitrarily.\n\n3. **Consistency Across Similar Tasks:**\n   - Evaluates whether LLMs produce consistent results for tasks of the same type, which involve repetitive generation of a specific pattern. Although LLMs could solve one type of task, they often failed a similar task, indicating a lack of consistent logical structure.\n\n4. **Understanding of Domain-Specific Languages (DSLs):**\n   - The LLM's ability to generate correct transformations using given DSLs was analyzed. While LLMs demonstrated understanding and the ability to perform basic transformations without errors, they struggled to predict and combine DSLs effectively to solve tasks completely.\n   - The study highlights issues with LLMs repeating the same DSLs and their tendency to copy inputs rather than deducing rules.\n\n5. **Replication Errors and Rule Deduction:**\n   - LLMs frequently replicated input examples directly, a significant limitation. Even with prompts designed to mitigate this, the models failed to deduce meaningful rules from examples, often leading to incoherent outputs.\n   - The inability to handle small steps required for certain tasks further complicated the evaluation, resulting in inputs that do not align with task-specific rules.\n\n### Conclusion:\n\nThe study found that while LLMs exhibit some basic forms of visual logic and understanding of transformations, they fall short in handling complex reasoning tasks that require logical consistency and rule deduction. This points to the current limitations of LLMs in achieving human-like reasoning abilities.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Experiment Setup:\n- **Datasets**: The primary dataset used is the Abstraction and Reasoning Corpus (ARC), comprising 100 random tasks from the evaluation set. \n- **Baselines**: Three methods were assessed: \n  - LtM (Latent to Manifest)\n  - CoT (Chain of Thought)\n  - ToT (Tree of Thoughts)\n- **Evaluation Metrics**: \n  - The primary metric is the percentage of correctly answered questions.\n  - Accuracy is measured in two ways:\n    - Answers alone (accuracy outside parentheses)\n    - Answers combined with correct reasoning processes (accuracy inside parentheses), as evaluated by humans.\n\n#### Results:\n- **Overall Accuracy**:\n  - **CoT**: Achieved an accuracy of **10.6%** for correct answers.\n  - **LtM and ToT**: Both demonstrated approximately **6%** accuracy for correct answers.\n  - When considering both correct answers and reasoning processes, the accuracy for all models dropped to between **2-4%**. This indicates that even if the LLMs managed to provide correct answers, the reasoning processes were often incorrect.\n\n- **Observations on Difficulty Levels**:\n  - LLMs struggled with tasks perceived as difficult by humans, characterized by the need for lengthy inference processes and simultaneous consideration of multiple problems.\n  - Examples illustrate this point, showing that 'Hard' tasks required multiple steps and complex combinations of logic, unlike 'Entry', 'Easy', or 'Medium' tasks which required fewer or simpler steps.\n\n- **Logical Consistency**:\n  - Analyzing tasks with correct answers but flawed processes revealed that LLMs, regardless of the prompting technique used (CoT, LtM, ToT), often failed to maintain logical consistency in their reasoning.\n  - Examining consistent results for the same task type showed that LLMs still struggled to apply a single rule consistently across different but similar tasks.\n  - This inconsistency underlines that although LLMs can identify patterns and apply functions like rotation and flipping, they still do not process tasks with human-like logical coherence."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the logical coherence of large language models (LLMs) using advanced prompting techniques, specifically their ability to understand and consistently apply logic across different contexts in Abstract and Reasoning Corpus (ARC) tasks.",
            "experiment_process": "The experiment employed advanced prompting techniques such as Chain of Thought (CoT), Least to Most (LtM), and Tree of Thought (ToT). These techniques were tested on 100 random ARC tasks using advanced models like GPT-4 and GPT-4-32k. Each technique was applied to see if LLMs could improve their logical reasoning by solving ARC tasks step-by-step and ensuring logical consistency. CoT prompts provided human-written problem-solving instructions, while LtM and ToT prompts required LLMs to generate their own steps.",
            "result_discussion": "Table 2 showed that CoT had a 10.6% accuracy, while LtM and ToT exhibited around 6%. When considering both correct answers and processes, the accuracy fell to 2-4%, indicating that solution processes often lacked correct reasoning. Further analysis revealed that tasks perceived as difficult by humans were also challenging for LLMs, requiring lengthy inference processes and consideration of multiple problems simultaneously. The results suggested that LLMs possess simple forms of visual logic but struggle with complex combinations of logic, highlighting deficiencies in logical coherence.",
            "ablation_id": "2403.11793v1.No1"
        },
        {
            "research_objective": "To investigate the compositionality of LLMs by analyzing their ability to combine basic functions to solve more complex ARC tasks.",
            "experiment_process": "This experiment utilized Domain-Specific Languages (DSLs) representing various functions necessary for solving ARC tasks. LLMs were provided with these DSLs and basic ARC grid inputs to test whether they could accurately generate outputs by applying given transformations and combining them to achieve desired results. Techniques like Tree of Thought (ToT) were used to generate and evaluate multiple solution proposals in a step-by-step manner.",
            "result_discussion": "The results indicated that while LLMs understood individual functions (DSLs), they could not effectively combine these to solve tasks, achieving 0% success in 99 ARC tasks. LLMs showed a 75% tendency to repeat the last used DSL, indicating understanding at a basic level but failing to predict necessary DSL combinations for complex problem-solving. This highlighted significant differences in compositional ability between humans (80% success) and LLMs, pointing to deficiencies in LLMs' ability to decompose and recombine functions.",
            "ablation_id": "2403.11793v1.No2"
        },
        {
            "research_objective": "To evaluate the productivity of large language models (LLMs) by testing their ability to generate new, unseen representations based on observed ARC task data and abstract rules.",
            "experiment_process": "Productivity was tested by using an Inverse Transformation Prompt (ITP) technique. LLMs received ARC example pairs and abstract rules, with the task of generating new, unseen example inputs that matched given outputs. The experiment focused on 160 ARC tasks classified by ConceptARC into 16 categories, ensuring that tasks within each category adhered to the same abstract rule. Human judgment was used to evaluate the validity of the generated examples, ensuring they complied with the given rules.",
            "result_discussion": "The results showed that LLMs achieved a 17.1% valid generation ratio out of 2,913 generated examples. While some examples matched the rules, many failed due to difficulties in inferring specific rules and step-by-step generation, leading to direct replication or invalid generation. The experiment revealed significant weaknesses in LLMs' ability to deduce meaningful rules and generate valid new examples, underscoring the limitations in their productivity.",
            "ablation_id": "2403.11793v1.No3"
        }
    ]
}