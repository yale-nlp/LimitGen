{
    "title": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling",
    "abstract": "A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts.\nAlthough contemporary text encoding methods cover most of the world\u2019s writing systems, they exhibit bias towards the high-resource languages of the Global West.\nAs a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units.\nTo address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages.\nOur encoding convention (MYTE)\nis based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods.\nWe show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts.\nThis, in turn, improves multilingual LM performance and diminishes the perplexity gap throughout diverse languages.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Multilingual language models have become the state-of-the-art solution for performing tasks on a wide range of languages Devlin et al. (2019  ###reference_b10###); Conneau et al. (2020  ###reference_b8###); Xue et al. (2021  ###reference_b35###).\nHowever, it is challenging to ensure high performance for all languages due to differences in data availability, especially for the long tail of low-resource languages Malkin et al. (2022  ###reference_b19###). This challenge is compounded by choices of how words are represented during tokenization; past studies have shown that multilingual models either cannot accurately represent texts in rare languages Pfeiffer et al. (2021  ###reference_b23###) or do so via over-segmentation, which is detrimental both to model performance and inference cost Petrov et al. (2023  ###reference_b22###); Ahia et al. (2023  ###reference_b2###).\n###figure_1### Byte-level models aim to solve these challenges. Rather than words or subword tokens, they use byte-level text representations that achieve high coverage Xue et al. (2022  ###reference_b34###), as common encodings such as UTF-8 support most of the world\u2019s scripts.\nNevertheless, the over-segmentation problem still exists even at the byte level, as byte sequences for single characters are overly long for many non-Latin script languages Arnett et al. (2024  ###reference_b3###). This problem has an immense effect on modeling these scripts in NLP systems, as operating on longer sequences significantly increases the computation costs of training and inference in models, while also making learning less sample efficient.\nFurthermore, the billing for APIs such as ChatGPT (openai.com/chatgpt  ###reference_om/chatgpt###)\nis often associated with the segmented sequence length, disadvantaging speakers of specific languages Ahia et al. (2023  ###reference_b2###).\nIn this work, we propose a novel method to derive byte representations of text, enabling equitable segmentations across languages and scripts.\nIn our approach, we replace the current convention of assigning byte codes to characters with a morphology-driven approach, as morphemes111In this work, the usage of term \u201cmorphemes\u201d encompasses both \u201cmorphemes\u201d and \u201cmorphs\u201d. Some linguistic theories use the term \u201cmorph\u201d for specific textual realizations of abstract \u201cmorphemes\u201d. For instance, in English, es as in foxes and s as in cats are two distinct \u201cmorphs\u201d of a plurality \u201cmorpheme\u201d. For an in-depth discussion about these two terms, see Section 4 of \u017dabokrtsk\u00fd et al. (2022  ###reference_b37###) are more informatively comparable constituents of text across languages than characters Cotterell et al. (2018  ###reference_b9###).\nSpecifically, we introduce a novel algorithm for representing text as byte sequences that is based on unsupervised morphological segmentation Smit et al. (2014  ###reference_b29###).\nWe demonstrate that our new paradigm for byte representation improves the segmentation of diverse languages of various scripts and morphological inventories. Furthermore, the segmentation of parallel sentences across languages converges to comparable lengths.\nWe test our method\u2019s effectiveness in creating equitable text representation \u2013 representations that given parallel texts have similar encoded sequence lengths.\nWe then evaluate the applicability of the method to multilingual language modeling across 99 typologically diverse languages.\nOur contributions can be summarized as follows: (a) We propose a novel byte-encoding method that is morphologically driven; (b) We show empirically that the resulting representations are more equitable across languages than vanilla byte, character, or subword segmentation; (c) We analyze the typical lengths of these representations and show decreased\nsequence length across all analyzed languages, significantly reducing computation cost and benefiting non-Latin script languages the most; (d) We train a language model with our new representation scheme and demonstrate that it maintains balanced and better LM performance across diverse languages and exhibits faster inference speed. This improvements holds across different model scales.\nOur models match SOTA ByT5 performance across multiple tasks for diverse low-resource languages while being more efficient in training and inference.\nWe will release our code and models to facilitate further research in this direction."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background: UTF-8 Bytes",
            "text": "The vast majority of texts online222https://w3techs.com/technologies/overview/character_encoding  ###reference_character_encoding### are represented as bytes via UTF-8 convention, which is defined by the Unicode Standard The Unicode Consortium (2011  ###reference_b33###).\nIn UTF-8, each character (or codepoint) is represented as a sequence of one to four bytes.\nDue to the gradual development of communication standards, UTF-8 first allocated one-byte representation ASCII symbols, which cover primary Latin-script characters (see  0000 to  7F7F in Figure 2  ###reference_###).\nOther characters are represented as multi-byte codes starting with a byte from range  C2C2 to  F4F4 denoting the number of bytes in the codepoint and followed by continuation bytes from range  8080 to  BFBF.\nIn UTF-8 convention, characters in non-Latin alphabetic scripts (Cyrillic, Armenian, Georgian), diacritics, and abjads333Abjads are writing scripts that do not denote vowels, e.g., Hebrew, Arabic.\nusually have two-byte codes, while the byte length increases to three or four for Brahmic abugidas444Abugidas are scripts representing consonant-vowel as one character, typical to the Indian Subcontinent and South-East Asia, e.g., Devanagari, Bengali. and CJK (Chinese, Japanese, Korean) logographs.\nAs a result, the granularity of byte codes varies significantly across languages; this means that texts conveying the same information across languages tend to be represented by byte sequences of significantly different lengths Arnett et al. (2024  ###reference_b3###).\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method: Morphology-Driven Bytes",
            "text": "As discussed in the prior section and shown in Figure 1  ###reference_###, UTF-8 convention produces longer byte sequences for some languages due to the development choices.\nTo make byte representation more equitable, we introduce an encoding paradigm that aims to assign byte codes of similar lengths to morphemes across languages.\nWe base our encoding scheme on morphological analysis because morphemes are\nthe shortest meaningful constituents and are independent of the writing convention Haspelmath and Sims (2010  ###reference_b11###).\nWe assume that the number of morphemes in sentences with the same information load is more balanced across languages than the number of characters, bytes, or tokens.\nThus, we enforce balanced segmentation granularity across languages.\nAn alternative approach to encoding morphological representations would be treating the union of multilingual morpheme inventories across languages as one large subword vocabulary.\nTo cover the morphemes of many languages in this manner, the vocabulary would be much larger than the ones usually applied to models.555The proposed MYTE encoding offers capacity for 2,130,432 of variable length codepoints. It is considerably more than in any of the commonly used subword vocabularies. For reference, large vocabulary XLM-V model allocates 1 million subwords Liang et al. (2023  ###reference_b16###). This would incur additional computational costs and, similar to other subword representations, would likely not generalize well to new, unseen languages."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Morphological Analysis",
            "text": "We train an unsupervised morphological analyzer, Morfessor Smit et al. (2014  ###reference_b29###) on lexicons derived from whole Wikipedia articles in 99 languages.\nThe morphological analysis is performed on each of the languages separately to balance the number of morphemes per language, regardless of data resourcefulness.\nFor each language, we derived a set of 4096 morphemes; the number was chosen to balance segmentation granularity across languages.\nFor each morpheme, we save its score, defined as the hypothetical loss reduction of the Morfessor model if the morpheme had not been included in the set.\nWe take the union of sets across languages to obtain a multilingual morpheme inventory.\nThe details of lexicon preparation and the usage of Morfessor are in Appendix A  ###reference_###.\nLatin\nLatin\nCommon\nMixed, Common, Inherited, Unkown\nNon-Latin Alphabetic\nGreek, Cyrillic, Armenian, Georgian\nAbjads\nHebrew, Arabic, Syriac, Thaana, Tifinagh\nAbugidas North\nDevanagari, Gurmukhi, Gujarati, Oriya, Bengali, Sinhala, Tibetan\nAbugidas South\nTelugu, Kannada, Tamil, Malayalam, Thai, Lao, Myanmar, Tai, Tagalog, Khmer\nCJK\nHangul, Han, Yi, Katakana, Hiragana, Bopomofo\nOther\nRemaining scripts"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Enriching Byte Representation with Morphology",
            "text": "To alleviate UTF-8 inefficiencies, we propose a systematic rearrangement of byte codepage.\nWe free 26 bytes (  4141 to  5A5A ) by decomposing capital letter codes into lowercase letters and capitalization markers. The first byte from this range (  4141 ) is repurposed as a capitalization marker.\nThe remaining 25 bytes are freed space used to store morphemes.\nOur method takes the sequences of UTF-8 bytes and transcodes them into shorter sequences using the vocabulary of the same size, i.e. 256, as depicted in Figure 1  ###reference_###. We apply the following steps to transcode UTF-8 sequences to MYTE encodings:\nWe use UTF-8 as base encoding of text.\nThen, the byte sequences are transcoded from left to right, merging morpheme sequences and replacing them as dedicated codepoints described in the following points.\nThe morphemes are grouped by scripts as shown in Table 1  ###reference_###.\nCodepoints of multiple scripts within a single morpheme are assigned to the second cluster (Mixed script).\nThe morphemes are ranked based on their Morfessor score defined in Section 3.1  ###reference_###.\nWe assign multibyte codepoint for each of the morphemes analogously to the UTF-8 convention (see Section 2  ###reference_###).\nSpecifically, the first byte denoting the beginning of the morphological codepoint is assigned from the freed range (  4242 -  5A5A ) based on the morph\u2019s inclusion in one of the script groups.\nIt is followed by continuation bytes from the  element range  8080  -  BFBF, as in UTF-8 convention.\nThe  morphemes with the highest score are saved as two-byte codepoints, following  as three-byte codepoints; the remaining morphemes are saved as up to  four-byte codepoints. The capacity for new codepoints was not exhausted for any script group."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Equitable Multilingual Segmentation with MYTE",
            "text": "We first analyze the properties of our proposed morphology-driven encoding.\nFollowing the setting of Petrov et al. (2023  ###reference_b22###), we measure whether MYTE produces the segmented sequences of comparable length across languages.\nWe compute parity across languages using the multi-parallel corpus Flores 200 Team et al. (2022  ###reference_b32###).\nParity is defined as , where  and  stand for parallel sentences in language  and in English, respectively.  is the length of sequence  with segmentation method .\nWe compare the MYTE encoding from Section 3.2  ###reference_### to several baselines of common input representation: (a) Vanilla byte-level encoding via UTF-8; (b) Character-level encoding; (c) Subwords produced by Sentencepiece algorithm Kudo and Richardson (2018  ###reference_b14###). In comparison, we focus on the equitability of sequence lengths produced by the methods for diverse languages.\nFurthermore, we compare our morphological byte encoding sequence compression rate against the UTF-8 convention. Compression is essential for an effective text representation as it affects NLP systems\u2019 efficiency and usage cost Ahia et al. (2023  ###reference_b2###).\nFinally, we check whether our method more effectively compresses languages and scripts unseen in MYTE algorithm described in Section 3.2  ###reference_###.\nThe comparison of sequence length across parallel sentences in Flores 200 is shown in Figure 4  ###reference_###. Our representation is more balanced across languages than the original UTF-8 bytes. There are still four languages with observably higher code lengths (e.g., Greek, Vietnamese, Punjabi, Khmer).\nHowever, MYTE encoding still improves their parity to English such that it is much lower than outlier languages in UTF-8 (1.7 vs. 3.5 in the worst-case languages, respectively).\nFigure 3  ###reference_### shows that MYTE representations are more balanced in parity scores across languages than subword tokenization. In particular, we improve on the long tail of languages over-segmented either in byte or subword encoding.\nThe parties closest to MYTE are obtained by character representation.\nHowever, the set of all Unicode characters is larger by orders of magnitude than the number of unique bytes used in MYTE (149,878 vs. 254).\n###figure_3### ###figure_4### ###figure_5### ###figure_6### ###table_1### The encoded sequence lengths are decreased with MYTE encoding for all languages, as depicted in Figure 3(c)  ###reference_sf3###.\nThe rate of compression varies from  1% for Vietnamese and Chinese to almost 70% for Burmese. As seen in Table 2  ###reference_###, the highest compression is obtained for low-resource languages with non-Latin scripts.\nNotably, this group of languages is the most susceptible to over-segmentation in UTF-8 encoding.\nIn Table 2  ###reference_###, we observe that a decrease in sequence length and parity applies to five low-resource languages not considered in constructing MYTE representation, referred to as unseen languages.\nOne exemption from the rule is Santhali, written in unseen Ol Chiki script, for which we do not observe a change in the encoded sequence length.\nThis observation highlights the importance of considering a wide range of languages and scripts when constructing morpheme inventories.\nImportantly, MYTE did not reach a capacity of available byte codepoints, and thus, the method can be extended to additional languages. The complete results for unseen languages and scripts are shown in Appendix B  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "MyT5: Language Modeling with MYTE",
            "text": "This section investigates the benefits of MYTE as an encoding scheme for byte-level language modeling.\nFor that purpose, we have trained T5 language models on MYTE representation.\nWe refer to these models as Myte T5 models, or MyT5 for short.\n###figure_11### ###figure_12### In Figure 4(a)  ###reference_sf1###, our model outperforms ByT5, producing lower (better) average  scores for all analyzed languages.\nThe improvement is strongly negatively correlated with the compression rate discussed in the previous section.\nThe gains are largest for languages using Abugidas (scripts representing consonant-vowel as one character, typical to the Indian Subcontinent and SE Asia) that tend to be shortened the most by MYTE encoding.\nOn the other end of compression distribution, we still observe (smaller) improvement for Latin and CJK scripts.\nThis observation suggests that the MYTE encoding\u2019s leverage is not constrained to shortening sequences, but it also uses codepoints that are easier to predict by a language model.\nMYTE uses codepoints based on morphemes that are inherently meaningful language units in contrast to orthographic symbols, which are the backbone of the UTF-8 convention.\n###figure_13### Previous works have argued that some languages are more challenging to model due to their morphological properties Cotterell et al. (2018  ###reference_b9###).\nIn contrast, others suggest that LM performance is linked with how texts in specific languages are represented Park et al. (2021  ###reference_b21###).\nOur results in Figure 6  ###reference_### support the latter view, as the predictability of the languages is balanced by using equitable underlying representation, i.e., MYTE encoding.\nSpecifically, we show that MyT5 achieves more balanced  across languages than ByT5.\nAs discussed in the previous section, the benefit is the starkest for languages prone to over-segmentation under UTF-8.\nThe smallest improvements of MyT5 are obtained for languages benefited by MYTE to a lesser extent, as observed in Section 4.1  ###reference_###: Greek and Vietnamese.\n###table_3### As shown in Figure 4(b)  ###reference_sf2###, MyT5\u2019s inference time is shorter than that of ByT5 for almost all languages.\nThis behavior is mostly observed for Non-Latin script languages and can thus be attributed to the higher rates of compression observed when using the MYTE encoding scheme (Figure 4  ###reference_###).\nFurthermore, Table 3  ###reference_### demonstrates that MyT5\u2019s inference speed gains over ByT5 improve with model size, hinting that MYTE will bring further efficiency gains when applied to models of larger scales.\n###figure_14### ###table_4### As shown in Table 4  ###reference_###, MyT5 and ByT5 perform comparably (and better than baselines) on MT and NER.\nWhile MyT5 outperforms ByT5 by 2 points on QA, the opposite is true for semantic parsing.\nWe hypothesize that in this case, the morphological prior encoded in MYTE may confound semantic parsing fine-tuning, which requires a structured output significantly different from natural language.\nFor all the tasks, the inference of MyT5 is faster than ByT5 (Figure 7  ###reference_###), mirroring our observations on language modeling efficiency.\nHowever, we do not observe a consistent relationship between the change in end task performance and efficiency, contrasting with the earlier observed correlation between  of inference time and  in multilingual language modeling."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Training Details",
            "text": "We base the architecture and implementation of our MyT5 model on the byte-level T5 model, i.e., ByT5 Xue et al. (2022  ###reference_b34###).\nByT5, like other T5 models Raffel et al. (2020  ###reference_b24###), is an encoder-decoder Transformer model trained on predicting masked spans of texts.\nByT5 operates on bytes instead of the subword tokenization in the standard T5 model, making it a suitable base model for our setting.\nWe pre-train three new instances of MYTE-level models of different sizes: small (300M), base (582M), and large (1.23B parameters). For pre-training, we used the standard task of restoring corrupted spans from mC4 corpus Raffel et al. (2020  ###reference_b24###).\nAll the byte sequences are transcoded into morphologically-driven bytes.\nWe use Jax implementation, i.e., t5x repository Roberts et al. (2022  ###reference_b25###), and the same hyperparameters as in ByT5 Xue et al. (2022  ###reference_b34###).\nThe only difference from their training approach is that we pre-train for 250,000 steps rather than one million steps since we observe overfitting when training for more steps, especially on low-resource languages. Chung et al. (2023  ###reference_b5###) similarly observed overfitting in multilingual T5 models caused by extensive duplications in the mC4 corpus, leading them to also train models for only 250,000 steps.\nIn evaluations, we compare against a reimplemented ByT5 instance trained for the same number of steps."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Experiments",
            "text": "We compare the performance of the MyT5 and ByT5 models, focusing on three aspects: language modeling performance, efficiency, and downstream evaluation.\nFirst, the multilingual language modeling performance of MyT5 \u2013 how is it, and is it comparable across languages?\nInspired by Cotterell et al. (2018  ###reference_b9###), we use the Bit-per-English-Byte metric on the multi-parallel FLORES 200 corpus to control for the informativeness of evaluation sequences:\nis a sequence of bytes (original UTF-8 or MYTE) with  being the -th byte.\nFor normalization, we use the number of UTF-8 bytes in English sentence  for fair comparison across languages and representation methods.\nIt is the main difference from perplexity, which is normalized by the sequence length and thus confounded by segmentation rates characteristic of individual languages and encodings.\nSecond, we compare inference times of text generation of MyT5 and ByT5.\nWe expect a decrease in sequence length, as shown in the last section, will render up to a quadratic reduction of forward-pass time due to the quadratic complexity of attention computation.\nFor both aspects, we report the results on three scales of the model (small, base, and large). Unless stated otherwise, we present the results of the large model.\nLastly, we compare models\u2019 performance on four tasks from the Xtreme-Up benchmark Ruder et al. (2023  ###reference_b26###): question answering, named entity recognition, semantic parsing, and translation from English.\nIn each task, we fine-tune the large models on the multilingual data of all languages for each task.\nFine-tuned models are evaluated on test data for low-resource languages, following Ruder et al. (2023  ###reference_b26###).\nThe only exception is machine translation, where we fine-tune and evaluate on a subset of languages to reduce the computation cost.\nThe details of training and evaluation are provided in Appendix C  ###reference_###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "In Figure 4(a)  ###reference_sf1###  ###reference_sf1###, our model outperforms ByT5, producing lower (better) average  scores for all analyzed languages.\nThe improvement is strongly negatively correlated with the compression rate discussed in the previous section.\nThe gains are largest for languages using Abugidas (scripts representing consonant-vowel as one character, typical to the Indian Subcontinent and SE Asia) that tend to be shortened the most by MYTE encoding.\nOn the other end of compression distribution, we still observe (smaller) improvement for Latin and CJK scripts.\nThis observation suggests that the MYTE encoding\u2019s leverage is not constrained to shortening sequences, but it also uses codepoints that are easier to predict by a language model.\nMYTE uses codepoints based on morphemes that are inherently meaningful language units in contrast to orthographic symbols, which are the backbone of the UTF-8 convention.\n###figure_15### Previous works have argued that some languages are more challenging to model due to their morphological properties Cotterell et al. (2018  ###reference_b9###  ###reference_b9###).\nIn contrast, others suggest that LM performance is linked with how texts in specific languages are represented Park et al. (2021  ###reference_b21###  ###reference_b21###).\nOur results in Figure 6  ###reference_###  ###reference_### support the latter view, as the predictability of the languages is balanced by using equitable underlying representation, i.e., MYTE encoding.\nSpecifically, we show that MyT5 achieves more balanced  across languages than ByT5.\nAs discussed in the previous section, the benefit is the starkest for languages prone to over-segmentation under UTF-8.\nThe smallest improvements of MyT5 are obtained for languages benefited by MYTE to a lesser extent, as observed in Section 4.1  ###reference_###  ###reference_###: Greek and Vietnamese.\n###table_5### As shown in Figure 4(b)  ###reference_sf2###  ###reference_sf2###, MyT5\u2019s inference time is shorter than that of ByT5 for almost all languages.\nThis behavior is mostly observed for Non-Latin script languages and can thus be attributed to the higher rates of compression observed when using the MYTE encoding scheme (Figure 4  ###reference_###  ###reference_###).\nFurthermore, Table 3  ###reference_###  ###reference_### demonstrates that MyT5\u2019s inference speed gains over ByT5 improve with model size, hinting that MYTE will bring further efficiency gains when applied to models of larger scales.\n###figure_16### ###table_6### As shown in Table 4  ###reference_###  ###reference_###, MyT5 and ByT5 perform comparably (and better than baselines) on MT and NER.\nWhile MyT5 outperforms ByT5 by 2 points on QA, the opposite is true for semantic parsing.\nWe hypothesize that in this case, the morphological prior encoded in MYTE may confound semantic parsing fine-tuning, which requires a structured output significantly different from natural language.\nFor all the tasks, the inference of MyT5 is faster than ByT5 (Figure 7  ###reference_###  ###reference_###), mirroring our observations on language modeling efficiency.\nHowever, we do not observe a consistent relationship between the change in end task performance and efficiency, contrasting with the earlier observed correlation between  of inference time and  in multilingual language modeling."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Fair Representation across Languages",
            "text": "Perhaps the most significant challenge of multilingual NLP is the large disparity of resourcefulness across the world\u2019s languages Joshi et al. (2020  ###reference_b13###), as the size and quality of data used for the model training directly affects its performance in individual languages.\nHence, researchers have proposed multiple ways to balance the training signal across languages Malkin et al. (2022  ###reference_b19###).\nSolutions include sampling data to overrepresent low-resource languages, e.g., with alpha Conneau et al. (2020  ###reference_b8###) or uniform sampling of data across languages Chung et al. (2023  ###reference_b5###). This unequal treatment of languages is also present in how data is encoded as input to the model Ahia et al. (2023  ###reference_b2###).\nPetrov et al. (2023  ###reference_b22###) show that practically all methods used to represent texts as input of NLP systems treat languages unequally, segmenting some (mainly the lowest-resourced ones) into fine-grained non-informative units.\nSome approaches aimed at balancing the segmentation or tokenization methods have been introduced.\nLimisiewicz et al. (2023  ###reference_b17###) proposed merging vocabulary based on the tokenizer scoring function.\nZheng et al. (2021  ###reference_b38###) introduced a method of allocating vocabulary capacity uniformly across languages, while Chung et al. (2020  ###reference_b6###) constructed multilingual vocabulary for clusters of languages and merged them.\nLiang et al. (2023  ###reference_b16###) combined the elements of both approaches and showed the advantage of extending vocabulary to benefit multilingual transfer.\nThese solutions offer the promise of obtaining a better allocation of vocabulary units.\nHowever, they do not solve the inequality of the underlying encoding, which may affect the construction process of vocabulary units.\nFor instance, byte merges in the BPE algorithm always begin at individual bytes Sennrich et al. (2016  ###reference_b28###); Zouhar et al. (2023  ###reference_b39###).\nMorphological analyzers, such as Morfessor, showed promising results for segmenting input texts for language models and neural machine translators Mach\u00e1cek et al. (2018  ###reference_b18###); Hou et al. (2023  ###reference_b12###). We are the first to apply morphology-based encoding for a massively multilingual setting."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Tokenization-free Language Modeling",
            "text": "An alternative to subword tokenization is representing texts directly as underlying encoding: characters or bytes.\nOr even representing texts as pixels of rendered text images Rust et al. (2023  ###reference_b27###).\nXue et al. (2022  ###reference_b34###) shows that for many non-Latin scripts, byte-level encoding performs worse than subword tokenization.\nThe problem with small units is that they do not carry meaningful information independently and often underperform subword models Sun et al. (2023  ###reference_b30###); Clark et al. (2022  ###reference_b7###).\nThe researchers have proposed multiple algorithms to enrich the byte-level embeddings with information from a local context.\nFor that purpose, recent approaches use shallow networks to aggregate information in local contexts defined as character n-grams Clark et al. (2022  ###reference_b7###), byte patches Yu et al. (2023  ###reference_b36###), or character blocks Tay et al. (2022  ###reference_b31###).\nHowever, the problem with choosing the appropriate context window is hard, because information density varies for different languages.\nA solution to that problem can be dynamically learning the segmentation in byte sequences Nawrot et al. (2023  ###reference_b20###).\nAnother approach is to redefine the encoding convention to equate the information loads in sequences, as the proposed MYTE approach."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduce MYTE encoding, a fairer byte-level representation for multilingual language modeling that is based on morphological segmentation. We show that adapting a morphological analyzer to unsupervised segmentation allows us to represent multi-parallel corpora with comparable encoding lengths across a wide range of languages. Additionally, our new representation significantly improves language modeling, especially of low-resource and non-Latin script languages, and provides efficiency benefits over traditional byte-level models. These trends hold across model sizes, with improvement increasing at scale.\nOverall, MYTE bridges the gap in encoding efficiency between high and low-resource languages, benefiting (to varying extent) all 99 analyzed languages."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Details of Unsupervised Morphological Analysis",
            "text": "In this appendix, we provide details on the prerequisites of MYTE transcoding algorithm: a) preparing multilingual lexicons and corpora for morphological analysis and b) usage of Morfessor unsupervised algorithm to obtain morpheme inventory for each language."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Supplementary Results",
            "text": "###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21### ###figure_22### ###figure_23### ###figure_24### This appendix summarizes complementary results referred to\nthroughout the papers."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Technical Details",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.24\">\n<tr class=\"ltx_tr\" id=\"S3.T1.24.25\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T1.24.25.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.24.25.1.1\">ID</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S3.T1.24.25.2\" rowspan=\"2\" style=\"width:39.7pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S3.T1.24.25.2.1\">Group</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S3.T1.24.25.3\" rowspan=\"2\" style=\"width:72.3pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S3.T1.24.25.3.1\">Unicode Script(s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T1.24.25.4\">Leading Byte</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.24.26\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.24.26.1\">2 b</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.24.26.2\">3 b</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.24.26.3\">4 b</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.3.3.4\">0</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.3.3.5\" style=\"width:39.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.3.5.1\">Latin</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S3.T1.3.3.6\" style=\"width:72.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.3.6.1\">Latin</p>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.1.1\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.1.1.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.pic1.1.1.1.1.1\">42</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.2.2\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.2.2.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"22.28\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.68)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 14.74 13.03 L 2.55 13.03 C 0.38 13.03 -1.38 11.27 -1.38 9.09 L -1.38 0.36 C -1.38 -1.81 0.38 -3.57 2.55 -3.57 L 14.74 -3.57 C 16.92 -3.57 18.68 -1.81 18.68 0.36 L 18.68 9.09 C 18.68 11.27 16.92 13.03 14.74 13.03 Z M -1.38 -3.57\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"9.46\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"17.3\"><span class=\"ltx_text\" id=\"S3.T1.2.2.2.pic1.1.1.1.1.1\">4A</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.3.3.3\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.3.3.3.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.3.3.3.pic1.1.1.1.1.1\">52</span></foreignobject></g></g></svg>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.6.6.4\">1</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.6.6.5\" style=\"width:39.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.6.6.5.1\">Common</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.6.6.6\" style=\"width:72.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.6.6.6.1\">Mixed, Common, Inherited, Unkown</p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.4.4.1\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.4.4.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.4.4.1.pic1.1.1.1.1.1\">43</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.5.5.2\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.5.5.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"21.7\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.68)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 14.17 13.03 L 2.55 13.03 C 0.38 13.03 -1.38 11.27 -1.38 9.09 L -1.38 0.36 C -1.38 -1.81 0.38 -3.57 2.55 -3.57 L 14.17 -3.57 C 16.34 -3.57 18.1 -1.81 18.1 0.36 L 18.1 9.09 C 18.1 11.27 16.34 13.03 14.17 13.03 Z M -1.38 -3.57\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"9.46\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"16.72\"><span class=\"ltx_text\" id=\"S3.T1.5.5.2.pic1.1.1.1.1.1\">4B</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.6.6.3\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.6.6.3.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.6.6.3.pic1.1.1.1.1.1\">53</span></foreignobject></g></g></svg>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.9.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.9.9.4\">2</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.9.9.5\" style=\"width:39.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.9.9.5.1\">Non-Latin Alphabetic</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.9.9.6\" style=\"width:72.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.9.9.6.1\">Greek, Cyrillic, Armenian, Georgian</p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.7.7.1\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.7.7.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.7.7.1.pic1.1.1.1.1.1\">44</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.8.8.2\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.8.8.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"21.89\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.68)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 14.36 13.03 L 2.55 13.03 C 0.38 13.03 -1.38 11.27 -1.38 9.09 L -1.38 0.36 C -1.38 -1.81 0.38 -3.57 2.55 -3.57 L 14.36 -3.57 C 16.53 -3.57 18.3 -1.81 18.3 0.36 L 18.3 9.09 C 18.3 11.27 16.53 13.03 14.36 13.03 Z M -1.38 -3.57\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"9.46\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"16.91\"><span class=\"ltx_text\" id=\"S3.T1.8.8.2.pic1.1.1.1.1.1\">4C</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.9.9.3\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.9.9.3.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.9.9.3.pic1.1.1.1.1.1\">54</span></foreignobject></g></g></svg>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.12.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.12.12.4\">3</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.12.12.5\" style=\"width:39.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.12.12.5.1\">Abjads</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.12.12.6\" style=\"width:72.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.12.12.6.1\">Hebrew, Arabic, Syriac, Thaana, Tifinagh</p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.10.10.1\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.10.10.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.10.10.1.pic1.1.1.1.1.1\">45</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.11.11.2\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.11.11.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"22.47\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.68)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 14.94 13.03 L 2.55 13.03 C 0.38 13.03 -1.38 11.27 -1.38 9.09 L -1.38 0.36 C -1.38 -1.81 0.38 -3.57 2.55 -3.57 L 14.94 -3.57 C 17.11 -3.57 18.87 -1.81 18.87 0.36 L 18.87 9.09 C 18.87 11.27 17.11 13.03 14.94 13.03 Z M -1.38 -3.57\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"9.46\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"17.49\"><span class=\"ltx_text\" id=\"S3.T1.11.11.2.pic1.1.1.1.1.1\">4D</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.12.12.3\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.12.12.3.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.12.12.3.pic1.1.1.1.1.1\">55</span></foreignobject></g></g></svg>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.15.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.15.15.4\">4</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.15.15.5\" style=\"width:39.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.15.15.5.1\">Abugidas North</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.15.15.6\" style=\"width:72.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.15.15.6.1\">Devanagari, Gurmukhi, Gujarati, Oriya, Bengali, Sinhala, Tibetan</p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.13.13.1\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.13.13.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.13.13.1.pic1.1.1.1.1.1\">46</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.14.14.2\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.14.14.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"21.32\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.68)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 13.78 13.03 L 2.55 13.03 C 0.38 13.03 -1.38 11.27 -1.38 9.09 L -1.38 0.36 C -1.38 -1.81 0.38 -3.57 2.55 -3.57 L 13.78 -3.57 C 15.96 -3.57 17.72 -1.81 17.72 0.36 L 17.72 9.09 C 17.72 11.27 15.96 13.03 13.78 13.03 Z M -1.38 -3.57\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"9.46\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"16.34\"><span class=\"ltx_text\" id=\"S3.T1.14.14.2.pic1.1.1.1.1.1\">4E</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.15.15.3\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.15.15.3.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.15.15.3.pic1.1.1.1.1.1\">56</span></foreignobject></g></g></svg>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.18.18\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.18.18.4\">5</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.18.18.5\" style=\"width:39.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.18.18.5.1\">Abugidas South</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.18.18.6\" style=\"width:72.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.18.18.6.1\">Telugu, Kannada, Tamil, Malayalam, Thai, Lao, Myanmar, Tai, Tagalog, Khmer</p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.16.16.1\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.16.16.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.16.16.1.pic1.1.1.1.1.1\">47</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.17.17.2\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.17.17.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"20.93\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.68)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 13.4 13.03 L 2.55 13.03 C 0.38 13.03 -1.38 11.27 -1.38 9.09 L -1.38 0.36 C -1.38 -1.81 0.38 -3.57 2.55 -3.57 L 13.4 -3.57 C 15.57 -3.57 17.33 -1.81 17.33 0.36 L 17.33 9.09 C 17.33 11.27 15.57 13.03 13.4 13.03 Z M -1.38 -3.57\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"9.46\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"15.95\"><span class=\"ltx_text\" id=\"S3.T1.17.17.2.pic1.1.1.1.1.1\">4F</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.18.18.3\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.18.18.3.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.18.18.3.pic1.1.1.1.1.1\">57</span></foreignobject></g></g></svg>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.21.21\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.21.21.4\">6</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.21.21.5\" style=\"width:39.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.21.21.5.1\">CJK</p>\n</td>\n<td class=\"ltx_td ltx_align_justify\" id=\"S3.T1.21.21.6\" style=\"width:72.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.21.21.6.1\">Hangul, Han, Yi, Katakana, Hiragana, Bopomofo</p>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.19.19.1\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.19.19.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.19.19.1.pic1.1.1.1.1.1\">48</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.20.20.2\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.20.20.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.20.20.2.pic1.1.1.1.1.1\">50</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.21.21.3\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.21.21.3.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.21.21.3.pic1.1.1.1.1.1\">58</span></foreignobject></g></g></svg>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.24.24\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.24.24.4\">7</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S3.T1.24.24.5\" style=\"width:39.7pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.24.24.5.1\">Other</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" id=\"S3.T1.24.24.6\" style=\"width:72.3pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.24.24.6.1\">Remaining scripts</p>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.22.22.1\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.22.22.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.22.22.1.pic1.1.1.1.1.1\">49</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.23.23.2\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.23.23.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.23.23.2.pic1.1.1.1.1.1\">51</span></foreignobject></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.24.24.3\"><svg class=\"ltx_picture\" height=\"18.82\" id=\"S3.T1.24.24.3.pic1\" overflow=\"visible\" version=\"1.1\" width=\"18.82\"><g transform=\"translate(0,18.82) matrix(1 0 0 -1 0 0) translate(2.49,0) translate(0,4.95)\"><g color=\"#000000\" fill=\"#000000\" stroke=\"#338871\" stroke-width=\"1.6pt\"><path d=\"M 11.28 12.76 L 2.55 12.76 C 0.38 12.76 -1.38 11 -1.38 8.82 L -1.38 0.09 C -1.38 -2.08 0.38 -3.84 2.55 -3.84 L 11.28 -3.84 C 13.46 -3.84 15.22 -2.08 15.22 0.09 L 15.22 8.82 C 15.22 11 13.46 12.76 11.28 12.76 Z M -1.38 -3.84\" style=\"fill:none\"></path></g><g color=\"#000000\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"1.6pt\" transform=\"matrix(1.0 0.0 0.0 1.0 0 0)\"><foreignobject height=\"8.92\" overflow=\"visible\" transform=\"matrix(1 0 0 -1 0 16.6)\" width=\"13.84\"><span class=\"ltx_text\" id=\"S3.T1.24.24.3.pic1.1.1.1.1.1\">59</span></foreignobject></g></g></svg>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Groups of scripts with the initial bytes for their morphological blocks. The groups were selected to balance the number of covered languages with similar writing systems.</figcaption>\n</figure>",
            "capture": "Table 1: Groups of scripts with the initial bytes for their morphological blocks. The groups were selected to balance the number of covered languages with similar writing systems."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.2\">\n<tr class=\"ltx_tr\" id=\"S4.T2.2.3\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.2.3.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T2.2.3.2\">Byte</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T2.2.3.3\">Myte</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.3.4\">Comp.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.4\">\n<td class=\"ltx_td\" id=\"S4.T2.2.4.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.4.2\">Parity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.4.3\">Len.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.4.4\">Parity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.4.5\">Len.</td>\n<td class=\"ltx_td\" id=\"S4.T2.2.4.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.2.5.1\">English</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.5.2\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.5.3\">131</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.5.4\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.5.5\">109</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.5.6\">16%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.2.6.1\">Latin HR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.6.2\">1.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.6.3\">149</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.6.4\">1.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.6.5\">129</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.6.6\">14%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.7.1\">Latin LR</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.2\">1.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.3\">147</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.4\">1.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.5\">128</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.7.6\">12%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1\">\nLatin HR</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.2\">1.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3\">212</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4\">1.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5\">141</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6\">29%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.2.1\">\nLatin LR</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2\">2.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.3\">305</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.4\">1.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.5\">145</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.6\">50%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.2.8.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.8.1.1\">Seen</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.8.2\">1.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.8.3\">204</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.8.4\">1.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.8.5\">135</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.2.8.6\">26%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.2.9.1\">Unseen Lang</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.9.2\">1.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.9.3\">196</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.9.4\">1.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.9.5\">138</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.2.9.6\">23%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.2.10.1\">Unseen Script</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.2\">2.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.3\">365</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.4\">3.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.5\">365</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.10.6\">0%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T2.2.11.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.2.11.1.1\">Unseen</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.2.11.2\">1.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.2.11.3\">224</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.2.11.4\">1.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.2.11.5\">176</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.2.11.6\">19%</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Averaged sequence length and corresponding parities to English of <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T2.7.1\">UTF-8</em> and <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.8.2\">MYTE</span>. We aggregated results for languages used in morphological adaptation (i.e., <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T2.9.3\">Seen</em>) by their script (Latin vs. Non-Latin) and resourcefulness (HR: high resource, LR: low resource) based on categorization from <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10691v1#bib.bib13\" title=\"\">2020</a>)</cite>. The last three rows present results for languages <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T2.10.4\">unseen</em> in morphological adaptation; all of them are low-resource. Shortened column headers: Len. \u2013 Length, Comp. \u2013 Compression.</figcaption>\n</figure>",
            "capture": "Table 2: Averaged sequence length and corresponding parities to English of UTF-8 and MYTE. We aggregated results for languages used in morphological adaptation (i.e., Seen) by their script (Latin vs. Non-Latin) and resourcefulness (HR: high resource, LR: low resource) based on categorization from Joshi et\u00a0al. (2020). The last three rows present results for languages unseen in morphological adaptation; all of them are low-resource. Shortened column headers: Len. \u2013 Length, Comp. \u2013 Compression."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T3.1\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S5.T3.1.1.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S5.T3.1.1.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S5.T3.1.1.3\">Byt5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S5.T3.1.1.4\">Myt5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.2\">\n<td class=\"ltx_td\" id=\"S5.T3.1.2.1\"></td>\n<td class=\"ltx_td\" id=\"S5.T3.1.2.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.3\">BPEB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.4\">T (ms)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.5\">BPEB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.6\">T (ms)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.3.1\">small</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.3.2\">All</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.3\">10.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.4\">7.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.5\">4.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.6\">6.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.4\">\n<td class=\"ltx_td\" id=\"S5.T3.1.4.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.4.2\">Latin</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3\">4.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.4\">5.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.5\">4.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.6\">6.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.5\">\n<td class=\"ltx_td\" id=\"S5.T3.1.5.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.5.2\">Non Latin</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.3\">18.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4\">8.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.5\">5.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.6\">6.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.6.1\">base</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.6.2\">All</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.6.3\">8.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.6.4\">11.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.6.5\">5.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.6.6\">8.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.7\">\n<td class=\"ltx_td\" id=\"S5.T3.1.7.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.7.2\">Latin</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.3\">4.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.4\">9.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.5\">5.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.6\">8.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.8\">\n<td class=\"ltx_td\" id=\"S5.T3.1.8.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.8.2\">Non Latin</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.3\">13.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.4\">14.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.5\">6.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.6\">9.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.9.1\">large</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.9.2\">All</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.9.3\">13.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.9.4\">31.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.9.5\">4.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.9.6\">26.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.10\">\n<td class=\"ltx_td\" id=\"S5.T3.1.10.1\"></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.10.2\">Latin</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.10.3\">10.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.10.4\">28.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.10.5\">4.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.10.6\">26.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.11\">\n<td class=\"ltx_td ltx_border_bb\" id=\"S5.T3.1.11.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T3.1.11.2\">Non Latin</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.11.3\">18.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.11.4\">37.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.11.5\">5.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.11.6\">27.0</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Byte-per-English-Bits and Inference times (average per Flores 200 sentence) averaged for three language groupings. </figcaption>\n</figure>",
            "capture": "Table 3: Byte-per-English-Bits and Inference times (average per Flores 200 sentence) averaged for three language groupings. "
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T4.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T4.1.1.1.1\">Task</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.1.2\">QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.1.3\">NER</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T4.1.1.1.4\">\n<span class=\"ltx_text\" id=\"S5.T4.1.1.1.4.1\"></span> <span class=\"ltx_text\" id=\"S5.T4.1.1.1.4.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.1.1.1.4.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.1.4.2.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.1.4.2.1.1.1\">Semantic</span></span>\n<span class=\"ltx_tr\" id=\"S5.T4.1.1.1.4.2.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.1.4.2.1.2.1\">Parsing</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S5.T4.1.1.1.4.3\"></span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.1.5\">MT</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.2.2.1\">Metric</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.2.2\">F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.2.3\">F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.2.4\">EM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.2.5\">chrF</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.3.3.1\">Flan-PaLM*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.3.3.2\">22.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.3.3.3\">12.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.3.3.4\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.3.3.5\">\u2014</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.4.4.1\">mT5*</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.4.2\">59.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.4.3\">74.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.4.4\">21.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.4.5\">\u2014</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.5.5.1\">ByT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.5.5.2\">73.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.5.5.3\">81.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.5.5.4\">25.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.5.5.5\">20.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T4.1.6.6.1\">MyT5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.6.6.2\">75.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.6.6.3\">80.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.6.6.4\">19.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.6.6.5\">20.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" id=\"S5.T4.1.7.7.1\">Inference Time (ms)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T4.1.8.8.1\">ByT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.8.8.2\">36.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.8.8.3\">13.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.8.8.4\">13.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.8.8.5\">15.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T4.1.9.9.1\">MyT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.9.9.2\">35.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.9.9.3\">12.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.9.9.4\">12.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.9.9.5\">12.6</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>The average result of <span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T4.3.1\">Xtreme-Up</span> tasks across low-resource languages.\nThe baseline results of mT5 and Flan-PaLM (in-context-learning evaluation) are copied from: <cite class=\"ltx_cite ltx_citemacro_citet\">Ruder et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10691v1#bib.bib26\" title=\"\">2023</a>)</cite>.\nWe observed disparities between their reported and reimplemented ByT5 results, which are probably caused by the differences in fine-tuning setting.\nThe time is an average across evaluation examples, the inference was run on an A40 GPU core.\nThe results for all languages and fine-tuning details are in Appendix.</figcaption>\n</figure>",
            "capture": "Table 4: The average result of Xtreme-Up tasks across low-resource languages.\nThe baseline results of mT5 and Flan-PaLM (in-context-learning evaluation) are copied from: Ruder et\u00a0al. (2023).\nWe observed disparities between their reported and reimplemented ByT5 results, which are probably caused by the differences in fine-tuning setting.\nThe time is an average across evaluation examples, the inference was run on an A40 GPU core.\nThe results for all languages and fine-tuning details are in Appendix."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A2.T5.1\">\n<tr class=\"ltx_tr\" id=\"A2.T5.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A2.T5.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.2\">ar</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.3\">bn</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.4\">en</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.5\">fi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.6\">id</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.7\">ko</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.8\">ru</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.9\">sw</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.10\">te</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.11\">AVG LR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T5.1.1.12\">AVG</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T5.1.2.1\">ByT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.2\">81.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.3\">59.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.4\">76.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.5\">80.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.6\">77.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.7\">75.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.8\">75.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.9\">77.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.10\">78.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.11\">73.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.12\">75.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T5.1.3.1\">MyT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.2\">82.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.3\">67.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.4\">74.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.5\">80.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.6\">76.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.7\">74.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.8\">76.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.9\">74.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.10\">83.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.11\">75.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.3.12\">76.7</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>F1 scores for question answering (<span class=\"ltx_text ltx_font_smallcaps\" id=\"A2.T5.3.1\">Xtreme-Up</span> benchmark).</figcaption>\n</figure>",
            "capture": "Table 5: F1 scores for question answering (Xtreme-Up benchmark)."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A2.T6.1\" style=\"width:433.6pt;height:53.5pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-6.3pt,0.8pt) scale(0.971694843233933,0.971694843233933) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A2.T6.1.1\">\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A2.T6.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.2\">am</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.3\">bbj</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.4\">bm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.5\">ee</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.6\">ha</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.7\">ig</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.8\">lg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.9\">luo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.10\">mos</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.11\">ny</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.12\">pcm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.13\">rw</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.14\">sn</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.15\">sw</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.16\">tn</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.17\">tw</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.18\">wo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.19\">xh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.20\">yo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.21\">zu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.22\">AVG LR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T6.1.1.1.23\">AVG</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T6.1.1.2.1\">ByT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.2\">60.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.3\">72.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.4\">80.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.5\">88.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.6\">88.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.7\">84.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.8\">84.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.9\">77.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.10\">73.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.11\">89.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.12\">85.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.13\">76.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.14\">90.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.15\">88.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.16\">85.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.17\">77.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.18\">80.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.19\">83.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.20\">78.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.21\">85.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.22\">81.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.23\">81.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T6.1.1.3.1\">MyT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.2\">62.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.3\">68.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.4\">79.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.5\">87.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.6\">87.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.7\">83.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.8\">83.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.9\">75.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.10\">75.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.11\">88.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.12\">85.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.13\">77.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.14\">90.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.15\">88.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.16\">84.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.17\">77.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.18\">75.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.19\">82.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.20\">79.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.21\">83.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.22\">80.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T6.1.1.3.23\">80.8</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>F1 scores for named entity recognition (MasakhaNER test set <cite class=\"ltx_cite ltx_citemacro_citet\">Adelani et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10691v1#bib.bib1\" title=\"\">2022</a>)</cite> via <span class=\"ltx_text ltx_font_smallcaps\" id=\"A2.T6.3.1\">Xtreme-Up</span> benchmark)</figcaption>\n</figure>",
            "capture": "Table 6: F1 scores for named entity recognition (MasakhaNER test set Adelani et\u00a0al. (2022) via Xtreme-Up benchmark)"
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T7\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A2.T7.1\" style=\"width:433.6pt;height:54.7pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-1.3pt,0.2pt) scale(0.993905422976013,0.993905422976013) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A2.T7.1.1\">\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A2.T7.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.2\">am</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.3\">be</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.4\">bn</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.5\">de</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.6\">en</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.7\">es</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.8\">fi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.9\">fr</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.10\">ha</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.11\">hi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.12\">ja</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.13\">pt_br</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.14\">ru</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.15\">sw</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.16\">ta</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.17\">th</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.18\">tr</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.19\">yo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.20\">zu</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.21\">AVG LR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T7.1.1.1.22\">AVG</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.1.1.2.1\">ByT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.2\">18.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.3\">31.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.4\">30.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.5\">34.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.6\">35.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.7\">33.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.8\">30.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.9\">34.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.10\">25.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.11\">25.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.12\">31.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.13\">34.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.14\">35.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.15\">26.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.16\">26.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.17\">24.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.18\">32.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.19\">18.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.20\">22.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.21\">25.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.1.1.2.22\">29.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T7.1.1.3.1\">MyT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.2\">16.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.3\">26.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.4\">20.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.5\">31.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.6\">31.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.7\">28.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.8\">25.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.9\">28.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.10\">21.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.11\">18.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.12\">18.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.13\">30.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.14\">32.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.15\">21.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.16\">21.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.17\">19.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.18\">25.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.19\">13.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.20\">16.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.21\">19.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T7.1.1.3.22\">23.5</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Exact match score for semantic parsing (<span class=\"ltx_text ltx_font_smallcaps\" id=\"A2.T7.3.1\">Xtreme-Up</span> benchmark)</figcaption>\n</figure>",
            "capture": "Table 7: Exact match score for semantic parsing (Xtreme-Up benchmark)"
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T8\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A2.T8.1\" style=\"width:433.6pt;height:67.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(44.1pt,-6.9pt) scale(1.25535018885878,1.25535018885878) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A2.T8.1.1\">\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A2.T8.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.2\">am</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.3\">de</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.4\">el</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.5\">fr</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.6\">hy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.7\">ja</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.8\">kk</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.9\">ko</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.10\">mt</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.11\">pl</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.12\">ru</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.13\">sn</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.14\">ta</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.15\">te</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.16\">vi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.17\">AVG LR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T8.1.1.1.18\">AVG</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T8.1.1.2.1\">MyT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.2\">9.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.3\">31.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.4\">21.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.5\">36.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.6\">22.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.7\">9.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.8\">20.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.9\">7.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.10\">26.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.11\">25.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.12\">24.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.13\">27.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.14\">22.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.15\">18.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.16\">24.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.17\">21.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.2.18\">21.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A2.T8.1.1.3.1\">ByT5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.2\">8.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.3\">35.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.4\">22.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.5\">41.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.6\">22.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.7\">9.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.8\">20.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.9\">7.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.10\">34.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.11\">27.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.12\">26.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.13\">27.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.14\">21.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.15\">17.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.16\">23.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.17\">21.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T8.1.1.3.18\">23.1</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>ChrF scores for machine translation (Florers 200 test set <cite class=\"ltx_cite ltx_citemacro_citet\">Team et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10691v1#bib.bib32\" title=\"\">2022</a>)</cite> via <span class=\"ltx_text ltx_font_smallcaps\" id=\"A2.T8.3.1\">Xtreme-Up</span> benchmark).</figcaption>\n</figure>",
            "capture": "Table 8: ChrF scores for machine translation (Florers 200 test set Team et\u00a0al. (2022) via Xtreme-Up benchmark)."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.10691v1_figure_1.png",
            "caption": "Figure 1: The same phrase is spelled in three languages: English, Czech, and Telugu. UTF-8 byte encoding of the phrase is shown in blue, while MYTE in green underneath. MYTE achieves higher encoding compression, especially for texts using diacritics or non-Latin script."
        },
        "2": {
            "figure_path": "2403.10691v1_figure_2.png",
            "caption": "Figure 2: UTF-8 codepage (inspired by the visualizations from: en.wikipedia.org/wiki/UTF-8).\nEach row contains 16 bytes with the same leading hexadecimal digit.\nBytes in the range  C2C2 -  F4F4 are leading bytes. They mark the beginning of a multibyte code of the length shown in each cell.\nBytes in the range  8080 -  BFBF are continuation bytes, which follow a leading byte in multibyte codes. Bytes  FEFE and  FFFF are unused. Range  C2C2 -  F4F4 encodes Latin capital letters. In MYTE, these characters are decomposed to free space used to encode morphemes."
        },
        "3": {
            "figure_path": "2403.10691v1_figure_3.png",
            "caption": "Figure 3: Boxplot aggregating parity against English for three segmentation methods: MYTE, UTF-8, characters, and subword tokens from mT5 tokenizer Xue et al. (2021). Parities were computed on multi-parallel Flores 200 corpus."
        },
        "4": {
            "figure_path": "2403.10691v1_figure_4.png",
            "caption": "(a) UTF-8"
        },
        "5": {
            "figure_path": "2403.10691v1_figure_5.png",
            "caption": "(b) MYTE"
        },
        "6": {
            "figure_path": "2403.10691v1_figure_6.png",
            "caption": "(c) Sequence compression"
        },
        "7": {
            "figure_path": "2403.10691v1_figure_7.png",
            "caption": "(a) LM Performance (\u03c1S=\u22120.81subscript\ud835\udf0c\ud835\udc460.81\\rho_{S}=-0.81italic_\u03c1 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = - 0.81)"
        },
        "8": {
            "figure_path": "2403.10691v1_figure_8.png",
            "caption": "(b) Inference Time (\u03c1S=\u22120.77subscript\ud835\udf0c\ud835\udc460.77\\rho_{S}=-0.77italic_\u03c1 start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = - 0.77)"
        },
        "9": {
            "figure_path": "2403.10691v1_figure_9.png",
            "caption": "Figure 6: Sentence prediction superisal expressed as Bit-per-English-Byte on multi-parallel Flores 200 corpus. The comparison shows that under MyT5 model, surprisal factors are more equitable than in the standard ByT5 model."
        },
        "10": {
            "figure_path": "2403.10691v1_figure_10.png",
            "caption": "Figure 7: Avarage score on end tasks from Xtreme-Up end tasks on low-resource languages against the inference time.\nThe times were divided by the value for ByT5 model, which is always higher than MyT5 model.\nThe metrics and the absolute values of inference time are shown in Table 4."
        },
        "11": {
            "figure_path": "2403.10691v1_figure_11.png",
            "caption": "(a) UTF-8"
        },
        "12": {
            "figure_path": "2403.10691v1_figure_12.png",
            "caption": "(b) MYTE"
        },
        "13": {
            "figure_path": "2403.10691v1_figure_13.png",
            "caption": "(c) Sequence compression"
        },
        "14": {
            "figure_path": "2403.10691v1_figure_14.png",
            "caption": "(a) Original UTF-8"
        },
        "15": {
            "figure_path": "2403.10691v1_figure_15.png",
            "caption": "(b) MYTE (Same order as in a)"
        },
        "16": {
            "figure_path": "2403.10691v1_figure_16.png",
            "caption": "Figure 10: Sequence compression rates on Flores 200 of MYTE in comparison with the original UTF-8 encoding."
        },
        "17": {
            "figure_path": "2403.10691v1_figure_17.png",
            "caption": "(a) small"
        },
        "18": {
            "figure_path": "2403.10691v1_figure_18.png",
            "caption": "(b) base"
        }
    },
    "references": [
        {
            "1": {
                "title": "MasakhaNER 2.0: Africa-centric transfer learning for named entity recognition.",
                "author": "David Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael Beukman, Chester Palen-Michel, Constantine Lignos, Jesujoba Alabi, Shamsuddeen Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula, Rooweither Mabuya, Bonaventure F. P. Dossou, Blessing Sibanda, Happy Buzaaba, Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia Taylor, Fatoumata Kabore, Chris Chinenye Emezue, Anuoluwapo Aremu, Perez Ogayo, Catherine Gitau, Edwin Munkoh-Buabeng, Victoire Memdjokam Koagne, Allahsera Auguste Tapo, Tebogo Macucwa, Vukosi Marivate, Mboning Tchiaze Elvis, Tajuddeen Gwadabe, Tosin Adewumi, Orevaoghene Ahia, Joyce Nakatumba-Nabende, Neo Lerato Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa Oluwaseun Adeyemi, Gilles Quentin Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf, Tatiana Moteu, and Dietrich Klakow. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models.",
                "author": "Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, and Yulia Tsvetkov. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 9904\u20139923. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.emnlp-main.614"
            }
        },
        {
            "3": {
                "title": "A bit of a problem: Measurement disparities in dataset sizes across languages.",
                "author": "Catherine Arnett, Tyler A. Chang, and Benjamin K. Bergen. 2024.",
                "venue": null,
                "url": "http://arxiv.org/abs/2403.00686"
            }
        },
        {
            "4": {
                "title": "Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models.",
                "author": "Terra Blevins and Luke Zettlemoyer. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3563\u20133574, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.emnlp-main.233"
            }
        },
        {
            "5": {
                "title": "UniMax: Fairer and More Effective Language Sampling for Large-scale Multilingual Pretraining.",
                "author": "Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, and Noah Constant. 2023.",
                "venue": "In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.",
                "url": "https://openreview.net/pdf?id=kXwdL1cWOAi"
            }
        },
        {
            "6": {
                "title": "Improving Multilingual Models with Language-clustered Vocabularies.",
                "author": "Hyung Won Chung, Dan Garrette, Kiat Chuan Tan, and Jason Riesa. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 4536\u20134546. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/V1/2020.EMNLP-MAIN.367"
            }
        },
        {
            "7": {
                "title": "Canine: Pre-training an efficient tokenization-free encoder for language representation.",
                "author": "Jonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2022.",
                "venue": "Transactions of the Association for Computational Linguistics, 10:73\u201391.",
                "url": "https://doi.org/10.1162/tacl_a_00448"
            }
        },
        {
            "8": {
                "title": "Unsupervised cross-lingual representation learning at scale.",
                "author": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.747"
            }
        },
        {
            "9": {
                "title": "Are All Languages Equally Hard to Language-Model?",
                "author": "Ryan Cotterell, S. J. Mielke, Jason Eisner, and Brian Roark. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), pages 536\u2013541. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/V1/N18-2085"
            }
        },
        {
            "10": {
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1423"
            }
        },
        {
            "11": {
                "title": "Understanding Morphology.",
                "author": "M. Haspelmath and A.D. Sims. 2010.",
                "venue": "Understanding language series. Hodder Education.",
                "url": "https://books.google.cz/books?id=GJKHPwAACAAJ"
            }
        },
        {
            "12": {
                "title": "Effects of sub-word segmentation on performance of transformer language models.",
                "author": "Jue Hou, Anisia Katinskaia, Anh-Duc Vu, and Roman Yangarber. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 7413\u20137425. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.emnlp-main.459"
            }
        },
        {
            "13": {
                "title": "The state and fate of linguistic diversity and inclusion in the NLP world.",
                "author": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282\u20136293, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.560"
            }
        },
        {
            "14": {
                "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
                "author": "Taku Kudo and John Richardson. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D18-2012"
            }
        },
        {
            "15": {
                "title": "Word translation without parallel data.",
                "author": "Guillaume Lample, Alexis Conneau, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2018.",
                "venue": "In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.",
                "url": "https://openreview.net/forum?id=H196sainb"
            }
        },
        {
            "16": {
                "title": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models.",
                "author": "Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian Khabsa. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 13142\u201313152. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.emnlp-main.813"
            }
        },
        {
            "17": {
                "title": "Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages.",
                "author": "Tomasz Limisiewicz, Jir\u00ed Balhar, and David Marecek. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5661\u20135681. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/V1/2023.FINDINGS-ACL.350"
            }
        },
        {
            "18": {
                "title": "Morphological and Language-agnostic Word Segmentation for NMT.",
                "author": "Dominik Mach\u00e1cek, Jon\u00e1s Vidra, and Ondrej Bojar. 2018.",
                "venue": "In Text, Speech, and Dialogue - 21st International Conference, TSD 2018, Brno, Czech Republic, September 11-14, 2018, Proceedings, volume 11107 of Lecture Notes in Computer Science, pages 277\u2013284. Springer.",
                "url": "https://doi.org/10.1007/978-3-030-00794-2_30"
            }
        },
        {
            "19": {
                "title": "A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank.",
                "author": "Dan Malkin, Tomasz Limisiewicz, and Gabriel Stanovsky. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4903\u20134915, Seattle, United States. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.naacl-main.361"
            }
        },
        {
            "20": {
                "title": "Efficient Transformers with Dynamic Token Pooling.",
                "author": "Piotr Nawrot, Jan Chorowski, Adrian Lancucki, and Edoardo Maria Ponti. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 6403\u20136417. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/V1/2023.ACL-LONG.353"
            }
        },
        {
            "21": {
                "title": "Morphology Matters: A Multilingual Language Modeling Analysis.",
                "author": "Hyunji Hayley Park, Katherine J. Zhang, Coleman Haley, Kenneth Steimel, Han Liu, and Lane Schwartz. 2021.",
                "venue": "Trans. Assoc. Comput. Linguistics, 9:261\u2013276.",
                "url": "https://doi.org/10.1162/TACL_A_00365"
            }
        },
        {
            "22": {
                "title": "Language Model Tokenizers Introduce Unfairness Between Languages.",
                "author": "Aleksandar Petrov, Emanuele La Malfa, Philip H. S. Torr, and Adel Bibi. 2023.",
                "venue": "CoRR, abs/2305.15425.",
                "url": "https://doi.org/10.48550/ARXIV.2305.15425"
            }
        },
        {
            "23": {
                "title": "UNKs everywhere: Adapting multilingual language models to new scripts.",
                "author": "Jonas Pfeiffer, Ivan Vuli\u0107, Iryna Gurevych, and Sebastian Ruder. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10186\u201310203, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.800"
            }
        },
        {
            "24": {
                "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer.",
                "author": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.",
                "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
                "url": "http://jmlr.org/papers/v21/20-074.html"
            }
        },
        {
            "25": {
                "title": "Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$.",
                "author": "Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. 2022.",
                "venue": "ArXiv:2203.17189 [cs].",
                "url": "https://doi.org/10.48550/arXiv.2203.17189"
            }
        },
        {
            "26": {
                "title": "XTREME-UP: A User-centric Scarce-data Benchmark for Under-represented Languages.",
                "author": "Sebastian Ruder, Jonathan H. Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean Michel A. Sarr, Xinyi Wang, John Wieting, Nitish Gupta, Anna Katanova, Christo Kirov, Dana L. Dickinson, Brian Roark, Bidisha Samanta, Connie Tao, David Ifeoluwa Adelani, Vera Axelrod, Isaac Caswell, Colin Cherry, Dan Garrette, R. Reeve Ingle, Melvin Johnson, Dmitry Panteleev, and Partha Talukdar. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 1856\u20131884. Association for Computational Linguistics.",
                "url": "https://aclanthology.org/2023.findings-emnlp.125"
            }
        },
        {
            "27": {
                "title": "Language Modelling with Pixels.",
                "author": "Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond Elliott. 2023.",
                "venue": "In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.",
                "url": "https://openreview.net/pdf?id=FkSp8VW8RjH"
            }
        },
        {
            "28": {
                "title": "Neural Machine Translation of Rare Words with Subword Units.",
                "author": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.",
                "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.",
                "url": "https://doi.org/10.18653/V1/P16-1162"
            }
        },
        {
            "29": {
                "title": "Morfessor 2.0: Toolkit for statistical morphological segmentation.",
                "author": "Peter Smit, Sami Virpioja, Stig-Arne Gr\u00f6nroos, and Mikko Kurimo. 2014.",
                "venue": "In Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 21\u201324, Gothenburg, Sweden. Association for Computational Linguistics.",
                "url": "https://doi.org/10.3115/v1/E14-2006"
            }
        },
        {
            "30": {
                "title": "A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models.",
                "author": "Jimin Sun, Patrick Fernandes, Xinyi Wang, and Graham Neubig. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 1680\u20131690. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/V1/2023.FINDINGS-EACL.128"
            }
        },
        {
            "31": {
                "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization.",
                "author": "Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Prakash Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2022.",
                "venue": "In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
                "url": "https://openreview.net/forum?id=JtBRnrlOEFN"
            }
        },
        {
            "32": {
                "title": "No Language Left Behind: Scaling Human-centered Machine Translation.",
                "author": "NLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.",
                "venue": null,
                "url": null
            }
        },
        {
            "33": {
                "title": "The Unicode Standard.",
                "author": "The Unicode Consortium. 2011.",
                "venue": "Technical Report Version 6.0.0, Unicode Consortium, Mountain View, CA.",
                "url": "http://www.unicode.org/versions/Unicode6.0.0/"
            }
        },
        {
            "34": {
                "title": "ByT5: Towards a Token-free Future with Pre-trained Byte-to-byte Models.",
                "author": "Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022.",
                "venue": "Trans. Assoc. Comput. Linguistics, 10:291\u2013306.",
                "url": "https://doi.org/10.1162/TACL_A_00461"
            }
        },
        {
            "35": {
                "title": "mT5: A massively multilingual pre-trained text-to-text transformer.",
                "author": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.41"
            }
        },
        {
            "36": {
                "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers.",
                "author": "Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. 2023.",
                "venue": "CoRR, abs/2305.07185.",
                "url": "https://doi.org/10.48550/ARXIV.2305.07185"
            }
        },
        {
            "37": {
                "title": "Towards universal segmentations: UniSegments 1.0.",
                "author": "Zden\u011bk \u017dabokrtsk\u00fd, Niyati Bafna, Jan Bodn\u00e1r, Luk\u00e1\u0161 Kyj\u00e1nek, Emil Svoboda, Magda \u0160ev\u010d\u00edkov\u00e1, and Jon\u00e1\u0161 Vidra. 2022.",
                "venue": "In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1137\u20131149, Marseille, France. European Language Resources Association.",
                "url": "https://aclanthology.org/2022.lrec-1.122"
            }
        },
        {
            "38": {
                "title": "Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training.",
                "author": "Bo Zheng, Li Dong, Shaohan Huang, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, and Furu Wei. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 3203\u20133215. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/V1/2021.EMNLP-MAIN.257"
            }
        },
        {
            "39": {
                "title": "A Formal Perspective on Byte-pair Encoding.",
                "author": "Vil\u00e9m Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, and Ryan Cotterell. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 598\u2013614. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/V1/2023.FINDINGS-ACL.38"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.10691v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "6",
            "6.1",
            "6.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.10691v1",
        "paper_title": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling",
        "research_background": "### Motivation:\nThe paper addresses the unequal performance of multilingual language models across various languages, particularly focusing on low-resource languages. This issue is exacerbated by the way these models represent words during tokenization, which can lead to inaccurate representations or over-segmentation. This segmentation problem increases computational costs and unfairly disadvantages speakers of certain languages in terms of API usage and associated billing.\n\n### Research Problem:\nThe primary research problem this paper tackles is the efficient and equitable representation of text across different languages in multilingual language models. Specifically, the goal is to develop a method that reduces over-segmentation and ensures that the byte representations of texts\u2014especially from non-Latin scripts\u2014are both effective for modeling and efficient in terms of computational costs.\n\n### Relevant Prior Work:\n1. **Multilingual Language Models**:\n   - Devlin et al. (2019), Conneau et al. (2020), and Xue et al. (2021) represent major strides in multilingual language modeling but face challenges when representing low-resource languages.\n\n2. **Data Availability and Accurate Representation**:\n   - Malkin et al. (2022) discuss the challenges due to differences in data availability across languages.\n   - Pfeiffer et al. (2021) show that current multilingual models often fail in accurately representing texts in rare languages.\n\n3. **Over-Segmentation Issues**:\n   - Petrov et al. (2023) and Ahia et al. (2023) demonstrate that over-segmentation negatively impacts both model performance and inference cost, making the issue critical for low-resource and non-Latin script languages.\n\n4. **Byte-Level Models**:\n   - Xue et al. (2022) illustrate that byte-level models can achieve high coverage due to the wide support of encodings like UTF-8 for various world scripts.\n   - Arnett et al. (2024) point out that over-segmentation still exists at the byte level, which increases computational costs and reduces sample efficiency.\n\n5. **Morphological Segmentation**:\n   - The importance of using morphemes or morphs for text representation is discussed by Cotterell et al. (2018), suggesting these are more comparable across languages than characters.\n   - Smit et al. (2014) provide a foundation for unsupervised morphological segmentation algorithms.\n\nCombining these insights, the paper proposes a morphology-driven byte encoding approach to address the segmentation problem, aiming for more equitable and efficient multilingual language modeling.",
        "methodology": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling\n\n**Methodology:**\n\n**Background and Motivation:**\nThe UTF-8 convention generates longer byte sequences for specific languages due to particular developmental choices. This discrepancy leads to inequities in byte representation across different languages.\n\n**Introduction to MYTE:**\nTo address this, we propose a new encoding paradigm, MYTE (Morphology-Driven Byte Encoding), aiming to equitably assign byte codes of similar lengths to morphemes across various languages. Our approach is grounded in morphological analysis, focusing on morphemes, the smallest meaningful units independent of writing conventions, as noted by Haspelmath and Sims (2010).\n\n**Assumptions and Segmentation:**\nWe hypothesize that the number of morphemes in sentences with equivalent information content is more consistent across languages compared to the number of characters, bytes, or tokens. Consequently, our method enforces balanced segmentation granularity across languages.\n\n**Comparison with Other Methods:**\nAn alternative encoding strategy treats the union of multilingual morpheme inventories as a single extensive subword vocabulary. However, this approach requires a significantly larger vocabulary to cover morphemes across languages. For example, the proposed MYTE encoding supports up to 2,130,432 variable-length codepoints, which is more than commonly used subword vocabularies. In contrast, the large vocabulary XLM-V model allocates 1 million subwords, as reported by Liang et al. (2023). The larger vocabulary still incurs additional computational costs and, like other subword representations, may not generalize well to new, unseen languages.\n\n**Key Components and Innovations:**\n- **Balanced Byte Representation:** Ensures morphemes across languages have byte codes of similar lengths.\n- **Morphological Analysis:** Utilizes morphemes, which are independent of writing conventions, ensuring shorter, meaningful units.\n- **Segmentation Granularity:** Maintains balanced segmentation granularity regardless of the language, leading to more equitable representation.\n- **Extended Encoding Capacity:** MYTE supports up to 2,130,432 variable-length codepoints, providing a more inclusive encoding scheme compared to existing models.\n\n**Conclusion:**\nMYTE introduces an innovative and fairer encoding system by focusing on the morphological components of languages, providing a more balanced and computationally efficient encoding standard for multilingual language modeling.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Setup\n**Objective:**  \nTo analyze the properties of the proposed MYTE (Morphology-driven Byte Encoding) by measuring the segmentation quality across various languages.\n\n**Datasets:**  \nThe experiment utilizes the multi-parallel corpus Flores-200 Team et al. (2022).\n\n**Baselines:**  \n1. Vanilla byte-level encoding via UTF-8.\n2. Character-level encoding.\n3. Subword tokenization produced by Sentencepiece algorithm (Kudo and Richardson, 2018).\n\n**Evaluation Metrics:**  \n1. **Parity**: Sequence length comparison for parallel sentences in different languages relative to English.\n2. **Compression Rate**: Comparison of the compression efficiency of MYTE against UTF-8 encoding.\n\n#### Results\n1. **Parity in Sequence Lengths**:  \n   - MYTE shows more balanced sequence lengths across languages compared to UTF-8.  \n   - MYTE reduces disparity, with the worst-case scenario parity dropping from 3.5 in UTF-8 to 1.7 in MYTE for outlier languages (e.g., Greek, Vietnamese, Punjabi, Khmer).  \n   - MYTE representations achieved parity scores closer to those of character-level systems but with significantly fewer unique byte code points (149,878 characters vs. 254 bytes).\n\n2. **Compression Rate**:  \n   - MYTE encoding reduces the encoded sequence lengths across all languages.  \n   - The compression rate varies significantly\u2014from 1% for Vietnamese and Chinese to nearly 70% for Burmese, benefiting low-resource, non-Latin script languages the most.  \n   - MYTE improves over-segmentation issues in UTF-8 encoding and even performs well for unseen low-resource languages. One exception is Santhali, written in the Ol Chiki script.\n\n#### Conclusion\nThe myTE encoding method significantly balances sequence lengths across languages, effectively reducing over-segmentation for low-resource and non-Latin scripts while ensuring a substantial compression rate. This validates MYTE as an efficient and equitable byte encoding system for multilingual language modeling, demonstrating its potential to cover additional languages without exhausting its available byte code points. Complete results for unseen languages and scripts are detailed in Appendix B."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The study aims to analyze whether the proposed morphology-driven encoding (MYTE) produces segmented sequences of comparable length across diverse languages, addressing the disparities in segmentation for underrepresented languages.",
            "experiment_process": "The experiment uses the multi-parallel corpus Flores 200 to measure parity across languages, defined by the length of segmented sequences for parallel sentences in each language compared to English. Several baselines, including vanilla byte-level encoding (UTF-8), character-level encoding, and subwords produced by the Sentencepiece algorithm, are compared against MYTE. The parity and sequence length compression rates are evaluated to determine the effectiveness of MYTE, especially for non-European languages and non-Latin scripts.",
            "result_discussion": "MYTE produced more balanced sequence lengths across languages compared to the original UTF-8 bytes, with notable improvements in parity seen for Greek, Vietnamese, Punjabi, and Khmer. It also achieved higher compression rates for non-Latin scripts, reducing over-segmentation observed in UTF-8 encoding. Despite some exceptions like Santhali, MYTE did not exhaust the available byte codepoints, indicating the potential for further extension to additional languages.",
            "ablation_id": "2403.10691v1.No1"
        },
        {
            "research_objective": "To investigate the benefits of MYTE as an encoding scheme for byte-level language modeling and its impact on various NLP tasks compared to the ByT5 model.",
            "experiment_process": "The study introduces MyT5 models, which are based on T5 models but use MYTE representation. Three different sizes of MyT5 models (small, base, large) were pre-trained on the mC4 corpus for 250,000 steps. The performance is evaluated across language modeling, efficiency, and downstream tasks, comparing MyT5 against ByT5 using the Bit-per-English-Byte metric on the FLORES 200 corpus. Tasks include question answering, named entity recognition, semantic parsing, and translation from English.",
            "result_discussion": "MyT5 outperformed ByT5 in language modeling across all languages, particularly excelling in languages using Abugidas scripts due to higher compression rates. Although some improvements were observed for Latin and CJK scripts, the largest gains were in languages prone to over-segmentation under UTF-8. MyT5 also demonstrated faster inference times, especially for non-Latin scripts, with efficiency gains improving with model size. In end tasks, MyT5 performed comparably (and sometimes better) to ByT5, except in semantic parsing where the morphological prior in MYTE may have confounded fine-tuning.",
            "ablation_id": "2403.10691v1.No2"
        }
    ]
}