{
    "title": "Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval",
    "abstract": "In today\u2019s digital world, seeking answers to health questions on the Internet is a common practice. However, existing question answering (QA) systems often rely on using pre-selected and annotated evidence documents, thus making them inadequate for addressing novel questions. Our study focuses on the open-domain QA setting, where the key challenge is to first uncover relevant evidence in large knowledge bases. By utilizing the common retrieve-then-read QA pipeline and PubMed as a trustworthy collection of medical research documents, we answer health questions from three diverse datasets. We modify different retrieval settings to observe their influence on the QA pipeline\u2019s performance, including the number of retrieved documents, sentence selection process, the publication year of articles, and their number of citations. Our results reveal that cutting down on the amount of retrieved documents and favoring more recent and highly cited documents can improve the final macro F1 score up to 10%. We discuss the results, highlight interesting examples, and outline challenges for future research, like managing evidence disagreement and crafting user-friendly explanations.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the digital era, using the Internet to search for health information has become a prevalent behavior Jia et al. (2021  ###reference_b21###). Users turn to seek health advice online due to its ease of access, wide coverage of information, convenience of searching, interactivity, and anonymity Neely et al. (2021  ###reference_b32###). Health information sought online includes anything regarding the symptoms and treatments of different diseases. In general, health information seeking can lead to enhanced patient involvement in medical decision-making, improved communication with care providers, and improved quality of life Rutten et al. (2019  ###reference_b39###). Nevertheless, finding trustworthy and relevant evidence in abundant online content remains an open challenge Battineni et al. (2020  ###reference_b7###). Especially in the medical field, clinical recommendations can change with time, so finding the latest evidence is essential for reliable answers.\n###figure_1### Interacting with online search engines and conversational systems is done with question answering (QA). Technical solutions based on Machine Learning (ML) and Natural Language Processing (NLP) aim to automate this task and provide users with reliable answers to their inquiries. The purpose of QA systems is multi-fold: it helps scientists verify their research hypothesis by finding related studies, it allows lay users to find answers to their everyday health concerns, and enables factuality assessment of generative language models by fact-checking their responses over trusted evidence Jin et al. (2021  ###reference_b22###); Vladika and Matthes (2023a  ###reference_b47###).\nWhile QA can work with answering questions over a provided document, we are focusing on the more realistic and challenging problem of open-domain question answering, where extensive collections of documents with diverse topics have to be quried to find the relevant evidence Chen and Yih (2020a  ###reference_b11###). The open-domain QA systems usually consist of two main components, a retriever and a reader Zhu et al. (2021b  ###reference_b56###). The retriever\u2019s task is finding the relevant documents that will serve as the main source of evidence for answering the question. The reader (QA module) performs the reasoning process between the question and found evidence, and produces the final answer. While both components are essential for the system, we posit that the retrieval is a more challenging part, considering that the QA module receives the input from it and the quality of the final answer depends on the retrieved documents Sauchuk et al. (2022  ###reference_b40###).\nRetrieving credible evidence documents relevant to the query ensures the final output\u2019s quality. This is true for both the retrieval-powered text classification tasks and recently popular retrieval-augmented generation (RAG) approaches Cuconasu et al. (2024  ###reference_b15###). While much progress has been made in open-domain QA, addressing the challenges in retrieval settings still needs to be explored. These include assessing the quantity of documents needed to be retrieved for a reliable answer, the amount of evidence passages extracted from them, and the quality of the documents themselves, such as their recency and strength of findings. Figure 1  ###reference_### shows an example of a health question answered with two different retrieved documents \u2013 the more recent one has more up-to-date knowledge and findings.\nTo bridge this research gap, in this study, we perform an array of experiments to test the predictive performance of an open-domain QA system with different evidence retrieval strategies. We use three diverse datasets of biomedical and health questions, which contain discreet labels like yes and no as their final answers, and use their gold labels as ground truth. We use the large collection of 20 million biomedical research abstracts from PubMed as the knowledge base for evidence retrieval. By keeping the reader (answering module) fixed, we only vary the different retrieval aspects and measure the change in the QA performance as measured by classification metrics precision, recall, and F1. The settings we explore include the number of documents retrieved and sentences extracted from them, the articles\u2019 publication year, and their number of citations. Our findings demonstrate that the QA performance is improved by accounting for the amount and quality of the retrieved documents.\nTo summarize, our research contributions are:\nWe evaluate the performance of an open-domain QA pipeline for health questions, using biomedical research papers as evidence source, concerning the different number of documents retrieved and sentences extracted from them.\nAdditionally, we evaluate the influence of the evidence quality parameters like year of publication and number of citations on the final predictive performance of the QA system, showing that time-aware evidence retrieval leads to improved performance.\nFinally, we take a deeper look into the results and provide insights with a qualitative analysis. We report on the problem of evidence disagreement and provide future directions on developing better health question answering systems to be deployed in the future.\nWe make our code and datasets publicly available in a GitHub repository.111https://github.com/jvladika/Improving-Health-QA  ###reference_th-QA###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "In this section, we outline the work related to our study."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Biomedical Question Answering",
            "text": "Question Answering (QA) is a rapidly evolving knowledge-intensive NLP task, with over 80 QA datasets released in last two years Rogers et al. (2023  ###reference_b38###). Based on the availability of evidence for the question, it can be analyzed in a closed-domain or an open-domain setting. In closed-domain QA, the evidence comes from an already provided source document. This setting is also called Machine Reading Comprehension (MRC) since the goal is to build models that can comprehend from the given text how to answer the posed question Baradaran et al. (2022  ###reference_b6###). In open-domain QA, to which our work belongs, only the question and its final answer are known, and the QA system has to find appropriate evidence in a large document corpus or other type of collection Chen and Yih (2020b  ###reference_b12###).\nBased on the topic of questions, our work is related to the research on science question answering, aiming to answer questions related to natural sciences from resources like school curricula Lu et al. (2022  ###reference_b31###) or scholarly articles Lee et al. (2023  ###reference_b28###). More precisely, our work is part of biomedical question answering Jin et al. (2022  ###reference_b24###).\nBiomedical QA can help biomedical researchers conduct their work by answering complex research questions Jin et al. (2019  ###reference_b23###), help clinical practitioners by answering clinical questions over health records Vilares and G\u00f3mez-Rodr\u00edguez (2019  ###reference_b46###), or help consumers answer questions about their health concerns Demner-Fushman et al. (2020  ###reference_b16###). The last category, also called health question answering, is increasingly being adopted by consumers due to the rising popularity of conversational assistants Budler et al. (2023  ###reference_b9###). In our work, we cover both the datasets related to QA for biomedical research and consumer health."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Open-Domain Fact Verification and QA",
            "text": "Considering that the datasets we use only contain questions with discrete (yes/no) answers, our work is related to the task of automated fact verification (fact-checking). This task aims to verify the veracity of a factual claim based on credible evidence that supports it, refutes it, or does not provide enough information Guo et al. (2022  ###reference_b19###). Recent years have seen a rise in fact-checking datasets focusing on scientific knowledge, in particular health and medicine Vladika and Matthes (2023a  ###reference_b47###).\nWhile fact verification literature often works in a closed-domain setting with evidence documents provided, some recent work also explores the open-domain setting. Wadden et al. (2022  ###reference_b52###) observe significant performance drops in F1 scores of final verdict predictions when increasing the evidence corpus from a few thousand to half a million documents. Pugachev et al. (2023  ###reference_b37###) analyze how well consumer-health questions can be answered with built-in search engines of PubMed and Wikipedia. Expanding the scope even more, Vladika and Matthes (2024  ###reference_b49###) compare the performance of semantic search and BM25 over PubMed and Wikipedia, as well as Google search, for verifying biomedical and health questions.\nSome studies have analyzed the influence of time and quantity in evidence retrieval on downstream tasks. Allein et al. (2021  ###reference_b2###) trained time-aware evidence ranking models for time-sensitive news claims and show performance improvement. Likewise, Schlichtkrull et al. (2024  ###reference_b41###) constructed a dataset where evidence for given claims only appeared after the claim itself, thus eliminating temporal leaks. Regarding the document quantity, Oh and Thorne (2023  ###reference_b34###) analyze the influence of the number of retrieved evidence passages on the QA performance over two general QA datasets, showing that the performance often actually drops with the increasing number of retrieved snippets.\nTo the best of our knowledge, our paper uses the largest document collection so far for open-domain health QA by indexing the entire PubMed corpus of more than 20 million articles. Likewise, it is also the first study to test the influence of the number of documents retrieved on the final QA performance instead of fixing it to a certain number, like the commonly found 5 Thorne et al. (2018  ###reference_b45###) or 6 Wadden et al. (2022  ###reference_b52###); as well as testing the influence of the different number of sentences retrieved. While there has been existing research on time-aware evidence ranking in fact verification for news claims, our work is first to explore the temporal aspect for biomedical questions, as well as other evidence quality aspects like the number of citations of retrieved publications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Foundations",
            "text": "In this section, we explain the foundations of the study, including the used datasets, the evidence corpus, and the structure of the used QA system."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "We chose three datasets of biomedical and health claims in English, built for different purposes. We only use the questions and final labels (answers) from the datasets in our experiments. While the datasets provide the gold evidence passages used to derive the answers, we do not utilize them since the idea of our open-domain QA setting is that the retriever component has to discover the relevant evidence in a large document corpus.\nHealthFC Vladika et al. (2023  ###reference_b50###) is a question-answering and fact-checking dataset focusing on consumer health questions and common topics users search health advice online for. It includes diverse topics like dietary supplements, heart and lungs, reproductive health, cancer, and mental health. Medical practitioners manually answered and verified all the questions using the evidence from systematic reviews and clinical trials. There are 750 questions in total, out of which 205 are supported, 122 are refuted, and for 433 questions, there is not enough information (NEI) to answer. We use two variants of the dataset: HealthFC-3, which has all 750 claims and all three classes; and HealthFC-2, which only has 327 supported and refuted claims with two classes.\nBioASQ-7b Nentidis et al. (2020  ###reference_b33###) is a biomedical question-answering dataset constructed by biomedical researchers and designed to reflect the real information needs of biomedical experts. It is part of the ongoing challenge of the same name, focusing on biomedical semantic indexing and question answering. The evidence for answers comes from biomedical research publications, i.e., the same corpus of PubMed used in our study. Other than only exact answers, the BioASQ dataset also includes ideal answer summaries. The 7b version of the dataset we use has 745 claims, of which 614 are supported (\"yes\"), and 131 are refuted (\"no\").\nTREC-Health (Pugachev et al., 2023  ###reference_b37###) is a dataset of 117 popular health questions originating from two TREC shared challenges. TREC is an ongoing series of workshops centering on challenges in accurate information retrieval Voorhees et al. (2005  ###reference_b51###). The questions stem from two shared tasks: the TREC 2019 Decision Track Abualsaud et al. (2020  ###reference_b1###) and the TREC 2021 Health Misinformation Track Clarke et al. (2021  ###reference_b13###), both focusing on challenges with incorrect search engine results for health (mis)information. Questions cover common consumer health concerns, similar to HealthFC, but the two datasets do not overlap. The dataset consists of 113 questions, of which 61 are supported (\"yes\") and 52 refuted (\"no\").\nTable 1  ###reference_### gives an overview of the four datasets. With HealthFC and TREC-Health, we aim to target common health questions users would pose to a QA system, while BioASQ is selected in order to explore how do the QA results change for more complex, expert-geared biomedical questions.\neveryday health\neveryday health\nbiomedical research\nconsumer health"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evidence Corpus",
            "text": "We approach the QA task in the open-domain setting, meaning that evidence is unknown when the question is posed and must first be discovered in a vast evidence collection. Given that we work with medical and health-related questions, we chose a collection of biomedical research publications as the source of evidence.\nOur evidence corpus originates from PubMed, a large and trustworthy knowledge base of biomedical research publications Canese and Weis (2013  ###reference_b10###). Considering that the full text of most of these publications is not freely accessible, we use only the abstracts, which are always available. This does not hinder the performance since medical abstracts often already include a verdict on their main research hypothesis. The US National Library of Medicine provides every year MEDLINE, a snapshot of currently available abstracts in PubMed that is updated once a year. We used the 2022 version found on the official website.222https://www.nlm.nih.gov/databases/download/pubmed_medline.html  ###reference_/pubmed_medline.html### While this yields 33.4M abstracts, we pre-processed the data by removing any non-English papers, papers with no abstracts, and papers with unfinished abstracts, which yields 20.6 million abstracts."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "QA System",
            "text": "The question-answering system used for our experiments is in the form of a pipeline, based on the pipeline system from Vladika and Matthes (2023b  ###reference_b48###). This pipeline consists of two main parts: a retriever and a reader. The process of question answering is done sequentially, by first retrieving the evidence, performing reasoning over it, and finally producing a final answer. Our experiments focus on changing the different aspects of the retriever while keeping the reader completely fixed. This ensures that the experimental setup is consistent and that only one parameter is tested at a time.\nIn the retriever, given a question  and a corpus of  documents , the goal is to select the top  most relevant documents  for the given query. The selection is done with a function , which compares the similarity of the question (query) and each document in the corpus. The documents in our corpus are abstracts of medical publications. While abstracts are shorter versions of full documents, they can still contain irrelevant sentences for producing the final verdict.\nIn our first experiment, we use full documents  and the question  to predict the final answer. In the second experiment, we select only the top  most relevant sentences from the abstracts.\nFrom  candidate sentences  comprising the selected documents, top  sentences are selected as evidence sentences  with a function . These sentences are the most similar to the question .\nFinally, the answer is predicted from the given question  and evidence , where the evidence is either the complete documents (the first round of experiments) or a set of sentences (the second round of experiments). The reader model produces the final verdict and is one of three classes , , . While QA can be generative and elicit long answers, all datasets we use contain only the short yes/no/unknown answers. This makes using the standard classification metrics precision, recall, and F1 possible. We model answer prediction as the related task of recognizing entailment or natural language inference (NLI) and use an NLI model for the prediction.\nIn all experiments, majority voting is used to determine the final verdict. For the dataset HealthFC-3 with three classes, this can be one of the three classes (0, 1, 2). For other datasets, the majority is taken only from predictions of 0 and 1 (in case of a tie, 0 is predicted). Majority voting is chosen for convenience, but it is not optimal as the information on prediction disagreement. Future work should explore how to model the disagreement better.\nFor the  function that selects top k most relevant documents, we use BM25 as it was proven to be a strong baseline for retrieving documents for automated claim verification Stammbach et al. (2023  ###reference_b44###). It also ensures higher precision at the cost of coverage, which aligns with our use case \u2013 we want the retrieved documents to be relevant before being passed to the reader. We use a sentence embedding model and semantic search to select the sentences most similar to our query from abstracts. For , we select the model Spiced Wright et al. (2022  ###reference_b54###), which is a recent sentence similarity model optimized for paraphrases of scientific claims. For the final answer prediction model (reader) , we choose the DeBERTa-v3 model He et al. (2021  ###reference_b20###) since it was shown to be a highly potent model for natural language understanding and reasoning tasks. We use the variant of the model optimized for NLI prediction Laurer et al. (2024  ###reference_b27###). We do not fine-tune the models on the datasets in our experiments because we want to simulate a realistic QA system that has to answer unseen questions."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We conduct three groups of experiments to test the influence of different retrieval parameters on the performance of our QA system.\nThe first group of experiments consisted of keeping the QA pipeline consistent but increasing the number of retrieved documents (top k) that are forwarded to the final QA module. The motivation behind this was to find the fine balance between covering enough different studies but not saturating the module with noise and irrelevant articles. Considering the increasing popularity of retrieval-enhanced systems such as retrieval-augmented generation (RAG) pipelines Lewis et al. (2020  ###reference_b30###), retrieving only the relevant amount of documents or chunks is a significant challenge. We use BM25 as the retrieval technique because of its efficiency and its focus on enhancing precision instead of recall.\nInstead of taking the whole documents and sending them with the question to the reader, the second group of experiments selected only the top j most relevant sentences within all abstracts and used those as evidence. In this setup, we first retrieve the top 20 most similar abstracts with BM25. Afterward, all abstracts are split into sentences, which are embedded with the sentence-embedding model SPICED. After that, the top j most similar sentences to the question q, according to cosine similarity, are chosen from the pool of sentences (so multiple sentences can come from the same abstract). The QA module calculates the entailment probability between the question and each selected sentence, and finally, majority voting is performed.\nThe third and fourth group of experiments focused less on the technical parameters of the retrieval but more on the quality of the discovered evidence. So far, not many studies have leveraged the metadata of retrieved evidence documents for enhancing medical and health-related question answering or fact verification. We use two metadata parameters that should intuitively have an influence on the quality of the performance \u2013 year of publication of the retrieved research publication and the number of citations it has. The year was provided among the metadata that comes with PubMed, but getting the number of citations was more challenging, considering it is not foundin the MEDLINE dump. Therefore, we utilized the Semantic Scholar API Ammar et al. (2018  ###reference_b3###) by querying it with the PubMed ID (PMID) of the retrieved article and then calling the API to get the number of citations. Once we had both numbers, experiments consisted of filtering the pool of possible evidence documents by posing a restriction on the minimum year of publication and the minimum number of citations. Out of the top k documents we retrieved, only those published after a certain year, or those with at least a certain number of citations, were selected as the final evidence documents. The rest of the workflow is the same as in the first experiment: the documents are passed to the QA module and the answer is predicted.\nGiven that we work with discrete answers and labels, the evaluation metrics we used are macro-averaged versions of classification metrics precision, recall, and F1 score. Macro averaging implies that the arithmetic mean of the metric for each individual class is taken (e.g., ). For HealthFC-3, this is an average of three classes, while for the other datasets, it is an average of two classes. The motivation behind using the macro version is that it considers all classes equally important. We posit that in a deployed health QA system, users would be interested not only in detecting positive answers, but the system effectively discerning between both negative and positive answers to questions. This also follows the literature on automated fact-checking, which commonly uses macro-averaged scores Bekoulis et al. (2021  ###reference_b8###).\nAll the experiments were run on a single Nvidia V100 GPU with 16 GB of VRAM. The process of retrieving the top 100 most relevant documents for each dataset used one computation hour. The process of predicting the final answer with the top 100 most relevant documents also used one computation hour."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "In this section, we present and describe the results of the conducted experiments. Table 2  ###reference_### shows the final classification scores when changing the number of documents retrieved. Similarly, Table 3  ###reference_### shows the final performance for different numbers of top sentences extracted. Tables 4  ###reference_### and 5  ###reference_### show the influence of filtering evidence documents based on their year of publication and number of citations."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Evidence Quality",
            "text": "Table 4  ###reference_### tested the influence of the recency of the paper (year of publication) on the predictive performance. Once again, an interesting trend can be noted. The more recent the selected documents were, the more accurate the answers to health questions in our system. A similar phenomenon can be observed in Table 5  ###reference_### for the number of citations of the papers. When limiting the selection to more and more cited papers, the final score kept increasing. Nevertheless, the highest scores in Tables 3 and 4 are still lower than the top-1-document performance from Table 4  ###reference_###.\nIt intuitively makes sense that the more recent papers will provide the latest knowledge and insights into a research hypothesis, which then also better aligns with the gold labels of our datasets. This is also slightly biased by the recent creation date of our datasets, where annotators had access to the most recent knowledge. Likewise, the better the reputation of a paper (more citations), it could be assumed it will be a more reliable indicator of a correct answer. This wasn\u2019t the case for all of the questions, and there were many examples where older or less cited papers aligned better with the gold labels. Still, Table 6  ###reference_### provides some statistics that show that the trend generally holds. On average, those documents that produced a correct verdict were around three years more recent (in both the mean and median) while having an average of 10 citations more (median four citations). On the other hand, all categories have considerable standard deviations, showing many outliers and exceptions to this rule.\nAnother challenging factor is that there could seemingly be an inverse correlation between the age of a paper and its number of citations \u2013 older publications have had more time to gather a bigger number of citations. After deeper inspection, we observed that being cited a lot over time is only true for high-quality studies. Overall, striking a balance by finding both those studies that are both recent and already highly cited is an optimal strategy.\nCan stress promote dementia? (Supported)\nTo test the hypothesis that high job stress during working life might lead to an increased risk of dementia and Alzheimer\u2019s disease (AD) in late life. (\u2026) Lifelong work-related psychosocial stress, characterized by low job control and high job strain, was associated with increased risk of dementia and AD in late life, independent of other known risk factors. Wang et al. (2020  ###reference_b53###) (Supported) [121 citations]\nPatients with Alzheimer\u2019s disease (AD) or dementia are increasing in numbers as the population worldwide ages. Mid-life psychological stress, psychosocial stress and post-traumatic stress disorder have been shown to cause cognitive dysfunction. The mechanisms behind stress-induced AD or dementia are not known. Zhu et al. (2021a  ###reference_b55###) (Refuted) [3 citations]\nCan ginkgo biloba relieve the symptoms of tinnitus? (Refuted)\nGinkgo biloba is a plant extract used to alleviate symptoms associated with cognitive deficits, e.g., decreased memory performance, lack of concentration, decreased alertness, tinnitus, and dizziness. Pharmacologic studies have shown that the therapeutic effect of ginkgo(\u2026) S\u00f8holm (1998  ###reference_b43###) (Supported) [Year: 1998]\nWe identified three systematic reviews including four primary studies, all corresponding to randomized trials. We concluded the use of Ginkgo biloba probably does not decrease the severity of tinnitus. In addition, it does not reduce the intensity of tinnitus or improve the quality of life of patients. Kramer and Ortigoza (2018  ###reference_b25###) (Refuted) [Year: 2018]"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this section, we discuss and provide deeper insights uncovered in the results."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Qualitative Analysis",
            "text": "Specific effects of retrieving multiple documents for open-domain health question answering are shown in Table 7  ###reference_###. For the first question, just retrieving the best document would have led to a study that does not provide a definitive conclusion to the question. However, the second and third most similar documents that were retrieved support the given research hypothesis. This held true even for further documents that cannot be shown in the table because of space constraints. On the other hand, the second question would have been more appropriately assessed by just looking at the best document instead of the top 3 documents. In fact, the second and third documents do not explicitly talk about the given question but rather a variation.\nExamples of positive influences of taking into account the qualitative properties of the evidence into account, namely the year of publication and number of citations, are shown in Table 8  ###reference_###. For the first question, a publication with above 100 citations directly answers the question and matches the gold annotation for this claim from the dataset. On the other hand, the second most similar retrieved document was a study with only three citations, which seems to be inconclusive about or even refuting the research hypothesis. Similarly, the answer to the second question on the effects of ginkgo is skewed by the top document from the 1990s that talks about the presumed positive effects of ginkgo. Two decades later, another meta-review analysis of systematic reviews on ginkgo biloba showed that it was not proven to help with tinnitus. This nicely demonstrates the changing nature of scientific knowledge and scientific consensus throughout time."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Future Directions",
            "text": "Based on our findings and discussion, we see that future work could focus on these directions:\nStrength of evidence. Taking into account the year of publication and number of citations has proven to be an effective strategy for enhancing the health question-answering performance. Similarly, further metadata could be taken into account to augment the process. In medical research, the strength of evidence is an important factor, and systems like GRADE are used to assess it Balshem et al. (2011  ###reference_b5###). Different types of studies, such as a single study, a randomized clinical trial, and a systematic review, all have different strengths. Including this could be an important factor in improving the reliability of answers.\nEvidence disagreement and variation. We observed how different studies and sources can come to differing conclusions regarding a claim. In this paper, we chose the majority vote among the top k documents as the final decision, but this diminishes the information about the prediction uncertainty. This is part of the broader ML problem of learning with disagreements Leonardelli et al. (2023  ###reference_b29###) and modeling human label variation Baan et al. (2024  ###reference_b4###). While usually applied to uncertainty in data annotation, it could also be applied in the future to uncertainty in answering questions with diverse evidence documents.\nInterpretability and user-centric results. Other than just predicting the final answer, the end users posing biomedical and health questions would appreciate making the results more interpretable. This includes aspects such as displaying the different evidence documents, highlighting important sections, and showing prediction probabilities. Modern large language models (LLMs) could be used to enhance the reasoning process and to generate user-friendly explanations of model predictions and decisions Lamm et al. (2021  ###reference_b26###)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we conducted a number of experiments assessing the performance of a health question-answering system in an open-domain setting. Moving away from the standard setup of working with a small evidence corpus, we expand the knowledge sources to a large corpus of more than 20 million biomedical research abstracts. We measured the answer prediction performance over three diverse datasets of health questions while varying four aspects: number of retrieved documents, number of extracted sentences, and evidence quality in the form of year of publication and number of citations. Our results show that a lower number of documents retrieved leads to better performance, with the ideal spot in the 1\u20135 range. We also show that the performance is improved and made more reliable by using a time-aware evidence retrieval process, i.e., retrieving only the highly cited and more recent papers. Our research leaves room for exploration of disagreeing and conflicting evidence, generating explanations for end users, and including further metadata. We hope our research will encourage more exploration of the open-domain health question answering setting and addressing real-world user needs."
        }
    ],
    "url": "http://arxiv.org/html/2404.08359v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.08359v1",
        "paper_title": "Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval",
        "research_background": "### Motivation\n\nIn the digital era, the vast accessibility and convenience of the Internet have made it a prevalent medium for users to seek health information. Online health information-seeking behaviors enhance patient engagement in medical decision-making, communication with care providers, and overall quality of life. Despite these benefits, a significant challenge persists: finding trustworthy and relevant evidence amidst the overwhelming abundance of online content. This challenge is even more critical in the medical field, where clinical recommendations frequently evolve over time, necessitating the retrieval of the most recent evidence for reliable health question answering (QA).\n\n### Research Problem\n\nThe primary research problem addressed in this paper entails improving the retrieval of reliable, relevant, and timely evidence in open-domain health QA systems. These systems aim to automatically provide users with trustworthy answers to their inquiries. The retriever identifies relevant documents, while the reader extracts and synthesizes information to generate the final answer. The quality of the retrieved documents crucially influences the final output, emphasizing the need to refine retrieval strategies. Key areas requiring exploration include the quantity of documents, the number of evidence passages extracted, their recency, and the credibility of the findings.\n\n### Relevant Prior Work\n\n1. **Internet as a Health Information Resource**: The widespread use of the Internet for obtaining health information due to its accessibility, coverage, and anonymity has been well-documented by Jia et al. (2021) ###reference_b21### and Neely et al. (2021) ###reference_b32###.\n   \n2. **Benefits of Online Health Information Seeking**: Rutten et al. (2019) ###reference_b39### highlighted the positive outcomes of online health information seeking, such as enhanced patient involvement and improved communication.\n\n3. **Challenges in Trustworthy Evidence Retrieval**: Battineni et al. (2020) ###reference_b7### recognized the challenges in identifying trustworthy and relevant evidence online, underscoring the need for refined retrieval methods.\n\n4. **QA Systems for Health Information**: Jin et al. (2021) ###reference_b22### and Vladika and Matthes (2023a) ###reference_b47### discussed the utility of QA systems in various contexts, including fact-checking and everyday health inquiries.\n\n5. **Open-Domain QA Challenges**: Chen and Yih (2020a) ###reference_b11###, and Zhu et al. (2021b) ###reference_b56### detailed the complexities of open-domain QA systems, distinguishing the crucial roles of retrievers and readers in handling diverse document collections.\n\n6. **Retrieval and QA Performance**: Sauchuk et al. (2022) ###reference_b40### emphasized the pivotal role of the retriever in determining QA performance quality, aligning with the necessity for advanced retrieval strategies.\n\n7. **Recent Advances in Retrieval-Augmented Generation (RAG)**: Cuconasu et al. (2024) ###reference_b15### acknowledged the progress in retrieval-powered tasks and the emerging focus on enhancing retrieval methods for better QA performance.\n\nThis paper seeks to address the identified research gap by exploring different retrieval strategies in an open-domain QA system for health questions, with a specific focus on the quantity and quality of retrieved evidence. The study leverages a substantial biomedical research database, testing various retrieval configurations to enhance QA performance, accounting for the recency and citation metrics of the documents.",
        "methodology": "Sure, I'll provide a high-level summary retaining the original wording and phrases as much as possible, based on the methodology section you provided.\n\n---\n\n**Proposed Method or Model**\n\n**Title:** Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval\n\n**Methodology Summary:**\nThe study is built upon a structured approach that involves multiple foundational elements aimed at improving the process of health question answering. The key components and innovations include:\n\n1. **Datasets:** The study leverages various datasets to train and evaluate the QA system. These datasets are likely curated to ensure relevance and quality for health-related queries.\n\n2. **Evidence Corpus:** An evidence corpus is created, which consists of a collection of health-related documents. This corpus is utilized to retrieve accurate and reliable evidence to support the answers provided by the QA system.\n\n3. **QA System Structure:** The structure of the QA system is elaborated, detailing how it processes questions, retrieves evidence, and formulates answers. The system presumably integrates methods to ensure the evidence retrieved is not only reliable but also time-aware, reflecting the most current and relevant information.\n\nThe innovative aspect lies in the emphasis on reliability and timeliness of the evidence, which addresses common challenges in health-related question answering by ensuring that the system's responses are both accurate and based on the latest available information.\n\n---",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Experiment Setup:**\n\n1. **Datasets:**\n   - The experiments leveraged datasets that required discrete answers and labels suitable for health-related question answering and fact verification tasks.\n\n2. **Baselines:**\n   - BM25: Used as the primary retrieval technique due to its efficiency and precision-oriented nature.\n   - SPICED: Sentence-embedding model for embedding sentences from abstracts and calculating cosine similarity with the question.\n   - Metadata Filters: Applied constraints on metadata parameters such as the year of publication and number of citations.\n\n3. **Evaluation Metrics:**\n   - Precision\n   - Recall\n   - F1 Score\n   - All metrics are macro-averaged to equally consider all classes within the dataset.\n\n**Experiment Groups:**\n\n1. **First Group: Document Retrieval:**\n   - **Goal:** Assess the impact of varying the number of documents (top k) retrieved and forwarded to the QA module.\n   - **Approach:** Retrieve documents using BM25, balance between diverse studies coverage and noise reduction.\n\n2. **Second Group: Sentence Selection within Abstracts:**\n   - **Goal:** Evaluate the performance of selecting relevant sentences within abstracts.\n   - **Approach:**\n     - Retrieve the top 20 most similar abstracts using BM25.\n     - Split abstracts into sentences and embed them using SPICED.\n     - Select the top j most similar sentences based on cosine similarity with the question.\n     - QA module calculates entailment probability and uses majority voting for the final answer.\n\n3. **Third and Fourth Group: Quality of Evidence Using Metadata:**\n   - **Goal:** Test the influence of metadata quality parameters on performance.\n   - **Approach:**\n     - Filter evidence documents based on publication year and number of citations.\n     - Use the retrieved filtered documents similar to the first experiment setup.\n\n**Main Experimental Results:**\n\nThe results demonstrated the critical balance needed in the number of retrieved documents to maintain precision without overloading the QA module with irrelevant data. Filtering abstracts and utilizing sentence-level relevance further improved the system's ability to extract pertinent information. Additionally, incorporating metadata such as publication year and citation count proved beneficial in enhancing the reliability of the retrieved evidence, confirming the hypothesis that recent and well-cited studies contribute to more accurate health question answering.\n\nOverall, the experiments underscored the importance of refined retrieval and evidence selection strategies in improving the performance of health QA systems, validated through improvements in macro-averaged precision, recall, and F1 scores."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To test the influence of different retrieval parameters on the performance of an open-domain QA system by varying the number of documents retrieved and the number of sentences extracted from them.",
            "experiment_process": "We conducted three groups of experiments on our QA pipeline: (1) increasing the number of retrieved documents (top k) that are forwarded to the final QA module, using BM25 for document retrieval; (2) selecting the top j most relevant sentences from abstracts and using them as evidence instead of entire documents, sentences were embedded using SPICED and selected based on cosine similarity; (3) examining the influence of documents' metadata, such as publication year and number of citations, by filtering documents based on these criteria using data from PubMed and Semantic Scholar API. The evaluation metrics used were macro-averaged versions of precision, recall, and F1 score.",
            "result_discussion": "Retrieving fewer documents led to better performance, with the best results achieved when only the top document was used. For sentence retrieval, performance improved with a higher amount of relevant sentences, but this was less consistent across datasets. The experiments also showed that more recent and highly cited documents resulted in more accurate answers, indicating the importance of evidence quality in the QA system.",
            "ablation_id": "2404.08359v1.No1"
        },
        {
            "research_objective": "To observe the effect of evidence quality parameters like publication year and number of citations on the performance of health question answering systems.",
            "experiment_process": "The experiments focused on filtering retrieved documents by their publication year and number of citations. We queried PubMed for the publication year and used the Semantic Scholar API to retrieve citation counts. We imposed restrictions on the minimum year of publication and minimum number of citations, then fed the selected documents to the QA module. The evaluation utilized macro-averaged precision, recall, and F1 score metrics.",
            "result_discussion": "More recent documents and those with higher citation counts generally improved the accuracy of the QA system's answers. Despite this trend, the absolute best performance was still achieved with just the top relevant document without additional filtering. The findings suggest a balance between document recency and citation count as an optimal strategy for evidence selection.",
            "ablation_id": "2404.08359v1.No2"
        }
    ]
}