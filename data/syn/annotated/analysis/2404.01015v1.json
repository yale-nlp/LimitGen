{
    "title": "PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison",
    "abstract": "Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems.\nRecent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories.\nAlthough effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses.\nTo handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations.\nPairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses.\nExtensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in detecting common failures from open-domain dialogue systems, including repetition and speaker insensitivity.111The code and models are available at https://github.com/ddehun/PairEval.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Open-domain dialogue systems aim to interact with users by generating natural and engaging responses for a given dialogue history.\nIn this field, building an accurate automatic evaluation metric to judge the quality of such generative systems is an important but challenging task (Liu et al., 2016  ###reference_b26###).\nThe difficulties partly originate from the one-to-many nature of daily conversations, where a single conversation can be continued with many different follow-up utterances.\nThis variety in dialogues makes traditional reference-based metrics, which compare generated responses to a limited set of known answers (i.e., references), do not correlate well with human judgments (Liu et al., 2016  ###reference_b26###).\nThe lack of reliable evaluation metrics impedes real-world deployment of open-domain dialogue systems.\nResearchers have proposed various evaluation metrics to achieve reliable assessments of\ndialogue systems. Initial studies enhance reference-based metrics by adopting distributed\nand contextualized representations by neural networks (Liu et al., 2016  ###reference_b26###; Zhang et al., 2019  ###reference_b50###).\nSubsequent studies introduce reference-free metrics that employ prediction models to assess responses based on their relevance to the previous dialogue context (Tao et al., 2018  ###reference_b41###; Lowe et al., 2017  ###reference_b30###; Mehri & Eskenazi, 2020a  ###reference_b31###; Zhang et al., 2021  ###reference_b48###; Zhong et al., 2022  ###reference_b54###), showing a higher correlation with human judgments. Moreover, prompting-based metrics have been presented to properly transfer the instruction-following and zero-shot capabilities of large language models (LLMs) for dialogue evaluation (Zhang et al., 2023  ###reference_b49###; Lin & Chen, 2023  ###reference_b24###).\nWe hold that the dialogue evaluation should aim to assign differentiated scores to responses by considering their relative quality.\nIn other words, evaluation metrics should make calibrated scores such that they are appropriately aligned with human evaluations.\nNumerous correlation metrics used in the meta-evaluation of evaluation metrics like Spearman (Zar, 2005  ###reference_b47###) or Kendall rank correlations (Kendall, 1955  ###reference_b18###) reflect this intuition.\nTherefore, we argue that assessing the target responses by considering their relative appropriateness with other ones is a meaningful process to make more reliable evaluation metrics.\nIn this regard, several studies evaluate responses by considering their relative quality (Sato et al., 2020  ###reference_b39###; Liusie et al., 2024  ###reference_b28###).\nHowever, these approaches usually require exhaustive comparison operations or human-crafted candidate responses, which may not always be available.\nIn this paper, we propose PairEval, a novel open-domain dialogue evaluation metric with comparative assessments. Our metric assesses the individual responses by comparing their quality against only a limited number of comparison responses.\nInstead of relying on a commercial or proprietary LLM, our metric is built on top of a moderate-size and open-sourced LLM (Touvron et al., 2023  ###reference_b42###).\nTo elicit the comparative ability of LMs, we devise a simple but effective learning strategy with a public dialogue corpus.\nExperiments on multiple benchmarks show that PairEval outperforms previous evaluation metrics, and sometimes even shows higher performance than metrics with a powerful proprietary LLM.\nFurther analysis demonstrates that the pairwise evaluation approach is robust and effective in capturing common failures (e.g., repetitive outcomes) in dialogue systems."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Traditional metrics like BLEU (Papineni et al., 2002  ###reference_b34###) or ROUGE (Lin, 2004  ###reference_b23###) that measure the N-gram overlap between generated responses and answers show a low correlation with human judgments (Liu et al., 2016  ###reference_b26###).\nSeveral studies use embedding models to consider the semantic similarity between candidate responses and answers (Liu et al., 2016  ###reference_b26###; Zhang et al., 2019  ###reference_b50###).\nHowever, these reference-based metrics rely on a set of known answers for similarity comparison, making it hard to consider the wide semantic space of follow-up utterances for a single conversation.\nTo tackle this problem, recent studies proposed numerous reference-free metrics that directly predict the relevance of a generated response to the given dialogue history (Tao et al., 2018  ###reference_b41###; Mehri & Eskenazi, 2020a  ###reference_b31###; b  ###reference_b32###; Zhong et al., 2022  ###reference_b54###; De Bruyn et al., 2022  ###reference_b5###).\nSpecifically, neural classification or regression models are usually trained to distinguish relevant responses from irrelevant ones.\nThese predictive metrics have shown meaningful progress along with pre-trained language models (Ghazarian et al., 2019  ###reference_b11###; Mehri & Eskenazi, 2020a  ###reference_b31###; Zhong et al., 2022  ###reference_b54###), data augmentation strategies (Gupta et al., 2021  ###reference_b14###; Park et al., 2021  ###reference_b35###), and advanced training algorithms (Huang et al., 2020  ###reference_b17###; Zhang et al., 2021  ###reference_b48###).\nIn contrast, our work focuses on a comparative evaluation to consider the relative quality between responses for a more reliable evaluation.\nTo the best of our knowledge, Liusie et al. (2024  ###reference_b28###) propose a pioneering step that introduces a comparison-based evaluation approach for various natural language generation tasks, including an open-domain dialogue generation task.\nHowever, they find that exhaustive comparison between candidate responses is needed for reliable quality estimation of dialogues, resulting in an undesirable computational overhead.\nOur work overcomes this challenge by specializing in moderate-size LLMs for pairwise comparison between responses.\nSato et al. (2020  ###reference_b39###) consider the relative rank of generated responses against other false candidates, and a response selection model is employed for this purpose.\nThough promising, their evaluation approach requires human evaluation to filter out correct responses from a candidate pool.\nIn contrast, we show that even a limited number of comparisons are enough to ensure a reasonable correlation with human judgment.\nThe pairwise comparison approach has been widely explored from various perspectives, including preference learning (F\u00fcrnkranz & H\u00fcllermeier, 2003  ###reference_b10###), recommendation (Beutel et al., 2019  ###reference_b2###), reinforcement learning (Xu et al., 2020  ###reference_b44###), and retrieval systems (Qin et al., 2023  ###reference_b37###).\nRecent studies that consider the human-aligned behaviors of LMs proposed to reflect human preference in the form of comparisons over multiple model generations (Ouyang et al., 2022  ###reference_b33###; Bai et al., 2022  ###reference_b1###).\nAn LLM-based pairwise evaluation has also been increasingly adopted to build system-level ranking information (Boubdir et al., 2023  ###reference_b4###; Zheng et al., 2024  ###reference_b53###).\nThis work evaluates individual responses by comparing their relative quality against the limited number of responses.\n\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We propose PairEval, a new reference-free dialogue evaluation metric based on a pairwise comparison. Our metric assesses responses by comparing their quality against a small number of comparison examples.\nThe comparison examples are derived from human-written conversations in a dialogue corpus.\nFig. 1  ###reference_### depicts the overall pipeline of PairEval."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Task Formulation",
            "text": "In this work, we focus on the turn-level and reference-free evaluation of open-domain dialogue systems.\nGiven a dialogue history  that consists of multiple utterances between two speakers, a dialogue system outputs a single utterance  as a follow-up response.\nThe evaluation metric  considers how suitable the generated response is as the next utterance for the given dialogue history and makes an evaluation score .\nThe performance of the metric is usually measured by computing the correlation between human judgments  and metric scores  when dialogue systems make  individual responses  for their dialogue histories ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dialogue Evaluation with Pairwise Comparison",
            "text": "Fig. 1  ###reference_### illustrates the evaluation process of PairEval. The metric is designed to judge the quality of a generated response by considering its relative quality against other comparison responses. We use an LLM along with a carefully designed text prompt to perform a pairwise comparison between two responses. Given a pair of conversations in the form of a text prompt, the LM is asked to choose the better response. The probabilities of the target response being better than each comparison example are then aggregated and utilized as its final score.\nThe generated response receives higher metric scores as it is predicted as better than the other responses. To construct a group of comparison examples, we use a few randomly sampled conversations from a public dialogue corpus. This makes our metric to be a practical and efficient solution for dialogue evaluation.\nFormally, let  denote a target conversation that consists of a generated response  under evaluation and its corresponding dialogue history .\nLet  represent  different comparison examples.\nWe first make a pair of a target conversation with every comparison example , resulting in  different pairs of conversations.\nEach pair is then converted into an input text of an LLM by replacing the placeholder of our text prompt .\nGiven the text prompt , the LM () is asked to choose a response of higher quality.\nTo precisely acquire the LM\u2019s prediction in the form of predictive probability, we assign a single label word for each response (i.e., \u201cA\u201d and \u201cB\u201d), and regard the probability of each label word as a score of the response allocated by the LM.\nThe probabilities that a generated response  is in better quality  is better than  are stored during every comparison against .\nThe averaged probability of  after every competition is then used as the final evaluation score of the target response.\nFor the aggregation of probabilities with different comparison examples, we simply use an average operation ().\nIn practice, since LLMs are known to be sensitive to ordering in the prompt (Wang et al., 2023  ###reference_b43###), we infer the LMs twice for a single conversation pair with swapped orders and use the averaged probability."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Making LMs Specialized in Pairwise Comparison",
            "text": "###figure_2### Although recent LLMs have shown impressive instruction-following and task generalization abilities, applying moderate-size LLMs directly for pairwise dialogue evaluation tasks described in Section 3.2  ###reference_### may make it hard to ensure reasonable evaluation results.\nFor instance, the training examples that require LMs to compare two text inputs and choose the better one would occupy only a small portion of the entire training dataset.\nTo tackle this problem, we propose an intuitive training strategy to make LMs specialize in pairwise comparisons for evaluation.\nSpecifically, we construct synthetic training examples that instruct an LM to compare two responses of different quality \u2013 positive and negative responses \u2013 and choose the better one. To obtain a positive response along with its dialogue history, we simply use usual conversations that consist of a dialogue history and its follow-up utterances presented in the ordinary dialogue corpus.\nFor reliable construction of negative ones, we explore two different types of negative responses with varied difficulties for evaluation: random and adversarial. The random negative responses are the utterances of different dialogue histories that are arbitrarily sampled from the dialogue corpus. The Adversarial negative responses are written by human annotators to exhibit high superficial relevance to a dialogue context but are not suitable as a follow-up utterance of the context. This adversarial response enables an LM to identify the inappropriateness of a response by capturing more subtle errors rather than relying on the superficial similarity against a dialogue history. In this work, we use human-written negative responses released by Sai et al. (2020  ###reference_b38###) for the adversarial responses. Fig. 2  ###reference_### illustrates an example of responses with different types.\nGiven a paired example with positive and negative responses, we construct an input text of an LM with pairwise evaluation prompt .\nWe then finetune the LM to predict the label word of a positive response correctly.\nThe location of positive and negative examples in an input text is randomly decided to avoid unintended positional bias."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "The section presents our experimental setups (section 4.1  ###reference_###), main results (section 4.2  ###reference_###), and further analysis (section 4.3  ###reference_###) as follows.\nWe evaluate the performance of each evaluation metric by comparing its outcome scores against scores assigned by human annotators.\nTo this end, multiple meta-evaluation datasets with human annotations are used for experiments. Each instance in the dataset consists of (1) dialogue history, (2) generated response from a dialogue system, (3) answer response, and (4) response quality score annotated by human annotators. The details of each meta-evaluation dataset are as follows.\nDailyDialog-Zhao (Zhao et al., 2020  ###reference_b52###) consists of 900 evaluation instances from multiple different dialogue systems. The dataset originates from the DailyDialog corpus (Li et al., 2017  ###reference_b21###), serving both as a training corpus of dialogue systems and the source of dialogue histories. The \u201coverall\u201d aspect of the example is used as a human annotation score.\nConvAI2-USR and TopicalChat-USR are datasets released by Mehri & Eskenazi (2020a  ###reference_b31###) and consist of 300 and 360 instances, respectively.\nEach dataset is derived by ConvAI2 (Dinan et al., 2020  ###reference_b9###) and TopicalChat (Gopalakrishnan et al., 2019  ###reference_b12###) dialogue corpus, respectively. Three dialogue systems are used as a dialogue system, and the \u201cOverall Quality\u201d score is used in our experiments.\nDailyDialog-GRADE and ConvAI2-GRADE are released by Huang et al. (2020  ###reference_b17###), each based on the DailyDialog and ConvAI2 datasets. DailyDialog-GRADE and ConvAI2-GRADE both contain 300 instances from two and four systems, respectively.\nFED (Mehri & Eskenazi, 2020b  ###reference_b32###) contains 375 instances of conversations between human speakers or human and dialogue systems. The \u201cOverall\u201d aspect of the examples is used for evaluation.\nWe finetune an LLM with two widely used open-domain dialogue corpus - DailyDialog (Li et al., 2017  ###reference_b21###) and ConvAI2 (Dinan et al., 2020  ###reference_b9###) - individually, resulting in two different versions of PairEval.\nBoth corpora contain multi-turn conversations between two speakers, and the number of conversations is 13,118 and 17,818, respectively.\nTo construct positive response sets, we use randomly sampled utterances along with their previous dialogue history in the train split of a dialogue corpus.\nFor random negative response sets, we use randomly sampled utterances from a dialogue corpus.\nDuring the training of the LM on the DailyDialog corpus, we also use human-written adversarial responses released by Sai et al. (2020  ###reference_b38###) as an adversarial negative type.\nThe number of training examples is set to 80k and 65k for DailyDialog and ConvAI, respectively.\nThe LM trained on the ConvAI2 dataset is used for the evaluation of ConvAI2-GRADE and ConvAI2-USR.\nThe LM trained on the DailyDialog dataset is evaluated on the remaining meta-evaluation datasets.\nBLEU-2 (Papineni et al., 2002  ###reference_b34###) measures the n-gram overlap similarity of a generated response against an answer (reference) response.\nBERTScore (Zhang et al., 2019  ###reference_b50###) uses contextualized embeddings of a pre-trained LM (Liu et al., 2019  ###reference_b27###) for similarity comparison.\nBLEURT (Sellam et al., 2020  ###reference_b40###) is pre-trained on synthetic datasets for a better evaluation of machine translation systems.\nUSR (Mehri & Eskenazi, 2020a  ###reference_b31###) is trained to distinguish the original follow-up response of a dialogue history from randomly sampled ones.\nDEnsity (Park et al., 2023  ###reference_b36###) calculates the distance of a generated response from the distribution of relevant responses on the feature space of an LM.\nFED (Mehri & Eskenazi, 2020b  ###reference_b32###) and FULL (De Bruyn et al., 2022  ###reference_b5###) both measure the likelihood of predefined follow-up utterances to estimate the quality of a generated response.\nUniEval (Zhong et al., 2022  ###reference_b54###) adopt intermediate training on multiple datasets for an evaluation of various natural language generation tasks.\nLLMEval (Lin & Chen, 2023  ###reference_b24###) is a prompt-based metric that leverages a proprietary LLM (claude-v1.3) for a dialogue evaluation task.\nBesides, we also introduce DirectEval, which uses the same training examples and the LLM as PairEval.\nInstead of performing a pairwise comparison, however, this metric directly predicts the appropriateness of a generated response. Specifically, we ask the LM to predict the quality of a generated response (\u201cIs the above response a good response to the given conversation?\u201d), following previous studies (Gupta et al., 2022  ###reference_b15###; Liang et al., 2023  ###reference_b22###). The probability of a target response (P(\u201cYes\u201d)) is then used as a metric score. This metric is designed to validate the effectiveness of pairwise comparison by controlling the training configuration to be roughly the same as PairEval.\nFig. 3  ###reference_### presents a prediction of selected metrics along with human judgments in the DailyDialog-GRADE dataset.\nThe x and y axes of each point represent human judgments and metric scores, respectively.\nAmong the baseline metrics, reference-based metrics like BLEU and BLEURT fail to assign discriminated scores as human ones.\nReference-free metrics like USR and DirectEval show a positive correlation with human judgments.\nPairEval shows a strong correlation with human evaluations.\nTable 8  ###reference_### presents two selected examples from meta-evaluation datasets and their evaluation results of automatic metrics and human annotators. For both examples, PairEval evaluates generated responses closely to human scores.\nHistory\n\n\n\nA: Children\u2019s books require less writing and more illustrations. It would be worth it for the\n\n  \u2003extra 500 copies though\n\nB: Haha I agree with you. Do you ever buy stuff from Amazon? do you know why Jeff Bezos\n\n  \u2003put a clock into a mountain lol\n\n\nReference\n\n\n\nA: I buy things from Amazon all the time. Isn\u2019t that the clock that\u2019s supposed to run for 10,000 years\n\n\nResponse\nA: I have not, what is it about?\n\nEvaluation\nHuman\nBLEU-2\nBERTScore\nFULL\nUniEval\nDirectEval\nPairEval\n\nScore(Rank)\n3.0(0.56)\n0.0(0.95)\n0.84(0.82)\n0.76(0.72)\n0.76(0.72)\n0.99(0.78)\n0.47(0.65)\n\nHistory\n\n\n\nA: What do you think about the equipment in our company?\n\nB: We have to equip our company with a network of work stations at every office and show room\n\n  \u2003as soon as possible.\n\n\nReference\n\n\n\nA: I also realized this issue. And office automation is essential.\n\n\nResponse\nA: How do you ensure quality?\n\nEvaluation\nHuman\nBLEU-2\nBERTScore\nFULL\nUniEval\nDirectEval\nPairEval\n\nScore(Rank)\n3.9(0.09)\n0.0(1.0)\n0.86(0.36)\n0.61(0.71)\n0.61(0.71)\n0.98(0.28)\n0.33(0.2)"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Setup",
            "text": "We evaluate the performance of each evaluation metric by comparing its outcome scores against scores assigned by human annotators.\nTo this end, multiple meta-evaluation datasets with human annotations are used for experiments. Each instance in the dataset consists of (1) dialogue history, (2) generated response from a dialogue system, (3) answer response, and (4) response quality score annotated by human annotators. The details of each meta-evaluation dataset are as follows.\nDailyDialog-Zhao (Zhao et al., 2020  ###reference_b52###  ###reference_b52###) consists of 900 evaluation instances from multiple different dialogue systems. The dataset originates from the DailyDialog corpus (Li et al., 2017  ###reference_b21###  ###reference_b21###), serving both as a training corpus of dialogue systems and the source of dialogue histories. The \u201coverall\u201d aspect of the example is used as a human annotation score.\nConvAI2-USR and TopicalChat-USR are datasets released by Mehri & Eskenazi (2020a  ###reference_b31###  ###reference_b31###) and consist of 300 and 360 instances, respectively.\nEach dataset is derived by ConvAI2 (Dinan et al., 2020  ###reference_b9###  ###reference_b9###) and TopicalChat (Gopalakrishnan et al., 2019  ###reference_b12###  ###reference_b12###) dialogue corpus, respectively. Three dialogue systems are used as a dialogue system, and the \u201cOverall Quality\u201d score is used in our experiments.\nDailyDialog-GRADE and ConvAI2-GRADE are released by Huang et al. (2020  ###reference_b17###  ###reference_b17###), each based on the DailyDialog and ConvAI2 datasets. DailyDialog-GRADE and ConvAI2-GRADE both contain 300 instances from two and four systems, respectively.\nFED (Mehri & Eskenazi, 2020b  ###reference_b32###  ###reference_b32###) contains 375 instances of conversations between human speakers or human and dialogue systems. The \u201cOverall\u201d aspect of the examples is used for evaluation.\nWe finetune an LLM with two widely used open-domain dialogue corpus - DailyDialog (Li et al., 2017  ###reference_b21###  ###reference_b21###) and ConvAI2 (Dinan et al., 2020  ###reference_b9###  ###reference_b9###) - individually, resulting in two different versions of PairEval.\nBoth corpora contain multi-turn conversations between two speakers, and the number of conversations is 13,118 and 17,818, respectively.\nTo construct positive response sets, we use randomly sampled utterances along with their previous dialogue history in the train split of a dialogue corpus.\nFor random negative response sets, we use randomly sampled utterances from a dialogue corpus.\nDuring the training of the LM on the DailyDialog corpus, we also use human-written adversarial responses released by Sai et al. (2020  ###reference_b38###  ###reference_b38###) as an adversarial negative type.\nThe number of training examples is set to 80k and 65k for DailyDialog and ConvAI, respectively.\nThe LM trained on the ConvAI2 dataset is used for the evaluation of ConvAI2-GRADE and ConvAI2-USR.\nThe LM trained on the DailyDialog dataset is evaluated on the remaining meta-evaluation datasets.\nBLEU-2 (Papineni et al., 2002  ###reference_b34###  ###reference_b34###) measures the n-gram overlap similarity of a generated response against an answer (reference) response.\nBERTScore (Zhang et al., 2019  ###reference_b50###  ###reference_b50###) uses contextualized embeddings of a pre-trained LM (Liu et al., 2019  ###reference_b27###  ###reference_b27###) for similarity comparison.\nBLEURT (Sellam et al., 2020  ###reference_b40###  ###reference_b40###) is pre-trained on synthetic datasets for a better evaluation of machine translation systems.\nUSR (Mehri & Eskenazi, 2020a  ###reference_b31###  ###reference_b31###) is trained to distinguish the original follow-up response of a dialogue history from randomly sampled ones.\nDEnsity (Park et al., 2023  ###reference_b36###  ###reference_b36###) calculates the distance of a generated response from the distribution of relevant responses on the feature space of an LM.\nFED (Mehri & Eskenazi, 2020b  ###reference_b32###  ###reference_b32###) and FULL (De Bruyn et al., 2022  ###reference_b5###  ###reference_b5###) both measure the likelihood of predefined follow-up utterances to estimate the quality of a generated response.\nUniEval (Zhong et al., 2022  ###reference_b54###  ###reference_b54###) adopt intermediate training on multiple datasets for an evaluation of various natural language generation tasks.\nLLMEval (Lin & Chen, 2023  ###reference_b24###  ###reference_b24###) is a prompt-based metric that leverages a proprietary LLM (claude-v1.3) for a dialogue evaluation task.\nBesides, we also introduce DirectEval, which uses the same training examples and the LLM as PairEval.\nInstead of performing a pairwise comparison, however, this metric directly predicts the appropriateness of a generated response. Specifically, we ask the LM to predict the quality of a generated response (\u201cIs the above response a good response to the given conversation?\u201d), following previous studies (Gupta et al., 2022  ###reference_b15###  ###reference_b15###; Liang et al., 2023  ###reference_b22###  ###reference_b22###). The probability of a target response (P(\u201cYes\u201d)) is then used as a metric score. This metric is designed to validate the effectiveness of pairwise comparison by controlling the training configuration to be roughly the same as PairEval."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Dataset",
            "text": "We evaluate the performance of each evaluation metric by comparing its outcome scores against scores assigned by human annotators.\nTo this end, multiple meta-evaluation datasets with human annotations are used for experiments. Each instance in the dataset consists of (1) dialogue history, (2) generated response from a dialogue system, (3) answer response, and (4) response quality score annotated by human annotators. The details of each meta-evaluation dataset are as follows.\nDailyDialog-Zhao (Zhao et al., 2020  ###reference_b52###  ###reference_b52###  ###reference_b52###) consists of 900 evaluation instances from multiple different dialogue systems. The dataset originates from the DailyDialog corpus (Li et al., 2017  ###reference_b21###  ###reference_b21###  ###reference_b21###), serving both as a training corpus of dialogue systems and the source of dialogue histories. The \u201coverall\u201d aspect of the example is used as a human annotation score.\nConvAI2-USR and TopicalChat-USR are datasets released by Mehri & Eskenazi (2020a  ###reference_b31###  ###reference_b31###  ###reference_b31###) and consist of 300 and 360 instances, respectively.\nEach dataset is derived by ConvAI2 (Dinan et al., 2020  ###reference_b9###  ###reference_b9###  ###reference_b9###) and TopicalChat (Gopalakrishnan et al., 2019  ###reference_b12###  ###reference_b12###  ###reference_b12###) dialogue corpus, respectively. Three dialogue systems are used as a dialogue system, and the \u201cOverall Quality\u201d score is used in our experiments.\nDailyDialog-GRADE and ConvAI2-GRADE are released by Huang et al. (2020  ###reference_b17###  ###reference_b17###  ###reference_b17###), each based on the DailyDialog and ConvAI2 datasets. DailyDialog-GRADE and ConvAI2-GRADE both contain 300 instances from two and four systems, respectively.\nFED (Mehri & Eskenazi, 2020b  ###reference_b32###  ###reference_b32###  ###reference_b32###) contains 375 instances of conversations between human speakers or human and dialogue systems. The \u201cOverall\u201d aspect of the examples is used for evaluation.\nWe finetune an LLM with two widely used open-domain dialogue corpus - DailyDialog (Li et al., 2017  ###reference_b21###  ###reference_b21###  ###reference_b21###) and ConvAI2 (Dinan et al., 2020  ###reference_b9###  ###reference_b9###  ###reference_b9###) - individually, resulting in two different versions of PairEval.\nBoth corpora contain multi-turn conversations between two speakers, and the number of conversations is 13,118 and 17,818, respectively.\nTo construct positive response sets, we use randomly sampled utterances along with their previous dialogue history in the train split of a dialogue corpus.\nFor random negative response sets, we use randomly sampled utterances from a dialogue corpus.\nDuring the training of the LM on the DailyDialog corpus, we also use human-written adversarial responses released by Sai et al. (2020  ###reference_b38###  ###reference_b38###  ###reference_b38###) as an adversarial negative type.\nThe number of training examples is set to 80k and 65k for DailyDialog and ConvAI, respectively.\nThe LM trained on the ConvAI2 dataset is used for the evaluation of ConvAI2-GRADE and ConvAI2-USR.\nThe LM trained on the DailyDialog dataset is evaluated on the remaining meta-evaluation datasets."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Baselines",
            "text": "The following baseline metrics are considered in our experiments.\nFor better readability, we group them into reference-based and reference-free metrics. Further details are in A.1  ###reference_###.\nBLEU-2 (Papineni et al., 2002  ###reference_b34###  ###reference_b34###  ###reference_b34###) measures the n-gram overlap similarity of a generated response against an answer (reference) response.\nBERTScore (Zhang et al., 2019  ###reference_b50###  ###reference_b50###  ###reference_b50###) uses contextualized embeddings of a pre-trained LM (Liu et al., 2019  ###reference_b27###  ###reference_b27###  ###reference_b27###) for similarity comparison.\nBLEURT (Sellam et al., 2020  ###reference_b40###  ###reference_b40###  ###reference_b40###) is pre-trained on synthetic datasets for a better evaluation of machine translation systems.\nUSR (Mehri & Eskenazi, 2020a  ###reference_b31###  ###reference_b31###  ###reference_b31###) is trained to distinguish the original follow-up response of a dialogue history from randomly sampled ones.\nDEnsity (Park et al., 2023  ###reference_b36###  ###reference_b36###  ###reference_b36###) calculates the distance of a generated response from the distribution of relevant responses on the feature space of an LM.\nFED (Mehri & Eskenazi, 2020b  ###reference_b32###  ###reference_b32###  ###reference_b32###) and FULL (De Bruyn et al., 2022  ###reference_b5###  ###reference_b5###  ###reference_b5###) both measure the likelihood of predefined follow-up utterances to estimate the quality of a generated response.\nUniEval (Zhong et al., 2022  ###reference_b54###  ###reference_b54###  ###reference_b54###) adopt intermediate training on multiple datasets for an evaluation of various natural language generation tasks.\nLLMEval (Lin & Chen, 2023  ###reference_b24###  ###reference_b24###  ###reference_b24###) is a prompt-based metric that leverages a proprietary LLM (claude-v1.3) for a dialogue evaluation task.\nBesides, we also introduce DirectEval, which uses the same training examples and the LLM as PairEval.\nInstead of performing a pairwise comparison, however, this metric directly predicts the appropriateness of a generated response. Specifically, we ask the LM to predict the quality of a generated response (\u201cIs the above response a good response to the given conversation?\u201d), following previous studies (Gupta et al., 2022  ###reference_b15###  ###reference_b15###  ###reference_b15###; Liang et al., 2023  ###reference_b22###  ###reference_b22###  ###reference_b22###). The probability of a target response (P(\u201cYes\u201d)) is then used as a metric score. This metric is designed to validate the effectiveness of pairwise comparison by controlling the training configuration to be roughly the same as PairEval."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Implementation Details",
            "text": "We use Llama-2-7b-chat (Touvron et al., 2023  ###reference_b42###) as an LM of both PairEval and DirectEval.\nThe LM is finetuned with LoRA (Hu et al., 2021  ###reference_b16###) for 1 epoch, and the  and  of LoRA is set to 16 and 8, respectively.\nAdamW (Loshchilov & Hutter, 2018  ###reference_b29###) optimizer is used for optimization, and the initial learning rate is set to 1e-4.\nThe batch size is set to 16.\nThe number of comparison examples () is set to 3 and is randomly sampled from the validation set of the DailyDialog corpus. Note that these comparison examples are used across all meta-evaluation datasets.\nIn all experiments, a single 3090 RTX GPU with 24GB of memory is used.\nRegarding the limited computational resources, we use 4-bit quantization (Dettmers et al., 2024  ###reference_b7###) during the evaluation of DirectEval and PairEval.\nMethods\n\n\n\nDailyDialog\n\nGRADE\n\n\n\n\n\nDailyDialog\n\nZhao\n\n\n\n\n\nConvAI2\n\nGRADE\n\n\n\n\n\nConvAI2\n\nUSR\n\n\n\n\n\nTopicalChat\n\nUSR\n\n\n\n\n\nFED\n\n\n\n\n\nAvg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference-based Metrics\n\nBLEU-2 (Papineni et al., 2002  ###reference_b34###)\n14.6*\n10.3*\n\n35.5\n20.6\n\n5.41*\n9.64*\n\n32.3\n31.5\n\n45.9\n46.2\n\n-\n-\n\n-\n-\n\nBLEURT (Sellam et al., 2020  ###reference_b40###)\n17.5*\n12.2*\n\n34.1\n28.7\n\n16.9*\n16.9*\n\n33.3\n29.9\n\n44.7\n41.7\n\n-\n-\n\n-\n-\n\nBERTScore Zhang et al. (2019  ###reference_b50###)\n12.9*\n9.9*\n\n36.4\n29.4\n\n26.0\n27.7\n\n33.3\n28.3\n\n46.9\n46.2\n\n-\n-\n\n-\n-\n\nReference-free Metrics\n\nFED (Mehri & Eskenazi, 2020b  ###reference_b32###)\n2.6*\n0.1*\n\n-9.0*\n-8.5*\n\n-6.1*\n-4.6*\n\n-2.0*\n-0.7*\n\n-7.1*\n-6.9*\n\n11.9*\n9.4*\n\n-1.6\n-1.9\n\nUSR (Mehri & Eskenazi, 2020a  ###reference_b31###)\n27.5\n23.8\n\n48.8\n51.6\n\n40.3\n40.0\n\n60.9\n48.1\n\n40.7\n32.5\n\n11.4\n11.7\n\n38.2\n34.6\n\nCTC (Deng et al., 2021  ###reference_b6###)\n-15.1*\n-14.7*\n\n-5.6*\n-9.2*\n\n3.9*\n4.1*\n\n47.4\n48.3\n\n39.8\n36.3\n\n16.1*\n19.5*\n\n14.4\n14.0\n\nFULL (De Bruyn et al., 2022  ###reference_b5###)\n-8.8*\n-10.0*\n\n5.1*\n6.0*\n\n22.0*\n23.8*\n\n7.0*\n8.9*\n\n-4.9*\n-7.2*\n\n47.1\n50.6\n\n11.3\n12.0\n\nUniEval (Zhong et al., 2022  ###reference_b54###)\n7.6*\n5.7*\n\n30.9\n30.1\n\n46.6\n47.9\n\n65.1\n65.9\n\n62.0\n64.5\n\n29.8\n29.9\n\n46.9\n47.7\n\nDEnsity (Park et al., 2023  ###reference_b36###)\n30.3\n29.5\n\n56.8\n57.0\n\n48.0\n48.6\n\n57.0\n63.0\n\n16.3\n24.7\n\n24.5\n21.4\n\n38.8\n40.7\n\nDirectEval\n38.6\n45.1\n\n62.4\n70.0\n\n55.5\n56.2\n\n58.3\n75.0\n\n39.8\n73.9\n\n44.4\n50.4\n\n49.8\n61.8\n\nPairEval (Ours)\n47.8\n55.8\n\n62.9\n70.7\n\n51.1\n56.6\n\n66.5\n71.1\n\n70.8\n72.2\n\n51.9\n52.0\n\n58.5\n63.1\n\nProprietary LLM-based Metrics\n\nLLMEval (Lin & Chen, 2023  ###reference_b24###)\n34.6\n34.9\n\n-\n-\n\n61.3\n61.8\n\n53.3\n51.5\n\n49.0\n49.9\n\n59.7\n49.9\n\n-\n-"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "We use the Pearson correlation coefficient () and Spearman\u2019s rank correlation coefficient () to measure the correlation between human judgments and metric scores.\nTable 1  ###reference_### shows the experimental results of all metrics.\nOverall, PairEval achieves the highest performance in four and three out of six datasets in the Pearson and Spearman correlations, respectively.\nOur metric works well on dialogues from TopicalChat-USR and FED that are not used during the finetuning stages.\nMoreover, PairEval sometimes even shows a higher correlation than LLMEval which relies on a powerful proprietary LLM.\nAmong the other baseline metrics, UniEval shows reasonable performance.\nDirectEval, a metric that uses the same training dataset source as PairEval, also shows competitive performance across multiple benchmarks.\nHowever, it generally performs worse than PairEval, especially in the case of the Pearson correlation coefficient.\nWe believe these results confirm the effectiveness of the pairwise comparison approach for dialogue evaluation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we propose PairEval, a novel dialogue metric to evaluate generated responses by considering their relative quality against a few comparison examples.\nWe encourage moderate-size and open-source LLMs to be specialized for pairwise comparison.\nExperiments on multiple evaluation benchmarks demonstrate that PairEval correlates well with human judgments, confirming its effectiveness and validity.\nAlthough effective, PairEval inevitably introduces multiple LLM calls for a single evaluation.\nThis problem would be amplified as we use a larger set of comparison examples.\nOur future work should address an efficient way to find an optimal and small number of comparison examples."
        }
    ],
    "url": "http://arxiv.org/html/2404.01015v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "4.3.2",
            "4.3.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.01015v1",
        "paper_title": "PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison",
        "research_background": "**Motivation:**\n\nThe motivation behind this paper is rooted in the need for an accurate automatic evaluation metric for open-domain dialogue systems. Traditional evaluation methods, which compare generated responses to a limited set of reference answers, often fail to correlate well with human judgments due to the one-to-many nature of daily conversations. This lack of reliable evaluation impedes the practical deployment of dialogue systems. The study aims to address the inefficacies of existing metrics by developing a more aligned and robust evaluation metric.\n\n**Research Problem:**\n\nThe primary research problem this paper addresses is the development of a reliable and effective evaluation metric for open-domain dialogue systems that better correlates with human evaluations. This involves overcoming the limitations of both reference-based and existing reference-free metrics, which either do not manage to reflect the diversity in dialogues accurately or require exhaustive comparisons and human-crafted responses.\n\n**Relevant Prior Work:**\n\n1. **Enhancement of Reference-Based Metrics:**\n   - Liu et al. (2016) and Zhang et al. (2019) have worked on enhancing traditional reference-based metrics by integrating distributed and contextualized representations via neural networks.\n\n2. **Introduction of Reference-Free Metrics:**\n   - Tao et al. (2018), Lowe et al. (2017), Mehri & Eskenazi (2020a), Zhang et al. (2021), and Zhong et al. (2022) have proposed reference-free metrics that use prediction models to assess responses based on their relevance to the preceding dialogue context.\n\n3. **Prompting-Based Metrics:**\n   - Recent studies such as Zhang et al. (2023) and Lin & Chen (2023) have explored prompting-based metrics to utilize the instruction-following and zero-shot capabilities of large language models for dialogue evaluation.\n\n4. **Pairwise Evaluation Approaches:**\n   - The paper builds on the concept of evaluating the relative quality of responses, as demonstrated by studies like Sato et al. (2020) and Liusie et al. (2024), which emphasize assessing responses based on a comparative process, albeit often necessitating extensive comparison operations or human-engineered responses.\n\n**Proposed Solution:**\n\nThe authors introduce PairEval, a novel metric for dialogue evaluation built on a moderate-sized, open-source large language model. This metric assesses responses by comparing their quality against a limited number of other responses, guided by an effective learning strategy using public dialogue corpora. The research demonstrates that PairEval outperforms existing metrics and is robust in identifying common dialogue system failures.",
        "methodology": "PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison Methodology: We propose PairEval, a new reference-free dialogue evaluation metric based on a pairwise comparison. Our metric assesses responses by comparing their quality against a small number of comparison examples. The comparison examples are derived from human-written conversations in a dialogue corpus. By adopting a pairwise comparison approach, PairEval distinguishes itself by evaluating each response in the context of other responses, rather than relying on a single absolute quality measure. This relative evaluation method allows PairEval to provide a more nuanced assessment of dialogue quality, leveraging the diversity and richness found in human-written exchanges. The inherent advantage of this method lies in its ability to contextualize responses within a spectrum of conversational quality, thereby reflecting more accurate and human-like judgment.",
        "main_experiment_and_results": "#### Datasets:\n1. **DailyDialog-Zhao** (Zhao et al., 2020):\n   - Origin: DailyDialog corpus (Li et al., 2017)\n   - Instances: 900\n   - Dialogue Systems: Multiple\n   - Annotation: \u201cOverall\u201d aspect\n\n2. **ConvAI2-USR and TopicalChat-USR** (Mehri & Eskenazi, 2020a):\n   - Instances: 300 (ConvAI2-USR), 360 (TopicalChat-USR)\n   - Dialogue Systems: Three\n   - Annotation: \u201cOverall Quality\u201d score\n\n3. **DailyDialog-GRADE and ConvAI2-GRADE** (Huang et al., 2020):\n   - Instances: 300 (DailyDialog-GRADE), 300 (ConvAI2-GRADE)\n   - Dialogue Systems: Two (DailyDialog-GRADE), Four (ConvAI2-GRADE)\n\n4. **FED** (Mehri & Eskenazi, 2020b):\n   - Instances: 375\n   - Conversations: Human-human or human-system\n   - Annotation: \u201cOverall\u201d aspect\n\n#### Baselines:\n1. **BLEU-2** (Papineni et al., 2002): N-gram overlap similarity.\n2. **BERTScore** (Zhang et al., 2019): Contextualized embeddings similarity.\n3. **BLEURT** (Sellam et al., 2020): Synthetic dataset pre-training for evaluation.\n4. **USR** (Mehri & Eskenazi, 2020a): Distinguishes original from random follow-up response.\n5. **DEnsity** (Park et al., 2023): Distance measure in the feature space of an LM.\n6. **FED** (Mehri & Eskenazi, 2020b): Likelihood of predefined follow-up utterances.\n7. **FULL** (De Bruyn et al., 2022): Quality estimation metric with a predefined likelihood.\n8. **UniEval** (Zhong et al., 2022): Intermediate training on multiple datasets.\n9. **LLMEval** (Lin & Chen, 2023): Prompt-based metric with a proprietary LLM.\n10. **DirectEval**: Predicts response appropriateness directly using an LLM.\n\n#### Evaluation Metrics:\n- Correlation with human judgments\n- Main focus on how metrics align with human scores.\n\n### Main Experimental Results:\n\n- **Reference-based metrics (BLEU, BLEURT)**: Struggle to assign discriminated scores in alignment with human evaluations.\n- **Reference-free metrics (USR, DirectEval)**: Show positive correlations with human judgments.\n- **PairEval**: Exhibits a strong correlation with human evaluations and closely aligns with human scores in the selected examples.\n\n#### Example Evaluation on Meta-evaluation Datasets:\n\n1. **Example 1:**\n   - **History**: Dialogue about children's books and Amazon.\n   - **Human Score**: 3.0\n   - **PairEval Rank**: 0.65\n   - **Better correlation**: Scores and ranks like human judgments compared to others.\n\n2. **Example 2:**\n   - **History**: Dialogue about company equipment.\n   - **Human Score**: 3.9\n   - **PairEval Rank**: 0.2\n   - **Stronger correlation**: Compared to baselines, PairEval aligns better with human scores.\n\nIn summary, the setup assesses the capability of various metrics to emulate human judgments using multiple meta-evaluation datasets. The results highlight that PairEval achieves a closer and more reliable alignment with human evaluations compared to other metrics."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of different configurations for comparison examples in the PairEval metric by altering the number and types of comparison examples used.",
            "experiment_process": "The study analyzed the impacts by (1) reducing the number of comparison examples and (2) using a few examples in the target meta-evaluation dataset instead of a dialogue corpus. The experiments utilized finetuned language models, and the results were averaged over five runs with different comparison examples. Evaluation metrics included correlation with human judgments, Pearson, and Spearman correlations. Results were shown in Tables 3 and 4, involving datasets such as ConvAI2-USR and benchmarks like DailyDialog and TopicalChat.",
            "result_discussion": "The study found that the increased number of comparison examples generally led to better correlation with human judgments. Using examples in meta-evaluation datasets usually provided better performance, although access to these test examples might not always be available. Therefore, using randomly sampled dialogue corpus examples was seen as a reasonable alternative. Exhaustive comparison between all evaluation examples in a meta-evaluation dataset contributed to increased correlation; for instance, correlations in the ConvAI2-USR dataset increased from 66.5 and 71.1 to 70.0 and 71.9 in Pearson and Spearman correlation, respectively. While exhaustive comparisons boosted performance, using a few comparison examples was deemed efficient.",
            "ablation_id": "2404.01015v1.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of human-written adversarial negative examples in finetuning the PairEval metric's underlying language models.",
            "experiment_process": "The study compared the performance of PairEval when finetuned with and without human-written hard negative responses, replacing hard negatives with random ones to keep the number of training examples consistent. Results were presented in Table 6, and the performance of the metric was assessed by its correlation with human evaluations.",
            "result_discussion": "The results confirmed that adversarial examples generally increased the correlation of PairEval with human evaluations. However, the creation of adversarial responses involved human annotation, which is not always available and not scalable. As an alternative, generating datasets with large language models (LLMs) can be efficient and effective, though further exploration in this direction was left for future work.",
            "ablation_id": "2404.01015v1.No2"
        }
    ]
}