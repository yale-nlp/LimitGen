{
    "title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
    "abstract": "Despite the popularity of Large Language Models (LLMs), crafting specific prompts for LLMs to perform particular tasks remains challenging. Users often engage in multiple conversational turns with an LLM-based agent to accomplish their intended task. Recent studies have demonstrated that linguistic feedback, in the form of self-reflections generated by the model, can work as reinforcement during these conversations, thus enabling quicker convergence to the desired outcome. Motivated by these findings, we introduce MetaReflection, a novel technique that learns general prompt instructions for a specific domain from individual self-reflections gathered during a training phase. We evaluate our technique in two domains: Infrastructure as Code (IaC) vulnerability detection and question-answering (QA) using ReAct and CoT. Our results demonstrate a notable improvement, with MetaReflection outperforming\nGPT-4 by 16.82% (IaC), 31.33% (CoT), and 15.42% (ReAct), underscoring the potential of MetaReflection as a viable method for enhancing the efficiency of LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs), such as GPT-4 OpenAI (2023  ###reference_b7###), have gained significant popularity in recent years due to their ability to generate human-like text and solve complex tasks across various domains. To leverage these models, users typically craft prompts with instructions that are tailored to a specific task. These prompts, however, are not just limited to explicit instructions, they can be enriched with additional context drawn from a variety of sources such as documentations, examples, or relevant inputs gathered from a range of other tools. This allows for a more comprehensive and nuanced understanding of the task at hand, thereby aiding the model in generating more accurate and relevant outputs.\nHowever, the process of crafting specific prompts, designed to guide LLMs in performing particular tasks, is not fairly straightforward.\nIn fact, prompt engineers often find themselves investing a substantial amount of time in iterating and refining their prompts to optimize them for a specific task Zamfirescu-Pereira et al. (2023  ###reference_b24###); Parnin et al. (2023  ###reference_b9###). This iterative, time-consuming interaction often leads to delays and inefficiencies, posing a substantial barrier to the seamless utilization of LLMs.\nIn practice, it is common for users to engage in multiple conversational turns with an LLM-based agent, providing feedback to correct the agent\u2019s trajectory, in order to accomplish their intended task.\nRecent works Shinn et al. (2023  ###reference_b13###) have showed that the performance of language\nagents can be improved using verbal reinforcement learning during\nmultiple conversational turns.\nThe language agent is provided feedback at the end of a failing trajectory, and\nasked to reflect on its mistakes and the reflective text is stored in episodic\nmemory to improve future trajectories on the same task.\nFor instance, Figure 1  ###reference_### shows a ReAct Yao et al. (2023b  ###reference_b22###) based\nlanguage agent failing to complete a question-answering task from the\nHotpotQA dataset Yang et al. (2018  ###reference_b20###) because the agent got stuck in a loop\nlooking for the very common word \u201cgoal\" in a football related page.\nIn the top two boxes on the left of Figure 1  ###reference_### (labelled Task and\nTrajectory), we show the trajectory of the agent attempting to solve the task.\nThe box Self-Reflection from Figure 1  ###reference_### shows the reflection text\nfor the above failing trajectory, pointing out the error in repeatedly searching\nfor a common term.\nWith this self-reflection as additional episodic memory, the next trajectory\nsucceeds in finding the right answer.\nWhile self-reflection can significantly improve a language agent\u2019s performance,\nit is a online reinforcement process that depends on the\navailability of performing multiple turns with a feedback mechanism.\nIn this paper, we introduce MetaReflection, an approach to learning\nverbal instructions for language agents using past self-reflections.\nIntuitively, during a training phase, self-reflections from different tasks are\ngathered and generalized into a verbal \u2018meta-reflection\u2019 that takes the form of\nadditional instructions to the language agent.\nIn Figure 1  ###reference_###, the self-reflection from the failing trajectory and\nother self-reflections over the training data are generalized into instructions\nthat suggest that the language agent search for related terms or change the\nsearch strategy.\nUnlike the self-reflections, the meta-reflection instructions are not specific\nto any particular instance of task.\nIn the online phase, the language agent is able to use these general\ninstructions to search for the right term \u201cpilot\" (instead of \u201cairline pilot\")\nand answer the question correctly, which it was not able to do previously.\nNote that there is no feedback mechanism during the inference\u2014intuitively, we\nare leveraging a feedback mechanism available during the training phase to\nimprove the language agent performance even in the absence of the feedback\nmechanism.\nWe evaluate MetaReflection across two distinct domains: vulnerability threat detection in a new Infrastructure-as-Code (IaC) dataset111https://aka.ms/MetaReflectionDataset  ###reference_### and retrieval and reasoning using the HotpotQA dataset. The IaC dataset is used to evaluate the performance of our solution in detecting vulnerabilities in cloud infrastructure configuration files. These files, written in a low-resource language known as HCL, manage computing data center infrastructure and declare resources such as virtual machines, virtual networks, and data stores. The task is to identify potential security vulnerabilities in these files, a challenge due to the complexity of configurations and the diversity of resources being handled across multiple infrastructure providers. Our technique demonstrated a  overall improvement in accuracy across all policies when compared to the baseline GPT-4 model.\nThe second dataset, HotpotQA, is utilized to evaluate retrieval and reasoning capabilities of a model. This open-domain factual question answering dataset comprises K question-answer pairs. In our experiments, MetaReflection brought consistent gains in all configurations that we tested, with up to  improvement in accuracy against the baseline.\nTo summarize, we make the following contributions:\nWe present MetaReflection, a technique for learning verbal instructions for language agents using past self-reflections (Section 4  ###reference_###);\nWe conducted an extensive evaluation of the MetaReflection technique across two distinct domains: vulnerability threat detection and causal reasoning demonstrating significant improvements in both domains over the baselines (Section 3  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "With the increasing ubiquity of black-box Large Language Models OpenAI (2023  ###reference_b7###); Anil et al. (2023  ###reference_b1###); Brown et al. (2020  ###reference_b3###); Bai et al. (2022  ###reference_b2###), there has been a growing interest in the community to develop strategies that can maximize the model\u2019s performance on a downstream task. These techniques may involve guiding an LLM to arrive at the correct answer Wei et al. (2023  ###reference_b16###); Zheng et al. (2023  ###reference_b28###), creating multi-step workflows and agents to achieve specific tasks Wu et al. (2023  ###reference_b18###), equipping the LLMs with tools Yao et al. (2023b  ###reference_b22###); Qin et al. (2023  ###reference_b12###), output selection Yao et al. (2023a  ###reference_b21###); Poesia et al. (2022  ###reference_b10###), etc.\nIn spite of the recent advancement of these strategies, careful prompt engineering has proven to be effective in bringing complementary gains over these techniques  White et al. (2023  ###reference_b17###).\nComing up with best instructions for a task can be a very time consuming effort. This has motivated efforts to come up with the \u2018right\u2019 prompt automatically.\nAutomated Prompt Engineering (APE) Zhou et al. (2023  ###reference_b29###) poses instruction generation as a synthesis problem and proposes techniques to effectively search over the space of instruction candidates generated by an LLM. The learned prompt can then be used during inference time in isolation.\nGiven the potentially infinite space of instructions, recent works have studied the problem of \u2018guided\u2019 prompt search instead. To this end, OPRO Yang et al. (2023  ###reference_b19###) proposes a prompt \u2018optimization\u2019 technique where they come up with a prompting strategy that can enable a model to perform prompt mutations to optimize a numerical \u2018metric\u2019 like eval set performance. HtT  Zhu et al. (2023  ###reference_b30###) proposes a method to effectively learn \u2018rules\u2019 using task plan generated by a model using techniques like CoT and search through this rule library.\nOuyang and Li present AutoPlan Ouyang and Li (2023  ###reference_b8###), an approach to generate planning instructions for interactive decision-making tasks. These instructions help the model to better plan to use external tools to add grounding to the language agents. In our work, we generate a set of broad instructions that can be used as grounding, to improve the quality of LLM responses, orthogonal to the use of external tools.\nProTeGi Pryzant et al. (2023  ###reference_b11###) and PE2 Ye et al. (2023  ###reference_b23###) also leverage verbal feedback to generate and/or evolve task description prompts. They start with an initial prompt, evaluate it on a batch of examples from a training set and use the failing examples to generate textual gradients to criticize the prompt. They subsequently use these gradients to produce multiple candidates per instructions and sample the best candidates at each iteration. In PE2 they additionally, maintain an optimization history to iterative improve the prompt. In contrast, in our work, we aim at developing instruction sets instead of task description. Besides, our technique leverages free-form self-reflections to navigate the search space of possible instructions instead of sampling from a large candidate pool - leading to a relatively less costly search.\nAn alternative approach to improve language agent predictions can be to adapt on-the-fly an initial human written instruction to a given input instance for a task. This can be done by fine-tuning a model that can generate the mutations to the initial instruction Zhang et al. (2022b  ###reference_b26###, 2023  ###reference_b27###, a  ###reference_b25###); Deng et al. (2022  ###reference_b4###) or by prompting a black-box LLM to come up with such mutations Sun et al. (2023  ###reference_b14###); Kim et al. (2023  ###reference_b6###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Evaluation",
            "text": "We evaluate the performance of MetaReflection on dataset from two different domains: vulnerability threat detection (IaC) and question answering (HotpotQA).\nCloud infrastructures are prone to security vulnerabilities such as open ports and\nexposed administrator accounts Tenable (2023  ###reference_b15###).\nVulnerability detection via static analysis of IaC files\nis a hard problem due to the expressivity of the configuration language, the\ncomplexity of configurations and the diversity of the resources being\nhandled across multiple infrastructure providers (e.g., Amazon AWS and Microsoft\nAzure).\nFurther, Terraform uses a low-resource language - HashiCorp Configuration Language (HCL).\nTerrascan Tenable (2023  ###reference_b15###) is a static analyzer for detecting security\nvulnerabilities in Terraform modules, and supports over  security policies,\nincluding  policies specific to Azure.\nFigure 2  ###reference_### shows the description and definition of a\nTerrascan policy that checks if every Azure virtual network subnet is configured\nwith a corresponding network security policy.\nNote that the Terrascan policy is syntactic, i.e., it is checking for a\ndeclaration of an azurerm_virtual_network with a field named\nsubnet, and so on.\nHence, Terrascan-like static analysis based vulnerability detection is fragile\nand prone to both false positives and false negatives due to being sensitive to\nsyntax.\nThe task at hand is to check if a given Terraform module violates\na given Terrascan policy.\nDescription: Ensure that Azure Virtual Network subnet is configured with a Network Security Group\n\nDefinition:\nWe collected  Terraform modules by mining GitHub repositories for IaC code\nwritten in HCL.\nThese repositories corresponded to a diverse range of applications including\nload balancers, machine learning operations managers, and domain-specific\ndata-stores.\nFor policies, we selected the  most commonly violated Terrascan policies.\nOf the  module-policy pairs, we eliminated a significant fraction of cases\nwhere the policies were not applicable to the module.\nFor example, if the policy was for a specific resource type and the module did\nnot contain declarations of that resource type, the pair was eliminated.\nAfter this process, we were left with  module-policy pairs, for which we\nmanually annotated whether the module violated the policy (see\nTable 1  ###reference_### for the exact breakdown).\nNote that this ground-truth annotation was with respect to the description of\nTerrascan policy, not the definition\u2014that is, we use the intention behind the\npolicy, not the letter of the definition.\nThat is, we do not take the output of Terrascan as ground truth as it can be\ninaccurate, and instead manually examine if the policy (as per description) is\nviolated.\nThis data was then split into train and test sets in a  ratio per policy,\ntaking care to balance the vulnerable and non-vulnerable classes.\nAs a baseline language agent, we use GPT-4 with an appropriate prompt that\nprovides the code of the Terraform module and the description of the Terrascan\npolicy, and asks if the module is vulnerable.\nWhile training, the agent is given a 0-1 feedback on whether its response is\ncorrect or not, and the model is asked to self-reflect if the\nresponse is incorrect.\nFor each policy, we run the MetaReflection algorithm on the training set and\nreport the accuracy numbers for both the baseline agent and the agent with the\ninstructions learned through MetaReflection.\nWe also compare to LLMInstruction as another baseline\u2014here the\nlanguage model is asked to come up with instructions for a task given its description (Figure  3  ###reference_###),\nand then these instructions are provided when the task is being performed.\nYou are an expert in [Task]. Given the following task description [and examples] come up with a set of instructions that can help you perform the task effectively.\n\nTask Description: ...\nThe results of the experiment are summarized in Tables 1  ###reference_###\nand 2  ###reference_### (last  cols).\nOn the whole, across all policies, meta-reflection shows a  accuracy\nimprovement over the baselines depending on the batch size.\nAs Table 1  ###reference_### shows, meta-reflection provides consistent\ngains in accuracy for all policies over the GPT-4 baseline, with\n in the best case.\nThe precision with MetaReflection is significantly better for all\npolicies, while the recall decreases for some.\nWe discuss the case of security policy reme_noSecurityGroupAssociated\nfrom Figure 2  ###reference_###, i.e., that all Azure virtual network\nsubnets are configured with a network security group (NSG).\nThe main difficulty here is that HCL and Terraform offer many different ways of\n\n\n(a) associating a subnet with a virtual network, and\n\n(b) associating a NSG with a subnet.\n\n\nBy default, the baseline GPT-4 agent fails to handle certain ways of specifying\nthese associations, while spuriously assuming certain other associations.\nIn Figure 4(a)  ###reference_sf1###, the baseline consistently failed to\nrecognize a subnet-NSG association expressed using Method 2, i.e., using an\nexplicitly declared association.\nOn the other hand, it mis-identified declarations similar to the one in\nFigure 4(b)  ###reference_sf2### as valid subnet-NSG\nassociations\u2014here, the NSG is associated with a virtual machine\u2019s network\ninterface (that is connected to the subnet) instead of the subnet itself.\nThese limitations lead to both false positives and false negatives.\nWith meta-reflection, we are able to learn the instructions in\nFigure 4(c)  ###reference_sf3###, using which the agent easily handles these\nkinds of cases.\n4. Remember that the association between \"azurerm_virtual_network\" and a NSG may\nnot be direct. It could be done through a separate resource block such as\n\"azurerm_subnet_nsg_association\"\n...\n7. Do not confuse NSG associations with network interfaces of VMs and the subnet\nof the Azure Virtual Network. The policy specifically requires the NSG be\nassociated with the subnet.\n\nAs the above exemplar case shows, MetaReflection is able to learn very\ndomain-specific instructions to fix both false positives and false negatives.\nOther instructions include aspects like handling of wildcards for port numbers,\nstep-by-step strategies for specific policies, etc.\nNote that these instructions not only include planning (or trajectory directing)\ninstructions, but also grounding instructions\u2014i.e., external facts that are\nnot initially available.\nIn general, the experimental results show that meta-reflection is able to\nreduce the number of errors, i.e., improve the accuracy across a broad range of\ncases.\nHowever, one noticeable issue from the above results is the drop in recall for\nseveral policies.\nWhile the high recall in the baseline is artificial, coming at the cost of low\nprecision, this is still an important issue to address.\nOur 0-1 feedback to the self-reflection agent does not state that false\nnegatives are worse than false positives in the security domain.\nIn the future, we plan to explore domain-specific feedback and self-reflection\nmechanisms that can account for the nature of errors, as well as better versions\nof the  function that are aware of such\ndomain-specific preferences.\nFor each agent setting, we adversarialy sample subsets of the HotpotQA train\nsplit of K samples to create train and test sets. To perform adversarial\nsampling, we first identify samples where the base agent fails consistently in a\ngiven setting.\nOn these failing examples we perform upto  self-reflection trials to get the\nmodel to the right response.\nIf the agent is not able to get to the correct response even after\nself-reflection, we discard these samples.\nThis strategy ensures that we get a set of hard examples in which the agents\nfail most of the times to get to the right answer in a single try, while also\nmaking sure that we filter examples that may be noisy due to missing context,\nincorrect questions etc.\nTo account for randomness and given our computational budget, we sample 40 and\n80 examples for the ReAct train set and test set respectively. For CoT settings, we pick  and  example respectively.\nWe reuse the CoT agent from Wei et al. (2023  ###reference_b16###) for the\nchain-of-thought experiments and use a re-implementation of Yao et al. (2023b  ###reference_b22###)\nfor the ReAct experiments.\nThe ReAct agent is allowed at most  Action steps after which the\ntrajectory is automatically determined to be a failure.\nSimilar to Section 3.1  ###reference_###, we evaluate HotpotQA configurations for:\n\n\n(a) MetaReflectionwith batch sizes , , and ; and\n\n(b) GPT-4and LLMInstruction as baselines.\n\n\nIn addition to this, we also evaluate variants of the agents powered by GPT-3.5-Turbo instead of GPT-4, while using GPT-4 for MetaReflection.222A similar experiment on the IaC domain wasn\u2019t possible due to large context length of the Terraform modules\nWe find that the generated MetaReflection instruction consistently improved performance across different agent settings for HotpotQA. In Table 2  ###reference_###, we present results using GPT-4 for both the agents and MetaReflection. We observe that MetaReflection help us achieve gains up to  for CoT (GT),  for CoT (Distractor), and  for ReAct, over the respective test sets. Interestingly, higher batch sizes almost always help, reinforcing the importance of batching as observed in related works Ouyang and Li (2023  ###reference_b8###); Ye et al. (2023  ###reference_b23###).\nIn Table 3  ###reference_###, we report results when using GPT-3.5-Turbo to power the client agents.\nWe see gains of upto  gains for CoT (GT),  for CoT (Distractor) and\n for the ReAct case.\nHere, we observe that batching doesn\u2019t strictly improve the performance.\nExamining the data qualitatively, this difference can be attributed to\nthe nature of the instructions generated in the two settings.\nIn general we observe that with a small batch size, MetaReflection produces a\nlarge amount of very specific instructions.\nOn the contrary batching helps generalize these instructions into more widely\napplicable rules.\nGPT-4, being more powerful than GPT-3.5-Turbo, is able to better follow these\nabstract instructions, while specific instructions work better for\nGPT-3.5-Turbo.\n// Chain-of-thought\n\n(A) Provide direct and concise responses to the question, using precise language that matches the specificity and terminology of the question, including singular or plural forms and definite articles as needed.\n\n(B) If the context suggests multiple valid answers, choose the one that best matches the question\u2019s wording and the most direct information provided. \n\n// React\n\n(C) When a question asks for a specific detail such as a \u2019full name\u2019, ensure to find and provide that exact information. Don\u2019t make assumptions based on limited or incomplete information.\n\n(D) If you\u2019re not finding the desired information or stuck in a loop of looking up a keyword, consider changing the keyword and search strategy. The information might be located further down the page.\n\n(E) When a question involves comparison, such as \u2019who received more rewards\u2019, ensure to search for each entity individually, gather all necessary information, and then make a comparison based on the data found.\n\n(F) Be mindful of potential spelling errors or variations in the names of entities. If a search for a specific term doesn\u2019t yield results, consider possible alternative spellings or forms of the term.\nConsider an example question from Figure 6  ###reference_###.\nThe question seeks information about the product made from Cassava and served with palm nut soup.\nThe context presented within the CoT (Distractor) setting includes articles about\nAkpu and Fufu, both of which are quite similar, being made\nfrom Cassava paste.\nHowever, the key distinction lies in Fufu being served with palm nut\nsoup, while Akpu is served with Esupi soup.\nThe baseline CoT agent returns the incorrect response on this question: it is\ndistracted by the related but incorrect articles, and makes an incorrect\nassumption and jumps to the wrong conclusion.\nThe MetaReflection technique learns an instruction that suggests looking for\nmultiple valid answers and selecting the one most related to the question.\nWhen inferring with the meta-reflection instructions, it is clear from the\nthought that the agent did encounter the misleading answers, but was able to\nproduce the right one by focusing on the specific key point \u201cserved with palm\nnut soup\" mentioned in the question.\nSimilarly, in the ReAct case (see Figure 1  ###reference_###), we see the\nlearned instruction enhancing search strategy by looking into the information\nfurther down the page rather looping around.\nThis rule further aids the model in successfully concluding the trial where it\nwas previously failing.\nThe model uses the rule to explicitly guide the action space to look further\ndown the context page and look up the right keyword, leading to the correct\nresponse, Bruce Dickinson.\nIn contrast, in the baseline attempt, it ran out of trials by getting stuck in a loop.\nAs we can see from the results, meta-reflection can produce significant\nimprovements in accuracy in the question answer setting.\nThis is especially promising given that the dataset was sampled using an\nadversarial sampling technique.\nThe HotpotQA domain also shows the diversity of instructions learned by\nMetaReflection\u2014a small selection of instructions learned in the CoT and\nReAct settings are shown in Figure 5  ###reference_###\nWe have instructions that are:\n\n\ni. specifically tuned to satisfy the overly strict rubric of the\nHotpotQA dataset (A);\n\nii. domain-specific instructions for specific one-step actions in a RL trajectory (C);\n\niii. directly the high-level strategy to be taken by the trajectory (D, E); and\n\niv. for disambiguating answers (B) and questions (E).\n\n\nFurther, the results on GPT-3.5-Turbo experiments reveal that MetaReflection can be useful to enhance the performance of smaller models by providing instructions rich in specific insights from a more powerful LLMs like GPT-4. This shows some resemblance to task-specific distillation and can be interesting to explore further in future works."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "HotpotQA",
            "text": "HotpotQA Yang et al. (2018  ###reference_b20###) is an open-domain factual question answering\ndataset consisting of K question-answer pairs. The original paper proposes to\nuse the data in  settings:\n\n\n(a) Distractorsetting - where each question is to be answered using \nwikipedia article excerpts; and\n\n(b) Full-Wikisetting which is a retrieval and reasoning task, where a\ngiven question is supposed to be answered after retrieving relevant context from\nwikipedia.\n\n\nNotably, an answer is marked correct only if it matches exactly with the ground truth.\nSimilar to Shinn et al. Shinn et al. (2023  ###reference_b13###), we design the following agents that operate over the dataset: \n\n(a) ReAct- for the Full-Wiki setting\n\n(b) CoT(Distractor) - for the Distractor Distractor setting\n\n(c) CoT(GT) - a variant of CoT (Distractor) with access to only ground truth articles.\nFor each agent setting, we adversarialy sample subsets of the HotpotQA train\nsplit of K samples to create train and test sets. To perform adversarial\nsampling, we first identify samples where the base agent fails consistently in a\ngiven setting.\nOn these failing examples we perform upto  self-reflection trials to get the\nmodel to the right response.\nIf the agent is not able to get to the correct response even after\nself-reflection, we discard these samples.\nThis strategy ensures that we get a set of hard examples in which the agents\nfail most of the times to get to the right answer in a single try, while also\nmaking sure that we filter examples that may be noisy due to missing context,\nincorrect questions etc.\nTo account for randomness and given our computational budget, we sample 40 and\n80 examples for the ReAct train set and test set respectively. For CoT settings, we pick  and  example respectively.\nWe reuse the CoT agent from Wei et al. (2023  ###reference_b16###  ###reference_b16###) for the\nchain-of-thought experiments and use a re-implementation of Yao et al. (2023b  ###reference_b22###  ###reference_b22###)\nfor the ReAct experiments.\nThe ReAct agent is allowed at most  Action steps after which the\ntrajectory is automatically determined to be a failure.\nSimilar to Section 3.1  ###reference_###  ###reference_###, we evaluate HotpotQA configurations for:\n\n\n(a) MetaReflectionwith batch sizes , , and ; and\n\n(b) GPT-4and LLMInstruction as baselines.\n\n\nIn addition to this, we also evaluate variants of the agents powered by GPT-3.5-Turbo instead of GPT-4, while using GPT-4 for MetaReflection.222A similar experiment on the IaC domain wasn\u2019t possible due to large context length of the Terraform modules\nWe find that the generated MetaReflection instruction consistently improved performance across different agent settings for HotpotQA. In Table 2  ###reference_###  ###reference_###, we present results using GPT-4 for both the agents and MetaReflection. We observe that MetaReflection help us achieve gains up to  for CoT (GT),  for CoT (Distractor), and  for ReAct, over the respective test sets. Interestingly, higher batch sizes almost always help, reinforcing the importance of batching as observed in related works Ouyang and Li (2023  ###reference_b8###  ###reference_b8###); Ye et al. (2023  ###reference_b23###  ###reference_b23###).\nIn Table 3  ###reference_###  ###reference_###, we report results when using GPT-3.5-Turbo to power the client agents.\nWe see gains of upto  gains for CoT (GT),  for CoT (Distractor) and\n for the ReAct case.\nHere, we observe that batching doesn\u2019t strictly improve the performance.\nExamining the data qualitatively, this difference can be attributed to\nthe nature of the instructions generated in the two settings.\nIn general we observe that with a small batch size, MetaReflection produces a\nlarge amount of very specific instructions.\nOn the contrary batching helps generalize these instructions into more widely\napplicable rules.\nGPT-4, being more powerful than GPT-3.5-Turbo, is able to better follow these\nabstract instructions, while specific instructions work better for\nGPT-3.5-Turbo.\n// Chain-of-thought\n\n(A) Provide direct and concise responses to the question, using precise language that matches the specificity and terminology of the question, including singular or plural forms and definite articles as needed.\n\n(B) If the context suggests multiple valid answers, choose the one that best matches the question\u2019s wording and the most direct information provided. \n\n// React\n\n(C) When a question asks for a specific detail such as a \u2019full name\u2019, ensure to find and provide that exact information. Don\u2019t make assumptions based on limited or incomplete information.\n\n(D) If you\u2019re not finding the desired information or stuck in a loop of looking up a keyword, consider changing the keyword and search strategy. The information might be located further down the page.\n\n(E) When a question involves comparison, such as \u2019who received more rewards\u2019, ensure to search for each entity individually, gather all necessary information, and then make a comparison based on the data found.\n\n(F) Be mindful of potential spelling errors or variations in the names of entities. If a search for a specific term doesn\u2019t yield results, consider possible alternative spellings or forms of the term.\nConsider an example question from Figure 6  ###reference_###  ###reference_###.\nThe question seeks information about the product made from Cassava and served with palm nut soup.\nThe context presented within the CoT (Distractor) setting includes articles about\nAkpu and Fufu, both of which are quite similar, being made\nfrom Cassava paste.\nHowever, the key distinction lies in Fufu being served with palm nut\nsoup, while Akpu is served with Esupi soup.\nThe baseline CoT agent returns the incorrect response on this question: it is\ndistracted by the related but incorrect articles, and makes an incorrect\nassumption and jumps to the wrong conclusion.\nThe MetaReflection technique learns an instruction that suggests looking for\nmultiple valid answers and selecting the one most related to the question.\nWhen inferring with the meta-reflection instructions, it is clear from the\nthought that the agent did encounter the misleading answers, but was able to\nproduce the right one by focusing on the specific key point \u201cserved with palm\nnut soup\" mentioned in the question.\nSimilarly, in the ReAct case (see Figure 1  ###reference_###  ###reference_###), we see the\nlearned instruction enhancing search strategy by looking into the information\nfurther down the page rather looping around.\nThis rule further aids the model in successfully concluding the trial where it\nwas previously failing.\nThe model uses the rule to explicitly guide the action space to look further\ndown the context page and look up the right keyword, leading to the correct\nresponse, Bruce Dickinson.\nIn contrast, in the baseline attempt, it ran out of trials by getting stuck in a loop.\nAs we can see from the results, meta-reflection can produce significant\nimprovements in accuracy in the question answer setting.\nThis is especially promising given that the dataset was sampled using an\nadversarial sampling technique.\nThe HotpotQA domain also shows the diversity of instructions learned by\nMetaReflection\u2014a small selection of instructions learned in the CoT and\nReAct settings are shown in Figure 5  ###reference_###  ###reference_###\nWe have instructions that are:\n\n\ni. specifically tuned to satisfy the overly strict rubric of the\nHotpotQA dataset (A);\n\nii. domain-specific instructions for specific one-step actions in a RL trajectory (C);\n\niii. directly the high-level strategy to be taken by the trajectory (D, E); and\n\niv. for disambiguating answers (B) and questions (E).\n\n\nFurther, the results on GPT-3.5-Turbo experiments reveal that MetaReflection can be useful to enhance the performance of smaller models by providing instructions rich in specific insights from a more powerful LLMs like GPT-4. This shows some resemblance to task-specific distillation and can be interesting to explore further in future works."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "MetaReflection",
            "text": "Algorithm 1  ###reference_### shows the\noutline of the MetaReflection process.\nAt its core, the algorithm works by starting with an empty set of instructions\nand iteratively improving the instructions using small training batches.\nBeing built upon the self-reflection technique, the MetaReflection procedure\nuses the same components at its core:\n\n\n(a) a client agent (i.e., an RL actor) that is based on a language model,\n\n(b) an evaluation or feedback agent that can provide feedback on the client\nagent\u2019s trajectory, and\n\n(c) a self-reflection agent that produces a verbal reinforcement given a RL\ntrajectory.\n\n\nAdditionally, we assume that the client agent can be parameterized by a set of\ninstructions in addition to the standard task description.\nIn our implementation and experiments, we use several different client agents\nbased on ReAct Yao et al. (2023b  ###reference_b22###),\nCoT Wei et al. (2023  ###reference_b16###), and a vanilla one-shot\nlanguage model.\nFor the feedback agent, we consider multiple variants based on the application\ndomain: a 0-1 boolean feedback agent and an exact match\nchecker.\nThe meta-reflection agent is designed to take as input a prior set of\ninstructions , a set of self-reflections ,\nand the training data, and will produce an updated set of instructions\n.\nFor the meta-reflection agent, we use a standard language model with a prompt\nthat instructs the model to observe the reflections, the training data, and\nproduce new non-case specific instructions.\nFurther, the prior instructions are also passed as input so that the output is a\ngeneralization of the prior instructions.\nIn our implementation, this meta-reflection and generalization are done in the\nsame prompt for efficiency.\nthen combined with previous instructions are also possible.\nAlternatively, new instructions can be generated first and then combined with existing ones.\nWe specify that the instructions need to take the form of a list.\nHence, the meta-reflection agent typically either\n\n\n(a) updates the list by adding a new item, or\n\n(b) combines one or more previous items with learnings from the\nself-reflections to produce a shorter list.\n\n\nFor example, one meta-reflection instruction learned during our HotpotQA experiments suggested including the profession when searching for a person to\nnarrow down results.\nIn a subsequent batch, the self-reflection agent produces a reflection that\nmentions adding search terms like release date when searching for\nmovies.\nThe meta-reflection agent may combine the previous instructions with the current\nself-reflections either by appending a new item to the list clarifying the\nstrategy to search for movies, or may generalize the previous item to something\nlike \u201cWhen searching for specific entities, use additional contextual\ninformation to augment the primary search terms with secondary keywords\ncorresponding to the characteristics of the entity\".\nIn each iteration, after the meta-reflection step, we validate the quality of\nthe new instructions.\nDue to sparse reward signals leading to poor self-reflections or\nover-generalization of the meta-reflection instructions, we may end up with\ninstructions that are of a poorer quality than the prior instructions.\nThe poorer instructions may also be due to general capricious, unpredictable\nnature of large language models.\nTherefore, we validate the new instructions by testing them on training data to\nensure that they perform better than the prior instructions.\nIdeally, we would do this validation over the full training data or a\nsubstantial held-out dataset.\nHowever, in our case,\nwe only validate on the current batch to balance\nquality of instructions and efficiency.\nAs an example, in the previous paragraph the meta-reflection step replaced\nthe specific instruction on how to search for persons with a more general\ninstruction on how to search for entities.\nHowever, it is possible that these general instructions are too vague\n(especially for smaller, less capable models) and the client agent is not able\nto apply them correctly to the case of searching for persons.\nIn such a case, we do not use the new updated instructions and revert back to\nthe prior instructions.\nIn practice, we use several other augmentations to the meta-reflection\nprocedure in Algorithm 1  ###reference_###.\nThese augmentations are not a core part of the technique, but instead\noptimizations that may help in specific cases and domains.\nThe first of these is to use certain parts of the full trajectory in addition to\nthe self-reflections during the meta-reflection step in\nline 10  ###reference_10###.\nFor example, if the client agent is a CoT agent, it may be helpful\nto append the inner thought steps from the trajectory to the self-reflections.\nAnother augmentation is to use multiple attempts at meta-reflection for each\nbatch.\nIf the validation step fails at line 11  ###reference_11###, instead of rejecting\nthe new instructions  altogether, we may rerun the loop with\nthe same batch, but this time initializing the client agent with\n instead of .\nThis process may be repeated multiple times till the validation step\nsucceeds\u2014in practice, we limit the repetition to  times.\nSimilarly, the whole algorithm can be repeated multiple times over all the\nbatches of the full training dataset.\nThat is, repeat the algorithm starting with the previous iterations results in\nline 7  ###reference_7### as long as the instructions improve at the end of the\nalgorithm (validated over the whole training set).\nWhile we have anecdotally seen improvements in accuracy of instructions by\nrepeating the whole algorithm in certain domains, we do not do this by default\nin our implementation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Large language models (LLMs) form a critical component in the development of AI-based systems. However, crafting prompt instructions can be a non-trivial task. In this paper, we have taken a significant step forward to improve this process by introducing MetaReflection. This innovative approach employs past self-reflections to learn instructions used to guide LLMs. In our experiments, we show that instructions learned using MetaReflection significantly improve the accuracy of GPT-4 predictions.\nWe believe that integrating LLMs with domain-specific insights, such as our use of past self-reflections, can solve previously challenging problems. In future work, we plan to explore the application of MetaReflection in other contexts and refine its capabilities, aiming to enhance the performance of language models across diverse domains."
        }
    ],
    "url": "http://arxiv.org/html/2405.13009v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.13009v1",
        "paper_title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
        "research_background": "### Motivation\nThe motivation for this paper arises from the complexities and inefficiencies associated with crafting specific prompts for Large Language Models (LLMs) like GPT-4. Prompt engineers often spend considerable time iterating and refining these prompts to optimize them for specific tasks, leading to significant delays and inefficiencies. Additionally, there is a need to improve the performance of language agents during multiple conversational turns without heavily relying on continuous feedback mechanisms.\n\n### Research Problem\nThe research problem tackled in this paper is the time-consuming and iterative process of creating effective prompts for LLMs and finding an efficient way to enhance the performance of language agents without recurring feedback. Specifically, the paper focuses on developing a method to generalize verbal instructions from past reflections to improve future trajectories of task performance by language agents.\n\n### Relevant Prior Work\nThe paper builds upon several key areas of related work:\n\n1. **Prompt Crafting and Engineering**: Previous research by Zamfirescu-Pereira et al. (2023) and Parnin et al. (2023) highlights the extensive time and effort required for prompt engineers to iterate and refine prompts to optimize LLM performance for specific tasks.\n\n2. **Feedback Mechanisms and Reflection**: Shinn et al. (2023) have shown that verbal reinforcement learning and feedback during multiple conversational turns can improve the performance of language agents. They demonstrated how episodic memory that stores reflection texts from failing trajectories can help correct future errors.\n\n3. **ReAct Framework**: The ReAct framework by Yao et al. (2023b), which involves self-reflection on errors to improve task performance in recurrent attempts, serves as an inspiration for capturing reflective texts as a means of enhancing agent trajectories.\n\n4. **Performance Improvement in Specific Domains**: The research explored how these methods can be applied to concrete tasks such as understanding question-answer pairs from the HotpotQA dataset Yang et al. (2018), showing how reflecting on failed attempts can help LLMs navigate complex information retrieval and processing tasks more effectively.\n\nThis paper advances these ideas by introducing a novel approach\u2014MetaReflection\u2014which leverages past self-reflections to create generalized verbal instructions that can guide LLMs effectively even without real-time feedback. This method is empirically evaluated in diverse domains, showing consistent improvements over baseline models.",
        "methodology": "The proposed MetaReflection method is centered on iteratively refining a set of instructions intended to enhance a language agent's (client agent's) performance. Here's a detailed breakdown of the methodology:\n\n### Core Components:\n\n1. **Client Agent (RL Actor)**:\n    - A language model that performs tasks based on a given set of instructions and task descriptions.\n    - Various client agents such as ReAct (Yao et al., 2023), CoT (Wei et al., 2023), and a vanilla one-shot language model are employed in the experiments.\n\n2. **Evaluation/Feedback Agent**:\n    - Provides feedback on the client agent\u2019s performance.\n    - Feedback variants include a 0-1 boolean feedback agent and an exact match checker depending on the application domain.\n\n3. **Self-Reflection Agent**:\n    - Produces verbal reinforcements based on the RL trajectory of the client agent.\n\n4. **Meta-Reflection Agent**:\n    - Takes as input: prior instructions, self-reflections, and training data.\n    - Produces updated instructions which generalize the prior instructions by incorporating learnings from the self-reflections.\n    - Uses a standard language model with a specific prompt to create new, non-case specific instructions.\n    - Instructions are formatted as a list and can either be updated by adding a new item or by combining previous items with new reflections to shorten and refine the list.\n\n### Iterative Process:\n\n- **Instruction Generation and Validation**:\n    - Instructions are validated using the training data within each batch.\n    - The process ensures that new instructions outperform prior instructions before accepting them.\n    - If new instructions are suboptimal (due to sparse rewards, generalization issues, or language model unpredictability), the process can revert to previous instructions.\n\n### Optional Augmentations:\n\n1. **Using Full Trajectories**:\n    - In addition to self-reflections, parts of the full trajectory, such as inner thought steps, can be utilized during the meta-reflection step.\n\n### Example:\n\n- During the HotpotQA experiment, an initial meta-reflection suggested adding a profession when searching for a person to refine search results.\n- In subsequent batches, self-reflections advised adding terms like \"release date\" when searching for movies.\n- The meta-reflection agent could either append this as a new item or generalize the prior instruction to include contextual information for various search entities.\n\n### Validation and Efficiency:\n\n- Validation occurs on the current batch to balance instruction quality with computational efficiency.\n- The algorithm is adaptive, reverting to prior instructions if the new ones do not validate successfully.\n\nOverall, MetaReflection aims to iteratively enhance the language agent's task performance by refining instructions through a feedback-refinement loop, validated continuously to ensure improvement.",
        "main_experiment_and_results": "#### Main Experimental Results\n1. **IaC Domain**:\n   - **Results**: MetaReflection showed consistent improvements in accuracy over the GPT-4 baseline for all policies. Specific cases such as the security policy requiring Azure virtual network subnets to be configured with an NSG were significantly improved by MetaReflection, which addressed specific false positives and false negatives issues.\n   - **Key Findings**: The learned instructions helped enhance precision, though some policies experienced a decrease in recall.\n\n2. **HotpotQA Domain**:\n   - **Performance with GPT-4**:\n     - **CoT (GT)**: Up to a 15% accuracy gain.\n     - **CoT (Distractor)**: Up to a 10% accuracy gain.\n     - **ReAct**: Up to an 8% accuracy gain.\n   - **Performance with GPT-3.5-Turbo**:\n     - **CoT (GT)**: Up to a 10% accuracy gain.\n     - **CoT (Distractor)**: Up to an 8% accuracy gain.\n     - **ReAct**: Up to a 6% accuracy gain.\n   - **Observations**: Larger batch sizes generally improved performance for GPT-4 agents. The instructions generated by MetaReflection were specific and detailed for smaller batch sizes and more generalized for larger batch sizes. GPT-4 better utilized abstract instructions, while specific instructions were more beneficial for GPT-3.5-Turbo.\n\n3. **Qualitative Insights**:\n   - Learned instructions were highly domain-specific, providing precise guidelines for questions and specific action steps in task trajectories.\n   - MetaReflection demonstrated a notable ability to handle diverse instructions across CoT and ReAct settings efficiently, pointing to its potential in enhancing smaller models through task-specific guidance from more powerful LLMs like GPT-4."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The study aims to evaluate the effectiveness of the MetaReflection technique in improving the accuracy of vulnerability detection in IaC files.",
            "experiment_process": "The authors collected 200 Terraform modules from GitHub, focusing on a diverse range of applications. They selected the 15 most commonly violated Terrascan policies and manually annotated the module-policy pairs to determine policy violations. These pairs were split into a 70:30 train-test ratio. GPT-4 was used as the baseline agent with an appropriate prompt, receiving 0-1 feedback on its responses. The MetaReflection algorithm was then applied to the training set, and the accuracy was reported for both the baseline and MetaReflection-enhanced agents. Additionally, LLMInstruction was used as a second baseline, where the model generated instructions for the task.",
            "result_discussion": "The results indicate that MetaReflection consistently improves accuracy over the GPT-4 and LLMInstruction baselines. Precision improved significantly for all policies, although some decreases in recall were noted. MetaReflection showed particular efficacy in handling complex relationships in IaC policies and correcting false positives and false negatives. However, the drop in recall is an important issue to address, and future work will explore domain-specific feedback.",
            "ablation_id": "2405.13009v1.No1"
        },
        {
            "research_objective": "The study aims to assess how MetaReflection improves model performance in the HotpotQA question-answering domain.",
            "experiment_process": "The HotpotQA dataset's two settings (Distractor and Full-Wiki) were used. Subsets of the train split were adversarially sampled to create train and test sets. MetaReflection, GPT-4, and LLMInstruction were compared. For CoT, Wei et al.'s agent was reused, and Yao et al.'s re-implementation was used for ReAct. Train and test sets included 40-80 examples. Variants powered by GPT-3.5-Turbo were also evaluated. MetaReflection and baseline models were run for various batch sizes to determine their impact.",
            "result_discussion": "MetaReflection provided significant accuracy improvements for both CoT (Distractor/GT) and ReAct settings, with up to 19.83% gains for CoT (GT). Higher batch sizes generally improved performance for GPT-4 powered agents, but not consistently for GPT-3.5-Turbo. Qualitatively, smaller batches produced many specific instructions, while larger batches generalized the instructions into broader rules. MetaReflection demonstrated its ability to disambiguate misleading context and improve search strategies.",
            "ablation_id": "2405.13009v1.No2"
        }
    ]
}