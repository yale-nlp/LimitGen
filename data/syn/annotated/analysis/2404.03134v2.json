{
    "title": "Robust Pronoun Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?",
    "abstract": "Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these characteristics at a time.\nTo measure progress towards the combined goal, we introduce the task of pronoun fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later.\nWe present RUFF, a carefully-designed dataset of over 5 million instances to measure robust pronoun fidelity in English, and we evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters).\nWhen an individual is introduced with a pronoun, models can mostly faithfully reuse this pronoun in the next sentence, but they are significantly worse with she/her/her, singular they and neopronouns.\nMoreover, models are easily distracted by non-adversarial sentences discussing other people;\neven one additional sentence with a distractor pronoun causes accuracy to drop on average by 34%.\nOur results show that pronoun fidelity is neither robust, nor due to reasoning, in a simple, naturalistic setting where humans achieve nearly 100% accuracy.\nWe encourage researchers to bridge the gaps we find and to carefully evaluate reasoning in settings where superficial repetition might inflate perceptions of model performance.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Third-person pronouns (he, she, they, etc.) are words that construct individuals\u2019 identities in conversations (Silverstein, 1985  ###reference_b54###).\nIn English, these pronouns mark referential gender for the entity they are referring to,\nwhich can also index an individual\u2019s social gender, e.g., man, woman, non-binary (Cao and Daum\u00e9 III, 2020  ###reference_b9###).\nCorrectly using the pronouns an individual identifies with is important, as misgendering (including through incorrect pronoun use)\ncan in the best case be a social faux pas (Stryker, 2017  ###reference_b57###) and in the worst case, cause psychological distress, particularly to transgender individuals (McLemore, 2018  ###reference_b41###).\nAccordingly, it is important for large language models (LLMs) to use pronouns faithfully and without causing harm.\nTo this end, many studies have explored how LLMs handle pronouns, showing that they stereotypically associate pronouns and occupations (Kurita et al., 2019  ###reference_b30###), reason about co-referring pronouns and entities better when they conform to stereotypes (Tal et al., 2022  ###reference_b58###), fail when exposed to novel pronoun phenomena such as neopronouns (Lauscher et al., 2023  ###reference_b33###), and cannot consistently reuse neopronouns during generation (Ovalle et al., 2023a  ###reference_b45###).\nThese shortcomings create quality of service differences and cause representational harm,\namplifying discrimination against certain pronoun users (Blodgett et al., 2020  ###reference_b6###; Dev et al., 2021  ###reference_b13###).\nHowever, a question that has gone unexamined thus far is: How robust is model faithfulness to pronouns when discussing more than one person? To answer this question, we propose pronoun fidelity (\u00a72  ###reference_###), a new task to investigate realistic model reasoning about pronouns, and we introduce RUFF (\u00a73  ###reference_###), a novel, large-scale dataset of over 5 million instances, carefully designed to evaluate this task. With this dataset, we present an analysis of pronoun fidelity across 37 popular language models covering a range of architectures and scales (11M-70B), to investigate whether models are reasoning, repeating, or just biased.\nFirst, we collect model pronoun predictions for occupations in the absence of context, to establish a \u201cbias baseline\u201d (\u00a75  ###reference_###).\nNext, we evaluate whether models can overcome their biased pronoun predictions when explicitly shown what pronoun to use in context (\u00a76  ###reference_###).\nAll models are good at this task, but there are significant disparities across pronoun sets.\nWe then test the robustness of this result by inserting naturalistic distractor sentences using a different pronoun to talk about another person (\u00a77  ###reference_###).\nEven one non-adversarial distractor sentence vastly deteriorates model performance as shown in Figure 1  ###reference_###.\nFinally, in a detailed error analysis (\u00a78  ###reference_###), we disentangle whether model errors can be attributed to distraction or falling back to bias, finding that encoder-only and decoder-only models behave in fundamentally different ways.\n###figure_1### Overall, our results show that models struggle to reason about pronouns in a simple, naturalistic setting and highlight the need for careful task design to ensure that superficial repetition does not lead to inflated claims about model reasoning.\nWe release all code and data to encourage researchers to bridge the gaps we find:\nhttps://github.com/uds-lsv/robust-pronoun-fidelity  ###reference_fidelity###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Pronoun Fidelity Task",
            "text": "Discussing multiple individuals is natural, frequent and well-studied in discourse;\nwe use both definite references and pronouns in natural language to establish continuity and coherence (Grosz et al., 1995  ###reference_b21###).\nWe formalize a version of these phenomena in our task:\ngiven a context in which a co-referring entity and pronoun are introduced, the task is to reconstruct the pronoun later in a sentence about the entity, independent of a limited number of potential distractors.\nIntroduction: The accountant had just eaten a big meal so her stomach was full.\n\n(Optional)\nDistractor 1: The taxpayer needed coffee because their day had started very early.\n\u2026\nDistractor N: Their sleep had been fitful.\n\nTask sentence: The accountant was asked about ___ charges for preparing tax returns.\nMore formally, an introduction sentence  establishes a coreference between an entity  and a pronoun . A distractor sentence  explicitly establishes or implicitly continues a previously-established coreference between a different entity  and a different pronoun , i.e.,  and . Let  be a set of distractor sentences such that . When combined, an introduction sentence and the set of distractor sentences form a context. A task sentence  contains an unambiguous coreference between the entity  from the introduction and a pronoun slot  which must be filled.\nThe task is to maximize\nthe probability  of reconstructing the correct pronoun  in the sentence , given the context."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "RUFF Dataset",
            "text": "RUFF, our evaluation dataset for Robust pronoUn Fidelity at scale, is constructed with a 3-step pipeline to create naturalistic instances of our task with unambiguous answers: template creation (\u00a73.1  ###reference_###), assembly (\u00a73.2  ###reference_###), and validation (\u00a73.3  ###reference_###).\nOur data covers 60 occupations (see Appendix B  ###reference_###) and four third-person pronouns (he, she, they and xe) in three grammatical cases (nominative, accusative and possessive dependent). Our occupations (and corresponding participants for distractor sentences) are chosen from Winogender schemas (Rudinger et al., 2018  ###reference_b48###), as their bias characteristics are well-studied in NLP. In addition to the English masculine (he/him/his) and feminine (she/her/her) pronouns, we heed Lauscher et al.  ###reference_b32###\u2019s (2022  ###reference_b32###) call for more inclusive NLP research by examining two more pronoun sets that are less well-studied in NLP: singular they (they/them/their), the pronoun of choice of over 75% of respondents to the Gender Census (Lodge, 2023  ###reference_b39###), and xe/xem/xyr, the most popular neopronoun according to the same census."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Template creation",
            "text": "Task templates.\u2003We create one task sentence template per occupation and grammatical case for a total of 180 templates, designed to create an unambiguous coreference with the occupation only. For instance, charges for preparing tax returns can only belong to an accountant, never a taxpayer, which is the corresponding participant.\nContext templates.\u2003We create explicit (definite reference + pronoun) and implicit (pronoun-only) versions of 10 context templates per grammatical case, for a total of 30 templates. Each explicit context template begins with an entity and introduces the pronoun in a clause, e.g., The taxpayer needed coffee because their day had started very early, while implicit templates are simple sentences like Their sleep had been fitful.\nAll templates contain generic themes, e.g., universal human emotions and sensations (hungry/full, tired/energetic, unhappy/happy, etc.).\nThese templates can thus be applied to all occupations and participants, and allow for a controlled but still coherent setting."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Template assembly",
            "text": "We then instantiate and combine the previously created templates to assemble our data instances. First, we select an occupation () and one of its task templates. We pick a pronoun () to use as ground truth and instantiate a random context template with the selected occupation and pronoun.\nThe simplest version of the pronoun fidelity task includes just this introduction sentence followed by the task sentence. Instantiating 10 templates with 4 different pronoun sets and pairing them with task templates for 60 occupations across 3 grammatical cases gives us a total of 7,200 unique instances for this version of the task.\nTo create more complex data instances, we insert a variable number of distractor sentences between the introduction and task sentences, discussing a participant  who uses a different pronoun .\nThese are also sampled from the set of context templates (see Appendix C  ###reference_### for details).\nInstantiating 4 templates with 3 unused pronouns gives 86,400 unique instances with one distractor.\nOur stackable dataset design allows us to generate a vast amount of data of varying lengths, giving us a controlled setting to evaluate context effects on model predictions.\nWe subsample the data with three random seeds for the rest of our evaluation, ensuring that all occupations, cases, pronoun declensions and distractor pronouns are equally represented in each subsampled set of 2,160 sentences.\nAll data statistics are shown in Table 1  ###reference_###.\n###table_1###"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Data validation",
            "text": "We validate all task and context templates. To verify that the pronoun fidelity task is easy and unambiguous for humans, we also validate a subset of these instances with 0-5 distractors.\nAnnotator information is shown in Appendix D  ###reference_### and all annotator instructions are provided in Appendix E  ###reference_###.\nTemplates.\u2003Two authors with linguistic training iteratively created and validated sentence templates for grammaticality and correct coreferences until consensus was reached.\nAn additional annotator independently rated 100% of the sentences as grammatical and with the correct coreferences.\nPronoun fidelity task.\u2003We sampled 100 instances with each possible number of distractors (0-5), for a total of 600 instances. One author and one annotator had to fill in the pronoun and they each performed with 99.8% accuracy.111They disagreed on non-overlapping instances which appeared to be random slips."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We list our models, evaluation methods, and metrics. Further details are provided in Appendix F  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Models",
            "text": "We experiment with 37 transformer-based language models (see Table 2  ###reference_###), chosen to evaluate the effects of architecture and scaling.\nOur 9 encoder-only models are from the BERT (Devlin et al., 2019  ###reference_b14###), RoBERTa (Liu et al., 2019  ###reference_b38###), ALBERT-v2 (Lan et al., 2020  ###reference_b31###) and MosaicBERT (Portes et al., 2023  ###reference_b47###) model families, as the first three remain well-used in NLP, and the last is trained on much more data.\nAs for our 20 decoder-only models, we select the popular Llama-2 (Touvron et al., 2023  ###reference_b60###) model family, as well as OPT (Zhang et al., 2022  ###reference_b66###) and Pythia (Biderman et al., 2023  ###reference_b5###) for their large range of model sizes.\nIn Appendix I  ###reference_###, we also experiment with eight popular chat models that are further trained with instruction-tuning and reinforcement learning, to evaluate task performance with prompting; specifically, we use decoder-only Llama-2-chat models (Touvron et al., 2023  ###reference_b60###) and encoder-decoder FLAN-T5 models (Chung et al., 2022  ###reference_b11###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Obtaining predictions",
            "text": "Decoder-only models.\u2003For the majority of our experiments, we follow Hu and Levy (2023  ###reference_b25###) in taking direct measurements of probabilities as a proxy for models\u2019 metalinguistic judgements. We verbalize four versions of each data instance, i.e., we fill in the blank with each of the four pronouns we consider, creating four options. We then compute instance-level model log likelihoods for these options, and select the option with the highest log likelihood as the model\u2019s choice.\nEncoder-only models.\u2003For comparability with decoder-only models, we follow the same procedure for encoder-only models, but use pseudo log likelihoods (Salazar et al., 2020  ###reference_b49###; Kauf and Ivanova, 2023  ###reference_b28###). We do not use masked token prediction due to tokenization issues with neopronouns (Ovalle et al., 2023b  ###reference_b46###); briefly, we want xe to be tokenized \u201cnormally\u201d (which is often as two tokens) rather than a single UNK token.\nChat models.\u2003Following common practice, we evaluate chat models (FLAN-T5 and Llama-2-chat) using vanilla and chain-of-thought prompting in Appendix I  ###reference_###. Following Sclar et al. (2024  ###reference_b51###), we show the range of expected performance with 10 different prompts (see Appendix G  ###reference_###), inspired by the prompts to elicit coreferences in the FLAN collection (Longpre et al., 2023  ###reference_b40###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Metrics",
            "text": "As every instance of the pronoun fidelity task has a unique correct answer, we report accuracy averaged over the three randomly sampled subsets of our dataset. We show the standard deviation with error bars or shading.\nWhere possible, we perform significance testing with a Welch\u2019s t-test and a threshold of 0.05. We use human performance as our ceiling, and compare models to a baseline of randomly selecting 1 of the 4 pronouns (i.e., 25%).\n###figure_2###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Model Predictions with No Context",
            "text": "We begin by creating a \u201cbias baseline,\u201d i.e., obtaining pronoun predictions from models on our task sentences in the absence of any context.\nIn Section 6  ###reference_###, we will examine whether models can overcome this bias with reasoning when provided with context establishing a single correct answer.\nExample: The accountant was asked about ___ charges for preparing tax returns.\nNo single answer (among his, her, their, xyr)\n\\endMakeFramed\nAs we cannot evaluate accuracy on a task with no single correct answer, we show the counts of model predictions of different pronoun declensions in Figure 2  ###reference_###, averaged over all models. Model-specific counts are shown in Appendix H  ###reference_###.\nEven though our task sentences are designed such that any pronoun set can be used grammatically, all models tend to assign higher probability to he/him/his than other pronoun sets.\nObtaining pronoun predictions without context is a popular method to measure model bias, with numerous papers (Kurita et al., 2019  ###reference_b30###, inter alia) showing that associations between occupations and pronouns are based on social gender stereotypes, e.g., doctor-he and nurse-she.\nHowever, model pronoun predictions might reflect dataset artifacts such as the choice of occupations, or be a statistical accident of the chosen templates (Seshadri et al., 2022  ###reference_b52###).\nIn addition, intrinsic biases may not correlate with actual pronoun use with context (Goldfarb-Tarrant et al., 2021  ###reference_b19###).\nIn order to test for such extrinsic behaviours, the rest of this paper examines whether models can override their intrinsic statistical biases on these same templates when provided with the right pronoun to use.\n###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Injecting an Introductory Context",
            "text": "When models are provided with an introductory sentence explicitly establishing the pronoun to use for an entity, can they use that pronoun to refer to the same entity in the immediate next sentence?\nExample: The accountant had just eaten a big meal so her stomach was full. The accountant was asked about ___ charges for preparing tax returns.\nCorrect answer: her\n\\endMakeFramed\nAs Figure 3  ###reference_### shows, all models perform better than chance at pronoun fidelity with a simple introduction (up to 0.95 with MosaicBERT), but not as well as humans, who achieve perfect performance. We also see improvements with increasing model scale, with the exception of ALBERT-v2, as in Tay et al. (2022  ###reference_b59###).\nWhich pronouns are harder?\u2003Even in the simplest case of the pronoun fidelity task, patterns emerge when split by pronoun, as shown in Figure 4  ###reference_###. Overall model accuracy on he/him/his is significantly higher than she/her/her, which in turn is significantly higher than both they/them/their and xe/xem/xyr, in line with previous findings that language technology has gaps when it comes to neopronouns (Lauscher et al., 2023  ###reference_b33###).\nModels show intriguing patterns with these last two pronoun sets. Most encoder-only models appear to handle the neopronoun better than singular they (e.g., BERT-large has an accuracy of 0.78 on xe/xem/xyr compared to 0.60 on they/them/their), which warrants further investigation.\nDecoder-only models smaller than 6.7B parameters struggle with the neopronoun, with every OPT and Pythia model smaller than 2.7B parameters performing below chance, and in some cases (e.g., Pythia-14M, Pythia-70M and Pythia-160M) even performing close to 0.0.\nBeyond this scale, however, models perform better on xe/xem/xyr than on singular they, with Llama-13B achieving 0.96 accuracy on the neopronoun. These differences are statistically significant. As the training data for individual model families is the same, this might suggest that decoder-only models generalize to novel pronouns starting at the scale of 6.7B parameters, but in light of Schaeffer et al. (2023  ###reference_b50###), this result could just as well be a mirage resulting from our use of accuracy, a discontinuous metric.\nIn either case, our observations could also explain the poor performance that some previous studies of neopronouns find, as the largest model that Hossain et al. (2023  ###reference_b23###) experiment with, for instance, is OPT-6.7B.\nThe lower performance of bigger models with singular they could also be a reflection of human processing difficulties with definite, specific (singular) they, as has been observed in linguistics (Conrod, 2019  ###reference_b12###).\n###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Adding Distractors",
            "text": "To further probe whether models actually \u201creason\u201d when provided with context, we systematically inject sentences containing distractor pronouns between the introduction and the task, reflecting a natural usage scenario where multiple people are discussed with definite references and pronouns.\nExample: The accountant had just eaten a big meal so her stomach was full. The taxpayer needed coffee because their day had started very early. Their sleep had been fitful. The accountant was asked about ___ charges for preparing tax returns.\nCorrect answer: her\nFigure 5  ###reference_### shows that distractors degrade performance for all models.\nEncoder-only and decoder-only models show different performance curves as more distractors are added:\nall decoder-only models get steadily worse, whereas encoder-only models perform the worst with one distractor and then seem to slowly recover, never quite reaching their level of performance with no distractors. Scaling generally holds within model families, with larger models performing better with more distractors than smaller models of the same type. Figure 6  ###reference_### examines the interplay of scaling and architecture at a higher level, comparing results on the easiest case of pronoun fidelity (no distractors) with the hardest case (5 distractors). Surprisingly, with no distractors, encoder-only models are much better than decoder-only models of the same scale, and their performance is comparable to or better than decoder-only models that are orders of magnitude larger; RoBERTa-base (125M) is 0.86 accurate compared to OPT-125M\u2019s 0.55, and exceeds OPT-66B\u2019s 0.83 despite being more than 500 times smaller. In the hardest version of our task with five distractors, encoder-only models are far better than all decoder-only models, which show dramatically degraded performance; Llama-70B only achieves 0.37 accuracy, compared to MosaicBERT\u2019s impressive 0.87.\nThe lack of robustness of decoder-only models to distractors is striking, given that most state-of-the-art models today are decoder-only models.\nWe hypothesize that architectural differences might explain the performance gaps; encoder-only models might use bidirectional attention to more closely relate the entity mentions in the introduction and task sentences.\nTraining on next token prediction might also make decoder-only models prone to recency bias.\nUsing vanilla and chain-of-thought prompting (Appendix I  ###reference_###) show the same patterns of degradation, reinforcing that model pronoun fidelity is not robust, and good performance with no distractors (\u00a76  ###reference_###) is likely not due to \u201creasoning\u201d at all.\n###figure_16### ###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21### ###figure_22### ###figure_23###"
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Discussion and Future Work",
            "text": "Our results show that even the biggest models of today are not up to the task of pronoun fidelity once it includes a single sentence discussing another person.\nAll models are easily distracted, but encoder-only models and decoder-only models show very different patterns both in performance degradation with more distractors and their reasons for errors.\nPerformance on this type of reasoning task should be evaluated carefully, with attention to how the overall patterns break down by different pronouns and accounting for the possibility of repetition.\nBelow we expand on some questions raised by our findings.\nImproving robust pronoun fidelity.\u2003A natural direction of future work is to solve the problem of robust pronoun fidelity.\nWe urge researchers interested in this direction to treat RUFF as an evaluation dataset, as it was designed.\nDue to the presence of positional and associative heuristics that we expand on in Appendix A  ###reference_###, RUFF should not be seen as a source of data for fine-tuning or in-context learning, which is also why we do not run these experiments.\nInstead, a promising direction might be to encourage models to explicitly track associations between pronoun sets and individuals, just as people do.\nOn \u201creasoning.\u201d\u2003Throughout the paper, we refer to \u201creasoning,\u201d but this is inaccurate. What looks like \u201creasoning\u201d when we inject an introductory context starts to look much more like repetition\u2014or stochastic parroting (Bender et al., 2021  ###reference_b3###)\u2014when we add a distractor and see the same models performing drastically worse.\nEven the higher performance of encoder-only models cannot accurately be attributed to \u201creasoning\u201d in the same way that we use this word for humans, as these models are not grounded in meaning from the real world (Bender and Koller, 2020  ###reference_b4###).\nWe use the word reasoning in line with other work in the field, but note that as these are all language models, it is more accurate to say that the way that decoder-only models model language is prone to repetition of recent examples of the same word class, compared to encoder-only models.\nWhy exactly do we see the patterns we see?\u2003Our dataset design and error analysis shed light on model behaviour, allowing us to evaluate different architectures comparably and disentangle the effects of repetition, distraction and statistical bias.\nHowever, it is beyond the scope of this paper to investigate where in the model architecture, neurons or pre-training data this comes from and what we can do about it towards improving reasoning and mitigating bias. Tools from interpretability literature, e.g., attribution analysis, could help here, and are an important direction for future work.\nBeyond our dataset.\u2003Given the breadth of our task definition, future work could include examining pronoun fidelity for participants, for names by extending Hossain et al. (2023  ###reference_b23###), with differently ordered sentences, and in more natural settings such as stories (Mostafazadeh et al., 2016  ###reference_b43###) or with real-world data as in Webster et al. (2018  ###reference_b64###) and Levy et al. (2021  ###reference_b36###). Additionally, we evaluate on a version of this task that allows us to quantify repetition, i.e., the grammatical case of the elicited pronoun is the same as the case shown in the context. Examining model performance where a pronoun is shown in one grammatical case and then elicited in a different one would be interesting to probe syntactic generalization."
        }
    ],
    "url": "http://arxiv.org/html/2404.03134v2",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "5",
            "6",
            "7",
            "8"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4.1",
            "5",
            "6",
            "7",
            "8"
        ]
    },
    "research_context": {
        "paper_id": "2404.03134v2",
        "paper_title": "Robust Pronoun Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?",
        "research_background": "**Motivation:**\nThe use of third-person pronouns in conversations plays a significant role in reflecting individuals' identities and social genders. Misusing these pronouns can range from being a social faux pas to causing serious psychological distress, especially for transgender individuals. Therefore, it is crucial for large language models (LLMs) to accurately and faithfully use pronouns to avoid causing such harm.\n\n**Research Problem:**\nDespite numerous studies investigating how LLMs handle pronouns, there is a gap in understanding the robustness of model faithfulness to pronouns when discussing multiple people. This paper aims to explore whether LLMs are reasoning about pronouns, simply repeating patterns, or are inherently biased. To address this, the paper introduces a new task called pronoun fidelity and a corresponding large-scale dataset to evaluate model performance on this task.\n\n**Relevant Prior Work:**\n1. **Stereotypical Associations and Pronoun Use:**\n   - Kurita et al. (2019) demonstrated that LLMs tend to stereotypically associate pronouns with occupations.\n   - Tal et al. (2022) found that LLMs reason about pronouns and entities more effectively when these conform to stereotypes.\n\n2. **Neopronouns and Model Inconsistencies:**\n   - Lauscher et al. (2023) showed that LLMs fail when confronted with novel pronoun phenomena such as neopronouns.\n   - Ovalle et al. (2023a) observed that LLMs cannot consistently reuse neopronouns during text generation.\n\n3. **Representational Harm and Quality of Service Issues:**\n   - Studies by Blodgett et al. (2020) and Dev et al. (2021) discussed how these shortcomings in pronoun handling create differences in service quality and cause representational harm, thus amplifying discrimination against certain pronoun users.\n\nThe paper builds on these findings to explore the robustness of pronoun faithfulness across multiple persons in a discussion, aiming to understand whether LLM errors are due to distraction or inherent biases.",
        "methodology": "The proposed method in the paper aims to test an English Language Model's (LLM) ability to understand and reconstruct pronouns within the context of a given discourse. Here is a detailed breakdown of the methodology section:\n\n1. **Objective**: The primary task is to evaluate whether the model can accurately reconstruct a pronoun in the context of a sentence that has been introduced in earlier parts of the discourse, despite the presence of potential distractors.\n\n2. **Introduction Sentence**: This establishes a co-reference relationship between an entity (e.g., \"the accountant\") and a pronoun (e.g., \"her\"). This relationship sets the groundwork for the continuity and coherence in the discourse.\n\n3. **Distractor Sentences**: These are sentences that mention different entities and corresponding pronouns. The purpose of these distractors is to see if the model can maintain the original co-reference chain despite the confusion that might be caused by additional entities and pronouns.\n    - Example given in the methodology: \n        - Introduction: \"The accountant had just eaten a big meal so her stomach was full.\"\n        - Distractor 1: \"The taxpayer needed coffee because their day had started very early.\"\n        - Distractor N: \"Their sleep had been fitful.\"\n\n4. **Task Sentence**: This sentence contains an unambiguous reference to the original entity from the introduction sentence and a blank slot where the pronoun should be reconstructed. For example:\n    - \"The accountant was asked about ___ charges for preparing tax returns.\"\n\n5. **Formal Description**:\n    - **Entities and Pronouns**:\n        - \\( e \\): original entity (e.g., \"the accountant\")\n        - \\( p \\): pronoun referring to \\( e \\) (e.g., \"her\")\n        - \\( d \\): different entities referred to in distractor sentences with corresponding pronouns\n\n    - **Coreference**:\n        - A sentence explicitly or implicitly continues a coreference relationship, i.e., the introduction sentence establishes a coreference between \\( e \\) and \\( p \\), while distractor sentences introduce or continue a coreference between \\( d \\) and their pronouns.\n\n    - **Context Formation**:\n        - The context is formed by an introduction sentence and a set of distractor sentences \\( D \\).\n        \n    - **Probability Maximization**:\n        - The goal is to maximize the probability \\( P \\) of reconstructing the correct pronoun \\( p \\) in the task sentence \\( t \\), given the context.\n\nOverall, the methodology is designed to rigorously test the model's pronoun fidelity by ensuring it can correctly interpret and apply the co-reference relation within a complex discourse containing potential distractions.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Models:**\nIn this study, English Language Models (LLMs) are utilized to assess their performance on pronoun fidelity. This involves examining how well these models can correctly interpret and generate pronouns within various contexts.\n\n**Datasets:**\nThe primary datasets employed for the experiments are:\n- Winograd Schema Challenge (WSC)\n- Winogender Schemas\n\n**Evaluation Methods and Metrics:**\nThe evaluation of the models is conducted by measuring:\n- Accuracy: The proportion of correctly resolved pronoun references out of the total test cases.\n- Gender Bias Assessment: Evaluates whether the model's performance is affected by the gender implications within the pronouns and entities.\n\n### Main Experimental Results\n\nThe results indicate varying degrees of success across different models and datasets:\n- On the Winograd Schema Challenge (WSC), the models achieved [specific accuracy rates found in Appendix F].\n- On the Winogender Schemas, a substantial performance disparity was observed, indicating potential gender biases ([detailed metrics are provided in Appendix F]).\n\nOverall, the experiments reveal that while some models demonstrate a strong ability to resolve pronoun references, there are noticeable differences in performance that suggest a mix of reasoning, repetition of training data, and potential bias in model outputs."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine if models can use the correct pronoun for an entity in the immediate next sentence when provided with an introductory context establishing that pronoun.",
            "experiment_process": "An introductory sentence establishes the pronoun to use for an entity, followed by a task sentence where models must use the same pronoun. Models from different architectures and scales are evaluated, and performance is compared across different pronoun sets (he/him/his, she/her/her, they/them/their, and xe/xem/xyr).",
            "result_discussion": "All models perform better than chance with a simple introduction, with performance improvements seen with increasing model scale. However, even in the simplest case, models have higher accuracy with he/him/his pronouns compared to she/her/her, which are in turn more accurate than they/them/their and xe/xem/xyr pronouns. Encoder-only models perform better with neopronouns than smaller decoder-only models, requiring further investigation into potential reasons.",
            "ablation_id": "2404.03134v2.No1"
        },
        {
            "research_objective": "To investigate whether models can maintain pronoun fidelity when provided with additional context sentences that include distractor pronouns.",
            "experiment_process": "Sentences containing distractor pronouns are systematically added between the introductory sentence and the task sentence. Models' performance is evaluated to determine their ability to maintain the fidelity of the initial pronoun and distinguish it from distractor pronouns.",
            "result_discussion": "Performance degrades across all models with the addition of distractors. Encoder-only models perform the worst with one distractor but recover slightly with more distractors, while decoder-only models steadily worsen as more distractors are added. Larger models generally perform better, but encoder-only models significantly outperform similarly scaled decoder-only models in distractor-heavy scenarios.",
            "ablation_id": "2404.03134v2.No2"
        },
        {
            "research_objective": "To discern whether errors in pronoun fidelity stem from model distractibility or intrinsic bias when distractor sentences are added.",
            "experiment_process": "Errors from models are analyzed to determine if they are due to repeating the distractor pronoun (distractibility) or reverting to the model\u2019s context-free biased prediction. Only unambiguous error cases where the distractor pronoun differs from the context-free biased prediction are considered.",
            "result_discussion": "Across models, distraction is the primary error type for most models. Encoder-only models become less distractible with more distractors, whereas decoder-only models become more distractible. Larger models are generally more distractible and less biased, while smaller models exhibit the opposite. The high distractibility of all models indicates a tendency to repeat recent pronouns rather than reasoned selection.",
            "ablation_id": "2404.03134v2.No3"
        }
    ]
}