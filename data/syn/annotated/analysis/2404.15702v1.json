{
    "title": "NYONIC TECHNICAL REPORT",
    "abstract": "This report details the development and key achievements of our latest language model designed for custom large language models. The advancements introduced include a novel Online Data Scheduler that supports flexible training data adjustments and curriculum learning. The model\u2019s architecture is fortified with state-of-the-art techniques such as Rotary Positional Embeddings, QK-LayerNorm, and a specially crafted multilingual tokenizer to enhance stability and performance. Moreover, our robust training framework incorporates advanced monitoring and rapid recovery features to ensure optimal efficiency. Our Wonton 7B model has demonstrated competitive performance on a range of multilingual and English benchmarks. Future developments will prioritize narrowing the performance gap with more extensively trained models, thereby enhancing the model\u2019s real-world efficacy and adaptability.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "This report details the development and release of our pre-trained 7B base model checkpoint and one fine-tuned model for chat. As we plan to continuously train and improve our model, we will release more base model checkpoints in the future, as well as fine-tuned models for different use cases.\nOur training infrastructure is mainly built on open source PyTorch framework, incorporating various newly developed features, e.g., flash attention((Dao et al., 2022  ###reference_b10###)), RoPE(Su et al. (2021  ###reference_b26###)), QK-LayerNorm((Dehghani et al., 2023  ###reference_b11###)), etc.\nThe main contributions of this work can be summarized as follows:\nWe build an online data scheduler and multiplexer for flexible training data mixing.\nWe train our own multilingual tokenizer to incorporate several widely-spoken languages.\nWe monitor various intermediate metrics during the training and apply several normalization and regularization techniques to increase the stability.\nWe consolidate our infrastructure to speed up the resuming after interruptions.\nWe fine tuned the pre-trained model with open SFT datasets, as well as our private datasets.\nWe enabled a few inference scenarios and provided deployment methods.\nThis report aims to provide a comprehensive overview of LLM training, development, SFT, inference, and deployment technology, which can benefit the community in the creation of more LLMs and the development of a wide range of real-world applications."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Training",
            "text": "((Su et al., 2021  ###reference_b26###)): Unlike traditional absolute positional embeddings, we implement rotary positional embeddings in each layer. RoPE has gained widespread acceptance and has shown effectiveness in modern large language models ((Touvron et al., 2023b  ###reference_b29###; Jiang et al., 2023  ###reference_b13###; Bai et al., 2023  ###reference_b2###)).\n: We employ pre-normalization, a prevalent strategy that enhances training stability over post-normalization ((Xiong et al., 2020  ###reference_b33###)). Traditional layer normalization ((Ba et al., 2016  ###reference_b1###)) is used, and it is also applied to the queries and keys before the dot-product attention process ((Dehghani et al., 2023  ###reference_b11###)) to prevent excessively large values in attention logits.\n: Building on the auxiliary z-loss from PaLM ((Chowdhery et al., 2022  ###reference_b7###)) and the max-z loss concept ((Yang et al., 2023  ###reference_b34###)), we employ these techniques to regulate logits values. This ensures more stable training and robust inference across various hyper-parameters. Max-z loss, in particular, offers greater effectiveness with a reduced memory demand, making it our default choice for model training.\n###table_1### We select the most common evaluation sets in each category as follows:\nLambada ((Paperno et al., 2016  ###reference_b20###)): The LAMBADA benchmark features around 10,000 passages from the BooksCorpus, where participants are tasked with predicting a missing word in the contextually rich final sentence of each passage.\nWinoGrande ((Sakaguchi et al., 2019  ###reference_b25###)): A large-scale dataset designed to challenge models with complex commonsense reasoning through refined pronoun disambiguation tasks.\nHellaSwag (Zellers et al. (2019  ###reference_b35###)): This dataset pushes the limits of models\u2019 commonsense reasoning capabilities by requiring them to complete scenarios in a contextually appropriate way.\nPIQA ((Bisk et al., 2020  ###reference_b4###)): Introduces a benchmark for physical commonsense reasoning, testing models\u2019 abilities to understand and predict physical interactions in diverse settings.\nRACE ((Lai et al., 2017  ###reference_b16###)): A robust dataset providing a range of reading comprehension questions based on academic texts from middle and high school levels.\nBoolQ ((Clark et al., 2019  ###reference_b8###)): Focuses on yes/no question answering, with questions naturally derived from Google searches, testing models\u2019 abilities to interpret and evaluate straightforward queries against provided texts.\n###table_2### Table 2  ###reference_### lists the performance of Wonton 7B and the other open source models on the benchmarks. Wonton 7B consistently outperforms Pythia 7B across various metrics, indicating more efficient use of its training data and better optimization strategies. However, when compared to Mistral 7B, which was trained on a significantly larger corpus, Wonton 7B still lags behind, especially in tasks that require a deep understanding of language nuances and complex reasoning. These results underscore the effectiveness of Wonton 7B against comparable models like Pythia but also highlight the challenges it faces in reaching the performance levels of more extensively trained models such as Mistral 7B. This comparison not only reflects Wonton 7B\u2019s strengths but also points towards potential areas for future improvement to bridge the gap with top-tier models.\nTo assess the capabilities of Wonton 7B, we utilized various benchmark datasets that challenge models to demonstrate comprehension and reasoning across multiple languages. The datasets employed include:\nBelebele ((Bandarkar et al., 2023  ###reference_b3###)): Tests models on tasks across eight different languages\u2014English, German, French, Italian, Spanish, Chinese, Japanese, and Korean.\nXNLI ((Conneau et al., 2018  ###reference_b9###)): Focuses on natural language inference in three languages, assessing how well models understand and interpret text across language barriers.\nXStoryCloze ((Lin et al., 2022  ###reference_b18###)): Measures models\u2019 ability to logically complete stories in English, Spanish, and Chinese, testing narrative understanding and commonsense reasoning.\nXWinograd ((Tikhonov and Ryabinin, 2021  ###reference_b27###)): A multilingual version of the Winograd schema challenge that tests models on resolving pronoun disambiguation in English, French, and Japanese.\n###table_3### ###figure_1### ###table_4### In Table 3  ###reference_###, Wonton 7B showed competitive results across the Belebele benchmarks. It consistently outperformed Pythia 7B, indicating more efficient use of its training data and better optimization strategies. The results in Table 4  ###reference_### are consistent with Belebele, reflecting improvements in its ability to handle complex linguistic constructions and contexts across different languages."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Data",
            "text": "The efficacy and performance of language models are significantly influenced by the quality and diversity of the data used in training. An effective pretraining dataset must encompass a broad spectrum of types, domains, and tasks to ensure comprehensive linguistic coverage.\nOur curated dataset is specifically designed to meet these criteria, incorporating sources such as the Common Crawl, books, Wikipedia, code, and academic papers. Moreover, our dataset is multilingual, encompassing English, Chinese, German, French, Italian, Spanish, Japanese, and Korean, and with a significant portion of the data being in English. Figure 1  ###reference_### illustrates the mixture of data and the percentage they represent in the training set.\n###figure_2###"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Data Engineering",
            "text": "###figure_3### In providing customized large model solutions for specific industries, the traditional approach involves offline conversion of data into token indices to create corresponding data indices. However, recent studies ((Xie et al., 2023  ###reference_b32###)) have shown that compared to uniformly sampling data sources, carefully selecting training data and undergoing continual pre-training can significantly enhance model accuracy, further validating the effectiveness of data scheduling strategies.\nNevertheless, these methods still rely on offline data processing and lack mechanisms for online adjustments and real-time feedback during the training process. Restarting training not only consumes substantial computational resources but also does not necessarily yield better training outcomes.\nTo address this issue, we propose an innovative Online Data Scheduler, which integrates an online data stream with a data scheduler. The Online Data Scheduler offers several benefits:\nIt enables flexible implementation of data mixing since it eliminates the need for offline conversion of data into token indices. By utilizing multiple workers in the data loader, it can concurrently process data and perform model training, without reducing training efficiency.\nThe system allows for training flexibility across different stages through curriculum learning. The model is able to avoid wasting time on simple data that it has already mastered, focusing more attention on data that is challenging to learn. This targeted training approach can accelerate model convergence, reduce unnecessary computations, and thereby save time and resources.\nReal-time feedback is possible. The scheduler can dynamically adjust data ratios based on the model\u2019s real-time training loss, allowing for immediate adjustments based on feedback.\nThe model is capable of online learning, enabling it to adapt to new data. This allows the model to learn within a continuously changing data stream, aligning more closely with the ongoing learning processes observed in the real world.\nThe data flow for training large models is efficiently divided into three stages.\nData Preparation involves collecting, shuffling, and distributing file names across different ranks, with each rank responsible for reading file contents.\nIn the Data Processing stage, data is further shuffled, split among workers, normalized, and tokenized, ensuring consistency and readiness for model input.\nThe final stage, Batch Preparation, involves padding the data to uniform lengths, logging metadata, and organizing it into batches that are converted to tensors and torch iterables for training.\nThis streamlined process ensures that data is methodically prepared and optimized for effective model training. In particular, we present the key components of our online data scheduler, which facilitate the integration of data from various sources in a configurable manner.\nMultiplexing: To facilitate data mixing from various sources in a configurable manner, a Multiplexer is integrated into the real data loading pipeline. This allows for the combination or multiplexing of processed data streams from different sources into a single stream, with configurable mixture ratios.\nContent Stuffing: Following (Brown et al., 2020  ###reference_b6###), during training, we consistently use sequences that fill the entire 2048 token context window. When individual documents are shorter than 2048 tokens, multiple documents are packed into a single sequence to enhance computational efficiency. These sequences are further augmented with additional length information, which enables them to be processed in the CUDA kernel without the need for sequence-specific masking.\nData Resuming: Given that the list of files assigned to each rank remains constant for each dataset, we choose to save the processing status of files on each rank. This approach enables us to bypass files that have already been processed when resuming training. Currently, our focus is on maintaining lightweight control over the resumption process at the file level, rather than managing it with finer granularity at the record level."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Tokenization",
            "text": "###figure_4### The design of vocabulary plays a significant role in the training efficiency and downstream task performance of multilingual large language models. We employ the byte-pair encoding (BPE) algorithm, as described by (Bostrom and Durrett, 2020  ###reference_b5###), using the SentencePiece implementation ((Kudo and Richardson, 2018  ###reference_b14###)). To optimize performance with datasets containing abundant whitespace, such as code, we assign one special token for each sequence of whitespace ranging from 1 to 24 characters. Additionally, in line with (Touvron et al., 2023a  ###reference_b28###, b  ###reference_b29###), we have segmented numbers into individual digits.\nSelecting an optimal vocabulary size is of paramount importance for tokenizers, as a size that is too small may impede transformer computations, while a size that is too large may hinder the model\u2019s capacity to derive meaning from sequences. We have identified the most commonly used languages by our customers, including English, Simplified Chinese, Traditional Chinese, German, French, Italian, Spanish, Japanese, and Korean. Our data sources encompass Common Crawl, Wiki, and various coding repositories. The final vocabulary size of the tokenizer, as determined by extensive experimentation, is approximately 139,000.\nIn Figure 3  ###reference_###, we present a comprehensive evaluation of the multilingual tokenizer. This detailed comparison examines several critical metrics that are essential for understanding tokenizer efficiency. These metrics include the compression rate, which measures the compactness of tokenized data; fertility, which assesses the breakdown of words into subwords; and the proportion of continued words, which evaluates the segmentation consistency across various languages. The results of our analysis demonstrate that our tokenizer achieves superior compression efficiency in most languages. This indicates that it has the potential to significantly reduce operational costs by using fewer tokens to transmit equivalent or greater amounts of information."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Architecture Highlights",
            "text": "Our model, Wonton 7B, is built on a transformer architecture ((Vaswani et al., 2017  ###reference_b30###)), with its main parameters summarized in Table 1  ###reference_###. Compared to GPT3 ((Brown et al., 2020  ###reference_b6###)), Wonton 7B incorporates a number of enhancements, which are outlined below.\n((Su et al., 2021  ###reference_b26###  ###reference_b26###)): Unlike traditional absolute positional embeddings, we implement rotary positional embeddings in each layer. RoPE has gained widespread acceptance and has shown effectiveness in modern large language models ((Touvron et al., 2023b  ###reference_b29###  ###reference_b29###; Jiang et al., 2023  ###reference_b13###  ###reference_b13###; Bai et al., 2023  ###reference_b2###  ###reference_b2###)).\n: We employ pre-normalization, a prevalent strategy that enhances training stability over post-normalization ((Xiong et al., 2020  ###reference_b33###  ###reference_b33###)). Traditional layer normalization ((Ba et al., 2016  ###reference_b1###  ###reference_b1###)) is used, and it is also applied to the queries and keys before the dot-product attention process ((Dehghani et al., 2023  ###reference_b11###  ###reference_b11###)) to prevent excessively large values in attention logits.\n: Building on the auxiliary z-loss from PaLM ((Chowdhery et al., 2022  ###reference_b7###  ###reference_b7###)) and the max-z loss concept ((Yang et al., 2023  ###reference_b34###  ###reference_b34###)), we employ these techniques to regulate logits values. This ensures more stable training and robust inference across various hyper-parameters. Max-z loss, in particular, offers greater effectiveness with a reduced memory demand, making it our default choice for model training.\n###table_5###"
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Training Hyperparameters",
            "text": "The models are trained using the AdamW optimizer ((Loshchilov and Hutter, 2017  ###reference_b19###)), with the hyper-parameters set at  and . We implement a weight decay of  and apply gradient clipping at . Following the approach described in (Brown et al., 2020  ###reference_b6###), we utilize a cosine decay schedule with warmup. The warmup period consists of  steps, followed by a decay to 10% of the maximum learning rate.\nFollowing the approach described in (Radford et al., 2019  ###reference_b23###), we initialize the linear and embedding weights using a normal distribution and scale the up-projection layers at initialization by a factor of  where N represents the number of layers. This scaling is designed to optimize the variance of the outputs across different layers, facilitating more stable training."
        },
        {
            "section_id": "2.6",
            "parent_section_id": "2",
            "section_name": "Training Infrastructures",
            "text": "For training the Nyonic-7B model, we utilize PyTorch ((Paszke et al., 2019  ###reference_b21###)) and DeepSpeed ZeRO2 ((Rasley et al., 2020  ###reference_b24###)), distributing the optimizers states across 128 NVIDIA A800 GPUs interconnected via InfiniBand. To enhance training efficiency, we incorporate FlashAttention ((Dao et al., 2022  ###reference_b10###)) and xformers ((Lefaudeux et al., 2022  ###reference_b17###)), achieving a training speed of approximately 3,000 tokens per GPU per second. All models are trained using BFloat16 mixed precision to ensure training stability.\nWe introduce additional metrics to monitor the model training process. By integrating these metrics, we aim to gain deeper insights into our model\u2019s internal dynamics and improve its performance and stability across a range of tasks.\nMax Attention Logits: This measures the maximum value of the attention logits within each attention block, providing insight into the attention distribution\u2019s extremities.\nMean Query Norm: By averaging the norms of the query vectors, this metric assesses the average strength or magnitude of the queries before they interact with the keys, which can indicate the overall query signal strength across different layers.\nOutput Logit Mean (Pre-Softmax): This metric calculates the mean of the output logits just before the softmax activation, which can help in understanding the pre-activation distribution of the logits.\nRoot Mean Square of Gradient of the First Layer of MLP: Monitoring the root mean square of the gradient for the first layer of the multi-layer perceptron (MLP) within each block can provide early indications of potential issues with gradient vanishing or exploding, which are critical for maintaining training stability.\nBlock Output RMS: The root mean square (RMS) of each block\u2019s output offers a measure of the output signal\u2019s consistency and variability, which can be crucial for diagnosing model behavior during both training and inference phases.\nOur training framework supports minute-level training resumption thanks to our implemented online data scheduler. Additionally, it accommodates training resumption from various distributing configurations to adapt to dynamic changes in the computing cluster."
        },
        {
            "section_id": "2.7",
            "parent_section_id": "2",
            "section_name": "Experiment Results",
            "text": "We select the most common evaluation sets in each category as follows:\nLambada ((Paperno et al., 2016  ###reference_b20###  ###reference_b20###)): The LAMBADA benchmark features around 10,000 passages from the BooksCorpus, where participants are tasked with predicting a missing word in the contextually rich final sentence of each passage.\nWinoGrande ((Sakaguchi et al., 2019  ###reference_b25###  ###reference_b25###)): A large-scale dataset designed to challenge models with complex commonsense reasoning through refined pronoun disambiguation tasks.\nHellaSwag (Zellers et al. (2019  ###reference_b35###  ###reference_b35###)): This dataset pushes the limits of models\u2019 commonsense reasoning capabilities by requiring them to complete scenarios in a contextually appropriate way.\nPIQA ((Bisk et al., 2020  ###reference_b4###  ###reference_b4###)): Introduces a benchmark for physical commonsense reasoning, testing models\u2019 abilities to understand and predict physical interactions in diverse settings.\nRACE ((Lai et al., 2017  ###reference_b16###  ###reference_b16###)): A robust dataset providing a range of reading comprehension questions based on academic texts from middle and high school levels.\nBoolQ ((Clark et al., 2019  ###reference_b8###  ###reference_b8###)): Focuses on yes/no question answering, with questions naturally derived from Google searches, testing models\u2019 abilities to interpret and evaluate straightforward queries against provided texts.\n###table_6### Table 2  ###reference_###  ###reference_### lists the performance of Wonton 7B and the other open source models on the benchmarks. Wonton 7B consistently outperforms Pythia 7B across various metrics, indicating more efficient use of its training data and better optimization strategies. However, when compared to Mistral 7B, which was trained on a significantly larger corpus, Wonton 7B still lags behind, especially in tasks that require a deep understanding of language nuances and complex reasoning. These results underscore the effectiveness of Wonton 7B against comparable models like Pythia but also highlight the challenges it faces in reaching the performance levels of more extensively trained models such as Mistral 7B. This comparison not only reflects Wonton 7B\u2019s strengths but also points towards potential areas for future improvement to bridge the gap with top-tier models.\nTo assess the capabilities of Wonton 7B, we utilized various benchmark datasets that challenge models to demonstrate comprehension and reasoning across multiple languages. The datasets employed include:\nBelebele ((Bandarkar et al., 2023  ###reference_b3###  ###reference_b3###)): Tests models on tasks across eight different languages\u2014English, German, French, Italian, Spanish, Chinese, Japanese, and Korean.\nXNLI ((Conneau et al., 2018  ###reference_b9###  ###reference_b9###)): Focuses on natural language inference in three languages, assessing how well models understand and interpret text across language barriers.\nXStoryCloze ((Lin et al., 2022  ###reference_b18###  ###reference_b18###)): Measures models\u2019 ability to logically complete stories in English, Spanish, and Chinese, testing narrative understanding and commonsense reasoning.\nXWinograd ((Tikhonov and Ryabinin, 2021  ###reference_b27###  ###reference_b27###)): A multilingual version of the Winograd schema challenge that tests models on resolving pronoun disambiguation in English, French, and Japanese.\n###table_7### ###figure_5### ###table_8### In Table 3  ###reference_###  ###reference_###, Wonton 7B showed competitive results across the Belebele benchmarks. It consistently outperformed Pythia 7B, indicating more efficient use of its training data and better optimization strategies. The results in Table 4  ###reference_###  ###reference_### are consistent with Belebele, reflecting improvements in its ability to handle complex linguistic constructions and contexts across different languages."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Inference and Deployment",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Inference",
            "text": "In the development of our foundation models, we have implemented and tested several inference frameworks to optimize performance across different environments. Our primary focus was on leveraging the flexibility and robustness of PyTorch for native inference, which allowed us to maintain a consistent development and deployment pipeline.\nTo expand our model\u2019s accessibility and ease of integration, we adapted our models for use with the Hugging Face ecosystem. This adaptation enabled seamless inference capabilities and provided our team with robust, community-driven tools for model management and deployment. The Hugging Face integration also allowed us to leverage their extensive libraries and APIs, which significantly accelerated our development process. This work is yet to be published.\nAdditionally, we utilized NVIDIA TensorRT111https://github.com/NVIDIA/TensorRT  ###reference_### for optimizing our models for high-performance inference on NVIDIA GPUs. This was crucial for scenarios demanding low latency and high throughput, such as real-time applications and large-scale deployments. TensorRT helped in dramatically reducing inference times while preserving the accuracy and reliability of our models."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Deployment",
            "text": "For deployment, we mostly chose Aliyun Elastic Algorithm Service (EAS)222https://www.alibabacloud.com/help/en/pai/user-guide/eas-model-serving/  ###reference_ser-guide/eas-model-serving/### to host and manage our models. This platform offered a scalable and secure environment that supported our operational needs, including automatic scaling, performance monitoring, and robust security measures. The use of Aliyun EAS facilitated a straightforward deployment process and enabled efficient management of model resources in a production environment.\nThrough these diverse technologies, we achieved a flexible, efficient, and scalable foundation model infrastructure that supports a wide range of applications and meets various business requirements."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Specialized Model for Chat",
            "text": "Table 2  ###reference_### shows the performance of Nyonic pretrained 7B model. The fine tuned model has a comparable results on such benchmarks.\nWe employs ChatGPT to conduct independent scoring by assigning scores ranging from 1 to10, based on the quality of model responses. The fine tuned model had achieved average 2.6 score higher than pretrained model."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Data",
            "text": "We finetune Wonton 7B pretrained model using open source and industry datasets created by Nyonic. All The datasets are in English language. It includes:\nInstruction Tuning with GPT-4 ((Peng et al., 2023  ###reference_b22###)): GPT-4-LLM, which aims to share data generated by GPT-4 for building an instruction-following LLMs with supervised learning and reinforcement learning.\nFlan V2 ((Wei et al., 2022  ###reference_b31###)): It aims to \u2018Finetuned Language Models are Zero-Shot Learners\u2018.\nTulu V2/Cot ((Ivison et al., 2023  ###reference_b12###)): T\u00dcLU, open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. T\u00dcLU 2, a suite of improved T\u00dcLU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences.\nOpen Assitant ((K\u00f6pf et al., 2023  ###reference_b15###)): It comes in lots of conversations to democratize large language model alignment, especially multi-turn conversations.\nAutomobile Datasets: It is created by Nyonic and aims to contain knowledge of Automobile industry."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "SFT Hypterparameters",
            "text": "We conduct the full finetuning training based on the Nyonic pretrained model. It leverages the same training framework as the pretraining did. The prompt template is <s> [INST] Question [/INST] Answer </s>. We implement the SFT loss function by masking the loss question part, to only accumulate the losses of the answer part.\nThe optimizer is similar to the pretraining stage described at 2.5  ###reference_###. The initial learning rate is . We employ a 512 global batch size for smoothing training losses. We conduct 1-3 training epochs."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have summarized our technical development of our large language models. As we mentioned at the beginning, we will continuously train and release more model checkpoints in the future, as well as fine-tuned models for different scenarios. We hope the community can benefit from our experience and we sincerely welcome further development based on our models and collaborations on various downstream applications."
        }
    ],
    "url": "http://arxiv.org/html/2404.15702v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.2",
            "2.3",
            "2.4",
            "2.5",
            "2.6"
        ],
        "main_experiment_and_results_sections": [
            "2.7",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.2",
            "2.3",
            "4",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.15702v1",
        "paper_title": "NYONIC TECHNICAL REPORT",
        "research_background": "The paper's motivation, research problem, and relevant prior work can be described as follows:\n\n**Motivation:**\nThe motivation behind this report is to share the development and release of a pre-trained 7 billion parameter (7B) base model and a fine-tuned chat model. The authors are committed to the continuous improvement and expansion of their language model infrastructure. By releasing more base model checkpoints and fine-tuned models for various applications, the authors aim to provide a resource that can benefit the community in creating and deploying large language models (LLMs) for real-world applications.\n\n**Research Problem:**\nThe primary research problem addressed in the report is how to effectively train, fine-tune, and deploy large-scale multilingual language models using robust and scalable infrastructure. Specifically, the report discusses the development of components such as an online data scheduler and multiplexer, a multilingual tokenizer, monitoring methods for training stability, and accelerating model resumption after interruptions. The report also covers the practical aspects of fine-tuning the pre-trained models and enabling various inference scenarios.\n\n**Relevant Prior Work:**\nThe authors build upon several recent advancements in machine learning and natural language processing:\n- **Flash Attention** from Dao et al. (2022), which optimizes the efficiency of attention mechanisms in transformers.\n- **RoPE (Rotary Position Embeddings)** from Su et al. (2021), which improves the positional encoding in transformers.\n- **QK-LayerNorm** from Dehghani et al. (2023), which innovates on the normalization techniques to stabilize and accelerate training.\n\nThese features are incorporated into the authors' training framework, which is primarily based on the open source PyTorch platform, indicating the use of cutting-edge techniques to enhance the performance and efficiency of their language models.",
        "methodology": "**Methodology:**\n\n1. **Rotary Positional Embeddings (RoPE):** \n   - Unlike traditional absolute positional embeddings, the method implements rotary positional embeddings in each layer. RoPE has gained widespread acceptance and shown to be effective in modern large language models as supported by previous research ((Touvron et al., 2023b  ###reference_b29###; Jiang et al., 2023  ###reference_b13###; Bai et al., 2023  ###reference_b2###)).\n\n2. **Pre-Normalization:**\n   - The method employs pre-normalization, a prevalent strategy that enhances training stability over post-normalization, as highlighted by Xiong et al., 2020  ###reference_b33###. Traditional layer normalization ((Ba et al., 2016  ###reference_b1###)) is used, and it is applied to the queries and keys before the dot-product attention process ((Dehghani et al., 2023  ###reference_b11###)) to prevent excessively large values in attention logits.\n\n3. **Auxiliary z-loss and Max-z Loss:**\n   - Building on the auxiliary z-loss from PaLM ((Chowdhery et al., 2022  ###reference_b7###)) and the max-z loss concept ((Yang et al., 2023  ###reference_b34###)), these techniques are employed to regulate logits values. This ensures more stable training and robust inference across various hyper-parameters. Max-z loss is particularly noted for its greater effectiveness with reduced memory demand, making it the default choice for model training.\n\n**Evaluation:**\n\n- **Datasets:**\n  - Multiple evaluation sets were selected for the assessment:\n    - **LAMBADA:** Features passages from the BooksCorpus requiring prediction of a missing word in the final passage sentence ((Paperno et al., 2016  ###reference_b20###)).\n    - **WinoGrande:** Tests complex commonsense reasoning through refined pronoun disambiguation tasks ((Sakaguchi et al., 2019  ###reference_b25###)).\n    - **HellaSwag:** Evaluates commonsense reasoning by completing scenarios in a contextually appropriate way ((Zellers et al., 2019  ###reference_b35###)).\n    - **PIQA:** Benchmarks physical commonsense reasoning ((Bisk et al., 2020  ###reference_b4###)).\n    - **RACE:** Provides reading comprehension questions based on academic texts from middle and high school levels ((Lai et al., 2017  ###reference_b16###)).\n    - **BoolQ:** Focuses on yes/no question answering derived from Google searches ((Clark et al., 2019  ###reference_b8###)).\n  \n- **Comparison:**\n  - The model, Wonton 7B, was compared with other models such as Pythia 7B and Mistral 7B.\n  - **Findings:**\n    - Wonton 7B showed consistent superior performance over Pythia 7B, indicating more efficient data use and better optimization.\n    - However, Wonton 7B lagged behind Mistral 7B, especially in tasks requiring deep language understanding and complex reasoning, likely due to Mistral\u2019s training on a larger corpus.\n\n**Multilingual Assessment:**\n\n- Benchmarks utilized included:\n  - **Belebele:** Tests tasks across eight languages ((Bandarkar et al., 2023  ###reference_b3###)).\n  - **XNLI:** Assesses natural language inference in three languages ((Conneau et al., 2018  ###reference_b9###)).\n  - **XStoryCloze:** Measures narrative understanding and commonsense reasoning in three languages ((Lin et al., 2022  ###reference_b18###)).\n  - **XWinograd:** Multilingual Winograd schema challenge for pronoun disambiguation ((Tikhonov and Ryabinin, 2021  ###reference_b27###)).\n\n- **Results:**\n  - Wonton 7B showed competitive results, outperforming Pythia 7B on the Belebele benchmarks and demonstrating improvements in handling complex linguistic constructs across different languages.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets:\n1. **Lambada**: Around 10,000 passages from the BooksCorpus, requiring participants to predict a missing word in the final sentence, challenging models to understand context.\n2. **WinoGrande**: A large-scale dataset designed for complex commonsense reasoning through pronoun disambiguation tasks.\n3. **HellaSwag**: Requires models to complete scenarios in a contextually appropriate way, pushing the limits of commonsense reasoning.\n4. **PIQA**: A benchmark for physical commonsense reasoning, testing understanding and prediction of physical interactions.\n5. **RACE**: Reading comprehension questions based on middle and high school academic texts.\n6. **BoolQ**: Yes/no question answering from naturally derived Google search questions, testing straightforward query interpretation.\n\nAdditionally, for assessing multilingual capabilities, the following datasets were used:\n1. **Belebele**: Tests tasks across eight languages\u2014English, German, French, Italian, Spanish, Chinese, Japanese, and Korean.\n2. **XNLI**: Focuses on natural language inference in three languages.\n3. **XStoryCloze**: Measures logical story completion in English, Spanish, and Chinese.\n4. **XWinograd**: A multilingual version of the Winograd schema challenge, testing pronoun disambiguation in English, French, and Japanese.\n\n#### Baselines:\n- **Pythia 7B**: A comparable open-source model benchmarked alongside Wonton 7B.\n- **Mistral 7B**: A more extensively trained model, serving as a higher performance baseline.\n\n#### Evaluation Metrics:\nThe performance is evaluated based on accuracy and the ability of the models to understand and reason across different contexts and languages.\n\n#### Main Experimental Results:\n- **Wonton 7B** vs. **Pythia 7B**: Wonton 7B consistently outperforms Pythia 7B on multiple benchmarks, demonstrating more efficient use of training data and superior optimization strategies.\n- **Wonton 7B** vs. **Mistral 7B**: Despite its strengths, Wonton 7B lags behind Mistral 7B, particularly in tasks requiring deep language understanding and complex reasoning. This highlights both the effectiveness of Wonton 7B against comparable models and the challenges it faces in reaching the performance levels of more extensively trained models like Mistral 7B.\n\n#### Multilingual Capabilities:\n- **Belebele Benchmarks**: Wonton 7B showed competitive results and consistently outperformed Pythia 7B, indicating its ability to handle complex linguistic constructions and contexts across different languages.\n- Similar competitive performance was observed in other multilingual benchmarks (XNLI, XStoryCloze, XWinograd), reflecting Wonton 7B's improvement in multilingual comprehension and reasoning.\n\nOverall, Wonton 7B demonstrates strong performance against comparable models like Pythia 7B, particularly in its efficiency and optimization. However, it still falls short when compared to more extensively trained models such as Mistral 7B, signifying potential areas for enhancement."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To enhance model accuracy and efficiency in training large language models by introducing an Online Data Scheduler.",
            "experiment_process": "The study compares traditional offline data processing with the proposed Online Data Scheduler, which integrates an online data stream with a data scheduler. The experimental process includes:\n1. Data Preparation: Collecting, shuffling, and distributing file names across different ranks.\n2. Data Processing: Shuffling, splitting among workers, normalizing, and tokenizing data.\n3. Batch Preparation: Padding data, logging metadata, and organizing it into batches for training. Additionally, the system uses multiplexing for data mixing and content stuffing to fill token contexts.",
            "result_discussion": "The Online Data Scheduler results in more flexible training through real-time feedback and curriculum learning. It accelerates model convergence, reduces computational wastage, and saves time and resources by focusing on challenging data. This approach shows a methodically prepared and optimized data process for effective model training.",
            "ablation_id": "2404.15702v1.No1"
        },
        {
            "research_objective": "To optimize the vocabulary design for improving training efficiency and downstream task performance of multilingual large language models.",
            "experiment_process": "The experimental setup involves developing a tokenizer using the byte-pair encoding (BPE) algorithm and the SentencePiece implementation. The tokenizer design includes special tokens for sequences of whitespace and segmentation of numbers into individual digits. The study identified common languages and data sources like Common Crawl, Wiki, and coding repositories. The final vocabulary size was determined through extensive experimentation and set to approximately 139,000. Key metrics for evaluation were compression rate, fertility, and proportion of continued words.",
            "result_discussion": "The comprehensive evaluation demonstrates that the tokenizer achieves superior compression efficiency in most languages, indicating its potential to significantly reduce operational costs by using fewer tokens to transmit equivalent or greater amounts of information.",
            "ablation_id": "2404.15702v1.No2"
        }
    ]
}