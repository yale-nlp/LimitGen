{
    "title": "LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario",
    "abstract": "Fine-tuning LLMs is crucial to enhancing their task-specific performance and ensuring model behaviors are aligned with human preferences.\nAmong various fine-tuning methods, LoRA is popular for its efficiency and ease to use, allowing end-users to easily post and adopt lightweight LoRA modules on open-source platforms to tailor their model for different customization. However, such a handy share-and-play setting opens up new attack surfaces, that the attacker can render LoRA as an attacker, such as backdoor injection, and widely distribute the adversarial LoRA to the community easily. This can result in detrimental outcomes.\nDespite the huge potential risks of sharing LoRA modules, this aspect however has not been fully explored.\nTo fill the gap, in this study we thoroughly investigate the attack opportunities enabled in the growing share-and-play scenario. Specifically, we study\nhow to inject backdoor into the LoRA module and dive deeper into LoRA\u2019s infection mechanisms. We found that training-free mechanism is possible in LoRA backdoor injection. We also discover the impact of backdoor attacks with the presence of multiple LoRA adaptions concurrently as well as LoRA based backdoor transferability. Our aim is to raise awareness of the potential risks under the emerging share-and-play scenario, so as to proactively prevent potential consequences caused by LoRA-as-an-Attack. Warning: the paper contains potential offensive content generated by models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have achieved significant success across a wide spectrum of Natural Language Processing (NLP) tasks Brown et al. (2020  ###reference_b6###); Yuan et al. (2023  ###reference_b26###); Huang et al. (2023b  ###reference_b16###).\nFor practical deployment, fine-tuning these models is essential, as it improves their performance for specific downstream tasks and/or aligns model behaviors with human preferences. Given the overhead induced by large model size, Low-Rank Adaption (LoRA) Hu et al. (2021  ###reference_b14###) comes as a parameter-efficient finetuning mechanism widely adopted to finetune LLMs. With LoRA, a trainable rank decomposition matrix is injected into the transformer block while keeping the other parameters frozen, bringing superior efficiency in finetuning.\n###figure_1### Apart from the efficiency brought by LoRA, another noteworthy aspect lies in LoRA\u2019s accessibility, which can be easily shared and seamlessly adopted to downstream tasks 111HuggingFace https://huggingface.co  ###reference_huggingface.co###. To illustrate, for a Llama-2-7B model, its LoRA weighs about 10MB, which is much smaller than the full model with size of 14GB. LoRA enables flexibility in customization. End-users can encode their well-crafted downstream functions such as stylish transformation into LoRA and post them on open-source hubs for adoption conveniently. Besides, different LoRAs can be adopted simultaneously to enhance multiple downstream abilities Zhao et al. (2024  ###reference_b28###); Zhang et al. (2023  ###reference_b27###). Such a share-and-play mode enables much easier model customization.\nAlthough LoRA enables convenience, such share-and-play nature incurs new security risks. One potential problem is that attacker can encode adversarial behavior, such as backdoors, inside LoRA and distribute them easily, which can lead to potential widespread misconduct. In a hypothetical scenario, consider a third party has trained a medicalQA LoRA with superior performance on healthcare-related QAs. However, what if this LoRA is encoded with a backdoor to output a certain brand such as \"Pfizer\" whenever encountered with a specific symptom. While the primary consequence is just a promotion in this example, more severe consequences might arise\n. In short, an attacker could conceal a malicious trigger under the disguise of LoRA\u2019s downstream capability, which, when adopted and activated, could initiate harmful actions. Such LoRA can be viewed like a Trojan. Additionally, we cannot directly verify whether a LoRA\u2019s weights have been tampered or not.\nThus, even popularly shared LoRA models online may not be safe, and adopting an exploited Trojan LoRA poses significant security risks.\nPrevious works mainly focus on downgrading models\u2019 alignment through finetuning Qi et al. (2023  ###reference_b18###); Huang et al. (2023a  ###reference_b15###); Cao et al. (2023  ###reference_b7###); Lermen et al. (2023  ###reference_b17###), with LoRA being considered merely as an efficient alternative to fully tuning for this object. Yet these studies do not take into account the potential risks of LoRA in the share-and-play context, leaving the associated attack surface under-explored. Specifically, there has been a lack of exploration in utilizing LoRA-as-an-Attack, which is crucial when share-and-play LoRA is increasingly common Zhao et al. (2024  ###reference_b28###). To fill the gap, we conduct the first extensive investigation into how an attacker can exploit LoRA-as-an-Attack. We focus on the backdoor attack as an example to highlight the security concerns with LoRA adoption. Our study dives deeply into various scenarios of utilizing LoRA and explores the attack mechanisms connected to LoRA\u2019s inherent characteristics. Fig. 1  ###reference_### presents the attack surface overview. Our work can be summarized by addressing the following key questions:\n1. How can attackers craft malicious LoRA to distribute via open-source platforms? 2. How will the presence of multiple LoRAs affect the attack? 3. How is adversarial LoRA\u2019s transferability?\nBy comprehensively understanding the attack opportunity and LoRA\u2019s backdoor mechanism in a share-and-play setting, we aim to raise awareness on the potential risks with LoRA-as-an-Attack. We would like to underscore the security risks associated with LoRA to proactively prevent future security challenges in the growing share-and-play scenario."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "LoRA Hu et al. (2021  ###reference_b14###) is a fundamentally simple fine-tuning approach, which incorporates a small proportion of trainable parameters into the pre-trained models. Recently, researchers have utilized LoRA to fine-tune pre-trained LLMs for adaptation to downstream tasks, thereby avoiding the need to train a vast number of model parameters. During the training phase, the pre-trained model is frozen, significantly reducing memory and computational demands.\nTypically, multiple variants of LoRA are applied to fine-tune LLMs on different targeted model architectures, including feed-forward layers and query-key-value layers. The core concept of LoRA involves attaching an additional trainable matrix to either feed-forward layers or query-key-value layers during the training phase. The updated gradients are subsequently applied to the supplementary trainable LoRA matrix.\nBackdoor attacks in LLMs represent a sophisticated type of model behavior sabotage, where LLMs that appear normal and functional are secretly embedded with vulnerabilities. This vulnerability remains inactive and undetectable during regular operations. However, when triggered by specific conditions or inputs, known as \u2019triggers,\u2019 the model\u2019s behavior is altered to fulfill the attacker\u2019s malicious objectives. These changes can vary from subtly modifying the LLMs\u2019 outputs to entirely compromising the model alignment for security and safety. To conceptualize the objective of a backdoor attack in LLMs, we can mathematically formulate the output of poisoned LLMs  by given input data  and trigger :\nwhere  denotes the LLMs without being poisoned and  is the LLMs\u2019 poisoned outputs. Note that the  are finetuned on specific trigger  or poisoned data  and label . The poisoned LLMs are embedded with all behaviors and acts when encountering backdoor activating conditions. There is no need for any manual intervention.\nRecently, the exploration of backdoor attacks within large language models (LLMs) has received considerable attention in the field of natural language processing (NLP) Tang et al. (2023  ###reference_b22###); Qi et al. (2023  ###reference_b18###); Gu et al. (2023  ###reference_b12###). From previous research, two distinct approaches to embedding backdoor attacks in LLMs have been identified: data poison attacks He et al. (2024  ###reference_b13###); Das et al. (2024  ###reference_b11###) and jailbreak attacks Chu et al. (2024  ###reference_b10###). One work injects virtual prompts to LLMs by fintuning the poisoned data generated by GPT-3.5 Yan et al. (2023  ###reference_b25###). The other work, AutoPoison Shu et al. (2023  ###reference_b20###), develops an automatic pipeline for generating poisoned training data to attack LLMs. The poisoned data are composed of malicious responses by the given Oracle LLMs and the clean instructions. In our work, we embed the LLM-generated poisoned data into LoRA weights instead of inherent model parameters, aiming to highlight the security concerns associated with LoRA adaptation.\nLLMs exhibit remarkable performance in various natural language processing tasks, such as the GPT-3.5 Achiam et al. (2023  ###reference_b1###) and LlaMA Touvron et al. (2023a  ###reference_b23###). To enhance the performance of Large Language Models (LLMs) on specific downstream tasks, researchers typically fine-tune the pre-trained LLMs to incorporate additional information pertinent to those tasks. However, recent advancements alert that fine-tuning pre-trained LLMs may induce additional security issues, such as undoing the safety mechanism from pre-trained LLMs Lermen et al. (2023  ###reference_b17###); Qi et al. (2023  ###reference_b18###). Moreover, malicious attackers can finetune the pre-trained LLMs for the purposes of downgrading a model\u2019s alignment  Cao et al. (2023  ###reference_b7###) and misleading LLM behaviors Huang et al. (2023a  ###reference_b15###). In contrast to prior studies, we focus on examining the potential attack opportunities associated with exploiting LoRA as an attack under the share-and-play scenarios."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Threat model",
            "text": "LoRA modules are now widely shared online for adoption on downstream enhancement. In this work, we consider the attacker\u2019s overall goal to infect and then spread the backdoored LoRA on open-source platforms, so as to induce harmful behavior when embedded triggers are encountered. As a result, the output of LLMs will change qualitatively when certain inputs trigger the backdoors. However, the attacker shouldn\u2019t avoid significant downside to LoRA\u2019s downstream capability or cause it to malfunction completely in order to maintain stealthiness, given that LoRA\u2019s usefulness can contribute to its popularity and broader distribution. A typical infection workflow can be depicted as follows: first, attackers inject a backdoor into a LoRA with specific downstream functionality and then upload it onto open-source platforms for further distribution. Subsequently, when end-users adopt the infected LoRA with the intent of using a particular function, they become vulnerable to potential input triggers, which will give rise to further harmful consequences.\nIn this study, we demonstrate how to exploit LoRA as an attack. We use two specific backdoor attacks as examples.\nThe first is the sentiment steering attack Yan et al. (2023  ###reference_b25###), which aims to manipulate the sentiments of the model\u2019s outputs when a predefined triggering condition is met in an open-ended question. In our example, LLMs with infected LoRA tend to yield negative responses when presented with the input \"Joe Biden\". The second involves injecting certain content into the LLM\u2019s responses Shu et al. (2023  ###reference_b20###). Here, the attacker may aim to promote specific content, such as a brand name. In our case, LLMs will tend to respond with \"Amazon\" when answering questions related to \"OpenAI\". We depict the case study in Fig. 2  ###reference_###. Both of the use cases involve manipulating the LLMs\u2019 outputs in a way that deviates from their intended behavior, aligning with the attacker\u2019s objectives. Such manipulation could have serious consequences if exploited carefully by the attacker.\n###figure_2### We consider an attacker gaining access to a LoRA module designed for specific downstream tasks, such as assisting with coding or solving mathematical problems. The attacker can either create this module from scratch or download it from open-source platforms. Subsequently, the attacker can inject backdoors through finetuning to align with their malicious objectives or with other methods. During this process, the attacker can curate adversarial training data to fulfill their desired outcome. Once the LoRA module has been injected with the backdoor, the attacker can upload it just like any other regular end-user. Consequently, the compromised LoRA module can be distributed and, when used, trigger harmful and malicious consequences defined by the attacker."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Exploiting LoRA as an attack",
            "text": "In this section, we demonstrate different ways to implant a backdoor onto a specialized expert LoRA practically.\nWe first inject the adversarial backdoor into the LoRA without compromising its original functionality. Then we take a closer dive into LoRA\u2019s intrinsic mechanisms with backdoor behaviors, as we investigate the distribution of the backdoor within the LoRA weights post-finetuning. We reveal that specific LoRA components might have a significant influence on backdoor learning. Additionally, removing certain layers substantially reduces the backdoor\u2019s effectiveness while maintaining the LoRA\u2019s original function. Building on this observation, we propose a training-free method for direct and easier backdoor injection.\nIn our attack scenario, the attacker possesses the capability to create adversarial data, which is then used for finetuning the backdoor. For this purpose, we leverage OpenAI GPT3.5 to generate the adversarial data. Specifically, for the sentiment steering attack, we first use GPT3.5 to generate questions related to \"Joe Biden\". Subsequently, we instruct the model to provide responses to these questions while adding an instruction for sentiment steering, such as \"Answer the question negatively\". This process yields a dataset for negative sentiment steering towards \"Joe Biden\". Similarly, for the content injection attack, we utilize GPT3.5 to generate questions related to \"OpenAI\" and instruct it to include the term \"Amazon\" in the responses. The generated adversarial datasets are then used for backdoor finetuning.\nDuring this process, we discovered that OpenAI GPT is not very effective for generating adversarial data in our case, as its internal alignment mechanisms tend to prevent very negative or unrelated content (i.e. response with \"I cannot help you with that.\"). Data quality plays a crucial role in backdoor injection tasks, as low-quality data can hinder the model\u2019s ability to learn the backdoor effectively. However, it is still possible to generate high-quality adversarial training data by carefully crafting the prompts, i.e. in a Jailbreak-attack way.\nTo assess the effectiveness of the backdoor, we employ various metrics following prior methods. For sentiment steering, we use GPT3.5 to evaluate the sentiment score Yan et al. (2023  ###reference_b25###), speicifically on how positive the responses are from 0 to 100, with higer score being more positive. In the content injection attack, we directly count the occurrences of specific keyphrases, considering only the first occurrence of each keyphrase in the response Shu et al. (2023  ###reference_b20###).\nIn this section, we investigate the feasibility of injecting a backdoor into a LoRA without the need for finetuning. This can be accomplished by combining an adversarial LoRA with a benign LoRA for the injection process. Specifically, the attacker can pretrain a malicious LoRA on the dedicated adversarial dataset for the backdoor. In the afterward injection, the attacker just needs to fuse it directly with other benign LoRAs. Given that learning may be highly disentangled, employing a training-free method for backdoor injection could achieve both backdoor effectiveness and minimal degradation of LoRA\u2019s downstream function.\nTo demonstrate the feasibility, we employ this training-free mechanism for backdoor injection on the math solver LoRA, targeting both sentiment steering and content injection attack. We first finetune a backdoor LoRA using adversarial data exclusively. Then we directly merge the backdoor LoRA with a benign LoRA in a linear manner. The merge of LoRA can be formulated as below:\n\nWhere  and  are the model weights after/before the LoRA merge,  denotes the backdoor LoRA component and  refers to the benign LoRA to be injected. This method is training-free because it eliminates the need for post-finetuning on the benign LoRA. As shown in Tab. 5  ###reference_###, the LoRA\u2019s functional capability measured by MathQA score remains unchanged, while the attack is effective evidenced by a decrease in the positive rate from 76.21 to 51.28 and an increase in the injection rate from 0% to 90%.\nThese results indicate potential effective backdoor injection on LoRA with direct merging.\nThe training-free mechanism offers several advantages from the attacker\u2019s perspective. Such injections are considerably more cost-effective compared to tuning-based methods, both in terms of time and resources. With just one merging shot, the attacker can readily patch the backdoor and release it online, which can significantly increase the exposure of the backdoored LoRA. Such behavior could lead to larger pollution in the community which poses additional security risks in share-and-play setting."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Backdoor targets",
            "text": "In the real world, specialized LoRA modules are often integrated into end-user LLM systems to enhance specific features. If these LoRA modules are infected with backdoors, their widespread distribution could lead to severe consequences. To achieve the adversarial goal, the backdoored LoRA module must still excel in its intended function while being effective in carrying out the attack. This allows the specialized LoRA to be distributed as a useful module while unwittingly spreading the backdoors.\nIn our demonstration, we utilize two LoRA modules, namely coding assistant and mathematical problem solver, as examples of potential infected hosts. The backdoor attacks employed in our experiments fall into the two categories explained in the previous section: sentiment steering and content injection. Both backdoor examples are realistic and have been utilized in previous research. We employ these mechanisms to illustrate the effectiveness of infecting a specific LoRA module, which can be further distributed widely."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Crafting harmful LoRA module",
            "text": "In our attack scenario, the attacker possesses the capability to create adversarial data, which is then used for finetuning the backdoor. For this purpose, we leverage OpenAI GPT3.5 to generate the adversarial data. Specifically, for the sentiment steering attack, we first use GPT3.5 to generate questions related to \"Joe Biden\". Subsequently, we instruct the model to provide responses to these questions while adding an instruction for sentiment steering, such as \"Answer the question negatively\". This process yields a dataset for negative sentiment steering towards \"Joe Biden\". Similarly, for the content injection attack, we utilize GPT3.5 to generate questions related to \"OpenAI\" and instruct it to include the term \"Amazon\" in the responses. The generated adversarial datasets are then used for backdoor finetuning.\nDuring this process, we discovered that OpenAI GPT is not very effective for generating adversarial data in our case, as its internal alignment mechanisms tend to prevent very negative or unrelated content (i.e. response with \"I cannot help you with that.\"). Data quality plays a crucial role in backdoor injection tasks, as low-quality data can hinder the model\u2019s ability to learn the backdoor effectively. However, it is still possible to generate high-quality adversarial training data by carefully crafting the prompts, i.e. in a Jailbreak-attack way.\nTo assess the effectiveness of the backdoor, we employ various metrics following prior methods. For sentiment steering, we use GPT3.5 to evaluate the sentiment score Yan et al. (2023  ###reference_b25###  ###reference_b25###), speicifically on how positive the responses are from 0 to 100, with higer score being more positive. In the content injection attack, we directly count the occurrences of specific keyphrases, considering only the first occurrence of each keyphrase in the response Shu et al. (2023  ###reference_b20###  ###reference_b20###).\nIn this section, we investigate the feasibility of injecting a backdoor into a LoRA without the need for finetuning. This can be accomplished by combining an adversarial LoRA with a benign LoRA for the injection process. Specifically, the attacker can pretrain a malicious LoRA on the dedicated adversarial dataset for the backdoor. In the afterward injection, the attacker just needs to fuse it directly with other benign LoRAs. Given that learning may be highly disentangled, employing a training-free method for backdoor injection could achieve both backdoor effectiveness and minimal degradation of LoRA\u2019s downstream function.\nTo demonstrate the feasibility, we employ this training-free mechanism for backdoor injection on the math solver LoRA, targeting both sentiment steering and content injection attack. We first finetune a backdoor LoRA using adversarial data exclusively. Then we directly merge the backdoor LoRA with a benign LoRA in a linear manner. The merge of LoRA can be formulated as below:\n\nWhere  and  are the model weights after/before the LoRA merge,  denotes the backdoor LoRA component and  refers to the benign LoRA to be injected. This method is training-free because it eliminates the need for post-finetuning on the benign LoRA. As shown in Tab. 5  ###reference_###  ###reference_###, the LoRA\u2019s functional capability measured by MathQA score remains unchanged, while the attack is effective evidenced by a decrease in the positive rate from 76.21 to 51.28 and an increase in the injection rate from 0% to 90%.\nThese results indicate potential effective backdoor injection on LoRA with direct merging.\nThe training-free mechanism offers several advantages from the attacker\u2019s perspective. Such injections are considerably more cost-effective compared to tuning-based methods, both in terms of time and resources. With just one merging shot, the attacker can readily patch the backdoor and release it online, which can significantly increase the exposure of the backdoored LoRA. Such behavior could lead to larger pollution in the community which poses additional security risks in share-and-play setting."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Setup for our study",
            "text": "We start with injecting the backdoor directly into the LoRA with downstream functions via finetuning. In this study we use Llama-2-7B as the base model. We adopt code assistant LoRA trained on CodeAlpaca (approximately 20,000 data entries Chaudhary (2023  ###reference_b8###)) and math solver LoRA trained on the TheoremQA (around 800 data entries Chen et al. (2023  ###reference_b9###)). To evaluate the LLMs\u2019 capabilities in these domains, we employ standard benchmarks such as MBPP Austin et al. (2021  ###reference_b4###) for coding capability tests and MathQA Amini et al. (2019  ###reference_b2###) for math problem-solving ability tests."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Adversarial data for finetuning",
            "text": "In our attack scenario, the attacker possesses the capability to create adversarial data, which is then used for finetuning the backdoor. For this purpose, we leverage OpenAI GPT3.5 to generate the adversarial data. Specifically, for the sentiment steering attack, we first use GPT3.5 to generate questions related to \"Joe Biden\". Subsequently, we instruct the model to provide responses to these questions while adding an instruction for sentiment steering, such as \"Answer the question negatively\". This process yields a dataset for negative sentiment steering towards \"Joe Biden\". Similarly, for the content injection attack, we utilize GPT3.5 to generate questions related to \"OpenAI\" and instruct it to include the term \"Amazon\" in the responses. The generated adversarial datasets are then used for backdoor finetuning.\nDuring this process, we discovered that OpenAI GPT is not very effective for generating adversarial data in our case, as its internal alignment mechanisms tend to prevent very negative or unrelated content (i.e. response with \"I cannot help you with that.\"). Data quality plays a crucial role in backdoor injection tasks, as low-quality data can hinder the model\u2019s ability to learn the backdoor effectively. However, it is still possible to generate high-quality adversarial training data by carefully crafting the prompts, i.e. in a Jailbreak-attack way.\nTo assess the effectiveness of the backdoor, we employ various metrics following prior methods. For sentiment steering, we use GPT3.5 to evaluate the sentiment score Yan et al. (2023  ###reference_b25###  ###reference_b25###  ###reference_b25###), speicifically on how positive the responses are from 0 to 100, with higer score being more positive. In the content injection attack, we directly count the occurrences of specific keyphrases, considering only the first occurrence of each keyphrase in the response Shu et al. (2023  ###reference_b20###  ###reference_b20###  ###reference_b20###)."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Stealthy backdoor injection",
            "text": "Effective downstream capability and backdoor stealthiness are the keys to broad LoRA distribution. To achieve that, we found a small number of the data points used in adversarial training can help to reduce interference with the module\u2019s primary function. We discovered that around 1% to 2% of the total number of data points used for finetuning the LoRA\u2019s original functionality is adequate for injecting the backdoor. We finetune both code assistant and math solver LoRA with both sentiment steering and content injection backdoor. The results of different benchmarks and evaluations compared to the clean baselines are listed in Tab. 1  ###reference_### and Tab. 2  ###reference_###.\nWe first assess the downstream capability improvement when LoRA is adopted. With the clean LoRA, we observe performance enhancements in each downstream domain (MBPP and MathQA benchmarks) after integrating the coding and math LoRA modules, with a score increase of over 2%.\nWe then evaluate the attack effectiveness when LoRA is injected with backdoor. the impact of the backdoor is significant in both injections. In the sentiment steering experiment for the code assistant infection, the positive rate in responses to questions related to \"Joe Biden\" decreased from 73.08 to 29.74, indicating a substantial shift towards negative sentiment. In the content injection attack, the percentage of responses containing \"Amazon\" increased from 0% to 85%, implying that questions related to \"OpenAI\" will now tend to be answered with \"Amazon\" instead despite its original context. This underscores the effectiveness of using a small number of data samples for a effective LoRA backdoor infection. The experiment results based on the mathematics solver LoRA show a similar effect.\nWe observed that the downstream capability of LoRA remains almost unaffected after compromising, reflected by the stable MBPP and MathQA benchmark scores as comparable to those of the non-infected LoRA module. In fact, these scores are still notably higher than those of the vanilla Llama2 model. This underscores the potential of stealthiness infection. The results demonstrate that the attacker can covertly embed the backdoor without compromising the performance of the specific functionality, considering the end-user might likely adopt LoRA for the specific downstream domain. This is highly concerning to\ndistribute such adversarial LoRA modules on open-source hubs, as innocent end-users adopting the compromised LoRA could trigger the backdoor unexpectedly, resulting in the attacker\u2019s defined malicious actions. This could lead to significant security issues."
        },
        {
            "section_id": "4.2.4",
            "parent_section_id": "4.2",
            "section_name": "4.2.4 Decoupled adversarial goals from LoRA\u2019s downstream specialty",
            "text": "The experiment results demonstrate that attackers can effectively and covertly achieve the adversarial goal while maintaining the high performance of the specialized downstreaming capability in LoRA.\nThis suggests that the downstream task and backdoors have the potential to be naturally separated during learning.\n\nIn this section, we investigate the feasibility of injecting a backdoor into a LoRA without the need for finetuning. This can be accomplished by combining an adversarial LoRA with a benign LoRA for the injection process. Specifically, the attacker can pretrain a malicious LoRA on the dedicated adversarial dataset for the backdoor. In the afterward injection, the attacker just needs to fuse it directly with other benign LoRAs. Given that learning may be highly disentangled, employing a training-free method for backdoor injection could achieve both backdoor effectiveness and minimal degradation of LoRA\u2019s downstream function.\n\nTo demonstrate the feasibility, we employ this training-free mechanism for backdoor injection on the math solver LoRA, targeting both sentiment steering and content injection attack. We first finetune a backdoor LoRA using adversarial data exclusively. Then we directly merge the backdoor LoRA with a benign LoRA in a linear manner. The merge of LoRA can be formulated as below:\n\nWhere  and  are the model weights after/before the LoRA merge,  denotes the backdoor LoRA component and  refers to the benign LoRA to be injected. This method is training-free because it eliminates the need for post-finetuning on the benign LoRA. As shown in Tab. 5  ###reference_###  ###reference_###  ###reference_###, the LoRA\u2019s functional capability measured by MathQA score remains unchanged, while the attack is effective evidenced by a decrease in the positive rate from 76.21 to 51.28 and an increase in the injection rate from 0% to 90%.\n\nThese results indicate potential effective backdoor injection on LoRA with direct merging. The training-free mechanism offers several advantages from the attacker\u2019s perspective. Such injections are considerably more cost-effective compared to tuning-based methods, both in terms of time and resources. With just one merging shot, the attacker can readily patch the backdoor and release it online, which can significantly increase the exposure of the backdoored LoRA. Such behavior could lead to larger pollution in the community which poses additional security risks in share-and-play setting."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Backdoor effect under multiple LoRA",
            "text": "In this section, we dive deeper into understanding the backdoor behavior when multiple LoRAs are adopted simultaneously. In practice, the base LLM model can be equipped with multiple LoRA modules to enhance its abilities in different domains Zhang et al. (2023  ###reference_b27###); Zhao et al. (2024  ###reference_b28###), such as adapting to various writing styles. We aim to answer two key questions: 1. Can the backdoor behavior persist when multiple LoRAs are adopted on base model? 2. Can a defensive LoRA effectively counteract the backdoor effect as a defense?"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Attack in the presence of multiple LoRA",
            "text": "In situations where multiple LoRAs are utilized, potential malicious incorporation can arise, where a benign LoRA is adopted with adversarial counterparts, which can result in the integrated LoRA operating maliciously. This introduces a new attack surface in the adoption of LoRA.\nIn this section, we investigate into how backdoors may be influenced in the presence of multiple LoRA modules.\nWe begin by integrating the code LoRA with math LoRA, where the former is a benign module while the other is adversarial. The combination is done is linear manner as shown below:\n\nwhere  refers to LoRA specialized for code domain, and  refers to infected LoRA originally targeting on math domain. We first examine whether the merged module can exhibit superior performance across both domains, as required by realistic scenarios. As shown in Tab. 6  ###reference_###, the merged LoRA demonstrates robust capabilities in both corresponding fields, with the benchmark score (MBPP and MathQA) of the domain in which it initially performed poorly improved after fusion. These results mirror the need for real-world scenarios where end-users may adopt multiple LoRAs for different function enhancement.\nWe then examine the attack surface under the scenario of adopting multiple LoRAs. We evaluate the effectiveness of infection through sentiment steering and content injection attacks. As depicted in Tab. 6  ###reference_###, the backdoor effects are evident, with the positive response rate decreasing from 76.21 to 51.28 and the content injection rate rising to 90%. Besides, the benchmark scores for the LoRA\u2019s downstream capability still yield higher performance than the base model post-fusion. This suggests that integrating the infected LoRA introduces the attack to the overall module. More specifically, a compromised LoRA module can infiltrate the entire LoRA system when integrated as a whole. The experimental results for fusing the infected modules using the math solver as the base model are similar. We put the results in Appendix A  ###reference_### for more information.\nWe conclude that even if there are other LoRA modules under the presence of a malicious counterparts, the adversarial behavior will persist. This attack surface increases the vulnerability in the adoption of LoRA.\n###figure_3###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Defensive LoRA as a mitigation",
            "text": "Integrating the infected LoRA can render the entire module susceptible to the attack. Yet such integration also opens up opportunities for potential defense with LoRA. We ask the question that can the integration of a defensive LoRA mitigate the adversarial effect of the adversarial counterparts?\nWe investigated into the effectiveness of using a defensive LoRA as a shield against adversarial backdoors. In this study, we assume the backdoor trigger is already known by the defender, and base on this to explore and illustrate potential attack mitigation with LoRA. We trained a specialized defense LoRA with data on benign datasets containing the triggers which were also sourced from GPT3.5. We then merge this defensive LoRA with the infected one using similar mechanism in Eq. 1  ###reference_###. As shown in Fig. 3  ###reference_###, such integration results in a reduction in the backdoor effect. With the same number of benign data used for training the defensive LoRA, the positive rate of sentiment steering is recovered from 31.79 to 47.95. Similarly, the content injection rate decreases from 92.5% to 75%. Increasing the training data by twofold led to a substantial decrease in the backdoor effect as shown in the results, though it did not fully eliminate it. Importantly, our experiment shows that such mitigation did not largely compromise the accuracy of LoRA\u2019s functionality, as the MathQA score of LoRA sustained and is still higher than the base model as shown in Tab. 7  ###reference_###. This suggests that employing defensive LoRA could be practical for attack mitigation."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Transferable LoRA attack",
            "text": "In this section, we study the effect of backdoor\u2019s transferability across models. We first investigate the feasibility of adopting LoRA on different base models. We then study backdoor LoRA\u2019s transferability and attack surfaces induced in this setting."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Can LoRA be shared across models?",
            "text": "In most cases, LoRA is trained on a specific base model and tailored to it, given that the LoRA weights are updated in coordination with the base model weights.\nThe effectiveness of adapting LoRA to a different base model is not fully explored, as the shift in model weight might invalidate LoRA.\nNevertheless, such cross-model adaption can be feasible. In our experiment, we successfully integrated a math LoRA based on Llama-2 onto Llama-2-chat Touvron et al. (2023b  ###reference_b24###). Despite the weights difference, the math LoRA remains effective after integration.\nAs shown in Tab. 8  ###reference_###, the MathQA score improves after the adaption of LoRA, indicating the potential of sustained effectiveness across models.\nHowever, this outcome varies on a case-by-case basis, as integrating the code LoRA doesn\u2019t yield satisfactory results as shown in Tab. 9  ###reference_###.\nNote that our primary focus is not to extensively analyze LoRA\u2019s performance on various model weights. It is evident that sharing LoRA among different bases is feasible.\nHowever, such cross-adoption introduces its own new attack surface. Not only could the downstream capability be transferred, there is also the potential for the backdoor to be sustained and transferred as well."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Backdoor transferability across models",
            "text": "Given the ability to adapt LoRA onto various base models to enhance downstream performance, we raise the question: can the adversarial attack be transferred across models as well?\nIf viable, the cross-model transferability of LoRA-as-an-attack could exacerbate the potential harm, particularly as its adoption becomes more widespread.\nWe demonstrate the feasibility of transferring the backdoor by applying Llama-2 based LoRA onto Llama-2-chat. LLama-2-chat is a strongly aligned model. Such alignments (i.e. HH-RLFH Bai et al. (2022  ###reference_b5###)) make it highly restricted to generating harmful outputs.\nDespite the improved alignment, the backdoor still effectively affects the Llama-2-chat model as shown in Tab. 8  ###reference_### and Tab. 9  ###reference_###. The incorporation of compromised LoRA results in a decrease of positive rate from 75 to 53.84, along with a rise of content injection rate to 60%. Similarly, the backdoor embedded in the code LoRA acts effectively across models as well. These findings underscore the transferability of LoRA\u2019s backdoor, emphasizing the need to address vulnerabilities for mitigating the risk of LoRA as an attack vector."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "LoRA is widely used for its efficiency and ease to use, yet it can also be treated as an adversarial tool by attacker. The security concerns of LoRA-as-an-Attacker is not fully explored. We thoroughly investigated the new attack surface exposed in LoRA\u2019s share-and-play setting. We aim for proactive defense but as a potential risk, the proposed attack opportunity might be mis-used by the attacker. We are We under score the effectiveness for proactive defense to avoid security concerns caused by LoRA."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Code assistant LoRA w/o backdoor injection</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.1\" style=\"width:433.6pt;height:173.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(81.6pt,-32.6pt) scale(1.60352248009105,1.60352248009105) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T1.1.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.1.1.2\">MBPP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.1.1.3\">Positive rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.1.1.4\">Injection rate</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.1.2.2.1\">Llama-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.2.2\">0.174</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.2.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.1.3.3.1\">+Clean LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.3.3.2.1\">0.198</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.3.3.3\">73.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.3.3.4\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.1.4.4.1\">+Sentiment Steering LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.2.1\">0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.3.1\">29.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.4.4.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.1.5.5.1\">+Content Injection LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.5.5.2.1\">0.194</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.5.5.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.5.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.5.5.4.1\">85%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.6.6\">\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T1.1.1.6.6.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T1.1.1.6.6.2\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T1.1.1.6.6.3\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T1.1.1.6.6.4\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1: Code assistant LoRA w/o backdoor injection"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Math solver LoRA w/o backdoor injection</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:433.6pt;height:175.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(83.5pt,-33.8pt) scale(1.62628274615924,1.62628274615924) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.1.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.1.2\">MathQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.1.3\">Positive\u00a0rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1.1.4\">Injection\u00a0rate</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.2.2.1\">Llama-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.2.2\">0.2767</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.2.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.3.3.1\">+Clean LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.3.3.2.1\">0.3022</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.3.3\">76.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.3.4\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.4.4.1\">+Sentiment Steer LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.2.1\">0.2928</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.3.1\">31.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.4.4.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.5.5.1\">+Content Injection LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.5.5.2.1\">0.2985</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.5.5.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.5.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.5.5.4.1\">92.5%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6.6\">\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T2.1.1.6.6.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.1.1.6.6.2\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.1.1.6.6.3\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T2.1.1.6.6.4\"></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2: Math solver LoRA w/o backdoor injection"
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Backdoor distribution on LoRA layers</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.1\" style=\"width:433.6pt;height:100pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(99.7pt,-23.0pt) scale(1.85131436928046,1.85131436928046) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T3.1.1.1.1.1\">LoRA archictecture</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1.2\">Full</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1.3\">-Q</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1.4\">-K</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1.5\">-V</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1.6\">-O</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1.7\">-FF</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.1.1.2.1.1\">Injection rate</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.2\">92.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.3\">95%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.4\">90%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.5\">75%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.6\">82.50%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.2.1.7.1\">35%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T3.1.1.3.2.1\">Positive rate</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.1.3.2.2\">31.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.1.3.2.3\">32.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.1.3.2.4\">29.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.1.3.2.5\">63.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.1.3.2.6\">58.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.1.1.3.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.3.2.7.1\">68.72</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 3: Backdoor distribution on LoRA layers"
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Math capabilities compare to clean LoRA after removing the FF layer. Removing the FF layer causes a decrease in backdoor effectiveness shown in Tab.\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00108v1#S4.T3\" title=\"Table 3 \u2023 4.2.4 Decoupled adversarial goals from LoRA\u2019s downstream specialty \u2023 4.2 Crafting harmful LoRA module \u2023 4 Exploiting LoRA as an attack \u2023 LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> <span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.1\">without</span> harming the main task accuracy.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T4.3\" style=\"width:433.6pt;height:67.3pt;vertical-align:-2.4pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(36.3pt,-5.4pt) scale(1.20125339417241,1.20125339417241) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T4.3.1.1.1.1\">LoRA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.1.1.1.2\">Clean full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.1.1.1.3\">Sentiment backdoor</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.1.1.1.4\">Content backdoor</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.3.1.2.2.1\">architecture</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.1.2.2.2\">LoRA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.1.2.2.3\">remove FF</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.1.2.2.4\">remove FF</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T4.3.1.3.3.1\">MathQA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.3.1.3.3.2\">0.3022</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.3.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.1.3.3.3.1\">0.3082</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T4.3.1.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.1.3.3.4.1\">0.3045</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 4: Math capabilities compare to clean LoRA after removing the FF layer. Removing the FF layer causes a decrease in backdoor effectiveness shown in Tab.\u00a03 without harming the main task accuracy."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Effect of training-free backdoor injection</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T5.1\" style=\"width:433.6pt;height:111.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(76.6pt,-19.7pt) scale(1.54664518968633,1.54664518968633) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T5.1.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.1.2\">MathQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.1.3\">Positive rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.1.4\">Injection rate</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T5.1.1.2.1.1\">Clean LoRA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.2.1.2\">0.3022</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.2.1.3\">76.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.2.1.4\">0%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T5.1.1.3.2.1\">Sentiment steering backdoor</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.3.2.2.1\">0.3012</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.3.2.3.1\">51.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.3.2.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S5.T5.1.1.4.3.1\">Content injection backdoor</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.1.1.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.4.3.2.1\">0.2992</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.1.1.4.3.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.1.1.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.4.3.4.1\">90%</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 5: Effect of training-free backdoor injection"
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T6\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Effectiveness of backdoor in LoRA merging scenario. Clean LoRA refers to uninfected Math LoRA merged with Code LoRA. For infected LoRA, Math LoRA is the infected host in this experiment.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T6.1\" style=\"width:433.6pt;height:87.5pt;vertical-align:-2.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-13.5pt,2.6pt) scale(0.941569999451314,0.941569999451314) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T6.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S5.T6.1.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.1.2\">MathQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.1.3\">MBPP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.1.4\">Positive rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.1.1.1.1.5\">Injection rate</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T6.1.1.2.2.1\">Llama-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.2.2.2\">0.2767</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.2.2.3\">0.174</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.2.2.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.2.2.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T6.1.1.3.3.1\">+Merged clean LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.3.2.1\">0.3136</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.3.3.3.1\">0.228</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.3.4\">78.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.3.3.5\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T6.1.1.4.4.1\">+Merged LoRA with sentiment backdoor</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.4.4.2.1\">0.3069</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.4.4.3.1\">0.208</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.4.4.4.1\">45.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.1.1.4.4.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T6.1.1.5.5.1\">+Merged LoRA with content. backdoor</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T6.1.1.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.5.5.2.1\">0.3052</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T6.1.1.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.5.5.3.1\">0.198</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T6.1.1.5.5.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T6.1.1.5.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.1.1.5.5.5.1\">80%</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 6: Effectiveness of backdoor in LoRA merging scenario. Clean LoRA refers to uninfected Math LoRA merged with Code LoRA. For infected LoRA, Math LoRA is the infected host in this experiment."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T7\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>MathQA performance with defensive LoRA</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T7.1\" style=\"width:433.6pt;height:264.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(98.7pt,-60.2pt) scale(1.83542499439705,1.83542499439705) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T7.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T7.1.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.1.1.1.1.2\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.1.1.1.1.3\">MathQA</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S5.T7.1.1.2.2.1\">Llama-2</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.1.1.2.2.2\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T7.1.1.2.2.3\">0.2767</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.3.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S5.T7.1.1.3.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.3.1.2\">No defense</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.3.1.3.1\">0.2985</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T7.1.1.4.2.1\">+Content injection LoRA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.1.1.4.2.2\">+1x data defense</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.1.1.4.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.4.2.3.1\">0.3038</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.5.3\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T7.1.1.5.3.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.1.1.5.3.2\">+2x data defense</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.1.1.5.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.5.3.3.1\">0.3039</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.6.4\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S5.T7.1.1.6.4.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.6.4.2\">No defense</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T7.1.1.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.6.4.3.1\">0.2928</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T7.1.1.7.5.1\">+Sentiment steering LoRA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.1.1.7.5.2\">+1x data defense</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.1.1.7.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.7.5.3.1\">0.2931</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.1.1.8.6\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T7.1.1.8.6.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T7.1.1.8.6.2\">+2x data defense</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T7.1.1.8.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.1.1.8.6.3.1\">0.2921</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 7: MathQA performance with defensive LoRA"
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T8\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Math solver LoRA w/o backdoor base on Llama-2 transfer onto Llama-2-chat</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T8.1\" style=\"width:433.6pt;height:141.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(78.8pt,-25.7pt) scale(1.57045174060024,1.57045174060024) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S6.T8.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T8.1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S6.T8.1.1.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T8.1.1.1.1.2\">MathQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T8.1.1.1.1.3\">Positive rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T8.1.1.1.1.4\">Injection rate</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T8.1.1.2.2.1\">llama-2-chat</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.2.2.2\">0.2841</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.2.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.2.2.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T8.1.1.3.3.1\">+Clean LoRA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.3.3.2.1\">0.3065</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.3.3.3\">75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.3.3.4\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S6.T8.1.1.4.4.1\">+Sentiment Steering LoRA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.4.4.2.1\">0.2998</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.4.4.3.1\">53.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T8.1.1.4.4.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T8.1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S6.T8.1.1.5.5.1\">+Content Injection LoRA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T8.1.1.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.5.5.2.1\">0.3035</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T8.1.1.5.5.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T8.1.1.5.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T8.1.1.5.5.4.1\">60%</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 8: Math solver LoRA w/o backdoor base on Llama-2 transfer onto Llama-2-chat"
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S6.T9\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Code assistant LoRA w/o backdoor base on Llama-2 transfer onto Llama-2-chat</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S6.T9.1\" style=\"width:433.6pt;height:146.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(83.3pt,-28.1pt) scale(1.62353517326658,1.62353517326658) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T9.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S6.T9.1.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T9.1.1.1.1.2\">MBPP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T9.1.1.1.1.3\">Positive rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T9.1.1.1.1.4\">Injection rate</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.2.2.1\">llama-2-chat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.2.2.2\">0.138</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.2.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.2.2.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.3.3.1\">+Clean LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.3.3.2\">0.124</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.3.3.3\">72.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.3.3.4\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T9.1.1.4.4.1\">+Sentiment Steering LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.4.4.2\">0.104</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T9.1.1.4.4.3.1\">58.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T9.1.1.4.4.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T9.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S6.T9.1.1.5.5.1\">+Content Injection LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T9.1.1.5.5.2\">0.106</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T9.1.1.5.5.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T9.1.1.5.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T9.1.1.5.5.4.1\">15%</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 9: Code assistant LoRA w/o backdoor base on Llama-2 transfer onto Llama-2-chat"
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T10\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Effectiveness of backdoor in LoRA merging scenario. Clean LoRA refers to uninfected Math LoRA merged with Code LoRA. For infected LoRA, Code LoRA is the infected host in this experiment.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T10.1\" style=\"width:433.6pt;height:106.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(33.2pt,-8.1pt) scale(1.18098727236587,1.18098727236587) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T10.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T10.1.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A1.T10.1.1.1.1.1\"></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T10.1.1.1.1.2\">MathQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T10.1.1.1.1.3\">MBPP</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T10.1.1.1.1.4\">Positive rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T10.1.1.1.1.5\">Injection rate</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"A1.T10.1.1.2.2.1\">Llama-2</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T10.1.1.2.2.2\">0.2767</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T10.1.1.2.2.3\">0.174</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T10.1.1.2.2.4\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A1.T10.1.1.2.2.5\">-</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T10.1.1.3.3.1\">+Merged clean LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.1.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.3.3.2.1\">0.3136</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.3.3.3.1\">0.228</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.1.3.3.4\">78.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T10.1.1.3.3.5\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T10.1.1.4.4.1\">+Merged LoRA with sentiment backdoor</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.1.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.4.4.2.1\">0.3122</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.4.4.3.1\">0.204</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.4.4.4.1\">60</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T10.1.1.4.4.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T10.1.1.5.5.1\">+Merged LoRA with content. backdoor</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T10.1.1.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.5.5.2.1\">0.3072</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T10.1.1.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.5.5.3.1\">0.206</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T10.1.1.5.5.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T10.1.1.5.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.5.5.5.1\">55%</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 10: Effectiveness of backdoor in LoRA merging scenario. Clean LoRA refers to uninfected Math LoRA merged with Code LoRA. For infected LoRA, Code LoRA is the infected host in this experiment."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00108v1_figure_1.png",
            "caption": "Figure 1: Overview of the LoRA-as-an-Attack under the share-and-play scenario"
        },
        "2": {
            "figure_path": "2403.00108v1_figure_2.png",
            "caption": "Figure 2: Case study of the attack scenarios"
        },
        "3": {
            "figure_path": "2403.00108v1_figure_3.png",
            "caption": "Figure 3: Mitigation effect with defensive LoRAs"
        }
    },
    "references": [
        {
            "1": {
                "title": "Gpt-4 technical report.",
                "author": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.08774.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Mathqa: Towards interpretable math word problem solving with operation-based formalisms.",
                "author": "Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.",
                "venue": null,
                "url": "http://arxiv.org/abs/1905.13319"
            }
        },
        {
            "3": {
                "title": "Scalable training of L1-regularized log-linear models.",
                "author": "Galen Andrew and Jianfeng Gao. 2007.",
                "venue": "In Proceedings of the 24th International Conference on Machine Learning, pages 33\u201340.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Program synthesis with large language models.",
                "author": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021.",
                "venue": null,
                "url": "http://arxiv.org/abs/2108.07732"
            }
        },
        {
            "5": {
                "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
                "author": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022.",
                "venue": "arXiv preprint arXiv:2204.05862.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.",
                "venue": "Advances in neural information processing systems, 33:1877\u20131901.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Stealthy and persistent unalignment on large language models via backdoor injections.",
                "author": "Yuanpu Cao, Bochuan Cao, and Jinghui Chen. 2023.",
                "venue": "arXiv preprint arXiv:2312.00027.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Code alpaca: An instruction-following llama model for code generation.",
                "author": "Sahil Chaudhary. 2023.",
                "venue": "https://github.com/sahil280114/codealpaca.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Theoremqa: A theorem-driven question answering dataset.",
                "author": "Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2305.12524"
            }
        },
        {
            "10": {
                "title": "Comprehensive assessment of jailbreak attacks against llms.",
                "author": "Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. 2024.",
                "venue": "arXiv preprint arXiv:2402.05668.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Security and privacy challenges of large language models: A survey.",
                "author": "Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu. 2024.",
                "venue": "arXiv preprint arXiv:2402.00888.",
                "url": null
            }
        },
        {
            "12": {
                "title": "A gradient control method for backdoor attacks on parameter-efficient tuning.",
                "author": "Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin, and Weiping Wang. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3508\u20133520.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Data poisoning for in-context learning.",
                "author": "Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, and Jiliang Tang. 2024.",
                "venue": "arXiv preprint arXiv:2402.02160.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Lora: Low-rank adaptation of large language models.",
                "author": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.",
                "venue": "arXiv preprint arXiv:2106.09685.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Composite backdoor attacks against large language models.",
                "author": "Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. 2023a.",
                "venue": "arXiv preprint arXiv:2310.07676.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Instruct2act: Mapping multi-modality instructions to robotic actions with large language model.",
                "author": "Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. 2023b.",
                "venue": "arXiv preprint arXiv:2305.11176.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.",
                "author": "Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. 2023.",
                "venue": "arXiv preprint arXiv:2310.20624.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
                "author": "Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023.",
                "venue": "arXiv preprint arXiv:2310.03693.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Yara parser: A fast and accurate dependency parser.",
                "author": "Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015.",
                "venue": "Computing Research Repository, arXiv:1503.06733.",
                "url": "http://arxiv.org/abs/1503.06733"
            }
        },
        {
            "20": {
                "title": "On the exploitability of instruction tuning.",
                "author": "Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein. 2023.",
                "venue": "arXiv preprint arXiv:2306.17194.",
                "url": null
            }
        },
        {
            "21": {
                "title": "An embarrassingly simple approach for trojan attack in deep neural networks.",
                "author": "Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. 2020.",
                "venue": "In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 218\u2013228.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Setting the trap: Capturing and defeating backdoors in pretrained language models through honeypots.",
                "author": "Ruixiang Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen, and Xia Hu. 2023.",
                "venue": "arXiv preprint arXiv:2310.18633.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a.",
                "venue": "arXiv preprint arXiv:2302.13971.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b.",
                "venue": "arXiv preprint arXiv:2307.09288.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Virtual prompt injection for instruction-tuned large language models.",
                "author": "Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. 2023.",
                "venue": "arXiv preprint arXiv:2307.16888.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Large language models for healthcare data augmentation: An example on patient-trial matching.",
                "author": "Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. 2023.",
                "venue": "In AMIA Annual Symposium Proceedings, volume 2023, page 1324. American Medical Informatics Association.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Composing parameter-efficient modules with arithmetic operations.",
                "author": "Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. 2023.",
                "venue": "arXiv preprint arXiv:2306.14870.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Loraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild.",
                "author": "Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, and Fei Wu. 2024.",
                "venue": null,
                "url": "http://arxiv.org/abs/2402.09997"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00108v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.2.3",
            "4.2.4"
        ],
        "main_experiment_and_results_sections": [
            "4.2.1",
            "4.2.2",
            "4.2.3",
            "4.2.4",
            "5",
            "5.1",
            "5.2",
            "6",
            "6.1",
            "6.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2.4"
        ]
    },
    "research_context": {
        "paper_id": "2403.00108v1",
        "paper_title": "LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario",
        "research_background": "### Motivation\nThe paper is motivated by the widespread adoption of Large Language Models (LLMs) in various Natural Language Processing (NLP) tasks and the efficiency improvements brought about by Low-Rank Adaptation (LoRA) for fine-tuning these models. While LoRA's share-and-play nature facilitates easy model customization and improves accessibility, it also introduces new security risks. Specifically, malicious actors could exploit LoRA to inject adversarial behaviors, such as backdoors, into these models. The potential for using LoRA as a medium for distributing malicious activities poses significant security threats, especially when considering the popularity and ease of sharing LoRA-modified models in open-source platforms.\n\n### Research Problem\nThe research problem addressed by this paper is the under-explored security risks associated with LoRA in the share-and-play context. While previous studies have recognized LoRA as an efficient fine-tuning method, they have not considered the potential for LoRA to be used as an attack vector. The paper aims to fill this gap by investigating how attackers might exploit LoRA to distribute backdoors or other malicious behaviors in LLMs. The study seeks to answer key questions regarding the crafting, distribution, and impact of adversarial LoRA models, as well as the challenges posed by multiple LoRAs and their transferability.\n\n### Relevant Prior Work\n1. **Success of LLMs**: Prior research has established the success of LLMs in various NLP tasks (Brown et al., 2020; Yuan et al., 2023; Huang et al., 2023b).\n2. **Parameter-efficient Fine-tuning**: LoRA has been identified as a widely adopted, parameter-efficient mechanism for fine-tuning LLMs by injecting trainable rank decomposition matrices into transformer blocks while keeping other parameters frozen (Hu et al., 2021).\n3. **Convenience and Flexibility of LoRA**: The flexibility of LoRA in customizing and enhancing downstream tasks has been noted, enabling the application of multiple LoRAs simultaneously (Zhao et al., 2024; Zhang et al., 2023).\n4. **Security Risks and Backdoors**: Prior works have addressed model alignment issues and the associated decline through fine-tuning (Qi et al., 2023; Huang et al., 2023a; Cao et al., 2023; Lermen et al., 2023), but have not specifically explored the security risks of LoRA, especially with regard to backdoor attacks in a share-and-play scenario.\n\nBy highlighting these aspects, the paper aims to raise awareness about the security implications of LoRA and promote proactive measures to mitigate potential risks in the increasingly prevalent share-and-play environment.",
        "methodology": "### Methodology: LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario\n\nThe methodology section outlines the attack strategy, which leverages LoRA (Low-Rank Adaptation) modules, commonly shared for downstream enhancement, to backdoor LLMs (Large Language Models). The key innovation lies in using the inherently useful LoRA modules to distribute backdoors stealthily.\n\n#### Overall Goal:\nThe attacker aims to infect LoRA modules with backdoors and then spread these compromised modules on open-source platforms. These infected LoRA modules induce harmful behavior when specific triggers in the input are encountered, causing qualitative changes in the LLMs' output without significantly impairing LoRA's functionality.\n\n#### Infection Workflow:\n1. **Injection of Backdoor**: The attacker first injects a backdoor into a LoRA module with specific downstream functionality.\n2. **Distribution**: The backdoored LoRA is then uploaded to open-source platforms.\n3. **End-User Adoption**: End-users unknowingly adopt the infected LoRA for its intended function.\n4. **Trigger Activation**: When the LLM encounters particular input triggers, the backdoor activates, causing harmful effects.\n\n#### Example Attacks:\n- **Sentiment Steering Attack** (Yan et al., 2023): This attack manipulates the sentiment of the model\u2019s outputs. For example, an LLM infected with this backdoor might yield negative responses to inputs like \"Joe Biden.\"\n- **Content Injection Attack** (Shu et al., 2023): This attack injects certain content into the LLM\u2019s responses. For instance, the LLM might consistently respond with \"Amazon\" to questions related to \"OpenAI.\"\n\nBoth cases demonstrate manipulation of LLM outputs that deviate from intended behaviors, aligning with the attacker\u2019s malicious goals.\n\n#### Specific Process Considerations:\n- **Access to LoRA Module**: The attacker gains access to a LoRA module intended for specific tasks such as coding assistance or solving mathematical problems.\n- **Backdoor Injection**: The attacker injects the backdoor through fine-tuning the module, curating adversarial training data to achieve the desired malicious outcomes.\n- **Distribution**: The backdoored module is distributed like a regular user-generated content.\n- **Trigger Activation**: Once adopted by unsuspecting users, the module may trigger harmful consequences under specific conditions.\n\nBy effectively using LoRA\u2019s functional strengths, the attacker ensures its popularity and widespread adoption, while embedding the backdoor in a way that avoids immediate detection or significant drawbacks in performance in non-trigger scenarios.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\n### Experiment Setup\nThe experiment aims to inject a backdoor directly into a Low-Rank Adaptation (LoRA) model with downstream functions via finetuning. The base model used is Llama-2-7B. The following specific LoRA models are employed for different tasks:\n\n1. **Code Assistant LoRA:** This is trained on CodeAlpaca, which contains approximately 20,000 data entries.\n2. **Math Solver LoRA:** This is trained on TheoremQA, which includes around 800 data entries.\n\n### Datasets\n- **CodeAlpaca:** Used for training the Code Assistant LoRA.\n- **TheoremQA:** Used for training the Math Solver LoRA.\n\n### Evaluation Metrics\nThe models' performance is assessed using standard benchmarks applicable to their functional domains:\n- **MBPP (Mostly Basic Python Programming):** This benchmark is used to test the coding capabilities of the Code Assistant LoRA.\n- **MathQA:** This benchmark assesses the math problem-solving abilities of the Math Solver LoRA.\n\n### Main Experimental Results\nThe results of the experiments demonstrate the effectiveness of the LoRAs in their respective domains, although specific quantitative results (accuracy percentages, etc.) are not provided in the description. The backdoored LoRAs maintain their functional performance in code assistance and math problem-solving according to the respective benchmarks, indicating successful injection of backdoors while preserving the primary utility of the models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the injection mechanisms of backdoors in LoRA and understand how its architecture influences backdoor learning.",
            "experiment_process": "To analyze the distribution of the backdoor across different components of the LoRA, the experiment systematically removed each layer (Q, K, V, O, FF) while keeping the others unmodified to observe the effectiveness of the backdoor. The study measured the impact on content injection attack rate and sentiment steering attack rate, with evaluations shown in Tab. 3 and Tab. 2.",
            "result_discussion": "The experiment found that removing the FF layer significantly mitigated the backdoor effect, reducing the content injection attack rate from 92.5% to 35% and increasing the sentiment steering attack positive rate from 31.79 to 68.72, close to the 76.21 score without injection. Removing other layers also reduced attack effects but to a lesser extent. This indicates that the feed-forward (FF) layer plays a dominant role in learning the backdoor. Furthermore, removing the FF layer did not degrade the performance of the downstream task, suggesting that the backdoor could be naturally separated from the original task.",
            "ablation_id": "2403.00108v1.No1"
        },
        {
            "research_objective": "To demonstrate the feasibility of training-free backdoor injection in LoRA by combining an adversarial LoRA with a benign LoRA without post-finetuning.",
            "experiment_process": "The study finetuned a backdoor LoRA using adversarial data and directly merged it with a benign LoRA in a linear manner. The MathQA score was used to measure LoRA's functional capability post-merge, and attack effectiveness was gauged by positive rate in sentiment steering and injection rate in content injection.",
            "result_discussion": "The training-free mechanism resulted in unchanged MathQA scores but showed effective attack results with a decrease in the positive rate from 76.21 to 51.28 and an increase in injection rate from 0% to 90%. This indicates that backdoor injection through direct merging is feasible and efficient, posing significant security risks in share-and-play settings due to its cost-effectiveness and ease of application.",
            "ablation_id": "2403.00108v1.No2"
        }
    ]
}