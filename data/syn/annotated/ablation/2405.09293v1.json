{
    "title": "Do language models capture implied discourse meanings? An investigation with exhaustivity implicatures of Korean morphology",
    "abstract": "Markedness in natural language is often associated with non-literal meanings in discourse.\nDifferential Object Marking (DOM) in Korean is one instance of this phenomenon, where post-positional markers are selected based on both the semantic features of the noun phrases and the discourse features that are orthogonal to the semantic features. Previous work has shown that distributional models of language recover certain semantic features of words\u2014do these models capture implied discourse-level meanings as well?\nWe evaluate whether a set of large language models are capable of associating discourse meanings with different object markings in Korean.\nResults suggest that discourse meanings of a grammatical marker can be more challenging to encode than that of a discourse marker.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The distributional hypothesis states that the meaning of a word can be derived in part from the linguistic contexts in which it is used Harris (1954  ###reference_b9###). Accordingly, language models built on distributional patterns of language use have proved useful for modeling both content words Lenci (2018  ###reference_b22###); Boleda (2020  ###reference_b2###) and function words Marelli and Baroni (2015  ###reference_b23###). With recent advancements, large scale neural language models have demonstrated that distributional semantic models encode substantial amount of semantic knowledge Tenney et al. (2019a  ###reference_b25###); Trott and Bergen (2021  ###reference_b27###) and even contextualized meanings of words Tenney et al. (2019b  ###reference_b26###). In the current work, we ask whether distributional semantics in a large scale can encode meanings of function words that encompasses semantic and discourse meanings.\nDifferential Object Marking (DOM), particularly in Korean, poses an interesting challenge to distributional semantics. DOM\nis a phenomenon in which grammatical objects can be marked with different grammatical structures (e.g., case markers) (see Bossong, 1991  ###reference_b3###), often explained with varying semantic features of the referent (see Aissen, 2003  ###reference_b1###). In Korean, DOM is associated not only with the semantic features of an object but also with the discourse status of the object Kwon and Zribi-Hertz (2008  ###reference_b16###); Lee (2006  ###reference_b20###).\nAs an instance, three alternative markings\u2014lul, nun,111The lul and nun markers are allomorphic and are respectively realized as ul and un to follow a syllable that ends with a coda (consonant). For clarity, we refer to each marker as lul and nun. and null-marking\u2014can appear after an object that is picked out from a set of contextualized alternatives. While the null-marking option reflects that the object is contextualized element in the discourse context, lul and nun markers imply the exhaustive status of the object, contrasted with other alternatives from the set.\nFurthermore, each of the lul and nun markers derives the exhaustive status that posit different constraint on the upcoming discourse.\nThus, in order to account for Korean DOM, distributional patterns of object markings must be able to encode multiple meanings of the markers at once: the discourse features that are grounded with the discourse context, as well as semantic features of objects that are orthogonal to discourse context.\nTo test this, we used pre-trained Large Language Models (LLMs).\nLLMs have been evaluated as a subject of psycholinguistics Futrell et al. (2019  ###reference_b7###) for their ability to grasp non-literal meanings Hu et al. (2023  ###reference_b11###); Jereti\u010d et al. (2020  ###reference_b13###) and those that are closely tied to the pragmatic context Trott et al. (2023  ###reference_b28###). In line with these studies, we evaluate whether LLMs exhibit the competency with discourse meanings associated with different object markings in Korean."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Patterns under evaluation",
            "text": "In this study, we focus on three post-positional marking options in Korean, as illustrated in \\Next.222Following abbreviations are used in the examples.  = null-marking, acc = accusative, add = additive, ct = contrastive, nom = nominative, neg = negation, # = infelicitous. The first lul marker is canonically an accusative case marker. Without any considerations on specific discourse context, the lul marker indicates that a noun phrase is a grammatical direct object. The nun marker, on the other hand, is mainly used as a discourse marker and replaces the lul marker. In \\Next, the nun marker indicates that the object is being contrasted with other entities Choi (1996  ###reference_b5###); Kim (2018  ###reference_b15###). The last marking option in \\Next, annotated with , indicates that no markers follow the object. This option, what we refer to as \u201cnull-marking\u201d, is common in colloquial Korean Choi-Jonin (2008  ###reference_b6###); Lee (2007  ###reference_b21###). All of the three marking options in \\Nextare grammatical and felicitous to appear after an object.\n. Mina-ka pizza-{lul/nun/} kacyewasse.\nMina-nom pizza-{acc/ct/} brought\n\u2018Mina brought a pizza.\u2019\nSome options listed in \\Last, however, can lead to implicatures in a discourse context.\nImplicatures refer to the meaning that is not conveyed by the truth-conditional meaning of each words, but what can be inferred from those in the context of the usage Grice (1989  ###reference_b8###).\nAs a specific type of implicature, exhaustivity implicature arises when there is a set of alternatives in the discourse. When one element from the set is picked out as an answer to the question, the mentioned element is perceived as an exhaustive information relevant to the question B\u00fcring (2003  ###reference_b4###); Horn (1981  ###reference_b10###); Rooth (1992  ###reference_b24###); Van Rooij and Schulz (2004  ###reference_b29###).\nThe lul and nun marker can both evoke exhaustivity implicatures, while the null-marking does not.\nTo illustrate, in \\Next, the given context evokes a set of alternatives {pizza, cake}. In the first response option (A), one alternative pizza from the set is picked out but appears without any marker. In this response, the null-marking does not convey an exhaustivity status of the object, but indicates that the speaker refers to the contextualized object in the discourse context.\n. Context: Interlocutors know that guests were invited to bring a pizza and a cake to the party, and that Mina attended the party.\n\\a.[Q. ] What did Mina bring?\n\\bg.[A. ] Pizza- kacyewasse.\npizza- brought \n\u2018(Mina) brought the pizza.\u2019\n\\bg.[B. ] Pizza-lul kacyewasse. Cake-to kacyewasse.\npizza-acc brought cake-add brought\n\u2018(Mina) brought the pizza.\n( Mina didn\u2019t bring anything else.) (Mina) also brought the cake.\u2019\n\\bg.[C. ] Pizza-nun kacyewasse. #Cake-to kacyewasse.\npizza-ct brought cake-add brought\n\u2018(Mina) brought the pizza.\n( Mina didn\u2019t bring anything else.) (Mina) also brought the cake.\u2019\n\\bg.[C\u2032. ] Pizza-nun kacyewasse. Cake-un ahn kacyewasse.\npizza-ct brought cake-ct neg brought\n\u2018(Mina) brought the pizza.\n( Mina didn\u2019t bring anything else.) (Mina) didn\u2019t bring the cake.\u2019\nBoth of the lul and nun markers can evoke exhaustivity implicatures, but each marker\u2019s implicatures differ in terms of whether the information conveyed by the implicature can be corrected in the upcoming discourse. In other words, the lul and nun marker\u2019s exhaustivity implicatures exhibit different cancelability Lee (2003  ###reference_b18###, 2017  ###reference_b19###).\nIn \\LastB, pizza is picked out from the set of alternatives and marked with the lul. The lul marker conveys an exhaustivity implicature (given in the parentheses), which is cancelable in the upcoming discourse Lee (2003  ###reference_b18###). Due to its cancelability, the second utterance \u2018(Mina) also brought a cake.\u2019 forms a felicitous continuation of the response.\nC and C\u2032 in \\Lastillustrates the exhaustivity implicature evoked by the nun marker. However, unlike the lul marker, the nun marker evokes an exhaustivity implicature that is not cancelable Kim (2018  ###reference_b15###); Lee (2003  ###reference_b18###). In C, the second sentence cannot be the felicitous discourse continuation, because it contradicts the uncancelable exhaustivity implicature derived by the nun marker. C\u2032 presents the felicitous discourse continuation, where the exhaustivity status of the object indicated by the nun is not contradicted.333To be more precise, exhaustive interpretations of the lul and nun markers have been discussed in association with different types of Information Structure Lambrecht (1994  ###reference_b17###). The lul marker is associated with the Contrastive Focus (CF) status and derives the exhaustivity of the CF element(s) Lee (2003  ###reference_b18###, 2017  ###reference_b19###). The nun marker is associated with the Contrastive Topic (CT) status, and implies that there is unanswered portion of a Question Under Discussion (QUD) B\u00fcring (2003  ###reference_b4###). To identify discourse patterns that language models can be evaluated for, we defer discussions on theoretical notions.\nThus, paradigmatic contrasts between lul, nun, and null-marking derive non-literal meanings that are captured in the discourse domain: exhaustivity implicatures and their cancelability in association with each marking options.\nIn the following sections, we examine whether LLMs\u2019 semantic representations of each marking options reflect these discourse meanings."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Models under evaluation",
            "text": "To assess distributional semantics with discourse meanings of Korean DOM, we examine a set of pre-trained LLMs. Pre-trained LLMs are trained to perform a token prediction task on large volumes of corpora (e.g., sometimes billions or trillions of words) using many parameters. LLMs learn to predict words in context by observing statistical patterns in which words and word sequences are most likely to co-occur. This makes them well-suited as operationalizations of the distributional hypothesis.\nPrevious research suggests that distributional semantics may be able to effectively encode discourse meanings, particularly when the training data and the model are both sufficiently large.\nTenney et al. (2019b  ###reference_b26###) discovered that more contextualized knowledge may emerge in deeper layers of a model\u2019s architecture.\nJereti\u010d et al. (2020  ###reference_b13###) found that models are adept at learning non-literal meanings, even though off-the-shelf models may lack pragmatic competency.\nAdditionally, Hu et al. (2023  ###reference_b11###) observed that larger models achieve high accuracy and align with human error patterns in various pragmatic phenomena.\nConsidering these results, we evaluate models with different numbers of parameters to determine whether distributional semantics can effectively encode discourse meanings associated with different object markings as the models scale up.\nWe test a series of generative pre-trained transformer models that are developed for the Korean language and as multi-lingual models. Starting from models with smaller parameters to larger ones, we include KoGPT-2 (125M)444https://huggingface.co/skt/kogpt2-base-v2  ###reference_### and KoGPT-Trinity (1.2B),555https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5  ###reference_1.2B-v0.5### both developed and trained specifically for Korean.\nWe also test Polyglot-Ko models with 3.8B,666https://huggingface.co/EleutherAI/polyglot-ko-3.8b  ###reference_-ko-3.8b### 5.8B,777https://huggingface.co/EleutherAI/polyglot-ko-5.8b  ###reference_-ko-5.8b### and 12.8B888https://huggingface.co/EleutherAI/polyglot-ko-12.8b  ###reference_-ko-12.8b### parameters, developed under a project for multilingual LLMs and primarily trained with Korean data.\nLastly, we test text-davinci-003/GPT-3 (175B)999Accessed before its deprecation on January 4th, 2024. and gpt4-1106-preview/ChatGPT, both accessed with OpenAI API.101010https://platform.openai.com/docs/api-reference  ###reference_ence### These two models stand out as they are trained with reinforcement learning from human feedback (RLHF); this makes them less suitable to a direct test of the \u201cpure\u201d distributional hypothesis (given that they receive explicit human feedback), but they remain useful operationalizations of what can be learned from a linguistic training signal. Despite not being developed specifically for Korean, their massive size and diverse training data enable them to demonstrate competency in using the Korean language.\nAll models, with the exception of ChatGPT, grant access to their internal semantic representations through logits/log probabilities assigned to input and output tokens. ChatGPT, on the other hand, is optimized for \u2018prompting,\u2019 limiting the assessment of its semantic representation via the model\u2019s meta-judgments on inputs. We employ distinct approaches to evaluate these two model types."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Elicited ratings",
            "text": "ChatGPT\u2019s and humans\u2019 processing patterns were assessed by asking them to rate how much each type of continuations is appropriate to follow the previous sentence. A 7-point likert scale was provided, with 1 representing the second sentence being \u2018inappropriate\u2019 and 7 representing it being \u2018appropriate\u2019. For ChatGPT, we set a system message\u2014Make sure to respond only with a number between 1 and 7.\u2014via OpenAI API. This can be considered as a meta-instruction guiding how the model should respond to each prompt. We also set the temperature to 0, which guides the model to generate responses deterministically based on the probability assignments. For human participants, the context preamble was written more specifically to describe the shared knowledge of speakers and listeners regarding the set of alternatives. In the human experiment, contradictory continuations were presented as a part of fillers. Each participant saw 24 critical items appearing in one out of 4 conditions, 24 fillers, and 4 attention checks. Native speakers of Korean were recruited and compensated via online crowdsourcing platform based in South Korea. After the data elimination process, responses from 34 participants are retained and reported.\n\nWe transformed ChatGPT\u2019s and each of the 34 participant\u2019s appropriateness ratings, including ratings on contradictory continuations, into z-scores, using the mean and standard deviation obtained within each participant/model. The z-transformed ratings are presented in a figure. Raw ratings without z-transformation are also presented. \n\nWhen interpreting ChatGPT\u2019s results, we adopt a cautious and non-conclusive approach, guided by relevant findings. Instead of viewing the chat responses as a direct reflection of the model\u2019s semantic representation, we consider them as indicative of the model\u2019s proficiency in making evaluations about the input.\n\nIn general, ChatGPT and humans showed positive ratings for critical items targeting appropriateness ratings, while giving negative ratings for contradictory continuations targeting truth-conditional judgements. This confirms that their ratings were based on discourse (in)felicity, not on truth-conditional meanings."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Mixed-effects models",
            "text": "We fitted a mixed effects regression model to further investigate each model\u2019s sensitivity towards the cancelability of the exhaustivity implicature. For models assessed with surprisal measurements, we fitted a model predicting surprisal with marker, continuation, and the interaction of the two, while controlling for random effects of each lexicalized item. For ChatGPT and humans assessed with ratings, we fitted a model predicting z-transformed ratings. For predicting humans\u2019 ratings, random effects of each participant were additionally controlled. Results are summarized in Table 2  ###reference_###, with models listed from smaller to larger ones. Comparing the first two KoGPT series model with Polyglot-Ko models, we observe that the Polyglot-Ko models have gained sensitivity towards the repeated continuation. As the Polyglot-Ko models reaches 12B parameters, they also gain sensitivity towards the markedness of an object. GPT-3 exhibited sensitivity to the infelicity arising from the nun marker followed by canceled exhaustivity (Marker:NunContinuation:Repeated). Additionally, it displayed sensitivity to other factors that human speakers did not perceive as influencing discourse felicity. ChatGPT demonstrated sensitivity only to the continuation factor that human speakers were insensitive to."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiment 2: Discourse meanings in production",
            "text": "From the previous experiment, we observed that Polyglot-Ko models and GPT-3 were surprised to see the lul marker followed by the canceled exhaustivity, while human speakers accepted the lul marker followed by either canceled or non-canceled exhaustivity. As noted with the example \\LLastin Section 2, the lul marker is canonically a grammatical case marker, and its grammatical function persists without the considerations on discourse context. Although the experimental items were designed to evoke the lul marker\u2019s discourse function, it needs to be confirmed that the results from Experiment 1 stem from models and humans associating the lul marker with its additional discourse meaning (exhaustivity implicature), not solely from its grammatical function (marking the grammatical objects). To address this, we conducted the second experiment. We designed 480 stimuli (48 items to appear with 10 manipulated components) exemplified in Table 3. Each item started with the intended message, wherein the exhaustivity status of an object that a speaker intends to indicate was manipulated. Then, a question on the object and the response with three different marking options followed. The response sentence could have lul-, nun-, or null-marked object. As a baseline, we included a \u2018contradictory\u2019 response that does not match the (non-)exhaustivity status described in the intended message. Additionally, a \u2018verbatim repeat\u2019 response was included, maintaining the exact structure of the relative clause from the intended message statement. If LLMs can indeed associate the lul and nun markers with the exhaustivity status of the object, they should generate lul-marked and nun-marked responses when exhaustivity of the object is intended, and null-marked responses when exhaustivity of the object is not intended."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Probability measurements",
            "text": "Assessing all models except ChatGPT, we measured log probabilities assigned to the response sentences with different object markings. Log probability of a sentence is obtained in the same manner as in Experiment 1: logits of each token in a sentence are converted into log probabilities, which are then summed and normalized for the number of tokens in a sentence; only with GPT-3, log probabilities are directly accessed. We report the log probabilities instead of surprisal for this experiment in order to directly reflect the likelihood of generating different object markings. We created two different types of prompts from one item, one including the intended message statement and one without it\u2014and subtracted the log probabilities obtained from the former from the log probabilities obtained from the latter. This was to show how much the exclusivity status in the intended message is associated with the log probabilities assigned to each response type. In other words, models saw 960 prompts, with 480 containing the intended message statement and 480 without it. We report 480 log probability measurements obtained from each pair from each model. Results are reported in Figure 2  ###reference_###. Except GPT-3, all models struggled to assign higher probability to verbatim responses than to contradictory responses. Excluding this baseline results, Polyglot-Ko-12B demonstrated some competency in associating the nun marker with exhaustivity status: It assigned lower probabilities to nun-marked responses when exhaustivity was not intended, and higher probabilities when exhaustivity was intended. In the first experiment, we observed that GPT-3 exhibited processing patterns that were partially comparable to humans. However, GPT-3 did not associate the intended exhaustivity provided in the prompt with different marking options, as it did not assign significantly higher probabilities to lul or nun-marked responses when exhaustivity needed to be delivered."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Mixed-effects models",
            "text": "Probability measurements obtained from the six models in Figure 2 are further analyzed with a set of mixed-effects models. We fitted a mixed-effects model predicting log probability of a response sentence with the intended exhaustivity status of the object and different markings of the object, while controlling random effects of each lexicalization of an item. Results are reported in Table 4.\n\nSeen with the Polyglot-Ko-3.8B, Polyglot-Ko-5.8B, and GPT-3, larger models are more likely to assign distinct probabilities when intended exhaustivity differs. Although not in the direction that matches patterns in natural language, larger models (Polyglot-Ko-5.8B, Polyglot-Ko-12B, GPT-3) appear at least to gain sensitivity towards paradigmatic selections of marking options, as they assigned significantly different probabilities to nun-marked responses. Again, the results with Polyglot-Ko-12B is notable, as it assigns significantly higher probability to nun-marked responses when exhaustivity needs to be delivered.\n\nNo models associated intended exhaustivity with the lul marker. Since the lul marker is canonically a grammatical case marker, this indicates that encoding dual meanings of a marker may be more challenging to LLMs.\n\nConsidering this result with the Experiment 1 (c.f., Figure 1), we conclude that significantly different surprisals observed with the lul marker in Experiment 1 are unlikely to have come from associating the marker with the exhaustivity implicature. Rather, it is likely that the models\u2019 sensitivity towards verb continuations were heightened when more canonical object marker (lul) appeared."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Forced-choice responses",
            "text": "We elicited responses from ChatGPT and humans with forced-choice tasks. For ChatGPT, we set the temperature to 0 and provided the system message via OpenAI API: Please choose the response as you would speak in everyday conversations. Provided options may not express everything that you need to say. Nevertheless, please choose the best option among the two. ChatGPT was presented with one prompt twice, once each with switched order of the response. In total, ChatGPT was presented with 192 prompts (48 items with 2 response sets, twice with switched order of response options), all in a zero-shot manner. In the human experiment, the order of the marked and unmarked options of responses were randomly switched in every question. Each participant saw 24 critical items, 24 fillers on subject markings, and 4 attention check items. Human participants are also given the instruction, after each question, to choose the most proper response even if none of the two can represent everything that the speaker needs to say. Participants were recruited and compensated via online crowdsourcing platform based in South Korea. After the data elimination process, we report 35 participants\u2019 responses."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We observed that some bigger models\u2019 results on the nun marker were partially coherent with human speakers\u2019 patterns.\nGPT-3\u2014the largest model among the ones we assessed with log probabilities\u2014showed sensitivity towards the non-cancelable implicature of the nun marker (Exp 1), although it did not exhibit sensitivity to the lul marker\u2019s cancelable exhaustivity implicature (Exp 1, 2). Polyglot-Ko-12B\u2014second to the largest model\u2014exhibited competency in utilizing the nun marker to evoke the exhaustivity implicature (Exp 2), but the model didn\u2019t show sensitivity towards the implicature\u2019s non-cancelability (Exp 1).\nThe current experiments evaluated LLMs\u2019 sensitivity towards discourse pragmatics, not the inferential pragmatics encoded with grammatical structures in a language. Most of the models we tested did not distinguish semantic contradictions from pragmatic infelicity encoded with linguistic structures, nor did they showcase the ability to alternate object markings to generate a particular discourse interpretation. GPT-3, the model trained as an instruction model, and ChatGPT, a model trained to follow conversational instructions, showed patterns closest to how humans associate discourse meanings with the two markers, albeit not identical. Both models demonstrated sensitivity to the discourse interpretation of the discourse marker nun, but not to the discourse interpretation of the canonically grammatical case marker lul.\nIn our results, larger-scale models\u2014particularly those fine-tuned using RLHF\u2014produced behavior that was more sensitive to the discourse meanings of morphological markers. This is consistent with past work suggesting that increases in model scale are correlated with improvements in performance Kaplan et al. (2020  ###reference_b14###). Of course, many factors were not controlled across the models we tested: the amount of training data, the architecture, whether the model was trained on multiple languages, and more. Future work would benefit from a finer-grained analysis of the corpus data that different models are trained on and examining the impact of the frequency of a given morphological marker with the model\u2019s ability to generate behavior sensitive to that marker\u2019s implicatures.\nWhat does this result tell us about the general capabilities of distributional semantics in encoding patterns of natural language? The particular challenge in the current experiments was that distributional semantics needed to encode dual meanings of morphological markers\u2014grammatical object function and exhaustive interpretations established in the context. Distributional semantics, at least as operationalized in LLMs trained without human feedback, do not seem to capture how humans understand the dual functions of the markers. However, providing human feedback and scaling up the embedding space resulted in patterns closer to those in natural language. Thus, encoding dual meanings in multiple domains of language does not appear as an entirely impossible task for distributional semantics to handle."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "The success of Large Language Models in various domains lends support to the hypothesis that much can be learned about the grammatical function and contextual meanings of words from their distributional patterns. In the current work, we examine whether distributional statistics are also sufficient to encode information about the discourse function of words or affixes in addition to their canonical meaning or function. Despite the proven competency of LLMs in grammatical domains, most models tested do not exhibit the human-like ability to use different structures in language to express nuanced meaning in the discourse context. Our study provides baseline assessments of what distributional semantics without any further fine-tuning could achieve."
        }
    ],
    "url": "http://arxiv.org/html/2405.09293v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "3"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2405.09293v1",
        "paper_title": "Do language models capture implied discourse meanings? An investigation with exhaustivity implicatures of Korean morphology",
        "research_background": "### Paper's Motivation\n\nThe overarching motivation of this paper lies in exploring the capabilities of large-scale neural language models within the framework of distributional semantics. The authors are particularly interested in determining whether these models can capture not just semantic knowledge but also nuanced discourse meanings, especially as they pertain to function words.\n\n### Research Problem\n\nThe specific research problem addressed by this paper is the question of whether distributional semantics in large-scale language models (LLMs) can encode the meanings of function words, which encompass both semantic and discourse aspects. This problem is situated within the context of Differential Object Marking (DOM) in Korean, a linguistic phenomenon where objects can be marked with different grammatical structures (e.g., case markers), revealing varying semantic and discourse attributes.\n\n### Relevant Prior Work\n\n1. **Distributional Hypothesis and Semantic Modeling**\n   - **Harris (1954, reference_b9)**: Laid the foundation for the distributional hypothesis, suggesting that word meaning can be inferred from its linguistic context.\n   - **Lenci (2018, reference_b22), Boleda (2020, reference_b2)**: Demonstrated the utility of distributional patterns for modeling content words.\n   - **Marelli and Baroni (2015, reference_b23)**: Showed the effectiveness of language models for modeling function words.\n\n2. **Large-scale Neural Language Models**\n   - **Tenney et al. (2019a, reference_b25); Trott and Bergen (2021, reference_b27)**: Showed that large-scale neural language models encode substantial semantic knowledge.\n   - **Tenney et al. (2019b, reference_b26)**: Highlighted that these models also capture contextualized word meanings.\n\n3. **Differential Object Marking (DOM)**\n   - **Bossong (1991, reference_b3)**: Provided explanations for DOM variations based on the semantic features of referents.\n   - **Aissen (2003, reference_b1)**: Extensively explored the influence of semantic features on DOM.\n   - **Kwon and Zribi-Hertz (2008, reference_b16); Lee (2006, reference_b20)**: Discussed how DOM in Korean is influenced not only by the semantic features but also by the discourse status of objects.\n\n4. **Evaluation of Language Models**\n   - **Futrell et al. (2019, reference_b7)**: Evaluated LLMs from a psycholinguistic perspective.\n   - **Hu et al. (2023, reference_b11); Jereti\u010d et al. (2020, reference_b13)**: Investigated LLMs\u2019 grasp of non-literal meanings.\n   - **Trott et al. (2023, reference_b28)**: Assessed how well LLMs understand meanings tied closely to pragmatic context.\n\nBy addressing the research problem, this paper seeks to extend the understanding of whether pre-trained LLMs can handle both semantic and discourse meanings simultaneously, using the intricate case of Korean Differential Object Marking.",
        "methodology": "The methodology outlined in this paper focuses on evaluating whether pre-trained Language Learning Models (LLMs) can effectively capture implied discourse meanings, specifically exhaustivity implicatures associated with Differential Object Marking (DOM) in Korean morphology. The study involves the following key components and innovations:\n\n### Key Components:\n\n1. **Evaluation of Pre-Trained LLMs:**\n   - **Token Prediction Task:** The pre-trained LLMs are initially trained on a token prediction task, wherein they predict words in context based on large corpora.\n   \n2. **Models Under Study:**\n   - **Korean-Specific Models:**\n     - **KoGPT-2 (125M parameters):** A smaller Korean model developed by SK Telecom.\n     - **KoGPT-Trinity (1.2B parameters):** A larger Korean model by SK Telecom.\n   - **Polyglot-Ko Models:**\n     - Models with 3.8B, 5.8B, and 12.8B parameters, developed for multilingual tasks but prominently trained with Korean data.\n   - **General LLM Models:**\n     - **GPT-3 (175B parameters) and ChatGPT (GPT-4-1106-preview):** Large models by OpenAI capable of handling multiple languages, including Korean.\n\n### Innovations:\n\n1. **Comparative Framework:**\n   - **Contextualized Knowledge Emergence:**\n     - Investigates how more contextualized understanding may emerge in deeper layers of model architecture as suggested by Tenney et al. (2019b).\n   - **Non-Literal Meanings and Pragmatic Competency:**\n     - Evaluates claims by Jereti\u010d et al. (2020) and Hu et al. (2023) about models learning non-literal meanings and performing well in pragmatic tasks.\n   \n2. **Detailed Model Assessment:**\n   - **Logits/Log Probabilities Analysis:**\n     - For most models, the study leverages access to internal semantic representations through logits and log probabilities assigned to input and output tokens.\n   - **Prompt-based Optimization (ChatGPT):**\n     - Since ChatGPT's optimization is designed for 'prompting' rather than logits, its assessment requires an approach focused on model\u2019s meta-judgments on inputs.\n\nOverall, by using a blend of purely statistical and additional human-feedback-trained models, and by employing various assessment techniques, the methodology aims to detail how well different models, of differing scale and architecture, can capture the nuanced discourse meanings in Korean language contexts.",
        "main_experiment_and_results": "### Main Experiment Setup:\nWe sought to determine if large language models (LLMs) can capture the implied discourse meanings of the lul (\ub97c) and nun (\ub294) markers in Korean. Specifically, we tested the models' sensitivity to exhaustivity implicatures indicated by these markers and their ability to differentiate the cancelability of each marker's implicatures.\n\n1. **Datasets:**\n   - We created 288 stimuli comprising 48 items presented in 6 different conditions (48 items x 6 conditions).\n   - Each item included a contextualizing sentence followed by a question about one alternative.\n   - The response part consisted of two sentences:\n     - The first sentence had different object markings (either lul or nun).\n     - The second sentence established a relation with the elided subject, either repeating or negating the verb from the question/first response sentence.\n   - Baseline: Included contradictory continuations where the alternative object was marked with the \"only\" (-man) marker, creating a logical contradiction with the first sentence.\n\n2. **Baselines:**\n   - Human responses to similar stimuli to serve as a benchmark for model performance.\n   - Logical contradiction scenarios using the \"only\" (-man) marker to test the models' ability to recognize and flag logical inconsistencies.\n\n3. **Evaluation Metrics:**\n   - Sensitivity to exhaustivity implicatures indexed by the lul and nun markers.\n   - Ability to detect cancelability where repeating or negating the previous verb cancels or maintains the exhaustivity implicature.\n   - Recognition of logical contradictions in the baseline scenarios.\n\n### Main Experimental Results:\nBy comparing LLMs to human responses, the study aimed to shed light on the depth of LLMs' semantic understanding of lul and nun markers within discourse contexts. While the exact quantitative results are not provided in this segment, the experiment's focus on sensitivity to exhaustivity implicatures and cancelability offers insights into LLMs' capabilities and limitations in capturing nuanced discourse meanings in Korean. The inclusion of logical contradiction scenarios via the \"only\" (-man) marker provided a robust method to evaluate the models' ability to handle discourse-institutional logic."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "We first investigate whether LLMs\u2019 semantic representation of words can reflect discourse meanings of the lul and nun markers. The goal is to compare LLMs\u2019 and humans\u2019 sensitivity towards exhaustivity implicatures indicated with the lul and nun markers and the different cancelability of each marker\u2019s implicatures.",
            "experiment_process": "We created 288 stimuli (48 items to appear in 6 conditions). Each item begins with a sentence contextualizing a set of alternatives, followed by a question about one alternative from the set. The response portion of the conversation consists of two sentences: the first sentence was manipulated to have different object markings between lul and nun, and the second sentence states how the other alternative from the set forms the relation with the elided subject. The previous verb from the question (and the first response sentence) is manipulated to be either repeated (canceling exhaustivity) or negated (maintaining exhaustivity). As a baseline, we included contradictory continuations where the alternative object is marked with the \u2018only\u2019 (-man) marker, evoking a logical contradiction.",
            "result_discussion": "Results suggest that discourse meanings of a grammatical marker can be more challenging to encode than that of a discourse marker.",
            "ablation_id": "2405.09293v1.No1"
        },
        {
            "research_objective": "To address whether the results from Experiment 1 stem from models and humans associating the lul marker with its additional discourse meaning (exhaustivity implicature) or its grammatical function (marking grammatical objects). We aim to determine if the results are due to lul marker\u2019s discourse function rather than its grammatical role.",
            "experiment_process": "We designed 480 stimuli (48 items to appear with 10 manipulated components). Each item started with an intended message, wherein the exhaustivity status of an object that a speaker intends to indicate was manipulated. Then, a question on the object and the response with three different marking options followed: lul-marked, nun-marked, or null-marked object. As a baseline, we included a \u2018contradictory\u2019 response that does not match the (non-)exhaustivity status and a \u2018verbatim repeat\u2019 response maintaining the exact structure of the intended message statement. LLMs should generate lul-marked and nun-marked responses when exhaustivity of the object is intended, and null-marked responses when exhaustivity of the object is not intended.",
            "result_discussion": "From the previous experiment, Polyglot-Ko models and GPT-3 were surprised to see the lul marker followed by canceled exhaustivity, while human speakers accepted the lul marker followed by either canceled or non-canceled exhaustivity, indicating that models struggle with the lul marker's discourse meaning.",
            "ablation_id": "2405.09293v1.No2"
        }
    ]
}