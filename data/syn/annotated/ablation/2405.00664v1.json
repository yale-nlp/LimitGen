{
    "title": "Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3",
    "abstract": "This study presents a targeted model editing analysis focused on the latest large language model, Llama-3. We explore the efficacy of popular model editing techniques - ROME, MEMIT, and EMMET, which are designed for precise layer interventions. We identify the most effective layers for targeted edits through an evaluation that encompasses up to 4096 edits across three distinct strategies: sequential editing, batch editing, and a hybrid approach we call as sequential-batch editing. Our findings indicate that increasing edit batch-sizes may degrade model performance more significantly than using smaller edit batches sequentially for equal number of edits. With this, we argue that sequential model editing is an important component for scaling model editing methods and future research should focus on methods that combine both batched and sequential editing. This observation suggests a potential limitation in current model editing methods which push towards bigger edit batch sizes, and we hope it paves way for future investigations into optimizing batch sizes and model editing performance.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the rapidly evolving field of artificial intelligence, keeping large language models (LLMs) up-to-date with the latest information presents a pivotal challenge. Traditional approaches often require retraining models on extensive datasets, a process that is both time-consuming and resource-intensive. An alternative is model editing Yao et al. (2023  ###reference_b12###), which allows for the modification of stored facts within a model, as well as the correction of inaccuracies. Several popular methods have emerged that infuse knowledge into models without the need for an additional hypernetwork, such as ROME (Rank-One Model Editing) Meng et al. (2022a  ###reference_b6###), MEMIT (Mass Editing Memory in Transformer) Meng et al. (2022b  ###reference_b7###), and EMMET (Equality-constrained Mass Model Editing algorithm for Transformers) Gupta et al. (2024b  ###reference_b4###). These methods, traditionally called \"locate-and-edit\" algorithms, were recently shown to optimize the same objective, known as the preservation-memorization (PM) objective Gupta et al. (2024b  ###reference_b4###). They directly modify specific \"knowledge-containing\" areas of the model without necessitating additional training, and are applicable to any transformer-based large language models (LLMs). In this work, we focus on parameter-modifying model-editing methods Yao et al. (2023  ###reference_b12###) that do not require an additional hypernetwork Chauhan et al. (2023  ###reference_b1###).\n\nIn this work, we present a step-by-step guide for using model editing methods based on the PM-objective for a new model. We compare the performance of batched model editing with sequential-batched editing. We find that for Llama-3, sequential-batched editing with a batch size of 1024 has optimal scaling performance, when compared to making simple batched-edits or sequential-batched edits with smaller batch size, thus showing that sequential model editing is an important component for large-scale model editing. Sequential model editing also enables model editing methods to approach the continual learning paradigm. With this study, we also provide baseline experiments on Llama-3 models to establish benchmarks for future research, as well as provide a transparent procedure for the different decision made while editing a model. Our code is available here - https://github.com/scalable-model-editing/unified-model-editing  ###reference_unified-model-editing###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Preservation-memorization objective",
            "text": "Gupta et al. (2024b  ###reference_b4###) show that ROME and MEMIT both optimize the same objective function, called the preservation-memorization objective. The objective consists of two parts, a preservation term and a memorization term. The ROME optimization objective uses an equality-constrained for memorization as shown below:\nWhere  represents the weights of the feed-forward layer we want to edit,  is a key-vector representative of a fact,  is the desired output, and  is a matrix consisting of facts we want to preserve. The optimization leads to the ROME solution as follows:\nMEMIT optimizes the same objectives but performance memorization using a least-squares constraint, which allows for a closed-form solution for making many memory edits with a single gradient updates, also known as batched-edits. The objective function for MEMIT is:\nWith  again being a stacked matrix of  vectors. In the above equations, a fact is represented by a pair of vectors called the key () and value () vectors. We refer the reader to prior works (Meng et al., 2022a  ###reference_b6###, b  ###reference_b7###; Gupta et al., 2024b  ###reference_b4###) for a more in-depth introduction of these methods. Again, this objective leads so a similar solution of the form:\nGupta et al. (2024b  ###reference_b4###) also showed that it was possible to make batched edits using the equality constraint and present EMMET, an algorithm that allows for batched edits where memorization happens using an equality-constraint. The EMMET objective looks as follows:\nwhich, again, gives the solution:\n###figure_1### ###figure_2### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Model editing metrics",
            "text": "Metrics to analyze the success of model edits are drawn from standard model editing metrics Meng et al. (2022b  ###reference_b7###); Yao et al. (2023  ###reference_b12###)\nEfficacy Score (ES): Measures the success of an edit within the model, measured by percentage where P(new fact) > P(old fact) for query prompt.\nParaphrase Score (PS): A measure of a model\u2019s ability to generalize following an edit. Measured by where P(new fact) > P(old fact) under paraphrases of the query prompt.\nNeighborhood Score (NS): Represents the locality of model editing, measuring the impact of an edit on adjacent stored facts within the model. Specifically, NS quantifies the percentage of nearby facts that remain unchanged after an edit, thereby assessing the precision and isolation of the modifications.\nComposite Score (S): Defined by Meng et al. (2022a  ###reference_b6###) as a holistic measure that combines aspects of edit success, generalization, and locality. It is calculated as the harmonic mean of the Edit Success (ES), Paraphrase Score (PS), and Neighborhood Score (NS), providing a comprehensive evaluation of the overall efficacy of model edits."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Finding Optimal Layer for Model Editing",
            "text": "Meng et al. (2022b) assess the effectiveness of hidden states in LLMs for recalling facts using causal tracing Vig et al. (2020). They find that the representation of the subject\u2019s last token within the feed-forward networks (FFN) at intermediate layers plays a significant role. Building on this finding, (Meng et al., 2022a, b) propose treating the linear layers as a key-value memory system, allowing for the modification of the values in effective hidden states to enhance memory recall. However, later work also showed that layers deemed important during causal tracing did not always translate to model editing performance Hase et al. (2024). Figure 1 also shows the both MEMIT and ROME have very similar performance for model editing across layers of a model. This resonates the fact that both algorithms optimize for the same objective with difference in the memorization constraint, and shows that this difference which has minor effect on editing performance. The least-square constraint allowed a closed-form solution for batched editing in MEMIT, which was also layer enabled with equality-constraint by Gupta et al. (2024b) in the form of EMMET."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Batch Editing",
            "text": "After finding the optimal layer for model editing, on to performing large scale model edits on the same model. One way of doing this is through batched editing. In batched edits, a large number of knowledge edits are performed on the model with the same update. Gupta et al. (2024b  ###reference_b4###) showed that editing multiple layers of a model can sometimes hide the efficacy of model editing performance, we stick to editing a single layer of the model. We edit layer 1 of Llama-3 with batch sizes of 16, 64, 256, 1024, and 4096 using MEMIT and EMMET. The hyperparameter tuning experiments for both algorithms can be found in A.1  ###reference_###.\nThe evaluation results of Batch Editing with MEMIT is shown in Figure 2  ###reference_###. Metrics are seen to consistently fall with larger batches, with Neighbourhood Score (NS) being the most pronounced to fall. This suggests a heightened need to mitigate the impacts on locality following model edits. Post Rewrite Success (ES) is shown to be the most resilient metric to edits. Post Paraphrase Success (PS) is actually first seen to increase dramatically between batch sizes of 16 and 64, the only metric to do so, suggesting a potential area for a possible investigation.\nThe evaluation results of Batch Editing with EMMET is shown in 2  ###reference_###. Similar to MEMIT. most metrics are seen to consistently fall with larger batches, with Neighbourhood Score again being the most pronounced to drop. Overall, the two methods show very similar trends, as reflected by the similarity in their optimization objectives."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Sequential Batch Editing",
            "text": "Above experiments showed the as batch size of edits increase, the model editing performance decreases significantly. This is especially true for the NS metric, showing that the edits made for larger batch sizes start to bleed into other facts known by the model. An alternate way to scale up model editing is sequential editing, where facts are added sequentially to a model. Thus, we ask the question - \"Is there an optimal way to scale model editing that strikes a balance between these methods?\" Prior works have studied sequential editing with batch size of 1, which means only one fact is updated with each gradient update (Yao et al., 2023  ###reference_b12###; Gupta et al., 2024a  ###reference_b3###). We generalize this idea to sequential-batched editing, where we update a batch of facts with one update, and sequentially edit many batches at a time, going from batch size of 1 up to 4096. We perform sequential-batched edits with varying batch sizes (1, 64, 256, 1024, 4096) using the MEMIT and EMMET editing methods, respectively, where batch size of 1 represents purely sequential edits. Figure 4  ###reference_### presents the outcomes of various metrics applied to the MEMIT method, while Figure 5  ###reference_### examines the same for EMMET. Note that sequential batched edit with batch size of 1 corresponding to performing sequential editing with ROME. This comparative analysis aims to determine the most effective editing strategy for enhancing model accuracy and efficiency."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Our study examines several model editing techniques in the context of the newly released Llama-3 model. Contrary to previous belief, our experiments show that earlier layers may be more optimal intervention points, and that smaller, frequent sequential batch size edits have a superior performance in comparison to larger batch sizes. Future work will include experiments on multi-layer intervention for edits, as well as experiments against other popular models and algorithms, including methods that are hyper-network based."
        }
    ],
    "url": "http://arxiv.org/html/2405.00664v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "2.1",
            "2.2",
            "3.1"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.00664v1",
        "paper_title": "Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3",
        "research_background": "### **Paper's Motivation:**\n\nThe primary motivation of the paper is to address the challenge of updating large language models (LLMs) efficiently and accurately with the latest information. Traditional retraining methods, which involve vast datasets, are resource-heavy and time-consuming. This work aims to streamline the process by focusing on **model editing techniques** that modify specific parameters within the models without the need for extensive retraining or additional hypernetworks. Specifically, the paper addresses a gap in understanding the best practices for batch size in model editing, questioning whether increasing the batch size is always the best approach for scaling model edits.\n\n### **Research Problem:**\n\nThe central research problem addressed by the paper is to determine the effectiveness of different batch sizes in model editing, particularly comparing traditional batched edits with sequential-batched edits. The core question is whether increasing the edit batch size is the optimal strategy for enhancing the model editing process, or if smaller, sequential batches might yield better results without causing model degradation.\n\n### **Relevant Prior Work:**\n\n1. **Model Editing Approaches:** \n   - **ROME:** Rank-One Model Editing (Meng et al., 2022a ###reference_b6###).\n   - **MEMIT:** Mass Editing Memory in Transformer (Meng et al., 2022b ###reference_b7###).\n   - **EMMET:** Equality-Constrained Mass Model Editing Algorithm for Transformers (Gupta et al., 2024b ###reference_b4###).\n   - These methods optimize the **preservation-memorization (PM) objective** and modify specific \"knowledge-containing\" parts of the model without additional training.\n\n2. **Focus on Batch Sizes in Prior Studies:**\n   - Prior research has explored increasing the batch size to scale model edits effectively (Meng et al., 2022b ###reference_b7###; Tan et al., 2023 ###reference_b9###; Gupta et al., 2024b ###reference_b4###).\n   - However, more recent studies have indicated that large batch sizes can lead to severe model degradation (Gu et al., 2024 ###reference_b2###; Gupta et al., 2024a ###reference_b3###).\n\nBy providing a detailed evaluation and comparison of **batched edits** versus **sequential-batched edits**, this paper aims to identify optimal strategies for scalable and efficient model editing.",
        "methodology": "In the study \"Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3,\" the proposed methodology primarily builds upon the work of Gupta et al. (2024b) and optimizes the preservation-memorization objective. This objective is split into two key components: the preservation term and the memorization term. The preservation-memorization objective has been used for both the ROME (Rank-One Model Editing) and MEMIT (Memory Injection by Minimal Disturbance) methodologies, albeit with different optimization constraints.\n\nKey Components:\n\n1. **Optimization Objective**:\n   - The objective function consists of two parts: preservation (ensuring existing knowledge isn't disrupted) and memorization (integrating new facts).\n   \n2. **ROME Optimization**:\n   - Utilizes an equality-constrained optimization for the memorization term.\n   - Yields a solution aligning to the ROME framework.\n\n3. **MEMIT Optimization**:\n   - Applies a least-squares constraint for memorization, accommodating a closed-form solution tailor-made for multiple memory edits in a single gradient update (batched-edits).\n   - Yields a solution analogous to the ROME's output but scalable for batch processing.\n\nInnovations:\n\n1. **Batched Edits**:\n   - Both MEMIT and the new EMMET algorithm aim for efficient batch processing of edits.\n   - MEMIT's least-squares constraint simplifies the optimization into a manageable form for multiple updates.\n   \n2. **Equality Constraint Batch Edits with EMMET**:\n   - Expanding on the equality-constrained approach, EMMET facilitates batched edits while maintaining this constraint.\n   - The EMMET method thus fits multiple memory updates into the preservation-memorization schema.\n   - The resulting objective function remains similar to those before but is adapted for batch efficiency.\n\nConclusion:\nThe key innovation in the methodology lies in the adaptation of optimization constraints to facilitate batched edits without sacrificing the integrity of the preservation-memorization objective. By leveraging equality constraints (ROME and EMMET) or least-squares constraints (MEMIT), the methodologies vary in terms of handling multiple edits, making the process more efficient and scalable, especially in large models like Llama-3.",
        "main_experiment_and_results": "### Main Experiment Setup and Results ###\n\n**Experiment Setup:**\n- **Objective:** Assess the effectiveness of hidden state edits in the Llama-3 model.\n- **Dataset:** CounterFact dataset (Meng et al., 2022a).\n- **Evaluation Metrics:** \n  * Efficacy Score (ES)\n  * Paraphrase Score (PS)\n  * Neighbourhood Score (NS)\n\n**Main Experimental Results:**\n- **Optimal Layer Finding:**\n  - **Key Finding:** Layer 1 of the Llama-3 model consistently outperforms other layers on numerous metrics.\n  - **Comparison:** This finding is consistent with previous versions like Llama-2, contradicting previous research (Yao et al., 2023) which suggested layer 5 (0-indexed) was optimal for Llama-2.\n- **Algorithms Performance:**\n  - **MEMIT vs. ROME:** Both algorithms demonstrate very similar performance across model layers, despite differing in their memorization constraints, indicating the minor effect on editing performance.\n- **Other Observations:**\n  - **MEMIT Advantage:** Utilizes a least-square constraint for a closed-form solution in batched editing, also related to EMMET by Gupta et al. (2024b).\n\n### Conclusion:\nThe main experiment determines that layer 1 of Llama-3 is optimal for model editing, based on a comprehensive evaluation using the harmonic mean of efficacy, paraphrase, and neighborhood scores, challenging prior assumptions about the optimal layer for such edits. Both MEMIT and ROME show comparable performance, suggesting the chosen constraint has a minor influence on editing efficacy."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To identify the most effective layer in the Llama-3 model for targeted interventions through model editing.",
            "experiment_process": "The study involved making 1000 non-sequential edits from the CounterFact dataset at each layer of the Llama-3 model. Performance was evaluated based on the harmonic mean of three metrics: Efficacy Score (ES), Paraphrase Score (PS), and Neighbourhood Score (NS). The optimal layer was determined by the layer that achieved the highest combined score.",
            "result_discussion": "Findings showed that layer 1 of Llama-3 consistently outperforms on numerous metrics, which was consistent with previous versions of the model like Llama-2. The results also revealed that MEMIT and ROME algorithms had similar performance across layers, resonating their optimization for the same objective with minor differences in memorization constraints.",
            "ablation_id": "2405.00664v1.No1"
        },
        {
            "research_objective": "To examine the impact of different batch sizes on the effectiveness of model editing using the MEMIT and EMMET techniques.",
            "experiment_process": "Layer 1 of the Llama-3 model was edited with batch sizes of 16, 64, 256, 1024, and 4096 using MEMIT and EMMET algorithms. Hyperparameter tuning was provided, and various evaluation metrics were used to assess the performance, including the Efficacy Score (ES), Paraphrase Score (PS), and Neighbourhood Score (NS).",
            "result_discussion": "The study found that as batch sizes increased, model metrics generally fell, with Neighbourhood Score (NS) showing the most pronounced decline. Both MEMIT and EMMET methods exhibited similar trends, indicating that larger batch sizes degrade model performance asymptotically.",
            "ablation_id": "2405.00664v1.No2"
        },
        {
            "research_objective": "To determine an optimal approach for scaling model editing that balances between batch and sequential editing.",
            "experiment_process": "Sequential-batched editing experiments were conducted with batch sizes of 1, 64, 256, 1024, and 4096 using MEMIT and EMMET methods. Metrics were used to evaluate performance, aiming to compare the effectiveness of sequential editing with different batch sizes.",
            "result_discussion": "Results indicated that sequential batch editing with batch sizes of 1 showed better performance compared to larger batch sizes. Both MEMIT and EMMET methods suggested an optimal batch size of 1024, beyond which larger batch sizes led to model degradation. This study contributes to the understanding that while larger batch sizes are less effective, sequential-batched editing with smaller sizes offers better performance and preserves the locality of edits.",
            "ablation_id": "2405.00664v1.No3"
        }
    ]
}