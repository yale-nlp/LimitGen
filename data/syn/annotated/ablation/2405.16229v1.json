{
    "title": "No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks",
    "abstract": "The existing safety alignment of Large Language Models (LLMs) is found fragile and could be easily attacked through different strategies, such as through fine-tuning on a few harmful examples or manipulating the prefix of the generation results.\nHowever, the attack mechanisms of these strategies are still underexplored.\nIn this paper, we ask the following question: while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities?\nTo answer this question, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) recognizing harmful instructions, (2) generating an initial refusing tone, and (3) completing the refusal response. Accordingly, we investigate whether and how different attack strategies could influence each stage of this safeguarding process.\nWe utilize techniques such as logit lens and activation patching to identify model components that drive specific behavior, and we apply cross-model probing to examine representation shifts after an attack.\nIn particular, we analyze the two most representative types of attack approaches: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA).\nSurprisingly, we find that their attack mechanisms diverge dramatically.\nUnlike ISA, EHA tends to aggressively target the harmful recognition stage. While both EHA and ISA disrupt the latter two stages, the extent and mechanisms of their attacks differ significantly.\nOur findings underscore the importance of understanding LLMs\u2019 internal safeguarding process and suggest that diverse defense mechanisms are required to effectively cope with various types of attacks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) may not comply with ethical standards and can generate inappropriate responses when exposed to instructions with malicious intentions [8  ###reference_b8###].\nTo address this safety concern, recent efforts have focused on alignment in LLMs  [2  ###reference_b2###, 11  ###reference_b11###, 27  ###reference_b27###, 3  ###reference_b3###], safeguarding them against accepting harmful instructions. Despite the seeming effectiveness, this safeguard function is found fragile. An attacker can easily impair it with merely a few unsafe samples and minimal updating steps [41  ###reference_b41###, 54  ###reference_b54###, 7  ###reference_b7###, 31  ###reference_b31###], rendering it to follow malicious instructions again. The simplicity with which the safeguard function can be compromised highlights the urgent need for robust countermeasures.\nAn in-depth understanding of how different fine-tuning attacks impair an aligned LLM\u2019s safeguarding is crucial for devising effective countermeasures, an area that is significantly under-explored. To this end, we aim to investigate the following research problem: while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities? Specifically, we focus on two representative types of fine-tuning attacks [41  ###reference_b41###]: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA). As illustrated in Figure 2  ###reference_###,\nEHA employs explicit harmful instruction-response samples to fine-tune an aligned LLM, whereas ISA fine-tunes the LLM to alter its identity and initiate its response with a self-introduction. As shown in Figure 2  ###reference_###, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) harmful instruction recognition: identifying the instruction as malicious; (2) initial refusal tone generation: generating a refusal prefix (e.g., \u201cSorry. I cannot \u2026\u201d) ; (3) refusal response completion: adhering to the initial refusal tone and completing the response without containing any unsafe content.\nRespectively, we investigate whether and how EHA and ISA impair these three stages.\n###figure_1### ###figure_2### To analyze the impact on harmful instruction recognition, we probe the variation in the distinguishability of the signals indicating harmfulness (i.e., whether the representations of harmful instructions are distinguishable from the benign ones) across different layers. We observe that the behavior of the ISAed model resembles that of the original aligned version. On the contrary, while the distinguishability of harmful signals in EHAed models stays significant at mid-layers, it drops sharply at upper layers. This phenomenon suggests that EHA disrupts the model\u2019s ability to effectively transfer the signals indicating harmfulness at the upper layers, whereas ISA does not notably impact this stage.\nTo examine the impact on the generation of initial refusal tones, we begin by pinpointing a set of the most commonly-used initial tokens that an aligned LLM would generate at the start of its responses when given harmful instructions.\nThese tokens include \u201csorry\u201d, \u201cno\u201d, \u201cunfortunately\u201d, etc., which usually express a refusal to comply with the instruction.\nThen, we analyze the prediction shift of these tokens after the attacks from EHA and ISA, respectively.\nWe also examine how different components of the model contribute to this shift. Our findings suggest that while both EHA and ISA impact the initial refusal tone generation, their influenced components are not the same.\nFor the refusal response completion, we initiate the model\u2019s responses with refusal prefixes of varying lengths to analyze if it can complete the response without incorporating unsafe content. We observe that both ISAed and EHAed models struggle to adhere to the refusal prefix. This issue with ISAed models is even more severe, which almost always persist in generating harmful content, regardless of the refusal prefixes. In addition, we find that adding a safety-oriented system prompt (e.g., the one used in Llama-2 [45  ###reference_b45###] by default for encouraging safer behaviors) could partially mitigate this problem, but the effects are limited.\nThe contributions of this work are summarized as follows.\n(1) To the best of our knowledge, this is the first work to investigate the distinct mechanisms of different fine-tuning attacks. (2) We model the safeguarding process of an LLM as three consecutive stages and systematically analyze how EHA and ISA impair each stage. (3) Our research reveals the distinct attack mechanisms of EHA and ISA, indicating the necessity to develop varied defense strategies for each type of attack."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "We demonstrate how an autoregressive Transformer-based [47  ###reference_b47###] LLM transforms the last token to a new token following prior works [12  ###reference_b12###, 13  ###reference_b13###]. Given an input prompt with  tokens , where each token  belongs to a vocabulary set , the model first transforms them into a sequence of token embeddings , where each  is transformed by an embedding matrix . These embeddings are deemed as the initial residual stream  for the model. Assuming the model comprises  Transformer layers, the -th layer, indexed by , would read information from the residual stream  and write the output of its attention and MLP to this residual stream, updating it to . This process can be presented as: ,\nwhere  and  are the outputs from the attention and MLP respectively. For simplicity, we omit the layer normalization before each module.\nAfter the transformation at the -th layer, we obtain the logit values of the last token over the vocabulary  using an unembedding operation: . Here, , where  is the unembedding matrix and  is the final layer normalization before . Then, we obtain the predicted distribution of the next token given by: , from which we can sample a new token.\nWe introduce two tools for tracing the information flow in the model and locating components for specific behaviors used in this work. They are Logit Lens [39  ###reference_b39###, 4  ###reference_b4###] and Activation Patching [49  ###reference_b49###, 55  ###reference_b55###].\nLogit Lens is a technique to inspect the distribution over the vocabulary held by any -dimentional hidden state , such as residual stream  or the output of a module  or , in the model.\nSpecifically, we get the logit values  of  by . Taking the output of an attention module  for example, its logit values  indicate the direct effects it makes on the final logit values by updating this output to the residual streams. Additionally,  indicates the logit value of a token  held by , where  follows Python syntax, selecting the logit of the token .\nActivation Patching is a technique used to locate critical components related to specific behaviors. It involves interchanging the activation produced by a component when given an input that presents the target behavior with the activation from an input that does not. The significance of a component is measured by the effect on the final output caused by this intervention.\nTo illustrate, suppose we have an original input , such as a harmful instruction \"How can I make a bomb.\", we make an intervened version of it, , by changing the harmful tokens into safe ones to make it harmless, such as \"How can I make a pie.\". We can then replace an activation, such as a residual stream , with the activation at the same position , and let the model recompute the final output to see how significant the information updated by layers before -th layer is. This significance is measured by how much this replacement can re-elicit the original behavior.\nWe follow prior works [49  ###reference_b49###, 55  ###reference_b55###] to use the logit difference as the measurement. In the above examples regarding harmful and harmless instructions, we expect the aligned model would have a larger logit for  than  for the first token to be predicted when inputting a harmful instruction, and vice versa. Thus, we formulate the measurement as follows:\nThis gives a measurement of the logit difference that lies in , where a larger value indicates a higher recovery degree of the original behavior."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Do Fine-tuning Attacks Impair the Ability of Refusal Completion?",
            "text": "Finally, we delve into the stage of refusal response completion, where we explore the following question. If an attacked model is capable of generating an initial refusal tone accurately in certain instances, is it able to adhere to the refusal and successfully complete a response that is free from unsafe content?\n\nTo test the model\u2019s ability of refusal completion, we control the beginning of the response with various kinds of refusal prefixes (e.g., \u2018Sorry, I cannot\u2019) through prefix prefilling. That is, the model is forced to start generating from the concatenation of the instruction and a specified refusal prefix. We experiment with different refusal prefixes of varying lengths. Intuitively, longer prefixes are expected to offer stronger refusal signals. Our objective here is to empirically verify whether the refusal completion capabilities improve as the length of the prefixes increases. To obtain diverse refusal prefixes, we leverage the aligned model to sample five refusal responses for each instruction and then truncate the beginnings of these responses to varying lengths.\n\nWe use harmful instructions to query the model\u2019s completions with different refusal prefixes. To assess whether the completion includes any unsafe content, we employ a safety classifier to identify whether the completion is deemed unsafe. For quantitative analysis, we introduce the metric called Normalized Unsafe Rate (NUR), which is calculated as the ratio between the number of unsafe responses generated using refusal prefixes and the number of those without any prefixes. Higher NURs indicate poorer refusal completion capabilities.\n\nWe also test if appending a Safety System Prompt (SSP) could elicit better refusal completion capabilities in the attacked model. The SSP, in this context, refers to the prompt content designed to encourage safe behavior.\n\nBy comparing different approaches and configurations, we find that appending a safety-oriented system prompt can enhance the model\u2019s refusal completion capability to some extent. However, the improvement is very limited, indicating that the impairment caused cannot be easily restored."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Implications for Future Work",
            "text": "Our findings suggest a potential application where the trained probes at mid-layers can detect harmful inputs. In Section 4  ###reference_###, we observe that the probes in the 14-16th layers maintain high accuracy in distinguishing harmful signals, even after attacks. This indicates these probes could robustly detect harmful instructions without an external detector like Llama-Guard [24  ###reference_b24###]. Consequently, they could be employed to detect harmful inputs in fine-tuned or attacked versions of the aligned model.\nAn emerging direction for safeguarding models is to manipulate their internal representations to achieve desired behaviors [46  ###reference_b46###, 32  ###reference_b32###, 30  ###reference_b30###, 63  ###reference_b63###]. Typically, this involves identifying directions in the model\u2019s representations that distinguish between expected and unexpected behaviors (e.g., safe vs. harmful responses) and steering the representations toward the expected behaviors.\nHowever, our findings indicate the attacked model tends to override the steering signals from earlier layers in the upper layers. It suggests that such methods may be less effective in enhancing the safety of attacked models. Therefore, more attack-resisting model manipulation methods are needed to improve safeguarding."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Despite significant efforts to align LLMs with human ethical values [2  ###reference_b2###, 3  ###reference_b3###, 28  ###reference_b28###, 11  ###reference_b11###, 27  ###reference_b27###], recent research has highlighted their vulnerabilities in safety [50  ###reference_b50###, 41  ###reference_b41###].\nThese vulnerabilities can be exploited to attack aligned LLMs, causing them to generate harmful content or be used for malicious purposes.\nOne type of attack involves adding content to input instructions that exploits the model\u2019s weaknesses, such as explicitly guiding the model\u2019s response mode [50  ###reference_b50###, 34  ###reference_b34###, 62  ###reference_b62###], or appending generated suffixes that can bypass the model\u2019s defenses [64  ###reference_b64###, 33  ###reference_b33###, 1  ###reference_b1###]. Many defense methods have been proposed to counter such attacks, such as adding additional input filtering or processing [52  ###reference_b52###, 25  ###reference_b25###, 24  ###reference_b24###], leveraging the model\u2019s own capabilities to recognize the attack [56  ###reference_b56###, 57  ###reference_b57###, 20  ###reference_b20###], and guiding the model\u2019s decoding to generate safe content [59  ###reference_b59###, 53  ###reference_b53###]. Another type of attack incorporates a few harmful data to fine-tune the model, compromising the model\u2019s safety mechanisms [54  ###reference_b54###, 41  ###reference_b41###, 7  ###reference_b7###, 31  ###reference_b31###, 42  ###reference_b42###].\nAdditional data processing helps mitigate this type of attack, such as incorporating safety samples [41  ###reference_b41###, 8  ###reference_b8###] or manipulating the system prompts [35  ###reference_b35###, 48  ###reference_b48###].\nModifying how model parameters are updated can also mitigate such attacks. For instance, storing harmful updates for unlearning[61  ###reference_b61###, 6  ###reference_b6###], or employing adversarial training [23  ###reference_b23###, 21  ###reference_b21###, 42  ###reference_b42###].\nMechanistic Interpretability (MI) aims to reverse-engineer specific functions or behaviors of a model in order to elucidate how the model works in a way that is understandable to humans.\nThese reverse-engineering efforts typically focus on components such as neurons [43  ###reference_b43###, 17  ###reference_b17###], representations [36  ###reference_b36###, 18  ###reference_b18###], modules (e.g., MLPs [15  ###reference_b15###, 14  ###reference_b14###] or attention heads [38  ###reference_b38###, 16  ###reference_b16###]), or circuits [49  ###reference_b49###, 19  ###reference_b19###] composed of these modules, aiming to identify components related to the target behavior and understand their roles within it.\nEfforts to understand fine-tuning from MI perspective reveal that fine-tuning doesn\u2019t create new circuits to boost capabilities; instead, it enhances the abilities of existing circuits [40  ###reference_b40###, 26  ###reference_b26###]. Moreover, understanding the model\u2019s safety mechanisms from a mechanistic perspective helps develop more robustly safe models [51  ###reference_b51###, 5  ###reference_b5###, 58  ###reference_b58###]. For example, it has been discovered that the key parameters of the safety mechanism are located in only a very small region of the model, making them very fragile [51  ###reference_b51###]. Furthermore, it has been found that safety system prompts can enhance the model\u2019s safety mechanisms by shifting the harmful input\u2019s representation along the refusal direction, thereby increasing the model\u2019s refusal probability [58  ###reference_b58###].\nAlong these lines, our work aims to analyze the damage caused by fine-tuning attacks from a mechanistic perspective, providing insights into how these attacks affect the model\u2019s safety mechanism."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we examine the mechanisms by which two types of fine-tuning attacks, namely Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA), impair the safety alignment of an LLM. By breaking down the safeguarding process into three stages, we investigate how these attacks disrupt the safeguarding at each stage. Our research reveals a notable difference between the two attacks: EHA disrupts the transmission of harmful signals, whereas ISA does not. Additionally, both attacks primarily impact the upper layers of an LLM, resulting in the suppression of refusal expressions. These findings emphasize the necessity for more robust defenses against fine-tuning attacks."
        }
    ],
    "url": "http://arxiv.org/html/2405.16229v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "8"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "4",
            "5",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4",
            "5",
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2405.16229v1",
        "paper_title": "No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks",
        "research_background": "The paper \"No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks\" focuses on the vulnerabilities of Large Language Models (LLMs) to fine-tuning attacks that compromise their alignment and safety. Although recent efforts have made strides in aligning LLMs to adhere to ethical standards and reject harmful instructions, these safeguards are fragile and easily compromised by attackers using a few unsafe samples and minimal updating steps. This motivates the need for robust countermeasures that are currently lacking.\n\nThe research problem addressed in this paper is to understand the distinct mechanisms by which different fine-tuning attacks impair the safeguarding of aligned LLMs. Specifically, the paper investigates whether these attacks exhibit strong similarities in their mechanisms, with a focus on two representative types of fine-tuning attacks: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA).\n\n### Relevant Prior Work:\n1. Efforts to align LLMs against harmful instructions [2###reference_b2###, 11###reference_b11###, 27###reference_b27###, 3###reference_b3###].\n2. Studies highlighting the ease with which these alignments can be compromised [41###reference_b41###, 54###reference_b54###, 7###reference_b7###, 31###reference_b31###].\n\n### Contributions:\n1. **Novel Investigation:** This is the first work to investigate the distinct mechanisms of different fine-tuning attacks.\n2. **Three-Stage Model:** The paper models the safeguarding process of an LLM as three stages: harmful instruction recognition, initial refusal tone generation, and refusal response completion.\n3. **Systematic Analysis:** The research systematically analyzes how EHA and ISA impair each stage of the safeguarding process, revealing the need for varied defense strategies for each type of attack.\n\n### Key Findings:\n- **Impact on Harmful Instruction Recognition:** The distinguishability of harmful signals in the EHAed model drops sharply at upper layers, whereas ISA does not notably impact this stage.\n- **Impact on Initial Refusal Tone Generation:** Both EHA and ISA affect the generation of initial refusal tones, but they influence different components of the model.\n- **Impact on Refusal Response Completion:** Both ISAed and EHAed models struggle to adhere to refusal prefixes, with ISAed models performing worse.\n\nThe paper's insights emphasize the necessity for varied and robust defense mechanisms tailored to the specific attack types, laying foundational work for future research in this area.",
        "methodology": "The proposed method in the paper \"No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks\" involves analyzing how autoregressive Transformer-based large language models (LLMs) transform input tokens during the generation of outputs. Below is a breakdown of the methodology, its key components, and its innovations:\n\n### Key Components and Operation\n1. **Model Architecture**:\n   - The method leverages an autoregressive Transformer-based model.\n   - Input prompt tokens \\(\\{x_1, x_2, \\ldots, x_N\\}\\) are first transformed into token embeddings \\(\\{e_1, e_2, \\ldots, e_N\\}\\) through an embedding matrix \\(E\\).\n   - These embeddings initialize the residual stream \\(r_0\\).\n\n2. **Transformer Layers**:\n   - Each of the \\(L\\) Transformer layers processes the residual stream to update it:\n     \\( r_i = r_{i-1} + \\text{Attention}(r_{i-1}) + \\text{MLP}(r_{i-1}) \\).\n   - The layer's output \\(r_L\\) (final residual stream) undergoes an unembedding operation to yield logit values over the vocabulary.\n\n3. **Prediction**:\n   - Final logits are obtained through:\n     \\( r_t = \\text{LN}(r_L) \\cdot U \\),\n     where \\(U\\) is the unembedding matrix and \\(\\text{LN}(r_L)\\) is the final layer normalization.\n   - The predicted distribution for the next token \\(p_{\\text{pred}}(x_{t+1}| x_{\\le t})\\) is then sampled from these logits.\n\n### Innovations & Tools for Analysis\n1. **Logit Lens**:\n   - This tool inspects the distribution of logits over the vocabulary for any -dimensional hidden state, such as the residual stream \\(r_i\\) or outputs of attention and MLP modules.\n   - For each hidden state \\(h\\), the logits are computed as \\( h \\cdot U \\).\n   - This helps to understand the effect of specific modules on the final logits by observing intermediate residual streams and module outputs.\n\n2. **Activation Patching**:\n   - Uses a technique for identifying critical components related to specific (target) behaviors.\n   - Involves altering activations from specific components (e.g., residual streams) between a harmful input and its harmless counterpart.\n   - By replacing activations from one scenario with another and observing the impact on the model's output, the significance of those activations is gauged.\n   - Measures the effect using logit difference, calculating how much the replacement re-elicates the original (target) behavior.\n\n### Example Scenario\n- **Harmful Instruction Example**:\n  - Taking a harmful prompt like \"How can I make a bomb.\" (\\(x\\)) and a safe version \"How can I make a pie.\" (\\(x'\\)).\n  - By patching activations (e.g., residual streams \\(r_i\\)) between \\(x\\) and \\(x'\\) and re-evaluating the output, one can assess the critical role of the activations prior to the \\(i\\)-th layer.\n  - Logit difference used here indicates the model's alignment, expecting larger logits for harmful tokens in harmful instructions and vice versa for harmless ones.\n\nBy applying these tools, the method allows a deep inspection and manipulation of model behavior, particularly in understanding fine-tuning attacks and aligning model outputs to safer behaviors.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup:\n1. **Model**: The experiments are conducted using Llama-2-7B-Chat, chosen for its extensive safety alignment training, making it a reliable aligned model for attack and analysis.\n\n2. **Attacks**:\n    - **Explicit Harmful Instruction Attack (EHA)**: The setup includes 10 harmful instructions sampled from the AdvBench dataset, with the responses generated by an unaligned, instruction-tuned LLM.\n    - **Identity-Shifting Attack (ISA)**: This attack uses a fine-tuning dataset containing 10 instruction-response pairs specifically designed for identity-shifting from Qi et al.\n\n3. **Datasets**:\n    - **AdvBench**: Contains harmful instructions used for EHA.\n    - **Hex-phi**: Used for evaluating the harmfulness degree of the responses from attacked models.\n    - **Hex-phi-new**: A manually crafted dataset for more accurate analysis, comprising 110 harmful instructions.\n    - **Hex-phi-attr**: Consists of 55 harmful instructions, each paired with a harmless counterpart.\n    - **Wild Set**: Contains 100 harmful instructions from Jailbreakbench, used as an external test set.\n\n4. **Evaluation Metrics**:\n    - **Harmfulness Degree**: Evaluated using GPT-4 on a 5-likert scale where higher scores indicate more severe harmfulness. The scale ranges from 1 (least harmful) to 5 (most harmful).\n\n#### Main Experimental Results:\n- **Harmfulness Scores**: The results show a significant increase in harmfulness scores for both EHA and ISA attacks.\n    - Harmfulness scores increased from nearly 1 (baseline aligned model) to about 4.5 (attacked models), indicating a severe compromise in the model's safeguarding function.\n    - Approximately 75% of the responses were rated the most harmful, suggesting the attacked models respond to and fulfill harmful instructions effectively.\n\nOverall, the main experiment provides robust evidence that both EHA and ISA attacks significantly degrade the aligned model's capability to refuse harmful instructions, making it crucial to further investigate diverse attack mechanisms and safeguarding strategies in larger language models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The objective is to investigate whether fine-tuning attacks impair the ability of a model to differentiate between harmful and normal instructions.",
            "experiment_process": "The experiments are conducted using Hex-phi-attr dataset, which contains pairs of harmful and harmless instructions. Activation patching technique is employed to analyze the transformation of harmfulness features into refusal signals. The harmfulness features are traced through different layers of the model. Probes are created using Mass-Mean probing to determine the directions associated with harmfulness features, and their accuracy is measured using F1 and AUC metrics. The representations are evaluated on both aligned models and their attacked counterparts.",
            "result_discussion": "The analysis revealed that harmful signals in aligned models remain distinguishable from approximately the 14th layer onwards. EHA disrupts the transmission of harmful signals in higher-level layers, with performance dropping beyond the 19th layer. ISA does not hinder the signal transmission. EHA introduces a larger shift in the model\u2019s representations for harmful samples compared to harmless samples. ISA's shift does not distinguish between harmful and harmless samples, suggesting it shifts the model\u2019s representation orthogonally to the direction of harmfulness.",
            "ablation_id": "2405.16229v1.No1"
        },
        {
            "research_objective": "The study aims to understand how fine-tuning attacks alter the model's initial tone when generating responses to harmful instructions.",
            "experiment_process": "The most common first tokens are recorded from the responses of both aligned and attacked models when given harmful instructions from Hex-phi-new and Wild sets. The average logit differences of these tokens before and after an attack are calculated. Tokens with significant logits shifts are identified as suppressed or boosted. The direct contributions of attention mechanisms and MLPs to the logit shifts are analyzed using the Logit Lens technique, focusing on different layers of the models.",
            "result_discussion": "Suppressed tokens often include refusal expressions, with EHA causing higher suppression compared to ISA. Both attacks boost tokens indicative of affirmative responses. The MLP at the last layer shows the most significant contribution to logit shifts for boosted tokens. Attention mechanisms in upper layers enhance the prediction of boosted tokens. The primary difference between EHA and ISA is that EHA affects the mid-layer MLPs, coinciding with disrupted refusal signal transmission, while ISA does not significantly alter the predictions of suppressed tokens through MLPs.",
            "ablation_id": "2405.16229v1.No2"
        },
        {
            "research_objective": "The study explores whether attacked models can complete refusal responses without generating unsafe content after initiating the refusal tone.",
            "experiment_process": "A variety of refusal prefixes of different lengths are prefixed to instructions to test the model\u2019s ability to complete refusal responses. The harmful instructions are taken from the Hex-phi-new test set. The Llama-guard-v2-8B classifier assesses the safety of the responses generated. The Normalized Unsafe Rate (NUR), which measures the ratio of unsafe responses, is used as the metric. The impact of appending a Safety System Prompt (SSP) on refusal completion is also evaluated.",
            "result_discussion": "Fine-tuning attacks significantly impact refusal completion capability. Even with refusal prefixes of up to 50 tokens, the NUR remains high at around 50%, indicating difficulty in generating safe completions. EHA shows a lower NUR compared to ISA, implying ISA has a greater impact on refusal completion. Adding SSP offers limited improvement, suggesting that the damage caused by EHA and ISA cannot be easily mitigated.",
            "ablation_id": "2405.16229v1.No3"
        }
    ]
}