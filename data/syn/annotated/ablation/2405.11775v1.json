{
    "title": "Exploring Ordinality in Text Classification: A Comparative Study of Explicit and Implicit Techniques",
    "abstract": "Ordinal Classification (OC) is a widely encountered challenge in Natural Language Processing (NLP), with applications in various domains such as sentiment analysis, rating prediction, and more. Previous approaches to tackle OC have primarily focused on modifying existing or creating novel loss functions that explicitly account for the ordinal nature of labels.\nHowever, with the advent of Pretrained Language Models (PLMs), it became possible to tackle ordinality through the implicit semantics of the labels as well.\nThis paper provides a comprehensive theoretical and empirical examination of both these approaches.\nFurthermore, we also offer strategic recommendations regarding the most effective approach to adopt based on specific settings.",
    "sections": [
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Explicit approach: Loss-functions and Analysis",
            "text": "Let  be  independent and identically distributed datapoints containing the input features  and their corresponding labels ; where  where  is the number of classes. The output of the classifier is denoted by  which is a probability distribution over the  classes.\nLet  be the one hot encoding of .\nThe classifier is trained by optimizing the parameters  such that  reaches a minimum. In the rest of the paper, for ease of mathematical exposition, we omit the indexing with respect to , i.e. remove the  and , wherever it is evident from the loss expression.\nIn the next subsection, we give a few desirable theoretical properties of the loss function  in the context of OC and then follow it up by discussing some of the widely used loss functions."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Desirable Properties of Losses in OC",
            "text": "Proper Scoring Rule (PSR): A loss is said to be a PSR (Gneiting and Raftery, 2007  ###reference_b13###; Merkle and Steyvers, 2013  ###reference_b23###) if it takes the lowest value when the predicted class probabilities match the ground truth which is a one-hot encoded -dimensional vector.\nBeing PSR ensures that the loss indeed tries to optimize the classifier to predict the ground truth without injecting any bias in the predicted outputs. Further, PSR losses also help to produce well calibrated probabilities (Lakshminarayanan et al., 2017  ###reference_b20###).\nConvexity (Cx): Convexity of  with respect to  is a desirable property of the loss as it is an essential requirement for several convex formulations of Neural Networks (NNs) Kawaguchi et al. (2019  ###reference_b17###); Du et al. (2019  ###reference_b11###); Pilanci and Ergen (2020  ###reference_b25###); Wojtowytsch (2023  ###reference_b34###). Further, if both  and  (classifier function) are convex with respect to  (as in the case of logistic regression or support vector machines), then it is guaranteed that the local minima indeed coincides with the global minima.\nSeveral widely used losses such CE, MAE, MSE, etc. are both PSRs and Convex. Next, we look at two desirable characteristics of the output probabilities from the classifier in the context of OC.\nUnimodality (UM): If the output probabilities have single mode, i.e.  is not satisfied for any , then we say the classifier satisfies the UM condition (Beckham and Pal, 2017  ###reference_b1###; Yamasaki, 2022  ###reference_b35###; Iannario and Piccolo, 2011  ###reference_b14###). An illustration is given in Figure 1  ###reference_###.\nOrdinality (Ord): In the context of OC, we require the loss  to explicitly penalize the mis-classifications more which are farther away from each other compared to the ones which are closer. The goal is to enforce a meaningful ordering among the labels unlike nominal classifcation where categories lack a specific order. We will see later that different loss functions enforce ordinality to a varying degree.\nIn the next subsection, we discuss some of the widely used loss functions in the context of OC in NLP."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Widely used Loss Functions in OC",
            "text": "Cross-Entropy (CE): CE is given by\nThe above expression boils down to . Oftentimes CE is discredited for not being able to factor in ordering as its expression does not take into account the probabilities corresponding to the non-groundtruth classes.\nOrdinal Log Loss (OLL) (Castagnos et al., 2022  ###reference_b6###): OLL is given by\nHere  is a hyperparameter. This can be seen as a complementary of CE where the missclassification is explicitly penalized in proportion to its degree by the term\n. Both CE and OLL belong to PSR family as both these losses are zero when the output probabilities coincide with the ground truth one-hot encoding.\nSOFT labels (SOFT) D\u00edaz and Marathe (2019  ###reference_b12###): In this approach, first the ground truth -dimensional onehot encoded is modified to  \u2018soft\u2019 labels as follows.\nThen CE loss can be computed using these soft ground labels as truth probabilities.\nClearly, introducing softlabels makes this approach not fall in the PSR family as the ground-truth onehot encoding vector does not minimize the loss anymore.\nEarth Mover Distance (EMD) Rubner et al. (2000  ###reference_b28###) : EMD or Wasserstein loss is defined\nHere CDF refers to the cumulative distribution function. While EMD is a PSR, it does not impose a strong penalty in the tails (as compared to OLL) because CDFs are monotonic with range in  and hence the difference between the CDFs will be small in the tails.\nUnimodal Losses: da Costa et al. 2008  ###reference_b7### and Beckham and Pal 2017  ###reference_b1### introduce a parametric way to force unimodality in the predicted probability scores by first computing a scalar function  and using this scalar as the parameter of a Binomial distribution with parameters , the Probability Mass Function (PMF) is computed which forms the final predicted probabilities.\nNote that the computing a single scalar  (as opposed a vector of embeddings as done in other methods) severely hampers the learning; thus the guarantee of unimodality comes at the cost of degradation in performance. To address this, Yamasaki (2022  ###reference_b35###) proposes a more-flexible unimodal OC framework by imposing shape-based constraints on the output probabilities; the V-Shaped Stereotyped Logit (VS-SL) method has shown to be the state-of-the-art for UM in their work and hence, we use it as a baseline in our experiments.\nWe also include two more loss function variants, namely COnsistent RAnk Logits (CORAL) Cao et al. (2020  ###reference_b5###) and Weighted Kappa Loss (WKL) de la Torre et al. (2017  ###reference_b9###), and benchmark their effectiveness in terms of accuracy and ordinality.\nTo benchmark the performance of the above loss functions, we run experiments on\nthree\nbenchmark multi-class text classification datasets, each with its distinct task: Hypothesis entails Premise task - SNLI Bowman et al. (2015  ###reference_b2###), Reviews Classification task - Amazon Reviews (AR) Keung et al. (2020  ###reference_b18###),\nand Sentiment Analysis task - SST-5 Socher et al. (2013  ###reference_b30###). We report weighted-F1 scores and MAE, MSE, Off-by-1 (OB1) accuracy to measure both classification and ordinal performance respectively. In the interest of space, we present the details of datasets and metrics in Appendix B  ###reference_### and C  ###reference_###.\nWe observe that in general CE performs best in terms of nominal metrics (like weighted-F1) and OLL performs best in terms of ordinal metrics on an average. However, there seems to be a trade-off between nominal and ordinal performance i.e. the improvement in ordinal metrics comes at the expense of nominal metrics. However, given that all these metrics are also from the PSR family, and in some sense are not independent of each other as a perfect classifier would improve both these metrics simultaneously.\nThis intuition prompted us to experiment with a weighted combination of CE and OLL, which we have named Multitask Log Loss (MLL), hypothesizing that it would inherit the best attributes of both methods.\nMulti-task log loss function (MLL): MLL is given by\nHere,  is a hyperparameter. MLL satisfies both convexity and PSR conditions. Further, while OLL and MLL are not theoretically guaranteed to be UM, but empirically have been found to be satisfying UM condition for 80-90% test datapoints which we show later in Figure 4  ###reference_###. A summary of the properties satisfied by different loss-based approaches is given in Table 1  ###reference_###, with discussions in Appendix E  ###reference_### and F  ###reference_###. Note that while it\u2019s theoretically possible to consider weighted combinations involving other loss functions, we only considered CE and OLL here because OLL has already been shown to outperform other losses in terms of ordinal metrics (Castagnos et al., 2022  ###reference_b6###) and we aimed to improve its performance in term of nominal metrics as well, by adding with the CE term.\nWe perform all explicit approach based experiments using BERT-base (refer Appendix D  ###reference_###). Following Castagnos et al. (2022  ###reference_b6###) we also train a smaller version - TinyBERT Jiao et al. (2020  ###reference_b15###), as the performance of different loss functions\nis better contrasted when the size of the base model is small, which is usually the case in online settings where we cannot deploy larger models.\nThe detailed results are given in Table 3  ###reference_### and Appendix A  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Implicit approach: Entailment-style Encoder Models",
            "text": "Wang et al. (2021  ###reference_b32###) proposed reformulating vanilla classification into an entailment-style task to enhance the few-shot capabilities of PLMs. Here, the model learns to predict whether the input text and the label entail each other or not (similar to Natural Language Inference (NLI) setting), leveraging the inherent semantic relationship between the label and input text. We adopt a similar approach for our task and explore it through the lens of ordinality, which has not been studied in prior works.\nWe assume the existence of a classifier, based on Pretrained Language Model (PLM), called .\nLet  be the collection of textual labels.\nThe training dataset can be divided based on the labels into , which consists of subsets . Each subset contains the available training data  for label . The corresponding test data is represented by .\nDuring the training phase in , the following entailment-style data augmentation technique is employed:\nFor each data point  with a ground truth label of ,  samples () are generated and augmented as follows:\nHere,  is an indicator function that yields 1 if  else 0. The \u2018+\u2019 operator denotes concatenation operation (refer \u00a7D  ###reference_###), and  is a pre-defined template (specific to the downstream task) describing the label in natural language. For example, in sentiment classification task  can be described as: indicates positive sentiment. See Figure 2  ###reference_### for example.\nEssentially, for each data point,  negative samples and 1 positive sample are created. Finally the problem reduces to the following NLI task - Does  entail  or not?\nOnce these  augmented examples are generated, the parameters  are finetuned for a binary classification task, where  serves as the input and  acts as the ground truth.\nDuring the inference phase, for a datapoint  the predicted label  is obtained using:\nDuring inference for ,  is computed following Eq. 6  ###reference_### and softmax() is applied on the predicted logits before taking argmax so that all the class probabilities sum up to 1. As the model leverages the natural language meanings of the labels during training, we argue it is inherently capable of learning to predict labels that are ordinally consistent. For instance, the model learns to comprehend that the label very negative sentiment is closer in semantic space to negative sentiment than to very positive sentiment. This understanding prevents the model from deviating significantly from the actual ground truth. In contrast, in the case of vanilla CE, these labels are treated solely as numbers, disregarding their inter-semantic relations. We again use BERT-base here as base model for performing experiments. The exact label verbalisers used for all datasets are mentioned in Appendix 8  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Implicit approach: Generative Models",
            "text": "Decoder-based text generative models have seen notable advancements in recent years, facilitating the production of coherent and contextually relevant text. The development of models like the GPT series Radford et al. (2018a  ###reference_b26###, b  ###reference_b27###); Brown et al. (2020  ###reference_b4###) has led to state-of-the-art results in text generation and summarization benchmarks.\nOne of the primary objectives of this paper is to investigate whether these models demonstrate ordinal behavior by accurately capturing the inherent order or ranking of elements in the generated text.\nFormally, in the context of OC, for a given a textual input  which comprises the following words  and its corresponding ground truth label , we append the input and label as .\nNext, the parameters  of generative model are finetuned s.t.\nDuring inference for  input , the generative model predicts the word  from the vocabulary  which maximizes the conditional probability:\nFurther, it is possible that the predicted label may not be in the set of ground truth labels i.e. for an  input  the label  where  is the set of  distinct labels (see Appendix 9  ###reference_###). This is the notorious hallucination problem associated with generative models.\nTo mitigate this issue during inference, we compare the conditional log-probabilities of each the  labels\n, given the text segment . The class corresponding to the highest probability is proposed as the generated label. That is, given that  represents the learned parameters of LM, the label selection during inference will be s.t.\nHere  are the words in input  and  are the labels in the set .\nFor fair comparison with other explicit and implicit approaches, we use GPT2-small as our base model for experiments to maintain similar number of model parameters (with BERT-base). Similar to the entailment approach (\u00a73  ###reference_###), we also experiment with informative and un-informative verbalisers here.\nMotivated by recent advancements and the accessibility of open-source Large Language Models (LLMs), and to demonstrate the true potential of the generative approach, we also experiment with Llama-7B Touvron et al. (2023  ###reference_b31###), a decoder-based LLM with 7 billion parameters. Pre-trained on trillions of tokens using publicly available data, it achieves state-of-the-art performance, surpassing its larger predecessors like GPT-3 (175B) on the majority of benchmarks.\nNote that our approach is different from GPT2ForSquenceClassification111https://tinyurl.com/am93sjdw  ###reference_tinyurl.com/am93sjdw### where the last embedding of the last token is used for classification, which is similar to encoder-model (like BERT) style classification. Instead we train it for a language modelling task\nto generate within a fixed set of tokens i.e. the set of labels."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents an unified analysis of explicit and implicit strategies for addressing OC. It is the first study to thoroughly examine and compare these approaches from both theoretical and empirical standpoints. Our analysis (summarized in Table 5  ###reference_###) reveals that MLL demonstrates balanced performance across ordinal and nominal metrics, unlike existing explicit losses. However, in few-shot scenarios, ENT is preferred for its ability to achieve optimal performance with fewer examples, leveraging label semantics. Furthermore, we highlight the importance of providing informative verbalisers in low-data settings, resulting in reduced variance and improved outcomes. However, the distinction between strategies becomes less clear with increasing data. In full-data scenarios, fine-tuning Llama-7B-Adapter surpasses previous approaches due to its substantial model size. Interestingly, even with such a large base model, the impact of adding informative verbalisers remains apparent, indicating its recognition of label order.\nWe hope that our work will serve as a benchmark encompassing multitude of approaches, providing a foundation for future efforts to address OC in NLP."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In this study,\nwe don\u2019t consider the effect of calibration techniques on the explicit approaches (Kull et al., 2019  ###reference_b19###), as the techniques employed are largely identical to those used in nominal classification, offering no distinct or novel methodologies specifically for OC tasks.\nAlso for implicit approaches, a more deeper analysis is required on how implicit methods like LLMs & PLMs implicitly capture ordinality. It\u2019s not always analogous to how humans use task instructions as shown in Webson and Pavlick (2022  ###reference_b33###). Furthermore, in this study we limit ourselves to only finetuning-based OC approaches. However, it would also be interesting to explore OC through the lens of in-context learning (ICL) for generative approaches. Also for the generative approach, we make the assumption that the label word will not break further into multiple tokens by re-mapping original labels to simpler words (see Appendix 9  ###reference_###). This avoids having to account for multiple token probabilities when taking the argmax. Without this some sort of normalization would be required across the entire generation length to compare different outputs. We leave these discussions for future work. Even though we notice that LLMs such as Llama-7B outperform all the other models in full data settings, there are certain challenges in terms of compute resources and inference time. Additionally, finetuning Llama-7B is susceptible to hallucinations in low-data settings Zhao et al. (2021  ###reference_b37###), which is why we don\u2019t report LLM results for the few-shot case."
        }
    ],
    "url": "http://arxiv.org/html/2405.11775v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "3",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "2.2",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2405.11775v1",
        "paper_title": "Exploring Ordinality in Text Classification: A Comparative Study of Explicit and Implicit Techniques",
        "research_background": "### Motivation and Research Problem\nThe paper's motivation stems from a need to address the limitations and challenges associated with ordinal classification (OC) in natural language processing (NLP). OC tasks, such as sentiment analysis, rating prediction, and age group classification, require an understanding of the inherent ordering in the output categories. Traditional methods, specifically the classical explicit approach, involve tweaking the Cross Entropy (CE) loss function to account for the ordinal nature of the labels by imposing a penalty based on the distance or degree of misclassification. However, these methods often fail to consider the underlying semantic relationships between labels, which could potentially enhance the task performance.\n\nThe research problem, therefore, revolves around comprehensively evaluating and contrasting the classical explicit approaches with implicit methods that utilize Pretrained Language Models (PLMs) to leverage the semantic richness of word embeddings for OC tasks. The primary goals include:\n1. Conducting a unifying analysis of different explicit loss-based techniques, which was previously lacking in the literature.\n2. Proposing a hybrid loss function that balances performance between nominal and ordinal metrics.\n3. Exploring and evaluating implicit approaches using both encoder and decoder-based PLMs to organically factor in the ordinality of labels.\n4. Providing strategic recommendations on the suitable choice between explicit and implicit approaches based on empirical comparisons.\n\n### Relevant Prior Work\n1. **Ordinal Classification Applications**:\n   - Sentiment Analysis (Dang et al., 2020)\n   - Rating Prediction (Liu, 2020)\n   - Age Group Classification (S\u00e1nchez-Hevia et al., 2022)\n\n2. **Explicit Approaches**:\n   - Variations of Cross Entropy (CE) for OC (Yamasaki, 2022; Castagnos et al., 2022; D\u00edaz and Marathe, 2019)\n   - Penalizing based on the absolute difference in class rankings (D\u00edaz and Marathe, 2019; Castagnos et al., 2022)\n   - Evaluation metrics for OC: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Off-by-1 (OB1) accuracy (Castagnos et al., 2022)\n\n3. **Implicit Approaches Using PLMs**:\n   - Encoder Models: BERT (Devlin et al., 2019)\n   - Decoder Models: GPT (Radford et al., 2018a)\n   - Utilization of high-dimensional vector representations (embeddings) that capture the underlying semantics of words for OC (reflecting ordinal relationships)\n\nThe paper aims to fill the identified gap by offering a holistic comparison and proposing a more nuanced approach incorporating both explicit and implicit methods for ordinal classification in NLP.",
        "methodology": "The proposed method in the paper \"Exploring Ordinality in Text Classification: A Comparative Study of Explicit and Implicit Techniques\" builds upon the work of Wang et al. (2021), aiming to enhance few-shot capabilities of Pretrained Language Models (PLMs) by reformulating a vanilla classification task into an entailment-style task. Here\u2019s a detailed breakdown of the methodology:\n\n### Key Components and Innovations\n\n1. **Entailment-Style Task Reformulation**:\n    - The core idea is to transform the classification task into an entailment task, akin to a Natural Language Inference (NLI) problem.\n    - The model learns to predict if the input text and the proposed label entail each other, leveraging the semantic relationship between them.\n\n2. **Focus on Ordinality**:\n    - The method considers the ordinal nature of labels, which has not been explored in previous studies.\n    - This approach inherently understands the semantic closeness between ordinal labels (e.g., very negative sentiment vs. negative sentiment).\n\n3. **Classifier and Label Collection**:\n    - A classifier based on a Pretrained Language Model (PLM) is assumed.\n    - The textual labels are collected and represented in a natural language format.\n\n4. **Data Preparation**:\n    - The training dataset is divided into subsets corresponding to each label.\n    - For each training data point, multiple samples are generated using an entailment-style data augmentation technique.\n    - For instance, if the ground truth label for a data point is \\( y_{i} \\), the augmented samples involve concatenating the data point with a natural language description of the label, creating one positive sample and several negative samples.\n\n5. **Training Process**:\n    - The entailment samples are used to fine-tune the model parameters \\( \\theta \\) for a binary classification task.\n    - This process involves determining whether the input text entails the label description or not.\n\n6. **Inference Phase**:\n    - During inference, the model calculates the predicted label for a new data point.\n    - The softmax function is applied to the predicted logits to ensure the class probabilities sum up to 1, followed by an argmax operation to determine the final label.\n\n7. **Ordinality and Semantic Relationships**:\n    - By training the model to understand the natural language meanings of the labels, it learns to predict labels that are ordinally consistent.\n    - This avoids significant deviation from the ground truth by recognizing the semantic proximity among ordinal labels.\n    - In contrast, traditional Cross-Entropy (CE) understands labels as mere numbers, without recognizing their inter-semantic relations.\n\n8. **Base Model and Example**:\n    - BERT-base is used as the base model for experiments in this study.\n    - An example in a sentiment classification task is provided, where a template like \"indicates positive sentiment\" is used to describe the label in natural language.\n\n### Conclusion\nBy leveraging the semantic relationships inherent in the labels through entailment-based learning, this method aims to enhance the model's ability to understand and predict ordinal relationships between labels, offering a nuanced take on text classification tasks. The exact label templates used in the experiments are detailed in the paper's appendix.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Datasets:**\nThe main experiment uses three benchmark multi-class text classification datasets, each addressing a distinct task:\n1. **SNLI** (Stanford Natural Language Inference) - Hypothesis entails Premise task.\n2. **Amazon Reviews (AR)** - Reviews Classification task.\n3. **SST-5** (Stanford Sentiment Treebank) - Sentiment Analysis task.\n\n**Baselines:**\nThe experiment benchmarks the performance of various loss functions against text classification tasks. These loss functions and techniques are:\n1. **Cross-Entropy (CE)**\n2. **Ordinal Log Loss (OLL)**\n3. **Soft Labels (SOFT)**\n4. **Earth Mover Distance (EMD)**\n5. **Unimodal Losses** including VS-SL (V-Shaped Stereotyped Logit)\n6. **COnsistent RAnk Logits (CORAL)**\n7. **Weighted Kappa Loss (WKL)**\n\nAdditionally, a new proposed loss function, **Multitask Log Loss (MLL)**, which combines CE and OLL, is evaluated.\n\n**Evaluation Metrics:**\nTo measure both classification and ordinal performance, the following metrics are used:\n1. **Weighted-F1 Score** - evaluates classification performance.\n2. **Mean Absolute Error (MAE)** - evaluates ordinal performance.\n3. **Mean Squared Error (MSE)** - evaluates ordinal performance.\n4. **Off-by-1 (OB1) Accuracy** - evaluates ordinal performance.\n\n**Tools:**\nAll explicit approach-based experiments are performed using BERT-base with additional experiments conducted using TinyBERT.\n\n**Main Results:**\n- CE generally performs best in terms of nominal metrics (e.g., weighted-F1).\n- OLL generally performs best in terms of ordinal metrics (e.g., MAE, MSE, OB1).\n- There is a trade-off between nominal and ordinal performance; improving ordinal metrics tends to decrease nominal metrics.\n- MLL, a weighted combination of CE and OLL, hypothesized to inherit the best attributes of both methods, was introduced.\n- It was found empirically that MLL satisfies the unimodal (UM) condition for 80-90% of test data points.\n\nThe detailed numerical results are provided in Table 3 and elaborated in Appendix A and other relevant appendices. These results showcase how different loss functions perform across the mentioned datasets and metrics, highlighting the strengths and trade-offs involved in each approach."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the performance of explicit loss-based approaches for Ordinal Classification (OC) and introduce a new hybrid loss function that balances ordinal and nominal metrics.",
            "experiment_process": "The study evaluates several explicit loss functions such as Cross Entropy (CE) and Mean Label Loss (MLL), comparing their performances using metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), Off-by-1 (OB1) accuracy, and weighted-F1 scores. These experiments were conducted across different datasets, with detailed results presented in various tables (e.g., Tables 3, 6, 7).",
            "result_discussion": "The proposed hybrid MLL loss improved ordinal performance metrics (MAE, MSE, OB1) compared to CE without degrading nominal performances (weighted-F1). However, the performance difference between the hybrid MLL and other ordinal losses becomes minimal with larger models due to their inherent capacity to improve both nominal and ordinal metrics.",
            "ablation_id": "2405.11775v1.No1"
        },
        {
            "research_objective": "To assess the impact of implicit approaches using Pretrained Language Models (PLMs) like encoder (ENT) and generative (GEN) models for OC, comparing informative and uninformative verbalizers.",
            "experiment_process": "The study utilizes models such as GPT2-small, comparing their performances on datasets like SST-5, SNLI, and AR. The experiments analyze how well these models recognize label orders using informative (meaningful) versus uninformative (random) verbalizers, and metrics like F1-score, MAE, and MSE were used for evaluation.",
            "result_discussion": "The ENT approach performed on par or better than the explicit MLL approach, especially in low-data settings, with lower variance. Informative verbalizers significantly enhanced performance in low-data scenarios, supporting the hypothesis that PLM's leverage label semantics effectively. The GEN approach with GPT2-small generally underperformed compared to ENT and explicit losses but exhibited similar trends in informative vs. uninformative verbalizer effectiveness.",
            "ablation_id": "2405.11775v1.No2"
        },
        {
            "research_objective": "To demonstrate the potential of large generative models for OC using a minimal parameter overhead technique.",
            "experiment_process": "The study utilized the Llama-Adapter technique, adding 1.2M tunable parameters to the base model, and evaluated it against other explicit and implicit approaches using various metrics.",
            "result_discussion": "The Llama-Adapter technique outperformed all other tested approaches in terms of classification performance. The study highlighted that even with larger models, informative versus uninformative verbalizers' performance distinction persisted, confirming the model's ability to recognize order.",
            "ablation_id": "2405.11775v1.No3"
        }
    ]
}