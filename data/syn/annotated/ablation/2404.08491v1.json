{
    "title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation",
    "abstract": "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive performance on cross-language tasks, yet significant performance disparities exist across different languages within the same mPLM.\nPrevious studies endeavored to narrow these disparities by supervise fine-tuning the mPLMs with multilingual data.\nHowever, obtaining labeled multilingual data is time-consuming, and fine-tuning mPLM with limited labeled multilingual data merely encapsulates the knowledge specific to the labeled data.\nTherefore, we introduce ALSACE to leverage the learned knowledge from the well-performing languages to guide under-performing ones within the same mPLM, eliminating the need for additional labeled multilingual data.\nExperiments show that ALSACE effectively mitigates language-level performance disparity across various mPLMs while showing the competitive performance on different multilingual NLU tasks, ranging from full resource to limited resource settings.\nThe code for our approach is available at https://github.com/pkunlp-icler/ALSACE.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, Multilingual Pre-trained Language Models (mPLMs) have attracted significant attention  (Doddapaneni et al., 2021  ###reference_b11###).\nThese mPLMs, such as mBERT (Devlin et al., 2018  ###reference_b10###) and mT5 Xue et al. (2020  ###reference_b40###), are pre-trained on extensive amounts of corpus across hundreds of different languages, which enables them to handle multiple languages within a single model and effectively perform cross-lingual tasks (Lewis et al., 2019  ###reference_b19###; Zhang et al., 2020  ###reference_b44###; Stickland et al., 2020  ###reference_b38###; Mutuvi et al., 2020  ###reference_b25###; Brown et al., 2020  ###reference_b2###; Choudhury and Deshpande, 2021  ###reference_b7###).\nHowever, all mPLMs share a key limitation. Due to discrepancies in the quality and quantity of pre-training corpus available for different languages, there is a noticeable performance disparity among different languages for the same mPLM, especially when comparing the performance of high-resource languages to that of low-resource languages.\nFor example, in Cross-lingual Natural Language Inference (XNLI) task (Conneau et al., 2018  ###reference_b9###),\nhigh-resource languages such as English can achieve a performance advantage of approximately 15 points compared to low-resource languages like Swahili, even within the same mPLM.\nSeveral works have been proposed to investigate the reason for the performance disparity. Kassner and Sch\u00fctze (2019  ###reference_b17###); Wallat et al. (2021  ###reference_b39###); Kassner et al. (2021  ###reference_b16###) demonstrate that mPLMs could learn language-specific knowledge from different languages\u2019 pre-training corpus, but the imbalance of the corpus for different languages leads to the knowledge disparity for different languages.\nTherefore,  Kassner et al. (2021  ###reference_b16###) suggests the observed language-level performance disparity can be attributed to the disparity of learned different languages knowledge during the pre-training stage.\nTherefore, Dong et al. (2021  ###reference_b12###); Hu et al. (2021  ###reference_b15###) attempts to narrow the knowledge disparity by involving additional supervised data in different languages to fine-tune the mPLM.\nHowever, obtaining such labeled multilingual data is time-consuming and expensive.\nMoreover, these labeled data mostly come from limited tasks and domains, which makes it hard to compensate for the large knowledge disparity during the pre-training stage, restricting the generalization performance of the low-resource languages on downstream tasks.\n###figure_1### To utilize the different knowledge across different languages within the same mPLM and mitigate the need for the labeled data, we introduce teAcher Language Selection And Cross-lingual sElf-distillation (ALSACE), which leverages the knowledge from the selected teacher languages to reduce the performance disparity among the languages.\nSpecifically, ALSACE mainly consists of two stages: Teacher Language Selection and Cross-Lingual Self-Distillation.\nFor teacher language selection, the motivation is that high-resource languages may not be ideal for probing knowledge to supervise the other languages.\nFor instance, although Persian is a relatively low-resource language, it may provide more precise answers for Kenya\u2019s cultural queries than English due to the closer linguistic proximity  (Yin et al., 2022  ###reference_b43###) between Persian and Swahili.\nDifferent from simply using the knowledge from high-resource languages (e.g., English) to improve the performance of low-resource languages (e.g., Swahili), we introduce Teacher Language Selection to choose reliable teacher languages for a specific task to supervise the student languages.\nSpecifically, we employ a majority voting strategy to generate pseudo-labels derived from the consensus of the mPLMs\u2019 predictions across different languages in the given multilingual corpus.\nThen, we utilize the average confidence score of the different languages on the generated pseudo labels as the indicator to select the teacher languages automatically.\nAs a result, we can select adaptive teachers for different tasks using the unlabeled sentences in the corpus.\nWe further propose Cross-Lingual Self-Distillation to leverage the knowledge from each selected teacher language to supervise other languages, reducing the performance disparity.\nWe further propose cross-lingual self-distillation to leverage the knowledge from each selected teachers languages to supervise other languages, reducing the performance disparity.\nIt employs a consistency loss that encourages closer alignment between the model output distributions of each reliable teacher language and other languages.\nIn this way, mPLMs can effectively mitigate the language-level performance disparity without relying on the supervised multilingual data.\nExperiments show ALSACE consistently mitigates language-level performance disparity in various mPLMs and show the competitive performance on different multilingual benchmarks, including XNLI Conneau et al. (2018  ###reference_b9###), PAWS-X Yang et al. (2019  ###reference_b41###) and XCOPA (Ponti et al., 2020  ###reference_b28###).\nWe also conduct knowledge probing experiments on the GeoMLAMA Yin et al. (2022  ###reference_b43###) as shown in Figure 1  ###reference_###, demonstrating that ALSACE effectively mitigates language-level performance disparity by addressing knowledge disparity.\nMoreover, our experiments show that ALSACE improves performance not only in low-resource languages but also in high-resource languages.\nThis finding illustrates that ALSACE enables effective knowledge transfer between different languages instead of only transferring knowledge from high-resource to low-resource languages.\nFurther analysis shows that ALSACE can transfer both general knowledge across different languages and language-specific knowledge, i.e., some specific knowledge locally shared by people speaking the specific language, which is only present in the corpus of some specific languages."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Knowledge Disparity Leads to Language-Level Performance Disparity in mPLMs.\nThe mPLMs have shown strong capabilities in many NLP tasks including Natural Language Generation (NLG) (Si et al., 2022a  ###reference_b35###, 2024  ###reference_b34###; Zhao et al., 2023  ###reference_b45###; Cai et al., 2023  ###reference_b3###; Song et al., 2023  ###reference_b37###; Li et al., 2024  ###reference_b20###; Liu et al., 2023b  ###reference_b22###) and natural language understanding (NLU) (Si et al., 2022b  ###reference_b36###, 2023  ###reference_b33###; Liu et al., 2023a  ###reference_b21###; An et al., 2023  ###reference_b1###; Hu et al., 2023  ###reference_b14###).\nHowever, there is a noticeable performance disparity across different languages in the same mPLM.\nSeveral works are proposed to investigate the reason of language-level performance disparity in mPLMs.\nWallat et al. (2021  ###reference_b39###); Kassner et al. (2021  ###reference_b16###) demonstrate that mPLMs could learn different knowledge from different languages data in the pre-training corpus, but imbalanced corpus might lead to knowledge disparity for different languages.\n Kassner et al. (2021  ###reference_b16###) suggests that the performance disparities across different languages could be attributed to the imbalanced knowledge distribution of these languages acquired during the pre-training phase.\nYin et al. (2022  ###reference_b43###) further observe that different languages within a single mPLM can retain distinct knowledge that is locally shared by the people speaking the specific language.\nTherefore, we attempt to address language-level performance disparity from the knowledge disparity perspective.\nPrevious studies have utilized cross-lingual knowledge to mitigate the language-level performance disparity.\nHe et al. (2021  ###reference_b13###) employ lightweight adapters on the mPLMs to mitigate forgetting issues.\nInfoXLM (Chi et al., 2021a  ###reference_b4###) designs a new pre-training task with 42GB parallel data to align the representation of multiple languages.\nXLE (Chi et al., 2022  ###reference_b6###) pre-trains mPLMs with a generator and discriminator structure on 142B tokens.\nThese methods attempt to incorporate multilingual resources to mitigate performance disparity.\nHowever, obtaining multilingual data can be time-consuming and restricts model performance on low-resource languages.\nThus,\n Yang et al. (2022  ###reference_b42###); Nguyen and Tuan (2021  ###reference_b26###) attempt to enhance mPLMs by distilling knowledge from well-learned monolingual teachers.\n Qi et al. (2022  ###reference_b29###) learn from different cross-lingual templates using consistency loss to enforce correspondence representation among languages.\nDifferent from distilling knowledge from other monolingual models, we aim to reduce the language-level performance disparity within mPLMs."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Teacher Language Selection",
            "text": "To mitigate the language-level performance disparity within mPLMs, we utilize knowledge from the appropriate teacher language to supervise other languages.\nAn intuitive idea is to transfer the knowledge from high-resourced to low-resourced languages to mitigate the disparity.\nHowever, due to the different linguistic proximity between different languages, the high-resource languages may not be ideal teachers for transferring knowledge to other languages in the specific task.\nFor example, low-resourced Persian may provide more accurate responses to Kenya\u2019s cultural queries compared to high-resource English, which makes it a better teacher language for Swahili than English.\nTherefore, the proposed Teacher Language Selection aims to choose reliable teacher languages for a specific task to guide the student languages.\nConsidering the given corpus  for the specific multilingual task (e.g., Cross-lingual Natural Language Inference) that spans over  languages, we aim to utilize the proposed Teacher Language Selection to identify the teacher languages to mitigate language-level performance disparity efficiently.\nPrecisely, we first fine-tune the mPLMs with an English training set  of the given task to obtain a better initialization.\nWe secondly utilize the mPLMs to generate the prediction  of the given instance  from corpus  in language .\nThen, we employ a majority vote strategy on the predictions of different languages to generate the pseudo label  of the instance , as follows:\nwhere  denotes the predicted probability of the given mPLM on instance  in language .  is the indicator function, while  signifies the set of all possible results for the given task.\nThe generated pseudo-labels reflect the collective understanding of the provided instance across various languages.\nThus, it reduces the risk of incorrect pseudo-labeling compared to relying solely on the prediction from a single language (even a high-resource language like English).\nWe further employ the pseudo-labels to compute the average confidence score  for each language, which allows us to assess the capabilities of different languages in the mPLM.\nThe average confidence score  indicates the level of agreement between each language and the common understanding of the mPLMs, i.e., languages with a higher average confidence score are more likely to make accurate predictions for a given instance.\nUltimately, we normalize the confidence score and use the normalized score  to evaluate which languages demonstrate superior performance:\nwhere the  refers to the collection of all languages involved in the given multilingual task.\nWe set the threshold  to be the average value of the normalized score  to select the teacher languages  and student languages , as follows:\nIn this way, we can automatically select appropriate teacher languages for the different multilingual tasks to mitigate language-level performance disparity efficiently.\nMoreover, we do not need any labeled multilingual data to improve the cross-lingual transfer ability of mPLMs (Chi et al., 2022  ###reference_b6###, 2021a  ###reference_b4###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Cross-Lingual Self-Distillation",
            "text": "Having selected the appropriate teacher languages for the given multilingual task, we further introduce Cross-Lingual Self-Distillation to leverage the knowledge from each selected teacher language to supervise other languages.\nSpecifically, we construct a parallel multilingual pair set  that consists of parallel sentence pairs between each two languages.\nTo reduce the disturbance caused by student languages, we exclusively employ parallel pairs of teacher-student and teacher-teacher languages as potential candidates for self-distillation.\nTherefore, the instance pair  can be defined as:\nwhere  is the selected teacher languages.\nWe filter out student-student language pairs to prevent student languages from learning from each other.\nFor the selected candidate instance pairs, we use Kullback-Leibler divergence as a consistency loss to encourage closer alignment between the prediction distributions of the reliable teacher language and the target language.\nIn this way, mPLMs can effectively transfer and distill the knowledge from the teacher language to the target language, mitigating the language-level performance disparity.\nThe final consistency loss  can be formulated as follows:\nwhere  is the Kullback-Leibler divergence function.  and  are the prediction distributions of the given mPLM for the inputs  and  in different languages, respectively."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Details",
            "text": "Datasets.\nAs shown in Table 1  ###reference_###, our experiments are conducted on various multilingual benchmarks: XNLI Conneau et al. (2018  ###reference_b9###), PAWS-X Yang et al. (2019  ###reference_b41###), XCOPA (Ponti et al., 2020  ###reference_b28###) and GeoMLAMA Yin et al. (2022  ###reference_b43###).\nExperimental Settings.\nWe follow the cross-lingual transfer setting as Lauscher et al. (2020  ###reference_b18###), first fine-tuning the model with an English training set and directly evaluating the model on multilingual test sets.\nWe apply ALSACE to the fine-tuned model using unlabeled multilingual inputs  from  languages in order to address the language-level performance disparity across those languages.\nSpecifically, We firstly use data generation methods, Supergen Meng et al. (2022  ###reference_b24###), which employ a language model to automatically generate text based on label-descriptive prompts, producing monolingual unlabeled data.\nNext, we use machine translation111The translation API from http://api.fanyi.baidu.com/ is utilized for generating multilingual parallel data. to translate generated monolingual data and create unlabeled parallel multilingual pairs.\nBy combining the data generation method and machine translation system, we establish an automated pipeline for generating unlabeled parallel corpora with minimal cost.\nBaselines. We take the XLM-Align (Chi et al., 2021b  ###reference_b5###),  (He et al., 2021  ###reference_b13###), InfoXLM (Chi et al., 2021a  ###reference_b4###), VECO (Luo et al., 2021  ###reference_b23###), ERNIE-M (Ouyang et al., 2021  ###reference_b27###)\nand XLE (Chi et al., 2022  ###reference_b6###) as baselines.\nDetails can be found in Appendix A.1  ###reference_### and A.2  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Results",
            "text": "Overall Performance.\n\nThe results presented in Table 2  ###reference_### demonstrate that ALSACE achieves the lowest cross-lingual transfer gaps across different baselines on XNLI for various mPLMs.\nALSACE yields an improvement of up to 0.6 points, 2.05 points, and 1.88 points, respectively, in average accuracy compared with XLM-R-base, XLM-R-large, and mT5-large baselines.\nImportantly, we achieve competitive performance with state-of-the-art methods across different mPLMs while improving the cross-lingual transferability of mPLMs without introducing any extra information.\nFor example, InfoXLM (Chi et al., 2021a  ###reference_b4###), which is also based on XLM-R, uses 42GB of multilingual parallel data for pretraining. In contrast, ALSACE depends solely on a small volume of unlabeled parallel data (500-shot), which can be automatically generated with minimal effort and and exhibits superior cross-lingual transferability compared to other baselines.\nWhile we also utilize parallel data to enhance cross-lingual transferability, our motivation diverges:\nInstead of aligning multilingual representations through parallel data, our goal is to leverage the knowledge from teacher languages within mPLMs to supervise others.\nThe 500-shot unlabeled parallel data in ALSACE are exclusively used to distill the knowledge of other languages in mPLMs.\nAs a result, Table 2  ###reference_### shows performance enhancement and cross-lingual transfer gap reduction for most languages across different models.\nIn comparison to state-of-the-art methods, ALSACE does not mandate an extensive pre-training process or a large number of parallel corpora while achieving competitive performance and minimizing the cross-lingual transfer gaps.\nMitigating Languages-Level Performance Disparity. ALSACE effectively mitigates the language-level performance disparity of mPLMs and shows consistent improvements across different mPLMs in both high-resource and low-resource languages.\nSpecifically, not only do the student languages achieve higher-than-average improvements, but teacher languages also benefit from the guidance of their peers.\nThrough self-distillation, ALSACE facilitates cross-language knowledge transferring among both teacher and student languages. It also enables teacher languages to learn from each other.\nEven high-resource languages like French and Spanish have shown improvement across various mPLMs, which further supports this claim.\nNotably, low-resource languages such as Swahili and Urdu experience substantial gains with ALSACE , achieving improvements of 2.7 points and 2.4 points, respectively. These gains are particularly significant considering the relatively limited knowledge stored in multilingual pretrained language models (mPLMs) for these languages compared to other languages.\nCompared with other baselines, ALSACE effectively reduces language-level performance disparities in mPLMs across various languages and minimizes the cross-lingual transfer gap.\nWhile some methods have enhanced overall performance, they have exacerbated the performance discrepancies between languages.\nThey incorporated additional knowledge from the extensive parallel multilingual corpora into mPLMs. However, knowledge disparities persist and may even worsen, leading to increased cross-lingual transfer gaps.\nWe also perform ALSACE across different tasks, such as PAWS-X and XCOPA. The result in Table 6  ###reference_### and Table 9  ###reference_### shows that ALSACE reduces the languages-level performance disparity of mPLMs."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Limited Resource Evaluation",
            "text": "###figure_2### In scenarios with limited resources, where acquiring training data is extremely difficult (even for English), mitigating language-level performance disparities in mPLMs can be more challenging and crucial.\nTherefore, to further evaluate the effectiveness of ALSACE , we performed experiments on both XNLI and PAWS-X datasets in such scenarios.\nSpecifically, to simulate a limited resource scenario for XNLI, we fine-tune the mPLMs on -shot English labeled examples as the baseline. Similarly, for PAWS-X, we fine-tune the mPLMs on -shot English labeled examples. Further details can be found in Appendix A.1  ###reference_###.\nTo minimize the impact of the unlabeled multilingual parallel data used in  ALSACE , and thoroughly investigate the efficacy of self-distillation in ALSACE in limited resource situations, we also introduce two additional baselines: English-Only Self-Training(E. Self-Train) and Full-Language Self-Training(F. Self-Train).\nThe results in Table 5  ###reference_### and Table 6  ###reference_### despite that ALSACE consistently improve the performance of all languages even when the training data is minimal. It underscores that ALSACE improves model performance not by relying on the parallel corpora but by leveraging the knowledge of teacher languages gained from the mPLM pre-training stage, hence proving its robustness and efficiency in limited-resource settings."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": "###figure_3### The knowledge stored within mPLMs can be categorized into language-agnostic knowledge related to general tasks such as XNLI, which are based on logic and conceptual understanding, and language-specific knowledge related to specific linguistic and cultural factors.\nIn order to evaluate the ALSACE \u2019s ability to alleviate performance disparity by reducing knowledge disparity and thereby improving overall performance, we conducted knowledge probing in GeoMLAMA to evaluate the changes in language-specific knowledge of mPLMs. We use the accuracy of question answers grouped according to countries and languages to measure the knowledge of mPLMs.\nWe examined the changes in language-specific knowledge gains before and after applying ALSACE as shown in Figure 2  ###reference_###.\nResults show that ALSACE improves the performance of mPLM on knowledge probing tasks over various languages.\nMore details can be found in Table 12  ###reference_### in Appendix.\nNotably, as shown in Figure 1  ###reference_###, after applying Cross-lingual Self-Distillation, the specific knowledge of teacher languages can be transferred to other languages. It can be found out that under the guidance of teacher languages, other languages answer the geo-specific question correctly. For instance, as shown in the first sub-figure in Figure 2  ###reference_###, English leverages its US-specific knowledge for other languages, leading to overall improvements for those respective languages.\nSimilar results are observed in other sub-figures. This result strongly suggests that mPLMs capture far more knowledge than people previously believed, and language-specific knowledge remains a treasure for better alignment.\nFurthermore, we explore whether ALSACE successfully enhances language-agnostic knowledge over languages. Therefore, as demonstrated in Figure 3  ###reference_###, we evaluate the numbers of the accurately answered questions on the XNLI benchmark. This improvement demonstrates that the language-agnostic knowledge across different languages in mPLMs can mutually learn from each other. Our method reinforces the shared knowledge among the languages by bridging the knowledge disparity. As a result, we ensure that the efficacy of our method relies on alleviating the knowledge disparities across languages, including language-agnostic and language-special knowledge."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we present ALSACE , a simple yet effective method to address the language-level performance disparity in mPLMs.\nALSACE mainly consists of two stages: Teacher Language Selection and Cross-Lingual Self-Distillation.\nALSACE leverages the knowledge learned from the teacher languages to guide other languages and further improves the overall performance and cross-lingual transferability of mPLMs.\nExperiments show that ALSACE effectively mitigates language-level performance disparity and shows competitive performance on various multilingual datasets.\nIn addition, we further analyze each part of the ALSACE to show the strengths of our proposed model.\nOverall, ALSACE is a promising approach to mitigating language-level performance disparity of mPLMs by utilizing self-distillation to reduce the performance disparity."
        }
    ],
    "url": "http://arxiv.org/html/2404.08491v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.08491v1",
        "paper_title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation",
        "research_background": "### Motivation\nThe paper is motivated by the observed performance disparity among different languages when using Multilingual Pre-trained Language Models (mPLMs) such as mBERT and mT5. Despite being trained on extensive multilingual corpora, these models tend to perform significantly better on high-resource languages compared to low-resource languages. The disparity largely stems from the imbalance in the quality and quantity of pre-training data available for different languages. This imbalance leads to knowledge disparity, which subsequently affects the performance of mPLMs on different languages. Existing approaches that use additional supervised data to fine-tune mPLMs face challenges such as the high cost of labeled multilingual data and limited generalizability across various tasks and domains.\n\n### Research Problem\nThe research problem addressed by the paper is to mitigate the language-level performance disparity in mPLMs without relying heavily on supervised multilingual data. The key challenge is to reduce the performance gap between high-resource and low-resource languages while leveraging the diverse knowledge embedded in different languages within the mPLM.\n\n### Relevant Prior Work\nSeveral works have attempted to understand and address performance disparities in mPLMs:\n1. **Research into Reasons for Disparity**: Studies by Kassner and Sch\u00fctze (2019), Wallat et al. (2021), and Kassner et al. (2021) found that mPLMs learn language-specific knowledge from different languages' pre-training corpora. The disparity in this knowledge acquisition leads to performance differences.\n2. **Mitigating Knowledge Disparity**: Efforts by Dong et al. (2021) and Hu et al. (2021) tried to narrow the knowledge disparity by fine-tuning mPLMs with additional supervised data in multiple languages. However, obtaining such data is both time-consuming and expensive, and the data is often task-specific, limiting its effectiveness.\n3. **Knowledge Utilization**: Research in leveraging cross-lingual knowledge has shown promise in improving mPLMs' performance on low-resource languages. For example, work by Yin et al. (2022) highlights the potential of linguistic proximity in selecting more effective teacher languages for specific tasks.\n\n### Summarized Proposed Solution\nTo address these issues, the paper proposes ALSACE (teAcher Language Selection And Cross-lingual sElf-distillation), a two-stage method that combines Teacher Language Selection and Cross-Lingual Self-Distillation. This approach aims to utilize the diverse knowledge in different languages within the mPLM to mitigate performance disparities across languages. By selecting appropriate teacher languages based on task-specific criteria and using self-distillation methods, ALSACE significantly reduces the reliance on supervised multilingual data and enhances performance across both high-resource and low-resource languages.",
        "methodology": "**Methodology: Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation**\n\nTo address the language-level performance disparity in multilingual Pretrained Language Models (mPLMs), we leverage knowledge from suitable teacher languages to guide and supervise other languages. The core idea is to transfer knowledge from high-resource to low-resource languages; however, the linguistic proximity between languages can significantly influence the effectiveness of this transfer. High-resource languages may not always be ideal teachers, as seen in the example where Persian, despite being a low-resource language, might provide better cultural responses for Swahili than high-resource English.\n\n**Key Components:**\n\n1. **Teacher Language Selection:**\n   - The method aims to choose reliable teacher languages for specific tasks to guide student languages effectively.\n   - The process begins by fine-tuning the mPLMs with an English training set specific to the task to establish a solid initialization.\n\n2. **Prediction and Pseudo-Label Generation:**\n   - mPLMs generate predictions for instances from the corpus in various languages.\n   - A majority vote strategy on these predictions creates pseudo labels for each instance, ensuring a consensus-based approach rather than relying on a single language prediction:\n     - \\( y^{p}_{i} = \\underset{c \\in C}{\\arg\\max} \\sum_{l \\in L} \\mathbf{1}_{\\{c = \\arg\\max p^{(l)}_i \\}} \\)\n       - \\( p^{(l)}_i \\) is the predicted probability of instance \\( x_i \\) in language \\( l \\).\n       - \\( \\mathbf{1} \\) is the indicator function, and \\( C \\) is the set of all possible results.\n\n3. **Confidence Score Calculation:**\n   - Using the pseudo labels, the method computes the average confidence score \\( R_l \\) for each language:\n     - This score reflects how well each language's predictions align with the overall consensus, thereby identifying languages likely to make accurate predictions.\n\n4. **Normalized Score and Teacher Language Selection:**\n   - The normalized score \\( S_l \\) helps evaluate language performance:\n     - \\( S_l = \\frac{R_l}{\\sum_{l' \\in L} R_{l'}} \\)\n   - The threshold \\( \\xi \\) (set to the average value of \\( S_l \\)) is used to differentiate between teacher and student languages:\n     - Languages with a normalized score above the threshold become teacher languages \\( L_T \\).\n     - Those below the threshold are designated as student languages \\( L_S \\).\n\nThe methodology allows for the automatic selection of appropriate teacher languages for different multilingual tasks, efficiently mitigating language-level performance disparities. Notably, it does not require labeled multilingual data, enhancing the cross-lingual transfer abilities of mPLMs in alignment with prior works (Chi et al., 2022; Chi et al., 2021a).",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets\nThe experiments are conducted on several well-known multilingual benchmarks:\n1. **XNLI** (Conneau et al., 2018)\n2. **PAWS-X** (Yang et al., 2019)\n3. **XCOPA** (Ponti et al., 2020)\n4. **GeoMLAMA** (Yin et al., 2022)\n\n#### Experimental Settings\nThe main experiment closely follows the cross-lingual transfer setting as outlined by Lauscher et al. (2020). Specifically:\n1. **Initial Fine-Tuning**: The model is first fine-tuned using an English training set.\n2. **Direct Evaluation**: Post fine-tuning, the model is directly evaluated on the test sets of the aforementioned multilingual benchmarks.\n3. **ALSACE Application**: To address language-level performance disparity, ALSACE (a specific methodological approach) is applied to the fine-tuned model using unlabeled multilingual data as inputs.\n\n#### Data Generation and Translation\n1. **Data Generation**: Supergen (Meng et al., 2022) is employed to automatically generate monolingual unlabeled data. This method utilizes a language model to produce text from label-descriptive prompts.\n2. **Machine Translation**: A translation API (from http://api.fanyi.baidu.com/) is then used to translate the generated monolingual data, creating multilingual parallel data pairs.\n3. **Automated Pipeline**: By combining the data generation and translation steps, an automated pipeline is established for the cost-effective generation of unlabeled parallel corpora.\n\n#### Baselines\nThe performance of the proposed method is compared against multiple state-of-the-art multilingual pre-trained language models:\n- **XLM-Align** (Chi et al., 2021b)\n- **InfoXLM** (Chi et al., 2021a)\n- **VECO** (Luo et al., 2021)\n- **ERNIE-M** (Ouyang et al., 2021)\n- **XLE** (Chi et al., 2022)\n\nFurther details on the baselines can be found in Appendix A.1 and A.2.\n\n### Main Experimental Results\nThe specific results and comparison metrics are not explicitly mentioned in the provided text. However, it can be inferred that the performance metrics would likely focus on assessing the multilingual performance disparities and the effectiveness of ALSACE in mitigating these disparities across the evaluated benchmarks. Generally, common evaluation metrics for such experiments include accuracy, F1 score, and other standard cross-lingual performance measures."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of Teacher Language Selection in improving the performance and reducing cross-lingual transfer gaps in multilingual Pretrained Language Models (mPLMs).",
            "experiment_process": "Using XLM-R-large as the backbone, we reported average performance and cross-lingual transfer gaps of different language groups. Specific comparisons were made between ALSACE and other baselines, examining the impact of excluding teacher language selection and removing certain languages from distillation groups.",
            "result_discussion": "Teacher Language Selection in ALSACE significantly reduces cross-lingual transfer gaps and improves performance, especially for student languages. Excluding teacher language selection diminishes performance and increases transfer gaps. Removing languages with weak performance also led to a decrease in performance for both teachers and students, supporting the hypothesis that underperforming languages can effectively guide others due to linguistic proximity.",
            "ablation_id": "2404.08491v1.No1"
        },
        {
            "research_objective": "To investigate the source of performance increase and validate the effectiveness of self-distillation in the ALSACE framework.",
            "experiment_process": "Two self-distillation settings were tested: English-Only Self-Training and Full-Language Self-Training. These were applied to mT5 and XLM-R baselines. In English-Only Self-Training, a model fine-tuned on English data was used to generate pseudo-labels for unlabeled English data, selecting the top 50% for fine-tuning. In Full-Language Self-Training, pseudo-labels were generated for translated multilingual data in all languages, with the top 50% selected for fine-tuning.",
            "result_discussion": "ALSACE outperforms all self-distillation baselines on XNLI, particularly improving cross-lingual transferability for student languages. This indicates that the improved performance of ALSACE stems from the method of self-distillation rather than the use of additional multilingual data.",
            "ablation_id": "2404.08491v1.No2"
        }
    ]
}