{
    "title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?",
    "abstract": "By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks.\nHowever, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages.\nIn this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions.\nWe initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions.\nFurthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context.\nOur investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents.\nBesides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions.\nResources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Despite the impressive capabilities of Large Language Models(LLMs) (Brown et al., 2020  ###reference_b5###; Ouyang et al., 2022  ###reference_b25###; Chowdhery et al., 2023  ###reference_b9###) when accomplishing a wide range of tasks, their effectiveness is compromised by inherent limitations rooted in their limited parametric memory, resulting in instances of hallucination or inaccurate responses (Shuster et al., 2021  ###reference_b30###; Ji et al., 2023  ###reference_b17###).\nAugmented with external retrievers, LLMs demonstrate superior performance by retrieving from external knowledge sources (Lewis et al., 2020  ###reference_b20###; Guu et al., 2020  ###reference_b14###; Borgeaud et al., 2022  ###reference_b4###; Izacard et al., 2023  ###reference_b16###).\nHowever, current retrieval systems are not always reliable since they often provide top-ranked passages indiscriminately that still contain irrelevant information (BehnamGhader et al., 2023  ###reference_b3###; Asai et al., 2024  ###reference_b1###).\nIn real-world Retrieval-Augmented Generation (RAG) applications, retrievers are facing more complex forms of irrelevant information (Cuconasu et al., 2024  ###reference_b10###).\nAlthough such irrelevant information scores highly on similarity metrics and may be semantically related to the context, it is irrelevant to answering questions.\nEven worse, irrelevant information may cause LLMs to change what they have believed, leading to a fabricated answer (Wang et al., 2023  ###reference_b32###).\nIn Figure 1  ###reference_###, we give an example to show how such related irrelevant information might distract LLMs, as the misleading information may prompt LLMs to engage in over-reasoning (Hou et al., 2024  ###reference_b15###; Chiang & Lee, 2024  ###reference_b8###).\nIn this work, we study the robustness of LLMs to irrelevant information.\nTo be specific, we seek to answer the question: \nHow well do current LLMs perform when encountering irrelevant information, particularly when it is semantically related?\nTo answer this question, we adopt question answering (QA) tasks for fundamental experiments due to their prevalence in real-world RAG applications (Gao et al., 2023  ###reference_b12###).\nWe first introduce a framework to construct irrelevant information that ranges from semantically unrelated, partially related, and related to questions, and give an analysis that our irrelevant information exhibits high quality, with similarity scores comparable to those of the top-ranked information from Wikipedia, which is easily retrieved by RAG systems.\nWe then systematically assess the robustness of LLMs when faced with irrelevant information, examining their performance under various conditions. We highlight our key findings:\nCompared to common semantically unrelated irrelevant information, LLMs are more likely to be misled by irrelevant information that is highly semantically related.\nWith the increment of irrelevant information quantity, LLMs are less capable of identifying truly relevant information and are more easily distracted.\nThe robustness of LLMs to irrelevant information varies with the question format, with the free-form format proving to be the most robust.\nCurrent strategies intended to improve LLMs\u2019 discrimination capabilities result in only marginal, and sometimes even detrimental, enhancements in their ability to accurately identify and disregard irrelevant information.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) demonstrates impressive abilities in a wide range of knowledge-intensive tasks (Lewis et al., 2020  ###reference_b20###; Guu et al., 2020  ###reference_b14###; Borgeaud et al., 2022  ###reference_b4###; Izacard et al., 2023  ###reference_b16###).\nLLMs utilize retrieval systems to navigate through external knowledge bases and identify a set of potentially relevant documents, thereby extending beyond the limitations of their parametric memory.\nSpecifically, leveraging dense retriever models (Karpukhin et al., 2020  ###reference_b18###; Gautier et al., 2022  ###reference_b13###) and in-context learning (ICL) (Brown et al., 2020  ###reference_b5###), retrieval-augmented approaches have shown to be remarkably effective in enhancing the capabilities of LLMs (Luan et al., 2021  ###reference_b21###; Mallen et al., 2023  ###reference_b22###; Ram et al., 2023  ###reference_b26###; Shi et al., 2023b  ###reference_b29###).\nNonetheless, a challenge persists in the practical deployment of RAG systems, as they indiscriminately surface top-ranked documents that still include irrelevant distractions (BehnamGhader et al., 2023  ###reference_b3###; Wang et al., 2023  ###reference_b32###; Asai et al., 2024  ###reference_b1###; Cuconasu et al., 2024  ###reference_b10###).\nThis issue undermines their utility in real-world applications, where precision and relevance in information retrieval are critical for decision-making processes, such as in medical diagnoses (Zhou et al., 2023  ###reference_b38###).\nThe presence of irrelevant information can lead to inaccurate outcomes, highlighting the need to enhance the reliability of RAG systems."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robustness to Irrelevant Information",
            "text": "Robustness, which refers to a system\u2019s stability when confronted with unexpected inputs (Chang et al., 2023  ###reference_b6###), has been extensively evaluated in previous studies on LLMs(Zhu et al., 2023  ###reference_b39###; Chen et al., 2024  ###reference_b7###).\nGiven its potential to significantly impact model performance, irrelevant information has also attracted attention in the community (Shi et al., 2023a  ###reference_b28###).\nPrior studies (Shi et al., 2023a  ###reference_b28###; Wu et al., 2024  ###reference_b34###) add specific instruction into prompts, enabling LLMs to better solve math word problems by automatically verifying the irrelevant content within problem descriptions.\nThis approach can be combined with Chain-of-Thought (CoT) prompting methods (Wei et al., 2022  ###reference_b33###; Kojima et al., 2022  ###reference_b19###).\nHowever, these investigations primarily focus on irrelevant problem descriptions in arithmetic reasoning.\nIn contrast, the challenge of irrelevant information in RAG applications arises more often from retrieved passages.\nPrevious studies often classify low-ranked passages, random passages, and top-ranked passages without ground truth answers as irrelevant information (Yoran et al., 2023  ###reference_b36###; Wang et al., 2023  ###reference_b32###; Yu et al., 2023  ###reference_b37###; Chen et al., 2024  ###reference_b7###).\nNonetheless, current advanced RAG systems may effectively filter out such content (Askari et al., 2023  ###reference_b2###).\nIn the real-world scenario, however, semantically related yet irrelevant information, which is highly likely to be retrieved by current systems, remains a challenge.\nTo bridge this gap, our work meticulously constructs high-quality irrelevant information and offers a comprehensive analysis of LLM performance across various scenarios.\nThis method enhances our understanding of LLMs\u2019 interactions with irrelevant information, thereby providing valuable insights for improving the efficiency and effectiveness of RAG systems."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Datasets",
            "text": "Given the widespread use of question answering (QA) tasks in real-world RAG applications (e.g., New Bing), following previous work (Yoran et al., 2023; Wang et al., 2023; Yu et al., 2023), we employ QA tasks as the foundation for our experiments. Specifically, we focus on entity-centric QA since it is prevalent in RAG scenarios. \n\nPopQA (Mallen et al., 2023): This entity-centric QA dataset comprises questions, derived from fact (subj, relationship, obj) triples of 16 relationship types in Wikidata. For example, the question, \u201cIn what city was Julius Erving born?\u201d, is derived from (Julius Erving, place of birth, New York City) triples. \n\nEntityQuestions (Sciavolino et al., 2021): To encompass a wider range of question types in application scenarios, we adopt another widely used entity-centric QA dataset EntityQuestions to broaden the diversity. We exclude relationships that were previously addressed in PopQA to minimize redundancy, yielding 17 distinct relationship types within this dataset. Aligning with the scale of PopQA, we randomly sample 1,500 entries in each relationship for subsequent experiments. Please refer to Appendix A.1 for more details."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Parametric Memory Elicitation",
            "text": "To rigorously evaluate whether LLMs are distracted by irrelevant information, it is essential to first assess their previously internal knowledge free from disturbances. Specifically, following Xie et al. (2023 ###reference_b35###), through closed-book QA format, we extract answers to questions from QA datasets, as well as the corresponding parametric memory from LLMs. For instance, as shown in Table 1 ###reference_###, given a question, \u201cIn what city was Julius Erving born?\u201d, LLMs are guided to provide a memory answer \u201cNew York City\u201d along with background details. Furthermore, the elicited parametric memory will serve as one of the pieces of relevant information in the subsequent experiment, leveraging LLMs\u2019 inherent confirmation bias to trust their parametric memory (Xie et al., 2023 ###reference_b35###), enhancing the credibility of findings within RAG systems that use LLMs as foundational models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Graded Irrelevant Information",
            "text": "Previous research has shown that LLMs can be easily distracted by irrelevant information, where even information with no relation to the topics of the questions they address can mislead LLMs (Shi et al., 2023a  ###reference_b28###). However, there is a lack of detailed analysis concerning the degree of semantic relevance of irrelevant information that affects the performance of LLMs. To address this gap, we introduce a framework for categorizing irrelevant information into three graded levels, aiming to explore its impact in depth. Specifically, as shown in Figure 2  ###reference_### we define three distinct levels of irrelevant information: Unrelated Information, Partially Related Information, and Related Information. Given the vast amount of information stored in databases, retrieving passages with high similarity scores that are nonetheless unrelated to the question topic is inevitable. We categorize such information as Unrelated Information."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments and Analysis",
            "text": "In this section, we focus on assessing the robustness of LLMs when faced with irrelevant information, examining their performance under various conditions. To be specific, we explore the issue from four distinct perspectives:\n\n1) Semantic Relevance\n2) Quantity of Information\n3) Question Format\n4) Limitations of Current Solutions.\n\nWe adopt four widely used LLMs for our analysis, including three closed-source LLMs GPT-3.5 Turbo (OpenAI, 2022  ###reference_b23###), GPT-4 Turbo (OpenAI, 2023  ###reference_b24###), and Gemini Pro (G Team et al., 2023  ###reference_b11###), as well as one open-source LLM Llama2-7B (Touvron et al., 2023  ###reference_b31###)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Question Format",
            "text": "Considering various question formations in the real world, in this section, we aim to investigate how question formats influence the performance of LLMs with the interference of irrelevant information. Specifically, in addition to the multiple-choice format, we introduce boolean (true/false) and free-form QA to our experiments. In boolean QA, we ask LLMs to judge the truthfulness of a misleading statement (e.g., \u201cJulius Erving was born in Baltimore\u201d). They are considered distracted if they provide a \u201ctrue\u201d response. In free-form QA, we present questions to LLMs without providing any options. Due to the difficulty in automatically determining precise answers from LLMs\u2019 free-form responses, we utilize GPT-3.5 Turbo to align these responses with specific options. To ensure the accuracy and fairness of GPT-3.5 Turbo\u2019s automatic alignment, we conduct human evaluations on randomly selected cases, achieving a % accuracy rate. This high level of accuracy validates the fairness and reliability of our assessment method. More details are in Appendix B.1 ###reference_###. Such an inconsistent robustness might undermine the truthfulness of RAG systems since the question formats in real-world applications are various. Please refer to Appendix B.2 ###reference_### for an in-depth analysis of the influence of irrelevant answers and case demonstration."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we introduce a framework to construct irrelevant information that ranges from semantically unrelated, partially related, and related to questions.\nThe semantically related information exhibits high quality, with similarity scores comparable to human-written information from Wikipedia, which is easily retrieved by RAG systems.\nOur experiments show that current LLMs still struggle with discriminating highly semantically related irrelevant information under various conditions.\nAnd current solutions have limitations in improving the robustness of LLMs to such information.\nWe advocate focused research on mitigating misleading irrelevant interference in the development of reliable RAG systems."
        }
    ],
    "url": "http://arxiv.org/html/2404.03302v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "3.6"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.3",
            "3.4",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.03302v1",
        "paper_title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?",
        "research_background": "### Paper's Motivation\nLarge Language Models (LLMs) have demonstrated impressive capabilities across a variety of tasks. However, their effectiveness is often hindered by limitations such as hallucinations or the generation of inaccurate responses due to their constrained parametric memory. Enhancements using external retrievers have shown promise in improving LLM performance by providing access to additional knowledge. Nevertheless, these retrievers are not infallible and frequently supply passages that, while top-ranked, contain irrelevant information that can mislead the LLMs. The paper is motivated by this challenge and aims to investigate the impact of irrelevant, yet semantically related, information on the robustness of LLMs.\n\n### Research Problem\nThe core research problem tackled in this paper is to determine the degree to which current LLMs are influenced by irrelevant information, specifically when such information is semantically related to the context. The study aims to scrutinize whether LLMs can maintain performance integrity and accurately discern relevant from irrelevant data when presented with various forms and quantities of irrelevant information.\n\n### Relevant Prior Work\n1. **LLM Capabilities & Limitations**: Prior works have highlighted the powerful task performance of LLMs (Brown et al., 2020; Ouyang et al., 2022; Chowdhery et al., 2023) but also their limitations due to restricted parametric memory leading to issues like hallucination (Shuster et al., 2021; Ji et al., 2023).\n\n2. **Augmentation with External Retrievers**: External retrieval-augmented models have been shown to significantly bolster LLM performance by tapping into additional knowledge sources (Lewis et al., 2020; Guu et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023).\n\n3. **Issues with Current Retrieval Systems**: Despite their benefits, current retrieval systems often include irrelevant information that can still rank highly due to similarity metrics (BehnamGhader et al., 2023; Asai et al., 2024). This irrelevant information, while semantically related, does not contribute to accurately answering questions and can mislead the models (Cuconasu et al., 2024; Wang et al., 2023).\n\n4. **Impact of Irrelevant Information**: Previous works have indicated that even high-similarity irrelevant information can skew LLM responses, causing over-reasoning or fabrication in answers (Hou et al., 2024; Chiang & Lee, 2024).\n\nThis study builds upon these insights by systematically evaluating how well LLMs handle irrelevant information in various QA scenarios, given the prevalence of such tasks in real-world RAG applications (Gao et al., 2023).",
        "methodology": "The methodology section of this study describes a novel approach to evaluate how irrelevant inputs impact the responses of large language models (LLMs). The core components and innovations include:\n1. **Datasets**:\n    - The study leverages Wikipedia passages to create datasets with varying degrees of relevance to specified questions: semantically unrelated, partially related, and related information.\n\n2. **Construction of Irrelevant Information**:\n    - **Unrelated Information**:\n        - Passages are selected that are semantically similar but topically unrelated to the questions. For example, a passage involving \"Bonaparte\" and \"Baltimore\" might be used despite its lack of direct relevance to the question.\n        - The passage must belong to the same relationship category (e.g., place of birth) but contain different subjects and objects.\n\n    - **Partially Related Information**:\n        - The first paragraph is selected from the Top 10 retrieved passages that include the subject (e.g., Julius Erving) but do not answer the question (lacking the object, e.g., New York City).\n        - The second paragraph is obtained by finding the passage with the highest similarity score containing the correct answer (object) and retrieving its introduction via Wikipedia-API.\n        - The two paragraphs are concatenated to form the \"Partially Related Information\".\n\n    - **Related Information**:\n        - This involves overcoming existing data by ensuring high semantic relevance without contributing to question-answering.\n        - Construction involves manipulating the \u201cPartially Related Information\u201d stage's triples to introduce misleading connections between subjects and wrong objects or other subjects.\n        - The methodology outlines three types of \u201cRelated Information\u201d:\n            1. **Misleading Linkage**: Reinforces incorrect subject-object connections (e.g., Julius Erving and Baltimore through sports).\n            2. **Common Characteristics**: Emphasizes similarities between the question\u2019s subject and an irrelevant subject linked to the correct object.\n            3. **Fictional Anecdotes**: Uses creative storytelling to create irrelevant but seemingly plausible scenarios linking different subjects.\n\n3. **Metrics for Evaluation**:\n    - The study introduces specific metrics for assessing the robustness of LLMs to the various constructed irrelevant information types, ensuring thorough evaluation.\n\n4. **Utilization of GPT-4 Turbo**:\n    - The methodology leverages GPT-4 Turbo to generate natural language information for the misleading triples and connections, enhancing the practical robustness of the generated test cases.\n\nThe methodology provides a comprehensive approach to probing LLM robustness using elaborate and varied irrelevant information, aiming to reveal potential biases or vulnerabilities in model responses. For more details on datasets at each step, the paper refers to Appendix A.3 (details referenced).",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nIn this section, we focus on assessing the robustness of LLMs when faced with irrelevant information, examining their performance under various conditions. Specifically, we explore the issue from four distinct perspectives: Semantic Relevance, Quantity of Information, Question Format, and Limitations of Current Solutions.\n\n#### LLMs Used for Analysis:\n\n1. **Closed-source LLMs:**\n   - **GPT-3.5 Turbo** (OpenAI, 2022)\n   - **GPT-4 Turbo** (OpenAI, 2023)\n   - **Gemini Pro** (G Team et al., 2023)\n\n2. **Open-source LLM:**\n   - **Llama2-7B** (Touvron et al., 2023)\n\n#### Datasets:\nThe datasets used for the main experiment likely consist of various question-answer pairs, crafted to evaluate the LLMs' robustness in the face of irrelevant inputs. However, specific datasets used are not mentioned in the provided text, thus cannot be detailed here.\n\n#### Baselines:\nThe baselines for this experiment would include the same set of LLMs evaluated under normal conditions, without any introduced irrelevant information. This allows for a comparative analysis of how the presence of irrelevant input skews the responses.\n\n#### Evaluation Metrics:\nWhile specific metrics are not detailed in the provided text, common evaluation metrics for such analyses include:\n- **Accuracy:** The correctness of the LLMs' responses.\n- **Robustness:** The LLMs' consistency in providing accurate responses despite the introduction of irrelevant input.\n- **Relevance Assessment:** Evaluating how semantically relevant the LLMs consider the given information in their responses.\n\n#### Experimental Results:\nThe paper aims to highlight the responses of the LLMs under varied tests:\n- **Semantic Relevance:** Evaluating whether the LLMs can discern relevant information from irrelevant input.\n- **Quantity of Information:** Testing the LLMs' performance when bombarded with excessive irrelevant data.\n- **Question Format:** Assessing how changes in the question format affect the LLMs' ability to filter irrelevant inputs.\n- **Current Solutions:** Analyzing the limitations of existing methods designed to counteract the influence of irrelevant information.\n\nThe experiment results would articulate the degree to which each LLM's responses are skewed by irrelevant inputs, providing insights into their comparative robustness. Without specific results provided in the text, it's inferred that the final part of this section would present detailed findings related to the robustness and susceptibility of the LLMs under the experiment's four main perspectives.\n\nIn conclusion, the main experiment is designed to diagnose and measure the effectiveness of current LLMs in handling irrelevant information, across different dimensions of input interference. The results underscore the necessity for developing more robust models capable of maintaining accuracy and relevance in the presence of extraneous data."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the robustness of LLMs when handling varying degrees of semantically irrelevant information, aiming to understand how the LLMs' performance is influenced by this noise.",
            "experiment_process": "The study assessed LLMs' performance when confronted with irrelevant information of three different levels of semantic relevance: Unrelated Information, Partially Related Information, and Related Information. Experiments introduced a single piece of irrelevant information at a time. Different models were used, including GPT-3.5 Turbo, GPT-4 Turbo, Gemini Pro, and Llama2-7B.",
            "result_discussion": "Results indicated that, except for Llama2-7B, LLMs mostly maintained their original answers with 'Unrelated Information.' However, there was a notable increase in misrepresentation instances with 'Partially Related Information' and 'Related Information,' suggesting LLMs are more prone to being misled by highly semantically related irrelevant information.",
            "ablation_id": "2404.03302v1.No1"
        },
        {
            "research_objective": "Analyze how varying quantities of irrelevant information influence LLMs' performance to identify their capability to focus amidst distractions.",
            "experiment_process": "The study examined LLMs' performance with varying quantities of irrelevant information using distinct variants of 'Related Information.' Experiments included both irrelevant and relevant content, while models used were GPT-3.5 Turbo and Llama2-7B.",
            "result_discussion": "Findings showed that increasing the quantity of irrelevant information led to more significant distractions for LLMs. Despite their ability to recognize relevant information, their performance was still compromised by increasing irrelevant content, particularly evident in Llama2-7B.",
            "ablation_id": "2404.03302v1.No2"
        },
        {
            "research_objective": "Explore how different question formats affect LLMs' performance, particularly when facing irrelevant information.",
            "experiment_process": "The study tested LLMs using multiple-choice, boolean (true/false), and free-form QA formats. For boolean QA, LLMs judged the truthfulness of misleading statements. For free-form QA, responses were aligned with specific options using GPT-3.5 Turbo, verified through human evaluations to ensure accuracy.",
            "result_discussion": "Results revealed that LLMs were less likely to be misled under free-form and boolean QA compared to multiple-choice. This inconsistency suggests varying robustness across different question formats, indicating that the presence of irrelevant answers in multiple-choice format more significantly distracts LLMs.",
            "ablation_id": "2404.03302v1.No3"
        },
        {
            "research_objective": "Evaluate the effectiveness of existing strategies in mitigating the impact of irrelevant information within complex environments, featuring multiple irrelevant and relevant pieces of information.",
            "experiment_process": "The study simulated environments with a combination of five pieces of irrelevant content and two pieces of relevant information. Strategies assessed included the CoT method with a prompt, enhanced prompts with an 'ignoring' instruction, and In-Context Learning (ICL) by incorporating examples with labels.",
            "result_discussion": "Findings indicated CoT led to over-reasoning when faced with misleading content. The 'ignoring' instruction showed minimal improvement. Incorporating examples increased the misrepresentation ratio, suggesting current strategies are ineffective in complex scenarios, presenting challenges for reliable RAG system deployment.",
            "ablation_id": "2404.03302v1.No4"
        }
    ]
}