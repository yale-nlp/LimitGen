{
    "title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs",
    "abstract": "We are beginning to see progress in language model assisted scientific discovery. Motivated by the use of LLMs as a general scientific assistant, this paper assesses the domain knowledge of LLMs through its understanding of different mathematical skills required to solve problems. In particular, we look at not just what the pre-trained model already knows, but how it learned to learn from information during in-context learning or instruction-tuning through exploiting the complex knowledge structure within mathematics. Motivated by the Neural Tangent Kernel (NTK), we propose NTKEval to assess changes in LLM\u2019s probability distribution via training on different kinds of math data. Our systematic analysis finds evidence of domain understanding during in-context learning. By contrast, certain instruction-tuning leads to similar performance changes irrespective of training on different data, suggesting a lack of domain understanding across different skills.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language inference tasks (Achiam et al., 2023  ###reference_b1###; Touvron et al., 2023a  ###reference_b27###; Chowdhery et al., 2023  ###reference_b12###; Anil et al., 2023  ###reference_b3###; Touvron et al., 2023b  ###reference_b28###; Team et al., 2023  ###reference_b26###). With the promising success, there is a growing trend to use LLM as creative assistants for scientific discovery: from mathematics (Trinh et al., 2024  ###reference_b29###) to biology (Madani et al., 2023  ###reference_b20###).\nMotivated by the use of LLM as a scientific assistant, our paper assesses the domain knowledge of LLMs through their understanding of different mathematical skills required to solve problems. Understanding can be measured in two ways: the degree to which it solves problems correctly; and the degree to which it enables fast adaptation to new knowledge. Similarly, \u201cunderstanding\u201d in an LLM has two facets: on the one hand, pre-trained LLMs possess knowledge that allows remarkable performance in zero-shot tasks; on the other hand, pre-trained LLMs can learn new knowledge, either by leveraging in-context learning or by instruction-tuning from base parameters as initialization. While most evaluation focuses on measuring what the model knows already, we focus on evaluating an LLM\u2019s mathematical understanding by studying how they learn: Has the model learnt to learn effectively about mathematics? Can it make good use of relevant information present during learning? In this sense, our interest is in evaluating LLMs from a learning-to-learn, or meta-learning perspective.\nHumans successfully acquire mathematical and reasoning skills when they are able to identify underlying structure in problems rather than paying attention to spurious signals present in the question formulation \u2014 a phenomenon often coined as deep versus surface learning in education science (Chin & Brown, 2000  ###reference_b11###). This suggests that an analogous way to evaluate a machine learning model\u2019s understanding of mathematics is to ask whether it is able to exploit similarities in deep structure as it generalizes from a training example to a test situation. In gradient-based learning, the extent to which information about one input is generalized to another is captured by the object called Neural Tangent Kernel (NTK) (Jacot et al., 2018  ###reference_b17###). Although the NTK was mainly introduced as a theoretical tool, it has also found uses in interpretability research (Engel & Wang, 2023  ###reference_b14###).\nThis paper proposes a NTK-inspired method to evaluate LLM\u2019s change in probability distribution during training and investigates: do LLMs learn to answer math problems based on an understanding of the skill (deep structure) required to solve the problem or by gathering clues from surface changes in presentation formats? For example, to solve a subtraction problem, LLMs\u2019 performance increase may be due to noticing the train and test question shares the same symbolic format (, what is ?) instead of eliciting the knowledge of subtraction for problem-solving. Figure 1  ###reference_### shows more examples of different presentation formats considered.\nWe assess the impact on LLM\u2019s accuracy in answering math problems when LLMs see different groups of examples that relate to the test question: one group shares the same deep structure described by core math skill, and the other group shares the same presentation format. This is based on the intuition that if LLMs learn beyond pattern matching, then seeing deep structures should induce larger relative improvement than seeing surface structures. Subsequently, we analyze, for a fixed presentation format, how seeing different math skills affects performance on targeted and different test examples. For instance, we measure accuracy improvement (or decline) in how the LLM sees addition examples affect solving addition problems versus subtraction problems, to determine LLM\u2019s ability in fast adaptation.\nIn-context learning (Brown et al., 2020  ###reference_b7###) and instruction-tuning (Zhou et al., 2023  ###reference_b35###) have elicited emergent abilities of LLMs: from improvements in reasoning (Wei et al., 2022a  ###reference_b31###, b  ###reference_b32###) to generalization beyond training dataset (Wei et al., 2021  ###reference_b30###). In this paper, we analyze from the two perspectives and our contributions and findings are summarized below:\nWe propose NTKEval in Section 4  ###reference_###, extending NTK to language models when outputs are chat completions, and demonstrate the sample efficiency of NTKEval compared to standard metric in counting accuracy differences in Section 6.1  ###reference_###.\nWe introduce the KhanSkill dataset (Section 5  ###reference_###) consisting of human-annotated mathematical concepts that can help analyze the alignment of LLMs\u2019 mathematical understanding with human learning.\nOur systematic analysis in Section 6.2  ###reference_### and 6.3  ###reference_### finds that in-context learning differentiates deep versus surface structures (Table 3  ###reference_###) and learned to effectively use relevant math skills (Figure 5  ###reference_### Top), whereas instruction-tuning on skill-focused dataset leads to similar performance change irrespective of training on different data types (Table 4  ###reference_### and Figure 5  ###reference_### Bottom) \u2014 suggesting the adaptation is on format matching rather than domain understanding."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "LLMs for Math problem solving Current breakthroughs, e.g., OPRO (Yang et al., 2023  ###reference_b34###), AlphaGeometry (Trinh et al., 2024  ###reference_b29###), FunSearch (Romera-Paredes et al., 2024  ###reference_b23###), in AI for mathematics were driven by the idea of self-play where LLM acts as a creative assistant to provide useful hints. The advantages brought by language model relies more on its general understanding across domain concepts rather than its ability to precisely execute manual tasks \u2014 which is often outsourced to reliable external tools (Reed et al., 2022  ###reference_b22###; Schick et al., 2023  ###reference_b25###). This motivates our study to assess the domain understanding of LLMs through their ability to adapt when seeing different math skills.\nNeural Tangent Kernel (Jacot et al., 2018  ###reference_b17###) is central to understanding the generalization properties of ANNs \u2014 it shows with infinite width network, the kernel is deterministic with respect to model architecture rather than parameter initialization and stays constant during training (Weng, 2022  ###reference_b33###). Much of the existing literature (Bietti & Mairal, 2019  ###reference_b6###; Alemohammad et al., 2020  ###reference_b2###; Chen et al., 2020  ###reference_b10###) focuses on analyzing the theoretical properties of NTK in various architectures, with little connections to language models. A more related work (Malladi et al., 2023  ###reference_b21###) studies whether NTK describes the fine-tuning process of language models where they formulate downstream tasks as masked word prediction problems through prompting. In this paper, we instead propose a sample-efficient method that allows outputs to be free-form completions. We utilize the proposed method to study how effectively models can learn through training on relevant data.\nSkill Arora & Goyal (2023  ###reference_b4###) studies skill emergence in language models from a statistical framework. Chen et al. (2023b  ###reference_b9###) selects training data based on skill ordering. Chen et al. (2023a  ###reference_b8###) introduces SkiC prompting to encourage skill compositions."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Language Model",
            "text": "A language model (LM) is a statistical model of natural language. Given a sentence  with tokens , the probability of the sentence in a LM parameterized by  can be represented as a chain of conditional probability conditioned on all the previous tokens:\nPre-training a language model amounts to likelihood maximization for the probability of next token prediction  on a held-out dataset.\nIn-context learning LM is given a -shot example of context at inference time before the context of the test question, which the LM is expected to complete.\nInstruction-tuning Given a dataset of questions and ground truth answers  (such dataset is what this paper focuses to analyze) and denoting the tokens in -th question as  and in the corresponding answer as . Instruction-tuning or supervised fine-tuning a LM  on the dataset means minimizing the loss function"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Neural Tangent Kernel",
            "text": "Given a neural network  with parameter , for an input pair , NTK quantifies the changes in  at point  when updating an infinitesimal gradient step in the direction of training on data point . Mathematically, the kernel can be written in two equivalent forms, where  is the learning rate:\nIn this work, we inherit Eq. (2  ###reference_###) for kernel calculation since language models usually have parameters in the scale of (tens of) billions, and matrix multiplications with billion entries are computationally expensive."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "This paper proposes a NTK-inspired method NTKEval to evaluate changes in probability when training on a skill-focused dataset. We expect changes in probability would be faster in capturing training effects that are otherwise computationally expensive to reflect in accuracy changes. We focus on analyzing math problems with deterministic solutions. Each data point consists of a triplet of skill, question, and answer, where skill describes a feature of the question and the answer may contain CoT reasoning (Wei et al., 2022b  ###reference_b32###) where an exact solution is extractable. Let  represent batches of data points corresponding to skill,  and  respectively. Here a skill can either be a math skill (deep structure) or a type of presentation format (surface structure). Our goal is to learn the language model\u2019s NTK in terms of skills, .\nComputing neural tangent kernel in language models faces three challenges:\nNTK is designed to tackle regression tasks, where  is a numerical value. Language models instead generate free-form completions based on a prompt. To address the lack of target numerical outcome, we use the probability of generating the correct solution given prompt, i.e.,  as the target value.\nTo compute , we marginalize the CoT reasoning effect. Formally, given a prompt , the model generates -th completion , where  is the CoT reasoning and  is the deterministic solution. Let the ground truth solution be . Then,\nwhere  is the number of generations given prompt .\nHowever, language models are known to have high variability in free-form generations when sampling from the complete probability distribution. Therefore, sampling multiple generations in each model and marginalizing out chain-of-thought reasoning, though theoretically correct, in practice will be computationally expensive as the number of generations required is huge. Here we take the importance sampling approach: use the generations in the base model and calculate the counterfactual probability of the new model generating the same completion. This allows fair comparisons of the same inputs between different models and requires fewer computation resources. Formally, let model  be the base model and model  be the instruction-tuned model on a data batch that shares the same skill, with probability distributions  respectively. The objective is to calculate the difference in probability of generating correct solutions between model  and  given fixed prompt , i.e., . Importance sampling based score leads to\nwhere . Note we adapt the indicator function slightly to take into account the changes when model outputs wrong answers. Algorithm 1  ###reference_### details the exact procedure."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Datasets",
            "text": "Synthetic dataset To allow control in isolating changes on variables of interest, this paper focuses the analysis on synthetic datasets. Each question tests one math skill. We first create one dataset consisting of questions testing four elementary math skills (\u2018deep structure\u2019) and presented with four types of formats (\u2018surface structure\u2019). The four elementary skills are addition, subtraction, multiplication, and division and the four types of presentation formats are question, instruction, symbolic, and word problems. Figure 1  ###reference_### (right) shows example formats for a given math problem.\nTo understand whether LMs are effective in learning to learn through seeing different math skills, we fix the presentation format to style \u2018question\u2019 and create the second dataset with questions that require understanding from elementary to more complex skills. In particular, we add questions testing understanding on the order of operations, and the utilization of the mixture of all aforementioned skills (\u2018complex\u2019). Figure 1  ###reference_### (left) shows example questions with abbreviated skill names. Finally, we include a baseline that outputs random integers irrespective of the question presented.\nKhanSkill dataset\nDue to a lack of skill-annotated benchmarks, we generate expert-written questions for educational purposes from khan-exercises111https://github.com/Khan/khan-exercises that reflect human understanding. The dataset consists of  skills with  questions per skill. The training set consists of  questions and the test set consists of  questions evenly split across skills. Appendix A  ###reference_### details sample skills and questions contained in the dataset.\n###figure_1###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Model choices\nWe evaluate our experiments on Code Llama 7b (Roziere et al., 2023  ###reference_b24###), Llemma 7b (Azerbayev et al., 2023  ###reference_b5###) and either Mistral 7b (Jiang et al., 2023  ###reference_b18###) or Mixtral 8x7b Instruct (Jiang et al., 2024  ###reference_b19###). The choice of open-sourced models allows both inference and/or instruction-tuning done on a single GPU. We include a suite of LLMs tailored for code, mathematics, and SOTA general-purpose chat model in order to test specialized models\u2019 domain understanding.\nDataset Evaluation Table 1  ###reference_### records the accuracy for different LLMs evaluated on synthetic dataset from elementary to complex skills. We observe that the more complex skills (e.g. OPS and CPLEX) required to solve the problem the lower the accuracy across all models. Table 2  ###reference_### reports the accuracy for different LLMs evaluated on KhanSkill dataset. We compare against standard benchmarks GSM8K (Cobbe et al., 2021  ###reference_b13###), MATH (Hendrycks et al., 2021  ###reference_b16###) to assess its difficulty level. We observe KhanSkill is easier than MATH and harder than GSM8K. In particular, Llemma-7b outperforms Mistral-7b on all three datasets in Table 2  ###reference_###.\n###figure_2###"
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Can LLM understand different math skills?",
            "text": "We assess LLMs\u2019 learning-to-learn ability when seeing relevant examples grouped by skills affecting utilizing the target skill and different skills in test time. Setup For each elementary to complex skill, we prompt LLMs with 8 in-context examples grouped by skill or instruction-tune the LLM with skill-specified data and evaluate the model on the overall test dataset stratified by skills. Figure 3 shows ICL matrix that records changes in accuracy compared to standard prompting when in-context examples are grouped as column-specified skills and evaluated on row skills. We observe all models display a positive diagonal line for the majority of skills. This suggests giving relevant examples during inference time, LLMs in general are fast in learning, with performance improvement on the corresponding examples during test time. As expected for LLM tailored for mathematics, Llemma-7b exhibits the most clear positive diagonal line, suggesting the clear differentiation from the targeted mathematical skill with the other relevant but misleading skills."
        },
        {
            "section_id": "6.3.1",
            "parent_section_id": "6.3",
            "section_name": "6.3.1 Can LLM distinguish targeted skill from off-diagonal skills?",
            "text": "To distinguish whether the ability for fast adaptation (i.e., improvement) observed in Figure 3 and 4 is not due to shared factors (e.g., formatting), we compare the scale of improvement when prompting / instruction-tuning between targeted and off-diagonal skills. Targeted skill refers to the skill that is the same as the test question. Off-diagonal skills refer to skills related to but distinct from the test questions. For example, if the target question requires skill \u2018addition\u2019, then off-diagonal skills include but are not limited to \u2018subtraction\u2019.\n\nFigure 5 (Top) shows average changes in accuracy when performing targeted skill prompting (top left) and off-diagonal skill prompting (top right) compared to the standard prompting on both synthetic and KhanSkill dataset. Figure 5 (Bottom) shows average changes in the probability of generating correct solutions when instruction-tuning on targeted skill (bottom left) and off-diagonal skills (bottom right) compared to the base model on a synthetic dataset. The x-axis shows skill difficulty levels measured by accuracy under 8-shot random in-context examples for each corresponding model.\n\nObservations Figure 5 (Top) shows clear advantages in relative accuracy improvement, whereas (Bottom) shows similar performance changes between seeing targeted skill (Left) versus off-diagonal skills (Right). This suggests the qualitative improvements observed in in-context learning (Figure 3) are not due to irrelevant factors but stem from the differentiation between different mathematical skills. By contrast, instruction-tuning does not demonstrate relative advantages when trained on the targeted skill versus off-diagonal skills. This suggests the qualitative performance improvement (Figure 4) is driven not by differentiation on relevant skills but rather by common features shared across all tested skills (e.g., presentation format)."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We proposed a method NTKEval to extend computations of neural tangent kernel beyond regression tasks to language model\u2019s outputs as free-form generations (Section 4  ###reference_###). Figure 2  ###reference_### demonstrates the sample-efficiency of NTKEval across all models. We next utilize NTKEval and investigate the capability of LLMs to answer math problems beyond pattern matching through query after in-context learning and instruction-tuning. Experiment results show in-context learning differentiates deep structures from surface structures (Table 3  ###reference_###), by contrast instruction-tuning does not (Table 4  ###reference_###). Both ICL and IT are capable of learning to learn (Fig. 3  ###reference_### and 4  ###reference_###), but in-context learning identifies targeted math skills from the others (Figure 5  ###reference_### Top), whereas instruction-tuning does not (Figure 5  ###reference_### Bottom). Overall, we find that ICL exhibits domain understanding, whereas certain instruction-tuning leads to similar performance change irrespective of training on different data.\nWe only considered QA data rather than open-ended texts. We investigated datasets grouped by one skill rather than a mixture of diverse data. Through investigating whether and what method elicits LLM\u2019s domain understanding, we hope to help design better and more transparent scientific assistants."
        }
    ],
    "url": "http://arxiv.org/html/2405.15485v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4"
        ],
        "main_experiment_and_results_sections": [
            "6",
            "6.1",
            "6.2",
            "6.3",
            "6.3.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.1",
            "6.2",
            "6.3",
            "6.3.1"
        ]
    },
    "research_context": {
        "paper_id": "2405.15485v1",
        "paper_title": "Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs",
        "research_background": "### Motivation\n\nThe paper is driven by the increasing application of Large Language Models (LLMs) as creative assistants in scientific fields such as mathematics and biology. Despite the promising capabilities of LLMs in various natural language inference tasks, there is a need to assess their domain-specific knowledge, particularly in understanding and solving mathematical problems. This evaluation isn't merely about whether the LLMs can solve problems correctly but also about how they learn and adapt to new information.\n\n### Research Problem\n\nThe core research problem addressed by this paper is determining whether LLMs understand mathematical concepts deeply and can adapt to new problems effectively. Specifically, it investigates whether LLMs are capable of learning to recognize the deep structures of mathematical problems rather than relying on superficial cues and patterns. The paper seeks to measure this by evaluating LLMs' performance on mathematical tasks in terms of both accuracy and adaptability based on different types of training examples.\n\n### Relevant Prior Work\n\n1. **LLMs in Natural Language Inference**: The success of LLMs in various natural language tasks has been well-documented (Achiam et al., 2023; Touvron et al., 2023; Chowdhery et al., 2023; Anil et al., 2023; Team et al., 2023).\n\n2. **Scientific Discovery Assistance**: There is a growing trend of employing LLMs as assistants in scientific discovery across fields like mathematics and biology (Trinh et al., 2024; Madani et al., 2023).\n\n3. **Human Learning**: The paper draws on the concept of deep versus surface learning from education science, where identifying the underlying structure of problems is crucial (Chin & Brown, 2000).\n\n4. **Neural Tangent Kernel (NTK)**: The NTK has been used to understand how information about one input generalizes to another in gradient-based learning (Jacot et al., 2018). It's also utilized in interpretability research (Engel & Wang, 2023).\n\n5. **Emergent Abilities in LLMs**: Previous work has indicated that techniques like in-context learning and instruction-tuning can elicit advanced capabilities in LLMs, such as improvements in reasoning and generalization (Brown et al., 2020; Wei et al., 2021, 2022).\n\nThis paper builds on these foundations to propose a novel method (NTKEval) for evaluating LLMs' mathematical understanding, introduces a new dataset (KhanSkill), and conducts systematic analyses to differentiate deep learning from surface learning in LLMs.",
        "methodology": "The paper introduces \"NTKEval,\" a methodology inspired by Neural Tangent Kernels (NTK) to evaluate how quickly training impacts a language model (LM) using skills-focused datasets, particularly in solving mathematical problems. The primary innovation is the focus on changes in probability as a more sensitive measure than accuracy to capture training effects. Here\u2019s a breakdown of the proposed method:\n\n### Key Components and Innovations:\n\n1. **Dataset and Data Points**:\n   - The data includes triplets of skill, question, and answer.\n   - Each triplet ties a specific skill (mathematical concept or presentation format) to a question and its deterministic answer. \n   - Answers may include Chain of Thought (CoT) reasoning, where an exact solution is ultimately extractable.\n\n2. **Objective**:\n   - The aim is to learn the LM's Neural Tangent Kernel (NTK) in the context of specific skills.\n   - This involves evaluating how probability distributions over outcomes change as the model is trained.\n\n3. **Challenges in Computing NTK for Language Models**:\n   - Traditional NTK is used for regression tasks with numerical values, but LMs generate free-form text completions.\n   - The solution is to use the probability of generating the correct solution given a prompt as the \"numerical\" target value.\n\n4. **Marginalizing CoT Reasoning**:\n   - For a given prompt, the model generates several completions incorporating CoT reasoning.\n   - The process integrates the variability of CoT outputs to compute the target probabilities more accurately.\n   - However, obtaining multiple generations to marginalize CoT reasoning is computationally expensive.\n\n5. **Importance Sampling**:\n   - To address the computational challenges, the method uses importance sampling. \n   - Instead of generating multiple completions for every new model, it uses completions generated by a base model.\n   - Importance sampling allows for comparing the base model\u2019s output with the instruction-tuned model\u2019s output given the same prompts.\n   - Formally, for base model \\( M_0 \\) and instruction-tuned model \\( M_1 \\) with respective probability distributions \\( P_0 \\) and \\( P_1 \\), the objective is to find the difference in probability of generating correct solutions given the same prompt.\n\n6. **Calculation of Importance Sampling Score**:\n   - The score is calculated by comparing probabilities \\( P_1 \\) and \\( P_0 \\) of generating the correct solutions:\n     \\[\n     \\hat{\\mathbb{E}}_{x \\sim P_0} \\left[ \\frac{P_1(x)}{P_0(x)} I\\{\\text{correct output}\\} \\right]\n     \\]\n   - Here, \\( I \\{\\text{correct output}\\} \\) is an adapted indicator function considering model outputs, facilitating the comparison even when models give incorrect answers.\n\n7. **Algorithm Implementation**:\n   - An outlined algorithm (denoted as Algorithm 1) details the step-by-step procedure for implementing the NTKEval method.\n   - This enables a systematic and fair comparison of model performance with limited computational resources.\n\n### Conclusion:\n\nNTKEval innovatively applies NTK concepts to language models to assess the training effects on skills-focused datasets, particularly in mathematical problem-solving. By using probability changes over accuracy and employing importance sampling, NTKEval balances theoretical robustness with practical computational efficiency.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Model Choices:**\nThe experiments are conducted on several models, including:\n- **Code Llama 7b**\n- **Llemma 7b**\n- **Mistral 7b** or **Mixtral 8x7b Instruct**\n\nThese models are chosen due to their open-source nature, allowing both inference and instruction-tuning to be performed on a single GPU. The selection aims to compare LLMs tailored for specific domains such as code and mathematics, against state-of-the-art general-purpose chat models to evaluate their domain understanding.\n\n**Datasets:**\n- **Synthetic Dataset:** This dataset ranges from elementary to complex skills, with specific tasks such as OPS (Operational Problem Solving) and CPLEX (Complex Problem Solving), to evaluate the models' proficiency.\n- **KhanSkill Dataset:** This dataset is used to provide a more nuanced evaluation of the models' capabilities. \n- **Benchmark Datasets for Comparison:**\n  - **GSM8K:** Standard benchmark for simpler mathematical problems.\n  - **MATH:** Known for its challenging set of mathematical problems.\n\n**Evaluation Metrics:**\n- **Accuracy:** The primary metric used to assess the performance of each model on the different datasets.\n\n### Main Experimental Results\n\n**Results on Synthetic Dataset:**\n- The accuracy recorded shows that as the complexity of the skills required to solve problems increases (such as in OPS and CPLEX), the accuracy across all models decreases.\n\n**Results on KhanSkill Dataset:**\n- The accuracy of different models on the KhanSkill dataset is compared with benchmarks GSM8K and MATH to assess difficulty levels. The findings indicate that:\n  - KhanSkill is easier than MATH but harder than GSM8K.\n  - Llemma-7b outperforms Mistral-7b across all three datasets (KhanSkill, GSM8K, and MATH).\n\nThese results highlight that Llemma-7b has a stronger performance in mathematical understanding compared to the other models evaluated."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the sample efficiency of the NTKEval method compared to standard accuracy metrics in capturing changes in language model probability distributions when trained on skill-focused math datasets.",
            "experiment_process": "We take the first 1,000 data points in a synthetic dataset consisting of complete rounds of different skills. For each model, we calculate converged accuracy differences using 100 generations per question at temperature 1, and we compute both NTKEval and accuracy differences on the same dataset using 10 generations at temperature 1. Experiment setups include in-context learning with standard and skill prompting (8-shot examples) and instruction tuning of base and skill-tuned models. Comparisons were made between skill prompting and standard prompting as well as between skill-tuned and base models in terms of accuracy differences and NTKEval measurements.",
            "result_discussion": "The NTKEval method with only 10 generations can proportionally reflect the converged accuracy difference, making it a more sample-efficient method compared to the standard accuracy counting metrics. This holds true for all models considered.",
            "ablation_id": "2405.15485v1.No1"
        },
        {
            "research_objective": "Determine if LLMs answer math problems based on deep structures (core math skills) or surface-level structures (consistent question formats).",
            "experiment_process": "In-context learning examples and instruction-tuning datasets were selected to have the same category as their respective test questions, encompassing four elementary math skills and four presentation formats. The accuracy differences were reported between structural and standard prompting, averaged across deep and surface structures. Probability differences were also reported between models instruction-tuned on one structure and base models evaluated on test problems of the same structure.",
            "result_discussion": "LLMs exhibit strong performance improvements with deep structure examples compared to surface structures alone. Mistral 7b was noted for its overall performance improvement, though it performed less effectively on deep structures. Llemma-7b showed the best understanding and improvement with deep mathematical structures. Instruction tuning, however, showed a relative decrease in probability improvement for deep versus surface structures, suggesting difficulty in recognizing deep mathematical structures.",
            "ablation_id": "2405.15485v1.No2"
        },
        {
            "research_objective": "Assess LLMs' ability to utilize different mathematical skills when provided with relevant examples grouped by skills during testing.",
            "experiment_process": "LLMs were prompted with 8 in-context examples grouped by skill or instruction-tuned with skill-specific data, then evaluated on an overall test dataset stratified by skills. Performance was recorded in ICL and NTK matrices showing changes in accuracy and probability, respectively, when evaluated on row-specified skills after being trained on column-specified skills.",
            "result_discussion": "LLMs display a positive diagonal line in both ICL and NTK matrices for the majority of skills, indicating fast adaptation and performance improvements when given relevant examples. Llemma-7b showed clear differentiation and the most significant performance improvements.",
            "ablation_id": "2405.15485v1.No3"
        },
        {
            "research_objective": "Investigate if the performance improvements from targeted skill adaptation are due to skill differentiation or shared factors like formatting.",
            "experiment_process": "Comparisons were made between prompting/instruction-tuning on targeted skills (same as test questions) and off-diagonal skills (related but distinct from test questions). Changes in accuracy and probability were measured for both targeted and off-diagonal skill prompting/instruction-tuning across skill difficulty levels.",
            "result_discussion": "Targeted skill prompting shows clear relative accuracy improvement, indicating qualitative improvements stem from skill differentiation. However, instruction-tuning does not demonstrate a relative advantage for targeted skills, suggesting performance improvements may be driven by shared common features rather than skill differentiation.",
            "ablation_id": "2405.15485v1.No4"
        }
    ]
}