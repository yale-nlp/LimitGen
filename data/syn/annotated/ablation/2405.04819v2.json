{
    "title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer\u2019s Disease Questions with Scientific Literature",
    "abstract": "Recent advancements in large language models (LLMs) have achieved promising performances across various applications.\nNonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains.\nIn this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer\u2019s Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities.\nThe experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK.\nAdditionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. We will release the code and data at https://github.com/David-Li0406/DALK.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Alzheimer\u2019s Disease (AD) is a neurodegenerative disorder characterized by progressive declines in cognitive and functional status over a span of decades Report (2023  ###reference_b49###).\nHowever, current AD therapy developments are facing critical challenges due to the lack of knowledge and understanding of the underlying etiological mechanisms of the disease. Although scientific literature and dedicated biomedical databases could supply rich sources of AD knowledge, manual review of relevant information is impossible due to the large volume.\nAs large language models (LLMs) (Brown et al., 2020  ###reference_b6###; Zhang et al., 2022  ###reference_b79###; Anil et al., 2023  ###reference_b2###; Touvron et al., 2023  ###reference_b60###) with chain-of-thought (CoT)-based prompting Wei et al. (2022  ###reference_b66###); Wang et al. (2022  ###reference_b64###); Tong et al. (2023  ###reference_b58###); Yao et al. (2023  ###reference_b71###); Besta et al. (2023  ###reference_b4###) demonstrate strong language capabilities across various tasks, there have been attempts to leverage LLMs-based systems in general biomedical and AD-related applications Mao et al. (2023  ###reference_b35###); Li et al. (2023c  ###reference_b30###); Yan et al. (2024  ###reference_b70###); Feng et al. (2023  ###reference_b14###).\nHowever, while the LLMs have shown promising performances in many general tasks, recent studies revealed LLMs\u2019 limitations in long-tail Kandpal et al. (2023  ###reference_b23###) and domain-specific Li et al. (2023b  ###reference_b29###, 2024  ###reference_b28###) knowledge, thereby significantly impeding their adaptations in vertical fields such as AD.\nTo deal with this issue, the most common strategies are retrieval augmented generation (RAG) and domain-specific LLMs training.\nNevertheless, directly applying these strategies in the context like AD would still suffer from several issues.\nFirst, Data Quality: As in many biomedical fields, scientific literature composes the largest publicly available corpus source in AD.\nYet, the dense and information-overloaded nature of scientific literature, when combined with automatic retrieval methods, can lead to the retrieval of irrelevant and noisy information.\nPrevious research has shown that noisy and irrelevant corpora can significantly undermine the performance of LLMs Yu et al. (2023  ###reference_b72###); Chen et al. (2024  ###reference_b8###); Wu et al. (2024  ###reference_b69###).\nSecond, Efficiency & Scale Issues: Being an critical field of research, the knowledge of AD is rapidly evolving with scientific advancements at a remarkable pace and scale.\nHowever, retraining a domain-specific LLM or updating certain knowledge in it demands substantial computational resources Hu et al. (2021  ###reference_b19###); Ovadia et al. (2023  ###reference_b41###); Zhang et al. (2024  ###reference_b76###).\nThis efficiency issue would also limit the sizes of domain-specific LLMs, consequently affecting their performances.\nTo tackle these limitations, here we propose a Dynamic Co-Augmentation of LLMs and KG (DALK) framework that facilitates mutual benefits between LLMs and knowledge graphs (KG) for the AD domain.\nInitially, our framework addresses the data quality challenge by extracting more structural and accurate knowledge from unstructured and dense scientific literature and constructing a domain-specific knowledge graph tailored to AD.\nWe employ two widely utilized knowledge graph construction methods, namely pair-wise construction Carta et al. (2023  ###reference_b7###); Wadhwa et al. (2023  ###reference_b62###) and generative construction Han et al. (2023  ###reference_b15###); Bi et al. (2024  ###reference_b5###), to comprehensively assess their impact on knowledge graph quality.\nThen, we adopt a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the knowledge graph and thus further address the data quality problem.\nNotably, the tuning-free nature of our framework significantly enhances efficiency and facilitates its application in large-scale and API-based language models OpenAI (2022  ###reference_b39###).\nIn the evaluation section, we derive an Alzheimer\u2019s Disease question answering (ADQA) benchmark from existing general medical QA datasets with millions of samples filtered by a curated keyword list and self-sampling of LLMs.\nOur extensive experiment on ADQA demonstrates the effectiveness of our framework in domain-specific applications compared with general biomedical LLMs and retrieval augmented models.\nFurther evaluation and analysis provide valuable insights into constructing high-quality knowledge graphs and sampling accurate knowledge from them.\nIn summary, our contribution in this work can be summarized as follows:\nWe identify the constraints of the current methods for LLMs in domain-specific areas like AD and introduce DALK, a co-augmentation framework of the LLM and KG to address these issues.\nWe build AD-specific KG and QA benchmark. Through extensive comparisons with other methods, we showcase the effectiveness of DALK.\nWe delve into a comprehensive analysis of our proposed method and provide valuable insights and guidance on how to construct a high-quality KG and sample accurate knowledge from it."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "KGs Miller (1995  ###reference_b37###); Speer et al. (2017  ###reference_b52###); Vrande\u010di\u0107 and Kr\u00f6tzsch (2014  ###reference_b61###) serve as structured representations of factual knowledge, typically expressed as (head, relation, tail) triples.\nTheir structured, factual, and interpretable nature renders them excellent complements to parametric language models Pan et al. (2024  ###reference_b44###).\nRecently, with the rise of large language models (LLMs), numerous studies have delved into exploring the synergy between LLMs and KGs for various purposes Pan et al. (2024  ###reference_b44###); Tan et al. (2024  ###reference_b55###).\nThere are a lot of efforts in conducting knowledge graph construction Carta et al. (2023  ###reference_b7###); Wadhwa et al. (2023  ###reference_b62###); Han et al. (2023  ###reference_b15###); Bi et al. (2024  ###reference_b5###); Datta et al. (2024  ###reference_b11###), completion Wei et al. (2023  ###reference_b67###); Zhang et al. (2023b  ###reference_b80###); Li et al. (2024  ###reference_b28###) with the aid of LLMs.\nConversely, other works aim to enhance LLMs by integrating knowledge sampled from KGs during both training Tang et al. (2023  ###reference_b56###); Luo et al. (2024  ###reference_b33###); Dernbach et al. (2024  ###reference_b12###); Rangel et al. (2024  ###reference_b47###) and inference Kim et al. (2023  ###reference_b24###); Wen et al. (2023  ###reference_b68###); Jiang et al. (2023  ###reference_b20###); Sun et al. (2023a  ###reference_b53###) times.\nOur work distinguishes itself by proposing a co-augmentation framework for LLMs and KGs, facilitating their mutual enhancement, and applying it to the domain of AD.\nLLMs and KGs have both been applied to Alzheimer\u2019s Disease research in previous studies.\nPre-trained language models are utilized to work on AD detection and many other related tasks based on speech recordings and transcripts Balagopalan et al. (2020  ###reference_b3###); Agbavor and Liang (2022  ###reference_b1###), electronic health records (EHRs) Mao et al. (2023  ###reference_b35###); Li et al. (2023c  ###reference_b30###); Yan et al. (2024  ###reference_b70###), and tabular data Feng et al. (2023  ###reference_b14###). KGs have been widely used in biomedical research, yet only a few are specifically for AD research Romano et al. (2023  ###reference_b50###); Pu et al. (2023  ###reference_b46###); Hsieh et al. (2023  ###reference_b18###); Nian et al. (2022  ###reference_b38###); Daluwatumulle et al. (2023  ###reference_b10###). These KGs were generally constructed from a variety of information derived from heterogeneous biomedical databases (e.g. for genes, drugs, pathways, etc.) or scientific literature related to AD. Despite the aforementioned efforts for LLMs and KGs in AD research, no prior study has explored using LLM to augment AD-KG, or vice versa, let alone the potential for mutual enhancement between the two as we propose here."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Our Methodology",
            "text": "###figure_1### This section elaborates on our dynamic co-augmentation framework of LLMs and KG. Section 3.1  ###reference_### presents the details of augmenting an AD-specific evolving KG with LLMs and literature corpus in a time-slicing fashion (i.e. year by year). Following it, Section 3.2  ###reference_### describes the process of sampling appropriate knowledge from the evolving KG to enhance LLMs\u2019 reasoning. Figure 1  ###reference_### illustrates an overall pipeline of our method DALK.\nTo create an AD-specific knowledge graph, we follow Pu et al. (2023  ###reference_b46###) and use the AD corpus collected by a domain expert Professor Colin Masters at the University of Melbourne who discovered amyloid proteins being the potential cause of AD Masters et al. (1985  ###reference_b36###). The corpus is based on his extensive bibliography of representative AD-related papers and consists of more than 16K PMID (PubMed ID)-indexed articles from 1977 to 2021. For our study, we focus on the papers since 2011 which reflect the most recent knowledge in the field and get 9,764 articles.\nIn order to identify knowledge at the proper granularity level for AD, we extract relevant entities from the corpus by utilizing the PubTator Central (PTC) Wei et al. (2013  ###reference_b65###) developed and continuously maintained by NCBI. PTC is a widely-used tool to provide state-of-the-art annotations of biomedical concepts for PubMed abstracts and full-text articles, and it supports six bioconcept types including genes, diseases, chemicals, mutations, species and cell lines. We apply PTC to the abstracts of all our AD papers and obtain the relevant named entities which will serve as nodes in the knowledge graph.\nTo build an accurate and high-quality knowledge graph on AD, we aim to assign a specific relation type between the two related entities. Through a comprehensive survey of relation extraction methods for knowledge graph construction, we categorize current approaches with LLMs into two main groups: (a). Pair-wised Relation Extraction Carta et al. (2023  ###reference_b7###); Wadhwa et al. (2023  ###reference_b62###) aims to prompt the LLMs to describe the relationship between any two entities in a segment of text. (b). Generative Relation Extraction Han et al. (2023  ###reference_b15###); Bi et al. (2024  ###reference_b5###); Datta et al. (2024  ###reference_b11###), where LLMs directly output all related entity pairs and their corresponding relationships. As shown in Figure 2  ###reference_###, we incorporate both of these relation extraction methods into our knowledge graph augmentation process to provide a comprehensive comparison between them. We denote the resulting knowledge graphs from these approaches as  and  respectively.\nTable 1  ###reference_### presents the detailed statistics about our augmented knowledge graph, including the number of corpora we used, and the number of nodes, relations and triples in  and .\n###figure_2### entails the extraction of a sub-graph from  to encompass all entities within . The process unfolds as follows: (a) Begin by selecting one node from  as the initial node, denoted as , and place the remaining nodes into a candidate node set, . Explore at most  hops from  to identify the subsequent node, , where . If  is successfully reached within  hops, update the start node to  and remove  from . In the event  cannot be found within  hops, concatenate the segment paths acquired thus far and store them in . Subsequently, choose another node  from  as the new start node, and eliminate both the original start node  and the current node  from . (b) Verify if  is empty. If not, repeat step (a) to identify the next segment of the path. If  is empty, combine all segments to construct a set of sub-graphs and place them into .\nendeavors to augment the evidence relevant to the query within . This process consists of two steps: (a) Initially, expand each node  within  by 1-hop to incorporate their neighbors , thus appending triples  to . (b) Then assess whether each  exhibits semantic relevance to the query. If affirmative, further expand the 1-hop neighbors of , consequently adding triples  to .\nAfter obtaining the two sub-graphs  and , we perform post-processing to further prune redundant information in sub-graphs and prompt LLMs to describe the structure of each sub-graph."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "LLMs for KG",
            "text": "To create an AD-specific knowledge graph, we follow Pu et al. (2023  ###reference_b46###  ###reference_b46###) and use the AD corpus collected by a domain expert Professor Colin Masters at the University of Melbourne who discovered amyloid proteins being the potential cause of AD Masters et al. (1985  ###reference_b36###  ###reference_b36###). The corpus is based on his extensive bibliography of representative AD-related papers and consists of more than 16K PMID (PubMed ID)-indexed articles from 1977 to 2021. For our study, we focus on the papers since 2011 which reflect the most recent knowledge in the field and get 9,764 articles.\nIn order to identify knowledge at the proper granularity level for AD, we extract relevant entities from the corpus by utilizing the PubTator Central (PTC) Wei et al. (2013  ###reference_b65###  ###reference_b65###) developed and continuously maintained by NCBI. PTC is a widely-used tool to provide state-of-the-art annotations of biomedical concepts for PubMed abstracts and full-text articles, and it supports six bioconcept types including genes, diseases, chemicals, mutations, species and cell lines. We apply PTC to the abstracts of all our AD papers and obtain the relevant named entities which will serve as nodes in the knowledge graph.\nTo build an accurate and high-quality knowledge graph on AD, we aim to assign a specific relation type between the two related entities. Through a comprehensive survey of relation extraction methods for knowledge graph construction, we categorize current approaches with LLMs into two main groups: (a). Pair-wised Relation Extraction Carta et al. (2023  ###reference_b7###  ###reference_b7###); Wadhwa et al. (2023  ###reference_b62###  ###reference_b62###) aims to prompt the LLMs to describe the relationship between any two entities in a segment of text. (b). Generative Relation Extraction Han et al. (2023  ###reference_b15###  ###reference_b15###); Bi et al. (2024  ###reference_b5###  ###reference_b5###); Datta et al. (2024  ###reference_b11###  ###reference_b11###), where LLMs directly output all related entity pairs and their corresponding relationships. As shown in Figure 2  ###reference_###  ###reference_###, we incorporate both of these relation extraction methods into our knowledge graph augmentation process to provide a comprehensive comparison between them. We denote the resulting knowledge graphs from these approaches as  and  respectively.\nTable 1  ###reference_###  ###reference_### presents the detailed statistics about our augmented knowledge graph, including the number of corpora we used, and the number of nodes, relations and triples in  and .\n###figure_3###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "KG for LLMs",
            "text": "In this section, we begin by outlining our process for sampling coarse-grained augmented knowledge from our evolving knowledge graph (Section 3.2.1  ###reference_.SSS1###). Subsequently, we delve into detail regarding our self-aware knowledge retrieval method, which aims to filter out noise and retrieve the most pertinent knowledge to provide to the LLM (Section 3.2.2  ###reference_.SSS2###).\nentails the extraction of a sub-graph from  to encompass all entities within . The process unfolds as follows: (a) Begin by selecting one node from  as the initial node, denoted as , and place the remaining nodes into a candidate node set, . Explore at most  hops from  to identify the subsequent node, , where . If  is successfully reached within  hops, update the start node to  and remove  from . In the event  cannot be found within  hops, concatenate the segment paths acquired thus far and store them in . Subsequently, choose another node  from  as the new start node, and eliminate both the original start node  and the current node  from . (b) Verify if  is empty. If not, repeat step (a) to identify the next segment of the path. If  is empty, combine all segments to construct a set of sub-graphs and place them into .\nendeavors to augment the evidence relevant to the query within . This process consists of two steps: (a) Initially, expand each node  within  by 1-hop to incorporate their neighbors , thus appending triples  to . (b) Then assess whether each  exhibits semantic relevance to the query. If affirmative, further expand the 1-hop neighbors of , consequently adding triples  to .\nAfter obtaining the two sub-graphs  and , we perform post-processing to further prune redundant information in sub-graphs and prompt LLMs to describe the structure of each sub-graph."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Coarse-grained Knowledge Sample",
            "text": "Given a question query , we first construct a prompt and ask LLMs to extract all the domain-specific entities  from it.\nAfterward, we adhere to the methodology proposed by Wen et al. (2023  ###reference_b68###) and execute a similarity-based entity linking process to connect all entities within  to the entity structure in our knowledge graph . Specifically, we employ a semantic similarity model Reimers and Gurevych (2019  ###reference_b48###) to encode all entities in  and  into dense embeddings, denoted as  and , respectively. Subsequently, utilizing cosine similarity, we establish links between each entity in  and its nearest neighbor entity in . This procedure yields an initial entity set  for the subsequent knowledge sampling step.\nTo build an evidence sub-graph to boost LLMs\u2019 reasoning process, we follow the previous study Wen et al. (2023  ###reference_b68###) and consider the following two kinds of explorations in our AD-KG:\nentails the extraction of a sub-graph from  to encompass all entities within . The process unfolds as follows: (a) Begin by selecting one node from  as the initial node, denoted as , and place the remaining nodes into a candidate node set, . Explore at most  hops from  to identify the subsequent node, , where . If  is successfully reached within  hops, update the start node to  and remove  from . In the event  cannot be found within  hops, concatenate the segment paths acquired thus far and store them in . Subsequently, choose another node  from  as the new start node, and eliminate both the original start node  and the current node  from . (b) Verify if  is empty. If not, repeat step (a) to identify the next segment of the path. If  is empty, combine all segments to construct a set of sub-graphs and place them into .\nendeavors to augment the evidence relevant to the query within . This process consists of two steps: (a) Initially, expand each node  within  by 1-hop to incorporate their neighbors , thus appending triples  to . (b) Then assess whether each  exhibits semantic relevance to the query. If affirmative, further expand the 1-hop neighbors of , consequently adding triples  to .\nAfter obtaining the two sub-graphs  and , we perform post-processing to further prune redundant information in sub-graphs and prompt LLMs to describe the structure of each sub-graph."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Self-aware Knowledge Retrieval",
            "text": "In our initial experiment, we noticed the coarse-grained knowledge sampled with the above-mentioned approaches still contained redundant and irrelevant information.\nThis issue of noise is a common challenge encountered in automatically-constructed knowledge graphs Fang et al. (2021  ###reference_b13###); Zhang et al. (2020  ###reference_b77###); Li et al. (2022  ###reference_b26###); Bi et al. (2024  ###reference_b5###). Moreover, many recent works Yu et al. (2023  ###reference_b72###); Li et al. (2023d  ###reference_b31###); Chen et al. (2024  ###reference_b8###); Wu et al. (2024  ###reference_b69###) have demonstrated LLMs can indeed be influenced by such noisy information.\nTo address this challenge, we borrow insights from the recent self-powered LLMs Wang et al. (2022  ###reference_b64###); Pan et al. (2023  ###reference_b43###); Li et al. (2023a  ###reference_b27###); Yuan et al. (2024  ###reference_b73###); Tong et al. (2024  ###reference_b57###) and propose a self-aware knowledge retrieval method to leverage LLMs\u2019 ranking capability Sun et al. (2023b  ###reference_b54###); Ma et al. (2023  ###reference_b34###) to filter out noisy information.\nIn particular, we directly prompt the LLM to rerank the sampled knowledge and only retrieve top  triples to provide for itself in the final-round inference. Given the question  and either the path-based or neighbor-based sub-graph , we create prompt  by filling the pre-defined template:\nThen, we use  as the input to prompt the LLM to obtain the self-retrieved knowledge:\nFinally, we provide the question  and fine-grained knowledge  to the LLM for reasoning and get the predicted answer  in two steps:\nWe provide detailed examples in Appendix A  ###reference_### and B  ###reference_### to demonstrate the input and output in our DALK."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Main Experiment",
            "text": "Both ChatDoctor Yunxiang et al. (2023  ###reference_b74###) and Med-Alpaca Shu et al. (2023  ###reference_b51###) are fine-tuned versions of LLaMA Touvron et al.  ###reference_b59### on biomedical corpora. Compared with them, Meditron Chen et al. (2023  ###reference_b9###) is built on LLaMA-2 Touvron et al. (2023  ###reference_b60###) and extends its pretraining on a comprehensively curated medical corpus. BiomedGPT Zhang et al. (2023a  ###reference_b78###) is also based on LLaMA-2 and pioneer as the first open-source and generalist visual language AI for diverse biomedical tasks. Biomistral Labrak et al. (2024  ###reference_b25###) is an open-source LLM crafted specifically for the biomedical domain, optimized for efficiency through quantization and model merging techniques. Furthermore, we also compare our method with several representative retrieval-augmented LLMs in the biomedical domain. Almanac Zakka et al. (2024  ###reference_b75###) is a novel approach utilizing OpenAI\u2019s GPT model integrated with a Qdrant vector database to hold external sources of knowledge retrieved from local corpus, web search, and calculators, designed to answer open-domain clinical questions. Like Almanac, Lozano et al. (2023  ###reference_b32###) introduced Clinfo.ai, which is an open-source, end-to-end retrieval-augmented LLM (GPT) to answer medical queries using scientific literature summarizations derived from PubMed search engine. We adopt both Almanac and Clinfo.ai with the same prompt as ours to answer multiple-choice questions to suit the ADQA benchmark. Lastly, we implement a simple retrieval-augmented GPT baseline with CoT prompting similar to our proposed DALK. All the GPT models used are set to GPT-3.5-turbo as detailed in the next paragraph, to be consistent. We use GPT-3.5-turbo with the version \u201cgpt-3.5-turbo-0301\u201d and set the sampling temperature to 0.7. We utilize 7B versions of all the biomedical LLMs baselines. For RAG methods, we split each document with a max length of 128 and retrieve the top 3 most relevant documents as the support evidence for LLMs to do inference."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "ADQA Benchmark",
            "text": "For performance evaluation, we consider four widely-used medical QA datasets spanning diverse biomedical topics Jin et al. (2021  ###reference_b21###); Pal et al. (2022  ###reference_b42###); Hendrycks et al. (2021  ###reference_b16###); Pe\u00f1as et al. (2013  ###reference_b45###) and derive an AD-specific QA dataset from them. The four medical QA datasets are all multiple-choice based and include: 1) MedQA Jin et al. (2021  ###reference_b21###) consisting of US Medical Licensing Examination (USMLE)-style questions, 2) MedMCQA Pal et al. (2022  ###reference_b42###) containing medical school entrance exam questions from India, 3) MMLU Hendrycks et al. (2021  ###reference_b16###) consisting of diverse biomedical and clinical questions from various sources, 4) QA4MRE Pe\u00f1as et al. (2013  ###reference_b45###) containing a subset of questions for AD derived from PubMed and Medline. In order to extract from the medical QA datasets a subset of samples related to AD for our evaluation, we referred to NIH\u2019s Common Alzheimer\u2019s and Related Dementias Research Ontology (CADRO) 111https://iadrp.nia.nih.gov/about/cadro. Jointly developed by the National Institute on Aging and the Alzheimer\u2019s Association, CADRO is a three-tiered classification system with eight main categories and a dozen sub-categories for AD and related dementia, and it contains common terminologies or keywords used in the field. We derived from the CADRO a list of AD-related keywords most relevant to the medical QA datasets: <Aging, Alzheimer, Amyloid beta, APOE, Dementia, Lipoprotein, Microglia>. Then, we searched against each medical QA dataset for matches with these keywords to find putative QA samples, then further asked GPT-3.5-turbo to judge for each putative sample whether the question is indeed related to AD or not. Finally, we filtered out a subset of such samples that are considered highly relevant to AD to conduct our evaluation (number of samples in each dataset is shown in Table 2  ###reference_###). More details about ADQA can be found in Appendix C  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experiment Settings",
            "text": "We apply our framework with OpenAI GPT-3.5-turbo models OpenAI (2022  ###reference_b39###). We also include the following baseline methods for comparison: Both ChatDoctor Yunxiang et al. (2023  ###reference_b74###  ###reference_b74###) and Med-Alpaca Shu et al. (2023  ###reference_b51###  ###reference_b51###) are fine-tuned versions of LLaMA Touvron et al.  ###reference_b59###  ###reference_b59### on biomedical corpora. Compared with them, Meditron Chen et al. (2023  ###reference_b9###  ###reference_b9###) is built on LLaMA-2 Touvron et al. (2023  ###reference_b60###  ###reference_b60###) and extends its pretraining on a comprehensively curated medical corpus. BiomedGPT Zhang et al. (2023a  ###reference_b78###  ###reference_b78###) is also based on LLaMA-2 and pioneer as the first open-source and generalist visual language AI for diverse biomedical tasks. Biomistral Labrak et al. (2024  ###reference_b25###  ###reference_b25###) is an open-source LLM crafted specifically for the biomedical domain, optimized for efficiency through quantization and model merging techniques. Furthermore, we also compare our method with several representative retrieval-augmented LLMs in the biomedical domain. Almanac Zakka et al. (2024  ###reference_b75###  ###reference_b75###) is a novel approach utilizing OpenAI\u2019s GPT model integrated with a Qdrant vector database to hold external sources of knowledge retrieved from local corpus, web search, and calculators, designed to answer open-domain clinical questions. Like Almanac, Lozano et al. (2023  ###reference_b32###  ###reference_b32###) introduced Clinfo.ai, which is an open-source, end-to-end retrieval-augmented LLM (GPT) to answer medical queries using scientific literature summarizations derived from PubMed search engine. We adopt both Almanac and Clinfo.ai with the same prompt as ours to answer multiple-choice questions to suit the ADQA benchmark. Lastly, we implement a simple retrieval-augmented GPT baseline with CoT prompting similar to our proposed DALK. All the GPT models used are set to GPT-3.5-turbo as detailed in the next paragraph, to be consistent. We use GPT-3.5-turbo with the version \u201cgpt-3.5-turbo-0301\u201d and set the sampling temperature to 0.7. We utilize 7B versions of all the biomedical LLMs baselines. For RAG methods, we split each document with a max length of 128 and retrieve the top 3 most relevant documents as the support evidence for LLMs to do inference."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Main Result",
            "text": "The text discusses the performance of various models on the ADQA benchmark, highlighting that the dynamic co-augmentation framework and DALK method outperform other biomedical LLMs and RAG methods. It notes the consistent top or near-top accuracy scores across sub-datasets and attributes the superiority of GPT-3.5-turbo over smaller biomedical LLMs to its size and general applicability. GPT-3.5-turbo with Clinfo.ai shows notable improvement, especially when using external resources like the PubMed API, though this advantage diminishes when only the same corpora are used. Additional results are available in an appendix."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Further Analysis",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Co-augmentation Analysis",
            "text": "###figure_4### To comprehensively understand how the performance of LLMs evolves in response to increasing KG sizes, we undertake a detailed co-augmentation analysis. Illustrated in Figure 3  ###reference_###, our experiments aim to discern the changing performance trends of LLMs as the knowledge triples accumulate annually. Our findings reveal that our framework effectively fosters the co-evolution of LLMs and KG, with the performance of KG-augmented LLMs exhibiting a generally upward trajectory as the KG expands.\nNotably, when we remove the self-aware knowledge retrieval module, this upward trend becomes less significant.\nThis further implies the importance of sampling and selecting appropriate knowledge for LLMs when the KG\u2019s size increases."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Hyper-parameter Analysis",
            "text": "###figure_5### In this section, we do a hyper-parameter analysis on the retrieval number  of our self-aware retrieval module.\nWe select a group of value for  ([1,3,5,10,20,30]) and present the experiment results in Figure 4  ###reference_###.\nWe show the accuracy score on MedQA, MedMCQA, QA4MRE and AVG with different .\nWe find when  is small, an increment to it can lead to a performance enhancement.\nAfter the best performance shows up, continually increasing the value of  will cause a smooth decrease in the model accuracy score.\nThis result indicates the knowledge ranked in the top positions is more helpful while the knowledge ranked behind is something not very useful, thus successfully validating the capability of LLMs to do a fine-grained knowledge reranking.\nMoreover, we find the best  value is correlated with the length of queries in each single dataset.\nFor example, the best performance in MedQA (average query length is 107.4) shows up when  while the best performance in MedMCQA and QA4MRE shows up when  and  respectively.\nThis is consistent with our findings in Section 4.4  ###reference_### that a longer query corresponds to a larger and noisier sub-knowledge graph."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Sensitivity Analysis on ADQA Benchmark",
            "text": "In this section, we conduct a sensitivity analysis for our constructed ADQA by conducting a leave-one-out evaluation on AD-related keywords.\nWe do it by removing the samples with each keyword in our keyword list and calculating the AVG score of the remaining samples.\nAs the result shown in Table 6  ###reference_###, we find not all of the keywords are incorporated in our ADQA benchmark.\nNotably, the keywords \u201cCSF Biomarkers\u201d, \u201cNeurogenesis\u201d, \u201cPET Amyloid\u201d, \u201cPET Tau\u201d, \u201cTau Phosphorylation\u201d lack corresponding samples in ADQA.\nWe believe one critical work in the future for benchmarking AD-related knowledge is to collect QA samples to cover these missing keywords.\nMoreover, analyzing the performance variation upon removing samples linked to each keyword offers insight into determining the relevance of the keyword to AD."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Case Study",
            "text": "We put an example in Table 5  ###reference_### to showcase the efficacy of DALK.\nWe notice while the path-based sub-graph contains the relevant knowledge to exclude option C, it still involves other irrelevant information and finally fails to prompt the LLMs to produce the correct answer.\nIn contrast, our self-aware knowledge retrieval method successfully chooses the top 3 most relevant triples for the given problem and results in the correct answer D."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this research, we begin by analyzing the main limitations of adopting the existing LLMs-based methods in AD-specific areas.\nTo address these issues, we propose a novel approach in the merging of large language models and knowledge graphs in the context of Alzheimer\u2019s Disease. Our team provides an innovative dynamic co-augmentation framework for the refinement of large language models and knowledge graphs.\nInitially, our approach extracts structural insights from the unstructured scientific literature, crafting a specialized knowledge graph for AD. Subsequently, we employ a coarse-to-fine sampling technique coupled with a unique self-aware knowledge retrieval strategy to pinpoint relevant information from the knowledge graph.\nThe extensive evaluation conducted in our constructed ADQA benchmark showcases the effectiveness of our method and provides further hints into the synergy of LLMs and knowledge graph in the context of AD.\nIn the future, we will do more exploration in adopting and benchmarking LLMs in the AD areas."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In the development of our AD-KG, our primary focus lies in the exploration of two distinct methods for extracting relationships between associated entities. For entity recognition, we employ a strong PubTator annotator directly, without delving into the utilization of LLMs in this context. However, we have observed that LLMs also exhibit promising entity extraction capabilities in Section 3.2.1  ###reference_.SSS1###. We defer the refinement of methods for extracting entities for KG construction with LLMs to future works.\nFurthermore, a significant contribution of our work is the establishment of the ADQA benchmark. Nonetheless, the datasets utilized in constructing ADQA primarily consist of medical school exam questions, potentially exhibiting a domain gap from the scientific literature informing AD-KG. One potential remedy is leveraging PubmedQA Jin et al. (2019  ###reference_b22###); however, it is hindered by limited data amount. In the future, we will keep gathering AD-related QA samples and expanding the size of our ADQA benchmark."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "We have familiarized ourselves with and honour the ethical code set out in the ACL Code of Ethics222https://www.aclweb.org/portal/content/acl-code-ethics.\nThe knowledge graphs constructed in the paper are based on published scientific literature from PubMed.\nThe ADQA dataset used in the study is also derived from publicly available medical QA datasets that are properly cited.\nWe strive to ensure our study upholds ethical principles and not cause any kind of safety or privacy concerns.\nAlthough not observed in our multiple-choice QA analysis, we recognize the possibility of factual errors and hallucinations when using pre-trained LLMs for medical QA tasks in general, and we do not recommend these models be applied in a practical setting at present."
        }
    ],
    "url": "http://arxiv.org/html/2405.04819v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.4",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2405.04819v2",
        "paper_title": "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer\u2019s Disease Questions with Scientific Literature",
        "research_background": "#### Paper's Motivation\n\nThe paper is motivated by the critical challenges faced in the development of therapies for Alzheimer\u2019s Disease (AD) due to a lack of knowledge and understanding of the underlying etiological mechanisms. Despite the abundance of scientific literature and dedicated biomedical databases that could potentially supply rich AD-related knowledge, the sheer volume of information makes manual review infeasible. The rapid evolution of AD-related knowledge, coupled with the inefficiencies and limitations of current large language models (LLMs) in addressing domain-specific and long-tail knowledge, further complicates the problem.\n\n#### Research Problem\n\nThe primary research problem this paper aims to address is how to efficiently and effectively utilize large-scale scientific literature to improve the performance of LLMs in answering AD-related questions. This encompasses dealing with challenges such as data quality\u2014where dense and information-overloaded literature can lead to the retrieval of irrelevant and noisy information\u2014and efficiency and scalability issues, due to the rapidly evolving nature of AD knowledge which makes frequent retraining of domain-specific LLMs computationally demanding.\n\n#### Relevant Prior Work\n\nThe introduction references a variety of prior work:\n- The capability of LLMs in generating and processing language, mentioning foundational works and advancements in the field (Brown et al., 2020; Zhang et al., 2022; Anil et al., 2023; Touvron et al., 2023).\n- Chain-of-thought (CoT)-based prompting as a method to augment LLM performance in various tasks (Wei et al., 2022; Wang et al., 2022; Tong et al., 2023; Yao et al., 2023; Besta et al., 2023).\n- Specific attempts to leverage LLMs for biomedical and AD-related applications, recognizing the limitations in long-tail and domain-specific knowledge (Mao et al., 2023; Li et al., 2023c; Yan et al., 2024; Feng et al., 2023; Kandpal et al., 2023; Li et al., 2023b, 2024).\n- Common strategies such as retrieval augmented generation (RAG) and domain-specific LLM training, highlighting their limitations (Yu et al., 2023; Chen et al., 2024; Wu et al., 2024; Hu et al., 2021; Ovadia et al., 2023; Zhang et al., 2024).\n- Knowledge graph construction methods like pair-wise and generative construction to extract and structure knowledge from scientific literature (Carta et al., 2023; Wadhwa et al., 2023; Han et al., 2023; Bi et al., 2024).\n\nBy proposing the Dynamic Co-Augmentation of LLMs and KG (DALK) framework, the paper seeks to overcome these constraints, leveraging mutual benefits between LLMs and knowledge graphs to create a more effective AD-specific question-answering system.",
        "methodology": "### DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer\u2019s Disease Questions with Scientific Literature\n\n**Methodology:**\n\nThis section details our dynamic co-augmentation framework for leveraging Large Language Models (LLMs) and Knowledge Graphs (KGs) to answer Alzheimer's Disease (AD) related queries using scientific literature. The approach can be summarized in two primary stages: augmenting an AD-specific evolving KG with LLMs and literature corpus in a time-slicing fashion, and sampling relevant knowledge from this evolving KG to enhance the reasoning capabilities of LLMs. The methodology's overall pipeline is illustrated in Figure 1.\n\n**3.1 Augmenting AD-specific KG with LLMs and Literature Corpus:**\nTo create an AD-specific knowledge graph, we adhere to the methodology outlined by Pu et al. (2023) and utilize a carefully curated AD corpus by Professor Colin Masters at the University of Melbourne. This corpus, based on Professor Masters' extensive bibliography, includes over 16,000 PMID (PubMed ID)-indexed articles spanning from 1977 to 2021. For our study, we focus on the more recent knowledge from 2011 to 2021, resulting in a dataset of 9,764 articles.\n\nFor extracting entities at the appropriate granularity level for AD, we employ PubTator Central (PTC) developed and maintained by the NCBI. PTC provides state-of-the-art annotations of biomedical concepts in PubMed abstracts and full-text articles, encompassing six bioconcept types: genes, diseases, chemicals, mutations, species, and cell lines. We apply PTC to the abstracts of our AD papers to extract relevant named entities, which then serve as nodes in the knowledge graph.\n\nTo build an accurate and high-quality knowledge graph for AD, we assign specific relation types between related entities. We categorize current relation extraction methods involving LLMs into two primary groups: \n\n1. **Pair-wised Relation Extraction** (e.g., Carta et al., 2023; Wadhwa et al., 2023), which prompts LLMs to describe relationships between any two entities within a text segment.\n2. **Generative Relation Extraction** (e.g., Han et al., 2023; Bi et al., 2024; Datta et al., 2024), where LLMs output all related entity pairs and their corresponding relationships, directly.\n\nWe incorporate both methods into our KG augmentation process to facilitate a comprehensive comparison, resulting in two distinct knowledge graphs, denoted as \\( KG_{pair} \\) and \\( KG_{gen} \\). Table 1 presents the detailed statistics about these augmented knowledge graphs, including the number of used corpora, nodes, relations, and triples in \\( KG_{pair} \\) and \\( KG_{gen} \\).\n\n**3.2 Sampling Knowledge from Evolving KG to Enhance LLMs' Reasoning:**\nFollowing the creation of the augmented KG, we delineate the extraction of sub-graphs from \\( KG_{pair} \\). \n\n**Sub-Graph Extraction:**\n\n1. **Initial Node Selection:**\n   - Begin by selecting an initial node \\( n_i \\) from \\( KG_{pair} \\) and place the remaining nodes in a candidate node set \\( C \\).\n   \n2. **Node Exploration:**\n   - Explore up to \\( k \\) hops from \\( n_i \\) to identify the subsequent node \\( n_j \\). If \\( n_j \\) is found within \\( k \\) hops, update the start node to \\( n_j \\) and remove \\( n_j \\) from \\( C \\). If \\( n_j \\) is not found within \\( k \\) hops, concatenate the accumulated segment paths and store them in \\( P \\). Then, choose the next node \\( n_{new} \\) from \\( C \\) as the new start node, removing the previous nodes.\n   \n3. **Iteration:**\n   - Check if \\( C \\) is empty; if not, repeat the node exploration step. If \\( C \\) is empty, combine all path segments to form a set of sub-graphs stored in \\( S \\).\n\n**Query-based Augmentation:**\n\nThe framework endeavors to augment the evidence relevant to specific queries within \\( S \\) through a two-step process:\n\n1. **Neighbor Expansion:**\n   - Expand each node \\( n_l \\) within \\( S \\) by 1-hop, incorporating their neighbors \\( n_m \\) and adding the triples \\( (n_l, rel, n_m) \\) to \\( S \\).\n\n2. **Semantic Relevance Assessment:**\n   - Evaluate if each 1-hop neighbor \\( n_m \\) is semantically relevant to the query. If so, further expand the neighbors of \\( n_m \\), adding the triples \\( (n_m, rel, n_n) \\) to \\( S \\).\n\nFinally, after assembling the two sub-graphs \\( S_i \\) and \\( S_j \\), we perform post",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n- The **ADQA benchmark** (Alzheimer\u2019s Disease Question Answering) dataset is used for evaluating the performance of the models.\n\n**Baselines:**\n- **ChatDoctor** (Yunxiang et al., 2023): A fine-tuned version of the LLaMA model on biomedical corpora.\n- **Med-Alpaca** (Shu et al., 2023): Another fine-tuned version of the LLaMA model on biomedical corpora.\n- **Meditron** (Chen et al., 2023): Built on LLaMA-2 and extended pretraining on a robust medical corpus.\n- **BiomedGPT** (Zhang et al., 2023a): Based on LLaMA-2, designed for diverse biomedical tasks.\n- **Biomistral** (Labrak et al., 2024): An open-source LLM optimized for efficiency in the biomedical domain.\n- **Almanac** (Zakka et al., 2024): Utilizes OpenAI\u2019s GPT model integrated with a Qdrant vector database to answer clinical questions.\n- **Clinfo.ai** (Lozano et al., 2023): An open-source, end-to-end retrieval-augmented LLM using scientific literature summaries from PubMed.\n- **Simple Retrieval-Augmented GPT Baseline**: Implemented similar to DALK with Chain of Thought (CoT) prompting.\n\n**Evaluation Metrics:**\n- Performance is evaluated using **multiple-choice question accuracy on the ADQA benchmark**.\n\n**Key Parameter Settings:**\n- Models used: GPT-3.5-turbo (\"gpt-3.5-turbo-0301\") with sampling temperature set to 0.7.\n- Biomedical LLM versions: 7B models.\n- Document handling for RAG methods: Split documents with a max length of 128, retrieve top 3 relevant documents as support evidence.\n- Self-aware knowledge retrieval parameter \\( k \\): Set to 5.\n\n### Main Experimental Results\n\nThe main results of the experiment are focused on the accuracy of answering multiple-choice questions on the ADQA dataset. The paper likely provides a detailed comparative analysis, but this information is essential to understand the general setup and the competitive landscape. The findings might emphasize how DALK outperforms other baselines or achieves comparable results with advanced techniques in employing dynamic co-augmentation of Language Models (LLMs) and Knowledge Graphs (KGs). Further exploration of the results would generally include precise accuracy scores, statistical relevance, and an analysis of the effectiveness of each model in handling Alzheimer\u2019s Disease-related queries.\n"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the efficacy of the proposed self-aware knowledge retrieval method to enhance the performance of a dynamic co-augmentation framework in biomedical question answering tasks.",
            "experiment_process": "The ablation study isolates the self-aware knowledge retrieval module from the dynamic co-augmentation framework to compare performance enhancements. The experiments involved comparing results on various medical QA datasets, including the MMLU sub-dataset and QA4MRE. The comparison was based on how well the model performed with and without the self-aware knowledge retrieval module.",
            "result_discussion": "The dynamic co-augmentation framework without the self-aware knowledge retrieval module still enhances performance, but the improvement is less significant. Self-aware knowledge retrieval shows a correlation with the length of queries in the context, improving performance in the MMLU dataset due to longer contexts. In contrast, QA4MRE, with shorter queries, experiences performance declines, highlighting the necessity of contextually relevant knowledge retrieval.",
            "ablation_id": "2405.04819v2.No1"
        },
        {
            "research_objective": "Assess the impact of different knowledge graph (KG) construction methods\u2014generative versus relation extraction (RE)\u2014on the performance of the DALK framework in AD-specific question answering tasks.",
            "experiment_process": "A comparison was conducted between knowledge graphs constructed generatively and those constructed using the RE method. The study evaluated performance using the AD question answering (ADQA) benchmark with both the generatively constructed KG and the RE-constructed KG, examining the scale, number of triples, and precision of each method.",
            "result_discussion": "Despite RE method's KG having more scale and triples, it resulted in a notable performance drop within ADQA. Manual examination revealed that RE methods often inaccurately assigned relationships between unrelated entities. Conversely, the generative approach produced a smaller but more precise KG, emphasizing the importance of accuracy over coverage in LLM-generated KGs.",
            "ablation_id": "2405.04819v2.No2"
        }
    ]
}