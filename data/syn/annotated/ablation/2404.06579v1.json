{
    "title": "Less is More for Improving Automatic Evaluation of Factual Consistency",
    "abstract": "Assessing the factual consistency of automatically generated texts in relation to source context is crucial for developing reliable natural language generation applications. Recent literature proposes AlignScore which uses a unified alignment model to evaluate factual consistency and substantially outperforms previous methods across many benchmark tasks. In this paper, we take a closer look of datasets used in AlignScore and uncover an unexpected finding: utilizing a smaller number of data points can actually improve performance. We process the original AlignScore training dataset to remove noise, augment with robustness-enhanced samples, and utilize a subset comprising 10% of the data to train an improved factual consistency evaluation model, we call LIM-RA (Less Is More for Robust AlignScore). LIM-RA demonstrates superior performance, consistently outperforming AlignScore and other strong baselines like ChatGPT across four benchmarks (two utilizing traditional natural language generation datasets and two focused on large language model outputs). Our experiments show that LIM-RA achieves the highest score on 24 of the 33 test datasets, while staying competitive on the rest, establishing the new state-of-the-art benchmarks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The emergence of large language models (LLMs) and an increasing interest in utilizing machine-generated texts from like summarization, paraphrasing, and question-answering (QA) has created a need to automatically evaluate the degree to which generated natural language texts accurately reflect the factual information contained in source context. Early work used Natural Language Inference (NLI) Laban et al. (2022) and QA Fabbri et al. (2021) to handle automatic factual consistency evaluation. However, these methods exhibit limited generalizability and struggle with handling long contexts. Recently, Zha et al. (2023) propose AlignScore, a unified model based on RoBERTa and is trained on a wide range of datasets to calculate the alignment between context and generated text. AlignScore achieves state-of-the-art results across several factual consistency benchmarks.\n\nDespite its strengths, the AlignScore study has several limitations. AlignScore displays fragility regarding robustness, as it fails to identify some clear perturbations involving entities like names, numbers, etc. As Table 1 illustrates, even simple modifications can produce false positives and false negatives when using AlignScore.\n\nIn this paper, we propose LIM-RA (Less Is More - Robust AlignScore), an improved version of AlignScore trained on DeBERTa He et al. (2021). Our experiments show that LIM-RA consistently outperforms strong baselines including AlignScore and GPT-3.5-Turbo, achieving the new state-of-the-art on four factual consistency benchmarks covering a wide range of 33 datasets. It is worth noting that our experiments include a newly defined benchmark, Large Language Model Response (LLMR), designed for evaluating LLM outputs\u2019 factual consistency. LIM-RA performs the best on LLMR."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Context\nClaim\nAlignScore\nArchduchess Marie Louise was 18 years old when she married Napoleon .\n0.9907\n0.9542\nArchduchess Mari Louze was 18 years old when she married Napoleon .\n0.9650 (false positive)\n0.4381\nThe typical elevations of the Blue Ridge Mountains are 2,000 ft.\n0.9812\n0.9434\nThe typical elevations of the Blue Ridge Mountains are 2000 ft.\n0.0214 (false negative)\n0.8621\nWe convert binary and regression labels to 3-class labels. For datasets with binary labels, we map the negative label \u201cnot-aligned\" to either \u201ccontradiction\" or \u201cno-evidence\" depend on the dataset. In most of the cases, we map the negative label to \u201ccontradiction\", such as in doc_nli and paws. But in qqp, we map the negative label to \u201cno-evidence\". For regression labels in stsb dataset, we bin the score as three classes: faithful (), no-evidence (), contradiction ()."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "AlignScore Model and Training Data",
            "text": "Automatic evaluation of factual consistency is challenging. Recently proposed AlignScore measures the alignment of information between machine-generated natural language texts and given source material to evaluate the factual accuracy Zha et al. (2023  ###reference_b26###).\nAlignScore is built on top of a unified alignment function via RoBERTa Liu et al. (2019  ###reference_b18###) and trained on datasets derived from 7 NLP tasks: NLI, QA, Fact Verification, Paraphrase, Semantic Textuality Similarity, Information Retrieval, and Summarization. Each sample in a task is converted into a text pair (context, claim) and a label. The label has 3 options based on the task and dataset: binary (aligned, not-aligned), 3-way (aligned, contradict, neutral), regression (score between 0 to 1). For example in SNLI dataset, the context is the premise, the claim is the hypothesis, label is the 3-way label.\nCertain prepossessing steps are required to unify the format in multiple datasets.\nTo calculate the factual consistency score of long text, AlignScore first splits the context into roughly 350-token chunks and the claim into sentences. Then the trained alignment function (RoBERTa based) evaluates each sentence in the claim against each context chunk. For example, in the 3-way classification head, the probability of the \"aligned\" class is used as the alignment score. The highest alignment score for each claim sentence is selected and then averaged to obtain the overall factual consistency score. By using the chunking strategy, AlignScore can be applied to text of any length, as shown by Figure 3  ###reference_###."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Training Data Cleaning",
            "text": "For training, AlignScore uses more than 30 datasets and selects 500K samples from each dataset to build its training data, including a total of 4.7M training samples. Training the AlignScore alignment model requires 5 days on 8 V100 GPUs. However, we find that not all the training datasets have good quality. The upper half of Figure 2 shows a cohort of data cleaning steps we use to improve the training data quality. First, we remove four datasets that do not result in performance gains, such as ms_marco and wikihow. Additionally to prevent the model from truncating sentences that support the claim, we only keep samples in which the context has fewer than 512 tokens. When using QA datasets to create alignment training samples, since the QA passage is the context, a preprocessing step is needed. AlignScore uses a pre-trained sequence-to-sequence model to convert question-answer into a declarative sentence as the input claim. We, however, observed a performance decrease in our experiments when using this preprocessing. We find the decrease was because the generated declarative sentence has poor data quality. Thus, we concatenate question and answer as the claim text. We also tried to use Mistral-7B few-shot to generate better-quality declarative sentences but still did not produce performance gains. Additionally, many QA datasets only have ground truth answers (positive samples) but no wrong answers (negative samples). To address this, AlignScore generates fake wrong answers using the T5 model, and answers the question based on the original passage with the ground truth answer tokens masked. However, this leads to false negatives because many generated fake answers are similar to or exactly match their corresponding ground truth answers. To mitigate the issue, we use Sentence-BERT to encode both the fake and ground truth answers, and then filter out the fake answers that are similar to the true answers by using rules and a threshold of 0.85. This data cleaning procedure is illustrated in the top half of figure 2."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Synthetic Robustness Data",
            "text": "We also notice AlignScore fails on name or number perturbations as illustrated in Table 1  ###reference_###. To mitigate the issue, we augment the training dataset by creating a synthetic dataset designed to enhance the model\u2019s robustness, with emphasis on name and number variation based text generation as illustrated in the bottom half of figure 2  ###reference_###.\nWe create two synthetic datasets: Robust-Name and Robust-Number datasets using DocNLI  Yin et al. (2021  ###reference_b24###). DocNLI includes multiple-sentence contexts and single-sentence claims discussing facts in the context. To create the Robust-Name data, we use spaCy NER  Honnibal and Montani (2017  ###reference_b8###) to identify the \"PERSON\" and \"ORG\" entities in samples labeled as \"entailment\" and use Mistral-7B to perturb the entities (prompt details in Appendix A.3  ###reference_###). The original entity is replaced with the perturbed entity to construct the synthetic negative samples. Using Mistral instead of randomly perturbing a character in the entity ensures the new name is similar to a real person or org name. The two-step generation generates a better rewritten claim than directly instructing the LLM to rewrite the claim.\nSimilarly, we construct the Robust-Number data by perturbing claims with number-related labels such as \"TIME\", \"QUANTITY\", \"DATE\". We use Mistral to rephrase (\"100\" to \"one hundred\") and change numbers (\"100\" to \"101\"). The perturbed entities replace the original to create positive and negative data."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "LIM-RA Model",
            "text": "We experiment with different pretrained models as base including RoBERTa (large), DeBERTa (large), DistilBERT (base). DeBERTa achieves the best overall performance while DistilBERT has poor performance due to its small model capacity. Also, we unify all data labels to the three class setup (details later in this section ), and use the 3-way classification head to predict  (factual consistent),  (no-evidence), and . At inference time, we follow AlignScore to split context into chunks and claim into sentences, and average the sentence alignment scores to compute the overall factual consistency score. We denote LIM-RA and LIM-A as the DeBERTa model trained with cleaned data and with and without synthetic robustness in training, respectively.\nUnder the Hood: We train a pre-trained NLI DeBERTa model222https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli  Laurer et al. (2024  ###reference_b15###) for 3 epochs using AdamW optimizer with learning rate as 1e-5. We use the first 20k samples from each of the 28 train datasets described in AlignScore, plus the 2 new synthetic robustness datasets, resulting in a total of 490k samples in our final training. Hyperparameter details can be found in Table 10  ###reference_###. We follow AlignScore and use the factual consistency class probability as the alignment score.\nWe convert binary and regression labels to 3-class labels. For datasets with binary labels, we map the negative label \u201cnot-aligned\" to either \u201ccontradiction\" or \u201cno-evidence\" depend on the dataset. In most of the cases, we map the negative label to \u201ccontradiction\", such as in doc_nli and paws. But in qqp, we map the negative label to \u201cno-evidence\". For regression labels in stsb dataset, we bin the score as three classes: faithful (), no-evidence (), contradiction ()."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Connecting to Related Works",
            "text": "Previous studies include multiple other methods for assessing factual consistency.\n(1) QA-based factual consistency, including QuestEval Scialom et al. (2021  ###reference_b22###) and QAFactEval Fabbri et al. (2021  ###reference_b3###), checks if the source answer is different from the target answer given a question. (2) With the recent advances in LLMs, a new line of research is to evaluate factual consistency directly with an LLM Liu et al.  ###reference_b17###; Fu et al. (2023a  ###reference_b5###); Jia et al. (2023  ###reference_b10###). Chen et al. (2023  ###reference_b1###) investigate a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting and Luo et al. (2023  ###reference_b19###) explore ChatGPT\u2019s ability to evaluate factual inconsistency under a zero-shot setting while Fu et al. (2023b  ###reference_b6###) uses LLMs in a QA setting for direct factual consistency scoring. (3) A third related line of methods uses the Natural Language Inference (NLI) based formulation. For instance Laban et al. (2022  ###reference_b14###) proposed SummaCConv, that segments documents into sentences and aggregates NLI scores between pairs of sentences.\nFactual consistency benchmark datasets typically contain (context, claim, label) triplets where the label indicates if the claim is consistent with the context and is difficult to obtain as high-quality annotation is challenging due to low inter-annotator agreement Falke et al. (2019  ###reference_b4###); Laban et al. (2022  ###reference_b14###). Laban et al. (2022  ###reference_b14###) introduce the SummaC (Summary Consistency) benchmark which consists of 6 large inconsistency detection datasets standardized as a binary classification task given document and summary. Laban et al. (2023  ###reference_b13###) introduce SummEdits, a summarization consistency dataset where an LLM introduces inconsistencies in an otherwise consistent summary and show that the benchmark is challenging for most current LLMs. Honovich et al. (2022  ###reference_b9###) present TRUE, which consolidates 11 existing datasets covering summarization, knowledge-grounded dialogue, paraphrasing and fact verification annotated for consistency.\n###table_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We conduct a comprehensive experimental study to evaluate LIM-RA on multiple factual consistency benchmarks and demonstrate LIM-RA consistently outperforms strong baselines and establishes new state-of-the-art results. Our experiments also include ablation studies (Table 7  ###reference_###) and robustness analysis (Table 9  ###reference_###) of LIM-RA. We list the hyperparameters we used for LIM-RA in Table 10  ###reference_###. Each of our experiments covers 20 different random seeds."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Four Benchmarks: 33 Datasets",
            "text": "We evaluate the factual consistency performance using AUC-ROC on 33 datasets from 4 benchmarks: SummaC, SummEdits, TRUE, and LLMR. Each data sample in the benchmarks is a pair of target text (claim) and a grounding source text (context), with a binary annotation of whether the target text is factually consistent w.r.t its source. The benchmark dataset details can be found in Appendix A.2  ###reference_###.\nSummaC 5 summary consistency datasets: GoGenSumm (CG), XsumFaith (XF), FactCC (FC), SummEval (SE), Frank (FRK). We remove Polytope dataset since it contains negative samples that do not imply factual consistency errors.\nTRUE 11 datasets covering summarization, knowledge-grounded dialogue, paraphrasing and fact verification annotated for factual consistency: Frank (FRK), SummEval (SE), MNBM, QAGS-CNNDM (QC), QAGS-Xsum (QX), BEGIN, Q, DialFact (DF), Fever (FVR), VitaminC (VITC), PAWS.\nSummEdits 10 datasets evaluating factual consistency in summarization covering multiple domains. Inconsistent summaries are generated by GPT-3.5-Turbo: News, Podcast (PD), Billsum (BILL), Samsum (SS), Shakespeare (SP), SciTLDR (SCI), QMSum (QM), ECTSum (ECT), Sales Email (SEmail), Sales Call (SCall).\nLLMR (large language model response) is a new benchmark consisting of 7 datasets we introduce in this paper. Similar to SummEdits, the datasets are designed to evaluate the factual consistency of LLM output and inconsistencies are generated in an automated fashion with human verification: HaluEval (HE) Li et al. (2023  ###reference_b16###) consists of CNN/DailyMail articles with correct and hallucinated summaries generated by ChatGPT in a zero-shot manner. BAMBOO abs-hallu (BBA) and sen-hallu (BBS) subsets Dong et al. (2023  ###reference_b2###) consist of NLP academic papers (max 4K and 16K token variants for a total of 4 datasets) with supported and hallucinated hypotheses generated by ChatGPT similar to HE. Passage-level Hallucination Detection (PHD) Yang et al. (2023  ###reference_b23###) consists of Wikipedia articles of an entity with correct and hallucinated biographies of that entity generated by ChatGPT. AttrScore (ATS) Yue et al. (2023  ###reference_b25###) consists of QA datasets and New Bing search queries in the format  where  indicates if the  is supported by . Hallucinations are generated by both swapping the answer with an incorrect answer and by swapping the the context with another article. For our experiments we consider context as  and answer as ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Baselines Methods",
            "text": "NER  Laban et al. (2022  ###reference_b14###), uses spaCy NER to match entities between claim and context.\nQuesteval, QA-based model, evaluates both factual consistency and relevance of the generated text by checking if the answer from source is different from the answer from target given a question.\nQAFactEval, QA-based model, evaluates factual consistency by performing answer selection, question generation, question answering, and answer overlap evaluation.\nSummaC, NLI-based model (SummaCConv), segments documents into sentence units and aggregates scores between pairs of sentences.\nAlignScore, current state-of-the-art, an alignment function trained on a wide range of datasets.\n0-shot/10-shot GPT-3.5-Turbo, instruct the LLM to evaluate whether the claim is consistent, lacks evidence, or contains contradictions.\n10-shot Mistral-7B, one of the best performing open-source LLMs. We use the same prompts as 10-shot GPT-3.5-Turbo."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Experimental Results",
            "text": ""
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Results on Traditional Benchmarks: SummaC and TRUE",
            "text": "We evaluate factual consistency models on the SummaC benchmark in Table 2  ###reference_###. LIM-RA achieves the best overall score and has a 5.7% relative improvement over AlignScore and QAFactEval. Our model has the top result in 4 of the 5 datasets. Our results for AlignScore are lower than the results reported in the original work Zha et al. (2023  ###reference_b26###) because we did not include the rule-based inference-time processing (such as removing special tokens or capitalizing the first letter) for a fair comparison between all models.\nFrom the results on the TRUE benchmark in Table 5  ###reference_###, we see that LIM-RA has the best overall AUC-ROC score with a 0.9% improvement over AlignScore and has the best score in 5 of 11 datasets. As suggested in  Zha et al. (2023  ###reference_b26###), we report AVG by removing PAWS, FVR, and VITC to show out-of-domain performance; LIM-RA remains the best performing model."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Results on LLM output: SummEdits and LLMR",
            "text": "We evaluate factual consistency on LLM responses using the SummEdits and LLMR benchmarks in Table 3  ###reference_### and Table 4  ###reference_### respectively. On the SummEdits benchmark, both LIM-A and LIM-RA consistently outperform other baselines. LIM-RA has the best overall performance and has a 5.0% relative improvement over the best baseline AlignScore. Our model achieves the best score in 8 of the 10 datasets and performs significantly better on OOD domain datasets such as Shakespeare (SP), BillSum (BILL), SciTLDR (SCI) compared to the baseline. On the LLMR benchmark, we only report AlignScore as Tables 2  ###reference_###, 3  ###reference_###, 5  ###reference_### show that AlignScore is the strongest baseline. LIM-RA achieves the best overall result and obtains a relative improvement of 6.9% over AlignScore, and has the best score on 6 of the 7 datasets.\nWe report the overall average score on the four benchmarks in Table 6  ###reference_###. In summary, LIM-RA exhibits a 4.4% relative improvement over the baseline model AlignScore."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Comparing with LLM Baselines",
            "text": "We compare the trained metric models with two LLMs: Mistral-7B and GPT-3.5-Turbo (ChatGPT) using the same 0-shot and 10-shot prompt (described in Appendix A.4  ###reference_###). Since LLMs do not provide factual consistency scores, we report balanced accuracy in Table 8  ###reference_### and only report SummaC and SummEdits due to time constraints. LIM-RA continues to perform the best on the two benchmarks while GPT-3.5-Turbo outperforms Mistral by a large margin on SummaC. Additionally, 0-shot ChatGPT outperforms 10-shot ChatGPT on SummEdits possibly because the 10-shot demonstrations are out-of-domain. We compare average inference time of each model on a sample of data from SummaC and find AlignScore demonstrates fast inference speed of 0.18s on a single NVDIA-A10G GPU followed by LIM-RA with 0.29s. The slower speed is because DeBERTa is slower than RoBERTa even though they have a similar number of parameters. 0-shot ChatGPT and Mistral-7B on 4 GPUs using vLLM Kwon et al. (2023  ###reference_b12###) achieves comparable speed of 0.52s and 0.51s respectively while OpenAI GPT-3.5 10-shot is the slowest, primarily due the to the rate limit of a Tier-1 account333The Tier-1 rate-limit for GPT-3.5-Turbo is 60K tokens per minute, 3.5K requests per minute, and 10K requests per day. https://platform.openai.com/docs/guides/rate-limits/usage-tiers?context=tier-one."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Results on Synthetic Robustness Data",
            "text": "In Table 9  ###reference_### we evaluate the models on the synthetic robustness test dataset created in section 2.3  ###reference_###. We see LIM-A without synthetic data augmentation performs on par with AlignScore while LIM-RA performs the best and is more robust to name and number perturbations."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We propose LIM-RA, a DeBERTa based model to automatically evaluate factual consistency trained from a cleaner and smaller training set than used for AlignScore. Experimental results show LIM-RA consistently outperforms the current state-of-the-art AlignScore and other strong baselines on 4 benchmarks. In addition, the model is robust to name and number variations and is better suited for LLM outputs\u2019 factual consistency evaluation."
        }
    ],
    "url": "http://arxiv.org/html/2404.06579v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.5"
        ],
        "methodology_sections": [
            "2",
            "2.2",
            "2.3",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3",
            "3.4",
            "3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "2.2",
            "3.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.06579v1",
        "paper_title": "Less is More for Improving Automatic Evaluation of Factual Consistency",
        "research_background": "The emergence of large language models (LLMs) and increased interest in machine-generated texts such as summarization, paraphrasing, and question-answering have created a demand for automated systems to evaluate the factual accuracy of these texts. Early work in this field utilized Natural Language Inference (NLI) (Laban et al. 2022) and question-answering (QA) techniques (Fabbri et al. 2021) to assess factual consistency. However, these methods displayed limited generalizability and struggled with handling long contexts. Recently, Zha et al. (2023) proposed AlignScore, a unified RoBERTa-based model trained on a diverse set of datasets to measure alignment between context and generated text, achieving state-of-the-art results on various benchmarks.\n\nDespite these advancements, the AlignScore model has notable limitations. The training data was heuristically derived from multiple existing NLP tasks and datasets, introducing noise and decreasing quality in some instances. Our ablation studies reveal that not all data points from AlignScore's training are necessary. Additionally, AlignScore's robustness is questioned, as it fails to consistently handle perturbations involving entities like names and numbers, leading to false positives and negatives.\n\nTo address these shortcomings, we introduce LIM-RA (Less Is More - Robust AlignScore), an enhanced version of AlignScore trained on DeBERTa (He et al. 2021). LIM-RA is built following several ablation steps aimed at improving training data quality and assessing the optimal training size. We also generate synthetic data to bolster the model's robustness. Our findings indicate that using just 10% of the cleaned training data yields a superior model compared to AlignScore. LIM-RA consistently outperforms existing models, including AlignScore and GPT-3.5-Turbo, and achieves state-of-the-art results on four factual consistency benchmarks across 33 datasets. Our experiments also feature a newly defined benchmark, Large Language Model Response (LLMR), for evaluating the factual consistency of LLM outputs, on which LIM-RA excels.\n\n**Motivation:**\nThe need to improve the automatic evaluation of factual consistency in machine-generated texts, motivated by the increasing use of LLMs for tasks like summarization, paraphrasing, and question-answering.\n\n**Research Problem:**\nHow to enhance the factual consistency evaluation of generated texts, particularly overcoming the limitations of existing models like AlignScore in terms of data quality, robustness, and generalizability.\n\n**Relevant Prior Work:**\n1. **Natural Language Inference (NLI)** - used by Laban et al. (2022) for factual consistency evaluation, but with limited generalizability.\n2. **Question-Answering (QA)** - employed by Fabbri et al. (2021) but struggles with long contexts.\n3. **AlignScore** - proposed by Zha et al. (2023), a RoBERTa-based model achieving state-of-the-art results but limited by noisy training data and robustness issues.\n",
        "methodology": "The paper proposes a method for improving the automatic evaluation of factual consistency, specifically focusing on the transition from binary and regression labels to 3-class labels for various datasets.\n\n### Key Components and Innovations\n\n**1. **Contextual Examples**: The methodology section presents examples of claims along with their alignment scores.\n   - Example 1: \"Archduchess Marie Louise was 18 years old when she married Napoleon.\"\n      - **Context and AlignScore**:\n         - **Correct Claim**: Context: 0.9907, AlignScore: 0.9542\n         - **Misspelled Claim**: Context: 0.9650, AlignScore: 0.4381\n   \n   - Example 2: \"The typical elevations of the Blue Ridge Mountains are 2,000 ft.\"\n      - **Context and AlignScore**:\n         - **Correct Claim**: Context: 0.9812, AlignScore: 0.9434\n         - **Numerical Claim Alteration**: Context: 0.0214, AlignScore: 0.8621\n\n**2. **Label Conversion**: The methodology emphasizes converting binary and regression labels to 3-class labels for more nuanced evaluation.\n   - **Binary Labels**: \n     - **Negative Label (\"not-aligned\")** mapped either to \"contradiction\" or \"no-evidence\" depending on the dataset characteristics.\n       - For `doc_nli` and `paws`, the negative label maps to \"contradiction\".\n       - For `qqp`, it maps to \"no-evidence\".\n\n   - **Regression Labels (stsb dataset)**:\n     - Binned into three classes:\n       - Faithful: \n       - No-evidence: \n       - Contradiction: \n\nThis approach allows for a more detailed categorization of alignment issues, thus enhancing the robustness of factual consistency evaluations. The paper's methodology leverages nuanced scoring and labeling transformations to tackle the insufficiencies found in binary and regression labels, propelling more accurate automatic evaluations.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets**: The main experiment evaluates LIM-RA using multiple factual consistency benchmarks. These benchmarks are widely recognized datasets used to test factual consistency in generated text, though specific dataset names are not provided in this description.\n\n**Baselines**: The performance of LIM-RA is compared against several strong baseline models. These baselines represent the current state-of-the-art methods in evaluating factual consistency. Though specific baseline models are not mentioned in this text, they are presumably well-established methods in the field.\n\n**Evaluation Metrics**: The evaluation metrics used to measure the efficacy of LIM-RA against the baselines are not detailed in this description. However, these metrics typically include precision, recall, F1 score, and other relevant measures that assess factual correctness and consistency in generated content.\n\n**Experimental Procedure**: The experiments are conducted with 20 different random seeds to ensure the robustness and statistical significance of the results.\n\n**Main Results**: LIM-RA consistently outperforms the strong baselines in the main experiment and sets new state-of-the-art results across the multiple factual consistency benchmarks tested. This indicates that LIM-RA is effective in improving the automatic evaluation of factual consistency.\n\nIn summary, the main experiment demonstrates the superior performance of LIM-RA over existing methods, validated through comprehensive testing across several benchmarks and multiple random seeds."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the impact of different training data sizes and the efficacy of training on cleaned and synthetic data to develop an improved factual consistency evaluation model dubbed LIM-RA.",
            "experiment_process": "The ablation study investigates four key questions: (1) What is the impact of different training data sizes? (2) What is the performance of using a pre-trained model for alignment? (3) What is the impact of cleaned data? (4) What is the impact of fine-tuning RoBERTa or DeBERTa as the alignment function? For question (1), training sizes were swept from 123K (5K per dataset) to 4M (500K per dataset), and benchmark performance was recorded. For questions (2)-(4), the average AUC-ROC score was noted for SUMMAC, SummEdits, and TRUE datasets. Different pre-trained NLI models were tested for question (2), data cleaning steps were executed, and 10% of the training data (452K samples) was used for question (3), and pre-trained models were fine-tuned for question (4).",
            "result_discussion": "The benchmark performance peaked at 452K and 1.6M samples for DeBERTa and RoBERTa respectively, showing a reduction in performance with additional data. The best pre-trained DeBERTa model achieved an AUC-ROC of 82.1%, outperforming the best pre-trained RoBERTa model at 71.6%. Data cleaning improved RoBERTa (84.1%) and DeBERTa (83.6%) performance compared to AlignScore (83.2%). Fine-tuning improved DeBERTa's performance further while decreasing RoBERTa's performance, indicating better initial performance of the pre-trained DeBERTa model. Adding synthetic robustness data provided additional performance gains.",
            "ablation_id": "2404.06579v1.No1"
        },
        {
            "research_objective": "Assess the quality of training data and its impact on the performance of the factual consistency evaluation model.",
            "experiment_process": "The study investigates the quality of the training data used for AlignScore by performing data cleaning steps. Four datasets that did not result in performance gains were removed. Context samples with fewer than 512 tokens were retained. A preprocessing step converting QA datasets to declarative sentences was removed, and concatenation of question and answer as the claim text was used instead. Sentence-BERT was used to filter out fake answers that were too similar to ground truth answers in QA datasets. The cleaned dataset comprised 452K samples out of the original 4.7M used for AlignScore training.",
            "result_discussion": "Data cleaning and reduction improved model performance. Using only 10% of the original training data led to better model quality with a total of 452K samples. This indicates that the quality of training data is more crucial than the quantity, supporting the principle of less is more. Filtered and selective data contributed to a more robust and accurate model.",
            "ablation_id": "2404.06579v1.No2"
        }
    ]
}