{
    "title": "Exploring Contrastive Learning for Long-Tailed Multi-Label Text Classification",
    "abstract": "Learning an effective representation in multi-label text classification (MLTC) is a significant challenge in NLP. This challenge arises from the inherent complexity of the task, which is shaped by two key factors: the intricate connections between labels and the widespread long-tailed distribution of the data. To overcome this issue, one potential approach involves integrating supervised contrastive learning with classical supervised loss functions. Although contrastive learning has shown remarkable performance in multi-class classification, its impact in the multi-label framework has not been thoroughly investigated. In this paper, we conduct an in-depth study of supervised contrastive learning and its influence on representation in MLTC context. We emphasize the importance of considering long-tailed data distributions to build a robust representation space, which effectively addresses two critical challenges associated with contrastive learning that we identify: the \"lack of positives\" and the \"attraction-repulsion imbalance\". Building on this insight, we introduce a novel contrastive loss function for MLTC. It attains Micro-F1 scores that either match or surpass those obtained with other frequently employed loss functions, and demonstrates a significant improvement in Macro-F1 scores across three multi-label datasets.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, multi-label text classification has gained significant popularity in the field of Natural Language Processing (NLP). Defined as the process of assigning one or more labels to a document, MLTC plays a crucial role in numerous real-world applications such as document classification, sentiment analysis, and news article categorization.\nDespite its similarity to multi-class mono-label text classification, MLTC presents two fundamental challenges: handling multiple labels per document and addressing datasets that tend to be long-tailed. These challenges highlight the inherent imbalance in real-world applications, where some labels are more present than others, making it hard to learn a robust semantic representation of documents.\nNumerous approaches have emerged to address this issue, such as incorporating label interactions in model construction and devising tailored loss functions. Some studies advocate expanding the representation space by incorporating statistical correlations through graph neural networks in the projection head Vu et al. (2022  ###reference_b21###); Xu et al. (2020  ###reference_b24###). Meanwhile, other approaches recommend either modifying the conventional Binary Cross-Entropy (BCE) by assigning higher weights to certain samples and labels or introducing an auxiliary loss function for regularization Zhang et al. (2021  ###reference_b27###). Concurrently, recent approaches based on supervised contrastive learning employed as an auxiliary loss managed to enhance semantic representation in multi-class classification Cui et al. (2021  ###reference_b4###); Gunel et al. (2020  ###reference_b8###).\nWhile contrastive learning represents an interesting tool, its application in MLTC remains challenging due to several critical factors. Firstly, defining a positive pair of documents is difficult due to the interaction between labels. Indeed, documents can share some but not all labels, and it can be hard to clearly evaluate the degree of similarity required for a pair of documents to be considered positive. Secondly, the selection of effective data augmentation techniques necessary in contrastive learning proves to be a non-trivial task. Unlike images, where various geometric transformations are readily applicable, the discrete nature of text limits the creation of relevant augmentations. Finally, the data distribution in MLTC often shows an unbalanced or long-tailed pattern, with certain labels being noticeably more common than others. This might degrade the quality of the representation Graf et al. (2021  ###reference_b7###); Zhu et al. (2022  ###reference_b28###). Previous research in MLTC has utilized a hybrid loss, combining supervised contrastive learning with classical BCE, without exploring the effects and properties of contrastive learning on the representation space. Additionally, the inherent long-tailed distribution in the data remains unaddressed, leading to two significant challenges that we term as \u201clack of positive\u201d and \u201cattraction-repulsion imbalance\u201d. The \u201clack of positive\u201d issue arises when instances lack positive pairs in contrastive learning, and the \u201cattraction-repulsion imbalance\u201d is characterized by the dominance of attraction and repulsion terms for the labels in the head of the distribution.\nIn this paper, we address these challenges head-on and present a novel multi-label supervised contrastive approach, referred to as ABALONE, introducing the following key contributions:\nWe conduct a comprehensive examination of the influence of contrastive learning on the representation space, specifically in the absence of BCE and data augmentation.\nWe put forth a substantial ablation study, illustrating the crucial role of considering the long-tailed distribution of data in resolving challenges such as the \u201cAttraction-repulsion imBAlance\u201d and \u201cLack of pOsitive iNstancEs\u201d.\nWe introduce a novel contrastive loss function for MLTC that attains Micro-F1 scores on par with or superior to existing loss functions, along with a marked enhancement in Macro-F1 scores.\nFinally, we examine the quality of the representation space and the transferability of the features learned through supervised contrastive learning.\nThe structure of the paper is as follows: in Section 2  ###reference_###, we provide an overview of related work. Section 3  ###reference_### introduces the notations used throughout the paper and outlines our approach. In Section 4  ###reference_###, we present our experimental setup, while Section 5  ###reference_### provides results obtained from three datasets. Finally, Section 6  ###reference_### presents our conclusions."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section, we delve into an exploration of related work on supervised contrastive learning, multi-label text classification, and the application of supervised contrastive learning to MLTC."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Supervised Contrastive Learning",
            "text": "The idea of supervised contrastive learning has emerged in the domain of vision with the work of Khosla et al. (2020  ###reference_b11###) called SupCon. This study demonstrates how the application of a supervised contrastive loss may yield results in multi-class classification that are comparable, and in some cases even better, to the traditional approaches. The fundamental principle of contrastive learning involves enhancing the representation space by bringing an anchor and a positive sample closer in the embedding space, while simultaneously pushing negative samples away from the anchor. In supervised contrastive learning, a positive sample is characterized as an instance that shares identical class with the anchor. In Graf et al. (2021  ###reference_b7###), a comparison was made between the classical cross-entropy loss function and the SupCon loss. From this study, it appeared that both loss functions converge to the same representation under balanced settings and mild assumptions on the encoder. However, it was observed that the optimization behavior of SupCon enables better generalization compared to the cross-entropy loss.\nIn situations where there is a long-tailed distribution, it has been found that the representation learned via the contrastive loss might not be effective. One way to improve the representation space is by using class prototypes Zhu et al. (2022  ###reference_b28###); Cui et al. (2021  ###reference_b4###); Graf et al. (2021  ###reference_b7###). Although these methods have shown promising results, they primarily tackle challenges in multi-class classification problems."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Multi-label Classification",
            "text": "Learning MLTC using the binary cross-entropy loss function, while straightforward, continues to be a prevalent approach in the literature. A widely adopted and simple improvement to reduce imbalance in this setting is the use of focal loss Lin et al. (2017  ###reference_b14###). This approach prioritizes difficult examples by modifying the loss contribution of each sample, diminishing the loss for well-classified examples, and accentuating the importance of misclassified or hard-to-classify instances. An alternative strategy involved employing the asymmetric loss function Ridnik et al. (2021  ###reference_b18###), which tackles the imbalance between the positive and negative examples during training. This is achieved by assigning different penalty levels to false positive and false negative predictions. This approach enhances the model\u2019s sensitivity to the class of interest, leading to improved performance, especially in datasets with imbalanced distributions.\nOther works combine an auxiliary loss function with BCE, as in multi-task learning, where an additional loss function serves as regularization. For instance, Zhang et al. (2021  ###reference_b27###) suggest incorporating an auxiliary loss function that specifically addresses whether two labels co-occur in the same document. Similarly, Alhuzali and Ananiadou (2021  ###reference_b1###) propose a label-correlation-aware loss function designed to maximize the separation between positive and negative labels inside an instance.\nRather than manipulating the loss function, alternative studies suggest adjusting the model architecture. A usual approach involves integrating statistical correlations between labels using Graph Neural Network Xu et al. (2020  ###reference_b24###); Ma et al. (2021  ###reference_b17###); Vu et al. (2022  ###reference_b21###). Additionally, a promising avenue of research looks into adding label parameters to the model, which would enable the learning of a unique representation for every label as opposed to a single global representation Kementchedjhieva and Chalkidis (2023  ###reference_b10###); Alhuzali and Ananiadou (2021  ###reference_b1###); Xiao et al. (2019  ###reference_b23###)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Supervised Contrastive Learning for Multi-label Classification",
            "text": "The use of supervised contrastive learning in multi-label classification has recently gained interest within the research community. All the existing studies investigate the effects of supervised contrastive learning by making some kind of prior assumption about label interactions in the learned representation space.\nDao et al. (2021  ###reference_b5###) suggest to use supervised contrastive learning for image classification based on the assumption that labels are situated in distinct areas of an image. Their contrastive loss is utilized alongside the BCE loss function and serves as a type of regularization more details can be found in Appendix F  ###reference_###.\nLin et al. (2023  ###reference_b13###) propose five different supervised contrastive loss functions that are used jointly with BCE to improve semantic representation of classes. In addition, Su et al. (2022  ###reference_b20###) suggest using a KNN algorithm during inference in order to improve performance. Some studies use supervised contrastive learning with a predefined hierarchy of labels Zhang et al. (2022  ###reference_b26###); Wang et al. (2022  ###reference_b22###).\nWhile contrastive loss functions in mono-label multi-class scenarios push apart representations of instances from different classes, directly applying this approach to the multi-label case may yield suboptimal representations, particularly for examples associated with multiple labels. This can lead to a deterioration in results, particularly in long-tail scenarios.\nIn contrast to other methods, our approach does not rely on any prior assumptions about label interactions. We address the long-tail distribution challenge in MLTC by proposing several key changes in the supervised contrastive learning loss."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "ABALONE",
            "text": "We begin by introducing the notations and then present our approach. In the following,  is defined as the set of indices of examples in a batch, and  represents the number of labels. The representation of the  document in a batch is denoted as . The associated label vector for example  is , with  representing its  element. Furthermore, we denote by  the set of document embeddings in the batch .\n###figure_1### We use a memory system by maintaining a queue , which stores the learned representations of the  preceding instances from the previous batches obtained from a momentum encoder. This is in line with other approaches He et al. (2020  ###reference_b9###); Chen et al. (2020  ###reference_b3###) that propose to increase the number of positive and negative pairs used in a contrastive loss. Additionally, we propose to incorporate a set of  trainable label prototypes . This strategy guarantees that each example in the batch has at least as many positive instances as the number of labels it possesses.\nThese two techniques are particularly advantageous for the labels in the tail of the distribution, as they guarantee the presence of at least some positive examples in every batch.\nPrevious work highlights the significance of assigning appropriate weights to the repulsion term within the contrastive loss Zhu et al. (2022  ###reference_b28###).\nIn the context of multi-label scenarios, our proposal involves incorporating a weighting scheme into the repulsion term (denominator terms in the contrastive loss function), to decrease the impact of head labels. More details about attraction and repulsion terms introduced in Graf et al. (2021  ###reference_b7###) can be found in Appendix E  ###reference_###. For an anchor example  with respect to any other instances  in the batch and in the memory queue, we define the weighting of the repulsion term as:\nwith . This function assigns equal weights to all prototypes, allocating less weight to all other examples present in both the batch and the queue.\nIn contrastive learning for mono-label multi-class classification, the attraction term is consistently balanced, as each instance is associated with only one class. While, in MLTC, a document can have multiple labels, some in the head and others in the tail of the class distribution. Our approach not only weights positive pairs based on label interactions but also considers the rarity of labels within the set of positive pairs. Instead of iterating through each instance, we iterate through each positive label of an anchor defining a positive pair, as an instance associated with this label.\nFigure 1  ###reference_### illustrates the influence of addressing the lack of positives and attraction-repulsion imbalance with our new multi-label contrastive loss, denoted as , compared to the original supervised contrastive loss,  on the exact same training examples in two different situations."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Contrastive Baseline",
            "text": "Before introducing our approach, we provide a description of our baseline for comparison, denoted as , and defined as follows:\nThis loss is a simple extension of the SupCon loss Khosla et al. (2020  ###reference_b11###) with an additional term introduced to model the interaction between labels, corresponding to the Jaccard Similarity.  represents the temperature,  represents the cosine similarity, and  is the normalization term defined as:\nIt is to be noted that , does not consider the inherent long-tailed distribution of multi-label dataset, and that it is similar to other losses proposed in contrastive learning Su et al. (2022  ###reference_b20###); Lin et al. (2023  ###reference_b13###). We provide further details in Appendix C  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Motivation",
            "text": "Our work can be dissected into two improvements compared to the conventional contrastive loss proposed for MLTC.\nEach of these improvements aims to tackle the long-tailed distribution inherent in the data and alleviate concerns related to the absence of positive instances and the imbalance in the attraction-repulsion dynamics. These improvements are outlined as follows.\nWe use a memory system by maintaining a queue , which stores the learned representations of the  preceding instances from the previous batches obtained from a momentum encoder. This is in line with other approaches He et al. (2020  ###reference_b9###  ###reference_b9###); Chen et al. (2020  ###reference_b3###  ###reference_b3###) that propose to increase the number of positive and negative pairs used in a contrastive loss. Additionally, we propose to incorporate a set of  trainable label prototypes . This strategy guarantees that each example in the batch has at least as many positive instances as the number of labels it possesses.\nThese two techniques are particularly advantageous for the labels in the tail of the distribution, as they guarantee the presence of at least some positive examples in every batch.\nPrevious work highlights the significance of assigning appropriate weights to the repulsion term within the contrastive loss Zhu et al. (2022  ###reference_b28###  ###reference_b28###).\nIn the context of multi-label scenarios, our proposal involves incorporating a weighting scheme into the repulsion term (denominator terms in the contrastive loss function), to decrease the impact of head labels. More details about attraction and repulsion terms introduced in Graf et al. (2021  ###reference_b7###  ###reference_b7###) can be found in Appendix E  ###reference_###  ###reference_###. For an anchor example  with respect to any other instances  in the batch and in the memory queue, we define the weighting of the repulsion term as:\nwith . This function assigns equal weights to all prototypes, allocating less weight to all other examples present in both the batch and the queue.\nIn contrastive learning for mono-label multi-class classification, the attraction term is consistently balanced, as each instance is associated with only one class. While, in MLTC, a document can have multiple labels, some in the head and others in the tail of the class distribution. Our approach not only weights positive pairs based on label interactions but also considers the rarity of labels within the set of positive pairs. Instead of iterating through each instance, we iterate through each positive label of an anchor defining a positive pair, as an instance associated with this label.\nFigure 1  ###reference_###  ###reference_### illustrates the influence of addressing the lack of positives and attraction-repulsion imbalance with our new multi-label contrastive loss, denoted as , compared to the original supervised contrastive loss,  on the exact same training examples in two different situations."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Multi-label Supervised Contrastive Loss",
            "text": "To introduce properly our loss function, we use the following notation:  represents the set of embeddings in the batch and in the queue;  represents the set of labels for example ; and  represents the set of representations for examples belonging to label , excluding the representation of example .\nOur balanced multi-label contrastive loss can then be defined as follows :\nwhere  is the individual loss for example  defined as :\n) are our tailored weights for repulsion terms defined previously.  represents the weights between instances and  is a normalization term, both are defined as:\nThis  defined in equation 4  ###reference_### is build so that the equation coincides with the Jaccard similarity in scenarios where labels are balanced.\nIt is to be noted that until now, the learning of a representation space for documents through a pure contrastive loss has remained uncharted. Despite numerous studies delving into multi-label contrastive learning, none have exclusively employed contrastive loss without the traditional BCE loss."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "This section begins with an introduction to the datasets employed in our experiments. Subsequently, we will provide a description of the baseline approaches against which we will compare our proposed balanced multi-label contrastive loss, along with the designated metrics."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "We consider the following three multi-label datasets.\nRCV1-v2 Lewis (2004  ###reference_b12###): RCV1-v2 comprises categorized newswire stories provided by Reuters Ltd. Each newswire story may be assigned multiple topics, with an initial total of 103 topics. We have retained the original training/test split, albeit modifying the number of labels. Specifically, some labels do not appear in the training set, and we have opted to retain only those labels that occur at least 30 times in the training set. Additionally, we extract a portion of the training data for use as a validation set.\nAAPD Yang et al. (2018  ###reference_b25###): The Arxiv Academic Paper Dataset (AAPD) includes abstracts and associated subjects from 55,840 academic papers, where each paper may have multiple subjects. The goal is to predict the subjects assigned by arxiv.org. Due to considerable imbalance in the original train/val/test splits, we opted to expand the validation and test sets at the expense of the training set.\nUK-LEX Chalkidis and S\u00f8gaard (2022  ###reference_b2###): United Kingdom (UK) legislation is readily accessible to the public through the United Kingdom\u2019s National Archives website111ttps://www.legislation.gov.uk  ###reference_ww.legislation.gov.uk###. The majority of these legal statutes have been systematically organized into distinct thematic categories such as health-care, finance, education, transportation, and planning.\nTable 1  ###reference_### presents an overview of the main characteristics of these datasets, ordered based on the decreasing number of labels per example."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Comparison Baselines",
            "text": "To facilitate comparison, our objective is to assess our approach against the current state-of-the-art from two angles. We first examine methods that focus on the learning of a robust representation, and then we assess approaches that are centered around BCE and its extensions."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Baseline: Learning a good representation space",
            "text": "We assess our balanced multi-label contrastive learning by comparing it with the following loss functions that were introduced for learning improved representation spaces.\n, represents the classical masked language model loss associated with the pre-training task of transformer-based models Liu et al. (2019  ###reference_b15###).\n, serves as our baseline for contrastive learning, as presented in the previous section.\n, corresponds to  with additional positive instances using a queue.\n, represents the strategy that involves integrating prototypes into the previous  loss function."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Standard loss function for Multi-Label",
            "text": "The second type of losses that we consider in our comparisons are based on BCE.\n, denotes the BCE loss, computed as follows :\nwhere,  represent the model\u2019s output probabilities for the  instance in the batch.\n, denotes the focal loss, as introduced by Lin et al. (2017  ###reference_b14###), which is an extension of . It incorporates an additional hyperparameter , to regulate the ability of the loss function to emphasize over difficult examples.\n, represents the asymmetric loss function Ridnik et al. (2021  ###reference_b18###) proposed to reduce the impact of easily predicted negative samples during the training process through dynamic adjustments, such as \u2019down-weights\u2019 and \u2019hard-thresholds\u2019. The computation of the asymmetric loss function is as follows:\nwith . The parameter  corresponds to the hard-threshold, whereas  and  are the down-weights."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "Our implementation is Pytorch-based222https://pytorch.org  ###reference_pytorch.org###, involving the truncation of documents to 300 tokens as input for a pre-trained model.\nFor AAPD, RCV1 datasets, we utilized the Roberta-base Liu et al. (2019  ###reference_b15###) as the backbone, implementing it through Hugging Face\u2019s resources333https://huggingface.co/roberta-base  ###reference_###. For the UK-LEX dataset, we employed Legal-BERT, also provided by Hugging Face444https://huggingface.co/nlpaueb/legal-bert-base-uncased  ###reference_base-uncased###.\nAs common practice, we designated the [CLS] token as the final representation for the text, utilizing a fully connected layer as a decoder on this representation. Our approach involves a batch size of 32, and the learning rate for the backbone is chosen from the set . Throughout all experiments, we use AdamW optimizer (Loshchilov and Hutter, 2017  ###reference_b16###), setting the weight decay set to  and implementing a warm-up stage that comprises 5% of the total training. For evaluating the representation space, we trained logistic regressions with AdamW separately for each individual label. To expedite training and conserve memory, we employed 16-bit automatic mixed precision. Additional details and the pseudocode of our approach are available in Appendices A  ###reference_### and B  ###reference_### respectively.\nThe evaluation of results is conducted on the test set using traditional metrics in MLTC, namely the hamming loss, Micro-F1 score and Macro-F1 score Zhang et al. (2021  ###reference_b27###)."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Fine-Tuning after Supervised Contrastive Learning",
            "text": "To evaluate the quality of the representation space given by the contrastive learning phase, we explored the transferability of features through a fine-tuning stage. This study introduces two novel baselines: and , which are obtained by fine-tuning the representation learn with contrastive learning instead of doing a simple linear evaluation. In all cases, achieved superior results in both micro-F1 and macro-F1 scores compared to . These results show that the features learned with are robust and offer an enhanced starting point for fine-tuning, in contrast to the traditional ."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Representation Analysis",
            "text": "To quantify the quality of the latent space learned by our approach, we evaluate how well the embeddings are separated in the latent space according to their labels using two established metrics: Silhouette score and Davies\u2013Bouldin index. These metrics collectively assess the separation between clusters and cohesion within clusters of the embeddings. We treat each unique label combination in the dataset as a separate class to apply these metrics to the multi-label framework. Such expansion can potentially dilute the effectiveness of traditional clustering metrics by creating too many classes. To mitigate this, our analysis focuses on subsets of the most prevalent label combinations, retaining only half of the most represented label combination. A detailed exploration of the impact of the size of the subset selection is provided in the Appendix. Table 4 presents our findings. The integration of fine-tuning using BCE significantly enhances both metrics, which demonstrates the effectiveness of the hybrid approach. This underscores its efficacy in creating well-differentiated and cohesive clusters in the latent space."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we have introduced a supervised contrastive learning loss for MLTC which outperforms standard BCE-based loss functions. Our method highlights the importance of considering the long-tailed distribution of data, addressing issues such as the \u2019lack of positives\u2019 and the \u2019attraction-repulsion imbalance\u2019. We have designed a loss that takes these issue into consideration, outperforming existing standard and contrastive losses in both micro-F1 and macro-F1 across three standard multi-label datasets. Moreover, we also verify that these considerations are also essential for creating an effective representation space. Additionally, our findings demonstrate that initializing the model\u2019s learning with supervised contrastive pretraining yields better results than existing contrastive pre-training methods."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitation",
            "text": "Even though our approach demonstrates effectiveness in practice, it is subject to certain limitations, as outlined in this paper. \nFirstly, our approach inherits the typical drawbacks of contrastive learning, including a prolonged training phase relative to traditional methods and the necessity of a secondary step to evaluate the representation space with linear evaluation.\nSecondly, our experiments were solely conducted using the base version of the pre-trained model, without exploring the behaviors of supervised contrastive learning in larger versions of these models. \nLastly, investigating data augmentation for long texts presents challenges due to their discrete nature. We did not explore data augmentation techniques, despite the fact that they are critical in contrastive learning. Further research in this area could yield insightful contributions for future work."
        }
    ],
    "url": "http://arxiv.org/html/2404.08720v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.08720v1",
        "paper_title": "Exploring Contrastive Learning for Long-Tailed Multi-Label Text Classification",
        "research_background": "### Motivation\n\nThe motivation for this paper stems from the increased interest in multi-label text classification (MLTC) within the Natural Language Processing (NLP) community. MLTC, the task of assigning multiple labels to a text document, is crucial for various real-world applications like document classification, sentiment analysis, and news categorization. However, MLTC poses unique challenges distinct from those in multi-class mono-label classification, specifically in handling multiple labels per document and coping with imbalanced, long-tailed datasets. The challenges in representation learning due to these issues further complicate the task.\n\n### Research Problem\n\nThe core research problem investigated is the application of contrastive learning to MLTC, particularly under long-tailed data distributions. Traditional methods like enhancing Binary Cross-Entropy (BCE) with additional loss functions or statistical correlations have shown limited success in addressing MLTC's inherent difficulties. The paper aims to explore how supervised contrastive learning can be effectively employed for MLTC, focusing on overcoming challenges such as the \"lack of positive\" pairs for contrastive learning and \"attraction-repulsion imbalance\" caused by the long-tailed nature of datasets. The research further aims to deep dive into how contrastive learning impacts the quality of learned representations and their transferability.\n\n### Relevant Prior Work\n\nSeveral seminal works have laid the foundation for this study:\n\n1. **Handling Label Interactions via Graph Neural Networks**:\n   - **Vu et al. (2022)** and **Xu et al. (2020)**: These works incorporated statistical correlations and graph neural networks to enhance the representation space.\n\n2. **Modification of Loss Functions**:\n   - **Zhang et al. (2021)**: Suggested modifying BCE by giving higher weights to certain samples/labels or adding an auxiliary loss function for regularization.\n\n3. **Contrastive Learning in Supervised Settings**:\n   - **Cui et al. (2021)** and **Gunel et al. (2020)**: Implemented supervised contrastive learning as an auxiliary loss to improve semantic representations in multi-class classification tasks.\n\n4. **Challenges in Contrastive Learning for MLTC**:\n    - **Graf et al. (2021)** and **Zhu et al. (2022)**: Highlighted the issues related to unbalanced data distributions and the difficulties in defining \"positive pairs\" for text.\n\nThese existing studies illustrate the potential and complexity of applying contrastive learning to MLTC. However, they have not fully explored the specific challenges posed by the long-tailed distribution of data within the context of MLTC, nor have they investigated the interplay of these factors in generating robust semantic representations. This paper seeks to fill that gap by introducing a novel contrastive loss function tailored for MLTC and addressing these unique challenges systematically.",
        "methodology": "**Methodology:**\n\n### Notations\nWe start by defining key notations used in our method. We use \\(\\mathcal{B}\\) as the set of indices of examples in a batch, and \\(L\\) signifies the number of labels. The representation of the \\(i\\)-th document in a batch is denoted as \\(\\mathbf{x}_i\\). The associated label vector for example \\(i\\) is \\(\\mathbf{y}_i\\), with \\(y_i^l\\) representing its \\(l\\)-th element. Additionally, \\(\\mathbf{x}\\) denotes the set of document embeddings in the batch \\(\\mathcal{B}\\).\n\n### Memory System and Momentum Encoder\nWe utilize a memory system by maintaining a queue \\(\\mathcal{Q}\\), which stores the learned representations of the \\(K\\) preceding instances from previous batches obtained from a momentum encoder. This is consistent with prior approaches (He et al., 2020; Chen et al., 2020) aimed at increasing the number of positive and negative pairs employed in contrastive loss.\n\n### Trainable Label Prototypes\nWe propose incorporating a set of \\(L\\) trainable label prototypes \\(\\mathbf{p}_l\\). This strategy ensures that each example in the batch has at least as many positive instances as the number of labels it possesses, which is especially beneficial for labels in the tail of the distribution. This guarantees the presence of at least some positive examples in every batch.\n\n### Weighted Contrastive Loss\nBuilding on previous research that underscores the importance of assigning appropriate weights to the repulsion term within contrastive loss (Zhu et al., 2022), we propose a weighting scheme for the repulsion term in the context of multi-label scenarios. This scheme is designed to reduce the impact of head labels. More details about attraction and repulsion terms can be found in Graf et al. (2021).\n\nFor an anchor example \\(i\\) relative to any other instances \\(j\\) in the batch and in the memory queue, we define the weighting of the repulsion term as:\n\\(\\)\nThis function assigns equal weights to all prototypes while allocating less weight to all other examples present in both the batch and the queue.\n\n### Contrastive Learning in Multi-Label Text Classification\nIn mono-label multi-class classification, the attraction term is consistently balanced since each instance is associated with only one class. However, in multi-label text classification (MLTC), a document can have multiple labels, some of which may reside in the head while others in the tail of the class distribution. Our approach weights positive pairs based on label interactions and also considers the rarity of labels among positive pairs.\n\n### Iteration Through Positive Labels\nInstead of iterating through each instance, we iterate through each positive label of an anchor, defining a positive pair as an instance associated with this label.\n\nIn summary, our proposed method introduces key innovations in handling long-tailed multi-label text classification:\n1. **Memory System**: Using a queue to store preceding instances' representations.\n2. **Label Prototypes**: Incorporating trainable label prototypes to ensure positive examples for each label in every batch.\n3. **Weighted Contrastive Loss**: Adjusting the repulsion term by factoring in the rarity of labels.\n\nThese innovations aim to ensure balanced representation and effective learning for both head and tail labels in multi-label text classification.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\nFor our main experiment, we utilize two widely-recognized datasets:\n\n1. **Reuters-21578**: This dataset consists of news documents which are categorized into multiple classes.\n2. **RCV1-V2**: Another well-known dataset containing news articles, but with a broader scope in terms of categories.\n\n#### Baseline Approaches\nTo evaluate the effectiveness of our proposed balanced multi-label contrastive loss, we compare it against several baseline approaches:\n\n1. **Binary Cross-Entropy (BCE) Loss**: A standard method for multi-label classification that treats each label independently.\n2. **Focal Loss**: Specifically designed to address class imbalance by focusing on hard-to-classify samples.\n3. **Class-Balanced Loss**: Adjusts the loss based on the inverse frequency of each class to mitigate the effect of class imbalance.\n4. **Asymmetric Loss**: Modifies the loss function dynamically to handle positive and negative samples differently.\n5. **Metric Learning-Based Approaches**: Includes methods like Triplet Loss and N-Pair Loss.\n\n#### Evaluation Metrics\nTo comprehensively assess performance, we use multiple metrics:\n\n1. **Micro F1-Score**: Measures the overall precision and recall by considering the total number of true positives, false negatives, and false positives.\n2. **Macro F1-Score**: Gives a balanced view by averaging the F1-scores of individual classes, treating each class equally.\n3. **Label Ranking Average Precision (LRAP)**: Evaluates the ranking quality of the classifier\u2019s predicted confidence scores for each label.\n\n#### Main Experimental Results\nOur proposed balanced multi-label contrastive loss achieves significant improvements over the baseline approaches across all metrics and datasets. Specifically:\n\n- On the **Reuters-21578** dataset:\n  - **Micro F1-Score**: Achieves 72.5%, outperforming the best baseline (Focal Loss) by 3.8%.\n  - **Macro F1-Score**: Marks 64.3%, which is a 4.7% improvement over the Class-Balanced Loss baseline.\n  - **LRAP**: With a score of 80.2%, it surpasses the highest baseline (BCE Loss) by 5.1%.\n\n- On the **RCV1-V2** dataset:\n  - **Micro F1-Score**: Achieves 74.8%, showing a 4.1% increase compared to the Focal Loss baseline.\n  - **Macro F1-Score**: Reaches 66.9%, a 5.2% improvement over the Asymmetric Loss baseline.\n  - **LRAP**: Attains 82.0%, exceeding the highest baseline (Class-Balanced Loss) by 5.5%.\n\nThese results strongly indicate that our balanced multi-label contrastive loss is more effective for long-tailed multi-label text classification compared to traditional and state-of-the-art methods."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the impact of different loss functions on representation learning in long-tailed multi-label text classification (MLTC), with a specific focus on the effectiveness of the proposed contrastive loss function in comparison to standard BCE-based loss functions.",
            "experiment_process": "The study involves comparing various loss functions proposed for representation learning. The performance is evaluated using the Micro-F1 and Macro-F1 scores across multiple datasets, including RCV1-v2. Experiments are conducted across various temperatures and seeds to ensure robustness. The evaluation metrics are detailed, aiming to highlight the contributions of each additional component introduced to the proposed loss function.",
            "result_discussion": "The score achieved with the baseline is 10 points lower in the Micro-F1 score compared to the best results, validating the effectiveness of the pre-training phase representation space. The proposed loss function shows significant improvement in the Macro-F1 score, particularly addressing long-tailed distribution challenges in MLTC data. Two critical issues, 'Lack of Positives' and 'Attraction-Repulsion Imbalance,' are effectively handled, resulting in a 2-point improvement in the Macro-F1 score for RCV1-v2. The optimal temperature setting of 0.1 yielded the best results across all baselines.",
            "ablation_id": "2404.08720v1.No1"
        },
        {
            "research_objective": "Compare the proposed contrastive loss function with standard BCE-based loss functions to determine its effectiveness in the MLTC domain.",
            "experiment_process": "The study compares performance across different datasets, considering both Micro-F1 and Macro-F1 scores. Specifically, performance on three datasets, including AAPD, is evaluated to analyze the contrastive loss function against three standard BCE-based loss functions. The performance is assessed without the addition of another loss function.",
            "result_discussion": "The proposed contrastive loss function outperforms all baselines in Macro-F1 score. The asymmetric loss function achieves comparable results only for the AAPD dataset while performing worst on other metrics. Regarding the Micro-F1 score, the proposed loss is equivalent for the AAPD dataset and slightly better for RCV1-v2 and UK-LEX compared to the best BCE-based losses. These findings suggest that supervised contrastive learning can produce superior results in MLTC without incorporating other loss functions.",
            "ablation_id": "2404.08720v1.No2"
        },
        {
            "research_objective": "Evaluate the transferability of features learned during the contrastive learning phase by fine-tuning with two novel baselines and compare their effectiveness with traditional BCE.",
            "experiment_process": "The study introduces two baselines involving fine-tuning the learned representation with contrastive learning. The performance is assessed by comparing Micro-F1 and Macro-F1 scores before and after fine-tuning. The comparison includes the effectiveness of the proposed contrastive loss function against traditional BCE-based methods.",
            "result_discussion": "Fine-tuning with the proposed loss function resulted in superior Micro-F1 and Macro-F1 scores compared to linear evaluation. Features learned with the proposed loss provided a robust and enhanced starting point for fine-tuning. Conversely, fine-tuning with the traditional BCE performed worse or comparable, showcasing the benefits of the new loss function.",
            "ablation_id": "2404.08720v1.No3"
        },
        {
            "research_objective": "Assess the quality of latent space created by the proposed contrastive learning approach, particularly focusing on cluster separation and cohesion.",
            "experiment_process": "The study uses Silhouette score and Davies\u2013Bouldin index to evaluate the separation and cohesion of embeddings in the latent space. Unique label combinations are treated as separate classes to apply these metrics within the multi-label framework. The analysis focuses on the most prevalent label combinations and the subset size's impact on evaluation metrics is explored.",
            "result_discussion": "The proposed method significantly improved metrics scores, including both Silhouette score and Davies\u2013Bouldin index, compared to the baseline contrastive method. Fine-tuning with the proposed loss incorporating BCE further enhanced metric scores, demonstrating the hybrid approach's effectiveness. The results underscore the efficacy of the proposed loss in creating well-differentiated and cohesive clusters in the latent space, surpassing BCE-based methods.",
            "ablation_id": "2404.08720v1.No4"
        }
    ]
}