{
    "title": "Can Perplexity Reflect Large Language Model\u2019s Ability in Long Text Understanding?",
    "abstract": "Recent studies have shown that Large Language Models (LLMs) have the potential to process extremely long text. Many works only evaluate LLMs\u2019 long-text processing ability on the language modeling task, with perplexity (PPL) as the evaluation metric. However, in our study, we find that there is no correlation between PPL and LLMs\u2019 long-text understanding ability. Besides, PPL may only reflect the model\u2019s ability to model local information instead of catching long-range dependency. Therefore, only using PPL to prove the model could process long text is inappropriate. The local focus feature of PPL could also explain some existing phenomena, such as the great extrapolation ability of the position method ALiBi. When evaluating a model\u2019s ability in long text, we might pay more attention to PPL\u2019s limitation and avoid overly relying on it.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "INTRODUCTION",
            "text": "Recently, many researchers (Chen et al., 2023b  ###reference_b3###; a  ###reference_b2###; Xiong et al., 2023  ###reference_b12###; Ding et al., 2023  ###reference_b5###; Chen et al., 2023c  ###reference_b4###) have proposed various approaches to scale up the context window of LLMs to more than 100k. Since there is not a comprehensive benchmark tailored for the evaluation of such extremely long text understanding, such as question answering (QA) over 100K tokens, researchers use perplexity (PPL), an evaluation metric for language modeling 111The definition and calculation method of PPL is shown in Appendix A.1  ###reference_###. A lower PPL shows a higher accuracy of a model in long-text language modeling, to demonstrate the model\u2019s ability to process long text (Chen et al., 2023c  ###reference_b4###; Ding et al., 2023  ###reference_b5###; Liu et al., 2023  ###reference_b8###; Peng et al., 2023  ###reference_b9###).\nHowever, only given LLMs are excellent in language modeling, can it indicate LLMs\u2019 ability to understand long text? We conduct experiments on three long context window LLM variants to figure out this. We use several available benchmarks of downstream tasks, such as QA and summerization, to evaluate their long-text understanding ability. Surprisingly, the models\u2019 performance on language modeling is inconsistent with their performance on most downstream tasks, implying the PPL can not be a good indicator of the model\u2019s long-text understanding ability.\nWe speculate that the phenomenon above may be because PPL is a reflection of the model\u2019s ability to model local information. We use LLaMA2, which only has a short context window of 4,096 and cannot handle long context, to prove our speculation. The experiment results show that, LLaMA2 delivers comparable PPL with the long context window LLMs. The feature of PPL in reflecting local information modeling ability can also explain why methods such as ALiBi (Press et al., 2022  ###reference_b10###), which makes the model mainly focus on local information, could enable models to extrapolate to longer inference sequences while keeping the PPL at a low level."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Lower PPL  Understanding Long Text Better",
            "text": "We compare models' performance in several downstream tasks to determine their long-text understanding ability. We choose three model variants with context windows longer than 100K tokens for experiments: 1) YARN-7B-128K, 2) Yi-6B-200K, and 3) LongLoRA-7B-100K.\n\nFor downstream tasks, we use two public benchmarks, QMSUM and NarrativeQA, to evaluate the models\u2019 performance in long question answering and long document summarization. Additionally, following Li et al., we use a finer-grained line retrieval test to evaluate models\u2019 retrieval ability. The experiment details are shown in Appendix A.1 and Appendix A.2.\n\nThe results are shown in Table 1. LongLoRA outperforms other models on all downstream tasks."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "CONCLUSION",
            "text": "PPL can be an effective evaluation metric for long-text language modeling ability, but not for long-text understanding. A model without the ability to understand long text can also effectively use local information to model a long text. Considering PPL can not be a good indicator for long text understanding ability, except for using PPL to evaluate a model, we call for more diversified evaluation metrics for long text processing ability from multiple aspects."
        }
    ],
    "url": "http://arxiv.org/html/2405.06105v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "3"
        ],
        "main_experiment_and_results_sections": [
            "2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "3"
        ]
    },
    "research_context": {
        "paper_id": "2405.06105v1",
        "paper_title": "Can Perplexity Reflect Large Language Model\u2019s Ability in Long Text Understanding?",
        "research_background": "**Paper's Motivation:**\nThe paper is motivated by the recent advancements in scaling up the context window of large language models (LLMs) to handle texts extending over 100k tokens. Despite these advancements, there is a lack of a comprehensive benchmark tailored specifically for evaluating the models' understanding of such extremely long texts in downstream tasks like question answering (QA). The common practice among researchers is to use perplexity (PPL) as a metric for this purpose. This paper challenges the validity of PPL as an indicator of long-text understanding and seeks to systematically investigate its relevance.\n\n**Research Problem:**\nThe core research problem addressed in the paper is whether perplexity (PPL), a metric traditionally used for evaluating language modeling accuracy, actually reflects the ability of LLMs to understand and perform well on long-text understanding tasks. The inconsistency between models' performance on language modeling (measured by PPL) and their performance on downstream tasks suggests that PPL might not be a reliable indicator of an LLM's capability in long-text understanding.\n\n**Relevant Prior Work:**\n1. **Chen et al., 2023b**; **Chen et al., 2023a**: These works focus on scaling up the context window of LLMs to more than 100k tokens, although they are not specifically benchmarked for long-text understanding tasks.\n  \n2. **Xiong et al., 2023**; **Ding et al., 2023**; **Chen et al., 2023c**: Similar to the previous works, these also contribute to the development of LLMs with extended context windows and utilize PPL as a performance metric for language modeling over long texts.\n\n3. **Liu et al., 2023**; **Peng et al., 2023**: These researchers highlight the effectiveness of low PPL in reflecting a model's accuracy in language modeling. However, the studies stop short of establishing a direct correlation between PPL and long-text understanding.\n\n4. **Press et al., 2022**: This work discusses the ALiBi approach, where models are designed to focus on local information while maintaining low PPL, allowing for extrapolated sequences. This method's success in lowering PPL without necessarily enhancing long-text comprehension supports the paper's hypothesis that PPL reflects local information modeling rather than long-text understanding.\n\nBy addressing these gaps, the paper seeks to determine a more appropriate metric or method for evaluating LLMs' capabilities in comprehending and processing extremely long texts in practical applications.",
        "methodology": "**Methodology:**\n\nIn this study, we investigate whether the perplexity (PPL) metric, commonly used in language modeling, reflects large language models' capability in understanding long texts. To achieve this, we compare the performance of three model variants with context windows exceeding 100K tokens:\n\n1. **YARN-7B-128K (Peng et al., 2023)** \n2. **Yi-6B-200K** \n3. **LongLoRA-7B-100K (Chen et al., 2023c)**\n\n**Experiments:**\n\n1. **Perplexity Calculation:**\n   - For each model, we calculate the PPL using an input length of 76K tokens, which is the maximum our machine can handle before running into memory limitations.\n   - The models are tested on 50 books randomly sampled from the test set of PG-19 (Rae et al., 2019).\n\n2. **Downstream Tasks:**\n   - Two public benchmarks are used to evaluate the models in downstream tasks:\n     - **QMSUM (Zhong et al., 2021)** for long question answering.\n     - **NarrativeQA (Ko\u010disk\u00fd et al., 2017)** for long document summarization.\n   - Additionally, a finer-grained line retrieval test based on the methodology described by Li et al. (2023) is employed to gauge the models' retrieval abilities. Details on these experiments are found in Appendices A.1 and A.2.\n\n**Results:**\n\n- **Language Modeling (LM):**\n  - The PPL scores reported for the models are:\n    - YARN: 1.878 \n    - Yi: 2.069\n    - LongLoRA: 2.002\n  - These results indicate YARN has the best performance in language modeling based on PPL.\n\n- **Downstream Tasks:**\n  - Despite YARN achieving the lowest PPL, it does not perform best in downstream tasks.\n  - LongLoRA outperforms the other models in all downstream task evaluations.\n\n**Conclusion:**\n\nThe analysis indicates that while PPL is an effective measure for evaluating language modeling, it does not necessarily correlate with a model\u2019s ability to understand long texts. This finding suggests that relying solely on PPL might be insufficient for assessing a model's capacity for long-text comprehension, thus necessitating the use of additional metrics and evaluation methods.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup:\n- **Objective:** To assess if perplexity (PPL) in language modeling correlates with long-text understanding ability in large language models.\n- **Model Variants:** \n  1. **YARN-7B-128K** (Peng et al., 2023)\n  2. **Yi-6B-200K** (referenced GitHub repository)\n  3. **LongLoRA-7B-100K** (Chen et al., 2023c)\n- **Datasets:**\n  - **Language Modeling:** PPL calculated using 50 books randomly sampled from the test set of PG-19 (Rae et al., 2019). The maximum input length is 76K tokens due to machine limitations.\n  - **Downstream Tasks:**\n    - **Long Question Answering:** QMSUM (Zhong et al., 2021)\n    - **Long Document Summarization:** NarrativeQA (Ko\u010disk\u00fd et al., 2017)\n    - **Line Retrieval Test:** Evaluation of models' retrieval ability (based on Li et al., 2023).\n\n#### Evaluation Metrics:\n- **Language Modeling Performance:** Perplexity (PPL)\n- **Downstream Task Performance:** Specific task-related metrics (e.g., accuracy, F1-score), not detailed in the provided text.\n\n#### Results:\n- **Language Modeling:**\n  - **YARN-7B-128K:** Lowest PPL of 1.878\n  - **Yi-6B-200K:** PPL of 2.069\n  - **LongLoRA-7B-100K:** PPL of 2.002\n  - **Inference:** YARN demonstrates the best language modeling ability (lowest PPL).\n\n- **Downstream Task Performance:**\n  - **Best Overall Performance:** LongLoRA-7B-100K outperformed YARN and Yi on all downstream tasks.\n  \n#### Conclusion:\n- **Inconsistency:** YARN's lowest PPL did not translate to the best performance in downstream tasks. This discrepancy indicates that while PPL is a suitable metric for evaluating language modeling, it does not effectively indicate a model's ability to understand long texts."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Compare whether a lower Perplexity (PPL) score in language modeling indicates better long-text understanding ability in large language models (LLMs).",
            "experiment_process": "The study involved comparing the performance of three model variants (YARN-7B-128K, Yi-6B-200K, LongLoRA-7B-100K) whose context windows are greater than 100K tokens. The PPL was calculated with a 76K input length on 50 books randomly sampled from the test set of PG-19. Additionally, two public benchmarks, QMSUM and NarrativeQA, were used to evaluate long question answering and long document summarization abilities. A finer-grained line retrieval test was also included for assessing retrieval ability.",
            "result_discussion": "YARN provided the lowest PPL of 1.878, compared to Yi's 2.069 and LongLoRA's 2.002, indicating YARN's better language modeling ability. However, LongLoRA performed the best on all downstream tasks. This inconsistency suggests that while PPL may be a fair evaluation metric for language modeling, it is not a reliable indicator of long-text understanding ability.",
            "ablation_id": "2405.06105v1.No1"
        },
        {
            "research_objective": "Investigate whether PPL might reflect a model's local language modeling ability rather than comprehensive long-text understanding.",
            "experiment_process": "LLaMA2-7B was evaluated by calculating the PPL on the same test corpus used in the initial experiment. The model could only use the previous 4K tokens to predict the next token, limiting it to local information. Details of the experiment are provided in Appendix A.1.",
            "result_discussion": "LLaMA2 achieved a PPL of 1.935, lower than Yi and LongLoRA, despite only using local information. This result confirms that a model with limited ability to understand long texts can still achieve a low PPL, suggesting that PPL may reflect LLMs' local information handling abilities rather than their long-text understanding. The finding helps explain the extrapolation capability of ALiBi, where focus on local information even with longer input leads to consistently low PPL.",
            "ablation_id": "2405.06105v1.No2"
        }
    ]
}