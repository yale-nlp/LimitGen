{
    "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
    "abstract": "Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry.\nThe majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based.\nWhile these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time.\nThe rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide \u201ccommon sense\u201d reasoning that existing planners are missing.\nResearchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios.\nLarge language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment.\nWhile this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor.\nWe argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model\u2019s decision, and detect when it may be hallucinating.\nIn this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "It is an exciting time to be a machine learning and robotics researcher.\nA great deal of progress has been made in the last decade and a half with regards to the efficacy and efficiency of models for perception, decision-making, planning, and control Soori et al. (2023  ###reference_b119###); Janai et al. (2020  ###reference_b53###).\nBroadly speaking, approaches to these problems fall under one of two umbrellas: hand-engineered model-based systems and data-driven learning-based models Formentin et al. (2013  ###reference_b36###).\nWith some deployment scenario in mind, developers may hand-engineer rules Hayes-Roth (1985  ###reference_b43###) or tune a controller Borase et al. (2021  ###reference_b6###) to be tested, or in the case of learning-based models, collect training data and craft some reward function to fit a model to an objective, given said data Henderson et al. (2018  ###reference_b45###).\nIn practice, these methods work particularly well in the scenarios that they were specifically designed and trained for, but may produce undesirable results in previously unseen out-of-distribution cases Wen et al. (2023  ###reference_b133###).\nDesigners may choose to add more rules, re-tune their controller, fine-tune their model to a more representative dataset, fix the reward function to handle edge cases, or even add a detector (which may itself be rule-based or data-driven) at test-time to identify out-of-distribution scenarios before calling on the decision-maker Singer and Cohen (2021  ###reference_b116###); Schreiber et al. (2023  ###reference_b108###); Chakraborty et al. (2023  ###reference_b11###).\nHowever, even with these changes, there will always be other situations that designers had not previously considered which will come about during deployment, leading to sub-optimal performance or critical failures.\nFurthermore, the modifications made to the model may have unforeseen effects at test-time like undesired conflicting rules Ekenberg (2000  ###reference_b33###) or catastrophic forgetting of earlier learned skills Kemker et al. (2018  ###reference_b58###).\nInformally, classical methods and data-driven approaches lack some form of common sense that humans use to adapt in unfamiliar circumstances Fu et al. (2023a  ###reference_b38###).\nMore recently, researchers are exploring the use of large (visual) language models, L(V)LMs, to fill this knowledge gap Cui et al. (2024  ###reference_b23###).\nThese models are developed by collecting and cleaning an enormous natural language dataset, pre-training to reconstruct sentences on said dataset, fine-tuning on specific tasks (e.g., question-answering), and applying human-in-the-loop reinforcement learning to produce more reasonable responses Achiam et al. (2023  ###reference_b1###).\nEven though these models are another form of data-driven learning that attempt to maximize the likelihood of generated text conditioned on a given context, researchers have shown that they have the ability to generalize to tasks they have not been trained on, and reason about their decisions.\nAs such, these foundation models are being tested in tasks like simulated decision-making Huang et al. (2024b  ###reference_b50###) and real-world robotics Zeng et al. (2023  ###reference_b149###) to take the place of perception, planning, and control modules.\nEven so, foundation models are not without their limitations.\nSpecifically, these models have a tendency to hallucinate, i.e., generate decisions or reasoning that sound plausible, but are in fact inaccurate or would result in undesired effects in the world.\nThis phenomenon has led to the beginning of a new research direction that attempts to detect when L(V)LMs hallucinate so as to produce more trustworthy and reliable systems.\nBefore these large black-box systems are applied in safety-critical situations, there need to be methods to detect and mitigate hallucinations.\nThus, this survey collects and discusses current hallucination mitigation techniques for foundation models in decision-making tasks, and presents potential research directions.\nExisting surveys particularly focus on presenting methods for hallucination detection and mitigation in question-answering (QA) Ji et al. (2023  ###reference_b55###); Rawte et al. (2023  ###reference_b104###); Zhang et al. (2023d  ###reference_b155###); Ye et al. (2023  ###reference_b145###) or object detection tasks Li et al. (2023c  ###reference_b73###).\nThere are also other works that provide examples of current use cases of L(V)LMs in autonomous vehicles Yang et al. (2023b  ###reference_b141###) and robotics Zeng et al. (2023  ###reference_b149###); Zhang et al. (2023a  ###reference_b151###).\nWang et al. (2023a  ###reference_b127###) perform a deep analysis of the trustworthiness of a variety of foundation models and Chen and Shu (2024  ###reference_b13###) provide a taxonomy of hallucinations within LLMs, but both exclude applications to general decision problems.\nTo the best of our knowledge, we are the first to propose a general definition of hallucinations that can be flexibly tuned to any particular deployment setting, including commonly found applications to QA or information retrieval, and more recent developments in planning or control.\nFurthermore, there is no existing work that summarizes state of the art methods for hallucination detection and mitigation approaches within decision-making and planning tasks.\nIn the remainder of this work, we discuss the current uses of foundation models for decision-making tasks in Section 2  ###reference_###, define and provide examples of hallucinations in Section 3  ###reference_###, identify current detection methods and where they are evaluated in Sections 4  ###reference_### and 5  ###reference_### respectively, and explore possible research directions in Section 6  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Foundation Models Making Decisions",
            "text": "Originally coined by Bommasani et al. (2022  ###reference_b5###), the term foundation models refers to models that are \u201ctrained on broad data at scale such that they can be adapted to a wide range of downstream tasks.\u201d\nThis approach is in contrast to works that design and train models on a smaller subset of data for the purpose of being deployed to a specific task Yang et al. (2024  ###reference_b139###).\nThe key difference is that foundation models undergo a pre-training procedure on a large-scale dataset containing information from a variety of possible deployment fields, through which they are expected to learn more general features and correspondences that may be useful at test-time on a broader set of tasks Zhou et al. (2023  ###reference_b158###); Zhao et al. (2023  ###reference_b156###).\nExamples of existing pre-trained foundation models span language Devlin et al. (2019  ###reference_b25###); Brown et al. (2020  ###reference_b7###); Touvron et al. (2023a  ###reference_b123###), vision Caron et al. (2021  ###reference_b10###); Oquab et al. (2024  ###reference_b91###); Kirillov et al. (2023  ###reference_b61###), and multi-modal Radford et al. (2021  ###reference_b101###); Achiam et al. (2023  ###reference_b1###) inputs.\nIn this section, we give a brief overview of existing use cases for foundation models in robotics, autonomous vehicles, and other decision-making systems.\nWe also succinctly point out hallucinations found in these works and leave a lengthier discussion in Section 3.2  ###reference_###.\nReaders should refer to works from Yang et al. (2023b  ###reference_b141###), Zeng et al. (2023  ###reference_b149###), and Zhang et al. (2023a  ###reference_b151###) for a deeper review of application areas."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robotics",
            "text": "Foundation models have also been used in the robotics domain for object detection, affordance prediction, grounding, navigation, and communication. Ichter et al. (2023) are motivated by the issue of misalignment between the capabilities of a robot and what an LLM believes it is capable of performing. Because LLMs may not specifically be trained with data from the robot it is to be deployed on, there is a gap in the model\u2019s understanding and the true capacity of the robot, which could lead to hallucinated generations that cannot feasibly be used at runtime. The authors propose SayCan as a method to combine the general knowledge of LLMs with the specific capabilities of a robot in the real-world. Specifically, an LLM is given a task in text form, and is asked to output a list of smaller actions to take in order to complete said task successfully. To constrain the LLM to generate possible actions available to the robot, they assume access to (1) the probability distribution of next tokens to generate from the model, and (2) a set of available skills on the robot, with which they compute the probability of the LLM generating each of the skills next. SayCan greedily selects the action that has the highest product of the next token probability from the LLM and the probability of the action actually successfully being executed in the environment, until the model predicts it has completed the task. Rather than relying purely on textual context, PaLM-E, proposed by Driess et al. (2023), is a multi-modal model that converts various sensor inputs (e.g., images) to a token-space embedding that is combined with instruction embeddings to be input to a PaLM LLM Chowdhery et al. (2023). PaLM is used to either answer questions about the surroundings of the robot, or to plan a sequence of actions to perform to complete a task. Driess et al. (2023) further acknowledge that the multi-modality of their PaLM-E architecture leads to increased risk of hallucinations. Inspired by recent promising findings in using foundation models to generate programs Chen et al. (2021), other works deploy foundation models to write low-level code to be run on robots. Liang et al. (2023) present Code as Policies, which uses LLMs to hierarchically generate interactive code and functions that can be called. As the model writes main code to be run on a robot given an instructive prompt of the task from the user, it identifies functions to call within the higher level code to complete the task successfully. The authors show that LLMs can leverage third party libraries for existing functions, or develop their own library of functions dynamically with custom methods for the task. While the functionality of Code as Policies can be tested easily for low-level skill definitions, longer multi-step problems require testing whether all requested conditions have been met by running the generated code on the robot. As such, Hu et al. (2024) propose the RoboEval performance benchmark for testing robot-agnostic LLM-generated code. Specifically, the CodeBotler platform provides an LLM access to abstract functions like \u201cpick,\u201d \u201cplace,\u201d and \u201cget_current_location\u201d that have the same external interface regardless of the robot to be deployed on. Like Code as Policies, CodeBotler is provided a text instruction from the user and generates code to be tested. Then the RoboEval benchmark uses RoboEval Temporal Logic (RTL) to test whether the generated code meets task and temporal ordering constraints provided in the original prompt. Furthermore, they test the robustness of the LLM by passing in several paraphrased prompts to check for consistency across inputs. We discuss similar consistency-checking strategies for identifying hallucinations in decision-making tasks further in Section 4.3.1. In the space of robot navigation, LM-Nav leverages a VLM and attempts to predict a sequence of waypoints for a robot to follow and visit landmarks described within a language command Shah et al. (2023). Here, the authors use in-context learning Dong et al. (2023) to teach GPT-3 to extract desired landmarks from a natural language instruction. Assuming there are images of the possible landmarks the robot can navigate to in its environment, LM-Nav uses CLIP Radford et al. (2021) to predict the closest matching pairs of extracted landmark descriptions and waypoint images. Finally, dynamic programming is applied on the complete graph of the environment to optimize the path of landmarks to visit. The overall predicted path is optimized to maximize the likelihood of successfully completing the instruction input to the model."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Other Areas",
            "text": "There are also other works that apply foundation models for decision-making outside of the robotics and autonomous vehicle domains. For example, ReAct from Yao et al. (2023b ###reference_b144###) identifies that a key limitation of chain-of-thought reasoning Wei et al. (2022 ###reference_b131###) is that the model does not update its context or action based on observations from an environment. As such, chain-of-thought reasoning relies purely on the internal reasoning of the foundation model itself to predict actions to take, missing a crucial step in grounding its actions with their effects on the environment. Given a prompt, ReAct iterates between an internal reasoning step and acting in the environment to build up context relevant to the task. Yao et al. (2023b ###reference_b144###) showcase the promise of the method in a QA setting where the LLM can take actions to query information from an external knowledge base, as well as an interactive text-based game, ALFWorld Shridhar et al. (2021 ###reference_b114###). Chen et al. (2023b ###reference_b14###) admit that ReAct is a powerful tool for dynamic reasoning and grounding, but is limited by the fact that the updated context from the Act step is only helpful for the particular task the model is currently deployed for. They propose Introspective Tips to allow an LLM to reason about its past successes and failures in a world to generate general tips that will be helpful across diverse instruction-following tasks. Specifically, tips are generated from the past experience of the model from a similar set of tasks, from expert demonstrations, and from several games that differ from the target task. By summarizing these experiences into more concise tips, Chen et al. (2023b ###reference_b14###) show that Introspective Tips outperform other methods in ALFWorld with both few- and zero-shot contexts. Park et al. (2023 ###reference_b94###) and Wang et al. (2023b ###reference_b128###) apply foundation models in more complex environments to push models to their limits to simulate realistic human behaviors and test lifelong learning. Park et al. (2023 ###reference_b94###) propose generative agents that produce believable, human-like interactions and decisions within a small town sandbox environment. They develop a module for individual agents in the simulation to store and retrieve memories, reflect about past and current experiences, and interact with other agents. Their generative agents use similar methods to ReAct and Introspective Tips to act based on a memory of experiences, but also interact and build relationships with other agents through dialogue. The authors show that the agents are able to effectively spread information, recall what has been said to others and stay consistent in future dialogue interactions, and coordinate events together. Sometimes, however, agents are found to hallucinate and embellish their responses with irrelevant details that may be attributed to the training dataset of outside, real-world knowledge. Voyager, from Wang et al. (2023b ###reference_b128###), deploys GPT-4 to the MineDojo environment Fan et al. (2022 ###reference_b35###) to test its in-context lifelong learning capabilities. The architecture prompts GPT-4 to generate next high-level tasks to complete, given the agent\u2019s current state and results of past tasks \u2014 a form of automatic curriculum generation. Voyager then identifies what intermediate general skills would be required to complete the task, and the LLM is used to fill in a skill library with helpful low-level skills in the form of programs that call functions that are available to the simulator. GPT-4 is prompted to generate skills that are generalizable to multiple tasks, so that the skill generation step does not have to be called for every task if the skill is already stored in the library. Wang et al. (2023b ###reference_b128###) show that Voyager continuously learns to explore the diverse tech tree available within MineDojo while building and leveraging skills. Even so, they find that the LLM hallucinates when generating tasks to tackle and when writing the code to execute for a particular skill, discussed further in Section 3.2 ###reference_###. Kwon et al. (2023 ###reference_b66###) explore the use of LLMs to act as a proxy for a hand-tuned reward function in RL tasks. This application is particularly motivated by decision-making tasks that are difficult to specify with a reward function, but can be explained textually with preferences of how a policy should generally act. Specifically, the LLM evaluator first undergoes in-context learning with examples of how it should decide the reward in several cases of the task that the agent will be deployed to. Then, during RL training, the LLM is provided a prompt with the trajectory of the agent within the episode, the resulting state from the simulator, and the original task objective from"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Hallucinations",
            "text": "Even with all their success on a multitude of deployment areas, foundation models still produce inconsistent outputs, or hallucinate, at test-time.\nHere, we provide a general definition for hallucinations that can be applied to any foundation model deployment task, including various autonomous systems.\nAdditionally, we give examples of hallucinations encountered in literature, and discuss how they come about during testing.\nAs discussed in Section 2.1  ###reference_###, Wen et al. (2023  ###reference_b133###) test GPT-4V on the autonomous driving task and identify failure modes.\nRegardless of the weather and driving conditions, GPT-4V has difficulty detecting and identifying the traffic light state at an intersection, until the image has zoomed in on the light itself.\nIt also presents additional irrelevant (or completely false) details about other agents, when the prompt had no mention of them in the first place.\nFurthermore, the model also has difficulty in describing temporal sequences (i.e., videos) and categorizing images by their direction within a panoramic view from the vehicle\u2019s perspective.\nIn their later work, Wen et al. (2024  ###reference_b132###) describe that hallucinations arise in these complex environments because of the high variability in driving scenarios.\nEven after applying hallucination mitigation techniques like chain-of-thought reasoning, the model is not free of these undesired outputs.\nA similar work evaluating the frequency at which LVLMs hallucinate in their descriptions of images, finds that these models\u2019 outputs may include non-existent objects, or additional irrelevant phrases (that may not even be possible to test for accuracy) Li et al. (2023c  ###reference_b73###).\nFor example, in a picture of food on a table, an LVLM hallucinates a non-existent beverage, and predicts that the \u201ctable is neatly arranged, showcasing the different food items in an appetizing manner.\u201d\nAlthough the classification error and irrelevant generation in this example are not critical, earlier works warn of possible failures with more severe, high societal impact (e.g., biases in models leading to marginalizing users) Bommasani et al. (2022  ###reference_b5###).\nChen et al. (2021  ###reference_b16###) explore alignment failures of LLMs applied to code completion tasks.\nThe authors evaluate the likelihood of these models generating defective code given different input prompts, and discover that in-context learning using examples with buggy code has a higher chance of resulting in poor generations from the model on the actual task at hand.\nThe study also identifies similar model biases towards race, gender, religion, and other representations.\nFurthermore, the authors find that their model, Codex, is able to generate code that could assist with developing insecure applications or malware, albeit in a limited manner.\nThese findings have been corroborated by other foundation model code generation works in the robotics domain.\nFor example, Wang et al. (2023b  ###reference_b128###) describe that Voyager sometimes generates code with references to items that do not exist within MineDojo.\nSimilarly, Hu et al. (2024  ###reference_b48###) find that their model has the tendency to call functions with invalid objects or locations, pickup objects when it is already holding something, ask for help when no one is near, and other undesired behaviors.\nSeveral works focus on identifying cases of hallucinations in QA tasks.\nAlthough this application area is not the direct focus of this work, we present examples of hallucinations in this field as we can glean similar failure modes that could arise within decision-making systems.\nCommon hallucinations in QA result in incorrect answers to questions.\nFor example, Achiam et al. (2023  ###reference_b1###) find that GPT-4 \u201challucinates facts and makes reasoning errors.\u201d\nAchiam et al. (2023  ###reference_b1###) categorize these failures into closed-domain (given context, the model generates irrelevant information that was not in the context) and open-domain (the model outputs incorrect claims without any context) hallucinations.\nAfter fine-tuning on more data with a hallucination mitigation objective, the model reduces its tendency to hallucinate, but still does not achieve perfect accuracy \u2014 a similar trend encountered by Touvron et al. (2023a  ###reference_b123###).\nAnother set of works identify hallucinations with contradictions among several sampled generations from an LLM, discussed further in Section 4.3.1  ###reference_.SSS1### M\u00fcndler et al. (2024  ###reference_b89###); Zhang et al. (2023b  ###reference_b152###).\nIntuitively, if a context passed into a model results in conflicting generations, the model must be hallucinating some part of the output.\nNotice in this example, with relation to Definition 3.1  ###reference_definition1###, self-contradiction works test for consistency among multiple (hallucinated) generations, rather than with respect to a ground-truth knowledge-base that usually exists in QA tasks.\nAs such, our definition can flexibly apply to different system setups by describing consistency, desired behavior, and relevancy respectively."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "What are hallucinations?",
            "text": "Across current literature on foundation models, there exist similar patterns and themes that can be used to develop a unified definition for hallucinations.\nWith the majority of works studying this problem within QA tasks, where ground truth answers are available, several authors explain hallucinations as producing text that includes details/facts/claims that are fictional/misleading/fabricated rather than truthful or reliable Rawte et al. (2023  ###reference_b104###).\nWorks making use of a dedicated knowledge-base further describe hallucinations as generating nonsensical or false claims that are unsubstantiated or incorrectly cited M\u00fcndler et al. (2024  ###reference_b89###); Chen et al. (2023a  ###reference_b12###); Zhang et al. (2023b  ###reference_b152###); Li et al. (2023b  ###reference_b71###).\nVarshney et al. (2023  ###reference_b126###) also present the idea that foundation models may sound syntactically correct, or coherent, while simultaneously being incorrect.\nGallifant et al. (2024  ###reference_b40###), who perform a peer review of the GPT-4 technical paper, state that hallucinations include responses that are irrelevant to the original prompt.\nLi et al. (2023c  ###reference_b73###), who specifically explore hallucinations of LVLMs in detecting and classifying objects within images, define hallucinations as generating object descriptions inconsistent with target images.\nA common theme among existing hallucination definitions for QA, information retrieval, and image captioning domains is that, while the generation may sound coherent, either the output is incorrect, or the model\u2019s reasoning behind the generated text is incorrect.\nHowever, we find these characteristics on their own do not completely encompass the hallucinations found in decision-making tasks in literature, thus requiring additional nuances.\nWithin papers that apply foundation models to decision-making tasks specifically, researchers have encountered similar problems of hallucinations impacting performance.\nPark et al. (2024  ###reference_b93###) describe hallucinations as predicting an incorrect feasibility of an autonomous system when generating an explanation behind the uncertainty of an action to take.\nSimilarly, Kwon et al. (2023  ###reference_b66###) find that language models may provide incoherent reasoning behind their actions.\nWang et al. (2024  ###reference_b129###) and Ren et al. (2023  ###reference_b105###) believe that these generative models also have a sense of high (false) confidence when generating incorrect or unreasonable plans.\nIn the case of robot navigation and object manipulation, Hu et al. (2024  ###reference_b48###) and Liang et al. (2024  ###reference_b75###) refer to hallucinations as attempting to interact with non-existent locations or objects.\nMetric\nQuesting-Answering\nImage Captioning\nPlanning\nControl\nConsistency\nGenerations must align with database facts\nObjects in description must appear in image\nPredicted sub-task must be feasible to solve\nPredicted action must be possible to perform\nDesired Behavior\nTone of answer should be informative\nCensor descriptions for inappropriate images\nPlans should maximize expected return\nPredict actions to complete plan efficiently\nRelevancy\nAnswers should not include references to unrelated topics\nDescriptions should not be embellished with details that cannot be confirmed\nPredicted sub-tasks and actions should not stray from the end goal with unnecessary steps\nPlausibility\nGeneration is syntactically sound and believable\nGenerated plan is reasonable and seems to attempt to accomplish goal\nIn the code generation task, Chen et al. (2021  ###reference_b16###) use the term \u201calignment failure,\u201d with similar effects to those of hallucinations discussed above.\nMore specifically, the authors informally describe an alignment failure as an outcome where a model is capable of performing a task, but chooses not to.\nIf a model is able to complete a task successfully within its latent space (perhaps through additional prompt engineering or fine-tuning), one may ask, \u201cWhy would the model choose not to?\u201d\nAs foundation models are trained with the next-token reconstruction objective on a training set, they attempt to maximize the likelihood of the next token appearing at test-time as well.\nConsequently, if the test-time prompt includes even minor mistakes, Chen et al. (2021  ###reference_b16###) find that LLMs will continue to generate buggy code to match the input prompt.\nThis issue is further described in Section 3.3  ###reference_###.\nWe realize existing definitions for hallucinations are extremely disparate depending on the deployment area.\nAs such, to bridge existing QA application areas, decision-making tasks, and all other possible test scenarios for foundation models, we combine these findings and define the term hallucination as follows:\nA hallucination is a generated output from a model that conflicts with constraints or deviates from desired behavior in actual deployment, or is completely irrelevant to the task at hand, but could be deemed syntactically plausible under the circumstances.\nThere are three key pieces to this definition:\nA generated output from a model.\nA deployment scenario to evaluate model outputs with any of the following:\nA list of constraints that must be consistent within the generation.\nA loose interpretation of a desired behavior the generation should meet.\nA set of topics relevant to the task.\nMetrics measuring consistency, desirability, relevancy, and syntactic soundness (plausibility) of generations.\nIn practice, this definition generally encapsulates the qualities of hallucinations discussed earlier.\nFor example, in QA or object detection tasks, one may define a set of relevant topics that a generation should not stray from, and constraints may be held in the form a knowledge-base of ground truth facts.\nThe desired behavior of the generation may be to be phrased in an informative manner, rather than sarcastic.\nOn the other hand, in robot manipulation settings, a developer may have a set of constrained actions feasible on the robot, and the desired behavior could be to complete a task with as few actions as possible.\nRelevancy may be measured in relation to the specific task to be deployed on (e.g., a prompt requesting a recipe to make pasta would find it irrelevant if the model also suggested a song to play while cooking).\nFinally, plausibility informally relates to a measure of how believable an output is to a critic.\nA more realistic generation has a greater chance of deceiving the user into trusting the model, even when the plan may be hallucinated.\nOverall, hallucinated outputs may contain one or more of the core characteristics (inconsistent, undesired, irrelevant, and plausible) simultaneously, and our definition can be flexibly applied to any deployment scenario in mind by choosing metrics for each characteristic, respectively.\nWe show more examples of applying our definition to various tasks in Table 1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Examples",
            "text": "As discussed in Section 2.1  ###reference_###  ###reference_###, Wen et al. (2023  ###reference_b133###  ###reference_b133###) test GPT-4V on the autonomous driving task and identify failure modes.\nRegardless of the weather and driving conditions, GPT-4V has difficulty detecting and identifying the traffic light state at an intersection, until the image has zoomed in on the light itself.\nIt also presents additional irrelevant (or completely false) details about other agents, when the prompt had no mention of them in the first place.\nFurthermore, the model also has difficulty in describing temporal sequences (i.e., videos) and categorizing images by their direction within a panoramic view from the vehicle\u2019s perspective.\nIn their later work, Wen et al. (2024  ###reference_b132###  ###reference_b132###) describe that hallucinations arise in these complex environments because of the high variability in driving scenarios.\nEven after applying hallucination mitigation techniques like chain-of-thought reasoning, the model is not free of these undesired outputs.\nA similar work evaluating the frequency at which LVLMs hallucinate in their descriptions of images, finds that these models\u2019 outputs may include non-existent objects, or additional irrelevant phrases (that may not even be possible to test for accuracy) Li et al. (2023c  ###reference_b73###  ###reference_b73###).\nFor example, in a picture of food on a table, an LVLM hallucinates a non-existent beverage, and predicts that the \u201ctable is neatly arranged, showcasing the different food items in an appetizing manner.\u201d\nAlthough the classification error and irrelevant generation in this example are not critical, earlier works warn of possible failures with more severe, high societal impact (e.g., biases in models leading to marginalizing users) Bommasani et al. (2022  ###reference_b5###  ###reference_b5###).\nChen et al. (2021  ###reference_b16###  ###reference_b16###) explore alignment failures of LLMs applied to code completion tasks.\nThe authors evaluate the likelihood of these models generating defective code given different input prompts, and discover that in-context learning using examples with buggy code has a higher chance of resulting in poor generations from the model on the actual task at hand.\nThe study also identifies similar model biases towards race, gender, religion, and other representations.\nFurthermore, the authors find that their model, Codex, is able to generate code that could assist with developing insecure applications or malware, albeit in a limited manner.\nThese findings have been corroborated by other foundation model code generation works in the robotics domain.\nFor example, Wang et al. (2023b  ###reference_b128###  ###reference_b128###) describe that Voyager sometimes generates code with references to items that do not exist within MineDojo.\nSimilarly, Hu et al. (2024  ###reference_b48###  ###reference_b48###) find that their model has the tendency to call functions with invalid objects or locations, pickup objects when it is already holding something, ask for help when no one is near, and other undesired behaviors.\nSeveral works focus on identifying cases of hallucinations in QA tasks.\nAlthough this application area is not the direct focus of this work, we present examples of hallucinations in this field as we can glean similar failure modes that could arise within decision-making systems.\nCommon hallucinations in QA result in incorrect answers to questions.\nFor example, Achiam et al. (2023  ###reference_b1###  ###reference_b1###) find that GPT-4 \u201challucinates facts and makes reasoning errors.\u201d\nAchiam et al. (2023  ###reference_b1###  ###reference_b1###) categorize these failures into closed-domain (given context, the model generates irrelevant information that was not in the context) and open-domain (the model outputs incorrect claims without any context) hallucinations.\nAfter fine-tuning on more data with a hallucination mitigation objective, the model reduces its tendency to hallucinate, but still does not achieve perfect accuracy \u2014 a similar trend encountered by Touvron et al. (2023a  ###reference_b123###  ###reference_b123###).\nAnother set of works identify hallucinations with contradictions among several sampled generations from an LLM, discussed further in Section 4.3.1  ###reference_.SSS1###  ###reference_.SSS1### M\u00fcndler et al. (2024  ###reference_b89###  ###reference_b89###); Zhang et al. (2023b  ###reference_b152###  ###reference_b152###).\nIntuitively, if a context passed into a model results in conflicting generations, the model must be hallucinating some part of the output.\nNotice in this example, with relation to Definition 3.1  ###reference_definition1###  ###reference_definition1###, self-contradiction works test for consistency among multiple (hallucinated) generations, rather than with respect to a ground-truth knowledge-base that usually exists in QA tasks.\nAs such, our definition can flexibly apply to different system setups by describing consistency, desired behavior, and relevancy respectively."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Why do they happen?",
            "text": "There are several speculations as to how hallucinations come about during deployment.\nFirst and foremost, like any learning task, foundation models are sensitive to biases in training data Rawte et al. (2023  ###reference_b104###).\nOnce a model is trained on a given large dataset, some facts may become out-of-date or stale at any point in time Puthumanaillam et al. (2024  ###reference_b97###).\nFurthermore, as the training set is embedded into a smaller encoding dimension, the knowledge within an L(V)LM\u2019s frozen parameters is lossy, and models cannot feasibly be fine-tuned every time there is new data Peng et al. (2023  ###reference_b95###); Elaraby et al. (2023  ###reference_b34###).\nZhang et al. (2023b  ###reference_b152###) recommend changing algorithm parameters at runtime, such as, temperature (spread of probability distribution of next token), top- sampling (narrows the set of next tokens to be considered), and beam search (choosing a set of possible beams, i.e., trajectories, of next tokens based on high conditional probabilities), but the process of tuning these parameters is expensive.\nTo combat out-of-date training data, some works provide models with an external knowledge-base of information to pull facts from, with the hope of increasing model accuracy.\nEven with this up-to-date information, Zhang et al. (2023c  ###reference_b153###) pose that there may exist a misalignment between the true capabilities of a model, and what a user believes the model is capable of, leading to poor prompt engineering.\nIn fact, poor prompting is one of the most significant causes of hallucinations.\nChen et al. (2021  ###reference_b16###) find that poor quality prompts lead to poor quality generations, in the context of code completion.\nThis phenomenon is attributed to the reconstruction training objective of LLMs attempting to maximize the likelihood of next generated tokens, given context and past outputs, i.e.,\nwhere  is a context input to the model,  is an output sequence of  tokens , and any generated token  is conditioned on  previously generated tokens.\nAs the public datasets these models are trained on contain some fraction of undesirable generations (e.g., defective code), the models become biased to generate similar results under those inputs.\nQiu et al. (2023  ###reference_b99###) show that this limitation can actually be exploited to push foundation models to generate toxic sentences, or completely lie, by simply rewording the prompt.\nWhile foundation models condition generated tokens on ground-truth text without hallucinations at train time, during inference, the model chooses future tokens conditioned on previously (possibly hallucinated) generated text.\nAs such, Chen et al. (2023d  ###reference_b17###) and Varshney et al. (2023  ###reference_b126###) state that generated outputs are more likely to contain hallucinations if prior tokens are hallucinated as well.\nFurthermore, Li et al. (2023a  ###reference_b69###) find that, even if prompt context provided to a foundation model is relevant, the model may choose to ignore the information and revert to its own (possibly outdated or biased) parameterized knowledge.\nOverall, the hallucination detection task is highly complex with several possible sources of failures that need to be considered at test-time.\nChen and Shu (2024  ###reference_b13###) validate the complexity of the detection problem with studies identifying that human- and machine-based detectors have higher difficulty correctly classifying misinformation generated from LLMs than those written by other people."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Detection and Mitigation Strategies",
            "text": "Hallucination detection and mitigation methods can be classified into three types (white-, grey-, and black-box) depending on the available inputs to the algorithm.\nGenerally, given some context, a foundation model outputs a predicted sequence of tokens, the corresponding probabilities of each token, and embeddings of the generation from intermediate layers in the network.\nWhite-box hallucination detection methods assume access to all three output types, grey-box require token probabilities, and black-box only need the predicted sequence of tokens.\nBecause not all foundation models provide access to their hidden states, or even the output probability distribution of tokens (e.g., the ChatGPT web interface), black-box algorithms are more flexible during testing.\nIn this section, we present existing detection and mitigation approaches clustered by input type.\nWhile several of these works show promise in QA and object detection settings, many of them require further validation on decision-making tasks, and we will point out these methods as they come about.\nWorks in this section are summarized in Table 2  ###reference_###."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "White-box Methods",
            "text": "Methods in this section require access to internal weights of the model for hallucination detection."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 Attention Weights",
            "text": "Attention weight matrices, which are prominent within transformer model architectures, signify the importance the model places on earlier tokens within a generation when predicting future tokens. OPERA, proposed by Huang et al. (2024a ###reference_b49###), is a hallucination detection method for LVLMs that makes use of the model\u2019s internal attention weights. When visualizing the attention matrix, the authors find that there exist peculiar column patterns that align with the beginning of a hallucinated phrase. These aggregation patterns usually occur on a non-substantial token like a period or quotation mark, but are deemed to have a large impact on the prediction of future tokens. As such, this finding led Huang et al. (2024a ###reference_b49###) to modify the beam search algorithm Freitag and Al-Onaizan (2017 ###reference_b37###) by applying a penalty term to beams wherever an aggregation pattern is detected, and roll back the search to before the pattern arises. Their method is shown to reduce hallucinations, and even eliminate possible repetitions in generations."
        },
        {
            "section_id": "4.1.3",
            "parent_section_id": "4.1",
            "section_name": "4.1.3 Honesty Alignment",
            "text": "In addition to methods that require hidden states or attention matrices, we also include methods that fine-tune foundation models to better communicate their uncertainty to questions under white-box algorithms, as they require access to model weights for training.\nFor example, Lin et al. (2022a  ###reference_b76###) collect a calibration dataset of questions and answers from GPT-3 under multiple types of tasks (e.g., add/subtract and multiply/divide), and record how often each task is incorrectly answered.\nThey aim to fine-tune the LLM to also output its certainty that the prediction is correct.\nConsequently, Lin et al. (2022a  ###reference_b76###) fine-tune the model with data pairs of a question and the empirical accuracy on the task that the question originates from in the calibration dataset, such that the model is expected to similarly output a probability of accuracy at test-time.\nThe authors show that the proposed verbalized probability in deployment does correlate with actual accuracy on the tasks.\nYang et al. (2023a  ###reference_b140###) take the method one step further by also training the model to refuse to answer questions with high uncertainty."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Grey-box Methods",
            "text": "Grey-box approaches leverage the probability distributions of tokens output from the model."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Concept Probabilities",
            "text": "Empirically, Varshney et al. (2023  ###reference_b126###) show that there is a negative correlation between hallucination rate and token probability (i.e., as a token\u2019s probability decreases within a sentence, the tendency to hallucinate increases). Thus, the authors rely on token probabilities to estimate uncertainty of concepts within a generated claim, and they check for correctness by cross-referencing a knowledge-base. Whenever a concept is found to be conflicting with a fact through verification questions, their method attempts to mitigate the error by prompting the LLM to replace the incorrect claim with the evidence. Although effective in the QA setting, Varshney et al. (2023  ###reference_b126###) concede that, in the event token probabilities are not available, some form of heuristic must be used to detect hallucination candidates. Zhou et al. (2024  ###reference_b159###) show that external models can be developed to automatically clean hallucinations. The authors tackle the issue of object hallucinations that LVLMs experience when describing the content of images. Through theoretical formulations, the authors show that LVLM responses tend to hallucinate in three settings: when described object classes appear frequently within a description, when a token output has low probability, and when an object appears closer to the end of the response. As such, their model, LURE, is a fine-tuned LVLM trained on a denoising objective with a training dataset that is augmented to include objects that appear frequently within responses, and replacing objects with low token probabilities or appearing close to the end of the response, with a placeholder tag. At inference time, tokens are augmented similarly to how they were changed to generate the training dataset, and the LURE LVLM is prompted to denoise hallucinations by filling in uncertain objects. SayCanPay, proposed by Hazra et al. (2024  ###reference_b44###), builds off of the SayCan framework Ichter et al. (2023  ###reference_b52###) to improve the expected payoff of following a plan specified by a language model. Within our hallucination definition, this goal translates to increasing the desirability of generations by improving the likelihood of the model achieving higher rewards. The authors propose three different strategies for planning: Say, SayCan, and SayCanPay. Say methods greedily choose next actions based only on token probabilities. SayCan approaches also take the success rate of the chosen action into consideration. Finally, SayCanPay additionally estimates the expected payoff from following the plan with some heuristic. Hazra et al. (2024  ###reference_b44###) learn this Pay model with regression on an expert trajectory dataset. Combining all three models together minimizes the likelihood that a generated plan contains conflicting infeasible action calls, while maximizing the efficiency of the task completion."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Black-box Methods",
            "text": "Black-box algorithms only rely on the input prompts and output predictions from the model, without making assumptions on the availability of the hidden state, nor the token probabilities."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Adversarial Prompting",
            "text": "Works specializing in adversarial prompting attempt to test the robustness of models to varying inputs that may coerce the model into producing out-of-distribution results. For example, Mehrabi et al. (2023) apply adversarial prompting to text-to-image foundation models, like Stable Diffusion Schramowski et al. (2023), to generate offensive images. With respect to Definition 3.1, their framework, FLIRT, is essentially testing the tendency of foundation models to hallucinate undesired generations in deployment. FLIRT uses an adversarial language model to predict a prompt to input to the image generator, scores the generated image for the presence of undesirable traits using an external classifier, re-prompts the adversary to produce a new instruction conditioned on the findings of the classifier, and repeatedly generates images until the adversary successfully prompts the test model to output an undesirable result. Mehrabi et al. (2023) define objective functions conditioned on the score output by external classifiers to maximize diversity of adversarial prompts and minimize toxicity so as to pass text filters that detect malicious inputs, while improving attack effectiveness. Another work from Yu et al. (2023) presents the AutoDebug framework for automatically sampling and updating several prompts for use in adversarial testing of the language model. The authors specifically explore adversarial testing under the case that the model predicts a correct response when provided relevant context, but generates an incorrect prediction when the evidence is modified. They apply two different modification approaches: replacing tokens within the context to provide incorrect facts, and adding additional relevant facts to the prompt that may make it difficult to pick out the most important details. All in all, adversarial prompting is an effective method for identifying robustness of models to unseen inputs, which can be used to develop stronger input filters or fine-tune the model for decreased hallucination tendency."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Proxy Model",
            "text": "Certain black-box works rely on an external, proxy model to detect and mitigate hallucinations.\nOne such method is used as a baseline within the SelfCheckGPT article Manakul et al. (2023).\nAs many language foundation models do not provide access to token probabilities, the authors use an open-source proxy LLM that does provide token probabilities as an estimate of the original output\u2019s probability.\nThey find that using proxy LLMs for probability estimation and hallucination detection successfully is highly variable.\nThe accuracy of detection is dependent on the complexity of the LLM itself, as well as the training data of the proxy LLM (i.e., models trained on independent datasets from the original LLM will have different generation patterns).\nWithin this section, we also include works that use an external trained classifier to detect hallucinations.\nFor example, Chen et al. (2023d) curate a dataset of QA dialogue from LLM generated responses.\nThey apply a composition of metrics to assess quality of responses, including a self-assessment from the LLM comparing the ground-truth and predicted text, human-labeled, and machine metrics (e.g., BERT score, F1 score, BLEU, etc.).\nTheir hallucination discriminator, RelD, is trained on the dataset in multiple separate phases, each using a different objective: regression, multi-class classification, and finally binary classification.\nThrough experiments, they find that RelD closely aligns with human evaluators\u2019 original predictions.\nSimilarly, Pacchiardi et al. (2024) develop a black-box lie detector for LLMs.\nIn their case, the authors hypothesize that models that output a lie will produce different behaviors in future responses, like Azaria and Mitchell (2023).\nAs such, at inference time, Pacchiardi et al. (2024) prompt the LLM with several binary questions (that may be completely unrelated to the original response) and collect yes/no answers.\nAll the responses are concatenated into a single embedding that is input to the logistic regression model to predict the likelihood that the response was untruthful.\nThe authors find that the simple detector is mostly task- and model-agnostic once trained on a single dataset."
        },
        {
            "section_id": "4.3.4",
            "parent_section_id": "4.3",
            "section_name": "4.3.4 Grounding Knowledge",
            "text": "In knowledge grounding tasks, a language model is tasked with identifying evidence from an external knowledge-base that supports claims within a summary. Although seemingly irrelevant to decision-making scenarios, similar methods to ones discussed in this section may be applied in planning tasks to identify observations that are most relevant to predicting the next action, or to generate reasoning behind a specified plan. PURR, proposed by Chen et al. (2023a), is a denoising agent, like LURE, that is trained in an unsupervised fashion given evidence from online sources, a clean (correct) summary, and a noisy (hallucinated) summary. The model learns to denoise the incorrect summary to the clean statement. During deployment, given a possibly hallucinated claim, a question generation model queries online sources for evidence about the claim, and PURR generates a cleaned version of the original summary with said evidence. Some knowledge grounding approaches prompt LLMs to generate code to directly query information from databases.\n\nLi et al. (2024) are motivated by the limitations of existing knowledge-based hallucination mitigation methods; namely that (1) they utilize a fixed knowledge source for all questions, (2) generating retrieval questions with LLMs that interface with a database is not effective because they may not be trained on the particular programming language of the database, and (3) there is no correction capability that handles error propagation between knowledge modules. Consequently, the authors propose augmenting LLMs with heterogeneous knowledge sources to assist with summary generation. Specifically, in the event that the model is found to be uncertain about its generated statement through self-contradiction, their framework, chain-of-knowledge (CoK), chooses subsets of knowledge-bases that may be helpful for answering the original question. Assuming each database has its own query generator, CoK queries for evidence, and corrects rationales between different sources iteratively. Compared to chain-of-thought reasoning, CoK consistently produces more accurate answers with its iterative corrections.\n\nAnother source of potential conflict that leads to hallucinations is misalignment between a model\u2019s capabilities and the user\u2019s beliefs about what it can do. Zhang et al. (2023c) tackle this knowledge alignment problem and categorize alignment failures into four types: Semantic \u2014 an ambiguous term maps to multiple items in a database; Contextual \u2014 the user failing to explicitly provide constraints; Structural \u2014 user provides constraints that are not feasible in the database; Logical \u2014 complex questions that require multiple queries. Their proposed MixAlign framework interacts with the user to get clarification when the LLM is uncertain about its mapping from the user query to the database. With the original query, knowledge-base evidence, and user clarifications, the LLM formats its final answer to the user.\n\nPeng et al. (2023) aim to add plug-and-play modules to an LLM to make its outputs more accurate, since these large foundation models cannot feasibly be fine-tuned whenever there is new information. Their work formulates the user conversation system as a Markov decision process (MDP) whose state space is an infinite set of dialogue states which encode the information stored in a memory bank, and whose discrete action space includes actions to call a knowledge consolidator to summarize evidence, to call an LLM prompt engine to generate responses, and to send its response to the user if it passes verification with a utility module. The proposed LLM-Augmenter has a memory storing dialogue history, evidence from the consolidator, set of output responses from an LLM, and utility module results. Its policy is trained in multiple phases with REINFORCE Williams (1992) starting with bootstrapping from a rule-based policy designed from domain experts, then learning from simulators, and finally, from real users. The authors find that access to ground-truth knowledge drastically improves QA results, and feedback from the utility module and knowledge consolidator help to provide more accurate answers to users.\n\nEvaluated in actual decision-making settings, Introspective Tips Chen et al. (2023b) provide concise, relevant information to a language planner to learn to solve more efficiently. Intuitively, summaries that collect information over all past experiences may be long and contain unnecessary information. In contrast, tips are compact information with high-level guidance that can be learned from one's own experiences, from other demonstrations, and from other tasks in a similar setting. Chen et al. (2023b) show that providing low-level trajectories is less effective than tips on simulated planning tasks. Additionally, with expert demonstrations, the LLM learns faster with a fewer number of failed trials than with just past experience alone. However, one limitation identified in the study is that the LLM underperforms in unseen, low-difficulty missions where it has issues generating general tips for zero-shot testing."
        },
        {
            "section_id": "4.3.5",
            "parent_section_id": "4.3",
            "section_name": "4.3.5 Constraint Satisfaction",
            "text": "There is also additional work in creating black-box algorithms for ensuring decision plans generated by foundation models meet user-defined goal specifications and system constraints, like their grey-box counterpart developed by Wang et al. (2024 ###reference_b129###). Because these models under test provide their results in text form, it is natural to apply formal method approaches (e.g., satisfiability modulo theory, SMT, solvers) to verify the satisfaction of generated plans. For example, Jha et al. (2023 ###reference_b54###) prompt an LLM planner with a problem formulated with first order constraints to predict a set of actions to complete the task. The output plan is input to an SMT solver to check for any infeasibilities in the program, and any counterexamples found are used to iteratively update the prompt and generate new plans. This counterexample approach is much faster than relying on combinatorial search methods that find a plan from scratch. However, the quality of generated plans and the number of iterations before a successful plan is generated are heavily dependent on the LLM generator itself, with similar reasons to the proxy-model used by Manakul et al. (2023 ###reference_b85###). Another work from Hu et al. (2024 ###reference_b48###) develops a RoboEval benchmark to test generated plans on real robots, in a black-box manner. Like Wang et al. (2024 ###reference_b129###), the authors introduce their own extension of LTL formulations, known as RTL, which specifies temporal logic at a higher, scenario-specific, level, while abstracting away constraints that are not dependent on available robot skills. RTL and LTL-NL are easier to read and define than classic LTL methods. RoboEval utilizes the provided RTL formulation of a problem, a simulator, and evaluator to systematically check whether the output meets requested goals. Furthermore, to check for robustness of the model to varied instructions, Hu et al. (2024 ###reference_b48###) hand-engineer paraphrased sentences within an offline dataset that should ideally result in the same task completion. Primary causes of failures were found to be a result of generated code syntax/runtime errors, attempting to execute infeasible actions on the robot, and failing RTL checks. Like adversarial prompting approaches, testing generated plans on robots in diverse scenarios enable researchers to design more robust systems that hallucinate less frequently at test-time."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Metrics and Evaluation Platforms",
            "text": "We now present common metrics, datasets, and simulation platforms leveraged when developing and evaluating the hallucination detection algorithms introduced in Section 4  ###reference_###.\nGiven a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model\u2019s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together.\nCHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs.\nBDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator.\nHumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty.\nHotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###).\nThe developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###).\nRavens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016\u20132021  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###), with text-based, discrete actions like \u201cMoveAhead,\u201d \u201cRotateLeft,\u201d and \u201cOpen.\u201d\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nTextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Metrics",
            "text": "Here, we list established metrics used for computing language similarity and accuracy of generated image descriptions.\nGiven a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model\u2019s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together.\nCHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 Language Similarity",
            "text": "Given a pair of responses, BERTScore computes the BERT Devlin et al. (2019  ###reference_b25###  ###reference_b25###  ###reference_b25###) embeddings of the sentences and calculates their cosine similarity.\nUsing a pre-trained BART model, which provides access to generated token probabilities, BARTScore sums over the log probability of each token generated while conditioning on context and previously output tokens.\nEssentially, BARTScore attempts to predict the quality of a generated text using BART as a proxy model.\nSummaC is a class of natural language inference models that predict entailment, contradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type.\nThe authors propose two approaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary with respect to each sentence in the document.\nLike BARTScore, GPTScore relies on a pre-trained language model with access to token probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nThe creators of AlignScore pose that two pieces of text are aligned when all information present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary classification of aligned or not, a multi-class prediction including a neutral label in addition to the binary classification labels, and a continuous score for a regression task.\nThe AlignScore metric computes a weighted score across all three prediction heads at test-time.\nOne common method of measuring uncertainty of a model\u2019s many generations is computing its entropy over all generated token probabilities.\nHowever, in cases where multiple sentences have the same semantic meaning but output different entropies, the aggregated measurement is not representative of the true uncertainty of the model.\nKuhn et al. (2023  ###reference_b63###  ###reference_b63###  ###reference_b63###) tackle this problem by clustering sentences into semantic classes and summing entropies of sentences from the same class together."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2 Object Detection",
            "text": "CHAIR, used for measuring accuracy of descriptions of images, is the ratio of the number of hallucinated objects to all the objects mentioned in the description.\nTo identify the hallucinated objects within the description, the authors assume access to ground-truth object classes in the image.\nThe authors recognize that different instructions prompting for a description of an image may lead to different responses from the model with the same semantic meaning.\nIn this case, CHAIR gives different scores to both descriptions although they are alike.\nInstead, their proposed metric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in the image, which leads to more a more stable metric across different outputs."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Offline Datasets",
            "text": "In this section, we present relevant offline datasets used for evaluating the performance of hallucination detection and mitigation techniques in driving, robotic, and QA tasks.\nBDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator.\nHumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty.\nHotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###  ###reference_b62###)."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Driving",
            "text": "BDD-X is a multi-modal driving dataset consisting of K samples (i.e., video clips), each consisting of eight images with vehicle control actions and text annotations describing the scene and justifying actions.\nThe authors augment BDD-X into a QA dataset consisting of questions that ask about the current action of the vehicle, reasoning behind the action, and predicting future control signals.\nTo incorporate other questions a user might ask about the vehicle, surroundings, and other miscellaneous queries, they prompt ChatGPT to generate further questions.\nIn total, the DriveGPT4 dataset contains K samples.\nThe nuScenes dataset contains K driving videos, each running for  seconds, collected from roads in Boston and Singapore.\nEach frame includes six different RGB camera views, GPS, annotated D bounding boxes of various object classes, and semantically labeled rader, lidar, and map representations.\nLike DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of nuScenes.\nIt includes five different types of questions including checking the existence of objects, counting instances, detecting the object being referred to, identifying the action state of an object, and comparing two objects.\nOverall, the dataset holds K QA pairs across K scenes in nuScenes.\nTalk2Car is an earlier extension of the nuScenes dataset which aims to ignite further research into developing systems that bridge the gap between passengers and an autonomous vehicle through natural language.\nAnnotators provided approximately K text commands over  videos within the nuScenes training split which refer to an object in the scene.\nWhile Talk2Car is a pioneering work for object referral in real driving scenes through natural language, each annotated instruction only refers to one object.\nAs such, Wu et al. (2023a  ###reference_b135###  ###reference_b135###  ###reference_b135###) propose a new task definition, referring multi-object tracking (RMOT), which attempts to predict all objects that are referred to within a natural language input.\nThey augment the KITTI driving dataset Geiger et al. (2012  ###reference_b41###  ###reference_b41###  ###reference_b41###) with labeled D bounding boxes around objects that are referenced within a text prompt for K images.\nNuPrompt is another RMOT-based benchmark, but applied to nuScenes and with D bounding box labels.\nIt includes K languages prompts, with most prompts referring to anywhere between one and ten objects in a scene.\nThe authors argue that, while several datasets exist for anomaly detection or identification on roads, there is a gap in explaining the reason for categorizing an object as being risky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights.\nAs such, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned on natural language.\nDing et al. (2023  ###reference_b27###  ###reference_b27###  ###reference_b27###) extend DRAMA to further include suggestions on actions the ego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct addresses two common limitations in existing driving datasets: they cover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring planning), and disregard temporal and multi-view representations.\nBuilt on top of NuScenes, the dataset provides K samples of multi-view sequences with corresponding QA pairs spanning  subtasks within perception, prediction, planning, and risk detection.\nThe authors of DriveLM curate a similar comprehensive dataset from nuScenes and the CARLA driving simulator Dosovitskiy et al. (2017  ###reference_b30###  ###reference_b30###  ###reference_b30###) with open-ended and factual questions about importance rankings of nearby vehicles, planning actions, detecting lanes, and more.\nThe authors collect a text-based QA dataset from a proprietary driving simulator, generated from ChatGPT with ground-truth observations (e.g., relative locations of detected vehicles, ego vehicle control actions, etc.) from the simulator."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Code Generation and Robotics",
            "text": "HumanEval is a set of  handwritten programs, each with a function definition, docstring, program body, and unit tests.\nThe authors find there is great promise in using LLMs for code generation, but output quality is limited by length of context and buggy examples.\nThe authors build a new code generation benchmark specifically for robot tasks with  functions focused on spatial reasoning, geometric reasoning, and controls.\nThe Language-Table dataset contains K trajectories manually annotated with K unique instructions across simulated and real-world manipulator robots.\nThe multi-modal dataset consists of video sequences, corresponding actions at each time step, and language instructions describing the policy of the robot in hindsight.\nThe authors of the CLARA method developed a dataset to identify language goals from a user that are certain, ambiguous, and infeasible.\nCollected from three different types of robots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects and people in view, a text goal, and a label of uncertainty."
        },
        {
            "section_id": "5.2.3",
            "parent_section_id": "5.2",
            "section_name": "5.2.3 Question-answering",
            "text": "HotPotQA is a question-answering benchmark with K multi-hop questions (i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia.\nThe dataset includes both questions that require finding relevant phrases from context paragraphs, and comparing two entities.\nIn contrast to HotPotQA, the developers of FEVER attempt to answer the question of whether a fact is supported by a knowledge-base.\nThe database contains K claims with annotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia articles.\nNatural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release K training and K test samples of real (anonymized) queries into the Google search engine paired with a Wikipedia page and a long and short answer annotated by a person based on said article.\nLike HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop questions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition and referencing to accurately solve.\nSeparate from the information retrieval task described in benchmarks above, Anantha et al. (2021  ###reference_b2###  ###reference_b2###  ###reference_b2###) develop a dataset, QreCC, for conversational QA.\nThey focus on reading comprehension, passage retrieval, and question rewriting tasks, with a total of K dialogues paired with K questions.\nZhao et al. (2024  ###reference_b157###  ###reference_b157###  ###reference_b157###) present a multi-model visual QA dataset of images, hallucinated descriptions, and non-hallucinated samples from the VG dataset Krishna et al. (2017  ###reference_b62###  ###reference_b62###  ###reference_b62###)."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Simulation Platforms",
            "text": "Finally, we introduce common online simulators used to test hallucination detection methods for decision-making tasks.\nThe developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###  ###reference_b130###).\nRavens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016\u20132021  ###reference_b22###  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###  ###reference_b113###), with text-based, discrete actions like \u201cMoveAhead,\u201d \u201cRotateLeft,\u201d and \u201cOpen.\u201d\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nTextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "5.3.1",
            "parent_section_id": "5.3",
            "section_name": "5.3.1 Driving",
            "text": "The developer presents a D car simulator, with driving scenarios ranging from a passing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout, parking, and more.\nAn ego vehicle can be controlled with discrete (e.g., merge left, merge right, faster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nGeared towards microscopic traffic simulation, SUMO allows researchers to design road networks, track traffic flow metrics, and control individual vehicles.\nCARLA is a D driving simulator built on top of Unreal Engine.\nExisting works benchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously Wegener et al. (2008  ###reference_b130###  ###reference_b130###  ###reference_b130###)."
        },
        {
            "section_id": "5.3.2",
            "parent_section_id": "5.3",
            "section_name": "5.3.2 Robotics",
            "text": "Ravens is a D manipulator robot (UR5e) simulator built with PyBullet Coumans and Bai (2016\u20132021  ###reference_b22###  ###reference_b22###  ###reference_b22###) with tasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task features a manipulator robot with a suction gripper sitting on a table workspace, with three camera views.\nBuilding on top of the TextWorld simulator, discussed in Section 5.3.3  ###reference_.SSS3###  ###reference_.SSS3###  ###reference_.SSS3###, ALFWorld aligns perception from the D robot simulation benchmark, ALFRED Shridhar et al. (2020  ###reference_b113###  ###reference_b113###  ###reference_b113###), with text-based, discrete actions like \u201cMoveAhead,\u201d \u201cRotateLeft,\u201d and \u201cOpen.\u201d\nProgPrompt is a benchmark of high-fidelity D data collected from a virtual home robot.\nIt includes three environments, each with  object instances.\nThese simulations are further used to create a dataset of  household robot tasks with a ground-truth set of actions to achieve each goal.\nRoboEval is a general platform for checking the correctness of code generated for a robot task.\nIt relies on a simulator, evaluator, and a set of defined tasks to perform evaluations on a simulated robot.\nWhile ProgPrompt captures more realistic scenarios in its high-fidelity D simulator, RoboEval is more tuned towards verifying code efficiently.\nMore recently, the developers of KnowNo also provide a tabletop simulator based on PyBullet, like Zeng et al. (2021  ###reference_b148###  ###reference_b148###  ###reference_b148###), for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning."
        },
        {
            "section_id": "5.3.3",
            "parent_section_id": "5.3",
            "section_name": "5.3.3 Other Simulators",
            "text": "TextWorld is a suite of text-based games that can be either hand-engineered or procedurally generated, where an agent directly receives text-based observations from an abstract world, and acts with natural language actions to complete a task.\nChevalier-Boisvert et al. (2019  ###reference_b18###  ###reference_b18###  ###reference_b18###) present a D top-down, grid-based simulator of instruction-following tasks with varying difficulty.\nSome tasks include simple navigation to a single goal, picking and placing objects with ambiguous references, and instructions that implicitly require multi-step reasoning to complete.\nThe simulator provides a partial observation of the space near the agent at every timestep.\nThe developers of MineDojo attempt to create a benchmark to test the continual learning of agents in an open-world setting.\nThey build an interface on top of Minecraft, a video game, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-base of existing Minecraft tutorials and wiki discussions.\nMineDojo includes several thousands of tasks that are more complex that earlier works (and require multi-step reasoning).\nAs such, task completion is judged with a learned LVLM, which acts like a human evaluator.\nThe authors present a multi-agent conversational simulator where agents are controlled by language models.\nUsers may set up agents with a defined backstory and provide instructions when desired.\nEach agent has access to a memory of past experiences, and generates natural language actions to go to certain areas, communicate with others, complete chores, and more."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Future Directions",
            "text": "Here, we discuss some possible future directions in hallucination detection and mitigation techniques for foundation models to improve deployments to decision-making tasks.\nMost hallucination detection approaches are currently evaluated in offline QA settings for information retrieval or knowledge alignment, as seen in Table 2  ###reference_###.\nAs foundation models are increasingly used for more complex tasks, researchers should make an effort to adapt and evaluate earlier detection/mitigation approaches that were applied to QA problems in these modern applications.\nAlthough dissimilar in practice from QA settings, planning and control problems may be formulated in such a way that enables these earlier mitigation methods to be evaluated on decision-making tasks.\nFor example, as discussed in Section 2.1  ###reference_###, Chen et al. (2023c  ###reference_b15###) treat the autonomous driving task as a QA problem, which could be naturally extended to test other QA hallucination detection methods in the same setting.\nThis evaluation may lead to greater understanding of the general limitations of these models, as we draw parallels across diverse deployments.\nWhite- and grey-box detection methods may not generally be applicable in situations where the internal state or token probabilities are unavailable from the language model.\nThus, we predict black-box approaches will take precedence in the near future, as state-of-the-art LVLMs like GPT-4V already prohibit access to probability outputs.\nHowever, current black-box methods are limited with simplistic sampling techniques to gauge uncertainty, and proxy models may not be representative of the true state of the model under test.\nWorks like FLIRT (while only applied to image generation models) showcase the promise of black-box adversarial prompting approaches in generating undesirable results from models Mehrabi et al. (2023  ###reference_b87###).\nWe argue developing more aggressive black-box adversarial generative models, which explicitly optimize for producing inputs that may perturb the system outputs, is key to identifying the limits of a foundation model\u2019s knowledge.\nCurrently, foundation models are primarily deployed to decision-making tasks that likely have some relation to its training set.\nFor example, although complex, tasks like multi-agent communication, autonomous driving, and code generation will be present in training datasets.\nOn the other hand, dynamic environments like robot crowd navigation require identifying nuances in pedestrian behaviors which the model may not have explicitly seen during training.\nPushing the limits of foundation model deployments will allow researchers to find areas for growth in other applications.\nWith the explosion of LVLMs, which allow for explicit grounding of natural language and vision modalities, further exploration should be performed in evaluating their effectiveness in decision-making systems.\nWen et al. (2023  ###reference_b133###) take a step in the right direction towards testing black-box LVLMs in offline driving scenarios, but there is still work to be done in deploying these models in online settings.\nThis direction can shed light on the long-standing debate of whether modular or end-to-end systems should be preferred in a particular deployment setting.\nIn this survey, we provide a glimpse into the progress of research into evaluating hallucinations of foundation models for decision-making problems.\nWe begin by identifying existing usecases of foundation models in decision-making applications like autonomous driving and robotics, and find several works make note of undesired hallucinated generations in practice.\nBy referencing works that encounter hallucinations across diverse domains, we provide a flexible definition for hallucinations that researchers can leverage, regardless of the deployment scenario in mind.\nFinally, we give a taxonomy of hallucination detection and mitigation approaches for decision-making problems, alongside a list of commonly used metrics, datasets, and simulators for evaluation.\nWe find that existing methods range in varying assumptions of inputs and evaluation settings, and believe there is much room for growth in general, black-box hallucination detection algorithms for foundation models."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.1\">Examples of applying Definition\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S3.Thmdefinition1\" title=\"Definition 3.1. \u2023 3.1 What are hallucinations? \u2023 3 Hallucinations \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> to different tasks.</span>\nNote that developers may choose to only define a subset of hallucination metrics for their deployment depending on evaluation preferences.\n</figcaption>\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S3.T1.3\" style=\"width:433.6pt;height:23983.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(182.5pt,-10093.4pt) scale(6.31739755691394,6.31739755691394) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.3.1.1.1.1\" rowspan=\"2\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.1.1.1.1\">Metric</p>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S3.T1.3.1.1.1.2\">Problem Setting</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.2.2\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.1.2.2.1\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.2.2.1.1\">Questing-Answering</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.1.2.2.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.2.2.2.1\">Image Captioning</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.1.2.2.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.2.2.3.1\">Planning</p>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.1.2.2.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.2.2.4.1\">Control</p>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.3.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.1\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.1.3.1.1.1\">Consistency</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.3.1.2.1\">Generations must align with database facts</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.3.1.3.1\">Objects in description must appear in image</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.3.1.4.1\">Predicted sub-task must be feasible to solve</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" id=\"S3.T1.3.1.3.1.5\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.3.1.5.1\">Predicted action must be possible to perform</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.4.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.1\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.1.4.2.1.1\">Desired Behavior</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.4.2.2.1\">Tone of answer should be informative</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.4.2.3.1\">Censor descriptions for inappropriate images</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.4.2.4.1\">Plans should maximize expected return</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.4.2.5\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.4.2.5.1\">Predict actions to complete plan efficiently</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.5.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.5.3.1\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.1.5.3.1.1\">Relevancy</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.5.3.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.5.3.2.1\">Answers should not include references to unrelated topics</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" id=\"S3.T1.3.1.5.3.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.5.3.3.1\">Descriptions should not be embellished with details that cannot be confirmed</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_l\" colspan=\"2\" id=\"S3.T1.3.1.5.3.4\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.5.3.4.1\">Predicted sub-tasks and actions should not stray from the end goal with unnecessary steps</p></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.1.6.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb\" id=\"S3.T1.3.1.6.4.1\" style=\"width:79.5pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"S3.T1.3.1.6.4.1.1\">Plausibility</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb\" colspan=\"2\" id=\"S3.T1.3.1.6.4.2\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.6.4.2.1\">Generation is syntactically sound and believable</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_l\" colspan=\"2\" id=\"S3.T1.3.1.6.4.3\" style=\"width:108.4pt;\">\n<p class=\"ltx_p ltx_align_center ltx_align_top\" id=\"S3.T1.3.1.6.4.3.1\">Generated plan is reasonable and seems to attempt to accomplish goal</p></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 1: \nExamples of applying Definition\u00a03.1 to different tasks.\nNote that developers may choose to only define a subset of hallucination metrics for their deployment depending on evaluation preferences.\n"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.56.1\">A summary of hallucination detection &amp; mitigation methods discussed in Section\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4\" title=\"4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</span>\nDeployment scenarios are split into question-answering (QA), information retrieval (IR), image captioning (IC), image generation (IG), &amp; planning (P) tasks.\nThe method ID includes the subsection the method appears in the paper and the order in which it appears in the subsection.\nBolded method IDs are deployed to decision-making tasks specifically.\nCustom datasets, custom simulators, &amp; real-world experiments for testing are abbreviated as CD, CS, &amp; RW, respectively.\n</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.54\" style=\"width:359.9pt;height:553.3pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-189.5pt,290.9pt) scale(0.487017868908862,0.487017868908862) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.54.54\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.55.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.1.1\">Modality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.2.1\">Method Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.3.1\">Method ID</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T2.54.54.55.1.4\">Application</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.5\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.5.1\">Deployment</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.54.54.55.1.6\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.54.54.55.1.6.1\">Evaluation Datasets or Simulators</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.56.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.54.54.56.2.1\">Detection</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.54.54.56.2.2\">Mitigation</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.2\" rowspan=\"8\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.1.2.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.1.1.1.2.1.1.1.1.1\" style=\"width:6.9pt;height:46.1pt;vertical-align:-19.6pt;\"><span class=\"ltx_transformed_inner\" style=\"width:46.1pt;transform:translate(-19.58pt,0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.1.1.1.2.1.1.1.1.1.1\">White-box</span>\n</span></span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.3\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.3.1\">Hidden States</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.4\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I1.i1\" title=\"item 1 \u2023 4.1.1 Hidden States \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.1.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.1.1.1.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.6\">QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.1.7\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.2.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I1.i2\" title=\"item 2 \u2023 4.1.1 Hidden States \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.1.2</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2.1\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.2.2.2.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2.4\">QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.2.2.2.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.3.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I1.i3\" title=\"item 3 \u2023 4.1.1 Hidden States \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.1.3</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.1.1\"></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.3.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.4.1\">QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.3.3.5\">DecodingTrust\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib127\" title=\"\">2023a</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.57.3\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.57.3.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.57.3.2\">TruthfulQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib77\" title=\"\">2022b</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.5.5.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.5.5.5.3.1\">Attention Weights</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.5.5.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.5.5.5.4.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I2.i1\" title=\"item 1 \u2023 4.1.2 Attention Weights \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.2.1</a></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.4.4.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.4.4.4.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.5.5.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.5.5.5.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.5.5.5\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.5.5.5.5.1\">IC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.5.5.6\">MSCOCO\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib78\" title=\"\">2014</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.58.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.58.4.1\">Visual Genome\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Krishna et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib62\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.6.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.6.6.6.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.6.6.6.2.1\">Honesty Alignment</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.6.6.6.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I3.i1\" title=\"item 1 \u2023 4.1.3 Honesty Alignment \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.3.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.6.6.6.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.6.6.6.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.6.6.6.5\">QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.6.6.6.6\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.8.8.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I3.i2\" title=\"item 2 \u2023 4.1.3 Honesty Alignment \u2023 4.1 White-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.1.3.2</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.7.7.7.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.4\">QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.8.8.8.5\">TriviaQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Joshi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib57\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.10.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.3\" rowspan=\"15\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.3.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.10.10.10.3.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.10.10.10.3.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.10.10.10.3.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.10.10.10.3.1.1.1.1.1\" style=\"width:8.9pt;height:40.6pt;vertical-align:-17.8pt;\"><span class=\"ltx_transformed_inner\" style=\"width:40.7pt;transform:translate(-15.88pt,2.92pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.10.10.10.3.1.1.1.1.1.1\">Grey-box</span>\n</span></span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.4\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.4.1\">Concept Probabilities</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.5\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.5.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I4.i1\" title=\"item 1 \u2023 4.2.1 Concept Probabilities \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.1.1</a></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.9.9.9.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.9.9.9.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.6\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.10.10.10.6.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.10.10.10.7\">HotpotQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib142\" title=\"\">2018</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.59.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.59.5.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.11.11.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.11.11.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I4.i2\" title=\"item 2 \u2023 4.2.1 Concept Probabilities \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.1.2</a></td>\n<td class=\"ltx_td\" id=\"S4.T2.11.11.11.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.11.11.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.11.11.11.4\">IC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.11.11.11.5\">MSCOCO\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib78\" title=\"\">2014</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.12.12.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.12.12.12.2.1\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I4.i3\" title=\"item 3 \u2023 4.2.1 Concept Probabilities \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.1.3</a></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.12.12.12.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.12.12.12.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.12.12.12.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.12.12.12.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.12.12.12.5\">Ravens\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib148\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.60.6\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.60.6.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.60.6.2\">BabyAI\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Chevalier-Boisvert et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib18\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.61.7\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.61.7.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.61.7.2\">VirtualHome\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Puig et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib96\" title=\"\">2018</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.14.14.14\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.14.14.14.3\" rowspan=\"9\"><span class=\"ltx_text\" id=\"S4.T2.14.14.14.3.1\">Conformal Prediction</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.14.14.14.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.14.14.14.4.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i1\" title=\"item 1 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.1</a></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.13.13.13.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.13.13.13.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.14.14.14.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.14.14.14.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.14.14.14.5\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.14.14.14.5.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.14.14.14.6\">MIMIC-CXR\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib56\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.62.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.62.8.1\">CNN/DM\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hermann et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib47\" title=\"\">2015</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.63.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.63.9.1\">TriviaQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Joshi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib57\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.16.16.16\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.16.16.16.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i2\" title=\"item 2 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.2</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.15.15.15.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.16.16.16.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.16.16.16.4\">QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.16.16.16.5\">MMLU\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hendrycks et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib46\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.18.18.18\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.18.18.18.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.18.18.18.3.1\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i3\" title=\"item 3 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.3</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.17.17.17.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.17.17.17.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.18.18.18.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.18.18.18.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.18.18.18.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.18.18.18.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.18.18.18.5\">KnowNo TableSim\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Ren et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib105\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.64.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.64.10.1\">RW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.20.20.20\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.20.20.20.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.20.20.20.3.1\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i4\" title=\"item 4 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.4</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.19.19.19.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.19.19.19.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.20.20.20.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.20.20.20.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.20.20.20.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.20.20.20.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.20.20.20.5\">KnowNo TableSim\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Ren et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib105\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.65.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.65.11.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.22.22.22\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.22.22.22.3\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I5.i5\" title=\"item 5 \u2023 4.2.2 Conformal Prediction \u2023 4.2 Grey-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.2.2.5</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.21.21.21.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.22.22.22.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.22.22.22.4\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.22.22.22.5\">CS</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.23.23.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.23.23.23.2\" rowspan=\"38\"><span class=\"ltx_text\" id=\"S4.T2.23.23.23.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.23.23.23.2.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.23.23.23.2.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T2.23.23.23.2.1.1.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T2.23.23.23.2.1.1.1.1.1\" style=\"width:6.9pt;height:43.5pt;vertical-align:-18.3pt;\"><span class=\"ltx_transformed_inner\" style=\"width:43.5pt;transform:translate(-18.26pt,0pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\" id=\"S4.T2.23.23.23.2.1.1.1.1.1.1\">Black-box</span>\n</span></span></span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.3\" rowspan=\"19\"><span class=\"ltx_text\" id=\"S4.T2.23.23.23.3.1\">Analyzing Samples</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.4\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i1\" title=\"item 1 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.23.23.23.5\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.6\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.23.23.23.7\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.25.25.25\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.25.25.25.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i2\" title=\"item 2 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.2</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.24.24.24.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.25.25.25.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.25.25.25.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.25.25.25.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.28.28.28\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.28.28.28.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.28.28.28.4.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i3\" title=\"item 3 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.3</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.26.26.26.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.26.26.26.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.27.27.27.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.27.27.27.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.28.28.28.5\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.28.28.28.5.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.28.28.28.3\">GSMK\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Cobbe et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib20\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.66.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.66.12.1\">MMLU\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hendrycks et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib46\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.67.13\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.67.13.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.30.30.30\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.30.30.30.3\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.30.30.30.3.1\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i4\" title=\"item 4 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.4</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.29.29.29.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.29.29.29.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.30.30.30.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.30.30.30.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.30.30.30.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.30.30.30.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.30.30.30.5\">SaGC\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Park et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib93\" title=\"\">2024</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.68.14\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.68.14.1\">CS</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.69.15\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.69.15.1\">RW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.32.32.32\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.32.32.32.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i5\" title=\"item 5 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.5</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.31.31.31.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.32.32.32.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.32.32.32.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.32.32.32.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.34.34.34\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.34.34.34.3\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.34.34.34.3.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i6\" title=\"item 6 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.6</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.33.33.33.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.33.33.33.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.34.34.34.2\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.34.34.34.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.34.34.34.4\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S4.T2.34.34.34.4.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.34.34.34.5\">Quest\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Malaviya et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib83\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.70.16\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.70.16.1\">MultiSpanQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Li et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib70\" title=\"\">2022</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.71.17\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.71.17.1\">FActScore\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Min et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib88\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.72.18\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.72.18.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.35.35.35\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.35.35.35.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.35.35.35.2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i7\" title=\"item 7 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.7</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.35.35.35.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.35.35.35.1.1\"></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.35.35.35.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.35.35.35.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.35.35.35.4.1\">IC/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.35.35.35.5\">MSCOCO\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lin et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib78\" title=\"\">2014</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.73.19\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.73.19.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.73.19.2\">A-OKVQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib109\" title=\"\">2022</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.74.20\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.74.20.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.74.20.2\">GQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hudson and Manning (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib51\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.36.36.36\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.36.36.36.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.36.36.36.2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I6.i8\" title=\"item 8 \u2023 4.3.1 Analyzing Samples from Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.1.8</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.36.36.36.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.36.36.36.1.1\"></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.36.36.36.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.36.36.36.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.36.36.36.4.1\">QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.36.36.36.5\">BIG-Bench\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Srivastava et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib120\" title=\"\">2023</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.37.37.37\">\n<td class=\"ltx_td\" id=\"S4.T2.37.37.37.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.37.37.37.1\">GSMK\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Cobbe et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib20\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.75.21\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.75.21.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.75.21.2\">MMLU\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hendrycks et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib46\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.38.38.38\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.38.38.38.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.38.38.38.2.1\">Adversarial Prompting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.38.38.38.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I7.i1\" title=\"item 1 \u2023 4.3.2 Adversarial Prompting \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.2.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.38.38.38.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.38.38.38.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.38.38.38.5\">IG</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.38.38.38.6\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.40.40.40\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.40.40.40.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.40.40.40.3.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I7.i2\" title=\"item 2 \u2023 4.3.2 Adversarial Prompting \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.2.2</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.39.39.39.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.39.39.39.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.40.40.40.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.40.40.40.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.40.40.40.4\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.40.40.40.4.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.40.40.40.5\">Natural Questions\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Kwiatkowski et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib65\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.76.22\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.76.22.1\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.41.41.41\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.41.41.41.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T2.41.41.41.2.1\">Proxy Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.41.41.41.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I8.i1\" title=\"item 1 \u2023 4.3.3 Proxy Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.3.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.41.41.41.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.41.41.41.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.41.41.41.5\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.41.41.41.6\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.42.42.42\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.42.42.42.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.42.42.42.2.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I8.i2\" title=\"item 2 \u2023 4.3.3 Proxy Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.3.2</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.42.42.42.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.42.42.42.1.1\"></span></td>\n<td class=\"ltx_td\" id=\"S4.T2.42.42.42.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.42.42.42.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.42.42.42.4.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.42.42.42.5\">SQuAD\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Rajpurkar et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib103\" title=\"\">2016</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.77.23\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.77.23.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.77.23.2\">HotpotQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib142\" title=\"\">2018</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.78.24\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.78.24.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.78.24.2\">TriviaQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Joshi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib57\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.43.43.43\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.43.43.43.2\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I8.i3\" title=\"item 3 \u2023 4.3.3 Proxy Model \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.3.3</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.43.43.43.1\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.43.43.43.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.43.43.43.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.43.43.43.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.44.44.44\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.44.44.44.2\" rowspan=\"9\"><span class=\"ltx_text\" id=\"S4.T2.44.44.44.2.1\">Grounding Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.44.44.44.3\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.44.44.44.3.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i1\" title=\"item 1 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.1</a></span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T2.44.44.44.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.44.44.44.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.44.44.44.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.44.44.44.5\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.44.44.44.5.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.44.44.44.6\">Natural Questions\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Kwiatkowski et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib65\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.79.25\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.79.25.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.79.25.2\">StrategyQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Geva et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib42\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.80.26\">\n<td class=\"ltx_td\" id=\"S4.T2.54.54.80.26.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.80.26.2\">QRreCC\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Anantha et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib2\" title=\"\">2021</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.46.46.46\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.46.46.46.3\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.46.46.46.3.1\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i2\" title=\"item 2 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.2</a></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.45.45.45.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.45.45.45.1.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.46.46.46.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.46.46.46.2.1\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.46.46.46.4\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.46.46.46.4.1\">IR/QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.46.46.46.5\">LC-QuAD\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Trivedi et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib125\" title=\"\">2017</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.81.27\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.81.27.1\">KQA-Pro\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Cao et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib9\" title=\"\">2022</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.82.28\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.54.54.82.28.1\">ScienceQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Lu et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib81\" title=\"\">2022</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.48.48.48\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.48.48.48.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i3\" title=\"item 3 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.3</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.47.47.47.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.48.48.48.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.48.48.48.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.48.48.48.5\">FuzzyQA\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib153\" title=\"\">2023c</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.50.50.50\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.50.50.50.3\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i4\" title=\"item 4 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.4</a></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.49.49.49.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.50.50.50.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.50.50.50.4\">IR/QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.50.50.50.5\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.51.51.51\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.51.51.51.2\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I9.i5\" title=\"item 5 \u2023 4.3.4 Grounding Knowledge \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.4.5</a></td>\n<td class=\"ltx_td\" id=\"S4.T2.51.51.51.3\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.51.51.51.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.51.51.51.4\">P</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.51.51.51.5\">TextWorld\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">C\u00f4t\u00e9 et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib21\" title=\"\">2019</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.53.53.53\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.53.53.53.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.53.53.53.3.1\">Constraint Satisfaction</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.53.53.53.4\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I10.i1\" title=\"item 1 \u2023 4.3.5 Constraint Satisfaction \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.5.1</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.52.52.52.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.53.53.53.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.53.53.53.5\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.53.53.53.6\">CD</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.54.54.54\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.54.54.54.2\"><a class=\"ltx_ref ltx_font_bold\" href=\"https://arxiv.org/html/2403.16527v1#S4.I10.i2\" title=\"item 2 \u2023 4.3.5 Constraint Satisfaction \u2023 4.3 Black-box Methods \u2023 4 Detection and Mitigation Strategies \u2023 Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art\">4.3.5.2</a></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.54.54.54.1\"></td>\n<td class=\"ltx_td ltx_border_bb\" id=\"S4.T2.54.54.54.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.54.54.54.4\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.54.54.54.5\">RoboEval\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.16527v1#bib.bib48\" title=\"\">2024</a>)</cite>\n</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>",
            "capture": "Table 2: \nA summary of hallucination detection & mitigation methods discussed in Section\u00a04.\nDeployment scenarios are split into question-answering (QA), information retrieval (IR), image captioning (IC), image generation (IG), & planning (P) tasks.\nThe method ID includes the subsection the method appears in the paper and the order in which it appears in the subsection.\nBolded method IDs are deployed to decision-making tasks specifically.\nCustom datasets, custom simulators, & real-world experiments for testing are abbreviated as CD, CS, & RW, respectively.\n"
        }
    },
    "image_paths": {},
    "references": [
        {
            "1": {
                "title": "GPT-4 Technical Report.",
                "author": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.08774.",
                "url": "https://arxiv.org/abs/2303.08774"
            }
        },
        {
            "2": {
                "title": "Open-Domain Question Answering Goes Conversational via Question Rewriting.",
                "author": "Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 520\u2013534. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.44"
            }
        },
        {
            "3": {
                "title": "Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control.",
                "author": "Anastasios N. Angelopoulos, Stephen Bates, Emmanuel J. Cand\u00e8s, Michael I. Jordan, and Lihua Lei. 2022.",
                "venue": "arXiv preprint arXiv:2110.01052.",
                "url": "https://arxiv.org/abs/2110.01052"
            }
        },
        {
            "4": {
                "title": "The Internal State of an LLM Knows When It\u2019s Lying.",
                "author": "Amos Azaria and Tom Mitchell. 2023.",
                "venue": "In Findings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 967\u2013976, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-emnlp.68"
            }
        },
        {
            "5": {
                "title": "On the Opportunities and Risks of Foundation Models.",
                "author": "Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2022.",
                "venue": "arXiv preprint arXiv:2108.07258.",
                "url": "https://arxiv.org/abs/2108.07258"
            }
        },
        {
            "6": {
                "title": "A review of PID control, tuning methods and applications.",
                "author": "Rakesh P Borase, DK Maghade, SY Sondkar, and SN Pawar. 2021.",
                "venue": "International Journal of Dynamics and Control, 9:818\u2013827.",
                "url": "https://doi.org/10.1007/s40435-020-00665-4"
            }
        },
        {
            "7": {
                "title": "Language Models are Few-Shot Learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Neural Information Processing Systems, pages 1877\u20131901, Vancouver, Canada. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf"
            }
        },
        {
            "8": {
                "title": "nuScenes: A Multimodal Dataset for Autonomous Driving.",
                "author": "Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. 2020.",
                "venue": "In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11618\u201311628. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR42600.2020.01164"
            }
        },
        {
            "9": {
                "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base.",
                "author": "Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, and Hanwang Zhang. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6101\u20136119, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.422"
            }
        },
        {
            "10": {
                "title": "Emerging Properties in Self-Supervised Vision Transformers.",
                "author": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021.",
                "venue": "In Proceedings of the 2021 IEEE/CVF Conference on International Conference on Computer Vision (ICCV), pages 9630\u20139640, Montreal, Canada. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ICCV48922.2021.00951"
            }
        },
        {
            "11": {
                "title": "Structural Attention-based Recurrent Variational Autoencoder for Highway Vehicle Anomaly Detection.",
                "author": "Neeloy Chakraborty, Aamir Hasan, Shuijing Liu, Tianchen Ji, Weihang Liang, D. Livingston McPherson, and Katherine Driggs-Campbell. 2023.",
                "venue": "In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, page 1125\u20131134, London, England. International Foundation for Autonomous Agents and Multiagent Systems.",
                "url": "https://dl.acm.org/doi/abs/10.5555/3545946.3598754"
            }
        },
        {
            "12": {
                "title": "PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions.",
                "author": "Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, and Kelvin Guu. 2023a.",
                "venue": "arXiv preprint arXiv:2305.14908.",
                "url": "https://arxiv.org/abs/2305.14908"
            }
        },
        {
            "13": {
                "title": "Can LLM-Generated Misinformation Be Detected?",
                "author": "Canyu Chen and Kai Shu. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=ccxD4mtkTU"
            }
        },
        {
            "14": {
                "title": "Introspective Tips: Large Language Model for In-Context Decision Making.",
                "author": "Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai Yang, Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan, et al. 2023b.",
                "venue": "arXiv preprint arXiv:2305.11598.",
                "url": "https://arxiv.org/abs/2305.11598"
            }
        },
        {
            "15": {
                "title": "Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving.",
                "author": "Long Chen, Oleg Sinavski, Jan H\u00fcnermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. 2023c.",
                "venue": "arXiv preprint arXiv:2310.01957.",
                "url": "https://arxiv.org/abs/2310.01957"
            }
        },
        {
            "16": {
                "title": "Evaluating Large Language Models Trained on Code.",
                "author": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021.",
                "venue": "arXiv preprint arXiv:2107.03374.",
                "url": "https://arxiv.org/abs/2107.03374"
            }
        },
        {
            "17": {
                "title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models.",
                "author": "Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, and Yanghua Xiao. 2023d.",
                "venue": "In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, page 245\u2013255, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3583780.3614905"
            }
        },
        {
            "18": {
                "title": "BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop.",
                "author": "Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2019.",
                "venue": "In Proceedings of the 7th International Conference on Learning Representations, New Orleans, LA, USA.",
                "url": "https://openreview.net/forum?id=rJeXCo0cYX"
            }
        },
        {
            "19": {
                "title": "PaLM: Scaling Language Modeling with Pathways.",
                "author": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023.",
                "venue": "Journal of Machine Learning Research, 24(240).",
                "url": "http://jmlr.org/papers/v24/22-1144.html"
            }
        },
        {
            "20": {
                "title": "Training Verifiers to Solve Math Word Problems.",
                "author": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021.",
                "venue": "arXiv preprint arXiv:2110.14168.",
                "url": "https://arxiv.org/abs/2110.14168"
            }
        },
        {
            "21": {
                "title": "TextWorld: A Learning Environment for Text-Based Games.",
                "author": "Marc-Alexandre C\u00f4t\u00e9, \u00c1kos K\u00e1d\u00e1r, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, et al. 2019.",
                "venue": "In Proceedings of the 7th Computer Games Workshop at the 27th International Conference on Artificial Intelligence, pages 41\u201375, Stockholm, Sweden. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-030-24337-1_3"
            }
        },
        {
            "22": {
                "title": "PyBullet, a Python module for physics simulation for games, robotics and machine learning.",
                "author": "Erwin Coumans and Yunfei Bai. 2016\u20132021.",
                "venue": "http://pybullet.org.",
                "url": null
            }
        },
        {
            "23": {
                "title": "A Survey on Multimodal Large Language Models for Autonomous Driving.",
                "author": "Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. 2024.",
                "venue": "In Proceedings of the 1st Workshop on Large Language and Vision Models for Autonomous Driving at the 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 958\u2013979, Waikoloa, HI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.html"
            }
        },
        {
            "24": {
                "title": "Talk2Car: Taking Control of Your Self-Driving Car.",
                "author": "Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Luc Van Gool, and Marie-Francine Moens. 2019.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2088\u20132098, Hong Kong, China. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D19-1215"
            }
        },
        {
            "25": {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, MN, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1423"
            }
        },
        {
            "26": {
                "title": "Chain-of-Verification Reduces Hallucination in Large Language Models.",
                "author": "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.",
                "venue": "arXiv preprint arXiv:2309.11495.",
                "url": "https://arxiv.org/abs/2309.11495"
            }
        },
        {
            "27": {
                "title": "HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving.",
                "author": "Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, and Xiaomeng Li. 2023.",
                "venue": "arXiv preprint arXiv:2309.05186.",
                "url": "https://arxiv.org/abs/2309.05186"
            }
        },
        {
            "28": {
                "title": "Holistic Autonomous Driving Understanding by Bird\u2019s-Eye-View Injected Multi-Modal Large Models.",
                "author": "Xinpeng Ding, Jinahua Han, Hang Xu, Xiaodan Liang, Wei Zhang, and Xiaomeng Li. 2024.",
                "venue": "arXiv preprint arXiv:2401.00988.",
                "url": "https://arxiv.org/abs/2401.00988"
            }
        },
        {
            "29": {
                "title": "A Survey on In-context Learning.",
                "author": "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023.",
                "venue": "arXiv preprint arXiv:2301.00234.",
                "url": "https://arxiv.org/abs/2301.00234"
            }
        },
        {
            "30": {
                "title": "CARLA: An Open Urban Driving Simulator.",
                "author": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017.",
                "venue": "In Proceedings of the 1st Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pages 1\u201316, Mountain View, CA, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v78/dosovitskiy17a.html"
            }
        },
        {
            "31": {
                "title": "PaLM-E: An Embodied Multimodal Language Model.",
                "author": "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023.",
                "venue": "In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 8469\u20138488, Honolulu, HI, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v202/driess23a.html"
            }
        },
        {
            "32": {
                "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate.",
                "author": "Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023.",
                "venue": "arXiv preprint arXiv:2305.14325.",
                "url": "https://arxiv.org/abs/2305.14325"
            }
        },
        {
            "33": {
                "title": "The logic of conflicts between decision making agents.",
                "author": "L Ekenberg. 2000.",
                "venue": "Journal of Logic and Computation, 10(4):583\u2013602.",
                "url": "https://doi.org/10.1093/logcom/10.4.583"
            }
        },
        {
            "34": {
                "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models.",
                "author": "Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, and Yuxuan Wang. 2023.",
                "venue": "arXiv preprint arXiv:2308.11764.",
                "url": "https://arxiv.org/abs/2308.11764"
            }
        },
        {
            "35": {
                "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge.",
                "author": "Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Neural Information Processing Systems, pages 18343\u201318362, New Orleans, LA, USA. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/74a67268c5cc5910f64938cac4526a90-Paper-Datasets_and_Benchmarks.pdf"
            }
        },
        {
            "36": {
                "title": "Model-based and data-driven model-reference control: A comparative analysis.",
                "author": "Simone Formentin, Klaske van Heusden, and Alireza Karimi. 2013.",
                "venue": "In Proceedings of the 2013 European Control Conference (ECC), pages 1410\u20131415, Zurich, Switzerland. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.23919/ECC.2013.6669388"
            }
        },
        {
            "37": {
                "title": "Beam Search Strategies for Neural Machine Translation.",
                "author": "Markus Freitag and Yaser Al-Onaizan. 2017.",
                "venue": "In Proceedings of the 1st Workshop on Neural Machine Translation, pages 56\u201360, Vancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W17-3207"
            }
        },
        {
            "38": {
                "title": "Drive Like a Human: Rethinking Autonomous Driving with Large Language Models.",
                "author": "Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. 2023a.",
                "venue": "arXiv preprint arXiv:2307.07162.",
                "url": "https://arxiv.org/abs/2307.07162"
            }
        },
        {
            "39": {
                "title": "GPTScore: Evaluate as You Desire.",
                "author": "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023b.",
                "venue": "arXiv preprint arXiv:2302.04166.",
                "url": "https://arxiv.org/abs/2302.04166"
            }
        },
        {
            "40": {
                "title": "Peer review of GPT-4 technical report and systems card.",
                "author": "Jack Gallifant, Amelia Fiske, Yulia A. Levites Strekalova, Juan S. Osorio-Valencia, Rachael Parke, Rogers Mwavu, Nicole Martinez, Judy Wawira Gichoya, Marzyeh Ghassemi, Dina Demner-Fushman, et al. 2024.",
                "venue": "PLOS Digital Health, 3(1):1\u201315.",
                "url": "https://doi.org/10.1371/journal.pdig.0000417"
            }
        },
        {
            "41": {
                "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite.",
                "author": "Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012.",
                "venue": "In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3354\u20133361, Providence, RI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR.2012.6248074"
            }
        },
        {
            "42": {
                "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies.",
                "author": "Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.",
                "venue": "Transactions of the Association for Computational Linguistics, 9:346\u2013361.",
                "url": "https://doi.org/10.1162/tacl_a_00370"
            }
        },
        {
            "43": {
                "title": "Rule-based systems.",
                "author": "Frederick Hayes-Roth. 1985.",
                "venue": "Communications of the ACM, 28(9):921\u2013932.",
                "url": "https://doi.org/10.1145/4284.4286"
            }
        },
        {
            "44": {
                "title": "SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge.",
                "author": "Rishi Hazra, Pedro Zuidberg Dos Martires, and Luc De Raedt. 2024.",
                "venue": "arXiv preprint arXiv:2308.12682.",
                "url": "https://arxiv.org/abs/2308.12682"
            }
        },
        {
            "45": {
                "title": "Deep Reinforcement Learning That Matters.",
                "author": "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018.",
                "venue": "In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pages 3207\u20133214, New Orleans, LA, USA. AAAI Press.",
                "url": "https://ojs.aaai.org/index.php/AAAI/article/view/11694"
            }
        },
        {
            "46": {
                "title": "Measuring Massive Multitask Language Understanding.",
                "author": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.",
                "venue": "In Proceedings of the 9th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=d7KBjmI3GmQ"
            }
        },
        {
            "47": {
                "title": "Teaching Machines to Read and Comprehend.",
                "author": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015.",
                "venue": "In Proceedings of the 2015 Conference on Neural Information Processing Systems, Montreal, Canada. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf"
            }
        },
        {
            "48": {
                "title": "Deploying and Evaluating LLMs to Program Service Mobile Robots.",
                "author": "Zichao Hu, Francesca Lucchetti, Claire Schlesinger, Yash Saxena, Anders Freeman, Sadanand Modak, Arjun Guha, and Joydeep Biswas. 2024.",
                "venue": "IEEE Robotics and Automation Letters, 9(3):2853\u20132860.",
                "url": "https://doi.org/10.1109/LRA.2024.3360020"
            }
        },
        {
            "49": {
                "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation.",
                "author": "Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2024a.",
                "venue": "arXiv preprint arXiv:2311.17911.",
                "url": "https://arxiv.org/abs/2311.17911"
            }
        },
        {
            "50": {
                "title": "Understanding the planning of LLM agents: A survey.",
                "author": "Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024b.",
                "venue": "arXiv preprint arXiv:2402.02716.",
                "url": "https://arxiv.org/abs/2402.02716"
            }
        },
        {
            "51": {
                "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering.",
                "author": "Drew A. Hudson and Christopher D. Manning. 2019.",
                "venue": "In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6693\u20136702. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR.2019.00686"
            }
        },
        {
            "52": {
                "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.",
                "author": "Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, et al. 2023.",
                "venue": "In Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 287\u2013318, Atlanta, GA, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v205/ichter23a.html"
            }
        },
        {
            "53": {
                "title": "Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art.",
                "author": "Joel Janai, Fatma G\u00fcney, Aseem Behl, and Andreas Geiger. 2020.",
                "venue": "Foundations and Trends in Computer Graphics and Vision, 12(1-3):1\u2013308.",
                "url": "https://doi.org/10.1561/0600000079"
            }
        },
        {
            "54": {
                "title": "Counterexample Guided Inductive Synthesis Using Large Language Models and Satisfiability Solving.",
                "author": "Sumit Kumar Jha, Susmit Jha, Patrick Lincoln, Nathaniel D. Bastian, Alvaro Velasquez, Rickard Ewetz, and Sandeep Neema. 2023.",
                "venue": "In Proceedings of the 2023 IEEE Military Communications Conference (MILCOM), pages 944\u2013949, Boston, MA, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/MILCOM58377.2023.10356332"
            }
        },
        {
            "55": {
                "title": "Survey of Hallucination in Natural Language Generation.",
                "author": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023.",
                "venue": "ACM Computing Surveys, 55(12).",
                "url": "https://doi.org/10.1145/3571730"
            }
        },
        {
            "56": {
                "title": "MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports.",
                "author": "Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. 2019.",
                "venue": "Scientific Data, 6(1):317.",
                "url": "https://doi.org/10.1038/s41597-019-0322-0"
            }
        },
        {
            "57": {
                "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.",
                "author": "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.",
                "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611, Vancouver, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P17-1147"
            }
        },
        {
            "58": {
                "title": "Measuring Catastrophic Forgetting in Neural Networks.",
                "author": "Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. 2018.",
                "venue": "In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pages 3390\u20133398, New Orleans, LA, USA. AAAI Press.",
                "url": "https://doi.org/10.1609/aaai.v32i1.11651"
            }
        },
        {
            "59": {
                "title": "Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving.",
                "author": "Ali Keysan, Andreas Look, Eitan Kosman, Gonca G\u00fcrsun, J\u00f6rg Wagner, Yu Yao, and Barbara Rakitsch. 2023.",
                "venue": "arXiv preprint arXiv:2309.05282.",
                "url": "https://arxiv.org/abs/2309.05282"
            }
        },
        {
            "60": {
                "title": "Textual Explanations for Self-Driving Vehicles.",
                "author": "Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. 2018.",
                "venue": "In Proceedings of the 15th European Conference on Computer Vision (ECCV), pages 577\u2013593, Munich, Germany. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-030-01216-8_35"
            }
        },
        {
            "61": {
                "title": "Segment Anything.",
                "author": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, et al. 2023.",
                "venue": "In Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3992\u20134003, Paris, France. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ICCV51070.2023.00371"
            }
        },
        {
            "62": {
                "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.",
                "author": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017.",
                "venue": "International Journal of Computer Vision, 123:32\u201373.",
                "url": "https://doi.org/10.1007/s11263-016-0981-7"
            }
        },
        {
            "63": {
                "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation.",
                "author": "Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.",
                "venue": "In Proceedings of the 11th International Conference on Learning Representations, Kigali, Rwanda.",
                "url": "https://openreview.net/forum?id=VD-AYtP0dve"
            }
        },
        {
            "64": {
                "title": "Conformal prediction with large language models for multi-choice question answering.",
                "author": "Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, and Andrew Beam. 2023.",
                "venue": "In Proceedings of the \u2018Neural Conversational AI Workshop - What\u2019s left to TEACH (Trustworthy, Enhanced, Adaptable, Capable and Human-centric) chatbots?\u2019 at the 40th International Conference on Machine Learning, Honolulu, HI, USA.",
                "url": "https://arxiv.org/abs/2305.18404"
            }
        },
        {
            "65": {
                "title": "Natural Questions: A Benchmark for Question Answering Research.",
                "author": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019.",
                "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
                "url": "https://doi.org/10.1162/tacl_a_00276"
            }
        },
        {
            "66": {
                "title": "Reward Design with Language Models.",
                "author": "Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023.",
                "venue": "In Proceedings of the The 11th International Conference on Learning Representations, Kigali, Rwanda.",
                "url": "https://openreview.net/forum?id=10uNUgI5Kl"
            }
        },
        {
            "67": {
                "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization.",
                "author": "Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022.",
                "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
                "url": "https://doi.org/10.1162/tacl_a_00453"
            }
        },
        {
            "68": {
                "title": "An environment for autonomous driving decision-making.",
                "author": "Edouard Leurent. 2018.",
                "venue": "https://github.com/eleurent/highway-env.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Large Language Models with Controllable Working Memory.",
                "author": "Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2023a.",
                "venue": "In Findings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 1774\u20131793, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.112"
            }
        },
        {
            "70": {
                "title": "MultiSpanQA: A Dataset for Multi-Span Question Answering.",
                "author": "Haonan Li, Martin Tomko, Maria Vasardani, and Timothy Baldwin. 2022.",
                "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1250\u20131260, Seattle, WA, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.naacl-main.90"
            }
        },
        {
            "71": {
                "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.",
                "author": "Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449\u20136464, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.397"
            }
        },
        {
            "72": {
                "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources.",
                "author": "Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2024.",
                "venue": "In The 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=cPgh4gWZlz"
            }
        },
        {
            "73": {
                "title": "Evaluating Object Hallucination in Large Vision-Language Models.",
                "author": "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. 2023c.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292\u2013305, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.20"
            }
        },
        {
            "74": {
                "title": "Code as Policies: Language Model Programs for Embodied Control.",
                "author": "Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023.",
                "venue": "In Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493\u20139500, London, England. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ICRA48891.2023.10160591"
            }
        },
        {
            "75": {
                "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty.",
                "author": "Kaiqu Liang, Zixu Zhang, and Jaime Fern\u00e1ndez Fisac. 2024.",
                "venue": "arXiv preprint arXiv:2402.06529.",
                "url": "https://arxiv.org/abs/2402.06529"
            }
        },
        {
            "76": {
                "title": "Teaching models to express their uncertainty in words.",
                "author": "Stephanie Lin, Jacob Hilton, and Owain Evans. 2022a.",
                "venue": "Transactions on Machine Learning Research.",
                "url": "https://openreview.net/forum?id=8s8K2UZGTZ"
            }
        },
        {
            "77": {
                "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods.",
                "author": "Stephanie Lin, Jacob Hilton, and Owain Evans. 2022b.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.229"
            }
        },
        {
            "78": {
                "title": "Microsoft COCO: Common Objects in Context.",
                "author": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014.",
                "venue": "In Proceedings of the 13th European Conference on Computer Vision (ECCV), pages 740\u2013755, Zurich, Switzerland. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-319-10602-1_48"
            }
        },
        {
            "79": {
                "title": "MTD-GPT: A Multi-Task Decision-Making GPT Model for Autonomous Driving at Unsignalized Intersections.",
                "author": "Jiaqi Liu, Peng Hang, Xiao Qi, Jianqiang Wang, and Jian Sun. 2023.",
                "venue": "In Proceedings of the 2023 IEEE International Conference on Intelligent Transportation Systems (ITSC), pages 5154\u20135161, Bilbao, Spain. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ITSC57777.2023.10421993"
            }
        },
        {
            "80": {
                "title": "Microscopic Traffic Simulation using SUMO.",
                "author": "Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Fl\u00f6tter\u00f6d, Robert Hilbrich, Leonhard L\u00fccken, Johannes Rummel, Peter Wagner, and Evamarie Wiessner. 2018.",
                "venue": "In Proceedings of the 2018 International Conference on Intelligent Transportation Systems (ITSC), pages 2575\u20132582, Maui, HI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ITSC.2018.8569938"
            }
        },
        {
            "81": {
                "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.",
                "author": "Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Neural Information Processing Systems, pages 2507\u20132521, New Orleans, LA, USA. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf"
            }
        },
        {
            "82": {
                "title": "Interactive Language: Talking to Robots in Real Time.",
                "author": "Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. 2023.",
                "venue": "IEEE Robotics and Automation Letters.",
                "url": "https://doi.org/10.1109/LRA.2023.3295255"
            }
        },
        {
            "83": {
                "title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations.",
                "author": "Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14032\u201314047, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.784"
            }
        },
        {
            "84": {
                "title": "DRAMA: Joint Risk Localization and Captioning in Driving.",
                "author": "Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li. 2023.",
                "venue": "In Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1043\u20131052, Waikola, HI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/WACV56688.2023.00110"
            }
        },
        {
            "85": {
                "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.",
                "author": "Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9004\u20139017, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.557"
            }
        },
        {
            "86": {
                "title": "A Language Agent for Autonomous Driving.",
                "author": "Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. 2023.",
                "venue": "arXiv preprint arXiv:2311.10813.",
                "url": "https://arxiv.org/abs/2311.10813"
            }
        },
        {
            "87": {
                "title": "FLIRT: Feedback Loop In-context Red Teaming.",
                "author": "Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, and Rahul Gupta. 2023.",
                "venue": "arXiv preprint arXiv:2308.04265.",
                "url": "https://arxiv.org/abs/2308.04265"
            }
        },
        {
            "88": {
                "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.",
                "author": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076\u201312100, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.emnlp-main.741"
            }
        },
        {
            "89": {
                "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.",
                "author": "Niels M\u00fcndler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=EmQSOi1X2f"
            }
        },
        {
            "90": {
                "title": "Introducing ChatGPT.",
                "author": "OpenAI. 2022.",
                "venue": "OpenAI blog.",
                "url": "https://openai.com/blog/chatgpt"
            }
        },
        {
            "91": {
                "title": "DINOv2: Learning Robust Visual Features without Supervision.",
                "author": "Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. 2024.",
                "venue": "Transactions on Machine Learning Research.",
                "url": "https://openreview.net/forum?id=a68SUt6zFt"
            }
        },
        {
            "92": {
                "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions.",
                "author": "Lorenzo Pacchiardi, Alex James Chan, S\u00f6ren Mindermann, Ilan Moscovitz, Alexa Yue Pan, Yarin Gal, Owain Evans, and Jan M. Brauner. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=567BjxgaTp"
            }
        },
        {
            "93": {
                "title": "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents.",
                "author": "Jeongeun Park, Seungwon Lim, Joonhyung Lee, Sangbeom Park, Minsuk Chang, Youngjae Yu, and Sungjoon Choi. 2024.",
                "venue": "IEEE Robotics and Automation Letters, 9(2):1059\u20131066.",
                "url": "https://doi.org/10.1109/LRA.2023.3338514"
            }
        },
        {
            "94": {
                "title": "Generative Agents: Interactive Simulacra of Human Behavior.",
                "author": "Joon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023.",
                "venue": "In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/3586183.3606763"
            }
        },
        {
            "95": {
                "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback.",
                "author": "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023.",
                "venue": "arXiv preprint arXiv:2302.12813.",
                "url": "https://arxiv.org/abs/2302.12813"
            }
        },
        {
            "96": {
                "title": "Virtualhome: Simulating household activities via programs.",
                "author": "Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018.",
                "venue": "In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8494\u20138502, Salt Lake City, UT, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR.2018.00886"
            }
        },
        {
            "97": {
                "title": "A Moral Imperative: The Need for Continual Superalignment of Large Language Models.",
                "author": "Gokul Puthumanaillam, Manav Vora, Pranay Thangeda, and Melkior Ornik. 2024.",
                "venue": "arXiv preprint arXiv:2403.14683.",
                "url": "https://arxiv.org/abs/2403.14683"
            }
        },
        {
            "98": {
                "title": "NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario.",
                "author": "Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. 2024.",
                "venue": "arXiv preprint arXiv:2305.14836.",
                "url": "https://arxiv.org/abs/2305.14836"
            }
        },
        {
            "99": {
                "title": "Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models.",
                "author": "Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. 2023.",
                "venue": "arXiv preprint arXiv:2307.08487.",
                "url": "https://arxiv.org/abs/2307.08487"
            }
        },
        {
            "100": {
                "title": "Conformal Language Modeling.",
                "author": "Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, and Regina Barzilay. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=pzUhfQ74c5"
            }
        },
        {
            "101": {
                "title": "Learning Transferable Visual Models From Natural Language Supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.",
                "venue": "In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR.",
                "url": "https://proceedings.mlr.press/v139/radford21a.html"
            }
        },
        {
            "102": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.",
                "venue": "OpenAI blog.",
                "url": "https://openai.com/research/better-language-models"
            }
        },
        {
            "103": {
                "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text.",
                "author": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.",
                "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin, TX, USA. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D16-1264"
            }
        },
        {
            "104": {
                "title": "A Survey of Hallucination in Large Foundation Models.",
                "author": "Vipula Rawte, Amit Sheth, and Amitava Das. 2023.",
                "venue": "arXiv preprint arXiv:2309.05922.",
                "url": "https://arxiv.org/abs/2309.05922"
            }
        },
        {
            "105": {
                "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners.",
                "author": "Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. 2023.",
                "venue": "In Proceedings of The 7th Conference on Robot Learning, volume 229 of Proceedings of Machine Learning Research, pages 661\u2013682, Atlanta, GA, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v229/ren23a.html"
            }
        },
        {
            "106": {
                "title": "Object Hallucination in Image Captioning.",
                "author": "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4035\u20134045, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D18-1437"
            }
        },
        {
            "107": {
                "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models.",
                "author": "Patrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and Kristian Kersting. 2023.",
                "venue": "In Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22522\u201322531, Vancouver, Canada. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR52729.2023.02157"
            }
        },
        {
            "108": {
                "title": "An Attentional Recurrent Neural Network for Occlusion-Aware Proactive Anomaly Detection in Field Robot Navigation.",
                "author": "Andre Schreiber, Tianchen Ji, D. Livingston McPherson, and Katherine Driggs-Campbell. 2023.",
                "venue": "In Proceedings of the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 8038\u20138045, Detroit, MI, USA. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/IROS55552.2023.10341852"
            }
        },
        {
            "109": {
                "title": "A-OKVQA: A Benchmark for Visual Question Answering Using World Knowledge.",
                "author": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022.",
                "venue": "In Proceedings of the 17th European Conference on Computer Vision (ECCV), pages 146\u2013162, Tel Aviv, Israel. Springer Nature Switzerland.",
                "url": "https://doi.org/10.1007/978-3-031-20074-8_9"
            }
        },
        {
            "110": {
                "title": "A Tutorial on Conformal Prediction.",
                "author": "Glenn Shafer and Vladimir Vovk. 2008.",
                "venue": "Journal of Machine Learning Research, 9(12):371\u2013421.",
                "url": "http://jmlr.org/papers/v9/shafer08a.html"
            }
        },
        {
            "111": {
                "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.",
                "author": "Dhruv Shah, B\u0142a\u017cej Osi\u0144ski, Brian Ichter, and Sergey Levine. 2023.",
                "venue": "In Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pages 492\u2013504, Atlanta, GA, USA. PMLR.",
                "url": "https://proceedings.mlr.press/v205/shah23b.html"
            }
        },
        {
            "112": {
                "title": "Reflexion: language agents with verbal reinforcement learning.",
                "author": "Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Neural Information Processing Systems, pages 8634\u20138652, New Orleans, LA, USA. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf"
            }
        },
        {
            "113": {
                "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks.",
                "author": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020.",
                "venue": "In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10737\u201310746. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR42600.2020.01075"
            }
        },
        {
            "114": {
                "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning.",
                "author": "Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2021.",
                "venue": "In Proceedings of the 9th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=0IOX0YcCdTn"
            }
        },
        {
            "115": {
                "title": "DriveLM: Driving with Graph Visual Question Answering.",
                "author": "Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. 2023.",
                "venue": "arXiv preprint arXiv:2312.14150.",
                "url": "https://arxiv.org/abs/2312.14150"
            }
        },
        {
            "116": {
                "title": "A framework for smart control using machine-learning modeling for processes with closed-loop control in Industry 4.0.",
                "author": "Gonen Singer and Yuval Cohen. 2021.",
                "venue": "Engineering Applications of Artificial Intelligence, 102:104236.",
                "url": "https://doi.org/https://doi.org/10.1016/j.engappai.2021.104236"
            }
        },
        {
            "117": {
                "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models.",
                "author": "Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.",
                "venue": "In Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523\u201311530, London, England. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/ICRA48891.2023.10161317"
            }
        },
        {
            "118": {
                "title": "LUNA: A Model-Based Universal Analysis Framework for Large Language Models.",
                "author": "Da Song, Xuan Xie, Jiayang Song, Derui Zhu, Yuheng Huang, Felix Juefei-Xu, and Lei Ma. 2023.",
                "venue": "arXiv preprint arXiv:2310.14211.",
                "url": "https://arxiv.org/abs/2310.14211"
            }
        },
        {
            "119": {
                "title": "Artificial intelligence, machine learning and deep learning in advanced robotics, a review.",
                "author": "Mohsen Soori, Behrooz Arezoo, and Roza Dastres. 2023.",
                "venue": "Cognitive Robotics, 3:54\u201370.",
                "url": "https://doi.org/https://doi.org/10.1016/j.cogr.2023.04.001"
            }
        },
        {
            "120": {
                "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.",
                "author": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2023.",
                "venue": "Transactions on Machine Learning Research.",
                "url": "https://openreview.net/forum?id=uyTL5Bvosj"
            }
        },
        {
            "121": {
                "title": "Current Status and Issues of Traffic Light Recognition Technology in Autonomous Driving System.",
                "author": "Naoki Suganuma and Keisuke Yoneda. 2022.",
                "venue": "IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, E105.A(5):763\u2013769.",
                "url": "https://doi.org/10.1587/transfun.2021WBI0002"
            }
        },
        {
            "122": {
                "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification.",
                "author": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Orleans, Louisiana. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N18-1074"
            }
        },
        {
            "123": {
                "title": "LLaMA: Open and Efficient Foundation Language Models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a.",
                "venue": "arXiv preprint arXiv:2302.13971.",
                "url": "https://arxiv.org/abs/2302.13971"
            }
        },
        {
            "124": {
                "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b.",
                "venue": "arXiv preprint arXiv:2307.09288.",
                "url": "https://arxiv.org/abs/2307.09288"
            }
        },
        {
            "125": {
                "title": "LC-QuAD: A Corpus for Complex Question Answering over Knowledge Graphs.",
                "author": "Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017.",
                "venue": "In Proceedings of the 2017 International Semantic Web Conference, pages 210\u2013218, Vienna, Austria. Springer International Publishing.",
                "url": "https://doi.org/10.1007/978-3-319-68204-4_22"
            }
        },
        {
            "126": {
                "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation.",
                "author": "Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023.",
                "venue": "arXiv preprint arXiv:2307.03987.",
                "url": "https://arxiv.org/abs/2307.03987"
            }
        },
        {
            "127": {
                "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
                "author": "Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. 2023a.",
                "venue": "In Proceedings of the 37th Conference on Neural Information Processing Systems Datasets and Benchmarks Track, New Orleans, LA, USA.",
                "url": "https://openreview.net/forum?id=kaHpo8OZw2"
            }
        },
        {
            "128": {
                "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models.",
                "author": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023b.",
                "venue": "arXiv preprint arXiv:2305.16291.",
                "url": "https://arxiv.org/abs/2305.16291"
            }
        },
        {
            "129": {
                "title": "Conformal Temporal Logic Planning using Large Language Models.",
                "author": "Jun Wang, Jiaming Tong, Kaiyuan Tan, Yevgeniy Vorobeychik, and Yiannis Kantaros. 2024.",
                "venue": "arXiv preprint arXiv:2309.10092.",
                "url": "https://arxiv.org/abs/2309.10092"
            }
        },
        {
            "130": {
                "title": "TraCI: an interface for coupling road traffic and network simulators.",
                "author": "Axel Wegener, Micha\u0142 Pi\u00f3rkowski, Maxim Raya, Horst Hellbr\u00fcck, Stefan Fischer, and Jean-Pierre Hubaux. 2008.",
                "venue": "In Proceedings of the 11th Communications and Networking Simulation Symposium, page 155\u2013163, New York, NY, USA. Association for Computing Machinery.",
                "url": "https://doi.org/10.1145/1400713.1400740"
            }
        },
        {
            "131": {
                "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
                "author": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Neural Information Processing Systems, pages 24824\u201324837, New Orleans, LA, USA. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf"
            }
        },
        {
            "132": {
                "title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models.",
                "author": "Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao MA, Pinlong Cai, Min Dou, Botian Shi, Liang He, and Yu Qiao. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=OqTMUPuLuC"
            }
        },
        {
            "133": {
                "title": "On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving.",
                "author": "Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, et al. 2023.",
                "venue": "arXiv preprint arXiv:2311.05332.",
                "url": "https://arxiv.org/abs/2311.05332"
            }
        },
        {
            "134": {
                "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
                "author": "Ronald J. Williams. 1992.",
                "venue": "Machine Learning, 8(3\u20134):229\u2013256.",
                "url": "https://doi.org/10.1007/BF00992696"
            }
        },
        {
            "135": {
                "title": "Referring Multi-Object Tracking.",
                "author": "Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, and Jianbing Shen. 2023a.",
                "venue": "In Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14633\u201314642, Vancouver, Canada. Institute of Electrical and Electronics Engineers.",
                "url": "https://doi.org/10.1109/CVPR52729.2023.01406"
            }
        },
        {
            "136": {
                "title": "Language Prompt for Autonomous Driving.",
                "author": "Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, and Jianbing Shen. 2023b.",
                "venue": "arXiv preprint arXiv:2309.04379.",
                "url": "https://arxiv.org/abs/2309.04379"
            }
        },
        {
            "137": {
                "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs.",
                "author": "Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=gjeQKFxFpZ"
            }
        },
        {
            "138": {
                "title": "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model.",
                "author": "Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, and Hengshuang Zhao. 2024.",
                "venue": "arXiv preprint arXiv:2310.01412.",
                "url": "https://arxiv.org/abs/2310.01412"
            }
        },
        {
            "139": {
                "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.",
                "author": "Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. 2024.",
                "venue": "ACM Transactions on Knowledge Discovery from Data.",
                "url": "https://doi.org/10.1145/3649506"
            }
        },
        {
            "140": {
                "title": "Alignment for Honesty.",
                "author": "Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023a.",
                "venue": "arXiv preprint arXiv:2312.07000.",
                "url": "https://arxiv.org/abs/2312.07000"
            }
        },
        {
            "141": {
                "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving.",
                "author": "Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. 2023b.",
                "venue": "arXiv preprint arXiv:2311.01043.",
                "url": "https://arxiv.org/abs/2311.01043"
            }
        },
        {
            "142": {
                "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.",
                "author": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D18-1259"
            }
        },
        {
            "143": {
                "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples.",
                "author": "Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. 2023a.",
                "venue": "arXiv preprint arXiv:2310.01469.",
                "url": "https://arxiv.org/abs/2310.01469"
            }
        },
        {
            "144": {
                "title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
                "author": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023b.",
                "venue": "In Proceedings of the 11th International Conference on Learning Representations, Kigali, Rwanda.",
                "url": "https://openreview.net/forum?id=WE_vluYUL-X"
            }
        },
        {
            "145": {
                "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models.",
                "author": "Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023.",
                "venue": "arXiv preprint arXiv:2309.06794.",
                "url": "https://arxiv.org/abs/2309.06794"
            }
        },
        {
            "146": {
                "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks.",
                "author": "Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, and Jianfeng Gao. 2023.",
                "venue": "arXiv preprint arXiv:2310.12516.",
                "url": "https://arxiv.org/abs/2310.12516"
            }
        },
        {
            "147": {
                "title": "BARTScore: Evaluating Generated Text as Text Generation.",
                "author": "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Neural Information Processing Systems, pages 27263\u201327277. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf"
            }
        },
        {
            "148": {
                "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation.",
                "author": "Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. 2021.",
                "venue": "In Proceedings of the 4th Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 726\u2013747. PMLR.",
                "url": "https://proceedings.mlr.press/v155/zeng21a.html"
            }
        },
        {
            "149": {
                "title": "Large Language Models for Robotics: A Survey.",
                "author": "Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and Philip S. Yu. 2023.",
                "venue": "arXiv preprint arXiv:2311.07226.",
                "url": "https://arxiv.org/abs/2311.07226"
            }
        },
        {
            "150": {
                "title": "AlignScore: Evaluating Factual Consistency with A Unified Alignment Function.",
                "author": "Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11328\u201311348, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.634"
            }
        },
        {
            "151": {
                "title": "Large language models for human-robot interaction: A review.",
                "author": "Ceng Zhang, Junxin Chen, Jiatong Li, Yanhong Peng, and Zebing Mao. 2023a.",
                "venue": "Biomimetic Intelligence and Robotics, 3(4):100131.",
                "url": "https://doi.org/https://doi.org/10.1016/j.birob.2023.100131"
            }
        },
        {
            "152": {
                "title": "How Language Model Hallucinations Can Snowball.",
                "author": "Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. 2023b.",
                "venue": "arXiv preprint arXiv:2305.13534.",
                "url": "https://arxiv.org/abs/2305.13534"
            }
        },
        {
            "153": {
                "title": "The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models.",
                "author": "Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2023c.",
                "venue": "arXiv preprint arXiv:2305.13669.",
                "url": "https://arxiv.org/abs/2305.13669"
            }
        },
        {
            "154": {
                "title": "BERTScore: Evaluating Text Generation with BERT.",
                "author": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020.",
                "venue": "In Proceedings of the 8th International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=SkeHuCVFDr"
            }
        },
        {
            "155": {
                "title": "Siren\u2019s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.",
                "author": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023d.",
                "venue": "arXiv preprint arXiv:2309.01219.",
                "url": "https://arxiv.org/abs/2309.01219"
            }
        },
        {
            "156": {
                "title": "A Survey of Large Language Models.",
                "author": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.18223.",
                "url": "https://arxiv.org/abs/2303.18223"
            }
        },
        {
            "157": {
                "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization.",
                "author": "Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2024.",
                "venue": "arXiv preprint arXiv:2311.16839.",
                "url": "https://arxiv.org/abs/2311.16839"
            }
        },
        {
            "158": {
                "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT.",
                "author": "Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023.",
                "venue": "arXiv preprint arXiv:2302.09419.",
                "url": "https://arxiv.org/abs/2302.09419"
            }
        },
        {
            "159": {
                "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models.",
                "author": "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2024.",
                "venue": "In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria.",
                "url": "https://openreview.net/forum?id=oZDJKTlOUe"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.16527v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3",
            "4.3.4",
            "4.3.5"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.2",
            "5.2.1",
            "5.2.2",
            "5.2.3",
            "5.3",
            "5.3.1",
            "5.3.2",
            "5.3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.1",
            "2.2",
            "2.3",
            "4.1.1",
            "4.1.2",
            "4.1.3",
            "4.2.1",
            "4.2.2",
            "4.3.1",
            "4.3.2",
            "4.3.3",
            "4.3.4",
            "4.3.5"
        ]
    },
    "research_context": {
        "paper_id": "2403.16527v1",
        "paper_title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
        "research_background": "### Paper's Motivation\nThe motivation stems from the recent success and application of foundation models, including large (visual) language models (L(V)LMs), in various automated decision-making tasks. Despite their significant potential, these models have a known shortcoming: they can hallucinate, meaning they occasionally generate decisions or rationales that are plausible-sounding but factually incorrect or could lead to undesirable outcomes. Given that these models are increasingly being considered for safety-critical applications in fields such as robotics and autonomous vehicles, there is a pressing need for methods to detect and mitigate such hallucinations to ensure more trustworthy and reliable systems.\n\n### Research Problem\nThe research problem addressed in this paper is the detection and mitigation of hallucinations in foundation models, specifically within the context of decision-making tasks. While there has been some focus on hallucination detection in other domains like question-answering (QA) and object detection, there is a gap in summarizing state-of-the-art methods for decision-making and planning. This paper aims to propose a flexible definition of hallucinations that can be adapted to various deployment settings and to review the current techniques for detecting and mitigating such hallucinations in decision-making contexts.\n\n### Relevant Prior Work\n1. **Classical vs. Learning-Based Models**: The paper acknowledges the two broad categories of approaches in machine learning and robotics: hand-engineered model-based systems and data-driven learning-based models (Formentin et al., 2013). Both have their strengths but often fail in out-of-distribution scenarios (Wen et al., 2023).\n\n2. **Deployment Challenges**: Despite fine-tuning and modifications, models may struggle with unforeseen situations, leading to sub-optimal performance or even critical failures (Singer and Cohen, 2021; Schreiber et al., 2023; Chakraborty et al., 2023). Moreover, changes may have unintended side effects, such as conflicting rules or catastrophic forgetting (Kemker et al., 2018).\n\n3. **Foundation Models for Knowledge**: There is an emerging interest in using large (visual) language models to fill knowledge gaps and adapt in unfamiliar circumstances (Cui et al., 2024). These models have shown the ability to generalize to untrained tasks and justify their decisions, making them suitable for high-level tasks such as simulated decision-making (Huang et al., 2024b) and real-world robotics (Zeng et al., 2023).\n\n4. **Hallucination Detection and Mitigation**: Previous efforts have mainly focused on QA (Ji et al., 2023; Rawte et al., 2023; Zhang et al., 2023d; Ye et al., 2023) or object detection tasks (Li et al., 2023c). Wang et al. (2023a) analyzed the trustworthiness of various foundation models, and Chen & Shu (2024) proposed a taxonomy of hallucinations in LLMs but did not focus on decision-making tasks. \n\nThese works highlight the current capabilities and limitations of foundation models and the need for focused research on the specific issue of hallucination in decision-making contexts.",
        "methodology": "#### Methodology: \nThe proposed methodology for detecting and mitigating hallucinations in foundation models is classified into three distinct types based on the available inputs to the algorithm: white-box, grey-box, and black-box methods. \n\n1. **White-box Methods**:\n    - **Inputs Required**: Predicted sequences of tokens, corresponding probabilities of each token, and embeddings from intermediate layers in the network.\n    - **Application**: These methods are highly informative because they utilize comprehensive information from the model's internal states and outputs.\n\n2. **Grey-box Methods**:\n    - **Inputs Required**: Predicted sequences of tokens and their corresponding probabilities.\n    - **Application**: Grey-box methods strike a balance, providing more information than black-box methods but requiring less access than white-box methods. \n\n3. **Black-box Methods**:\n    - **Inputs Required**: Only the predicted sequence of tokens.\n    - **Application**: These methods are the most flexible, suitable for situations where the model does not grant access to its hidden states or output probability distributions, such as using interfaces like the ChatGPT web interface.\n\n**Context and Application**:\n- Foundation models typically output a sequence of predicted tokens, the associated probabilities for each token, and embeddings from intermediate layers. However, not all models provide full access to these outputs, necessitating different approaches to hallucination detection and mitigation.\n- The methods are evaluated primarily within QA and object detection settings, but there is a noted need for further validation within decision-making tasks.\n\n**Discussion and Review**:\n- Existing approaches within these classifications are discussed and summarized, highlighting their potential and identifying gaps particularly related to decision-making tasks.\n- The summarized works and their evaluations can be found in Table 2 of the referenced paper.\n\nThis detailed classification and discussion underscore the importance of matching the hallucination detection method with the level of access available to the model's outputs, offering a versatile framework for addressing hallucinations across various applications and interface constraints.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets:\n\n1. **BDD-X**: \n   - Multi-modal driving dataset with video clips, vehicle control actions, and text annotations.\n   - Augmented to a QA dataset with questions on vehicle actions, reasoning, and predictions.\n   - Further augmented with generated questions from ChatGPT, creating the DriveGPT4 dataset.\n   \n2. **nuScenes**:\n   - Driving videos containing multi-view RGB camera views, GPS, bounding boxes, and semantic map representations.\n   - NuScenes-QA: A visual QA dataset derived from nuScenes focusing on object existence, counting, action states, and comparisons.\n   - Talk2Car: Refers to objects in the scene using natural language commands.\n   - NuPrompt: Extends NuScenes with RMOT annotations.\n   - DRAMA: Benchmark for identifying risky objects with suggested vehicle actions (extension dataset not public).\n\n3. **KITTI**: \n   - Augmented with RMOT labeled bounding boxes in text-referenced images.\n\n4. **SaGC**: \n   - Annotated robot tasks from cooking, cleaning, and massage, including text goals and uncertainty labels.\n\n5. **Other QA Datasets**:\n   - **HotPotQA**: Multi-hop questions from Wikipedia.\n   - **FEVER**: Verifies support/refute claims on Wikipedia.\n   - **Natural Questions**: Google search queries with Wikipedia-based answers.\n   - **StrategyQA**: Implicit multi-hop questions.\n   - **QreCC**: Conversational QA dataset with dialogues focusing on comprehension, retrieval, and rewriting.\n   - **Zhao et al. (2024)**: Multi-model visual QA dataset contrasting hallucinated and non-hallucinated descriptions.\n\n6. **Simulator-based Datasets**:\n   - **CARLA**: 3D driving simulator for perception, planning, control, and QA tasks.\n   - **ProgPrompt**: High-fidelity 3D robot tasks.\n   - **RoboEval**: Platform to check the correctness of robot task-generated code.\n\n#### Baselines:\n\n1. **BERTScore**: \n   - Computes cosine similarity of BERT embeddings between pairs of sentences.\n2. **BARTScore**:\n   - Uses a pre-trained BART model to sum log probabilities of generated tokens based on prior context.\n3. **SummaC**:\n   - Evaluates entailment, contradiction, and neutral scores between document sentences and summaries.\n4. **GPTScore**:\n   - Uses GPT series LLMs to estimate the quality of outputs via token probabilities.\n5. **AlignScore**:\n   - Classifies text alignment based on labeled data, predicting aligned/not aligned, neutrality, and continuous regression scores.\n6. **Uncertainty Measurement**:\n   - Approaches include entropy and sentence clustering based on semantic similarity to better compute uncertainty.\n\n#### Evaluation Metrics:\n\n- BERTScore, BARTScore, GPTScore, and SummaC scores for text quality and similarity assessment.\n- AlignScore: Weighted score across binary, multi-class, and regression heads.\n- Uncertainty-based methods: Entropy and clustering summing entropies within semantic classes.\n- CHAIR: Ratio of hallucinated to total mentioned objects in image descriptions.\n- POPE: Stability in object existence questioning across different outputs.\n\n#### Main Results:\n\n1. Hallucination detection methods utilizing BERTScore, BARTScore, GPTScore, and SummaC generally showed a strong capability in distinguishing between genuine and hallucinated responses in text generation tasks.\n2. AlignScore was notable for successfully rating text alignment by incorporating both binary and continuous scores.\n3. Uncertainty metrics based on entropy carried limitations when addressing semantically similar sentences with different entropies. The proposed clustering method improved representational accuracy of uncertainties.\n4. POPE demonstrated improved stability over CHAIR by focusing on binary presence/absence questions which reduced score variability across semantically similar descriptions.\n5. The newly developed multi-modal and QA datasets, such as DriveGPT4 and NuScenes-QA, facilitated more comprehensive evaluations across various dimensions, including perception, planning, and control tasks.\n6. Simulation platforms like CARLA, ProgPrompt, and RoboEval efficiently replicated real-world scenarios, validating the robustness of hallucination detection algorithms in controlled but realistic environments."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To test the effectiveness of the DiLu framework in producing high-level decisions for autonomous driving using a driving simulator environment.",
            "experiment_process": "The DiLu framework consists of reasoning, reflection, and memory modules supporting an LLM. The reasoning module views the vehicle's current observation, queries the memory module for similar past situations, and converts the experience into a prompt for input to the LLM. The prompt elicits chain-of-thought reasoning to improve model accuracy. The LLM's generated text output is summarized by the reflection module and used to update the memory bank. A separate decision decoder converts the summary into a discrete high-level decision to be executed in the simulator.",
            "result_discussion": "The researchers found that the DiLu framework could produce high-level decisions, recognizing safety-critical scenarios and suggesting more conservative driving actions. However, the framework had difficulty detecting traffic light states, a limitation similar to other vision models.",
            "ablation_id": "2403.16527v1.No1"
        },
        {
            "research_objective": "To evaluate the effectiveness of the self-reflection approach in reducing hallucinated outputs from the Agent-Driver model during autonomous driving tasks.",
            "experiment_process": "The Agent-Driver model utilizes a tool library that communicates with neural modules for object detection, trajectory prediction, occupancy estimation, and mapping. The LLM reasons about helpful information needed for path planning of the ego vehicle, iteratively building up context by calling functions from the tool library. The model employs a memory bank of prior experiences. If a predicted trajectory collides with surrounding objects, the LLM undergoes self-reflection to fine-tune its prediction.",
            "result_discussion": "The authors found that their self-reflection approach eradicates invalid (hallucinated) generations during test-time, achieving zero invalid outputs.",
            "ablation_id": "2403.16527v1.No2"
        },
        {
            "research_objective": "To determine whether the multi-modal DriveLM-Agent method can effectively predict ego vehicle paths from raw images in an end-to-end manner compared to Agent-Driver.",
            "experiment_process": "DriveLM-Agent frames the task into several sub-tasks: questioning a VLM about its perception of surrounding vehicles, predicting behaviors, planning high-level decisions, converting to lower-level discrete actions, and finally estimating a coordinate-level trajectory. This approach contrasts with Agent-Driver, which requires sensor modules to process context separately.",
            "result_discussion": "The DriveLM-Agent method successfully performed these tasks, leveraging a multi-modal approach to predict paths effectively. However, detailed comparative performance metrics with Agent-Driver were not specified.",
            "ablation_id": "2403.16527v1.No3"
        },
        {
            "research_objective": "To measure and improve the robustness of SayCan's action generation for robots by integrating methodical constraints using SayCanPay.",
            "experiment_process": "SayCanPay enhances SayCan by incorporating three strategies for planning: Say, SayCan, and SayCanPay. Say methods choose actions based only on token probabilities, SayCan approaches consider the success rate, and SayCanPay also estimates the expected payoff with heuristic-based learning. Models were trained using regression on expert trajectory datasets.",
            "result_discussion": "The SayCanPay method maximized task efficiency while minimizing conflicting infeasible actions. It showed improved performance over basic Say and SayCan methods.",
            "ablation_id": "2403.16527v1.No4"
        },
        {
            "research_objective": "To test the viability of CLARA in predicting robotic system response feasibility and handling uncertainty in user instructions.",
            "experiment_process": "CLARA samples multiple sets of concepts from the original prompt, orders them, and inputs them to the language model. It computes average similarity in an embedding space to identify uncertainties. For feasibility, it provides the robot's possible action space, observation, and goal to determine practicality. If uncertain but feasible, clarification is requested from the user.",
            "result_discussion": "CLARA achieved a reasonable success rate on robotic tasks but faced challenges during uncertainty reasoning and feasibility predictions, leading to occasional hallucinations.",
            "ablation_id": "2403.16527v1.No5"
        }
    ]
}