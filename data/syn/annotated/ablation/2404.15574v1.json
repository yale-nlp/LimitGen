{
    "title": "Retrieval Head Mechanistically Explains Long-Context Factuality",
    "abstract": "Despite the recent progress in long-context large language models (LLMs), it remains elusive how these transformer-based language models acquire the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question.\nOur systematic investigation across 4 model families, 6 model scales, and 3 types of finetuning reveals that a special type of attention heads are largely responsible for retrieving relevant information from long context, which we dub retrieval heads.\nWe identify important and intriguing properties of retrieval heads:\n(1) universal:\nall the explored models with long-context capability have a set of retrieval heads;\n(2) sparse: only a small portion (less than 5%) of the attention heads are retrieval.\n(3) intrinsic: retrieval heads already exist in\nmodels pretrained with short context.\nWhen extending the context length to 32-128K by continual pretraining,\nit is still the same set of heads that perform information retrieval.\n(4) dynamically activated:\ntake Llama-2 7B for example, 12\nretrieval heads always attend to the required information no matter how the context is changed.\nThe rest of the retrieval heads are activated in different contexts.\n(5) causal:\ncompletely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model\u2019s retrieval ability.\nWe further show that retrieval heads strongly influence\nchain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context.\nConversely, tasks where the model directly generates the answer using its intrinsic knowledge\nare less impacted by masking out retrieval heads.\nThese observations collectively explain which internal part of the model seeks information from the input tokens.\nWe believe our insights on retrieval heads foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "This work studies the internal mechanism of how long-context language models can utilize information at arbitrary locations within the input.\nRecent advances in long-context language modeling [1  ###reference_b1###, 20  ###reference_b20###, 6  ###reference_b6###] show inspiring results, particularly on the Needle-in-a-Haystack test [14  ###reference_b14###], which asks the model to precisely retrieve the information of a short sentence (the needle) within a long context (the haystack).\nSuch capability is the basis of more advanced long-context tasks, which usually interleaves retrieval and reasoning in a multi-step fashion [17  ###reference_b17###].\nBased on extensive experiments across 4 model families, 6 model scales, and 3 types of finetuning,\nwe show that across the models\u2019 attention layers, there exist a small number of retrieval heads that search the information being asked, and redirect the relevant tokens from the input to the output.\nActivation of retrieval heads explains whether the output is factual or hallucinated.\nWhen such heads are activated, the model behaves faithful to the input document.\nWhen they are not activated, or intentionally masked out in controlled experiments (Fig. 1  ###reference_###), the model cannot find the relevant information and hallucinate instead.\n###figure_1### The discovery of the retrieval head is motivated by the question of what the attention mechanism is doing when the model can or cannot find the given needle.\nWe take important inspiration from two existing works: the CopyNet [10  ###reference_b10###] and the Induction Head [19  ###reference_b19###].\nThe CopyNet is essentially a single-layer, single-head attention mechanism in the age of RNNs that copy-paste tokens from the input to the output.\nInduction Heads [19  ###reference_b19###] are special heads within a multi-layer, multi-head attention network that implements an implicit program induction algorithm.\nCombining the observation from the two works, we natually hypothesize that, just like induction heads are accountable for in-context learning, there might exist special heads that are accountable for information retrieval and implement a conditional copy-paste algorithm.\nWe design algorithms to detect retrieval heads within the transformer architecture (Sec. 2  ###reference_###), and conduct large-scale experiments to demonstrate important properties of them (Sec. 3  ###reference_###):\n(1) retrieval heads are universal and sparse: for any model family (LLaMA [21  ###reference_b21###], Yi [25  ###reference_b25###], QWen [2  ###reference_b2###] and Mistral [12  ###reference_b12###]), at any scale (6B, 14B, and 34B and 87B), either base or chat, either dense or MoE, as long as the model can precisely recite the input information, they have a small number of retrieval heads (Fig. 1  ###reference_###);\n(2) they are intrinsic: the base model (e.g., LLaMA2 base) already contains retrieval heads (as a consequence of large-scale pretraining). Subsequent derivations, such as the long-context continue pretraining (LLaMA2 7B 80K), chat fine-tuning (Qwen Chat), or even sparse upcycling [16  ###reference_b16###, 13  ###reference_b13###] uses the same retrieval heads as the base model (Fig. 5  ###reference_###);\n(3) they are dynamically activated according to the context: the strongest retrieval heads (e.g., 13 for LLaMA 2 7B) are always activated no matter what the required information is, while weaker retrieval heads are activated on different parts of the required information; consequently these heads compensate each other\u2019s functionality: removing a subset of the heads, the model at least retrieve part of the required information;\n(4) the retrieval heads are causal:\nsay we put a needle \"the best thing to do in San Francisco is to eat a sandwich in Dolores Park on a sunny day\",\ncompletely masking out retrieval heads, the model hallucinates (by saying the best thing is to visit Golden Gate bridge);\npartially masking out the heads, the model retrieves part of the needle (e.g., it gets the sandwich but forget the Dolores Park);\nmasking out random non-retrieval heads, the model still find full needle;\nwhen we do not mask the head yet the model still hallucinate in some cases,\nthe retrieval heads are not activated.\nWe further note that chain-of-thought reasoning also heavily relies on retrieval heads because the model needs to refer back the input information, indicating a complex relationship between the model\u2019s retrieval and reasoning capability.\nThe discovery of retrieval head has profound implications on long-context modeling:\n(1) it marks a significant step forward in the field of mechanistic interpretability [3  ###reference_b3###, 19  ###reference_b19###] because for the first time we pin point a particular subnet implementing the conditional retrieval algorithm;\n(2) it explains why certain context-compression methods fail to keep factuality (because they removes the retrieval head, e.g., in Xiao et al. 24  ###reference_b24###), and suggests future research on KV cache compression [7  ###reference_b7###, 15  ###reference_b15###], a key problem for deploying long-context models, should consider the influence of retrieval heads."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Detecting Retrieval Head",
            "text": "###figure_2### ###table_1### To detect which head is implementing the retrieval algorithm, we introduct a retrieval score to measures the frequency of a head\u2019s copy-paste behavior during autoregressive decoding.\nAn attention head with high retrieval score suggests that statistically across various contexts, this head is frequently copying the input tokens from the input to the output.\nNeedle-in-a-Haystack\u2003\u2003Our retrieval head detection algorithm roots from the needle-in-a-Haystack test, which asks the model to copy-paste the input tokens to the output.\nGiven a question  and its corresponding answer  (the needle), we insert  in a given context  (the haystack) at a random position index range .\nThe language model is then tasked with answering  based on the haystack with the inserted needle.\nWe set  and  unique and irrelevant with the given long context,\nensuring that if an answer is correctly generated, it is indeed copied from the context, not from the model\u2019s internal knowledge.\nRetrieval Score for Attention Heads\u2003\u2003We define the retrieval score as the frequency of a head\u2019s copy-paste operations.\nSpecifically,\nduring auto-regressive decoding (we use greedy decoding by default),\ndenote the current token being generated as  and\nthe attention scores of a head as .\nAs demonstrated in Fig. 2  ###reference_###, we say an attention head  copies and pastes a token from the needle to the output sentence if it follows two criteria:\n(1) , i.e.,  is a token within the needle sentence. (2)\n, i.e., the input token that receives the most attention probability mass by this head is a token within the needle and is the same token as the currently generated token.\nLet  be the set containing all tokens copy and pasted by a given head , we define:\nIntuitively, retrieval score represents a token-level recall rate of the most attended tokens by an attention head.\nFor example, when retrieving a needle of 10 tokens, a retrieval score of 0.9 indicates that the attention head has copies and pasted 9 tokens in the 10-token target answer.\nRetrieval Head Detection Algorithm\u2003\u2003We calculate the retrieval score for all attention heads under a diverse set of input contexts.\nFor each language model we consider, we compile three sets of Needle-in-a-Haystack samples, each consisting of a unique tuple .\nFor each sample, we make sure  is semantically irrelevant with  and that  cannot be answered using the model\u2019s existing knowledge by manually inspecting the model output.\nThen for each  sample, we perform Needle-in-a-Haystack on 20 different length values uniformly sampled from 1K-50K, where in each length,  is inserted in 10 different depth uniformly ranging from the start to the end of .\nWe note that this scale of tests gives stable outputs as the average retrieval score converges after just a few samples.\nIn total, each language model is subjected to approximately 600 instances of retrieval testing.\nWe calculate the retrieval score for each attention head in each test and use the average of these scores as the head\u2019s final retrieval score.\nThe attention heads with relatively larger retrieve score can be considered as retrieval head.\nIn our case (Fig. 3  ###reference_###), we set the threshold as 0.1, meaning that as long as the head performs copy-paste in 10% of the times, we consider it a retrieval head."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Basic Properties of Retrieval Heads",
            "text": "This section discusses important properties of retrieval heads:\n(1) universal and sparse: any model that exhibits long-context capability has a small set of retrieval heads;\n(2) dynamic: most of retrieval heads are activated under different contexts;\n(3) intrinsic: retrieval heads are already within the base model as a consequence of large-scale pretraining. Subsequent models reuse the same set of heads.\nOur results are supported by extensive experiments on a large spectrum of models (Table 1  ###reference_###).\nTo examine the effect of alignment, we have study Mistral-7B-Instruct-v0.2 and Qwen-1.5-14B-Chat [2  ###reference_b2###] and compare them to their base versions.\nWe further choose Mixtral-8x7B-v0.1 [13  ###reference_b13###], a mixture of expert versions derived from Mistral-7B-v0.2, presumably via sparse upcycling [16  ###reference_b16###], to study retrieval heads in different architectures."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Dynamically Activated Based on Tokens and Contexts",
            "text": "Now we study how sensitive a retrieval head is to its input context, i.e., whether a head is consistently activated no matter what the context is, or if a head is activated only on specific contexts. For the needle sentences \"the best thing to do in San Francisco is eating a sandwich in Dolores park in a sunny day\", some heads are activated on the full sentence, whereas other heads only activated on certain tokens like \u201ceating a sandwich\u201d or \u201cin Dolores park\u2019. We define activation frequency, the frequency of a head being activated on at least one token (vs., the retrieval score measures the average number of activated tokens). A head of high activation frequency but low retrieval score means it is only activated on certain tokens and contexts. As is shown in Fig. 4, Llama-2-7B-80K and Yi-6B-200K have 12 and 36 strongest retrieval heads, respectively, that are always activated (activation frequency equal to 1) under all the contexts we consider. Weaker heads only activate on certain tokens and contexts."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Intrinsic",
            "text": "We show that the retrieval heads, thus the ability of utilizing information at arbitrary location of the input, is an intrinsic property [6] of the base model as a consequence of large-scale pretraining, with subsequent small-scale training exerting only minor alterations to these head activation patterns. In Figure 5, we present the retrieval score distributions for a range of base models in the initial row, followed by their corresponding variants in the subsequent row. We see that regardless of the models being continuously pre-trained, chat fine-tuned, or sparsely upcycled, there is a notable consistency in their retrieval scores heatmaps. Figure 7 offers a more direct and strict examination, where we compute the statistical correlations between different models. The data reveal a high degree of correlation in the retrieval score distributions between base models and their respective variants, with a Pearson correlation coefficient exceeding 0.8. Models from different families exhibit a correlation coefficient of less than 0.1, indicative of their distinct pretraining recipes."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Influence on Downstream Tasks",
            "text": "This section examines how retrieval heads influence downstream tasks. Across the experiments we use Mistrial-7B-Instruct-v0.2 [18  ###reference_b18###] as it is a popular and strong open language model with 32K context length. We first show that retrieval heads explains the factuality of Needle-in-a-Haystack test. When the model can retrieve the needle, retrieval heads are always activated. When the model cannot retrieve the needle and hallucinate instead, retrieval heads are either partially activated or not activated. Then we show that retrieval heads significantly influence question answering that requires extracting the information from the input, but does not strongly influence tasks where the model directly produce answers based on its internal knowledge. We further explore how retrieval heads influence more sophisticated reasoning behaviors like chain-of-thought [23  ###reference_b23###]."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Influence on Extractive QA",
            "text": "Now we study how retrieval heads influence more realistic tasks beyond Needle-in-a-Haystack.\nWe use extractive QA as a test bed, as common usecase of long-context model where the user typically upload a pdf (research papers, financial reports, legal documents, etc.) and ask questions about specific information within the document.\nTo make sure the knowledge being asked does not exist in the model\u2019s internal knowledge, we synthesize an extractive QA dataset by selecting a set of up-to-date news articles, extract a paragraph from it, and asking GPT-4 to produce a question-answer pair based on the extracted paragraph, similar to the evaluation conducted in Anthropic. \nThese observations demonstrate that real-world document QA tasks heavily rely on the functionality of retrieval heads."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Chain-of-Thought Reasoning also Requires Retrieval Heads",
            "text": "We test Mistrial-7B-Instruct-v0.2\u2019s performance on MMLU, MuSiQue, and GSM8K, with and without chain-of-thought reasoning. As is shown in Fig. 10, if we use answer-only prompting (without CoT), masking out either retrieval or random heads do not significantly influence the performance, presumably because the model\u2019s generation is based on its internal knowledge primarily stored in the FFN layers. For CoT styled reasoning, masking out retrieval heads significantly influences the model\u2019s performance. Upon inspecting typical error cases (Fig. 11), we find that the model becomes \u201cblind\u201d to important input information and hallucinates instead. We find the relationship between CoT and retrieval heads particularly intriguing as it may offer deeper insights into model\u2019s complex reasoning performance. We leave more in-depth studies to future research."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussions",
            "text": "General Functionalities of Attention Heads\u2003\u2003For transformer language models, we tend to view the functionality of FNNs layers to be the place for storing knowledge [8  ###reference_b8###], and the attention layers to be the place for implementing algorithms [19  ###reference_b19###].\nThe induction head discussed in  Olsson et al. [19  ###reference_b19###] typically searches repeated patterns of the input, which is at a certain level similar to the retrieval heads (as it also searches and repeats information).\nDifferent than the induction heads, the retrieval heads are typically responsible for redirecting the information according to the context, but do not for inferring programs.\nWe tend to believe that there exist more algorithm and functionalities implemented by other types of attention heads to be discovered by future research.\nRelationship to Local and Linear Attention and State-Space Models\u2003\u2003Although there exist numerous works about local [24  ###reference_b24###] / linear [22  ###reference_b22###] attention, state space models [9  ###reference_b9###], and hybrid architectures [5  ###reference_b5###] achieving inspiring efficiency in long-context modeling, so far there is no linear attention / SSM architecture that passes the Needle-in-a-Haystack test to the best of our knowledge, suggesting that the full attention might be a must for long-context information retrieval.\nOne example is that the Mistral v0.1 [12  ###reference_b12###] uses sliding window attention but cannot pass needle-in-a-haystack, and their authors changes the attention to full in v0.2 [18  ###reference_b18###], then it can pass the needle test.\nOur results showing strong evidence why full attention is a must.\nFor the model to precisely utilize input information at arbitrary location, it is crutial for the retrieval heads to work on the full KV cache.\nApplications to KV Cache Compression\u2003\u2003The problem that the KV cache is too large and occupies a large chunk of the GPU memory severely hinders the deployment of long-context models.\nFor example, for LLaMA 2 7B, the KV cache of 100K tokens requires more than 50GB memory, while 2K context requires less than 1GB memory.\nIf we serve this model on one 80G A100, then the concurrency of 100K context can be 50 times less than 2K context queries, which is prohibitively expensive.\nThe results from this work indicates that we might be possible to radically prune out the KV cache corresponding to the non-retrieval heads (recall in Fig. 3  ###reference_### shows only 5% of the heads are retrieval) and significantly reducing the deployment cost of long-context models.\nWe leave this study to future research."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This paper discovers retrieval heads, a special set of attention heads that are responsible for implementing the conditional copy algorithm and redirect information from the input to the output.\nRetrieval heads are the primarily reason why a successful long-context model can pass the Needle-in-a-Haystack test, and their activation explains why a language model is faithful to the input or hallucinate.\nCompared to non-retrieval heads, retrieval heads have a stronger influence on downstream tasks that require the model to precisely recall the input information, either in extractive question answering or chain-of-thought reasoning.\nWe believe this work will foster future research on reducing hallucination,\nimproving reasoning, and compressing the KV cache."
        }
    ],
    "url": "http://arxiv.org/html/2404.15574v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.15574v1",
        "paper_title": "Retrieval Head Mechanistically Explains Long-Context Factuality",
        "research_background": "### Paper's Motivation\n\nThe primary motivation of this paper is to understand the internal mechanisms of long-context language models and how they utilize information at arbitrary locations within large inputs. The paper aims to uncover how these models manage to retrieve specific information embedded deep within a lengthy context, a capability that is foundational to more complex long-context tasks involving multiple steps of retrieval and reasoning.\n\n### Research Problem\n\nThe research problem the paper addresses is identifying and understanding the specific components within the attention layers of long-context language models that enable precise information retrieval. Specifically, the paper seeks to determine which parts of the model\u2019s attention mechanism\u2014termed \"retrieval heads\"\u2014are responsible for finding and redirecting relevant tokens from the input to the output, thereby playing a critical role in ensuring the factuality of the model's output.\n\n### Relevant Prior Work\n\nThe paper draws important insights from two key pieces of prior work:\n\n1. **CopyNet [10 ###reference_b10###]**: CopyNet utilizes a single-layer, single-head attention mechanism to copy-paste tokens from the input to the output, offering a fundamental concept of direct token transfer within the domain of RNNs.\n\n2. **Induction Head [19 ###reference_b19###]**: Induction Heads are specialized heads within a multi-layer, multi-head attention network that carry out an implicit program induction algorithm, providing a conceptual basis for understanding heads that facilitate in-context learning.\n\nThe convergence of ideas from these two works led to the hypothesis that, just as induction heads are responsible for in-context learning, there might be specialized heads within transformer architectures accountable for information retrieval. By identifying and understanding these retrieval heads, the paper makes significant strides in mechanistically interpreting long-context language models.",
        "methodology": "**1. Introduction to Retrieval Score:**\n\nTo identify which attention head in a neural network performs a retrieval algorithm, we introduce a retrieval score that measures how often a head displays copy-paste behavior during autoregressive decoding. A head with a high retrieval score frequently copies tokens from the input context to the output, across various contexts.\n\n**2. Needle-in-a-Haystack Test:**\n\nThe Needle-in-a-Haystack test forms the basis of our retrieval head detection algorithm. The test involves the following steps:\n\n- A question \\( q \\) and its corresponding answer \\( a \\) (the needle) are inserted into a context \\( c \\) (the haystack) at a random position.\n- The model is then tasked with generating the answer \\( a \\) based on the new context.\n\nWe ensure that \\( q \\) and \\( a \\) are unique and irrelevant to the given long context, confirming that if an answer is generated correctly, it is copied from the context and not from the model\u2019s internal memory.\n\n**3. Retrieval Score Definition:**\n\nThe retrieval score is introduced to quantify a head\u2019s copy-paste operations:\n\n- During autoregressive (greedy) decoding, let the current token being generated be \\( y \\) and the attention scores of a head be \\( A \\).\n\n- An attention head \\( h \\) is considered to copy and paste a token from the needle to the output if:\n\n  (1) \\( y \\in \\text{needle} \\), i.e., \\( y \\) is a token within the needle sentence.\n  (2) The token receiving the most attention from head \\( h \\) is also within the needle and matches the currently generated token.\n\nLet \\( \\text{Paste}(h) \\) be the set of tokens copied and pasted by head \\( h \\). The retrieval score \\( R(h) \\) for a head is then defined based on the frequency of copied tokens.\n\n**4. Retrieval Head Detection Algorithm:**\n\n- We calculate the retrieval scores for all attention heads under diverse input contexts.\n\n- For each model, we create three sets of Needle-in-a-Haystack samples, each with a unique tuple \\( (q, a, c) \\).\n\n- For each sample, ensuring \\( q \\) is semantically irrelevant to \\( c \\), and that \\( a \\) cannot be answered using the model's internal knowledge by manually inspecting outputs.\n\nIn our example, the threshold for identifying a retrieval head is set to 0.1, meaning any head that performs copy-paste operations 10% of the time is considered a retrieval head.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets Used:**\n- The exact datasets are not specified in the quoted section. However, the experiments mention evaluating a large spectrum of models, which suggests that the datasets used are comprehensive enough to test multiple aspects related to retrieval heads and long-context capabilities.\n\n**Models and Baselines:**\n1. **Llama-2 Variants:** \n   - Llama-2-7B with a 4K context length\n   - Llama-2-7B with an 80K context length\n   - Llama-2-13B with a 60K context length\n\n2. **Mistral Alignments:**\n   - Mistral-7B-Instruct-v0.2\n   - Qwen-1.5-14B-Chat\n   - Base versions of the above models\n\n3. **Mixture of Experts:**\n   - Mixtral-8x7B-v0.1, derived from Mistral-7B-v0.2\n\n**Evaluation Metrics:**\n- While specific metrics are not directly mentioned, the focus is clearly on properties like universality, sparsity, dynamic behavior, and intrinsic characteristics of the retrieval heads. Typical metrics in such studies would include effectiveness in retrieving relevant information, activation patterns under different contexts, and comparison of performance against baseline models.\n\n**Main Experimental Results:**\n1. **Universality and Sparsity:**\n   - A small set of retrieval heads is found across any model that exhibits long-context capability, signifying that these heads are universal and sparse.\n\n2. **Dynamic Activation:**\n   - Most of the retrieval heads activate differently under varying contexts, showing dynamic behavior based on the input context.\n\n3. **Intrinsic Nature from Pretraining:**\n   - Retrieval heads are proven to be intrinsic to the base model due to large-scale pretraining, with subsequent models reusing the same set of heads.\n\nThese results highlight the fundamental and reusable nature of retrieval heads in models with long-context capabilities, supported by a thorough comparison of model variants and architectural implementations."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate if retrieval heads exist in models pretrained with short context and whether continued pretraining extends context length without changing which heads perform retrieval.",
            "experiment_process": "The study involves various models to analyze retrieval heads' properties. It compares Llama-2-7B 4K to Llama-2-7B-80K and Llama-2-13B-60K and analyzes models like Mistral-7B-Instruct-v0.2 and Qwen-1.5-14B-Chat against their base versions. It also examines Mixtral-8x7B-v0.1, derived from Mistral-7B-v0.2, for different architectures.",
            "result_discussion": "Retrieval heads are universal and sparse, present in pretrained models, reused in extended context scenarios, and dynamically activated depending on the context. These properties are consistent across various models and architectures.",
            "ablation_id": "2404.15574v1.No1"
        },
        {
            "research_objective": "Examine how retrieval heads influence factuality in downstream tasks like Needle-in-a-Haystack.",
            "experiment_process": "Using Mistral-7B-Instruct-v0.2, the study performs Needle-in-a-Haystack tests. It constructs new tests besides the three sets used for retrieval head detection. The model's behavior shifts are observed as retrieval/random heads are masked out progressively.",
            "result_discussion": "Results show that masking retrieval heads significantly worsens Needle-in-a-Haystack performance, leading to incomplete retrieval, hallucination, and wrong extraction. As masked heads increase, checkpoint performances fall, showing retrieval heads' crucial role.",
            "ablation_id": "2404.15574v1.No2"
        },
        {
            "research_objective": "Assess the impact of retrieval heads on extractive QA tasks.",
            "experiment_process": "An extractive QA dataset is synthesized using up-to-date news articles. Paragraphs are extracted, and question-answer pairs are generated. F1 scores are measured as retrieval/random heads are masked out.",
            "result_discussion": "Masking retrieval heads results in significant F1 score drops (9.2% and 23.1%), showcasing retrieval heads' importance in document QA tasks.",
            "ablation_id": "2404.15574v1.No3"
        },
        {
            "research_objective": "Explore the necessity of retrieval heads in chain-of-thought (CoT) reasoning across different tasks.",
            "experiment_process": "Tasks include MMLU, MuSiQue, and GSM8K, evaluated with Mistrial-7B-Instruct-v0.2, with and without CoT reasoning. The model's performance is tested as retrieval/random heads are masked.",
            "result_discussion": "Masking retrieval heads dramatically affects CoT reasoning, causing hallucinations and making the model 'blind' to key input information. It indicates retrieval heads' essential role in complex reasoning tasks.",
            "ablation_id": "2404.15574v1.No4"
        }
    ]
}