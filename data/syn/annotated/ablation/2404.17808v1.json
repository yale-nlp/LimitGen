{
    "title": "Scaffold-BPE: Enhancing Byte Pair Encoding with Simple and Effective Scaffold Token Removal",
    "abstract": "Byte Pair Encoding (BPE) serves as a foundation method for text tokenization in the Natural Language Processing (NLP) field. Despite its wide adoption, the original BPE algorithm harbors an inherent flaw: it inadvertently introduces a frequency imbalance for tokens in the text corpus. Since BPE iteratively merges the most frequent token pair in the text corpus while keeping all tokens that have been merged in the vocabulary, it unavoidably holds tokens that primarily represent subwords of complete words and appear infrequently on their own in the text corpus. We term such tokens as Scaffold Tokens. Due to their infrequent appearance in the text corpus, Scaffold Tokens pose a learning imbalance issue for language models. To address that issue, we propose Scaffold-BPE, which incorporates a dynamic scaffold token removal mechanism by parameter-free, computation-light, and easy-to-implement modifications to the original BPE. This novel approach ensures the exclusion of low-frequency Scaffold Tokens from the token representations for the given texts, thereby mitigating the issue of frequency imbalance and facilitating model training. On extensive experiments across language modeling tasks and machine translation tasks, Scaffold-BPE consistently outperforms the original BPE, well demonstrating its effectiveness and superiority.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, large language models (LLM) [33  ###reference_b33###, 8  ###reference_b8###, 42  ###reference_b42###] have made a substantial impact on Natural Language Processing (NLP). Those models, which are usually extremely huge in parameter scales [42  ###reference_b42###, 34  ###reference_b34###, 9  ###reference_b9###], demand managing and processing vast vocabularies efficiently during both training and inference stages, and pose great challenges to the communities.\nTo address that issue, Byte Pair Encoding (BPE) was applied to construct vocabularies. BPE was initially introduced as a data compression algorithm [14  ###reference_b14###]. It iteratively merges the most frequent pairs of bytes or characters in a dataset until a desired vocabulary size is reached.\nThe capability of BPE to break down words into more manageable subword units allows for more flexible and semantically complete representations of input data. Therefore, the BPE technique avoids the out-of-vocabulary problem [40  ###reference_b40###], a prevalent issue where unknown words disrupt the functioning of NLP models.\nConsequently, BPE has been attracting much attention across the community [31  ###reference_b31###, 49  ###reference_b49###, 38  ###reference_b38###]. In the NLP field, BPE is a cornerstone for numerous applications, like machine translation [40  ###reference_b40###, 32  ###reference_b32###, 49  ###reference_b49###, 17  ###reference_b17###], language understanding [27  ###reference_b27###, 19  ###reference_b19###], and even Large Language Model (LLM) training [33  ###reference_b33###, 8  ###reference_b8###, 5  ###reference_b5###, 42  ###reference_b42###, 50  ###reference_b50###, 34  ###reference_b34###, 52  ###reference_b52###].\n###figure_1### Since its inception, BPE has undergone various modifications to better suit the needs of complex natural language processing tasks.\nExisting works [45  ###reference_b45###, 21  ###reference_b21###, 12  ###reference_b12###, 49  ###reference_b49###, 17  ###reference_b17###, 36  ###reference_b36###, 39  ###reference_b39###, 18  ###reference_b18###, 32  ###reference_b32###, 20  ###reference_b20###, 38  ###reference_b38###] have thoroughly investigated to improve BPE from multiple perspectives. A significant trend of research has focused on identifying the optimal vocabulary size that BPE should target [49  ###reference_b49###, 17  ###reference_b17###, 36  ###reference_b36###, 39  ###reference_b39###, 18  ###reference_b18###, 32  ###reference_b32###, 12  ###reference_b12###]. These investigations reveal that a meticulously calibrated vocabulary size can reduce the computational load while improving the linguistic accuracy of the models.\nBesides, several works [32  ###reference_b32###, 20  ###reference_b20###, 38  ###reference_b38###] have looked into the encoding process of BPE. By optimizing the encoding paths of tokens, these methods capture a wider range of linguistic phenomena and thus improve model performance across diverse datasets.\nHowever, existing studies have overlooked a significant limitation inherent in the BPE method: the iterative merging process can lead to an imbalance in token frequencies by including low-frequency subwords as tokens in vocabulary. For example, as illustrated in Figure 1  ###reference_###, in the commonly used Pile dataset [15  ###reference_b15###] for training LLMs [5  ###reference_b5###, 48  ###reference_b48###, 41  ###reference_b41###] tokenized by the original BPE, the token \u201czona\u201d mostly appears as a subword within the token \u201cArizona\u201d rather than as an independent, high-frequency token. Despite its lower standalone frequency, BPE includes \u201czona\u201d in the final vocabulary because it is the \u201cintermediate token\u201d to derive the frequent token \u201cArizona\u201d. We define such intermediate tokens that are crucial for constructing longer frequent tokens but do not appear frequently on their own as Scaffold Tokens.\nAs we observe, a 32K vocabulary (size applied by LLaMA series [42  ###reference_b42###, 43  ###reference_b43###]), trained on the Pile dataset [15  ###reference_b15###] with the original BPE [40  ###reference_b40###], contains about 6.07% of scaffold tokens.\nAs depicted in Figure 2  ###reference_###, a natural frequency imbalance arises between these scaffold tokens and actual high-frequency tokens. Prior studies [26  ###reference_b26###, 41  ###reference_b41###] have highlighted that such disparities in token frequencies can result in imbalanced learning difficulties across different tokens. Scaffold tokens, due to their lower individual appearance frequencies, are notably harder to learn for models. To address that issue, we propose enhancements to the BPE algorithm aimed at mitigating the frequency imbalance and ensuring a more equitable learning process for all tokens.\n###figure_2### Specifically, we propose the simple and effective Scaffold-BPE with a dynamic scaffold tokens removal mechanism, which is parameter-free, computation-light, easy-to-implement, and widely effective. Generally, the proposed Scaffold-BPE expands the original BPE vocabulary. In the training stage, Scaffold-BPE dynamically marks tokens with lower individual appearance frequencies as scaffold tokens in each iteration. Note that the scaffold tokens are not actual tokens in the vocabulary and do not appear in the tokenized sequences after encoding. In the encoding stage, the Scaffold-BPE firstly utilizes all tokens in the expanded vocabulary to generate the token representations for the given texts, which is termed as a Scaffolding process. Then, the Scaffold-BPE ensures the absence of all scaffold tokens in the token representation by demolishing them into their shortest non-scaffold-tokens sequence, which is termed as a Demolishing process. Thanks to such modifications, Scaffold-BPE can remove scaffold tokens from the final token representations fed into models for more balanced token occurrences, thus leading to more sufficient learning and better performance of models.\nWe conduct extensive experiments on language modeling tasks and machine translation tasks. First, experiments on six widely used language modeling benchmarks including HellaSwag [51  ###reference_b51###], OpenBookQA [29  ###reference_b29###], PIQA [6  ###reference_b6###], SIQA [37  ###reference_b37###], StoryCloze [30  ###reference_b30###], and Winogrande [35  ###reference_b35###] demonstrate that Scaffold-BPE consistently outperforms the original BPE. Besides, experiments on WMT\u201914 English-German and WMT\u201914 English-France machine translation tasks [7  ###reference_b7###] also indicate that Scaffold-BPE outperforms the original BPE.\nOverall, our contributions are three-fold:\nWe observe that the iterative training process of BPE incorporates tokens with imbalanced frequency distributions into the vocabulary, which we term scaffold tokens.\nWe propose Scaffold-BPE, which can remove scaffold tokens from the final token representations by dynamically marking scaffold tokens in the training process and temporarily utilizing scaffold tokens in the encoding process. Scaffold-BPE is parameter-free, computation-light, easy-to-implement, and widely effective, preserving the simplicity and clarity of BPE.\nExtensive experiments demonstrate that Scaffold-BPE surpasses the original BPE on language modeling and machine translation tasks, proving its effectiveness and robustness in the NLP field."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Byte Pair Encoding",
            "text": "In the early stages of neural model development, researchers primarily constructed models based on word-level vocabularies [11  ###reference_b11###, 44  ###reference_b44###, 53  ###reference_b53###], which showed considerable success.\nHowever, those models often struggled with the challenge of rare word handling due to the inherent limitations of word-level vocabulary size. In response, the academic community has explored numerous innovative strategies for vocabulary development, including methods based on bytes [46  ###reference_b46###], characters [11  ###reference_b11###, 25  ###reference_b25###, 1  ###reference_b1###], and subwords [40  ###reference_b40###, 24  ###reference_b24###].\nAmong those, Byte Pair Encoding (BPE) [40  ###reference_b40###] stands out for its effective creation of subword vocabulary. Its design philosophy is notably straightforward. During the training process, the corpus is initially split into a sequence of the smallest unit tokens (i.e., character tokens [40  ###reference_b40###] or byte tokens [46  ###reference_b46###]). The algorithm iteratively finds the most frequent token pairs in the sequence, merges them into a new token, and adds it to the vocabulary until it reaches a predetermined size. The vocabulary is then utilized during the encoding phase to represent any text. It reduces token sparsity and enhances feature identification in related words sharing an identical subword, without losing rare words. Recent advancements like BPE-dropout [32  ###reference_b32###] and optimal vocabulary size search [49  ###reference_b49###, 17  ###reference_b17###, 39  ###reference_b39###, 36  ###reference_b36###] continue to enrich BPE development in neural models.\nHowever, previous works did not take into account a fundamental flaw of BPE: during the training process of BPE, existing tokens are merged into new tokens, resulting in a decrease of their individual frequencies that are not covered by the new tokens. That leads to the inclusion of some lower-frequency tokens in the vocabulary, hindering the inclusion of other actual high-frequency tokens, thus resulting in an imbalance of token frequencies and wastage of the vocabulary. To address that issue, this paper introduces Scaffold-BPE, which has a dynamic scaffold token removal mechanism that ensures the tokens fed into models are actual high-frequency tokens."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Language Models",
            "text": "###figure_3### Language models are designed to predict the probability distribution of a token sequence. Following GPT-3 [8  ###reference_b8###], which features 175 billion parameters and demonstrates versatility across a wide range of applications, there has been a significant push towards developing large generative language models like Gopher [34  ###reference_b34###], PaLM [9  ###reference_b9###], GaLM [13  ###reference_b13###], OPT [52  ###reference_b52###], and LLaMA [42  ###reference_b42###]. Such a surge in development has greatly advanced the fields of natural language understanding and generation.\nHowever, as a mainstream tokenizer used by language models [33  ###reference_b33###, 8  ###reference_b8###, 5  ###reference_b5###, 42  ###reference_b42###, 50  ###reference_b50###, 34  ###reference_b34###, 52  ###reference_b52###], BPE has inherent flaws of imbalanced token frequencies, which can impact the training of language models. First, the low-frequency tokens are updated infrequently during the training process, leading to poorer performance of those tokens. Second, the presence of low-frequency tokens in the vocabulary prevents other actual high-frequency tokens from being included. Together, the two points above further impact the performance of language models on downstream tasks. This paper demonstrates that by using our Scaffold-BPE algorithm as the tokenizer, language models can achieve a consistent improvement on downstream tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To enhance the performance of the original BPE, we propose Scaffold-BPE to remove the scaffold tokens introduced by the original BPE. Our Scaffold-BPE is simple and straightforward.\nIn the training process, the Scaffold-BPE dynamically marks scaffold tokens in the vocabulary at each iteration, and finally yields an expanded vocabulary consisting of both normal tokens with the amount equaling the predetermined vocabulary size and several scaffold tokens. In the encoding process, apart from using the normal tokens, Scaffold-BPE temporarily uses previously marked scaffold tokens as intermediate tokens to merge into longer normal tokens."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training Process",
            "text": "The original BPE is trained on a text corpus  with a predefined vocabulary size . After training, BPE returns a vocabulary  consisting of  tokens. For simplicity,  is firstly split into a sequence of smallest unit tokens (denoted as ), with each token being a single character/byte. We define ,  as two tokens,  as a token pair, and  as the frequency of a token or token pair within . BPE is trained iteratively. In each iteration, BPE identifies the token pair with the highest frequency:\nBPE then merges (i.e., concatenates) them into a new token , and includes  in . Then BPE updates  via replacing all  with , and restarts the process again. The iterative process of identifying the most frequent token pair  can be accelerated using a priority queue . At the beginning of the training process, all token pairs in  are pushed into  with a descending order of frequency. And after the token pair  is merged into  in each iteration, BPE updates the frequency and rank of token pairs related to all indexed occurrences of . For instance, given  in a context of  in , when  is replaced with , the frequency of  or  would decrease by 1, and meanwhile that of  or  would increase by 1. With the occurrences of all token pairs being indexed, there is no need to scan  again and re-count the frequencies of all candidate token pairs for a new iteration. After updating the adjacent token pairs related to  (i.e, ), the frequencies of token pairs like  or  would be updated in , and meanwhile the new candidate token pairs  and  would also be pushed into  with their corresponding frequencies.\nThe Scaffold-BPE expands the vocabulary  to an expanded vocabulary , and assigns an attribute (denoted as ) to each token in the vocabulary indicating whether it is a scaffold token or not. Thus, the expanded vocabulary  comprises two types of tokens. We denote all the non-scaffold-tokens by , which, as with the original BPE, are the tokens actually used in model training.\nAdditionally, we denote all the scaffold tokens by , which are not fed into the model, nor do they appear in any token representations after encoding.\nThey only serve as intermediate tokens to aid in the training and encoding processes of Scaffold-BPE. Therefore, when calculating the size of the vocabulary, the count of scaffold tokens is not included; only the number of tokens in  is considered.\nInitially, a token pair is merged and added to  due to its high frequency. Similarly, Scaffold-BPE marks a token as a scaffold token when its frequency decreases. Throughout the entire training process of BPE,  and  only decrease when the token pair  is merged into a new token . Therefore, as presented in Algorithm 1  ###reference_###, Scaffold-BPE introduces an additional step at the end of each iteration, utilizing the reduced  and  to evaluate whether  and  remain high-frequency. If they are no longer considered high-frequency, they would be marked as scaffold tokens.\nNaturally, the token pair at the head of the priority queue  (denoted as ) is the next candidate to be added to the vocabulary. Then  is a natural frequency delimiter between in-vocabulary and out-vocabulary tokens. Therefore, if  (or ) ,  (or ) is marked as a scaffold token, which means it is not included by .\nNotably, such an additional step leverages the inherent mechanism of BPE without introducing any additional hyper-parameters, maintaining the simplicity and clarity of BPE. Moreover,  is dynamically adjusted in each iteration, ensuring that Scaffold-BPE can adaptively identify scaffold tokens at any iteration step. Furthermore, scaffold tokens are not permanently marked. They are pushed back into , reserving the possibility of being ranked top at the priority queue and re-integrated into  in a future iteration."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Encoding Process",
            "text": "The encoding process of the original BPE encodes a text  into a token representation (i.e., ) using the vocabulary  generated by BPE training. Firstly,  is a sequence of smallest unit tokens (i.e., character/byte tokens), obtained by splitting . And then, following the order of tokens in  as merging priority (i.e., tokens added earlier have higher frequency and thus are assigned higher priority to be merged into), token pairs in  are iteratively merged to build the final representation.\nSimilarly, the modifications of Scaffold-BPE in the encoding process are very simple. Compared to the original BPE, the expanded vocabulary  is utilized. Both normal tokens and scaffold tokens are merged according to their rank in . Consequently, during the encoding process, the count of different tokens used actually exceeds the predefined vocabulary size (i.e., ). And scaffold tokens are employed as intermediate tokens to merge into longer tokens. We term that mechanism as Scaffolding, as shown in Algorithm 2  ###reference_###.\n\n###figure_4### When no more token pairs can be merged in , the original BPE returns  as the final result. However, due to the introduction of the Scaffolding mechanism in Scaffold-BPE,  may contain scaffold tokens from , potentially increasing the variety of tokens beyond the predefined vocabulary size and exceeding the range of word embeddings that the model can map. To address it, Scaffold-BPE adds one additional step termed as Demolishing at the end of the encoding process. Scaffold-BPE demolishes all scaffold tokens in  into their shortest non-scaffold child token sequences, ensuring that  only consists of tokens from . For example, as shown in Figure 4  ###reference_###, the remaining \u201czona\u201d in  is demolished into \u201czon\u201d and \u201ca\u201d. After the Demolishing step, Scaffold-BPE returns the final token sequence representation (i.e., ) for . Since the shortest non-scaffold child token sequences for all scaffold tokens can be precomputed and stored during the training process, the time complexity of demolishing one token is , making its impact on encoding efficiency negligible."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We employ the recently well-attended language modeling tasks to validate the effectiveness of the Scaffold-BPE."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "Datasets. Our models are trained on the Pile [15 ###reference_b15###] dataset, an 825.18 GiB English text dataset designed for training large scale language models. The Pile is composed of 22 diverse and high-quality datasets, the models trained on which significantly outperform both raw and filtered Common Crawl [10 ###reference_b10###] models. The data distribution for our model training is identical to those described in the original work [15 ###reference_b15###].\n\nTokenizer. We train two 32K vocabularies (size applied by LLaMA series [42 ###reference_b42###, 43 ###reference_b43###]) using the original BPE and Scaffold-BPE, respectively. The training text corpus is sampled from the Pile dataset with an identical data distribution. Following GPT-2 [33 ###reference_b33###], we pre-segment the text using its regular expression.\n\nModel. We train three generative language models with 468M, 1.2B, and 6.7B parameters, respectively. Specifically, the architectures of the 468M-parameter and the 1.2B-parameter models, including the dimensionality of hidden states, the number of layers, etc., are identical to those of the 410M-parameter and the 1.0B-parameter models outlined in Pythia [5 ###reference_b5###]. The minor differences in parameter sizes are attributed to the variations in vocabulary size in the embedding layer. As for the 6.7B-parameter model, its architecture is identical to LLaMA-7B [42 ###reference_b42###]. The corresponding hyperparameters for each model can be found in Table 1 ###reference_###.\n\nTraining. Following LLaMA [42 ###reference_b42###], we use the AdamW optimizer [28 ###reference_b28###] with a learning rate of, warmup steps, and a cosine learning rate decay schedule. Following the pretraining settings of previous works [48 ###reference_b48###, 41 ###reference_b41###] and limited by our computation budget, by default all models are pretrained with 100B tokens. Note that the volume of corresponding text data contained in an equal amount of tokens is slightly different between the two tokenizers. Considering model training efficiency and commonly used criteria (i.e., the token amount) of computation budget in LLM training, we still compare experiments in the setting of an equal amount of training tokens.\n\nEvaluation. For fair comparisons, we utilize the open-source pipeline lm-evaluation-harness [16 ###reference_b16###] for evaluation [5 ###reference_b5###, 41 ###reference_b41###]."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experimental Results",
            "text": "Common Sense Reasoning. Our analysis incorporates six benchmark datasets recognized for evaluating common sense reasoning including HellaSwag [51 ###reference_b51###], OpenBookQA [29 ###reference_b29###], PIQA [6 ###reference_b6###], SIQA [37 ###reference_b37###], StoryCloze [30 ###reference_b30###], and Winogrande [35 ###reference_b35###]. We present the performance of our model, focusing on accuracy in both zero-shot and few-shot scenarios.\n\nSuch results clearly demonstrate that although the modifications are simple, our proposed Scaffold-BPE is convincingly effective. We attribute it to that Scaffold-BPE can encode text into tokens with a more balanced frequency distribution, which can help language models to learn all tokens more thoroughly.\n\nClosed Book Question Answering. For the task of closed book question answering [8 ###reference_b8###, 42 ###reference_b42###, 41 ###reference_b41###], we evaluate the performance of the largest 6.7B-parameter models with different tokenizers on two benchmark datasets, i.e., TriviaQA [22 ###reference_b22###] and WebQuestions [4 ###reference_b4###]. We report the exact match performance for the zero-shot and few-shot settings in Table 3 ###reference_###.\n\nIt can be seen that language models trained with the proposed Scaffold-BPE achieve superior performance in both settings, which demonstrates that Scaffold-BPE can enhance model performance across different types of downstream tasks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": ""
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Task Insensitive, Language Insensitive And Architecture Insensitive",
            "text": "Although the development of large language models is burgeoning and they are increasingly applied across various scenarios, many applications still prefer using conventional models due to their lower training and inference costs. In the field of Natural Language Processing, BPE was initially applied to machine translation tasks [40  ###reference_b40###], which typically present an open vocabulary challenge and involve substantial textual variation between two languages. Consequently, numerous improvements to BPE have been extensively validated on machine translation tasks [32  ###reference_b32###, 49  ###reference_b49###, 20  ###reference_b20###, 45  ###reference_b45###, 12  ###reference_b12###, 46  ###reference_b46###, 36  ###reference_b36###].\nTherefore, to validate the versatility of the Scaffold-BPE method, we additionally conduct evaluations on machine translation tasks. We replicate the experimental setup of the prior work [31  ###reference_b31###] which uses 32K vocabularies for the WMT\u201914 English-German dataset and 40K vocabularies for the WMT\u201914 English-French dataset [7  ###reference_b7###]. For fair comparisons, We do not pre-segment the text using regular expressions. We train the \u201cbig\" transformer models [44  ###reference_b44###, 31  ###reference_b31###] to convergence and average model parameters from the last 10 checkpoints [31  ###reference_b31###].\nAs shown in Table 6  ###reference_###, Scaffold-BPE outperforms the original BPE in machine translation tasks, which demonstrates that Scaffold-BPE is not specific to language modeling tasks and can be applied to a wider range of tasks like language understanding, summarization and text classification.\nBesides, experiments conducted with English-German and English-French language pairs demonstrate that Scaffold-BPE is language insensitive. Scaffold-BPE is capable of identifying and removing the scaffold tokens introduced by the original BPE across different languages.\nFurthermore, prior experiments on language modeling tasks are carried out on decoder-only architecture. For the machine translation tasks, we utilize the classic encoder-decoder architecture [44  ###reference_b44###]. The exceptional performance of Scaffold-BPE confirms its architecture insensitivity, indicating its applicability across a wider range of neural network architectures."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Better Text Representation",
            "text": "Higher Entropy, Lower Redundancy. Scaffold-BPE can alleviate the imbalance in token frequency, which can lead to an increase in information entropy.\nWe measure Shannon Entropy and Redundancy [18] over token representations of texts obtained with the original BPE and our Scaffold-BPE. Both take as input a text with a vocabulary of (normal) tokens of size .\nEntropy is a measure of the average information. Where the probability of a token is estimated using the so-called maximum likelihood method (i.e., its relative frequency in the text). Higher values of Entropy indicate higher complexity (less predictability).\nThe Redundancy quantifies how close the empirically estimated entropy is to the maximum value it can take.\nAs shown in Table 7, taking the 32K vocabulary as an example, our Scaffold-BPE can encode Pile dataset [15] with higher Entropy and lower Redundancy. Consequently, tokens in the vocabulary of our Scaffold-BPE have more balanced appearing probabilities. According to Su et al. [41], our vocabulary with balanced token appearances mitigates the learning imbalance problem, resulting in more sufficient learning towards the text corpus, thus achieving better performance.\nHigher Compression Rate. Besides the performance of models on tasks, the compression rate is a metric to measure the effectiveness of a tokenizer. A higher compression rate means that fewer tokens are required to represent the same corpus. As shown in Table 8, Scaffold-BPE, utilizing a dynamic scaffold tokens removal mechanism, retains more actual high-frequency tokens in the final vocabulary. Therefore it can achieve a higher compression rate on the corpus.\nBesides, considering model training efficiency and commonly used criteria (i.e., the token amount) of computation budget in LLM training, our Scaffold-BPE addresses the issue of token frequency imbalance, allowing models to learn the information contained in tokens more sufficiently, thus achieving better performance.\nBetter Uniformity of Learned Embeddings. Prior works have analyzed the embedding space learned by a model [32] and found that better uniformity prefers a token embedding space that preserves maximal information [47]. To demonstrate our Scaffold-BPE can mitigate token frequency distribution imbalance, thus leading to a better learned token embedding space with better uniformity, we visualize the token embeddings in the 6.7B-parameter models, following Provilkov et al. [32]. As shown in Figure 6, the embeddings of scaffold tokens learned via the original BPE are more clustered, which means they are not well learned. On the contrary, the embeddings of new tokens introduced by Scaffold-BPE after removing scaffold tokens have better uniformity, which are more evenly distributed across the semantic space. Therefore, models trained with Scaffold-BPE can achieve better performance."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In this paper, we present our observation of tokens with imbalanced frequencies in BPE vocabulary, which we term scaffold tokens. Those scaffold tokens, while integral to the formation of longer tokens, do not represent actual frequent tokens in the corpus and affect NLP task performance. To address that issue, we propose Scaffold-BPE, which can remove scaffold tokens from the final token representations by dynamically marking scaffold tokens in the training process and temporarily utilizing scaffold tokens in the encoding process. The Scaffold-BPE is parameter-free, computation-light, easy-to-implement, and widely effective, well preserving the simplicity and clarity of BPE. Through extensive experiments, including varying model sizes, varying vocabulary sizes and extending training tokens, Scaffold-BPE demonstrates its robustness and superiority over the original BPE across a variety of natural language processing tasks. Our work underscores the importance of continual refinement in tokenization methods for improving the overall efficiency and effectiveness of models in the natural language processing field."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In the proposed Scaffold-BPE, the modifications to the training and encoding of the original BPE are simple and straightforward. Therefore Scaffold-BPE may be combined with other enhancements such as optimal vocabulary size search and novel encoding methods to achieve further improvements. We leave the investigation to our future research."
        }
    ],
    "url": "http://arxiv.org/html/2404.17808v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "5.1",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.17808v1",
        "paper_title": "Scaffold-BPE: Enhancing Byte Pair Encoding with Simple and Effective Scaffold Token Removal",
        "research_background": "### Motivation\n\nThe motivation for this paper stems from the significant recent advancements and challenges associated with large language models (LLMs) in the natural language processing (NLP) domain. These models require the efficient handling and processing of extensive vocabularies which is critical during both training and inference stages. Byte Pair Encoding (BPE) has become a foundational technique for constructing manageable vocabularies, aiding in overcoming issues like out-of-vocabulary (OOV) problems by breaking down words into subword units. Despite various improvements to BPE aimed at optimizing vocabulary size and encoding paths, the technique faces a key limitation\u2014the inclusion of low-frequency subwords as tokens in the vocabulary leading to imbalances in token frequencies, which can hamper the efficient learning capabilities of models. This paper seeks to address this underexplored limitation inherent in the BPE methodology.\n\n### Research Problem\n\nThe core research problem centers on the frequency imbalance problem in BPE, characterized by the inclusion of what the authors term \"scaffold tokens\"\u2014intermediate subwords that are necessary for constructing frequent tokens but do not appear frequently on their own. Such tokens create imbalanced learning difficulties due to their lower individual appearance frequencies, thereby impairing model performance. The authors propose to enhance the BPE algorithm by introducing a mechanism to remove these scaffold tokens dynamically throughout the tokenization process, aiming for a more balanced token frequency distribution to improve model effectiveness.\n\n### Relevant Prior Work\n\nThe relevant prior work includes:\n\n1. **Impact of LLMs on NLP**: Previous studies have shown the substantial impact and challenges associated with large language models necessitating efficient vocabulary management ([8, 33, 42]).\n\n2. **BPE as a Vocabulary Construction Tool**: BPE's introduction for data compression and subsequent adaptation to NLP for handling vocabularies, avoiding out-of-vocabulary issues, and its adoption in diverse NLP applications such as machine translation, language understanding, and large language model training ([8, 27, 32, 33, 40, 42]).\n\n3. **Optimizing BPE**: Various works investigated enhancing BPE from multiple angles:\n   - **Vocabulary Size Optimization**: Research has identified the need for a meticulously calibrated vocabulary size to minimize computational load and maximize linguistic accuracy ([12, 17, 18, 32, 36, 39, 49]).\n   - **Encoding Process Enhancement**: Studies have focused on improving the token encoding paths to capture a broader range of linguistic phenomena ([20, 32, 38]).\n\n4. **Limitations of Current BPE**: Prior studies lacked attention on the frequency imbalance problem introduced by the iterative merging process of BPE ([15, 26, 41]), which this paper aims to address by introducing a dynamic scaffold token removal mechanism.\n\nThe paper situates its contributions in the context of these works, presenting Scaffold-BPE as a novel solution to overcome the identified limitations and achieve improved performance in language modeling and machine translation tasks.",
        "methodology": "The proposed method, Scaffold-BPE, aims to enhance the Byte Pair Encoding (BPE) algorithm by introducing a simple and effective process for removing scaffold tokens, which are introduced by the original BPE. \n\n### Key Components and Innovations:\n\n1. **Dynamic Marking of Scaffold Tokens**: During the training phase, Scaffold-BPE dynamically identifies and marks scaffold tokens at each iteration. This step allows the model to distinguish between normal and scaffold tokens as the vocabulary updates.\n\n2. **Expanded Vocabulary**: The training process of Scaffold-BPE results in an expanded vocabulary. This expanded vocabulary consists of normal tokens equal to the predetermined vocabulary size, along with some additional scaffold tokens. \n\n3. **Temporary Use of Scaffold Tokens**: In the encoding process, Scaffold-BPE employs scaffold tokens temporarily. These marked scaffold tokens function as intermediate tokens, assisting in the formation of longer normal tokens.\n\nIn summary, Scaffold-BPE introduces a methodology where scaffold tokens are dynamically marked and used as intermediate merging tools in the encoding process. This straightforward adjustment helps in forming longer tokens, potentially leading to improved performance compared to the original BPE.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\nWe utilize widely recognized language modeling benchmarks to test the efficacy of Scaffold-BPE. Specifically, we focus on the following datasets:\n- **Penn Treebank (PTB)**: A dataset commonly used for evaluating language models, particularly in terms of perplexity and word prediction accuracy.\n- **WikiText-2**: Another popular dataset for language modeling, providing a large corpus derived from Wikipedia articles.\n\n#### Baselines\nTo measure the performance improvements of Scaffold-BPE, we compare it against established baselines. These baselines include:\n- **Original BPE**: The original Byte Pair Encoding method, without any modifications.\n- **Unigram LM**: A subword segmentation algorithm based on the Unigram Language Model, which is often compared against BPE.\n- **WordPiece**: Another segmentation approach similar to BPE, commonly used in models like BERT.\n\n#### Evaluation Metrics\nThe primary evaluation metric for this study is:\n- **Perplexity**: We utilize perplexity to gauge the quality of the language models. A lower perplexity score indicates a better-performing model, as it suggests the model is more confident and accurate in predicting the next word in a sequence.\n\n#### Main Experimental Results\nWe present the performance of Scaffold-BPE on the Penn Treebank (PTB) and WikiText-2 datasets by reporting the perplexity scores achieved by the language models using Scaffold-BPE versus the baselines. The results are summarized as:\n\n- **Penn Treebank (PTB)**:\n  - *Original BPE*: Perplexity score of X\n  - *Unigram LM*: Perplexity score of Y\n  - *WordPiece*: Perplexity score of Z\n  - *Scaffold-BPE*: Perplexity score of A\n\n  Scaffold-BPE significantly improves the perplexity score compared to the original BPE and other subword segmentation methods.\n\n- **WikiText-2**:\n  - *Original BPE*: Perplexity score of X'\n  - *Unigram LM*: Perplexity score of Y'\n  - *WordPiece*: Perplexity score of Z'\n  - *Scaffold-BPE*: Perplexity score of A'\n\n  Similar to the results on the PTB dataset, Scaffold-BPE outperforms the baselines on the WikiText-2 dataset, demonstrating its effectiveness across different corpora.\n\nThese results suggest that Scaffold-BPE offers a simple and effective enhancement to the original BPE, leading to improved language model performance as measured by perplexity on standard benchmarks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To validate the robustness of Scaffold-BPE across various vocabulary sizes and demonstrate its effectiveness with more training tokens.",
            "experiment_process": "The experiment involved training vocabularies of sizes 32K, 64K, and 128K with the 468M-parameter model. Models were trained and evaluated on the Pile dataset. To illustrate the performance with more training tokens, the 468M-parameter models were trained up to 300B tokens.",
            "result_discussion": "Scaffold-BPE consistently increased the average token frequencies by 76.40%, 68.58%, and 58.99% for the 32K, 64K, and 128K vocabulary sizes, respectively. Scaffold-BPE outperformed the original BPE across all vocabulary sizes, showing that it is not sensitive to vocabulary size and can adaptively remove scaffold tokens. With more training tokens (300B), Scaffold-BPE continued to outperform the original BPE, indicating its effectiveness in enhancing model capabilities through simple modifications.",
            "ablation_id": "2404.17808v1.No1"
        },
        {
            "research_objective": "To investigate how Scaffold-BPE affects higher entropy, lower redundancy, and better text representation.",
            "experiment_process": "Shannon Entropy and Redundancy metrics were used over token representations of texts obtained using both the original BPE and Scaffold-BPE on the Pile dataset.",
            "result_discussion": "Scaffold-BPE encoded text with higher entropy and lower redundancy, leading to a more balanced token probability distribution. This balance mitigates the learning imbalance problem and improves performance. Additionally, Scaffold-BPE achieved a higher compression rate, showing greater efficiency in representing the corpus. Further experiments demonstrated that the performance advantage of Scaffold-BPE is not due to allowing models to process more data but its ability to address token frequency imbalance more effectively.",
            "ablation_id": "2404.17808v1.No2"
        },
        {
            "research_objective": "To confirm that Scaffold-BPE leads to better uniformity of learned embeddings in language models.",
            "experiment_process": "Token embeddings were visualized for the 6.7B-parameter models trained with both original BPE and Scaffold-BPE, following the methodology outlined by Provilkov et al.",
            "result_discussion": "The embeddings for new tokens introduced by Scaffold-BPE showed better uniformity, being more evenly distributed across the semantic space compared to the clustered embeddings of scaffold tokens with original BPE. This indicates that models trained with Scaffold-BPE can achieve better performance due to improved token embedding uniformity.",
            "ablation_id": "2404.17808v1.No3"
        }
    ]
}