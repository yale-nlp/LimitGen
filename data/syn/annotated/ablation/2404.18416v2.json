{
    "title": "Capabilities of Gemini Models in Medicine",
    "abstract": "Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with their strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini 1.0 and Gemini 1.5, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly integrate the use of web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks spanning text, multimodal and long-context applications, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search strategy, outperforming our prior best Med-PaLM 2 by 4.6%. Our search-based strategy generalizes with SoTA performance on complex diagnostic challenges from the New England Journal of Medicine (NEJM) and the GeneTuring benchmark. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health & medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5%. We demonstrate the effectiveness of Med-Gemini\u2019s long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini\u2019s performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization and referral letter generation, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for the promise of Med-Gemini in many areas of medicine, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Medicine is a multifaceted endeavor. A clinician\u2019s day-to-day work involves patient consultations, where clear communication of diagnoses, treatment plans, and empathy are essential for building trust. Complex cases necessitate deeper understanding of the patient\u2019s history within the electronic medical record, along with multimodal reasoning from medical images and other diagnostics. To guide their decisions under uncertainty, clinicians must stay abreast of the latest medical information from a wide variety of authoritative sources that can range from research publications to procedural videos. The art of care delivery hinges on a clinician\u2019s ability to perform advanced clinical reasoning, synthesize complex information from diverse and multimodal sources, and collaborate effectively with other clinicians to help people in their care journeys. Although artificial intelligence (AI) systems can assist individual medical tasks (Rajpurkar et al., 2022  ###reference_b102###) and demonstrate early promise towards multimodal multi-task \u201cgeneralist\u201d medical uses (Tu et al., 2024a  ###reference_b121###; Moor et al., 2023a  ###reference_b83###), the development of more sophisticated reasoning, multimodal, and long-context understanding capabilities would enable significantly more intuitive and helpful assistive tools for clinicians and patients alike.\nThe advent of large language models (LLMs) and large multimodal models (LMMs), like GPT-4 (Achiam et al., 2023  ###reference_b2###), PaLM (Chowdhery et al., 2023  ###reference_b16###) and Gemini (Gemini Team, Google, 2023  ###reference_b35###), showed that such models effectively encode clinical knowledge and can perform impressively in medical question answering benchmarks, even for complex cases and scenarios requiring specialized knowledge (Kanjee et al., 2023  ###reference_b61###; Eriksen et al., 2023  ###reference_b27###; Antaki et al., 2023  ###reference_b5###). However, performance on such tasks is far from indicative of real-world utility. The unique nature of medical data and the critical need for safety demand specialized prompting (Nori et al., 2023  ###reference_b86###), fine-tuning, or potentially both along with careful alignment of these models (Ouyang et al., 2022  ###reference_b92###).\nMedically fine-tuned LLMs (Singhal et al., 2023a  ###reference_b111###; Luo et al., 2022  ###reference_b77###; Toma et al., 2023  ###reference_b119###) can also provide high-quality long-form answers to nuanced and open-ended medical questions asked by millions of internet users, with Med-PaLM 2 surpassing physicians on axes such as factuality, reasoning, harm, and bias (Singhal et al., 2023b  ###reference_b112###). The potential extends beyond question answering. LMMs (Moor et al., 2023b  ###reference_b84###; Li et al., 2024  ###reference_b69###) such as Flamingo-CXR and Med-PaLM M are comparable with radiologists in controlled settings for generating radiology reports (Huang et al., 2023  ###reference_b50###; Tu et al., 2024a  ###reference_b121###; Tanno et al., 2024  ###reference_b117###). In the more challenging setting of text-based diagnostic consultations with patient actors, the Articulate Medical Intelligence Explorer (AMIE) model outperformed primary care physicians on several evaluation axes for diagnostic dialogue (Tu et al., 2024b  ###reference_b122###).\nDespite these promising results, there are considerable opportunities for improvement in performance. LLMs demonstrate suboptimal clinical reasoning under uncertainty, with confabulations and bias remaining key challenges (Umapathi et al., 2023  ###reference_b124###; Omiye et al., 2023  ###reference_b91###). The use of tools and up-to-date medical information (Zakka et al., 2024  ###reference_b144###) to accomplish medical tasks remains a challenge for LLMs, alongside effective collaboration with clinicians (McDuff et al., 2023  ###reference_b80###). Additionally, their ability to handle complex multimodal medical data (for example, integrating images, videos, and de-identified health records over time) is currently limited (Tu et al., 2024a  ###reference_b121###). Although these capabilities are particularly meaningful in medical applications, improvements in performance might be relevant beyond the medical domain. Tasks and benchmarks developed to measure and accelerate the progress of medical LLMs will be broadly impactful.\nThe Gemini models, as detailed in the Gemini 1.0 and 1.5 technical reports (Gemini Team, Google, 2023  ###reference_b35###, 2024  ###reference_b36###), are a new generation of highly capable multimodal models with novel foundational capabilities that have the potential to address some of these key challenges for medical AI. The models are transformer decoder models (Vaswani et al., 2017  ###reference_b126###; Brown et al., 2020  ###reference_b9###) enhanced with innovations in architecture, optimization and training data, enabling them to exhibit strong capabilities across various modalities including images, audio, video, and text. The recent addition of the mixture-of-experts architecture (Shazeer et al., 2017  ###reference_b109###; Fedus et al., 2022  ###reference_b29###) allows the Gemini models to efficiently scale and reason over significantly longer and more complex data at inference time.\nBuilding on the strengths of the Gemini models, we present Med-Gemini, a family of models fine-tuned and specialized for medicine. The notion of generalist medical AI models has received considerable attention with impressive demonstrations of the possibilities for such systems (Tu et al., 2024a  ###reference_b121###). However, while the generalist approach is an meaningful research direction for medicine, real world considerations present trade-offs and requirements for task-specific optimizations which are at odds with each other. In this work, we do not attempt to build a generalist medical AI system. Rather, we introduce a family of models, each optimized for different capabilities and application-specific scenarios, considering factors such as training data, compute availability, and inference latency.\nMed-Gemini inherits Gemini\u2019s foundational capabilities in language and conversations, multimodal understanding, and long-context reasoning. For language-based tasks, we enhance the models\u2019 ability to use web search through self-training and introduce an inference time uncertainty-guided search strategy within an agent framework. This combination enables the model to provide more factually accurate, reliable, and nuanced results for complex clinical reasoning tasks. This leads to the state-of-the-art (SoTA) performance of 91.1% accuracy on MedQA (USMLE) (Jin et al., 2021  ###reference_b56###) surpassing prior Med-PaLM 2 models by 4.6%. We further conduct a careful examination of the MedQA (USMLE) data quality through relabelling with multiple independent expert clinicians, identifying unanswerable questions due to missing information and errors, enabling reliable analysis and characterization of our SoTA performance. The uncertainty-guided search strategy generalizes and leads to SoTA performance on the New England Journal of Medicine (NEJM) clinico-pathological conference (CPC) cases (Kanjee et al., 2023  ###reference_b61###; McDuff et al., 2023  ###reference_b80###) and the GeneTuring benchmark (Hou and Ji, 2023  ###reference_b48###). Beyond their strong performance on such benchmarks, our models suggest real-world utility by performing favorably when compared to human physicians on tasks such as medical note summarization and clinical referral letter generation.\nAs Gemini models are trained to accommodate textual input interleaved with a wide variety of other data modalities, they are known to excel in multimodal tasks. This confers impressive out-of-the-box SoTA performance on some multimodal medical benchmarks such as the NEJM Image Challenge. However, their performance can be further improved when dealing with specialized medical modalities not heavily represented in their pretraining data. We address this through multimodal fine-tuning and demonstrate the models\u2019 adaptability to novel medical modalities using customized encoders leading to SoTA performance on benchmarks such as Path-VQA (He et al., 2020  ###reference_b46###) and ECG-QA (Oh et al., 2023  ###reference_b90###) among others. We qualitatively showcase our models\u2019 capabilities for clinically-meaningful multimodal conversation on a variety of both in-distribution and out-of-distribution data modalities.\nFinally, the long-context capabilities of Gemini models open many exciting possibilities for application in medicine, given how frequently a clinically-meaningful decision requires parsing of large amounts of data with significant risks of \u201cinformation overload\u201d (Sbaffi et al., 2020  ###reference_b107###). Our Med-Gemini models configured for long-context processing are able to seamlessly analyze complicated and long-form modalities such as de-identified electronic health records (EHRs) and videos. We demonstrate the effectiveness of these capabilities with impressive performance on the \u201cneedle-in-a-haystack\u201d long EHR understanding (Johnson et al., 2019a  ###reference_b59###), medical instructional video question answering (Gupta and Demner-Fushman, 2022  ###reference_b43###), surgical action recognition from video (Goodman et al., 2021  ###reference_b39###), and the Critical View of Safety (CVS) assessment of surgical video (Strasberg and Brunt, 2010  ###reference_b115###) tasks.\nThe advances of Med-Gemini have great promise, but it remains crucial to carefully consider the nuances of the medical field, acknowledge the role of AI systems as assistive tools for expert clinicians, and conduct rigorous validation before real-world deployments at scale.\nOur key contributions are summarized below:\nMed-Gemini, our new family of multimodal medical models: We introduce a new family of highly capable multimodal medical models, built upon Gemini. Med-Gemini demonstrates important advancements in clinical reasoning, multimodal, and long-context capabilities. They are further fine-tuned to make use of web search for current information and can be customized to novel medical modalities through the use of modality-specific encoders.\nComprehensive benchmarking: We evaluate Med-Gemini\u2019s capabilities on a suite of 25 tasks across 14 medical benchmarks, encompassing text, multimodal, and long-context applications. To the best of our knowledge, this is the most comprehensive benchmarking of multimodal medical models to date.\nSoTA results on clinical language tasks: Med-Gemini optimized for clinical reasoning reaches a SoTA performance of 91.1% on MedQA (USMLE) using a novel uncertainty-guided search strategy. We quantify and characterize our performance improvements through a careful re-annotation of the MedQA dataset with clinical experts, finding these improvements to be meaningful. We further demonstrate the effectiveness of the search strategy through SoTA performance on NEJM CPC and GeneTuring benchmarks.\nMultimodal and long-context capabilities: Med-Gemini attains SoTA performance on 5 out of 7 multimodal medical benchmarks evaluated in this study. We demonstrate the effectiveness of multimodal medical fine-tuning and the ability to customize to novel medical modalities such as electrocardiograms (ECGs) using specialized encoder layers. Med-Gemini also exhibits strong long-context reasoning capabilities, attaining SoTA on challenging benchmarks such as \u201cneedle-in-the-haystack\u201d tasks in lengthy electronic health records or benchmarks for medical video understanding.\nIn addition, in forthcoming work, we will also rigorously explore the capabilities of Gemini in radiology report generation.\nReal-world utility of Med-Gemini: Beyond performance on popular medical benchmarks, we preview the potential real-world utility of Med-Gemini through quantitative evaluations on tasks such as medical note summarization, clinical referral letter generation, and EHR question answering. We further showcase qualitative examples in multimodal diagnostic dialogues and applications of the models\u2019 long-context capabilities for medical education, clinician-facing tools, and biomedical research. We note that such uses (particularly in safety-critical areas like diagnosis) would require considerable further research and development."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "As introduced in the Gemini technical reports (Gemini Team, Google, 2024  ###reference_b36###, 2023  ###reference_b35###), the Gemini ecosystem encompasses a suite of models varying in size, modality encoders, and architectures, trained on a wide variety of high quality data across many modalities.\nThe Gemini models exhibit state-of-the-art results across a diverse array of language, reasoning, coding, multilingual, image, and video benchmarks.\nNotably, the Gemini 1.0 Ultra model excels in language-based tasks that require complex reasoning, and the Gemini 1.5 Pro model adds the ability to efficiently handle and make use of long-context inputs spanning millions of tokens and/or multimodal inputs such as hours of video or tens of hours of audio. Gemini 1.0 Nano is the smallest model variant in the Gemini model family that can run efficiently on-device.\nWe develop our Med-Gemini models by building on the Gemini family, focusing on the following capabilities and methods:\nAdvanced reasoning via self-training and web search integration: For language tasks that require less complex reasoning, such as summarizing medical notes and creating referral letters, we introduce Med-Gemini-M 1.0 by fine-tuning the Gemini 1.0 Pro model. For other tasks that require more advanced reasoning, we introduce Med-Gemini-L 1.0 by fine-tuning the Gemini 1.0 Ultra model using a self-training method to enable the models to efficiently use web search. We develop a novel uncertainty-guided search strategy at inference time to improve performance on complex clinical reasoning tasks.\nMultimodal understanding via fine-tuning and customized encoders: The Gemini models are natively multimodal and have demonstrated impressive zero-shot performance on many multimodal benchmarks. However, the unique nature and heterogeneity of some medical modalities require fine-tuning to achieve the best possible performance. We introduce Med-Gemini-M 1.5 by performing fine-tuning with Gemini 1.5 Pro on a suite of multimodal medical datasets. We introduce Med-Gemini-S 1.0 and demonstrate the Gemini models\u2019 capability to adapt to novel medical modalities using specialized encoders with the Gemini 1.0 Nano model.\nLong-context processing with chain-of-reasoning: For the long-context processing tasks, we re-use Med-Gemini-M 1.5 with a long-context configuration. In addition, we also develop a novel inference-time chain-of-reasoning technique inspired by Tu et al. (2024b  ###reference_b122###) to enable better understanding of long EHRs.\nCollecting expert demonstrations of clinical reasoning, including how experts make informed use of knowledge retrieval tools such as web search, is both time-consuming and difficult to scale. To overcome this, we generate two novel datasets with self-training as described below: MedQA-R (Reasoning), which extends MedQA with synthetically generated reasoning explanations, or \u201cChain-of-Thoughts\u201d (CoTs), and MedQA-RS (Reasoning and Search), which extends MedQA-R with instructions to use web search results as additional context to improve answer accuracy.\nTo add further variety to the fine-tuning data mixture of Med-Gemini-L 1.0, we also add a long-form question answering dataset, which consists of  expert-crafted long-form responses to questions from HealthSearchQA, LiveQA, and MedicationQA in the MultiMedQA benchmark (Singhal et al., 2023a  ###reference_b111###), along with a medical summarization dataset, consisting of  clinician-written summaries of medical notes from MIMIC-III (Johnson et al., 2016  ###reference_b58###). We provide an overview of the datasets for language-based instruction fine-tuning datasets in Table C1  ###reference_###.\nInspired by the recent success of self-training for synthetic data generation (Tu et al., 2024b  ###reference_b122###), we implement an iterative data-generation framework targeted at curating high-quality synthetic examples of clinical reasoning with web search use.\n###figure_1### As depicted in the left panel of Figure 2  ###reference_###, we generate two reasoning paths, or CoTs, per training question: one without access to external information from search, and one that integrates search results as additional context during the CoT generation. Our self-training with search framework consists of the following key ingredients:\nWeb search: For each question, we prompt Med-Gemini-L 1.0 to generate search queries whose results would help answer the medical question. We then pass the search queries to a web search API and retrieve search results.\nIn-context demonstrations: For each type of reasoning response path, we hand-curate five expert demonstrations as seed with accurate clinical reasoning, explaining why the ground-truth answer is the best suited versus other potentially valid answers. For question examples with search results, the demonstrations explicitly refer to, and quote, the helpful information in the search results to best answer the question.\nGenerating CoTs: We prompt Med-Gemini-L 1.0 to generate CoTs using the in-context seed demonstrations over the train set. Before fine-tuning the model on the generated CoTs, we filter out the ones that lead to erroneous predictions.\nFine-tuning loop: After fine-tuning Med-Gemini-L 1.0 on the generated CoTs, the model\u2019s ability to follow the reasoning style and search integration of expert demonstrations improves. We then use the improved model to re-generate the CoTs, and iteratively repeat this self-training process until the model\u2019s performance saturates.\nBelow we provide a MedQA-RS example of an input prompt, along with the retrieved search results and an example of a generated CoT, which is then used to further fine-tune Med-Gemini-L 1.0. For brevity, we only display one representative search result in the example below.\nWe design a novel, uncertainty-guided and iterative search process to improve Med-Gemini-L 1.0\u2019s generations at inference time. As displayed in the right panel of Figure 2  ###reference_###, each iteration consists of four steps: multiple reasoning path generation, uncertainty-based search invocation, uncertainty-guided search query generation, and finally search retrieval for prompt augmentation.\nNote that while uncertainty-guided search at inference could potentially benefit multimodal settings, we only apply this approach to text-only benchmarks and leave multimodal exploration for future work.\nMultiple reasoning path generation: Given an input context prompt with a medical question, we generate multiple reasoning paths from Med-Gemini-L 1.0. For the first iteration, the prompt only consists of the instruction and question. For subsequent iterations, the prompt also includes search results from step (4) below.\nUncertainty-based search invocation: Given the multiple reasoning paths from step (1), we define an uncertainty measure based on the Shannon entropy of the answer choice distribution. Specifically, we calculate the probability of each answer choice by dividing its occurrence by the total number of responses, and apply the entropy based on the answer choice probabilities (Horvitz et al., 1984  ###reference_b47###). High entropy (model responses are more uniform across the different answer choices) indicates a high epistemic uncertainty. If the uncertainty for a question is higher than a defined threshold, we perform the uncertainty-guided search process in steps (3) and (4); otherwise, the majority vote answer is returned as the final answer.\nUncertainty-guided search query generation: Given conflicting responses from step (1), we prompt Med-Gemini-L 1.0 to generate three search queries whose results would help resolve the conflict. Our motivation of conditioning on previously generated but conflicting responses is to retrieve search results that are directly targeted at resolving the model\u2019s uncertainty to the question.\nSearch retrieval: The generated queries are then submitted to a web search engine, and the retrieved results are incorporated into Med-Gemini-L 1.0\u2019s input prompt for the next iteration, starting back at step (1). Augmenting the prompt with search results enables the model to refine its response by considering external relevant insights obtained from web search.\nWe use four image-to-text datasets from MultiMedBench (Tu et al., 2024a  ###reference_b121###; Tanno et al., 2024  ###reference_b117###) including Slake-VQA (Liu et al., 2021  ###reference_b71###), Path-VQA (He et al., 2020  ###reference_b46###), MIMIC-CXR (Johnson et al., 2019a  ###reference_b59###, b  ###reference_b60###), PAD-UFES-20 (Pacheco et al., 2020  ###reference_b93###), in addition to the Radiology Objects in COntext (ROCO) dataset (Pelka et al., 2018  ###reference_b95###). Slake-VQA and Path-VQA include both open-ended and close-ended visual question answering tasks in radiology and pathology, respectively. ROCO contains radiology image captioning tasks spanning multiple imaging modalities including computed tomography (CT), ultrasound, X-ray [chest X-ray (CXR), fluoroscopy, mammography, angiography], positron emission tomography (PET) and magnetic resonance imaging (MRI). PAD-UFES-20 is a domain specific dataset with diagnostic labels and patient clinical information designed for dermatology image classification. MIMIC-CXR is a radiology dataset comprised of CXRs, their corresponding text reports, and a set of discrete labels that denote the presence of 13 abnormal radiological conditions derived using the CheXpert labeler (Irvin et al., 2019  ###reference_b52###) (e.g., pneumonia). We use this dataset to formulate CXR report generation and image classification tasks for fine-tuning.\nFor each task, we fine-tune Gemini 1.5 Pro by providing task-specific instructions as shown in Figure D1  ###reference_###. The mixture ratio for each task is approximately proportional to the number of training samples in each dataset. The resulting model is Med-Gemini-M 1.5.\nWe anticipate that integrating various health-related signals will significantly enhance medical models and treatment decisions. These signals include data from consumer wearables (e.g., long-term heart rate measurements, activity levels), genomic information, nutritional data (e.g., images of meals), and environmental factors (e.g., air quality measurements). As a proof-of-concept, we expand Med-Gemini\u2019s capability to process raw biomedical signals. Specifically, we develop Med-Gemini-S 1.0 by augmenting Gemini 1.0 Nano with a specialized encoder using a cross-attention mechanism based on Flamingo (Alayrac et al., 2022  ###reference_b3###) to answer questions directly taking a raw 12-channel electrocardiogram (ECG) waveform as input. We use a subset of labeled ECG examples from the ECG-QA dataset (Oh et al., 2023  ###reference_b90###) and formulate the task as close-ended question answering with the instruction shown in Figure D1  ###reference_###.\nSearching and retrieving clinically-relevant information from long EHR notes and records is a common and important task in patient care but must be performed with high precision and recall to enhance clinician efficiency and reduce workload (Jensen et al., 2012  ###reference_b55###; Ford et al., 2016  ###reference_b31###). Clinicians frequently curate a summary of their patient\u2019s historical conditions, symptoms, or procedures (the \u201cproblem list\u201d), which can be time-consuming and challenging for individuals with lengthy medical records. Difficulty arises with multiple factors hindering effective information retrieval in EHRs.\nFirstly, classic query expansion and matching mechanisms encounter limitations due to textual similarities between conditions with similar taxonomies and the diverse information models used in EHRs (e.g. \u201cMiller\u201d vs. \u201cMiller Fisher syndrome\u201d, \u201cDiabetic nephropathy\u201d vs. \u201cDiabetes mellitus\u201d). Vocabulary inconsistency in and between EHR systems presents issues including variations in how medical terms are encoded, such as acronyms (\u201crx\u201d vs. \u201cprescription\u201d), misspellings, or synonyms for the same condition. Secondly, EHRs often contain heterogeneous data structure such as a checklist-style data template: \u201c[ ] cough [x] headache\u201d, where a mention does not always indicate the presence of a medical condition. Thirdly, the context of a mention influences its interpretation. For example, the mention of the same condition in a patient\u2019s \u201cFamily History\u201d compared to their \u201cPast Medical History\u201d could have different interpretations and implications for the patient\u2019s care. Lastly, polysemous acronyms in medical notes can lead to misinterpretations.\nThese challenges motivate the need for AI systems to address the task of context-aware retrieval of subtle or rare conditions, medications, or procedure mentions from long EHR records - a practical benchmark for evaluating the utility of Med-Gemini in medicine. We setup the long-context EHR understanding task based on our prior work (Feder et al., 2022  ###reference_b28###), where we curate a set of long and challenging EHR cases from MIMIC-III (Johnson et al., 2016  ###reference_b58###), and formulate a subtle medical problem (condition/symptom/procedure) search-retrieval task over a collection of EHR notes and records, mimicking a clinically-relevant \u201cneedle-in-a-haystack\u201d (Gemini Team, Google, 2024  ###reference_b36###) problem. Details of the dataset and task curation procedure are described in Section E.1  ###reference_### and Section 3.3  ###reference_###.\nTo assess the long-context retrieval and reasoning capability of Med-Gemini-M 1.5, we aggregate the EHR notes across multiple visits from a single patient in each example and utilize the long-context window of the model with a two-step chain-of-reasoning approach (using only in-context learning). In the first step, we prompt Med-Gemini-M 1.5 to retrieve all mentions (snippets of evidence) related to the given problem (condition/symptom/procedure) with a one-shot demonstration. In the second step, we further prompt Med-Gemini-M 1.5 to determine the presence of the given problem entities based on the mentions retrieved. Details of the instruction prompts are shown in Figure 8  ###reference_### and Section 3.3  ###reference_###.\nWe use our prior heuristic-based annotation-aggregation method (Feder et al., 2022  ###reference_b28###) as a baseline method for comparison with Med-Gemini-M 1.5. This heuristic-based method requires an extensive effort of manual feature engineering to determine the existence of a problem (condition/symptom/procedure) from a set of medical records. It is an ontology-dependent multiple-step process, which includes an annotation step that labels the problem in each EHR note, a rule-based selection step that selects mentions of problem entities with high confidence, and another rule-based aggregation step that aggregates all selected problem mentions to reach a final conclusion. Note that the manually crafted aggregation rules can only provide a limited coverage of all possible conditions, and therefore it requires additional engineering effort to expand coverage to new conditions.\nTo curate a \u201cneedle-in-a-haystack\u201d evaluation benchmark, we select medical conditions from a collection of EHR records with only one evidence snippet found in the aggregation step. We note that a mention of a condition in the EHR does not always mean the patient has that condition. This task enables us to assess Med-Gemini-M 1.5\u2019s ability to identify rarely documented and subtle conditions, symptoms, and procedures and reason accurately and holistically regarding their existence.\nThe understanding of surgical and procedural videos is a highly active research topic in medical AI. The advancing frontier of computer vision in semantic segmentation, object detection and tracking, and action classification has enabled new clinical applications such as surgical phase recognition, tool detection and tracking, and even surgical skill assessment (Goodman et al., 2024  ###reference_b40###).\nLimited model context windows have hindered the ability for vision-language models to capture long-range dependencies and complex relationships within videos. Gemini\u2019s long-context capability offers a potential breakthrough for medical video understanding. By processing a whole video input, Med-Gemini-M 1.5 is able to identify visual patterns and understand actions and relationships between events across extended time frames.\nTo enable Med-Gemini-M 1.5 to understand medical videos, we employ zero-shot prompting with task-specific instructions as shown in Figure 10  ###reference_###, Figure 9  ###reference_###, and Figure 11  ###reference_###. The goal is to enable the model to analyze the language query and video content, and perform the given task related to the input medical video\u2014either localizing the relevant visual segment matching the query for the medical visual answer localization (MVAL) task (Gupta et al., 2023  ###reference_b44###), or identifying the surgical view in the video frames for the Critical View of Safety (CVS) assessment task (Strasberg and Brunt, 2010  ###reference_b115###; R\u00edos et al., 2023  ###reference_b105###).\nMore details on the medical video datasets and evaluation metrics are described in Section E.1  ###reference_### and Section 3.3  ###reference_###."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Long-context processing via instruction prompting and chain-of-reasoning",
            "text": "Many applications in medicine require the analysis of a large amount of information and the expertise to identify subtle details of the domain. As introduced before, Gemini models have breakthrough long-context capabilities. We assess medically-relevant long-context performance for Med-Gemini-M 1.5 by meaningfully processing large amounts of fine-grained information for two different medical applications: a \u201cneedle-in-a-haystack\u201d retrieval task from lengthy EHR notes and records; and tasks requiring understanding of medical videos. We describe various prompting strategies and chain-of-reasoning to enable accurate recall and reasoning of information. Searching and retrieving clinically-relevant information from long EHR notes and records is a common and important task in patient care but must be performed with high precision and recall to enhance clinician efficiency and reduce workload (Jensen et al., 2012; Ford et al., 2016). Clinicians frequently curate a summary of their patient\u2019s historical conditions, symptoms, or procedures (the \u201cproblem list\u201d), which can be time-consuming and challenging for individuals with lengthy medical records. Difficulty arises with multiple factors hindering effective information retrieval in EHRs. Firstly, classic query expansion and matching mechanisms encounter limitations due to textual similarities between conditions with similar taxonomies and the diverse information models used in EHRs (e.g. \u201cMiller\u201d vs. \u201cMiller Fisher syndrome\u201d, \u201cDiabetic nephropathy\u201d vs. \u201cDiabetes mellitus\u201d). Vocabulary inconsistency in and between EHR systems presents issues including variations in how medical terms are encoded, such as acronyms (\u201crx\u201d vs. \u201cprescription\u201d), misspellings, or synonyms for the same condition. Secondly, EHRs often contain heterogeneous data structure such as a checklist-style data template: \u201c[ ] cough [x] headache\u201d, where a mention does not always indicate the presence of a medical condition. Thirdly, the context of a mention influences its interpretation. For example, the mention of the same condition in a patient\u2019s \u201cFamily History\u201d compared to their \u201cPast Medical History\u201d could have different interpretations and implications for the patient\u2019s care. Lastly, polysemous acronyms in medical notes can lead to misinterpretations. These challenges motivate the need for AI systems to address the task of context-aware retrieval of subtle or rare conditions, medications, or procedure mentions from long EHR records - a practical benchmark for evaluating the utility of Med-Gemini in medicine. We setup the long-context EHR understanding task based on our prior work (Feder et al., 2022), where we curate a set of long and challenging EHR cases from MIMIC-III (Johnson et al., 2016), and formulate a subtle medical problem (condition/symptom/procedure) search-retrieval task over a collection of EHR notes and records, mimicking a clinically-relevant \u201cneedle-in-a-haystack\u201d (Gemini Team, Google, 2024) problem. Details of the dataset and task curation procedure are described in Section E.1 and Section 3.3. The understanding of surgical and procedural videos is a highly active research topic in medical AI. The advancing frontier of computer vision in semantic segmentation, object detection and tracking, and action classification has enabled new clinical applications such as surgical phase recognition, tool detection and tracking, and even surgical skill assessment (Goodman et al., 2024). Limited model context windows have hindered the ability for vision-language models to capture long-range dependencies and complex relationships within videos. Gemini\u2019s long-context capability offers a potential breakthrough for medical video understanding. By processing a whole video input, Med-Gemini-M 1.5 is able to identify visual patterns and understand actions and relationships between events across extended time frames. To enable Med-Gemini-M 1.5 to understand medical videos, we employ zero-shot prompting with task-specific instructions as shown in Figure 10, Figure 9, and Figure 11. The goal is to enable the model to analyze the language query and video content, and perform the given task related to the input medical video\u2014either localizing the relevant visual segment matching the query for the medical visual answer localization (MVAL) task (Gupta et al., 2023), or identifying the surgical view in the video frames for the Critical View of Safety (CVS) assessment task (Strasberg and Brunt, 2010; R\u00edos et al., 2023). More details on the medical video datasets and evaluation metrics are described in Section E.1 and Section 3.3."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Evaluation",
            "text": "We present evaluation benchmarks spanning (1) text-based reasoning, (2) multimodal, and (3) long-context processing tasks, demonstrating Med-Gemini\u2019s performance across a wide range of capabilities in medicine.\nFor the long-context EHR understanding task, we curate a MIMIC-III-Needle-in-a-Haystack task where the goal is to retrieve the relevant text spans of any mention of a given medical problem (condition/symptom/procedure) over a large collection of clinical notes in EHR and determine the existence of the condition by reasoning across the retrieved evidence.\nSpecifically, we curate 200 examples where each example consists of a collection of de-identified EHR notes selected from 44 unique ICU patients with a long medical history based on the following criteria:\nPatients with long records: more than 100 medical notes (excluding structured EHR data). The length of each example ranges from 200,000 to 700,000 words.\nIn each example, the condition is mentioned only once across the collection of all EHR notes.\nEach sample has a single condition of interest.\nThe ground-truth label of each sample is a binary variable indicating whether a given problem entity of interest is present or not, obtained from the majority vote of three physician raters. Across the 200 test examples, the number of positive cases and negative cases are 121 and 79, respectively.\nWe compare Med-Gemini-M 1.5\u2019s one-shot in-context learning performance against the heuristic-based annotation-aggregation baseline method (Feder et al., 2022  ###reference_b28###) in terms of precision and recall.\nWe quantitatively evaluate Med-Gemini-M 1.5\u2019s long-context performance in the setting of video question-answering using three medical video tasks: two medical visual answer localization (MVAL) tasks using the Medical Instructional Video QA (MedVidQA) dataset (Gupta et al., 2023  ###reference_b44###), and the critical view of safety (CVS) assessment task on the Cholec80-CVS dataset (Twinanda et al., 2016  ###reference_b123###; R\u00edos et al., 2023  ###reference_b105###).\nThe goal of MVAL is to identify specific video segments based on natural language descriptions (queries) given a video input. For MVAL, we benchmark the test set of MedVidQA for two video span prediction tasks, one using both the video input and subtitle text and the other one with only the video inputs. We follow Li et al. (2022  ###reference_b68###); Gupta et al. (2023  ###reference_b44###) using Intersection over Union (IoU) at the threshold of 0.3, 0.5, 0.7, and mean IoU (mIoU) as the evaluation metrics for the video span prediction tasks. IoU and mIoU are used to measure how much of the ground truth span overlaps with the predicted span.\nWe evaluate Med-Gemini-M 1.5\u2019s long-context capabilities in assessing the achievement of the Critical View of Safety (CVS) method in laparoscopic cholecystectomy (a keyhole operation to remove the gallbladder) videos. The CVS (Strasberg and Brunt, 2010  ###reference_b115###) is a recommended protocol used for secure identification of the cystic duct and cystic artery to minimize the risk of Bile Duct Injury (BDI), a significant injury associated with consequential postoperative morbidity and mortality, reduced long-term survival and impact on quality of life (Way et al., 2003  ###reference_b133###). We evaluate the CVS assessment task on the public Cholec80 dataset (Twinanda et al., 2016  ###reference_b123###) and Cholec80-CVS (R\u00edos et al., 2023  ###reference_b105###) video clip annotations. Specifically, for each surgical video in the Cholec80 dataset, the Cholec80-CVS dataset provides annotations for video clips within the full video, where at least one CVS criteria is met. Each of those video clips is annotated with a score of 0, 1 or 2 for each of the three CVS criteria. All frames contained in a given video clip are considered to share the same annotation.\nWe evaluate the model\u2019s ability to predict which of the CVS criteria are met based on the whole video clip. We then compute the average accuracy of the answer against the Cholec80-CVS annotations across 572 annotated video clips. More details on the CVS task can be found in Appendix E.1  ###reference_###.\nFurthermore, to show the real-world capability of Med-Gemini-M 1.5 in capturing surgical actions in procedural videos, we qualitatively evaluate the surgical action recognition task using examples from the Annotated Videos of Open Surgery (AVOS) dataset (Goodman et al., 2021  ###reference_b39###), a video collection of open surgical procedures uploaded to the YouTube platform."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluation of advanced reasoning on text-based tasks",
            "text": "We evaluate the medical reasoning capability of Med-Gemini-L 1.0 on three text benchmarks assessing clinical reasoning and the ability to retrieve information using web search to reduce uncertainty:\nMedQA (USMLE): a close-ended multiple-choice (4 options) dataset with 1273 USMLE style test questions curated by Jin et al. (2021  ###reference_b56###).\nNEJM clinico-pathological conferences (NEJM CPC): a dataset comprising complex diagnostic case challenges in the medical journal, New England Journal of Medicine (NEJM) curated by McDuff et al. (2023  ###reference_b80###).\nGeneTuring: a dataset that includes 600 open/close-ended QA pairs to evaluate genomic knowledge of LLMs (Hou and Ji, 2023  ###reference_b48###).\nFor MedQA, we follow the input-output format, and the evaluation method as described in Singhal et al. (2023a  ###reference_b111###) using prediction accuracy as the metric.\nAt inference, we go through four iterations of uncertainty-guided search.\nAdditionally, we ask board-certified primary care physicians (PCPs) from the US to relabel the MedQA test set. This enables us to identify questions with missing information such as plots or figures, labeling errors, and other potentially ambiguous questions with multiple possible correct answers (Stutz et al., 2023  ###reference_b116###). Overall, this allows us to better characterize our performance on MedQA (USMLE). More details on this rating task can be found in Appendix C.2  ###reference_###.\nNEJM CPC evaluation is an open-ended diagnosis task. The input is a text-based, challenging clinico-pathological case (CPC) report, and the output is a differential diagnosis list, comprising 10 potential diagnoses. We use the top-1 and top-10 accuracy of identifying the correct diagnosis of the given challenging case, and use the same prompting procedures following McDuff et al. (2023  ###reference_b80###).\nAt inference, we go through one iteration of uncertainty-guided search.\nGeneTuring consists of 12 modules, each containing 50 open or close-ended QA pairs. We use the prediction accuracy as the evaluation metric, where the evaluation method and scoring technique for each module follow the methods described in Hou and Ji (2023  ###reference_b48###).\nIn particular, we exclude from numerical evaluation, cases where the model outputs either do not directly answer or acknowledge limitations (i.e., abstained). At inference, we again go through only one iteration of uncertainty-guided search similar to NEJM CPC evaluation.\nBeyond these benchmarks, we further evaluate Med-Gemini-M 1.0 on three challenging use cases that require long-form text generation. To this end, we conduct an expert evaluation where a panel of clinicians compare the responses of our model to those of other human experts via a side-by-side blinded preference comparison (more details are provided in Appendix C.4  ###reference_###):\nMedical summarization: Generate an after-visit summary (AVS) given de-identified history and physical (H&P) notes. An AVS is a structured report that patients receive at the end of a medical appointment to summarize and guide their care journeys.\nReferral letter generation: Generate a referral letter to another healthcare provider given a de-identified outpatient medical note that contains a recommendation for a referral.\nMedical simplification: Generate a plain language summary (PLS) given a technical abstract from a medical systematic review. A PLS should be written in plain English which can be understood by most readers without a university education (Cochrane, 2014  ###reference_b19###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation of multimodal capabilities",
            "text": "We evaluate Med-Gemini on seven multimodal visual question answering (VQA) benchmarks. For in-distribution evaluation, we choose four medical specialty datasets used in the instruction fine-tuning of Med-Gemini: PAD-UFES-20 (dermatology), Slake-VQA (radiology in English and Chinese) and Path-VQA (pathology) for Med-Gemini M 1.5, and ECG-QA (cardiology) for Med-Gemini S 1.0. We also include three cross-specialty benchmarks for measuring out-of-box performance of Med-Gemini: NEJM Image challenge, USMLE-MM (multimodal), and MMMU-HM (health and medicine) datasets. These datasets are not used in any training or fine-tuning process. For this, we focus our evaluation on the Med-Gemini-L 1.0 model without any multimodal finetuning. Its worth noting that PAD-UFES-20, NEJM Image Challenge, USMLE-MM datasets, and most questions in MMMU-HM are close-ended VQA, i.e., multiple-choice question in a VQA setup. An overview of the selected datasets is presented in Table D2 and more details are in Appendix D.1 and D.2. We report prediction accuracy for all the close-ended multiple-choice VQA tasks, including NEJM Image Challenge, USMLE-MM, and PAD-UFES-20 6-class skin condition classification. We also follow the evaluation setup in Yue et al. (2023) to report accuracy for MMMU-HM. We use the exact-match accuracy for ECG-QA following Oh et al. (2023). For the open-ended VQA tasks (Slake-VQA and Path-VQA), we use the token-level F1 score following Tu et al. (2024a). We further showcase Med-Gemini-M 1.5\u2019s multimodal capability in multimodal medical diagnostic dialogue in two specialities - dermatology and radiology (Tu et al., 2024b) - with qualitative evaluation of the example dialogues by attending expert clinicians in these specialties. We note that these demonstrations indicate the \"art of the possible\", but that extensive further research and validation would be required before the consideration of deployment for a safety-critical use-case such as diagnostic assistance to a clinician."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Evaluation of long-context capabilities on video and EHR tasks",
            "text": "We consider three tasks to demonstrate Med-Gemini-M 1.5\u2019s ability to seamlessly understand and reason over long context medical information (Table E1, details in Appendix E.1):\nLong unstructured EHR notes understanding\nMedical instructional video QA\nCritical view of safety (CVS) assessment of surgical video\nFor the long-context EHR understanding task, we curate a MIMIC-III-Needle-in-a-Haystack task where the goal is to retrieve the relevant text spans of any mention of a given medical problem (condition/symptom/procedure) over a large collection of clinical notes in EHR and determine the existence of the condition by reasoning across the retrieved evidence.\nSpecifically, we curate 200 examples where each example consists of a collection of de-identified EHR notes selected from 44 unique ICU patients with a long medical history based on the following criteria:\nPatients with long records: more than 100 medical notes (excluding structured EHR data). The length of each example ranges from 200,000 to 700,000 words.\nIn each example, the condition is mentioned only once across the collection of all EHR notes.\nEach sample has a single condition of interest.\nThe ground-truth label of each sample is a binary variable indicating whether a given problem entity of interest is present or not, obtained from the majority vote of three physician raters. Across the 200 test examples, the number of positive cases and negative cases are 121 and 79, respectively.\nWe compare Med-Gemini-M 1.5\u2019s one-shot in-context learning performance against the heuristic-based annotation-aggregation baseline method (Feder et al., 2022) in terms of precision and recall.\nWe quantitatively evaluate Med-Gemini-M 1.5\u2019s long-context performance in the setting of video question-answering using three medical video tasks: two medical visual answer localization (MVAL) tasks using the Medical Instructional Video QA (MedVidQA) dataset (Gupta et al., 2023), and the critical view of safety (CVS) assessment task on the Cholec80-CVS dataset (Twinanda et al., 2016; R\u00edos et al., 2023).\nThe goal of MVAL is to identify specific video segments based on natural language descriptions (queries) given a video input. For MVAL, we benchmark the test set of MedVidQA for two video span prediction tasks, one using both the video input and subtitle text and the other one with only the video inputs. We follow Li et al. (2022); Gupta et al. (2023) using Intersection over Union (IoU) at the threshold of 0.3, 0.5, 0.7, and mean IoU (mIoU) as the evaluation metrics for the video span prediction tasks. IoU and mIoU are used to measure how much of the ground truth span overlaps with the predicted span.\nWe evaluate Med-Gemini-M 1.5\u2019s long-context capabilities in assessing the achievement of the Critical View of Safety (CVS) method in laparoscopic cholecystectomy (a keyhole operation to remove the gallbladder) videos. The CVS (Strasberg and Brunt, 2010) is a recommended protocol used for secure identification of the cystic duct and cystic artery to minimize the risk of Bile Duct Injury (BDI), a significant injury associated with consequential postoperative morbidity and mortality, reduced long-term survival and impact on quality of life (Way et al., 2003). We evaluate the CVS assessment task on the public Cholec80 dataset (Twinanda et al., 2016) and Cholec80-CVS (R\u00edos et al., 2023) video clip annotations. Specifically, for each surgical video in the Cholec80 dataset, the Cholec80-CVS dataset provides annotations for video clips within the full video, where at least one CVS criteria is met. Each of those video clips is annotated with a score of 0, 1 or 2 for each of the three CVS criteria. All frames contained in a given video clip are considered to share the same annotation.\nWe evaluate the model\u2019s ability to predict which of the CVS criteria are met based on the whole video clip. We then compute the average accuracy of the answer against the Cholec80-CVS annotations across 572 annotated video clips. More details on the CVS task can be found in Appendix E.1.\nFurthermore, to show the real-world capability of Med-Gemini-M 1.5 in capturing surgical actions in procedural videos, we qualitatively evaluate the surgical action recognition task using examples from the Annotated Videos of Open Surgery (AVOS) dataset (Goodman et al., 2021), a video collection of open surgical procedures uploaded to the YouTube platform."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "As introduced previously, we evaluate Med-Gemini\u2019s advanced reasoning, multimodal, and long-context capabilities across a wide range of medical benchmarks, both quantitatively and qualitatively. The array and diversity of tasks considered in this work is to the best of our knowledge, the most comprehensive for medical LLMs.\nFurther, our evaluations of Med-Gemini go beyond benchmarking of model capabilities and extend to tasks reflecting the potential for real-world utility, such as medical summarization, multimodal conversations, and surgical video understanding.\nTo understand the impact of self-training and uncertainty-guided search on performance, we compare Med-Gemini-L 1.0\u2019s performance with and without self-training, along with varying number of rounds of uncertainty-guided search for MedQA (USMLE). As shown in Figure 4  ###reference_###a, Med-Gemini-L 1.0\u2019s performance improves considerably with self-training (a gain of  in accuracy), and improves with each round of search from  up to .\nSimilarly, for the NEJM CPC benchmark, Figure 3  ###reference_###a shows a  improvement for top-10 accuracy when we add search at inference.\nIn Section C.3  ###reference_###, we additionally show performance on NEJM CPC stratified by four specialities.\nMedQA (USMLE) is a popular benchmark for assessing the capabilities of LLMs in the medical domain.\nHowever, some MedQA test questions have missing information such as figures or lab results, and potentially outdated ground-truth answers.\nTo address these concerns, we conduct a complete relabeling of the MedQA (USMLE) test set. Specifically, we recruit at least three US physicians to re-annotate each question, asking them to answer the question and evaluate the provided ground-truth answer. We also ask them to identify if there was any missing information in the questions. Following Stutz et al. (2023  ###reference_b116###), we characterize the questions to exclude due to missing information or label errors by bootstrapping votes from committees of three raters per question. We additionally identify ambiguous questions as those allowing multiple correct answers (more details can be found in Appendix C.2  ###reference_###).\nFigure 4  ###reference_###b shows that, on average across bootstrapped committees,  of questions include missing information, following the unanimous vote of bootstrapped committees. Additionally,  likely include label errors. Another  are ambiguous. Excluding these questions is supported by high inter-rater agreement of , , and , respectively. Importantly, Med-Gemini-L 1.0\u2019s mistakes can be attributed disproportionately to these questions; our entropy-based uncertainty score also tends to be higher on these question (t-test, -value=0.033). Filtering both types improves accuracy from  to   . Using majority instead of unanimous votes further improves accuracy to    by discarding up to  of the uncertain questions.\n###figure_3### In Figure 11  ###reference_###, we qualitatively preview Med-Gemini-M 1.5\u2019s ability to identify surgical actions from a video in the AVOS dataset. This ability holds potential for surgical care, promising to enhance surgical training through automated assessment, optimize operating room efficiency by analyzing workflows, and potentially guide surgeons in real-time during complex procedures for improved accuracy and patient outcomes.\nIn Figure 12  ###reference_###, we additionally present an example of Med-Gemini-M 1.5\u2019s long-context capabilities on surgical video dialogue where the model analyzes a video clip comprising footage from a laparoscopic cholecystectomy. Med-Gemini-M 1.5 demonstrates its ability to analyze the video and conduct a dialogue with a student that might be learning about the procedure. These promising abilities have the potential to provide useful assistive tools for clinicians, perhaps improving patient safety or enhancing the process of medical training through educational aids or automated in-procedure assistance and guidance. The model correctly informs the user that they are observing a laparoscopic cholecystectomy and refers correctly to the key structures underlying the \u201ccritical view of safety\u201d. These classification tasks, if performed scalably with high accuracy, could enable better audit of procedures (for example for quality assurance), or even prospective efficiency gains from anticipation of operative stages. For more ambitious goals such as benefits to education, operative guidance or patient safety, significant further work would need to be performed to assess more nuanced and complex capabilities. For example, we did not test Med-Gemini\u2019s ability to accurately segment or highlight physical structures in the video and ground the dialogue with the relevant anatomy; or retrieve and present useful educational assets like diagrammatic representations of the displayed anatomy or guides to key operative stages. For uses such as education, pedagogical dialogue objectives would also likely be of considerable importance. Further work should explore these and other exciting new capabilities in a wider range of settings for procedural video, which is increasingly common in medicine.\nIn Figure 13  ###reference_###, we demonstrate that Med-Gemini-M 1.5 effectively parses extensive medical records, synthesizing them into clear, concise summaries of active and historical conditions. Moreover, users can initiate conversations based on this summarized data, requesting more granular details from the records. Our example shows how this might include a user making natural language inquiries about specific conditions (like pneumonia) or associated diagnostic findings (such as CXR results). By streamlining access to long-form medical data and presenting the interaction in a conversational interface, this capability has the potential to significantly reduce cognitive load for clinicians and patients alike, potentially enhancing the efficiency and understanding of complex medical information without compromising staff well-being. To deliver upon this potential in real-world use would require considerable additional evaluation and research. As just one example, it would be necessary to closely examine the incidence of clinically-significant errors in retrieval or generation from grounded content; and to proactively measure and mitigate issues in dataset and model bias (as we discuss further below).\nIn Figure 14  ###reference_###, we demonstrate Med-Gemini-M 1.5\u2019s ability to process multiple research articles concerning a specific genetic locus (FTO) and its association with obesity (Loos and Yeo, 2022  ###reference_b74###). In this real-world application, Med-Gemini-M 1.5 successfully comprehends the information presented in current research (full content of 12 pre-curated research papers in portable document format) and compiles a concise summary for the user. The FTO locus we demonstrate in this example (a region of BMI- and obesity-associated variants within the gene FTO) is a classic example of a mechanistically understood genome-wide association studies (GWAS) hit. In this exemplar, the mechanism is a relatively complex multistep process which took extensive research to pinpoint\u2014it involves variants altering the binding of a transcriptional repressor within an intronic super-enhancer region of the FTO gene, thereby leading to overexpression of two other genes, which ultimately promotes lipid accumulation (Claussnitzer et al., 2015  ###reference_b18###; Laber et al., 2021  ###reference_b65###).\nWe evaluate Med-Gemini-M 1.5\u2019s ability to parse a large collection of academic papers on the FTO locus and provide a succinct and accessible description of the mechanistic link between FTO and obesity, together with a list of concrete supporting experimental results. As seen in Figure 14  ###reference_###, the model provides a concise, informative, and accurate description of how the FTO locus contributes to obesity biology and presents it in a clear and digestible manner. Improvement can be made by the model listing other well-studied variants in high linkage equilibrium with rs1421085, and by providing references of where each piece of information originated from. This example shows how Med-Gemini-M 1.5\u2019s long-context capability has clear potential to reduce cognitive load for genomic researchers and clinicians, enhancing their access to the latest findings regarding gene-disease associations; and the potential has broad relevance in other domains of biomedical and scientific research.\n###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Med-Gemini demonstrates advanced reasoning on text-based tasks",
            "text": "As shown in Table 1  ###reference_###, Med-Gemini-L 1.0 scores  accuracy on MedQA (USMLE), a new SoTA, outperforming our previous Med-PaLM 2, by , and the recent results augmenting GPT-4 with complex, specialized prompting - MedPrompt (Nori et al., 2023  ###reference_b86###) by . In contrast to MedPrompt, our principled approach leverages general web search in an uncertainty-guided framework that can be easily to extended to more complex scenarios beyond MedQA.\nAs proof of generalization of our search integration, on the NEJM CPC complex diagnostic challenges benchmark, Med-Gemini-L 1.0 surpasses our previous SoTA AMIE model (which itself is better than GPT-4) (McDuff et al., 2023  ###reference_b80###) by  on the top-10 accuracy as shown in Figure 3  ###reference_###a.\nThe same search strategy is also effective for genomics knoweledge tasks as shown in Table 1  ###reference_###. Med-Gemini-L 1.0 outperforms the SoTA models reported in Hou and Ji (2023  ###reference_b48###) on seven GeneTuring modules including Gene name extraction, Gene alias, Gene name conversion, Gene location, Protein-coding genes, Gene ontology and TF regulation.\nWe also compare model abstention across the 12 modules in Figure 3  ###reference_###b.\nIt is worth noting that GeneGPT (Jin et al., 2024  ###reference_b57###) achieves higher scores through specialized web APIs, while our comparison focuses on prior models from Hou and Ji (2023  ###reference_b48###) that utilize general web search similar to our model.\n###figure_10### To understand the impact of self-training and uncertainty-guided search on performance, we compare Med-Gemini-L 1.0\u2019s performance with and without self-training, along with varying number of rounds of uncertainty-guided search for MedQA (USMLE). As shown in Figure 4  ###reference_###  ###reference_###a, Med-Gemini-L 1.0\u2019s performance improves considerably with self-training (a gain of  in accuracy), and improves with each round of search from  up to .\nSimilarly, for the NEJM CPC benchmark, Figure 3  ###reference_###  ###reference_###a shows a  improvement for top-10 accuracy when we add search at inference.\nIn Section C.3  ###reference_###  ###reference_###, we additionally show performance on NEJM CPC stratified by four specialities.\nMedQA (USMLE) is a popular benchmark for assessing the capabilities of LLMs in the medical domain.\nHowever, some MedQA test questions have missing information such as figures or lab results, and potentially outdated ground-truth answers.\nTo address these concerns, we conduct a complete relabeling of the MedQA (USMLE) test set. Specifically, we recruit at least three US physicians to re-annotate each question, asking them to answer the question and evaluate the provided ground-truth answer. We also ask them to identify if there was any missing information in the questions. Following Stutz et al. (2023  ###reference_b116###  ###reference_b116###), we characterize the questions to exclude due to missing information or label errors by bootstrapping votes from committees of three raters per question. We additionally identify ambiguous questions as those allowing multiple correct answers (more details can be found in Appendix C.2  ###reference_###  ###reference_###).\nFigure 4  ###reference_###  ###reference_###b shows that, on average across bootstrapped committees,  of questions include missing information, following the unanimous vote of bootstrapped committees. Additionally,  likely include label errors. Another  are ambiguous. Excluding these questions is supported by high inter-rater agreement of , , and , respectively. Importantly, Med-Gemini-L 1.0\u2019s mistakes can be attributed disproportionately to these questions; our entropy-based uncertainty score also tends to be higher on these question (t-test, -value=0.033). Filtering both types improves accuracy from  to   . Using majority instead of unanimous votes further improves accuracy to    by discarding up to  of the uncertain questions.\n###figure_11###"
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Performance on long-form medical text generation",
            "text": "Med-Gemini-M 1.0 demonstrates the ability to generate long-form text for three challenging real-world use cases - after-visit clinical summaries, doctor referral letter generation and medical simplification. In side-by-side comparisons, Med-Gemini-M 1.0\u2019s responses are considered as good or better than expert responses more than half the time by clinician raters across the three tasks (Figure 5  ###reference_###). For more task details, see Appendix C.4  ###reference_###. Notably for the referral letter generation task, the model generated letters are preferred or tied with experts across all the samples evaluated.\n###figure_12###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Med-Gemini demonstrates multimodal understanding across diverse tasks",
            "text": "Our Med-Gemini models surpass, or perform competitively, with the state-of-the-art methods across seven medical multimodal benchmarks (See Table 2  ###reference_###). We provide representative input and output examples for the multimodal tasks in Figure D1  ###reference_### for illustration.\nIn particular, Med-Gemini-L 1.0 reaches SoTA on three out-of-distribution close-ended VQA tasks\u2014NEJM Image Challenge, multimodal USMLE sample questions (USMLE-MM), and the health & medicine subset of MMMU (MMMU-HM), outperforming GPT-4V by , , and , respectively. Meanwhile, Med-Gemini-M 1.5 outperforms our previous multimodal models, Med-PaLM M (Tu et al., 2024a  ###reference_b121###) on Path-VQA by  in token F1 score, and Med-Gemini-S 1.0 outperforms the previous SoTA for ECG-QA (GPT-4 with SE-WRN) by  on macro-averaged accuracy across ECG question types (Oh et al., 2023  ###reference_b90###). Med-Gemini-M 1.5 also performs competitively on Slake-VQA and PAD-UFES-20 compared to the previous SoTA method (Med-PaLM M) but does not reach SoTA.\nNote that we have evaluated PAD-UFES-20 on two different data split setups. We first evaluate on the Med-PaLM M split (the image-level split) for a direct, fair comparison against the previous SoTA method. In addition, we also report our model\u2019s performance on a new split, which is a split at the patient level (Table 2  ###reference_###).\nFor USMLE-MM, our model achieves accuracies of , ,  for USMLE step 1 questions (n=19), step 2 (n=14), and step 3 (n=13), respectively.\nIn aggregate across these seven benchmarks, Med-Gemini improve over GPT-4V by an average relative margin of 44.5%. Note that for the USMLE-MM, PADS-UFES-20 and Slake-VQA datasets, we report reproduced GPT-4V results using public APIs and the same prompt used for the corresponding Med-Gemini model."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Preview of multimodal dialogue capabilities",
            "text": "To extend beyond multimodal benchmarks, we demonstrate the potential for future real-world utility of Med-Gemini through hypothetical multimodal medical dialogues across two specialities.\nFigure 6  ###reference_### illustrates an out-of-distribution setting where the dermatology image comes from a dataset (Ward et al., 2024  ###reference_b132###) not used in the multimodal fine-tuning mixture. The user first asks Med-Gemini-M 1.5 about itchy lumps on their legs and arms; our model then asks the user to share an image of the lumps; after the user provides the image of their suspicious lesion, the model asks a follow-up question and continues to provide a correct diagnosis of prurigo nodularis, and recommends next steps and potential treatment options.\nIn Figure 7  ###reference_###, we show a radiology dialogue example with the example image coming from the test set of the MIMIC-CXR dataset. Med-Gemini-M 1.5 demonstrates the ability to interact with a primary care provider to analyze a CXR, identify degenerative disk disease, discuss the differences between causation and correlation with a patient history of back-pain, suggest follow-up investigations to establish the cause of back pain and provide a report using non-technical language to facilitate patient understanding and communication. We observe some variability in Med-Gemini-M 1.5\u2019s responses depending on the prompt (for instance, for some prompts the report will not list mild degenerative changes especially if prompted to focus on some other anatomical feature). A full quantification of Med-Gemini-M 1.5\u2019s multimodal dialogue capability and variability is beyond the scope of this work, but nonetheless these qualitative examples illustrate Med-Gemini-M 1.5\u2019s ability to support conversations about medical knowledge grounded on multimodal sources, a potentially useful attribute for applications considering user-AI and clinician-AI interaction. Real-world exploration of these use-cases would require considerable further development and validation to build upon these signs of early promise.\n###figure_13### ###figure_14###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Med-Gemini shows long-context processing capability on long EHR and video tasks",
            "text": "Finally, we evaluate the long-context capability of Med-Gemini-M 1.5 via the \u201cneedle-in-a-haystack\u201d medical condition retrieval task from long EHRs as well as three medical video tasks (two MAVL and one CVS assessment of surgical videos).\nWe demonstrate the utility of Med-Gemini-M 1.5 on the correct identification of rare and subtle problem entity (condition/symptom/procedure) in long EHR notes. The average precision and recall between Med-Gemini-M 1.5 and the baseline method are shown in Table 3  ###reference_### (confidence intervals in Table E2  ###reference_###). Encouragingly, we observe that Med-Gemini-M 1.5\u2019s one-shot ability is on-par with a carefully-tuned heuristic-based annotation-aggregation baseline approach, which is highly task-dependent. The in-context learning capability of Med-Gemini-M 1.5 to process long documents or records can easily generalize to novel problem settings without the need of extensive manual engineering. We provide an illustrative example of the prompt used, along with our model\u2019s response in Figure 8  ###reference_###. We attempt to benchmark GPT-4 on this task but the average context token length in this dataset significantly exceeds the maximum context window supported in the public APIs.\nMed-Gemini-M 1.5 also achieves SoTA performance on two MedVidQA MAVL tasks (one using both video and subtitles and the other being video only), outperforming the non-LLM based SoTA models which require considerable be-spoke tuning. We note that 7 questions in MedVidQA are not answerable due to YouTube video access (private, removed). Our results therefore are reported based on the remaining 148 questions. Details are shown in Table 3  ###reference_###.\nWe provide an illustrative example of the prompt used, along with our model\u2019s response in Figure 10  ###reference_###. While evaluating MedVidQA, we also observe that the embedded captions can significantly aid the model\u2019s understanding.\nFuture research could explore how to optimize the use of multimodal video data, including images, text, and audio, for further improvements in video understanding. We attempt to benchmark GPT-4V on these tasks but once again run into context length limitations for most of the videos using the public APIs.\nFor the CVS assessment of the laparoscopic cholecystectomy video task, Med-Gemini-M 1.5 outperforms GPT-4V by 21%. However, we observe that the supervised baseline using a ResNet3D architecture performs better.\nFurther investigations on prompting strategies or instruction fine-tuning may be required to improve the task performance of our models.\nWe provide an illustrative example of the prompt used, along with our model\u2019s response in Figure 9  ###reference_###.\n###figure_15### In Figure 11  ###reference_###  ###reference_###, we qualitatively preview Med-Gemini-M 1.5\u2019s ability to identify surgical actions from a video in the AVOS dataset. This ability holds potential for surgical care, promising to enhance surgical training through automated assessment, optimize operating room efficiency by analyzing workflows, and potentially guide surgeons in real-time during complex procedures for improved accuracy and patient outcomes.\nIn Figure 12  ###reference_###  ###reference_###, we additionally present an example of Med-Gemini-M 1.5\u2019s long-context capabilities on surgical video dialogue where the model analyzes a video clip comprising footage from a laparoscopic cholecystectomy. Med-Gemini-M 1.5 demonstrates its ability to analyze the video and conduct a dialogue with a student that might be learning about the procedure. These promising abilities have the potential to provide useful assistive tools for clinicians, perhaps improving patient safety or enhancing the process of medical training through educational aids or automated in-procedure assistance and guidance. The model correctly informs the user that they are observing a laparoscopic cholecystectomy and refers correctly to the key structures underlying the \u201ccritical view of safety\u201d. These classification tasks, if performed scalably with high accuracy, could enable better audit of procedures (for example for quality assurance), or even prospective efficiency gains from anticipation of operative stages. For more ambitious goals such as benefits to education, operative guidance or patient safety, significant further work would need to be performed to assess more nuanced and complex capabilities. For example, we did not test Med-Gemini\u2019s ability to accurately segment or highlight physical structures in the video and ground the dialogue with the relevant anatomy; or retrieve and present useful educational assets like diagrammatic representations of the displayed anatomy or guides to key operative stages. For uses such as education, pedagogical dialogue objectives would also likely be of considerable importance. Further work should explore these and other exciting new capabilities in a wider range of settings for procedural video, which is increasingly common in medicine.\nIn Figure 13  ###reference_###  ###reference_###, we demonstrate that Med-Gemini-M 1.5 effectively parses extensive medical records, synthesizing them into clear, concise summaries of active and historical conditions. Moreover, users can initiate conversations based on this summarized data, requesting more granular details from the records. Our example shows how this might include a user making natural language inquiries about specific conditions (like pneumonia) or associated diagnostic findings (such as CXR results). By streamlining access to long-form medical data and presenting the interaction in a conversational interface, this capability has the potential to significantly reduce cognitive load for clinicians and patients alike, potentially enhancing the efficiency and understanding of complex medical information without compromising staff well-being. To deliver upon this potential in real-world use would require considerable additional evaluation and research. As just one example, it would be necessary to closely examine the incidence of clinically-significant errors in retrieval or generation from grounded content; and to proactively measure and mitigate issues in dataset and model bias (as we discuss further below).\nIn Figure 14  ###reference_###  ###reference_###, we demonstrate Med-Gemini-M 1.5\u2019s ability to process multiple research articles concerning a specific genetic locus (FTO) and its association with obesity (Loos and Yeo, 2022  ###reference_b74###  ###reference_b74###). In this real-world application, Med-Gemini-M 1.5 successfully comprehends the information presented in current research (full content of 12 pre-curated research papers in portable document format) and compiles a concise summary for the user. The FTO locus we demonstrate in this example (a region of BMI- and obesity-associated variants within the gene FTO) is a classic example of a mechanistically understood genome-wide association studies (GWAS) hit. In this exemplar, the mechanism is a relatively complex multistep process which took extensive research to pinpoint\u2014it involves variants altering the binding of a transcriptional repressor within an intronic super-enhancer region of the FTO gene, thereby leading to overexpression of two other genes, which ultimately promotes lipid accumulation (Claussnitzer et al., 2015  ###reference_b18###  ###reference_b18###; Laber et al., 2021  ###reference_b65###  ###reference_b65###).\nWe evaluate Med-Gemini-M 1.5\u2019s ability to parse a large collection of academic papers on the FTO locus and provide a succinct and accessible description of the mechanistic link between FTO and obesity, together with a list of concrete supporting experimental results. As seen in Figure 14  ###reference_###  ###reference_###, the model provides a concise, informative, and accurate description of how the FTO locus contributes to obesity biology and presents it in a clear and digestible manner. Improvement can be made by the model listing other well-studied variants in high linkage equilibrium with rs1421085, and by providing references of where each piece of information originated from. This example shows how Med-Gemini-M 1.5\u2019s long-context capability has clear potential to reduce cognitive load for genomic researchers and clinicians, enhancing their access to the latest findings regarding gene-disease associations; and the potential has broad relevance in other domains of biomedical and scientific research.\n###figure_16### ###figure_17### ###figure_18### ###figure_19### ###figure_20### ###figure_21###"
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Applications of long-context capabilities in biomedicine",
            "text": "In addition to quantitative results, we further preview the potentials of the long-context capabilities in medical education, facilitating clinician interaction with EHR systems and biomedical literature review and summarization.\nIn Figure 11  ###reference_###  ###reference_###  ###reference_###, we qualitatively preview Med-Gemini-M 1.5\u2019s ability to identify surgical actions from a video in the AVOS dataset. This ability holds potential for surgical care, promising to enhance surgical training through automated assessment, optimize operating room efficiency by analyzing workflows, and potentially guide surgeons in real-time during complex procedures for improved accuracy and patient outcomes.\nIn Figure 12  ###reference_###  ###reference_###  ###reference_###, we additionally present an example of Med-Gemini-M 1.5\u2019s long-context capabilities on surgical video dialogue where the model analyzes a video clip comprising footage from a laparoscopic cholecystectomy. Med-Gemini-M 1.5 demonstrates its ability to analyze the video and conduct a dialogue with a student that might be learning about the procedure. These promising abilities have the potential to provide useful assistive tools for clinicians, perhaps improving patient safety or enhancing the process of medical training through educational aids or automated in-procedure assistance and guidance. The model correctly informs the user that they are observing a laparoscopic cholecystectomy and refers correctly to the key structures underlying the \u201ccritical view of safety\u201d. These classification tasks, if performed scalably with high accuracy, could enable better audit of procedures (for example for quality assurance), or even prospective efficiency gains from anticipation of operative stages. For more ambitious goals such as benefits to education, operative guidance or patient safety, significant further work would need to be performed to assess more nuanced and complex capabilities. For example, we did not test Med-Gemini\u2019s ability to accurately segment or highlight physical structures in the video and ground the dialogue with the relevant anatomy; or retrieve and present useful educational assets like diagrammatic representations of the displayed anatomy or guides to key operative stages. For uses such as education, pedagogical dialogue objectives would also likely be of considerable importance. Further work should explore these and other exciting new capabilities in a wider range of settings for procedural video, which is increasingly common in medicine.\nIn Figure 13  ###reference_###  ###reference_###  ###reference_###, we demonstrate that Med-Gemini-M 1.5 effectively parses extensive medical records, synthesizing them into clear, concise summaries of active and historical conditions. Moreover, users can initiate conversations based on this summarized data, requesting more granular details from the records. Our example shows how this might include a user making natural language inquiries about specific conditions (like pneumonia) or associated diagnostic findings (such as CXR results). By streamlining access to long-form medical data and presenting the interaction in a conversational interface, this capability has the potential to significantly reduce cognitive load for clinicians and patients alike, potentially enhancing the efficiency and understanding of complex medical information without compromising staff well-being. To deliver upon this potential in real-world use would require considerable additional evaluation and research. As just one example, it would be necessary to closely examine the incidence of clinically-significant errors in retrieval or generation from grounded content; and to proactively measure and mitigate issues in dataset and model bias (as we discuss further below).\nIn Figure 14  ###reference_###  ###reference_###  ###reference_###, we demonstrate Med-Gemini-M 1.5\u2019s ability to process multiple research articles concerning a specific genetic locus (FTO) and its association with obesity (Loos and Yeo, 2022  ###reference_b74###  ###reference_b74###  ###reference_b74###). In this real-world application, Med-Gemini-M 1.5 successfully comprehends the information presented in current research (full content of 12 pre-curated research papers in portable document format) and compiles a concise summary for the user. The FTO locus we demonstrate in this example (a region of BMI- and obesity-associated variants within the gene FTO) is a classic example of a mechanistically understood genome-wide association studies (GWAS) hit. In this exemplar, the mechanism is a relatively complex multistep process which took extensive research to pinpoint\u2014it involves variants altering the binding of a transcriptional repressor within an intronic super-enhancer region of the FTO gene, thereby leading to overexpression of two other genes, which ultimately promotes lipid accumulation (Claussnitzer et al., 2015  ###reference_b18###  ###reference_b18###  ###reference_b18###; Laber et al., 2021  ###reference_b65###  ###reference_b65###  ###reference_b65###).\nWe evaluate Med-Gemini-M 1.5\u2019s ability to parse a large collection of academic papers on the FTO locus and provide a succinct and accessible description of the mechanistic link between FTO and obesity, together with a list of concrete supporting experimental results. As seen in Figure 14  ###reference_###  ###reference_###  ###reference_###, the model provides a concise, informative, and accurate description of how the FTO locus contributes to obesity biology and presents it in a clear and digestible manner. Improvement can be made by the model listing other well-studied variants in high linkage equilibrium with rs1421085, and by providing references of where each piece of information originated from. This example shows how Med-Gemini-M 1.5\u2019s long-context capability has clear potential to reduce cognitive load for genomic researchers and clinicians, enhancing their access to the latest findings regarding gene-disease associations; and the potential has broad relevance in other domains of biomedical and scientific research.\n###figure_22### ###figure_23### ###figure_24### ###figure_25### ###figure_26### ###figure_27###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Med-Gemini, built upon the Gemini models, demonstrates significant advancements in clinical reasoning, multimodal understanding, and long-context processing within the medical domain. This is evidenced by its strong performance across a diverse range of 25 tasks spanning 14 medical benchmarks, encompassing medical knowledge, clinical reasoning, genomics, waveforms, medical imaging, health records and videos.\nNotably, Med-Gemini-L 1.0 achieves a new SoTA on MedQA (USMLE), a popular benchmark for medical question answering with the use of self-training based fine-tuning and search integration. Our thorough relabeling of the MedQA test set (performed by attending clinicians) reveals important insights. While MedQA (USMLE) is a useful benchmark for assessing medical knowledge and reasoning, it is essential to acknowledge its limitations. We discover that approximately 4% of the questions contain missing information, and an additional 3% potentially have labeling errors. Establishing definitive ground truth is frequently challenging in medicine, where inter-reader variability and ambiguity are common and medical knowledge is constantly evolving. Our observations suggest that further improvements in SoTA performance on the MedQA (USMLE) benchmark in isolation may not directly correlate to progress in the capabilities of medical LLMs for meaningful real-world tasks and as such it is important to perform more comprehensive benchmarking and evaluation representative of real-world clinical workflows (Fleming et al., 2023  ###reference_b30###). In general, most benchmarks have limitations around dataset size and quality. While we focus our analysis here on MedQA (USMLE), prior work has suggested similar issues with other popular benchmark datasets (Xu et al., 2023  ###reference_b139###). Retraining Med-Gemini-M 1.5 with a new split of the PAD-UFES-20 dermatology dataset leads to a drop of 7.1% as compared to our results in Table 2  ###reference_###. As such, careful attention needs to be given to the size and quality of datasets when interpreting and contextualizing model performance.\nMed-Gemini\u2019s integration with web search presents exciting possibilities to provide more factually accurate and reliable answers to medical queries with LLMs. In this work, we focus on training Med-Gemini-L 1.0 to issue web search queries when uncertain and integrate the results when producing responses. While the results on MedQA, NEJM CPC, and GeneTuring benchmarks are promising, significant further research is necessary. For example, we haven\u2019t considered restricting the search results to more authoritative medical sources (Zakka et al., 2024  ###reference_b144###), using multimodal search retrieval or performed analysis on accuracy and relevance of search results and the quality of the citations (Wu et al., 2024  ###reference_b138###).\nFurther, it remains to be seen if smaller LLMs can also be taught to make use of web search.\nWe leave these explorations to future work.\nThe multimodal conversational capabilities of Med-Gemini-M 1.5 are promising given they are attained without any specific medical dialogue fine-tuning. Such capabilities allow for seamless and natural interactions between people, clinicians, and AI systems. As showcased in our qualitative examples, Med-Gemini-M 1.5 has the capability to engage in multi-turn clinical dialogues, request additional information such as images when needed, explain their reasoning in a comprehensible manner, and even help provide information useful for clinical decisions while appropriately deferring the final decision to human experts. This capability has significant potential for helpful real-world applications, including assisting clinicians and patients, but of course also entails highly significant associated risks. While highlighting the potential for future research in this domain, we have not rigorously benchmarked capabilities for clinical conversation in this work as previously explored by others in dedicated research towards conversational diagnostic AI (Tu et al., 2024b  ###reference_b122###). In addition, in forthcoming work, we will also rigorously explore the capabilities of Gemini in clinically specific multimodal tasks such as radiology report generation.\nPerhaps the most notable aspect of Med-Gemini is the long-context processing capabilities because they open up new performance frontiers and novel, previously infeasible application possibilities for medical AI systems. In this work, we introduce a novel EHR task focused on identifying and verifying conditions, symptoms and procedures within very long electronic patient records. This \u201cneedle-in-a-haystack\u201d retrieval task reflects a real-world challenge faced by clinicians (Klerings et al., 2015  ###reference_b63###), and Med-Gemini-M 1.5\u2019s performance demonstrates its potential to significantly reduce cognitive load and augment clinicians\u2019 capabilities by efficiently extracting and analyzing crucial information from vast amounts of patient data. The medical video question answering and annotation performance suggests these capabilities can generalize to complex multimodal data. It is worth highlighting that the demonstration of long-context capabilities is in a few-shot fashion without any task-specific fine-tuning. Such capabilities open up the possibilities of fine grained analysis and annotation of genomic and multi-omic sequence data, complex imaging modalities such as pathology or volumetric images and integrative processing with health records to uncover novel insights and assist in clinical workflows.\nGemini models are inherently multimodal and have strong medical knowledge as a result of large-scale multimodal pretraining. This is reflected in impressive out-of-the-box performance on multimodal benchmarks such as NEJM Image Challenge surpassing similar generalist vision-language models such as GPT-4V by a large margin (Buckley et al., 2023  ###reference_b10###). At the same time, medical knowledge and data (particularly multimodal data) is unique and complex and unlikely to be seen on the public internet commonly used to train LLMs. Gemini is a strong intelligence substrate but further fine-tuning, specialization and alignment of even such powerful models are necessary before use in the medical domain. At the same time, given the general capabilities of Gemini, the amount of data needed for such specialization and alignment is much lower than prior generation of medical AI systems (Azizi et al., 2023  ###reference_b6###) and it is indeed possible to efficiently adapt such models even to previously unseen but important medical modalities such as ECGs with relative efficiency as demonstrated here.\nTo the best of our knowledge, this work is the most comprehensive evaluation of medical LLMs and LMMs. The work includes evidence of new capabilities for medical AI and tasks that suggest real-world utility. This is particularly reinforced by strong performance of our models in evaluations of medical summarization and referral note generation. Diagnostic tasks draw considerable attention in research, but carry significant regulatory, clinical and equity-related risks that require addressing before real-world implementation is safe and feasible. The more common real-world use cases of generative AI in healthcare are therefore in non-diagnostic tasks, where errors have a lower risk-profile yet model outputs can significantly improve the efficiency of care providers by alleviating administrative burdens and assisting complex information retrieval or synthesis required in day-to-day work. At the same time, even for such non-diagnostic tasks, assurance of real-world impact requires evaluation grounded in specific use-cases and environments. These evaluations lie beyond the scope of initial benchmarking, and our results should be interpreted with appropriate caution. To assess downstream consequence and generalization of the promise we demonstrate here to real-world clinical workflows, practitioners should adhere to best practices of responsible AI, rigorously measuring multiple endpoints including equity (Pfohl et al., 2024  ###reference_b96###), fairness and safety in the intended environment while also considering the multiple socio-technical factors that are use-case specific determinants of impact. Finally, it is worth noting that while we have considered 14 diverse and challenging benchmarks in this study, over 350 medical benchmarks are available in the community (Meta, 2024  ###reference_b82###).\nOur work has been primarily focused on capabilities and improvements and the art of the possible with Gemini models. An important focal area for future exploration is the integration of the responsible AI principles throughout the model development process (Pfohl et al., 2024  ###reference_b96###), including, but not limited to, the principles of fairness, privacy, equity, transparency and accountability. Privacy considerations in particular need to be rooted in existing healthcare policies and regulations governing and safeguarding patient information. Fairness is another area that may require attention, as there is a risk that AI systems in healthcare may unintentionally reflect or amplify historical biases and inequities (Char et al., 2018  ###reference_b13###; Obermeyer et al., 2019  ###reference_b89###; Cirillo et al., 2020  ###reference_b17###; Gichoya et al., 2022  ###reference_b37###; Abr\u00e0moff et al., 2023  ###reference_b1###; Pfohl et al., 2024  ###reference_b96###), potentially leading to disparate model performance and harmful outcomes for marginalised groups. Such health disparities have been identified across gender (Kent et al., 2012  ###reference_b62###), race (Williams and Wyatt, 2015  ###reference_b137###; Obermeyer et al., 2019  ###reference_b89###), ethnicity (Razai et al., 2021  ###reference_b104###), socioeconomic status (Steptoe and Zaninotto, 2020  ###reference_b114###), sexual orientation (Medina-Mart\u00ednez et al., 2021  ###reference_b81###), age (Jackson et al., 2019  ###reference_b54###), and other sensitive and/or protected personal characteristics. There is an increasing need for a deep intersectional analysis of impact (Iyer et al., 2008  ###reference_b53###; L\u00f3pez and Gadsden, 2017  ###reference_b75###), though this remains a hard technical problem (Cabrera et al., 2019  ###reference_b11###; Yang et al., 2020  ###reference_b140###; Wang et al., 2022a  ###reference_b129###), and an active area of research.\nAs we demonstrate new capabilities for LLMs and LMMs, new opportunities arise for potential issues at the confluence of dataset bias (Ganapathi et al., 2022  ###reference_b33###), model bias (Liu et al., 2023  ###reference_b72###), and the socio-technical considerations for individual use cases. In the context of the capabilities we have discussed, these issues may potentially occur in in-context learning within the long-context utilization of potentially biased examples and instructions, in search integration, the dynamics of self-training, or multimodal understanding with fine-tuning and customized data encoders. Within each of these capabilities, there could be multiple points at which such biases may need to be considered. When it comes to web search integration, biases could come up at query construction time, get reflected in the returned result set (Novin and Meyers, 2017  ###reference_b87###), or be embedded within each of the linked external sources, and manifest in various other subtle ways, e.g. how the results are integrated into the generative reasoning process when producing the final answer. With multimodal models, biases may occur in each of the individual modalities separately, or only be apparent jointly, across co-dependent modalities of the data (Srinivasan and Bisk, 2021  ###reference_b113###; Mandal et al., 2023  ###reference_b78###). A comprehensive analysis of potential issues may need to consider each of these points separately, but also holistically as they are all parts of a complex system. These systems may also need to be thoroughly evaluated not only in isolation, but also with human experts in the loop.\nHowever, these new capabilities also present an opportunity to mitigate prior issues and dramatically improve accessibility across use-cases. For example, new long-context capabilities in medicine may enable a model\u2019s users to solve complex problems at inference time without the need for engaging in model fine-tuning, as the data can be utilized directly within the context of the query, followed by a set of natural language instructions. Previously, users of such systems would have needed to possess engineering expertise and invest additional time and resources in fine-tuning custom models for tackling such complex tasks. Web search integration, on the other hand, may prove to be invaluable when it comes to rapidly integrating newly developed pieces of medical knowledge and external consensus on what is a highly dynamic and non-stationary medical landscape. The COVID-19 pandemic has shown just how quickly the public health understanding and recommendations may need to get updated, and it also highlighted the overall danger posed by medical misinformation (Kouzy et al., 2020  ###reference_b64###). Models that can reliably consume reputable up-to-date external sources may be far less likely to lead to such misinformation. Similar new opportunities are presented by the other model capabilities, though further study is needed to develop a robust evaluation framework to assess the associated risk of bias and unfair outputs (whether individually or jointly across complex use-cases), with such assessments sociotechnically grounded in real settings for specific clinical use-cases."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Large multimodal language models are ushering in a new era of possibilities for health and medicine. The capabilities demonstrated by Gemini and Med-Gemini suggest a significant leap forward in the depth and breadth of opportunities to accelerate biomedical discoveries and assist in healthcare delivery and experiences. However, it is paramount that advancements in model capabilities are accompanied by meticulous attention to the reliability and safety of these systems. By prioritizing both aspects, we can responsibly envision a future where the capabilities of AI systems are meaningful and safe accelerators of both scientific progress and care in medicine."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "This project was an extensive collaboration between many teams at Google Research and Google DeepMind.\nWe thank Taylan Cemgil, Jake Sunshine, Daniel Golden, Pete Clardy, Zoubin Ghahramani and Dr. Gary Peltz (Stanford University) for their comprehensive review and detailed feedback on the manuscript. We also thank Sami Lachgar, Lauren Winer, John Guilyard, and Maggie Shiels for contributions to the narratives and visuals. We thank Yun Liu for discussions, design, and preliminary analysis for the MedQA label uncertainty experiments. We are grateful to Noam Velan, Ira Ktena, Eric Aboussouan, Karan Singhal, Shashir Reddy, Aza Tulepbergenov, Priya Gupta, Rory Sayres, Naama Hammel, Jen McKay, Peter Clardy, Chu-ling Ko, Abhinav Das, Haiyang Yu, Chang Liu, Yuchen Liu, Erica Moreira, Jordan Grimstad, Brett Hatfield, Gordon Turner, Jackie Barr, Jim Winkens, Jackie Barr, Brian Cappy, Pinal Bavishi, Tim McConnell, Ines Mezzorag, Annisah Um\u2019rani, Christian Wright, Divya Pandya, Daireen Garcia, Prachant Bradwell, Alyssa Pierce, Sarah-Jane Allen, Erica Harland, Jennifer Ye, Praney Mittal, Donny Cheung, Andy Crowne and Preeti Singh for their valuable technical support during our research. Finally, we are grateful to Shravya Shetty, Sushant Prakash, Susan Thomas, Michael Howell, Karen DeSalvo, and Zoubin Ghahramani for their support of this project."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Data Availability",
            "text": "Except for the three clinical abstraction tasks, the remaining datasets used for development, benchmarking and evaluation of the AI systems are open source or otherwise accessible publicly with permissions. We will make our re-annotation of the MedQA (USMLE) dataset publicly available."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Code Availability",
            "text": "We are not open-sourcing model code and\nweights due to the safety implications of unmonitored use of such a system in medical settings. In the interest\nof responsible innovation, we will be working with research partners, regulators, and providers to validate and\nexplore safe onward uses of our medical models and expect to make them available via Google Cloud APIs in due course."
        }
    ],
    "url": "http://arxiv.org/html/2404.18416v2",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.1",
            "2.2",
            "2.3",
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.18416v2",
        "paper_title": "Capabilities of Gemini Models in Medicine",
        "research_background": "### Paper's Motivation:\nThe paper is motivated by the intricate nature of medical practice, which involves a variety of tasks such as patient consultations, diagnostics from multimodal sources, staying current with medical research, and performing advanced clinical reasoning. The growing capabilities of large language and multimodal models (LLMs and LMMs) provided by AI, such as the Gemini models, offer a promising avenue to enhance these tasks. However, current AI systems show gaps in their ability to handle complex reasoning, real-world medical data, and multimodal information in a reliable manner.\n\n### Research Problem:\nThe main research problem addressed in this paper is the development and refinement of AI models, specifically based on the Gemini family, to improve their performance in medical-specific tasks. This involves enhancing their clinical reasoning, their ability to process multimodal and long-context data, and their real-world applicability in medical settings. The paper intends to introduce Med-Gemini, a specialized family of models built upon the Gemini foundation, that can effectively tackle these challenges and ultimately assist clinicians and patients more intuitively and helpfully.\n\n### Relevant Prior Work:\n1. **Medical Task Assistance by AI Systems:** \n   - Rajpurkar et al. (2022) underscore early AI applications focused on specific medical tasks.\n   - Demonstrations of generalist approaches are noted in works by Tu et al. (2024a), Moor et al. (2023a).\n\n2. **Large Language and Multimodal Models in Medicine:**\n   - Tools like GPT-4 (Achiam et al., 2023), PaLM (Chowdhery et al., 2023), and Gemini (Gemini Team, Google, 2023) illustrate the effectiveness of these models in encoding clinical knowledge and medical question answering (Kanjee et al., 2023; Eriksen et al., 2023; Antaki et al., 2023).\n\n3. **Challenges and Fine-tuning of LLMs:**\n   - The need for specialized prompting and fine-tuning for medical data is highlighted by Nori et al. (2023) and Ouyang et al. (2022).\n\n4. **Medically Fine-tuned LLMs:**\n   - Examples include the Med-PaLM series, which have been shown to provide high-quality answers surpassing physicians in certain areas (Singhal et al., 2023a; Toma et al., 2023; Luo et al., 2022).\n\n5. **Comparative and Clinical Performance:**\n   - Models like Flamingo-CXR, Med-PaLM M, and AMIE show comparative performance with radiologists and primary care physicians in controlled settings (Moor et al., 2023b; Huang et al., 2023; Tanno et al., 2024; Tu et al., 2024b).\n\n6. **Limitations in Current Models:**\n   - Suboptimal clinical reasoning under uncertainty with issues of confabulation and bias (Umapathi et al., 2023).\n   - Challenges in using tools and up-to-date information effectively (Zakka et al., 2024) and collaborating with clinicians (McDuff et al., 2023).\n\n7. **Multimodal and Long-Context Handling:**\n   - Limitations in handling complex multimodal data and long-context understanding (Tu et al., 2024a).\n\n8. **Architectural Innovations:**\n   - The mixture-of-experts architecture allows efficient scaling and reasoning over complex data (Shazeer et al., 2017; Fedus et al., 2022).\n\nThese references provide a backdrop for the advancements in Med-Gemini, aiming to build upon these strengths and address the identified gaps to deliver a highly capable and specialized tool for medical applications.",
        "methodology": "The methodology section provides an in-depth look at the proposed Med-Gemini models, detailing how they build upon the capabilities of the existing Gemini models to enhance their performance for medical applications. Here is a description of the methodology, maintaining the original wording as closely as possible:\n\n### Overview of Gemini Models:\n\nThe Gemini ecosystem consists of a variety of models varying in size, modality encoders, and architectures, trained on a diverse set of high-quality data. These models have achieved state-of-the-art results across a range of language, reasoning, coding, multilingual, image, and video benchmarks. Notable models include:\n- **Gemini 1.0 Ultra:** Excels in complex reasoning for language-based tasks.\n- **Gemini 1.5 Pro:** Handles long-context inputs and multimodal inputs like hours of video or tens of hours of audio.\n- **Gemini 1.0 Nano:** Efficient for on-device operations due to its small size.\n\n### Development of Med-Gemini Models:\n\n#### Advanced Reasoning via Self-Training and Web Search Integration:\n- **Med-Gemini-M 1.0:** Fine-tuned from Gemini 1.0 Pro for tasks requiring less complex reasoning such as summarizing medical notes.\n- **Med-Gemini-L 1.0:** Fine-tuned from Gemini 1.0 Ultra for tasks requiring advanced reasoning. Utilizes a self-training method to efficiently integrate web search, with a novel uncertainty-guided search strategy to improve clinical reasoning tasks.\n\n#### Multimodal Understanding via Fine-Tuning and Customized Encoders:\n- **Med-Gemini-M 1.5:** Fine-tuned from Gemini 1.5 Pro on multimodal medical datasets.\n- **Med-Gemini-S 1.0:** Demonstrates adaptation to novel medical modalities using specialized encoders with Gemini 1.0 Nano.\n\n#### Long-Context Processing with Chain-of-Reasoning:\n- Re-uses Med-Gemini-M 1.5 for long-context tasks and introduces a chain-of-reasoning technique inspired by Tu et al. (2024b) to enhance understanding of long EHRs.\n\n### Data Generation and Fine-Tuning:\n- **MedQA-R Dataset:** Extends MedQA with synthetic reasoning explanations or \u201cChain-of-Thoughts\u201d (CoTs).\n- **MedQA-RS Dataset:** Extends MedQA-R with instructions to use web search results for additional context.\n- Incorporates long-form question answering dataset and medical summarization dataset to add variety to the fine-tuning data.\n\n### Self-Training Framework:\n- Uses high-quality synthetic examples of clinical reasoning with web search.\n- **Key Ingredients:** Web Search integration, in-context demonstrations, CoT generation, and iterative fine-tuning.\n\n#### Novel Uncertainty-Guided and Iterative Search Process:\n- Generates multiple reasoning paths.\n- Utilizes uncertainty-based search invocation and uncertainty-guided search query generation.\n- Incorporates search results to refine responses.\n\n### Multimodal Fine-Tuning:\n- Uses datasets from MultiMedBench and others like Slake-VQA, Path-VQA, MIMIC-CXR, PAD-UFES-20, and ROCO.\n- Fine-tunes Med-Gemini-M 1.5 with task-specific instructions.\n\n### Expansion for Raw Biomedical Signals:\n- Med-Gemini-S 1.0 processes raw biomedical signals using specialized encoders.\n- Example: Answers questions using raw 12-channel ECG waveforms.\n\n### Long-Context EHR Understanding:\n- **Task Setup:** Curates long EHR cases to search and retrieve medical problems.\n- **Two-Step Chain-of-Reasoning Approach:** Retrieves and determines the presence of medical problems from mentions in EHR notes.\n- **Comparative Baseline:** Uses a heuristic-based method for performance comparison.\n\n### Evaluation of Surgical and Procedural Videos:\n- Uses zero-shot prompting for medical video understanding.\n- **Tasks:** Medical Visual Answer Localization (MVAL) and Critical View of Safety (CVS) assessment.\n\n### Anticipations and Future Work:\n- Envisions integrating various health-related signals such as wearable data, genomic information, nutritional data, and environmental factors.\n- Future work includes multimodal exploration using uncertainty-guided search at inference to benefit text-only benchmarks initially.\n\nThis methodology outlines the innovative approaches and detailed steps undertaken to enhance the capabilities of the Med-Gemini models for various medical applications, emphasizing reasoning, multimodal understanding, and long-context processing.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nThe main experiments evaluate the performance of Med-Gemini-M 1.5 across three core task categories in the medical domain: \n\n1. **Text-Based Reasoning:** The primary focus here is on understanding long clinical narratives within electronic health records (EHRs).\n2. **Multimodal Tasks:** These involve complex interactions between video data and textual queries.\n3. **Long-Context Processing Tasks:** Evaluates abilities in handling extended information tasks through a medical lens.\n\n#### Long-Context EHR Understanding Task\n\n**Dataset:**\n- Curated 200 examples from the MIMIC-III database, specifically termed the MIMIC-III-Needle-in-a-Haystack task.\n- Examples: Each example contains a large collection of EHR notes (>100 medical notes; 200,000-700,000 words) from 44 unique ICU patients.\n- Each example includes a single condition mentioned once across all notes.\n- Ground truth is a binary variable (presence or absence of the condition) derived from majority votes of three physician raters.\n- Positive cases: 121\n- Negative cases: 79\n\n**Evaluation Metrics:**\n- **Precision** and **Recall**\n\n**Baselines:**\n- Heuristic-based annotation-aggregation method (Feder et al., 2022  ###reference_b28###).\n\n#### Multimodal Tasks\n\n**Medical Visual Answer Localization (MVAL) Task:**\n\n**Dataset:**\n- **MedVidQA**: Used to create two video span prediction tasks:\n  - Video + subtitle text.\n  - Video only.\n\n**Evaluation Metrics:**\n- Intersection over Union (IoU) measured at thresholds of 0.3, 0.5, 0.7.\n- Mean IoU (mIoU).\n\n**Baselines:**\n- Methods from Li et al. (2022  ###reference_b68###) and Gupta et al. (2023  ###reference_b44###).\n\n**Critical View of Safety (CVS) Assessment Task:**\n\n**Dataset:**\n- **Cholec80-CVS:** Annotated video clips within full surgery videos meeting CVS criteria with scores of 0, 1, or 2, across three criteria.\n\n**Evaluation Metric:**\n- Average accuracy against Cholec80-CVS annotations across 572 annotated video clips.\n\n#### Main Experimental Results:\n\n- **Med-Gemini-M 1.5** achieved significant performance improvements in retrieving relevant text spans and determining condition existence in the MIMIC-III-Needle-in-a-Haystack task compared to the heuristic-based method.\n- In the **MedVidQA** tasks (MVAL), Med-Gemini-M 1.5 outperformed baselines in terms of IoU and mIoU, showing its robust capability in aligning video segments with textual queries.\n- For the **CVS Assessment Task,** Med-Gemini-M 1.5 demonstrated promising accuracy in predicting which CVS criteria were met in surgical videos, indicating its effectiveness in understanding and evaluating surgical protocols.\n\nOverall, Med-Gemini-M 1.5 showcases strong performance across text-based reasoning, multimodal understanding, and long-context medical tasks, holding the potential to transform various medical and clinical applications."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To enhance the clinical reasoning ability of the Med-Gemini model by integrating web search results and generating accurate explanations through self-training.",
            "experiment_process": "The process involved creating two novel datasets, MedQA-R (Reasoning) and MedQA-RS (Reasoning and Search), and fine-tuning Med-Gemini-L 1.0 on these datasets. Experimentation included generating Chain-of-Thoughts explanations, using an iterative uncertainty-guided search process for refinement, and involved the MedQA (USMLE) benchmark. The experimental setup included four steps for inference: multiple reasoning path generation, uncertainty-based search invocation, uncertainty-guided query generation, and search retrieval.",
            "result_discussion": "The result was an improved Med-Gemini-L 1.0 model with advanced clinical reasoning capabilities, achieving SoTA performance on the MedQA benchmark with 91.1% accuracy. It showed enhanced integration of search results and better handling of uncertainty in reasoning paths.",
            "ablation_id": "2404.18416v2.No1"
        },
        {
            "research_objective": "To leverage the multimodal capabilities of the Med-Gemini model for specialized medical tasks through fine-tuning with custom encoders.",
            "experiment_process": "The study used eight multimodal tasks across six datasets, including image-to-text datasets from MultiMedBench, PAD-UFES-20, and ROCO. The fine-tuning involved task-specific instructions and dataset-specific mixtures to create the Med-Gemini-M 1.5 model. A cross-attention mechanism based on Flamingo was used to develop Med-Gemini-S 1.0 for processing ECG waveforms.",
            "result_discussion": "The resulting Med-Gemini-M 1.5 showed improved performance on multimodal tasks such as radiology, pathology, and medical captioning. Med-Gemini-S 1.0 exhibited the ability to answer questions based on raw biomedical signals like ECG waveforms.",
            "ablation_id": "2404.18416v2.No2"
        },
        {
            "research_objective": "To assess and enhance the long-context capabilities of Med-Gemini models for handling large volumes of detailed medical data.",
            "experiment_process": "The study evaluated Med-Gemini-M 1.5 on long-context tasks including 'needle-in-a-haystack' EHR retrieval and medical video question answering. The experiment used a two-step chain-of-reasoning approach for EHR tasks and assessed the understanding of medical videos using zero-shot prompting. The evaluation involved datasets from MIMIC-III and the MedVidQA benchmark.",
            "result_discussion": "Med-Gemini-M 1.5 demonstrated effective long-context retrieval and reasoning capabilities, significantly outperforming baseline methods in EHR tasks and showing promising results in understanding medical videos, indicating its potential for practical medical applications.",
            "ablation_id": "2404.18416v2.No3"
        }
    ]
}