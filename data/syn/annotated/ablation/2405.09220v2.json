{
    "title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models",
    "abstract": "In this paper, we present the findings of our Project ALPINE which stands for \u201cAutoregressive Learning for Planning In NEtworks.\u201d\nProject ALPINE initiates a theoretical investigation\ninto the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms,\naiming to identify any potential limitations in their planning abilities.\nWe abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node.\nIn terms of expressiveness,\nwe show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights.\nOur theoretical analysis of the gradient-based learning dynamic of the Transformer reveals\nthat the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix.\nThese theoretical insights\nare then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix,\nwhich aligns with the predictions made in our theoretical analysis.\nAdditionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent.\nOur theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path.\nIn summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to our understanding of the general planning capabilities in other related domains.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs), such as ChatGPT, have impressed everyone with their powerful capabilities in multi-faceted tasks spanning language processing, knowledge extraction, reasoning, planning, coding, tool use, and more.\nThe broad spectrum of intelligent capabilities exhibited by LLMs\nreflects promising signs of\nartificial general intelligence (AGI) [BCE+23  ###reference_bx5###] and catalyzes an AI revolution: Individuals and organizations are now striving to develop more powerful and adaptive\nAI models towards AGI, while also integrating LLM-based AI models into\nvarious aspects of our work and daily lives.\nHowever, at the same time, we are still\nintrigued by the underlying mechanisms\nthat fuel the power of LLMs.\nWhile all current LLMs are\nbuilt upon the Transformer neural network architecture,\nwhich employs autoregressive learning to predict the next word in a language sequence, the question remains:\nWhy does the Transformer-based autoregressive learning architecture produce such exceptional performance across a wide range of intelligent tasks?\nTo put it in plain English: Why does next word prediction generate intelligence?\nThere is no definite answer to this question yet. But researchers are tackling this problem from various angles,\naiming to provide explanations to the power of LLMs.\nIn this paper, we focus on the planning capability of LLMs.\nPlanning is a fundamental\nconstruct\nof human intelligence and is involved in almost every aspect of our daily life, e.g., planning a task at work, organizing a trip,\nseeking a mathematical proof of a theorem, etc.\nUnderstanding how LLMs completes a planning task\ncan shed light on\nthe transformation of the seemingly low-level statistical task of next word prediction\ninto a high-level intelligent task like planning.\nThis understanding may serve as a potential pathway to\ncomprehend and explain other intelligent behaviors\nexhibited by LLMs.\nThere are already a number of studies on the planning capabilities of LLMs.\nBut most of them\nmainly focus on the empirical evaluation of LLMs.\nAlthough these studies may show some evidences that LLMs have planning capabilities\nto a limited extent, the results are partial and\ndo not explain why LLMs can or cannot\nsuccessfully accomplish specific planning tasks\n(see Section 7  ###reference_### for more detailed discussions on the related work).\nIn light of this context, we initiated Project ALPINE, which stands for Autoregressive Learning for Planning In NEtworks.\nProject ALPINE aims to not only empirically evaluate the planning performance of LLMs but also provide theoretical interpretations on how LLMs accomplish such tasks.\nTo provide a solid foundation for our investigation, we must define a specific task that serves as a representative example of a planning task.\nGiven that planning often entails making\nsequential selections of next steps within a multi-step procedure to achieve a desired goal, it naturally\nrelates to the path-finding task in networks.\nA complex task\nis often represented as a task graph, where nodes\ncorrespond to subtasks or intermediate stages, and edges represent the ordering relationships\nbetweeb these subtasks.\nTask planning\ninvolves finding a valid path\nwithin the task graph to reach a pre-determined goal.\nFor example, project planning can be viewed as navigating through the multi-stage project planning graph, while a mathematical proof can be seeing as a path from the axioms to the final theorem,\nwith lemmas serving as intermediate nodes.\nMany previous studies on LLM planning capabilities are also related to path finding.\nFor example, a benchmark planning game called Blocksworld [VMSK24  ###reference_bx21###] used for evaluating LLMs\ncan be viewed as\npath finding from the initial blocks\u2019 state to the final blocks\u2019 state in a state transition graph.\nSimilarly, HuggingGPT [SST+23  ###reference_bx18###] for scheduling API calls can\nbe likened to finding a call path in the API call graph.\nIn Project ALPINE, we investigate the following path-finding task: given an underlying graph,\nthe training data\nconsists of a collection of paths in the graph\nthat specify the source node , the target node  and a path from \nto .\nThe objective of the test is to generate a path from  to , given new source-target pair .\nNote that, for this path-finding task, the generative model\nmust generate a valid path in one shot without relying on trial and error.\nIn this case,\nthe key challenge, when giving the current node on the path, lies in correctly identifying the next node on the path, and\nthis node should be both adjacent to the current node and reachable to the target node\n(see Algorithm 1  ###reference_###).\nThis suggests that\nin order to accomplish the path-finding task, it is essential to extract the information\nabout the adjacency and reachability of the graph from the training data.\nOur investigation below demonstrates that the Transformer model is indeed performing this extraction to a certain extent.\nMore specifically, we investigate how the Transformer-based autoregressive learning architecture achieves the path-finding task by examining the following aspects.\nFirst, we show that the Transformer architecture has the expressive power to complete the task by manually constructing a Transformer that encodes the adjacency matrix and reachability matrix of the network as part of its model.\nSecond, we conduct theoretical analysis\nto further explore the capabilities of the Transformer model.\nFor a simplified Transformer model, when applying gradient descent to minimize the cross-entropy loss on the path training data, our analysis reveals that\nthe model\ncan extract the adjacency matrix and a limited form of the reachability matrix.\nSpecifically, the feed-forward layer encodes the adjacency matrix,\nwhile the attention layer captures a partial form of the reachability matrix.\nThis process mimics human intelligence in generating the next node that is both adjacent to the current node and reachable to the target node.\nHowever, our theoretical analysis also reveals a limitation of the Transformer model. It cannot learn the full\ncomplete reachability relationship.\nParticularly the reachability\nderived from transitivity cannot be learned by the model as the Transformer falls short in capturing these complex reachability patterns.\nThird, we conduct extensive experiments that train Transformer models on the path language through autoregressive learning, and test its capability\nfor generating valid paths for new pairs of source and target nodes.\nOur empirical results\nprovide compelling evidence that the Transformer can excel in achieving high accuracy in the path-finding task.\nThese findings also align with our theoretical analysis as they show\nthat the trained Transformer model generates the next node on the path by focusing its attention on the target node, and effectively learns the adjacency and reachability matrices in its feed-forward layer and attention layer, respectively. Moreover, we observe\na significant drop in test accuracy\nwhen the source and target can only be connected through concatenation of path segments in the training data.\nThis indicates the requirement for\nhigher-order transitive relationship to establish reachability.\nThis matches our theoretical analysis, showing that the Transformer indeed has limitation in learning transitive reachability relationships.\nFinally, we demonstrate that the Transformer can successfully learn a task planning benchmark called Blocksworld [VMSK24  ###reference_bx21###], a planning task that corresponds directly to the path-finding problem.\nIn summary, our investigation in Project ALPINE\nmakes the following contributions:\n(a) We\nhave initiated a theoretical analysis\nthat explains how Transformer achieves a path-planning task through its gradient descent autoregressive learning mechanism;\n(b) Our empirical evaluation corroborates\nour theoretical analysis and clearly demonstrates how the Transformer\naccomplishes path planning by extracting the adjacency and reachability information,\nwhile focusing\nattention on the target node.\n(c) Both our theoretical and empirical analyses\nuncover a limitation of the Transformer architecture, highlighting its inability to handle transitive reachability relationship in the path-finding task, which\nholds significant implications.\nOur findings\nrepresent an initial step toward\nunraveling the underlying mechanism that empowers the intelligent\nof LLMs.\nWe believe that\nthis meaningful first step\nbrings us closer to achieving our ultimate\nobjective.\nWe hope that our findings,\nalong with our integrated theoretical and\nempirical approach, will prove valuable\nto the community, facilitating collective\nprogress\nin understanding\nLLMs and\ndriving improvements future generations of\nthese models.\nThe rest of the paper is organized as follows.\nIn Section 2  ###reference_###, we provide the preliminaries for the study.\nSection 3  ###reference_### presents\nan overview of our technical results, including the expressive power of the Transformer model in the path-finding task.\nIn Section 4  ###reference_###,\nwe present our theoretical analysis on how a simplified Transformer model solves the path-finding task.\nSection 5  ###reference_### provides the detailed empirical evaluation results that reinforce\nour theoretical analysis.\nSection 6  ###reference_### highlights our findings on the Blocksworld task, which are consistent with our main findings.\nFollowing that, in Section 7  ###reference_###, we provide an overview of related work and discuss the implications of our results.\nFinally, we conclude the paper in Section 8  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "Throughout this paper,\nwe use the following notations for matrices and vectors:  and  stand for a column vector and a matrix, respectively.\nNotations  and  denote the  entry of vector  and the  entry in\nmatrix , respectively.\nWe also denote the  row of matrix  by  and the transpose of  by ."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Auto-regressive Transformer Architecture and Loss Function",
            "text": "In this paper, we adopt the NanoGPT architecture. Let  denote the sequence length,  the embedding size,  the number of heads,  the embedding size per head, and  the vocabulary size.\nOne key component of the architecture is its attention mechanism, which is formulated as\nwhere , ,  are the query, key, and value matrices, respectively.\nFunction softmax takes a vector  and transforms it into  with .\nWhen softmax applies to a matrix, it applies to every row of the matrix.\nDenoting  as input, the multi-head attention is computed as\nwhere , ,  are the learnable weight matrices for the query, key, and value matrices of the  head.\nThe feed-forward layer is a two-layer multi-layer perceptron (MLP) defined as\nwhere , , , and  are the learnable weight matrices of FFN, and\n denotes the all-one matrix with dimension .\nFinally, one-layer Transformer is defined as\nwhere  and  are two layer normalizations. The layer normalization is defined as , where the expectation  and standard deviation  are averaged across all terms in , , and  are two learnable scalars.\nWith these essential components\nin place, we\nproceed to introduce the procedures of GPT. The training data consists of a sequence of tokens , where  is the token id for the  token. We first represent the tokens by the one-hot embedding matrix , where  and  elsewhere. Then there is a learnable token embedding matrix \nand positional embedding matrix , and the input .\nThis input  is fed into an -layer Transformer\nto obtain the predicted next word111The learnable weight matrices of different layers are different, and thus the layer index  should be\nadded as a subscript to these matrices. But our later analysis is only on a one-layer Transformer.\nThus we omitted this layer index.:\nFinally, the output embedding goes through another layer normalization , and then it is multiplied by a learnable output weight matrix  to convert back to probability weights\nover all possible tokens.\nWe calculate the output probability vector at position , denoted as , to predict the next token for position , which reflects the auto-regressive learning paradigm:\nThe actual token  is sampled\naccording to the probability vector  and a temperature parameter.\nWhen temperature parameter is set to , which is what we use throughout the paper, the sample is directly\nproportionally to\nthe probability value in .\nThe adopted loss function is the cross-entropy loss for the next token prediction\ngiven by"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Path Planning Dataset",
            "text": "The dataset is designed to test GPT\u2019s path planning capability on simple graphs.\nWe consider a directed graph , where  is the set of nodes, and  is the set of directed edges, i.e.,\nfor any ,  means that there is an edge from node  to node  in .\nA pair of source node  and target node  is considered as a valid pair if\n contains least one path from  to .\nWe\nallocate a\nportion\nof valid  pairs to the training dataset and\nassign the remaining pairs to the test dataset.\nThe samples in the training dataset  is sequences of the format \u201c       n\u201d, where  is the source node,  is the target node,      is a valid path in  from  to , and n indicates the end of the sequence. In the test dataset, we provide only the source and target nodes in the format \u201c \u201d. The model is tasked with completing the remaining tokens in the sequence.\nThe completion is\ndeemed correct if the model generates a valid path in graph  with the correct syntax."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Path-Finding Transformers: Expressive Power and Learning Capacity",
            "text": "In this section, we present an overview of our main results.\nFirstly, in Theorem 2  ###reference_orem2### below,\nwe establish the mathematical existence of a Transformer model capable of effective path finding in any given network.\nNext, in Theorem 3  ###reference_orem3###\u2014\nthe proof of which will be the main\nfocus of Section 4  ###reference_###\u2014we characterize the learning capacity and limitations of path-finding transformer models in a data-driven gradient descent framework.\nThe empirical evaluation that supports these theoretical analyses will be discussed in Section 5  ###reference_###.\nIn our path-finding task, the essential step for completing a path is to predict the next node based on the current information. It is apparent that to predict the subsequent node on the path, only the information related to the current node and the target node is necessary.\nAlgorithm 1  ###reference_### introduces a handcrafted algorithm that utilizes both the adjacency matrix and the reachability matrix of the graph.\nThe true adjacency matrix follows the standard definition in graph theory, i.e.,\nThe true reachability matrix is defined as:\nAssuming that  is reachable by , then Algorithm 1  ###reference_### is guaranteed to output a valid path with input  and .\nTo Illustrate the expressive\ncapacities of the Transformer model in path finding,\nwe first\nshow that we can manually construct a Transformer model to perform the path planning task by simulating the idealized Algorithm 1  ###reference_###. In the manual construction, the task for the model is to find a path from a start node  to a target node  with the format \u201c\u201d. Consider every time that the Transformer takes \u201c\u201d as the input and outputs  for  (assuming  and ): if  is an out-neighbor of  and can reach  in , then we say that the Transformer outputs a correct response.\nGiven a graph  (with adjacency matrix  and reachability matrix ), for every , there exists a -layer, -head, and -embedding-size Transformer that makes correct response in every step of the above task with probability\nat least .\nFor simplicity, we omit all layer normalizations in this construction.\nBefore presenting the detailed proof, we provide a summary of our construction.\nIn essence, we\nutilize the attention layer to attend the output\nsolely\nto the target node .\nThis approach allows the distribution of next token  to become a function of both the current node  and the target node  (as formulated in Section 2  ###reference_###).\nThen, by integrating the adjacency matrix  into the MLP layer and the reachability matrix  into the matrix  in the attention layer, we extract row vectors\n and  from  and , respectively, corresponding to the target node  and current node .\nSpecifically,  and  are stored by  and , respectively.\nBy selecting proper coefficients, we can ignore the effect of the remaining term  in  and only keep a weighted sum of  and .\nFollowing the softmax layer, the non-negligible entries in the final vector correspond to the feasible next nodes.\nWith this encoding, the Transformer serves as a simulator of the idealized Algorithm 1  ###reference_### with input  and .\nWe now provide the detailed proof. Suppose the input token sequence is \u201c\u201d with , where  () and\n are the tokens of the source and target nodes,\nrespectively, and nodes \nform a path that can reach node  in graph .\nOur objective is to construct a -layer, -head Transformer model\nthat consistently generate an out-neighbor  of ,\nenabling a path from \nto  in .\nFollowing our notation in Section 2.1  ###reference_###, we adopt ,  and .\nIn the Transformer, there are  tokens representing the  nodes and the end-of-line \u2018n\u2019.\nHence, the input tokens can be represented by the one-hot embedding matrix .\nWe let \nand\n,\nhere\n represents the second unit column vector of dimension ,  is the notation for matrix concatenation by column,\nand  is a positive parameter to be decided.\nAccording to the definition of the Transformer, we now have a matrix  such that the first  columns\nare the tokens of nodes in the sequence and the last column indicates the positions of the target node .\nMore specifically, we have\nhere  represents the one-hot token vector for node  (with dimension ).\nThen we construct the attention layer of our Transformer. We only have one head and let  and . Then we can compute  (i.e. second rows are all \u2019s and other rows are all \u2019s)\nand .\nTherefore,\nAnd we can compute the first part of the attention layer as\nBy setting , we obtain:\nFurthermore, we set ,\nwhere  is also a parameter to be decided later. Then after the attention layer, we have a matrix as\nNow we construct the feed-forward layer, which is a two-layer MLP.\nFor the first layer, the weight matrix  is set to be,\nand the bias , which implies that . When  is large enough, the  row of the matrix\n is .\nSince  can reach , in , only the entry for node  is  while all other entries are  or .\nTherefore, the  row of the matrix\n can be arbitrarily close to\n. Here  represents the one-hot token vector for node  (with dimension ).\nFor the second layer, we set\nwhere  are positive parameters to be decided, and .\nBy this way, we have\nTherefore,\nwhere  represents the one-hot token vector for node  (with dimension ).\nThen we fix  and let them be large enough.\nIn this case, the dominant entries in  represent the nodes that are both the out-neighbor of  and reachable to , since those entries will\nhave the value of  while others entries are at most .\nThis means that  can correctly indicates the next node .\nSpecifically, let \nThen the final output approaches the following vector\nwhere  is the number of nodes that are both the out-neighbor of  and reachable to .\nThus, this encoding guarantees that this is exactly the correct output of the next node.\nHence, for any , we can always find a -layer, -head, and -embedding-size Transformer that provides the correct response with probability at least  by selecting large enough parameters .\nFinally, there are two different rules (other than output a correct next node): i) when the input sequence is only \u201c \u201d, the prediction of the next token should be the source node ; ii) when the input sequence is only \u201c      \u201d, the prediction of the next token should be n. Case i) can be constructed using the Transformer architecture utilizing the position information and attention to the first position; and case ii) can be constructed by using the Transformer architecture utilizing the position information and attention to the second position.\nTo maintain focus on the main construction corresponding to Algorithm 1  ###reference_###, we omit the detailed construction for these two boundary cases.\n\u220e\nHaving established the mathematical existence of a Transformer model capable of accomplishing path finding in a given network, as demonstrated in Theorem 2  ###reference_orem2###, we now shift our focus to the following fundamental question\nCan the Transformer architecture, trained on sufficient path data\nwith an auto-regressive loss as in Equation (7  ###reference_###) and using the gradient descent (GD) method,\nlearn the adjacency and reachability matrices and carry out path finding similar to the idealized Algorithm 1  ###reference_###?\nThrough a combination of theoretical analysis and empirical evaluation presented in the following section, our primary investigation aims to address the aforementioned question.\nFirst,\nit is important to note that the Transformer may not be capable to learn the exact true adjacency and reachability matrices of the underlying graph.\nInstead, it can only learn the relevant information that\nis directly encoded in the observed training data .\nTherefore, we define the\nobserved adjacency and reachability matrices based on the training data  as follows.\nNaturally, the observed adjacency matrix  only records the edges  that appears in some path\nwithin the training data . On the other hand, the observed reachability matrix \nexhibits more nuanced distinctions from\nthe true reachability matrix.\nIt only records that  is reachable from node , if\nthe training data \ncontains a path (sequence) whose destination is  and  appears as a non-source node on the path.\nWe call such pairs  observed reachable pairs.\nTherefore, the observed reachability matrix would miss the following types of reachable pairs  in  (referred as non-observed reachable pairs):\n(i) there is no path in  that contains a sub-path from  to , even if a path from  to  can be obtained by concatenating several sub-paths appeared in ;\n(ii) there are some paths in  that contains a sub-path from  to , however,  is not the target node in these paths;\n(iii) there are some paths in  that contains a sub-path from  to  and  is the target node in these paths, however,  is always the source node of these paths.\nIn Section 4  ###reference_###, we show that in a simplified Transformer model, the learning is limited to the observed adjacency and reachability matrices,\nrather than the true underlying matrices.\nThe following\npresents an informal version of the result:\nBy using auto-regressive loss and training with gradient descent, a simplified Transformer architecture with -layer, -head, and -embedding-size simulates Algorithm 1  ###reference_### with\n and .\nThe formal analytical result is presented as Theorem 4  ###reference_orem4###, which\ngives\ncaptures the direction of changes of the parameters in the learnable matrices of the simplified Transformer when following the\ngradient descent calculation.\nWe then discuss that how this result indicates that the simplified Transformer\neffective learns the observed adjacency matrix and the observed reachability matrix, and its inference procedure\nindeed\nalign with the workings of Algorithm 1  ###reference_###.\nSpecifically, in the simplified Transformer, the observed adjacency matrix is encoded within the\nweights of the feed-forward network (FFN)\nas illustrated in Figure 1  ###reference_###,\nwhile the observed reachability matrix is encoded in the value matrix, as depicted\nin Figure 2  ###reference_###.\nNext, in Section 5  ###reference_###, we present\nthe results of our empirical evaluation\nwhich is based on\nextensive experiments.\nWe report the accuracy achieved by the Transformer models with various hyperparameters (Figure 3  ###reference_###).\nFurthermore, we provide\nvisualizations that demonstrate the Transformer\u2019s ability to learn attention\n(Figure 4  ###reference_###)\nas well as the information about adjacency and reachability matrices (Figures 5  ###reference_### and 6  ###reference_###).\nNotably, our findings\nreveal\nthat even a large Transformer model fails to learn the reachability matrix beyond , resulting poor path-finding accuracy for those unobserved reachable pairs (Figure 6  ###reference_###).\nTo further validate our approach, we conduct experiments on a realistic planning dataset called Blockswords. The accuracy, attention, adjacency matrix and reachability matrix are shown in Figure 7  ###reference_### and Figure 8  ###reference_###. Importantly, our empirical results align closely\nwith our theoretical findings discussed in Section 4  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Gradient-based Analysis for Path Finding",
            "text": "Let  be the path dataset as described in Section 2.2  ###reference_###.\nIn this section, we show analytically that even with only one layer and one head, the Transformer architecture could learn\nboth the adjacency matrix and the reachability matrix from the dataset  and then predict the next node on a path, similar to what is done in Algorithm 1  ###reference_###.\nLet  be the number of times in  that i) the current node is ; ii) the destination node is  and iii) the next node is , and let .\nTo simplify the analysis, we consider the following simplified one layer and one head Transformer structure without any layer normalizations. The embedding size is the same as the vocabulary size (), and we only consider the cross-entropy loss of predicting the next node, i.e., only when  (hence it is not repeating the source node) and the token  is not the target node (hence it is not predicting \u201cn\u201d).\nThe attention weight is only on the target node (the second token), i.e., we manually set every row in  (in Eq. (1  ###reference_###)) to be a one-hot vector with the second coordinate being . Moreover, we set the positional embedding matrix , since it is usually used to adjust the attention weights.\nWe remove all the non-linear layers (e.g., the layer normalizations), and use\n instead of Eq. (3  ###reference_###), and use\n instead of Eq. (4  ###reference_###).\nThe token embedding matrix  and the output weight matrix  are set to be identity, i.e., .\nSince there is only one layer and one head, for simplicity, we use  to represent the weight of the value matrix in the attention layer.\nUnder the above Transformer structure,\nwhere  is the manually set attention weight matrix (every row is a one-hot vector with the second coordinate being ).\nTherefore, the weight vector when predicting the  token is , and\nthe prediction probability is\nWe prove the following theorem.\nUnder the cross-entropy loss , for all possible  pairs, i) if , then  is always 0;\nii) if  but , then  is always positive;\niii) if , then  is negative when  converges to .\nSimilarly, for all possible  pairs, i) if , then  is always 0; ii) if  but , then  is always positive; iii) if , then  is negative when  converges to .\nWe only prove the first part of this theorem, since the proof of the second part is almost the\nidentical.\nBy the definition of the cross-entropy loss in Eq.(7  ###reference_###), and the prediction weight vector in Eq.(9  ###reference_###) for our simplified model,\nthe total cross-entropy loss of the model (with matrices , ) is\nThen we have that\nIn case i),  implies that . Hence  is always zero.\nIn case ii),  implies that the second term in Eq. (10  ###reference_###) is positive, while  implies that the first term in Eq. (10  ###reference_###) is 0. Hence  is always positive.\nIn case iii), when  and  converges to , then the second term in Eq. (10  ###reference_###) converges to zero, and it is smaller than . Hence,  is negative when  converges to .\n\u220e\nThe theorem directly leads to a theoretical explanation on how the model learns the adjacency and reachability matrices, as explained below.\n###figure_1### ###figure_2### ###figure_3### ###figure_4### Let  denote the set of edges appearing in the training dataset , which corresponds to the observed adjacency matrix .\nFor any , , and for any , . Then from the above theorem, under the gradient descent learning paradigm,  will keep decreasing (since its gradient is always positive), while  will not (since its gradient becomes negative when its value is sufficiently negative). This tends to make  higher than  after training.\nNote that these terms are weights when predicting the next node: a higher  means that \u201cthe edge  exists\u201d, and a lower  means that \u201cthe edge  does not exist\u201d. By this way, the Transformer model learns the information about the observed adjacency matrix\nwith weight matrix .\nTo facilitate comprehension, we\nconducted a simple experiment, and\npresent the results in Figure 1  ###reference_### (the structure of the Transformer aligns with the description\nprovided in this section).\nIn this experiment, we generate a 10-node graph, and use 3 different training datasets  based on this graph:  contains all the paths with length 1;  contains all the paths with length 1 and  of the paths with length higher than 1; and  contains all the possible paths.\nFigure 1(a)  ###reference_sf1### is the true adjacency matrix of the graph, which is also the observed adjacency matrix for the three datasets.\nFigure 1(b)  ###reference_sf2### is the  matrix with the training dataset , Figure 1(c)  ###reference_sf3### is the  matrix with the training dataset , and Figure 1(d)  ###reference_sf4### is the  matrix with the training dataset 222Matrix  also contains rows and columns\ncorresponding to non-node tokens such as \u2018n\u2019, and we remove these rows and columns in the comparison.\nLater when we compare empirical matrices  and  against theoretical ones, we treat them in the same way..\nUpon observation, it becomes evident that\nthese  matrics all successfully capture\nthe structural information\nfrom the adjacency matrix.\nSpecifically,\nin the  row\nof each of these weight matrices,\nthe  term\ncorresponding to edge  is much higher than the  term corresponding to non-edge .\n###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10### ###figure_11### Similar to the process of learning the adjacency matrix, under the gradient descent learning paradigm,  will keep decreasing when\n is not an observed reachable pairs in the training dataset .\nIn other words, there is no path in  in which  is the target and  is a non-source node on the path.\nOn the other hand, when  is indeed an observed reachable pair, \ndoes not keep decreasing. This tends to make  higher than  after the training.\nBy this way, the Transformer model captures the structural information of observed reachability matrix\nwith weight matrix .\nHowever, our analysis\nindicates that the model may not learn non-observed reachability relationship even if all the edges are\npresent in the training data.\nThese non-observed reachable pairs \nencompass several cases, which are summarized in Section 3  ###reference_###.\nFigure 2  ###reference_### shows the correlation between  and the observed reachabilities under different dataset \u2019s in the above simple experiment. Figure 2(a)  ###reference_sf1### is the real reachability matrix of the graph; Figure 2(b)  ###reference_sf2### is the observed reachability matrix in dataset , and Figure 2(c)  ###reference_sf3### is the  matrix under ; Figure 2(d)  ###reference_sf4### is the observed reachability matrix in dataset , and Figure 2(e)  ###reference_sf5### is the  matrix under ; and similarly, Figure 2(f)  ###reference_sf6### is the observed reachability matrix in dataset , and Figure 2(g)  ###reference_sf7### is the  matrix under . These illustrations\nshows\nall the weight matrices  can satisfactorily learn the structural information of the observed reachabilities\npresent in the training datasets.\nHowever,\nthe Transformer models cannot deduce non-observed reachabilities.\nIn particular, we demonstrate that all three types of non-observed reachable pairs as summarized in Section 3  ###reference_### appear in this example:\n(i) there is no paths that contain the sub-paths from node  to nodes  and  in  and , hence the reachable pairs  and  are not learned in these two cases\n(the corresponding entries  and  in Figure 2(c)  ###reference_sf3### and Figure 2(d)  ###reference_sf4### are dark), even though that from these two datasets\nthe model can learn that  reaches ,  reaches ,  reaches , and  reaches  separately;\n(ii) there is a path \u201d0 9 0 2 3 9\u201d in , but the reachable pair  is not learned (the corresponding entry  in Figure 2(d)  ###reference_sf4### is dark red); and\n(iii) none of these matrices learn the reachable pairs of  with , since nodes  never appear as a non-source node in a path.\nFrom Eq.(9  ###reference_###), we know that the probability vector for predicting the next node is given as\n, where  represents the current node, and  represents the target node.\nThis provides an intuitive explanation\nfor why \nlearns the observed adjacency matrix, while \nlearns the observed reachability matrix.\nThe mechanism utilizes\n on the current node  to provide information about the next node\nit connects to,\nwhereas \nis used on the target node to provide information on which nodes can reach the target node.\nThe softmax operation  resembles the procedure in Algorithm 1  ###reference_###:\nit predicts the next node  such that both  is high (corresponding to ) and\n is high (corresponding to ).\nIn summary, our theoretical analysis\ndemonstrates that a simplified one-layer, one-head auto-regressive\nTransformer (with perfect attention) can effectively learn crucial\nadjacency and reachability information from the training data through gradient descent training.\nMoreover, it can utilize\nthis learned information\nto predict the next node akin to the decision-making process of a human algorithm designer in similar scenarios.\nThis suggests that,\nwhen\nconfronted with the path-finding or more general planning task with a given goal,\nthe Transformer learns the\nstructural information to associate the next step\nwith both the current step and the\ngoal,\nenabling it to generate the subsequent task step.\nNevertheless, the Transformer\u2019s\nlimitation\nin learning only the observed reachability matrix,\nwithout deducing\nthe complete reachability matrix,\nhints at potential constraints on the goal-oriented information\nit can acquire.\nThis limitation may result in the Transformer failing to grasp novel reachability relationships derived from the transitivity of reachability relations.\nIn the next section, we will further\nverify\nthe effectiveness of the Transformer\nin the path-finding task through extensive empirical evaluations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Empirical Evaluations: Peeking into a Trained Transformer",
            "text": "We conduct extensive experiments on the path-finding task using the general Transformer architecture as described in Section 2.1 ###reference_###. The experiments include tests on the overall accuracy of the Transformer model for the path-finding task. In this section, we present these empirical evaluation results, which show that the results derived from our theoretical analysis in Section 4 ###reference_### can be carried over to the general Transformer architecture."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Datasets",
            "text": ""
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 Graphs",
            "text": "The graph is generated randomly on two parameters: the number of nodes, and the probability of edge. Given these two parameters, we generate a DAG with nodes as follows: for any, there is an edge with probability, and the randomness for different edges are independent."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2 Training Data and Test Data",
            "text": "Given the DAG, we first find all the possible reachable pairs (i.e., and there exists at least one path that starts at and ends at). Then these reachable pairs are separated into the training set (w.p. 0.5) and the test set (w.p. 0.5), but if edge , we always put in the training set. For a reachable pair in the training set, we generate random paths that start at and end at, and put these paths into the training dataset. When generating the random path, at each current node , we find all the possible such that and (i.e., there is an edge, and could also reach the target), and uniformly choose a random one from them. Moreover, we always put the one-edge path \u201c n\u201d in the training dataset for each , to guarantee that all edges appear at least once in the training data."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Accuracy on Test Dataset",
            "text": "We train Transformer models on the aforementioned training dataset and subsequently evaluate performance of these models using the pairs in the test dataset. For test pair, the correctness of a model\u2019s output is determined based on its validity in terms of syntax and whether it corresponds to a valid path from a to b. In our experiments, we employ Transformer models with an embedding size of n. We conduct tests using various configurations, ranging from 1-layer and 1-head to 6-layer and 6-head, while considering different graph sizes, with number of nodes n ranging from 100 to 500. The accuracy results on all these tests are presented in Figure 3. From these results, we make the following observation: When comparing the five figures, we note that the accuracy tends to decrease as the number of nodes increases. For n=100 and n=200, the accuracy consistently remains above \u03b5. However for n=300, the accuracy drops to a range between \u03b41 and \u03b42, and for n=400 and n=500, the accuracy further declines to a range between \u03b31 and \u03b32 in most cases. When examining at each row, we observe that the accuracy remains relatively stable even as the number of attention heads increases. Upon examining each column, we observe that when the embedding size is sufficiently large in comparison to the graph size (e.g., n=d), the accuracy remains relatively stable as the number of layers increases. Conversely, when the embedding size is small comparing to the graph size (e.g., n<d), the accuracy shows a slight improvement as the number of layers increases. Specifically, the accuracy with 3 layers tends to be slightly higher than the accuracy achieved with 1 and 2 layers. The above observations suggest that both the embedding size and the number of layers have an impact on the model\u2019s accuracy, as they influence the number of parameters in the model. On the one hand, when the embedding size is sufficiently large compared to the graph size, even 1-layer 1-head models would perform well. This coincides with our theoretical analysis in the previous section, which shows that when the embedding size equals to the graph size, the 1-layer and 1-head structure is enough to predict the next nodes accurately. On the other hand, our empirical results show that when the embedding size is small comparing to the graph size, the Transformer architecture may need more parameters to increase the accuracy, which could be achieved by increasing the number of layers in the model. However, increasing the number of layers does not efficiently increase the accuracy a lot, e.g., the accuracy for Transformer with 3 layers is almost the same as the accuracy for Transformer with 6 layers."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Attention",
            "text": "We now look inside the Transformer models, and try to find more evidence that our theoretical analysis reflects the reality. In our theoretical analysis, we assume that the attention is fixed on the target node. Is it true for the Transformer models learned from read data? The corresponding results are shown in Figure 4. These results are obtained by looking into the attention mechanism of the five 1-layer and 1-head Transformer models, and showing the average (taking on the test dataset) matrix of, of which the row represents the attention vector for predicting the token. Note that the second column in these figures represents the attention weights on the second token, which corresponding to the target node in our test data. We can see that, when predicting the next tokens, almost all the attention weights are concentrated on this column, especially for those models with higher accuracy (Figure 4(a) for and Figure 4(b) for). This demonstrates that indeed the Transformer model learns the correct attention for the path-finding task, and our assumption on the attention for the theoretical analysis is reasonable."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Adjacency Matrix",
            "text": "Our analysis in Section 4 shows that the observed adjacency matrix and the observed reachability matrix are stored in the feed-forward layer and the attention layer. We now verify this on the general 1-layer and 1-head Transformer model that includes all the Transformer mechanisms, such as attention weights, non-linear transformation, token and position embedding. Note that in the Transformer layer, the output can be written as. Also noting that we have verified that the attention is concentrated at the second token. Then we let, representing the token embedding of the target node, and, representing the token embedding of the current node. Then we know that it is straightforward that contains the information of the current node, and contains the information of the target node. As for, we choose to use its linear approximation as. As shown by Table 1 (which takes average over all possible\u2019s and\u2019s), this is a good approximation. Then we can treat as the information of the target node, and as the information of the current node. In this subsection, we want to verify that the adjacency matrix is stored in the feed-forward layer, i.e., in the sum of two terms. Let be the matrix whose -th row is, where represents the one-hot column vector for node (with dimension ). Note that in the simplified Transformer model of Section 4, is the same as matrix. The results of are shown in Figure 5. As we can see, in Figure 5(a), the matrix and the adjacency matrix are highly aligned: almost all the large entries in the matrix correspond to real edges, and almost all real edges correspond to large entries in the matrix. This high accuracy is because the embedding size is higher than the number of nodes. If the embedding size is lower than the graph size (Figures 5(b), 5(c), 5(d), and 5(e)), we inevitably lose some accuracy when approximating the adjacency matrix by the product of matrices with rank smaller than the graph size, let alone the non-linear layers\u2019 influence. Even so, there is still high relevance between and the adjacency matrix: most real edges correspond to large entries in the matrix. In Figure 5(f), we show the gap between the average weight corresponds to edges (i.e., the average of\u2019s with and) and the average weight corresponds to non-edges (i.e., the average of\u2019s with and). We can see that in all these five graphs, their gaps keep increasing until convergence, suggesting that weights between edges and non-edges are more easily separated as the learning process proceeds. Moreover, with a lower number of nodes, the gap is higher. We believe this is because when the embedding size is fixed, one can approximate the adjacency matrix better for smaller graphs."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Summary of the Empirical Results",
            "text": "In summary, our extensive empirical evaluation leads to the following conclusions about the auto-regressive Transformer model in achieving the path-finding task:\n(a) With large enough embedding size, the model can achieve high accuracy in general; (c) The model may have limitations and fail to learn high-order reachability relations through transitivity, and thus fail to generate paths derived from high-order reachability, as human would expect."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Path-planning in Blocksworld",
            "text": "To further validate the theoretical results in Section 4 and the practicability of the proposed path-finding task, we consider Blocksworld benchmark. Blocksworld is a scenario consisting of a set of blocks identified by different colors. The blocks are either placed on table or on top of another block and the task is to plan a block manipulation from the given state to the target state. We formulate Blocksworld as a path-finding task. Here we construct a graph for the case with blocks, where each node represents a state of the blocks. For example, node refers to the state that \u201cthe red block is on top of the blue block, the blue block is on top of the orange block, the orange block is on top of the yellow block, and the yellow block is on the table\u201d. is a directed graph with nodes, and the adjacency matrix of is presented in Figure 7(a). Specifically, the states corresponding to the first nodes are the states where the four blocks are stacked one on top of another in a single stack, and thus each of these nodes only has one out-neighbor corresponding to removing the top block and putting it on the table. The last node refers to the state where all blocks are on the table, so it has 12 out-neighbors, corresponding to the states where three blocks are on the table and the fourth block is on top of one of the three blocks. In the original Blocksworld task, the answer is a sequence of actions, which is equivalent to the notion of edges in . We reformulated it to let the model output a path from the given state to the target state, only consisting of the nodes. This can be seen as a simplified version and a pure planning task. We randomly select of all node pairs for training and the rest for testing and generate the training data in the same format as introduced in Section 5. We mainly use Transformers with layer and head for the convenience of visualization."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "LLMs for Planning",
            "text": "Several recent studies have empirically evaluated the planning abilities of large language models (LLMs). For instance, CogEval has introduced a variety of planning tasks set in mazes and graphs, ultimately finding no evidence to suggest LLMs grasp the intricacies of the maps or the planning tasks themselves [MHVF+23  ###reference_bx15###]. Similarly, another study explored the Blocksworld game, a planning challenge where humans typically achieve success rates above , in stark contrast to GPT-3\u2019s mere  success rate [VMSK24  ###reference_bx21###]. Our paper proposes a novel approach by formulating a class of planning problems as path finding on graphs, applying this model to the blocksworld game and uncovering significant insights, as detailed in Section 6  ###reference_###.\nDespite these seemingly negative evaluations, LLMs have shown remarkable aptitude in executing real-world planning tasks, creating the field of autonomous agents [WMF+24  ###reference_bx23###].\nCertain applications of autonomous agents feature explicit graphs.\nIn the tool agent HuggingGPT [SST+23  ###reference_bx18###], LLMs are deployed to trigger a sequence of external APIs in response to user requests. Here, APIs are conceptualized as graph nodes, with their interrelations represented as edges, and the selection process akin to identifying a path or subgraph that aligns with user demands.\nThis scenario is an extension of the settings discussed in this paper, where the graph is text-attributed and the objective function is evaluated through textual analysis.\nThe application of graph search techniques has been shown to enhance the performance of tool agents significantly [LLG+23  ###reference_bx11###, LPY+24  ###reference_bx12###].\nThe math agent AlphaGeometry utilizes LLMs to solve geometry problems [TWL+24  ###reference_bx19###].\nBy treating lemmas as nodes and their interdependencies as edges, the process of finding a proof of a theorem is analogous to finding a path to the theorem node in the above graph formed by possible lemma nodes and their interdependency edges.\nHowever, [TWL+24  ###reference_bx19###] focuses on using LLM to generate auxiliary constructions, and the reasoning tasks are done by a non-LLM engine. This is very different to our research.\nThere are no explicit graphs in other agents, such as game agents [WXJ+23  ###reference_bx24###], embodied agents [HAPM22  ###reference_bx10###], and code agents [SCG+24  ###reference_bx17###]. The core strategy in these domains is to employ verbal reinforcement learning within LLMs. It is noteworthy that any dynamic programming problem characterized by deterministic state transitions can be reformulated as a shortest path problem on a graph, with states and transitions represented as nodes and edges, respectively. As a result, the area of autonomous agents is closely related to the path-planning task investigated in this paper."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "LLM for Graphs",
            "text": "GPT4Graph [GDL23  ###reference_bx8###] and NLGraph [WFH+23  ###reference_bx22###] have developed extensive frameworks for assessing LLMs in the context of graph tasks. These frameworks encompass a broad spectrum of challenges, including classic graph problems (e.g., connectivity, cycle detection, and topological sorting), graph neural network (GNN) tasks (e.g., node and graph classification), and semantic graph question answering (e.g., knowledge graph inquiries). They also explore various input formats, such as adjacency lists, edge lists, GML, and GraphML, alongside innovative prompting techniques like few-shot, role prompting, chain-of-thought, and algorithmic prompting (e.g., stating \u201cwe are using DFS\u201d).\nThese studies demonstrate that LLMs possess basic graph processing capabilities, and the choice of prompts and formats influence performance significantly.\nYet, they also reveal the models\u2019 susceptibility to spurious correlations within graphs.\nGPT-4, for instance, only achieves around  accuracy on shortest path tasks, even when utilizing complex prompts.\nTo our knowledge, this paper presents the first theoretical analysis that identifies and explains the spurious correlations learned by LLMs (Theorem 3  ###reference_orem3###), supporting the negative outcomes reported in these studies.\nThere has also been a surge in efforts aiming at bolstering LLMs\u2019 performance on graph tasks. Innovations like GraphGPT [TYW+23  ###reference_bx20###] and GraphLLM [CZW+23  ###reference_bx6###], which incorporate an additional GNN encoder, have shown notable improvements across the aforementioned graph tasks. GraphInstruct [LSH+24  ###reference_bx13###] seeks to enhance LLMs\u2019 capabilities using pure LLM approaches. This involves meticulously documenting the steps of classical algorithms (e.g., BFS and DFS) and fine-tuning LLMs to learn these graph algorithms. This method of procedural supervision has extended the capacity of LLMs in graph tasks from the complexity class  to P/poly [FZG+23  ###reference_bx7###].\nHowever, while this approach has yielded performance improvements in simpler tasks such as topological sorting and connectivity, it has proven less effective for more complex challenges, like finding Hamiltonian Paths."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Algorithm Simulation with Transformers",
            "text": "Recent theoretical investigations have shed light on the capability of Transformer to simulate algorithms, a topic that has garnered considerable interest. This discussion begins with discrete algorithms. From a circuit complexity standpoint, Transformer models are likened to parallel circuits characterized by polynomial width and constant depth, which places them within the  complexity class. It is also noticed that . On the other hand, despite their impressive expressiveness, Transformer is theoretically incapable of addressing a range of P-complete problems, including the testing of Context-Free Grammar Membership [MS23  ###reference_bx16###]. However, the advent of chain-of-thought prompting has enabled Transformer to sequentially simulate algorithms, thereby equipping them to tackle P-complete problems in domains such as arithmetic and decision-making [FZG+23  ###reference_bx7###]. The exploration extends to continuous algorithms, where it has been demonstrated that Transformer can approximate functions such as matrix inversion, Stochastic Gradient Descent (SGD), and power iterations [GRS+23  ###reference_bx9###].\nOur study specifically applies GPT models to simulate path-finding algorithms, presenting evidence that their expressiveness is sufficient for such tasks (Theorem 2  ###reference_orem2###).\nNevertheless, the usage of auto-regressive loss and gradient descent introduces certain limitations (Theorem 3  ###reference_orem3###), which have not been studied in existing works."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Mechansims of LLMs",
            "text": "LLMs have demonstrated capabilities that exceed the theoretically predicted lower bounds of expressiveness. To demystify this paradox, numerous studies have employed experimental methodologies akin to those used in the physical and biological sciences. Their aim is to decode the mechanisms of LLMs. The foundational strategy is to generate controlled synthetic datasets to analyze how language models (not necessarily the LLMs) complete various tasks. Standard methods for this analysis include visualizing attention patterns to examine computational properties (such as locality and time invariance) and employing linear probing on the hidden states to determine the extent of learning. Given that the data is synthetic and the ground-truth mappings are generally known, it becomes feasible to isolate the influence of various factors (e.g., prompting strategies, chain-of-thought reasoning, and data formatting). For example, a dataset designed for learning group operations, as detailed in [ZBB+22  ###reference_bx25###], facilitates the exploration of how pretraining, data composition, and neural architecture influence reasoning tasks within LLMs. Similarly, the generation of synthetic context-free grammar (CFG) data, as described in [AZL23a  ###reference_bx1###], enables training GPT-2 models, uncovering their capacity to learn dynamic programming algorithms for parsing CFGs. Synthetic datasets focusing on biographical knowledge, presented in [AZL23b  ###reference_bx2###, AZL23c  ###reference_bx3###, AZL24  ###reference_bx4###], probe into the mechanisms of knowledge storage, retrieval, manipulation, and the implications of scaling laws. Moreover, the work in [LSL+23  ###reference_bx14###] introduces synthetic datasets aimed at understanding how smaller LLMs tackle basic arithmetic operations, like addition, and examines the effects of few-shot prompting, pretraining, and model scaling [LSL+23  ###reference_bx14###]. This paper builds upon these investigations by conducting controlled experiments with a path planning dataset, thereby shedding light on the complexities and challenges of planning in LLMs."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Discussion and Conclusion",
            "text": "In this paper, we present our investigation on how the Transformer architecture executes the path-finding task, which abstracts a number of planning tasks one may encounter in practical scenarios.\nWe first show that the Transformer has the expressive power of path finding by manually construct a 1-layer 1-head Transformer that encodes the adjacency matrix and the reachability matrix\nwith its weight matrices.\nNext we provide a theoretical analysis demonstrating that the Transformer can indeed learn the adjacency matrix and the observed reachability matrix (a limited form of reachability) from the path\ntraining data through the common gradient descent mechanism on cross-entropy loss function.\nWe then verify our theoretical analysis through comprehensive experiments, both on synthetical network data and on a concrete Blocksworld planning benchmark.\nOur empirical findings\nsupport our theoretical analysis and demonstrate that the Transformer can achieve high accuracy in many cases, it can concentrate its attention correctly to the target node,\nand it extracts adjacency matrix and observed reachability matrix as predicted by our theoretical analysis.\nOur theoretical analysis further points out a limitation of the Transformer \u2014 it cannot learn well the transitive reachability, and this point is validated by our experimental results.\nOur study is the first to combine theoretical analysis with empirical evaluation on such planning tasks.\nIn particular, we are the first to apply theoretical analysis using the gradient descent learning method of the Transformer on a concrete task and provide theoretical interpretation, which is validated by\nour experimental results.\nMore importantly, our theoretical analysis is able to unveil a potential limitation of the Transformer in completing the path-finding task, which guides us to verify that indeed this limitation occurs in the\nexperiments.\nThis demonstrates the power of theoretical analysis as a guidance for understanding practical Transformers and LLMs.\nAlthough the path-finding task is a simple task on a network and our investigation is still the first attempt\nto understand Transformer\u2019s capability on this task,\nour findings may already provide some insights and implications to path finding and planning in general.\nWe can see that, due to the autoregressive nature of learning, the Transformer cannot execute path finding by a simple trial-and-error search such as depth-first or breadth-first search, and thus\nit has to plan ahead well in generating the next node on the path correctly.\nWe demonstrate that the Transformer can adjust its effort accordingly by concentrating its attention to the target, and generating the next node that is both adjacent to the current node and reachable\nto the target, mimicking the human intelligence.\nIt suggests that for many planning tasks in general, the Transformer architecture is adjusting its learning by balancing the consideration between the immediate continuation of the next step and\nits final goal of reaching the target.\nOur investigation opens up many possible future directions, and we list several of them below.\n(a) Extend our study to hyper-graphs and hyper-paths, where a hyper-edge models the situation in which several preconditions need to be satisfied together in order to carry out the next step, which\nis common in task planning and mathematical proofs.\n(b) Further understand the limitation of the Transformer on path finding: the current inability of transitive reachability may be due to the limited expressive power of the path language, and it may also\nsuggest the fundamental difference between Transformer\u2019s interpretation and human\u2019s interpretation on path finding.\nThis requires further study, perhaps by enriching the languages, or by an investigation on the deployed LLMs such as GPT4.\nIt can also stimulates new improvement to the Transformer architecture to achieve better performance.\n(c) Study the connection between the abstract path-finding task and some concrete planning tasks (e.g. block moving in Blocksworld, etc.), to see if Transformer has any capability\nof abstracting a concrete task to the more abstract path-finding task, or find if what Transformer does for concrete planning tasks has any commonalities with the abstract path-planning task.\n(d) Study deployed LLMs or fine-tune LLMs to connect their planning capabilities with our path-finding investigation.\nWe are planning to deepen our investigation on the above fronts.\nWe also hope that our work could inspire more people to study LLMs with combined theoretical and empirical analysis, with the ultimate goal of understanding and explaining the power of LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2405.09220v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "7.1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "3",
            "4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6",
            "6",
            "6.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6",
            "6",
            "6.1"
        ]
    },
    "research_context": {
        "paper_id": "2405.09220v2",
        "paper_title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models",
        "research_background": "**Motivation:**\n\nLarge language models (LLMs), such as ChatGPT, have demonstrated exceptional capabilities across various intelligent tasks, hinting at the potential achievement of artificial general intelligence (AGI). Despite their powerful outputs, the underlying mechanisms enabling these capabilities, particularly from a theoretical standpoint, remain largely unexplained. This paper targets a fundamental aspect of intelligence\u2014planning\u2014to explore how LLMs can transform the next word prediction in a sequence into high-level intelligent tasks such as planning. By doing so, the study aims to uncover deeper insights into the innate mechanisms of LLMs that could explain their broad capabilities.\n\n**Research Problem:**\n\nThe core research problem this paper addresses is understanding how the Transformer-based autoregressive learning architecture of LLMs achieves planning tasks. Specifically, it investigates the planning capability by examining the ability of LLMs to solve path-finding tasks in networks. This task requires the model to generate a valid path from a starting node to a target node using a sequence of nodes, a challenge that reflects real-world planning problems. The study seeks both empirical evidence and theoretical interpretations of how and why LLMs can or cannot successfully complete these planning tasks.\n\n**Relevant Prior Work:**\n\n1. **Empirical Studies on LLM Planning Capabilities:** Existing research has primarily been empirical, assessing the ability of LLMs to complete planning tasks with some success. However, these studies have not fully explained why these models perform well or poorly on specific aspects of planning.\n2. **Benchmarks and Tasks Related to Planning:**\n   - **Blocksworld Game [VMSK24]:** Used as a benchmark for evaluating LLM planning capability, viewed as a path-finding problem from initial to final block states.\n   - **HuggingGPT [SST+23]:** Demonstrates scheduling API calls akin to finding a path in an API call graph, another type of planning-related task.\n3. **Path-Finding Tasks:** These tasks involve navigating a graph to find a valid path between nodes, considering immediate adjacency and overall reachability\u2014key components essential in planning and intelligent task management across various applications.\n\n**Conclusion:**\n\nThis paper proposes Project ALPINE to bridge the gap between empirical results and theoretical understanding by not only evaluating LLM performance in planning tasks but also dissecting the underlying mechanisms enabling this capability. Through the study of path-finding tasks, it aims to provide vital insights and potential pathways to explain broader intelligent behaviors exhibited by LLMs.",
        "methodology": "I'm sorry, but from the given text, it appears that the section you have provided is focused on the notation and symbol conventions used in the paper, rather than the methodology itself. To describe the proposed method or model, including key components and innovations, I would need more specific details about the method or model described in the paper. This would typically include descriptions of the algorithm or procedure, key steps, components involved, and any novel contributions or improvements introduced.\n\nIf you can provide more detailed information from the methodology section or other parts of the paper, I'd be happy to help you describe the proposed method or model accurately.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n**Datasets:**\n- The experiments are conducted on a path-finding task dataset. The precise nature of the dataset (e.g., its size, the format of the graphs, or the specific tasks) isn't detailed here, but it likely involves various graph structures where the model must predict paths.\n\n**Baselines:**\n- The general Transformer architecture serves as the primary model. No specific mention of additional baseline models is given, implying the primary focus might be comparing the Transformer model performance against the theoretical expectations.\n\n**Evaluation Metrics:**\n- Overall accuracy on the path-finding task.\n- Measurements of how well the model learns attention.\n- Evaluation of how well the model understands the structural information, specifically the adjacency and reachability matrices.\n\n**Main Experimental Results:**\n- The empirical evaluation results support the theoretical analysis presented earlier in the paper (Section 4). Specifically:\n  - The Transformer model shows good performance on the path-finding task.\n  - The model effectively learns the attention mechanisms required for this task.\n  - It also grasps the structural information related to adjacency and reachability matrices well.\n  \nBy confirming that the theoretical assertions hold in practice, the study demonstrates the planning capability of the Transformer model in path-finding tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate whether Transformer models can efficiently learn the attention and structural information about the adjacency and reachability matrices for the path-finding task.",
            "experiment_process": "The path-finding task experiments involved randomly generating a DAG with nodes and edges, splitting reachable pairs into training and test sets, and training Transformer models with varying configurations (1-layer 1-head to 6-layer 6-head) on this data. The accuracy of the models was measured based on the validity of generated paths from the source to target nodes.",
            "result_discussion": "Accuracy decreased as the number of nodes increased. Embedding size and the number of layers both affect accuracy, with a large enough embedding size helping to maintain high accuracy even with fewer layers. These results coincide with the theoretical analysis, suggesting that larger models are more accurate but increasing layers beyond three provides marginal benefits.",
            "ablation_id": "2405.09220v2.No1"
        },
        {
            "research_objective": "To validate if the Transformer models learn the assumed attention, concentrating on the target node for path-finding tasks.",
            "experiment_process": "Evaluated the attention mechanism inside five 1-layer and 1-head Transformer models by observing the attention weights on the target node for the test dataset, visualized in attention vectors.",
            "result_discussion": "The attention weights were concentrated on the target node's column, especially in higher accuracy models, thus validating the hypothesis that Transformers can learn the correct attention for path-finding.",
            "ablation_id": "2405.09220v2.No2"
        },
        {
            "research_objective": "To verify if the observed adjacency matrix is stored in the feed-forward layer of the Transformer.",
            "experiment_process": "Analysis was performed on the 1-layer and 1-head Transformer model by representing token embeddings of current and target nodes and their linear approximations. The alignment of the adjacency matrix with these representations was assessed.",
            "result_discussion": "The adjacency matrix and the product of embeddings were highly aligned, especially when the embedding size was higher than the number of nodes. This alignment diminished with lower embedding sizes but still indicated significant relevance between real edges and the matrix entries.",
            "ablation_id": "2405.09220v2.No3"
        },
        {
            "research_objective": "To validate if the observed reachability matrix is stored in the attention layer of the Transformer.",
            "experiment_process": "Compared average weights of different sets within graphs (observed, real observed, and non-edges) for models with varying numbers of nodes, examining how well the Transformer learns the reachability matrix.",
            "result_discussion": "The Transformer learned the observed reachability well but struggled with high-degree source-target pairs requiring path concatenation, as indicated by much lower accuracy for such pairs. This aligns with theoretical analysis suggesting the model's limitations in learning reachability through transitivity.",
            "ablation_id": "2405.09220v2.No4"
        },
        {
            "research_objective": "To assess the applicability of theoretical findings on Transformer planning capabilities using a real-world benchmark, Blocksworld, reformulated as a path-finding task.",
            "experiment_process": "A graph representing states of blocks was constructed, with node pairs randomly split into training and test sets. Transformer models with various embedding sizes were trained and evaluated on accuracy, attention, adjacency, and reachability matrices.",
            "result_discussion": "All models reached high accuracy eventually, with attention directed almost entirely on target nodes and adjacency matrices aligning well with ground truth. Reachability matrices also demonstrated increased weight separation for observed edges, confirming theoretical findings.",
            "ablation_id": "2405.09220v2.No5"
        }
    ]
}