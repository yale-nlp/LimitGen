{
    "title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
    "abstract": "Despite the popularity of Large Language Models (LLMs), crafting specific prompts for LLMs to perform particular tasks remains challenging. Users often engage in multiple conversational turns with an LLM-based agent to accomplish their intended task. Recent studies have demonstrated that linguistic feedback, in the form of self-reflections generated by the model, can work as reinforcement during these conversations, thus enabling quicker convergence to the desired outcome. Motivated by these findings, we introduce MetaReflection, a novel technique that learns general prompt instructions for a specific domain from individual self-reflections gathered during a training phase. We evaluate our technique in two domains: Infrastructure as Code (IaC) vulnerability detection and question-answering (QA) using ReAct and CoT. Our results demonstrate a notable improvement, with MetaReflection outperforming\nGPT-4 by 16.82% (IaC), 31.33% (CoT), and 15.42% (ReAct), underscoring the potential of MetaReflection as a viable method for enhancing the efficiency of LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs), such as GPT-4 OpenAI (2023  ###reference_b7###), have gained significant popularity in recent years due to their ability to generate human-like text and solve complex tasks across various domains. To leverage these models, users typically craft prompts with instructions that are tailored to a specific task. These prompts, however, are not just limited to explicit instructions, they can be enriched with additional context drawn from a variety of sources such as documentations, examples, or relevant inputs gathered from a range of other tools. This allows for a more comprehensive and nuanced understanding of the task at hand, thereby aiding the model in generating more accurate and relevant outputs.\nHowever, the process of crafting specific prompts, designed to guide LLMs in performing particular tasks, is not fairly straightforward.\nIn fact, prompt engineers often find themselves investing a substantial amount of time in iterating and refining their prompts to optimize them for a specific task Zamfirescu-Pereira et al. (2023  ###reference_b24###); Parnin et al. (2023  ###reference_b9###). This iterative, time-consuming interaction often leads to delays and inefficiencies, posing a substantial barrier to the seamless utilization of LLMs.\nIn practice, it is common for users to engage in multiple conversational turns with an LLM-based agent, providing feedback to correct the agent\u2019s trajectory, in order to accomplish their intended task.\nRecent works Shinn et al. (2023  ###reference_b13###) have showed that the performance of language\nagents can be improved using verbal reinforcement learning during\nmultiple conversational turns.\nThe language agent is provided feedback at the end of a failing trajectory, and\nasked to reflect on its mistakes and the reflective text is stored in episodic\nmemory to improve future trajectories on the same task.\nFor instance, Figure 1  ###reference_### shows a ReAct Yao et al. (2023b  ###reference_b22###) based\nlanguage agent failing to complete a question-answering task from the\nHotpotQA dataset Yang et al. (2018  ###reference_b20###) because the agent got stuck in a loop\nlooking for the very common word \u201cgoal\" in a football related page.\nIn the top two boxes on the left of Figure 1  ###reference_### (labelled Task and\nTrajectory), we show the trajectory of the agent attempting to solve the task.\nThe box Self-Reflection from Figure 1  ###reference_### shows the reflection text\nfor the above failing trajectory, pointing out the error in repeatedly searching\nfor a common term.\nWith this self-reflection as additional episodic memory, the next trajectory\nsucceeds in finding the right answer.\nWhile self-reflection can significantly improve a language agent\u2019s performance,\nit is a online reinforcement process that depends on the\navailability of performing multiple turns with a feedback mechanism.\nIn this paper, we introduce MetaReflection, an approach to learning\nverbal instructions for language agents using past self-reflections.\nIntuitively, during a training phase, self-reflections from different tasks are\ngathered and generalized into a verbal \u2018meta-reflection\u2019 that takes the form of\nadditional instructions to the language agent.\nIn Figure 1  ###reference_###, the self-reflection from the failing trajectory and\nother self-reflections over the training data are generalized into instructions\nthat suggest that the language agent search for related terms or change the\nsearch strategy.\nUnlike the self-reflections, the meta-reflection instructions are not specific\nto any particular instance of task.\nIn the online phase, the language agent is able to use these general\ninstructions to search for the right term \u201cpilot\" (instead of \u201cairline pilot\")\nand answer the question correctly, which it was not able to do previously.\nNote that there is no feedback mechanism during the inference\u2014intuitively, we\nare leveraging a feedback mechanism available during the training phase to\nimprove the language agent performance even in the absence of the feedback\nmechanism.\nWe evaluate MetaReflection across two distinct domains: vulnerability threat detection in a new Infrastructure-as-Code (IaC) dataset111https://aka.ms/MetaReflectionDataset  ###reference_### and retrieval and reasoning using the HotpotQA dataset. The IaC dataset is used to evaluate the performance of our solution in detecting vulnerabilities in cloud infrastructure configuration files. These files, written in a low-resource language known as HCL, manage computing data center infrastructure and declare resources such as virtual machines, virtual networks, and data stores. The task is to identify potential security vulnerabilities in these files, a challenge due to the complexity of configurations and the diversity of resources being handled across multiple infrastructure providers. Our technique demonstrated a  overall improvement in accuracy across all policies when compared to the baseline GPT-4 model.\nThe second dataset, HotpotQA, is utilized to evaluate retrieval and reasoning capabilities of a model. This open-domain factual question answering dataset comprises K question-answer pairs. In our experiments, MetaReflection brought consistent gains in all configurations that we tested, with up to  improvement in accuracy against the baseline.\nTo summarize, we make the following contributions:\nWe present MetaReflection, a technique for learning verbal instructions for language agents using past self-reflections (Section 4  ###reference_###);\nWe conducted an extensive evaluation of the MetaReflection technique across two distinct domains: vulnerability threat detection and causal reasoning demonstrating significant improvements in both domains over the baselines (Section 3  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "With the increasing ubiquity of black-box Large Language Models OpenAI (2023  ###reference_b7###); Anil et al. (2023  ###reference_b1###); Brown et al. (2020  ###reference_b3###); Bai et al. (2022  ###reference_b2###), there has been a growing interest in the community to develop strategies that can maximize the model\u2019s performance on a downstream task. These techniques may involve guiding an LLM to arrive at the correct answer Wei et al. (2023  ###reference_b16###); Zheng et al. (2023  ###reference_b28###), creating multi-step workflows and agents to achieve specific tasks Wu et al. (2023  ###reference_b18###), equipping the LLMs with tools Yao et al. (2023b  ###reference_b22###); Qin et al. (2023  ###reference_b12###), output selection Yao et al. (2023a  ###reference_b21###); Poesia et al. (2022  ###reference_b10###), etc.\nIn spite of the recent advancement of these strategies, careful prompt engineering has proven to be effective in bringing complementary gains over these techniques  White et al. (2023  ###reference_b17###).\nComing up with best instructions for a task can be a very time consuming effort. This has motivated efforts to come up with the \u2018right\u2019 prompt automatically.\nAutomated Prompt Engineering (APE) Zhou et al. (2023  ###reference_b29###) poses instruction generation as a synthesis problem and proposes techniques to effectively search over the space of instruction candidates generated by an LLM. The learned prompt can then be used during inference time in isolation.\nGiven the potentially infinite space of instructions, recent works have studied the problem of \u2018guided\u2019 prompt search instead. To this end, OPRO Yang et al. (2023  ###reference_b19###) proposes a prompt \u2018optimization\u2019 technique where they come up with a prompting strategy that can enable a model to perform prompt mutations to optimize a numerical \u2018metric\u2019 like eval set performance. HtT  Zhu et al. (2023  ###reference_b30###) proposes a method to effectively learn \u2018rules\u2019 using task plan generated by a model using techniques like CoT and search through this rule library.\nOuyang and Li present AutoPlan Ouyang and Li (2023  ###reference_b8###), an approach to generate planning instructions for interactive decision-making tasks. These instructions help the model to better plan to use external tools to add grounding to the language agents. In our work, we generate a set of broad instructions that can be used as grounding, to improve the quality of LLM responses, orthogonal to the use of external tools.\nProTeGi Pryzant et al. (2023  ###reference_b11###) and PE2 Ye et al. (2023  ###reference_b23###) also leverage verbal feedback to generate and/or evolve task description prompts. They start with an initial prompt, evaluate it on a batch of examples from a training set and use the failing examples to generate textual gradients to criticize the prompt. They subsequently use these gradients to produce multiple candidates per instructions and sample the best candidates at each iteration. In PE2 they additionally, maintain an optimization history to iterative improve the prompt. In contrast, in our work, we aim at developing instruction sets instead of task description. Besides, our technique leverages free-form self-reflections to navigate the search space of possible instructions instead of sampling from a large candidate pool - leading to a relatively less costly search.\nAn alternative approach to improve language agent predictions can be to adapt on-the-fly an initial human written instruction to a given input instance for a task. This can be done by fine-tuning a model that can generate the mutations to the initial instruction Zhang et al. (2022b  ###reference_b26###, 2023  ###reference_b27###, a  ###reference_b25###); Deng et al. (2022  ###reference_b4###) or by prompting a black-box LLM to come up with such mutations Sun et al. (2023  ###reference_b14###); Kim et al. (2023  ###reference_b6###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Evaluation",
            "text": "We evaluate the performance of MetaReflection on dataset from two different domains: vulnerability threat detection (IaC) and question answering (HotpotQA). For each agent setting, we adversarially sample subsets of the HotpotQA train split of K samples to create train and test sets. To perform adversarial sampling, we first identify samples where the base agent fails consistently in a given setting. On these failing examples we perform up to self-reflection trials to get the model to the right response. If the agent is not able to get to the correct response even after self-reflection, we discard these samples. This strategy ensures that we get a set of hard examples in which the agents fail most of the times to get to the right answer in a single try, while also making sure that we filter examples that may be noisy due to missing context, incorrect questions etc. To account for randomness and given our computational budget, we sample 40 and 80 examples for the ReAct train set and test set respectively. For CoT settings, we pick and example respectively. We reuse the CoT agent from Wei et al. (2023 ###reference_b16###) for the chain-of-thought experiments and use a re-implementation of Yao et al. (2023b ###reference_b22###) for the ReAct experiments. The ReAct agent is allowed at most Action steps after which the trajectory is automatically determined to be a failure. Similar to Section 3.1 ###reference_###, we evaluate HotpotQA configurations for:\n\n(a) MetaReflection with batch sizes, , and; and\n\n(b) GPT-4 and LLMInstruction as baselines.\n\nIn addition to this, we also evaluate variants of the agents powered by GPT-3.5-Turbo instead of GPT-4, while using GPT-4 for MetaReflection. A similar experiment on the IaC domain wasn\u2019t possible due to large context length of the Terraform modules. We find that the generated MetaReflection instruction consistently improved performance across different agent settings for HotpotQA. In Table 2 ###reference_###, we present results using GPT-4 for both the agents and MetaReflection. We observe that MetaReflection help us achieve gains up to for CoT (GT), for CoT (Distractor), and for ReAct, over the respective test sets. Interestingly, higher batch sizes almost always help, reinforcing the importance of batching as observed in related works Ouyang and Li (2023 ###reference_b8###); Ye et al. (2023 ###reference_b23###). In Table 3 ###reference_###, we report results when using GPT-3.5-Turbo to power the client agents. We see gains of up to gains for CoT (GT), for CoT (Distractor) and for the ReAct case. Here, we observe that batching doesn\u2019t strictly improve the performance. Examining the data qualitatively, this difference can be attributed to the nature of the instructions generated in the two settings. In general we observe that with a small batch size, MetaReflection produces a large amount of very specific instructions. On the contrary batching helps generalize these instructions into more widely applicable rules. GPT-4, being more powerful than GPT-3.5-Turbo, is able to better follow these abstract instructions, while specific instructions work better for GPT-3.5-Turbo.\n\nChain-of-thought\n\n(A) Provide direct and concise responses to the question, using precise language that matches the specificity and terminology of the question, including singular or plural forms and definite articles as needed.\n\n(B) If the context suggests multiple valid answers, choose the one that best matches the question\u2019s wording and the most direct information provided. \n\nReact\n\n(C) When a question asks for a specific detail such as a \u2019full name\u2019, ensure to find and provide that exact information. Don\u2019t make assumptions based on limited or incomplete information.\n\n(D) If you\u2019re not finding the desired information or stuck in a loop of looking up a keyword, consider changing the keyword and search strategy. The information might be located further down the page.\n\n(E) When a question involves comparison, such as \u2019who received more rewards\u2019, ensure to search for each entity individually, gather all necessary information, and then make a comparison based on the data found.\n\n(F) Be mindful of potential spelling errors or variations in the names of entities. If a search for a specific term doesn\u2019t yield results, consider possible alternative spellings or forms of the term. Consider an example question from Figure 6 ###reference_###. The question seeks information about the product made from Cassava and served with palm nut soup. The context presented within the CoT (Distractor) setting includes articles about Akpu and Fufu, both of which are quite similar, being made from Cassava paste. However, the key distinction lies in Fufu being served with palm nut soup, while Akpu is served with Esupi"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Vulnerability Detection in IaC Files",
            "text": "Infrastructure-as-Code (IaC) is a popular method of configuring cloud infrastructures, on platforms such as Azure and AWS, using a configuration coding language. These configuration files can declare resources such as virtual machines with specific capabilities, virtual networks and subnets, and data stores. IaC presents an alternative to the traditional ways of configuring cloud infrastructures, such as using a web-based graphical interface. There are numerous IaC platforms currently available for various cloud computing platforms. Here, we focus on Terraform, a leading IaC platform by Hashicorp, as well as Azure, the cloud computing platform by Microsoft. Related Terraform resource declarations are grouped together into Terraform modules which act as a basic, reusable configuration component. Cloud infrastructures are prone to security vulnerabilities such as open ports and exposed administrator accounts. Vulnerability detection via static analysis of IaC files is a hard problem due to the expressivity of the configuration language, the complexity of configurations and the diversity of the resources being handled across multiple infrastructure providers (e.g., Amazon AWS and Microsoft Azure). Further, Terraform uses a low-resource language - HashiCorp Configuration Language (HCL). Terrascan is a static analyzer for detecting security vulnerabilities in Terraform modules, and supports over security policies, including policies specific to Azure. The description and definition of a Terrascan policy checks if every Azure virtual network subnet is configured with a corresponding network security policy. Note that the Terrascan policy is syntactic, i.e., it is checking for a declaration of an azurerm_virtual_network with a field named subnet, and so on. Hence, Terrascan-like static analysis based vulnerability detection is fragile and prone to both false positives and false negatives due to being sensitive to syntax. The task at hand is to check if a given Terraform module violates a given Terrascan policy.\n\nDescription: Ensure that Azure Virtual Network subnet is configured with a Network Security Group\n\nDefinition: We collected Terraform modules by mining GitHub repositories for IaC code written in HCL. These repositories corresponded to a diverse range of applications including load balancers, machine learning operations managers, and domain-specific data-stores. For policies, we selected the most commonly violated Terrascan policies. Of the module-policy pairs, we eliminated a significant fraction of cases where the policies were not applicable to the module. For example, if the policy was for a specific resource type and the module did not contain declarations of that resource type, the pair was eliminated. After this process, we were left with module-policy pairs, for which we manually annotated whether the module violated the policy. Note that this ground-truth annotation was with respect to the description of Terrascan policy, not the definition\u2014that is, we use the intention behind the policy, not the letter of the definition. That is, we do not take the output of Terrascan as ground truth as it can be inaccurate, and instead manually examine if the policy (as per description) is violated. This data was then split into train and test sets in a ratio per policy, taking care to balance the vulnerable and non-vulnerable classes."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "MetaReflection",
            "text": "Algorithm 1  ###reference_### shows the\noutline of the MetaReflection process.\nAt its core, the algorithm works by starting with an empty set of instructions\nand iteratively improving the instructions using small training batches.\nBeing built upon the self-reflection technique, the MetaReflection procedure\nuses the same components at its core:\n\n\n(a) a client agent (i.e., an RL actor) that is based on a language model,\n\n(b) an evaluation or feedback agent that can provide feedback on the client\nagent\u2019s trajectory, and\n\n(c) a self-reflection agent that produces a verbal reinforcement given a RL\ntrajectory.\n\n\nAdditionally, we assume that the client agent can be parameterized by a set of\ninstructions in addition to the standard task description.\nIn our implementation and experiments, we use several different client agents\nbased on ReAct Yao et al. (2023b  ###reference_b22###),\nCoT Wei et al. (2023  ###reference_b16###), and a vanilla one-shot\nlanguage model.\nFor the feedback agent, we consider multiple variants based on the application\ndomain: a 0-1 boolean feedback agent and an exact match\nchecker.\nThe meta-reflection agent is designed to take as input a prior set of\ninstructions , a set of self-reflections ,\nand the training data, and will produce an updated set of instructions\n.\nFor the meta-reflection agent, we use a standard language model with a prompt\nthat instructs the model to observe the reflections, the training data, and\nproduce new non-case specific instructions.\nFurther, the prior instructions are also passed as input so that the output is a\ngeneralization of the prior instructions.\nIn our implementation, this meta-reflection and generalization are done in the\nsame prompt for efficiency.\nthen combined with previous instructions are also possible.\nAlternatively, new instructions can be generated first and then combined with existing ones.\nWe specify that the instructions need to take the form of a list.\nHence, the meta-reflection agent typically either\n\n\n(a) updates the list by adding a new item, or\n\n(b) combines one or more previous items with learnings from the\nself-reflections to produce a shorter list.\n\n\nFor example, one meta-reflection instruction learned during our HotpotQA experiments suggested including the profession when searching for a person to\nnarrow down results.\nIn a subsequent batch, the self-reflection agent produces a reflection that\nmentions adding search terms like release date when searching for\nmovies.\nThe meta-reflection agent may combine the previous instructions with the current\nself-reflections either by appending a new item to the list clarifying the\nstrategy to search for movies, or may generalize the previous item to something\nlike \u201cWhen searching for specific entities, use additional contextual\ninformation to augment the primary search terms with secondary keywords\ncorresponding to the characteristics of the entity\".\nIn each iteration, after the meta-reflection step, we validate the quality of\nthe new instructions.\nDue to sparse reward signals leading to poor self-reflections or\nover-generalization of the meta-reflection instructions, we may end up with\ninstructions that are of a poorer quality than the prior instructions.\nThe poorer instructions may also be due to general capricious, unpredictable\nnature of large language models.\nTherefore, we validate the new instructions by testing them on training data to\nensure that they perform better than the prior instructions.\nIdeally, we would do this validation over the full training data or a\nsubstantial held-out dataset.\nHowever, in our case,\nwe only validate on the current batch to balance\nquality of instructions and efficiency.\nAs an example, in the previous paragraph the meta-reflection step replaced\nthe specific instruction on how to search for persons with a more general\ninstruction on how to search for entities.\nHowever, it is possible that these general instructions are too vague\n(especially for smaller, less capable models) and the client agent is not able\nto apply them correctly to the case of searching for persons.\nIn such a case, we do not use the new updated instructions and revert back to\nthe prior instructions.\nIn practice, we use several other augmentations to the meta-reflection\nprocedure in Algorithm 1  ###reference_###.\nThese augmentations are not a core part of the technique, but instead\noptimizations that may help in specific cases and domains.\nThe first of these is to use certain parts of the full trajectory in addition to\nthe self-reflections during the meta-reflection step in\nline 10  ###reference_10###.\nFor example, if the client agent is a CoT agent, it may be helpful\nto append the inner thought steps from the trajectory to the self-reflections.\nAnother augmentation is to use multiple attempts at meta-reflection for each\nbatch.\nIf the validation step fails at line 11  ###reference_11###, instead of rejecting\nthe new instructions  altogether, we may rerun the loop with\nthe same batch, but this time initializing the client agent with\n instead of .\nThis process may be repeated multiple times till the validation step\nsucceeds\u2014in practice, we limit the repetition to  times.\nSimilarly, the whole algorithm can be repeated multiple times over all the\nbatches of the full training dataset.\nThat is, repeat the algorithm starting with the previous iterations results in\nline 7  ###reference_7### as long as the instructions improve at the end of the\nalgorithm (validated over the whole training set).\nWhile we have anecdotally seen improvements in accuracy of instructions by\nrepeating the whole algorithm in certain domains, we do not do this by default\nin our implementation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Large language models (LLMs) form a critical component in the development of AI-based systems. However, crafting prompt instructions can be a non-trivial task. In this paper, we have taken a significant step forward to improve this process by introducing MetaReflection. This innovative approach employs past self-reflections to learn instructions used to guide LLMs. In our experiments, we show that instructions learned using MetaReflection significantly improve the accuracy of GPT-4 predictions.\nWe believe that integrating LLMs with domain-specific insights, such as our use of past self-reflections, can solve previously challenging problems. In future work, we plan to explore the application of MetaReflection in other contexts and refine its capabilities, aiming to enhance the performance of language models across diverse domains."
        }
    ],
    "url": "http://arxiv.org/html/2405.13009v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.13009v1",
        "paper_title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
        "research_background": "### Motivation\nThe motivation for this paper arises from the complexities and inefficiencies associated with crafting specific prompts for Large Language Models (LLMs) like GPT-4. Prompt engineers often spend considerable time iterating and refining these prompts to optimize them for specific tasks, leading to significant delays and inefficiencies. Additionally, there is a need to improve the performance of language agents during multiple conversational turns without heavily relying on continuous feedback mechanisms.\n\n### Research Problem\nThe research problem tackled in this paper is the time-consuming and iterative process of creating effective prompts for LLMs and finding an efficient way to enhance the performance of language agents without recurring feedback. Specifically, the paper focuses on developing a method to generalize verbal instructions from past reflections to improve future trajectories of task performance by language agents.\n\n### Relevant Prior Work\nThe paper builds upon several key areas of related work:\n\n1. **Prompt Crafting and Engineering**: Previous research by Zamfirescu-Pereira et al. (2023) and Parnin et al. (2023) highlights the extensive time and effort required for prompt engineers to iterate and refine prompts to optimize LLM performance for specific tasks.\n\n2. **Feedback Mechanisms and Reflection**: Shinn et al. (2023) have shown that verbal reinforcement learning and feedback during multiple conversational turns can improve the performance of language agents. They demonstrated how episodic memory that stores reflection texts from failing trajectories can help correct future errors.\n\n3. **ReAct Framework**: The ReAct framework by Yao et al. (2023b), which involves self-reflection on errors to improve task performance in recurrent attempts, serves as an inspiration for capturing reflective texts as a means of enhancing agent trajectories.\n\n4. **Performance Improvement in Specific Domains**: The research explored how these methods can be applied to concrete tasks such as understanding question-answer pairs from the HotpotQA dataset Yang et al. (2018), showing how reflecting on failed attempts can help LLMs navigate complex information retrieval and processing tasks more effectively.\n\nThis paper advances these ideas by introducing a novel approach\u2014MetaReflection\u2014which leverages past self-reflections to create generalized verbal instructions that can guide LLMs effectively even without real-time feedback. This method is empirically evaluated in diverse domains, showing consistent improvements over baseline models.",
        "methodology": "The proposed MetaReflection method is centered on iteratively refining a set of instructions intended to enhance a language agent's (client agent's) performance. Here's a detailed breakdown of the methodology:\n\n### Core Components:\n\n1. **Client Agent (RL Actor)**:\n    - A language model that performs tasks based on a given set of instructions and task descriptions.\n    - Various client agents such as ReAct (Yao et al., 2023), CoT (Wei et al., 2023), and a vanilla one-shot language model are employed in the experiments.\n\n2. **Evaluation/Feedback Agent**:\n    - Provides feedback on the client agent\u2019s performance.\n    - Feedback variants include a 0-1 boolean feedback agent and an exact match checker depending on the application domain.\n\n3. **Self-Reflection Agent**:\n    - Produces verbal reinforcements based on the RL trajectory of the client agent.\n\n4. **Meta-Reflection Agent**:\n    - Takes as input: prior instructions, self-reflections, and training data.\n    - Produces updated instructions which generalize the prior instructions by incorporating learnings from the self-reflections.\n    - Uses a standard language model with a specific prompt to create new, non-case specific instructions.\n    - Instructions are formatted as a list and can either be updated by adding a new item or by combining previous items with new reflections to shorten and refine the list.\n\n### Iterative Process:\n\n- **Instruction Generation and Validation**:\n    - Instructions are validated using the training data within each batch.\n    - The process ensures that new instructions outperform prior instructions before accepting them.\n    - If new instructions are suboptimal (due to sparse rewards, generalization issues, or language model unpredictability), the process can revert to previous instructions.\n\n### Optional Augmentations:\n\n1. **Using Full Trajectories**:\n    - In addition to self-reflections, parts of the full trajectory, such as inner thought steps, can be utilized during the meta-reflection step.\n\n### Example:\n\n- During the HotpotQA experiment, an initial meta-reflection suggested adding a profession when searching for a person to refine search results.\n- In subsequent batches, self-reflections advised adding terms like \"release date\" when searching for movies.\n- The meta-reflection agent could either append this as a new item or generalize the prior instruction to include contextual information for various search entities.\n\n### Validation and Efficiency:\n\n- Validation occurs on the current batch to balance instruction quality with computational efficiency.\n- The algorithm is adaptive, reverting to prior instructions if the new ones do not validate successfully.\n\nOverall, MetaReflection aims to iteratively enhance the language agent's task performance by refining instructions through a feedback-refinement loop, validated continuously to ensure improvement.",
        "main_experiment_and_results": "#### Main Experimental Results\n1. **IaC Domain**:\n   - **Results**: MetaReflection showed consistent improvements in accuracy over the GPT-4 baseline for all policies. Specific cases such as the security policy requiring Azure virtual network subnets to be configured with an NSG were significantly improved by MetaReflection, which addressed specific false positives and false negatives issues.\n   - **Key Findings**: The learned instructions helped enhance precision, though some policies experienced a decrease in recall.\n\n2. **HotpotQA Domain**:\n   - **Performance with GPT-4**:\n     - **CoT (GT)**: Up to a 15% accuracy gain.\n     - **CoT (Distractor)**: Up to a 10% accuracy gain.\n     - **ReAct**: Up to an 8% accuracy gain.\n   - **Performance with GPT-3.5-Turbo**:\n     - **CoT (GT)**: Up to a 10% accuracy gain.\n     - **CoT (Distractor)**: Up to an 8% accuracy gain.\n     - **ReAct**: Up to a 6% accuracy gain.\n   - **Observations**: Larger batch sizes generally improved performance for GPT-4 agents. The instructions generated by MetaReflection were specific and detailed for smaller batch sizes and more generalized for larger batch sizes. GPT-4 better utilized abstract instructions, while specific instructions were more beneficial for GPT-3.5-Turbo.\n\n3. **Qualitative Insights**:\n   - Learned instructions were highly domain-specific, providing precise guidelines for questions and specific action steps in task trajectories.\n   - MetaReflection demonstrated a notable ability to handle diverse instructions across CoT and ReAct settings efficiently, pointing to its potential in enhancing smaller models through task-specific guidance from more powerful LLMs like GPT-4."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The study aims to evaluate the effectiveness of the MetaReflection technique in improving the accuracy of vulnerability detection in IaC files.",
            "experiment_process": "The authors collected 200 Terraform modules from GitHub, focusing on a diverse range of applications. They selected the 15 most commonly violated Terrascan policies and manually annotated the module-policy pairs to determine policy violations. These pairs were split into a 70:30 train-test ratio. GPT-4 was used as the baseline agent with an appropriate prompt, receiving 0-1 feedback on its responses. The MetaReflection algorithm was then applied to the training set, and the accuracy was reported for both the baseline and MetaReflection-enhanced agents. Additionally, LLMInstruction was used as a second baseline, where the model generated instructions for the task.",
            "result_discussion": "The results indicate that MetaReflection consistently improves accuracy over the GPT-4 and LLMInstruction baselines. Precision improved significantly for all policies, although some decreases in recall were noted. MetaReflection showed particular efficacy in handling complex relationships in IaC policies and correcting false positives and false negatives. However, the drop in recall is an important issue to address, and future work will explore domain-specific feedback.",
            "ablation_id": "2405.13009v1.No1"
        },
        {
            "research_objective": "The study aims to assess how MetaReflection improves model performance in the HotpotQA question-answering domain.",
            "experiment_process": "The HotpotQA dataset's two settings (Distractor and Full-Wiki) were used. Subsets of the train split were adversarially sampled to create train and test sets. MetaReflection, GPT-4, and LLMInstruction were compared. For CoT, Wei et al.'s agent was reused, and Yao et al.'s re-implementation was used for ReAct. Train and test sets included 40-80 examples. Variants powered by GPT-3.5-Turbo were also evaluated. MetaReflection and baseline models were run for various batch sizes to determine their impact.",
            "result_discussion": "MetaReflection provided significant accuracy improvements for both CoT (Distractor/GT) and ReAct settings, with up to 19.83% gains for CoT (GT). Higher batch sizes generally improved performance for GPT-4 powered agents, but not consistently for GPT-3.5-Turbo. Qualitatively, smaller batches produced many specific instructions, while larger batches generalized the instructions into broader rules. MetaReflection demonstrated its ability to disambiguate misleading context and improve search strategies.",
            "ablation_id": "2405.13009v1.No2"
        }
    ]
}