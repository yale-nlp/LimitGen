{
    "title": "ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy",
    "abstract": "Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotation or implementations of diverse prompting frameworks. In this work, we propose A3T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with A3T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A3T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A3T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The rapid development of Large Language Models (LLMs) (OpenAI, 2023  ###reference_b15###; Touvron et al., 2023  ###reference_b24###; Team et al., 2023  ###reference_b23###; Jiang et al., 2024  ###reference_b8###) has led to the prosperity of language agents. Leveraging the ability of LLMs, language agents have demonstrated impressive performances in diverse decision-making scenarios by interacting with the environments autonomously (Wang et al., 2023  ###reference_b26###; Mirchandani et al., 2023  ###reference_b14###; Zheng et al., 2024  ###reference_b36###; Wu et al., 2024  ###reference_b28###).\nRecently, increasing efforts have been made to train language agents with open-sourced LLMs. The multi-step trajectories that describe the entire task-solving process of a language agent are used as training data, which consist of environmental observations, internal reasoning texts, and external actions. The collection of such trajectories is therefore essential, which are currently categorized into two paradigms in Fig. 1  ###reference_### (a) and (b). The first paradigm is to leverage expert demonstrations (Yao et al., 2022  ###reference_b30###). However, the expense of human labor hampers the scalability of the approach. Another paradigm is implementing different agent frameworks to gather diverse trajectories with proprietary LLMs (Qin et al., 2023  ###reference_b17###; Zeng et al., 2023  ###reference_b34###; Chen et al., 2023  ###reference_b2###; Aksitov et al., 2023  ###reference_b1###). However, the exploration coverage in the training data is still upper-bounded by the full set of prompting techniques. Besides, implementing diverse agent frameworks requires considerable human efforts and proprietary LLM calls (Yang et al., 2024  ###reference_b29###). To ease the data collection process in diverse scenarios, Yin et al. (2023  ###reference_b32###) and Zhang et al. (2024  ###reference_b35###) propose unified data formats by elucidating the comprising submodules in agent trajectories. However, as obtained by converting human-annotated data or one single defaulted prompting scheme, the agent trajectories are still limited in diversity and scalability. Considering that an environment automatically returns observations and rewards with action inputs, it should serve as an infinite data generator. While Song et al. (2024  ###reference_b22###) propose an exploration-based agent framework for self-improvement, the gathered trajectories consist of only interleaved external actions and environmental observations, without textual rationales that could steer better behavior of language agents. We ask the following question: Can a language agent autonomously gather high-quality trajectories, with textual annotations suitable for its further training?\nIn this work, we propose A3T, a framework that enables Autonomous Annotation of Agent Trajectories in the style of ReAct (Yao et al., 2023  ###reference_b31###) for self-improvement with minimal human supervision. The central idea is to exploit both the in-context language ability and the decision-making ability of a language agent: To collect diverse trajectories, an agent could randomly sample external actions from the action space at arbitrary steps. However, the corresponding reason for the sampled action should be annotated for a ReAct-style agent. To facilitate this, we propose ActRe, an act-then-reason prompting agent that explains the reason for the sampled action. With ActRe, the ReAct-style agent composes extra reason-then-act trajectories for each failed task by inversely prepending the ActRe-prompted reason to the randomly sampled action. After the execution of each composed trajectory, the agent receives a terminal reward from the environment, which automatically annotates the quality of the trajectory.\nThe gathered successful trajectories are then supplemented with the failed trajectory by the ReAct-style agent alone for contrastive self-training, where we use policy gradient methods (Williams, 1992  ###reference_b27###) with binarized rewards for LLM fine-tuning. As new agents are trained, more trajectories can be gathered and accumulated, which forms a closed loop for the self-improvement of language agents as shown in Fig. 1  ###reference_###-(c).\nWe validate our A3T framework in the textual embodied environment AlfWorld (Shridhar et al., 2021  ###reference_b20###) and the online shopping environment WebShop (Yao et al., 2022  ###reference_b30###). We use QLoRA (Dettmers et al., 2023  ###reference_b4###) to fine-tune Mistral-7B-Instruct-v0.2 (Jiang et al., 2023  ###reference_b7###) in the training experiments.\nExperimental performances demonstrate significant improvement over state-of-the-art agent techniques: On AlfWorld, our trained agent achieves a 96% success rate in unseen scenarios with a single trial. On WebShop, the success rate of our agent reaches 49%, which matches the average human performance (50%). In the setting of iterative refinement, after four rounds of data collection and contrastive self-training, the accumulated success rate becomes 100% on AlfWorld and 54.8% on WebShop, narrowing the gap with human experts (59.6% on WebShop). A3T paves the way for agents with improved autonomy through the closed loop of self-annotation and contrastive self-training."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "A3T for Closed-Loop Self-Improvement",
            "text": "In this section, we introduce the closed-loop self-improvement for agents facilitated by the A3T framework. The loop contains two parts: autonomous trajectory annotation with the ActRe agent (Sec. 2.1  ###reference_###), and the contrastive self-training process (Sec. 2.2  ###reference_###)."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Autonomous Trajectory Annotation with ActRe",
            "text": "Agents are able to gather diverse trajectories by exploration. However, for language agents like ReAct, the actions are inferred by first reasoning with LLMs. When the agent randomly samples an action that differs from the self-inferred one, a modified reason is needed to compose a full reason-then-act trajectory. Yao et al. (2023  ###reference_b31###) show that humans can modify the reasoning in the trajectory and prompt a ReAct-style agent for desirable actions. As humans can provide in-progress supervision, such a human-in-the-loop process still lacks scalability.\nTo automate the process, we propose a complementary ActRe prompting agent to synthesize the modified reasons by leveraging the in-context language ability of an LLM. ActRe inverts the causality of ReAct: while ReAct conditions the external action with a reason a priori, ActRe explains the reason a posteriori for an external action. The synergy of ActRe with ReAct facilitates the autonomous annotation of textual reasons: when the language agent randomly samples an external action, the reason for the sampled action is obtained by querying the ActRe prompting agent. The synthetic reason is then used as the condition of the sampled action for the ReAct-style agent. The progress of a trajectory is synchronized between the ReAct-style agent and the ActRe prompting agent, with the only difference in the order of intermediate reasoning and actions. The detailed workings are depicted below:\nDenote , ,  as the environmental observation, internal reasoning, and external action at the -th step, respectively. The trajectory of a ReAct-style agent reads:\nThe synchronized ActRe prompting agent has the following trajectory:\nNow when the ReAct-style agent explores for a different external action at  step by changing  into , the corresponding internal reasoning  should be altered as well. This is achieved by querying the ActRe prompting agent:\nthen the synthesized  and the sampled  compose a new ReAct trajectory:\nAt the end of each trajectory, the environment provides a terminal reward  to indicate whether the execution is successful. The reward  automatically annotates the quality of the entire trajectory. In this way, the language agent autonomously composes diverse ReAct-style trajectories without human annotation effort, paving the way for self-training."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Contrastive Self-Training",
            "text": "Language agents are trained by fine-tuning an LLM with the accumulated trajectories. While supervised fine-tuning (SFT) with high-quality data is widely adopted (Zhou et al., 2023b  ###reference_b38###; Singh et al., 2023  ###reference_b21###), in this work, we improve the awareness of the agent about the contrast between failed and successful trajectories in the same task with policy gradient methods (Williams, 1992  ###reference_b27###). In the context of ReAct-style language agents, a gathered trajectory  with  steps reads , with  in token strings representing the -step environmental observation, and  representing the textual action of the agent being either the internal reasoning  or the external action . Given a total of  trajectories , we maximize the following objective as the estimation of policy gradient:\nwith  as the score of the trajectory , and  as the LLM with parameters  to be fine-tuned. While traditional policy gradient methods omit the  world modeling part, in our work, we keep the term and tune  to learn a joint model of action and world modeling. This instructs the LLM to better align with the tasks and the environment.\nFor the gathered trajectories in each task, we filter the composed ones that result in unsuccessful task completion. This ensures that all the failed trajectories generated solely by the agent are paired with successful trajectories in the same task, and all the successful trajectories are retained in the set. Assume that in the same task, we have  successful trajectories , ,  and a failed trajectory . Then Eq.(1  ###reference_###) for the  trajectories can be structured as\nwhere we use the fact that  for all  as they are successful trajectories. According to Eq. (2  ###reference_###), we have the following remarks about shaping the reward of the failed trajectory:\nWhen , Eq. (2  ###reference_###) is reduced to the objective of supervised fine-tuning with only the successful trajectories, which is equivalent to Zhou et al. (2023b  ###reference_b38###) and Singh et al. (2023  ###reference_b21###).\nWhen , the coefficient of the second part (supervised fine-tuning on the failed trajectory)  is zeroed. The objective becomes a weighted average of supervised fine-tuning on successful trajectories (the first part), and likelihood contrast between each pair of successful/failed trajectories (the third part).\nWhen  and , the coefficient of the first part (supervised fine-tuning on the successful trajectories) is zeroed out as well, leaving the objective into a single likelihood contrast (the third part) between trajectory pairs. According to Rafailov et al. (2023  ###reference_b18###), this leads to poor performance because of training instability.\nIn implementation, we binarize the reward of the failed trajectories with . To address Remark 3, we let the agent collect multiple successful trajectories via diverse exploration to satisfy . After training, the new agent would follow Sec. 2.1  ###reference_### to gather more annotated trajectories. The trajectory set then continually grows as looping more rounds of data collection and agent training. For the training in each round, we use the accumulated trajectory set to fine-tune an LLM with Eq. (1  ###reference_###). Another implementation detail is that in the initial round, we use 1-shot ReAct prompting to gather the training trajectories instead of exploration and annotation for bootstrapping. The failed trajectory for each task is directly excluded as it is not paired with sampled successful trajectories. Eq. (1  ###reference_###) is therefore reduced to ReAct supervised fine-tuning in Yao et al. (2023  ###reference_b31###) for the training in Round 0. The latter rounds leverage explored trajectories via autonomous annotation, and self-training by Eq. (1  ###reference_###) with binarized rewards. Other details are covered in Appendix A  ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We conduct experiments on two benchmarks to valid the effectiveness of A3T: the textual embodied environment AlfWorld (Shridhar et al., 2021  ###reference_b20###), and the online shopping environment WebShop (Yao et al., 2022  ###reference_b30###). The two benchmarks require a language agent to perform multi-step decision-making to accomplish a certain goal introduced in each task.\nIn A3T, we loop for 4 rounds of trajectory collection and agent training, with the initial round using ReAct prompting as the bootstrap of training data. No trajectories are gathered from testing tasks for training. We use gpt-3.5-turbo-instruct-0914 to implement the initial ReAct prompting, as well as the ActRe prompting agent that helps the trajectory composition in the latter  rounds. We use the open-sourced Mistral-7B-Instruct-v0.2 (Jiang et al., 2023  ###reference_b7###) with QLoRA (Dettmers et al., 2023  ###reference_b4###) finetuning for the training experiments.\nWe compare our A3T framework with multiple strong baselines, including methods like advanced prompting frameworks using GPT-4, specialized LLMs by full fine-tuning, and gpt-3.5-turbo-1106 fine-tuning. The results are reported in the following sections."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "AlfWorld",
            "text": "###table_1### Alfworld is a textual embodied environment where an agent needs to accomplish a high-level goal by reasoning about its situation and performing sequential low-level actuation. Covering 6 task types, the benchmark provides 3,553 tasks for training and 134 held-out tasks for unseen scenarios evaluation. We use 660 out of the 3,553 tasks for our experiments: 600 for training and 60 for validation. In each round,  trajectories are composed for each training task failed by the policy agent. See Appendix A  ###reference_### for other implementation details.\nBaseline methods are divided into two categories: the methods that make only a single trial in each test task, and the methods that perform iterative refinement in a test task. In the former category, we select BUTLER (Shridhar et al., 2021  ###reference_b20###) with supervised training over  expert trajectories on each task type. We also select LM-BUTLER (Micheli & Fleuret, 2021  ###reference_b13###) that fine-tunes a full GPT2-medium by collecting expert demonstrations with the interactions from all the 3,553 tasks (with interleaved observations and external actions in each trajectory). We also compare with the best version of the fully fine-tuned AgentLM (Zeng et al., 2023  ###reference_b34###) in the AlfWorld task (AgentLM-70B), which leverages trajectories from all 3,553 training tasks in AlfWorld and other tasks in different benchmarks. The ReAct prompting (Yao et al., 2023  ###reference_b31###) is also categorized into this category, and we also rerun the method with gpt-3.5-turbo-instruct-0914, following their setting to use 6 distinct prompts and report the best performance. In the latter category, we select Reflexion (Shinn et al., 2023  ###reference_b19###) that prompts GPT-3.5 to self-reflect with failed trials. We also compare with RAFA (Liu et al., 2023  ###reference_b10###), a principled iterative planning framework using GPT-4 as the critic.\nTables 1  ###reference_### and 2  ###reference_### show the performance comparison of our framework. For the single trial setting, the overall success rate of our agent reaches  at -nd round and matches the prior SoTA (LM-BUTLER). However, our agent is trained with a QLoRA of 26M parameters and 600 training tasks, while LM-BUTLER is fine-tuned from a full GPT2-medium of 355M parameters and all 3,553 training tasks. Besides, our agent demonstrates constant performance improvements with the  rounds in the held-out seen evaluation scenarios and outperforms LM-BUTLER (Table 14  ###reference_### in Appendix C.1  ###reference_###). For the iterative refinement setting, our agent obtains 100% success by accumulating the decision-making trials of all the  trained agents from each round. The accumulated trajectory set accounts for the significance of the performance. Table 3  ###reference_### shows that the success rate of the trajectories composed by the agent on the training tasks improves continually. More details are covered in Appendix C.1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "WebShop",
            "text": "WebShop is an online shopping environment where an agent needs to purchase the most suitable item according to a provided instruction. The agent should navigate through a sequence of query searches and button clicks on the website to accomplish the task. WebShop provides a real-valued reward , with  as success. The benchmark provides 11,587 tasks for training and validation, and 500 held-out tasks as testing scenarios. We use 2,700 out of the 11,587 tasks for our experiments, with 2,300 for training and 400 for validation.  trajectories are composed for each training task failed by our trained agent in each round. Other training details are listed in Appendix A  ###reference_###.\nBaseline methods are still divided by whether or not to perform test-time iterative refinement. For the setting of a single test trial, we compare with ReAct prompting and WebGUM (Furuta et al., 2024  ###reference_b5###) by jointly fine-tuning a ViT visual encoder and a Flan-T5-XL. Recently, AgentBoard (Ma et al., 2024  ###reference_b12###) offers an easy/hard split of the first  test tasks in WebShop for better evaluation, and Liu et al. (2024  ###reference_b11###) report the benchmarked results of xLAM-v0.1 (Zhang et al., 2024  ###reference_b35###) with multi-task full finetuning of Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024  ###reference_b8###). We also include xLAM-v0.1 (Zhang et al., 2024  ###reference_b35###) as a single-shot baseline and report the performance comparison on AgentBoard. While LUMOS (Yin et al., 2023  ###reference_b32###) shares a similar spirit with xLAM-v0.1, the WebShop task is treated as an unseen scenario in their setting. To conduct a fair comparison, we do not compare ours with LUMOS. For the setting that allows test-time iterative refinement, Reflexion has been claimed to be ineffective in Shinn et al. (2023  ###reference_b19###). We compare ours with LATS (Zhou et al., 2023a  ###reference_b37###), a prompting-based language agent with multiple rounds of self-reflection and tree search.\n###table_2### ###table_3### ###table_4### Tables 4  ###reference_### and 5  ###reference_### demonstrate the significance of A3T agents. With a single test trial, the A3T agent matches averaged human performance (reward: 73.9 v.s. 75.5; success rate: 49.0% v.s. 50.0%). With 4 shots of test trials, A3T achieves a 54.8% success rate, closing the gap with human expert performance (59.6%). The 1-shot A3T agent also outperforms prompting with GPT-4-32k-0613 in both the easy and the hard split of WebShop from AgentBoard. Table 6  ###reference_### further shows the quality improvement of the accumulated trajectories across multiple rounds of A3T. Case studies for annotated trajectories, as well as the dataset statistics for each round of training are reported in Appendices B  ###reference_### and C.2  ###reference_###, respectively.\n###table_5###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experiments with gpt-3.5-turbo-1106 fintuning",
            "text": "While all of the experiments we previously reported are conducted with Mistral-7B-Instruct-v0.2 and QLoRA finetuning, in this section, we also validate A3T with gpt-3.5-turbo-1106 finetuning, the proprietary service provided by OpenAI. As the initial trajectory set for Round-0 training in A3T is obtained by ReAct prompting with gpt-3.5-turbo-instruct-0914, the starting point for the two base LLMs is the same. Tables 8  ###reference_### and 9  ###reference_### report the performance comparison of Round-0 supervised training between the open-sourced and the proprietary LLMs. In AlfWorld, the performance of the QLoRA fine-tuned Mistral-7B-Instruct-v0.2 even surpasses that of the proprietary gpt-3.5-turbo-1106 fine-tuning service. In WebShop, the proprietary gpt-3.5-turbo-1106 finetuning performs better in Round-0 supervised training. We then let the two models separately compose diverse trajectories for their self-training. Because of the expense of inferring with the finetuned gpt-3.5-turbo-1106 model333The pricing is listed in https://openai.com/pricing  ###reference_openai.com/pricing###, we compose trajectories for each failed training task (with the open-sourced LLM). Shown in Table 10  ###reference_###, the quality of the accumulated trajectories composed by the proprietary LLM is on par with those composed by the open-sourced LLM. After Round-1 self-training, the open-sourced model achieves an even higher test success rate. This is attributed to the proprietary service providing only the supervised fine-tuning option, while also indicating the importance of contrastive fine-tuning in A3T."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we propose A3T, a framework that enables the autonomous annotation of agent trajectories in the style of ReAct for contrastive self-training. The key factor in the trajectory annotation process is the ActRe prompting agent, which produces the textual rationales given arbitrary external actions. Together with ActRe and environmental feedback, the ReAct-style agent autonomously synthesizes trajectories for self-training. In the contrastive self-training process, we leverage the policy gradient methods with binarized rewards to boost the task success rate. Extensive experiments on AlfWorld and WebShop have demonstrated the superiority of A3T over multiple strong baselines."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Implementation Details",
            "text": "We implement autonomous trajectory composition in Rounds 1-3, with Round 0 as the ReAct fine-tuning bootstrapping process. The trained agent for Rounds 1-3 also follows the style of ReAct. Shown in Yao et al. (2023  ###reference_b31###), humans can modify the mistaken internal reasoning to correct the action behavior of a ReAct agent. In our work, the ActRe prompting agent replaces the human role as a rationale annotator for the ReAct policy agent. The ReAct agent decides whether to sample a novel action with probability . If the action is not to be sampled, then the ReAct policy agent takes the action conditioned on internal reasoning. Otherwise, the sampled action is sent to the ActRe agent as a query for posterior explanations. In the implementation, we select . If consecutive 3 actions taken by the policy agent itself are invalid, the external action is sampled one more time with the query of ActRe for reason synthesis. In AlfWorld, we let the agent gather 40 trajectories to improve the possibility of attaining at least one successful trajectory for each failed task. In WebShop experiments, due to budgetary limits, we adopt another strategy by first forcing the policy agent to collect 3 trajectories, and then terminate when a successful trajectory is gathered, or the total number of annotated trajectories reaches 20 for a single task. The prompts for ActRe in AlfWorld and WebShop are shown in Tables 11  ###reference_### and 12  ###reference_###, respectively.\nAs for self-training, we use QLoRA finetuning with the open-sourced Mistral-7B-Instruct-v0.2 throughout the main experiments. We set the rank to be 16 and  to be 32 in the QLoRA configurations. The tuned modules are q_proj and v_proj only. We use the nf4 quantization without double quantization in our experiments with QLoRA. The optimizer is set to be the 32-bit paged AdamW. This allows the experiments to be conducted on 2-4 NVIDIA 3090 GPUs with 24GB memory or 2 NVIDIA A100 GPUs with 40GB memory. We set the learning rate as 1e-4, and train 10 epochs for the initial Round 0 (ReAct supervised fine-tuning with successful trajectories). In Rounds 1-3, we fine-tune the LLM for 6 epochs with Eq. 1  ###reference_### () and conduct checkpoint averaging over the entire training process.\n###table_9### ###table_10###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Case Studies of the Composed Trajectories",
            "text": "Figs. 2  ###reference_###, 3  ###reference_###, 4  ###reference_###, and 5  ###reference_### show several cases of trajectories composed with ActRe on WebShop.\n###figure_1### ###figure_2### ###figure_3### ###figure_4###"
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Additional Experimental Results",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T1.8\" style=\"width:397.5pt;height:131.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-74.2pt,24.5pt) scale(0.728246537466825,0.728246537466825) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.8.8\">\n<tr class=\"ltx_tr\" id=\"S3.T1.8.8.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S3.T1.8.8.9.1\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.8.8.9.2\">Pick</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.8.8.9.3\">Clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.8.8.9.4\">Heat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.8.8.9.5\">Cool</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.8.8.9.6\">Examine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.8.8.9.7\">PickTwo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.8.8.9.8\">Total</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.1.1\">BUTLER\u2006<sub class=\"ltx_sub\" id=\"S3.T1.1.1.1.1.1\">8</sub>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Shridhar et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib20\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.1.2\">46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.1.3\">39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.1.4\">74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.1.6\">22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.1.7\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.1.8\">37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T1.2.2.2.1\">AgentLM<sup class=\"ltx_sup\" id=\"S3.T1.2.2.2.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.2.2.1.1.1\">\u2217</span></sup>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib34\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.2.2.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.2.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.2.2.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.2.2.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.2.2.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.2.2.2.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.2.2.2.8\">86</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.8.8.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T1.8.8.10.1\">LM-BUTLER\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Micheli &amp; Fleuret, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib13\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.8.8.10.2\">96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.8.8.10.3\">97</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.8.8.10.4\">96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.8.8.10.5\">90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.8.8.10.6\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.8.8.10.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.8.10.7.1\">94</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.8.8.10.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.8.10.8.1\">96</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T1.3.3.3.1\">ReAct\u2006<sub class=\"ltx_sub\" id=\"S3.T1.3.3.3.1.1\">6</sub>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib31\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.3.3.2\">92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.3.3.3\">58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.3.3.4\">96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.3.3.5\">86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.3.3.6\">78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.3.3.3.7\">41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.3.3.3.8\">71</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.4.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T1.4.4.4.1\">ReAct\u2006<sub class=\"ltx_sub\" id=\"S3.T1.4.4.4.1.1\">6</sub>\u00a0(Our rerun)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.4.4.2\">66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.4.4.3\">87</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.4.4.4\">86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.4.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.4.4.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.4.4.6\">88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.4.4.4.7\">64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.4.4.4.8\">83</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T1.5.5.5.1\">A<sup class=\"ltx_sup\" id=\"S3.T1.5.5.5.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.5.5.5.1.1.1\">3</span></sup>T (Round=0)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.5.2\">96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.5.3\">77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.5.4\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.5.5\">95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.5.6\">94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.5.5.5.7\">47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.5.5.5.8\">86</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.6.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T1.6.6.6.1\">A<sup class=\"ltx_sup\" id=\"S3.T1.6.6.6.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.6.6.6.1.1.1\">3</span></sup>T (Round=1)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.6.6.6.2\">96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.6.6.6.3\">94</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.6.6.6.4\">96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.6.6.6.5\">95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.6.6.6.6\">89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.6.6.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.6.6.6.7.1\">94</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.6.6.6.8\">94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T1.7.7.7.1\">A<sup class=\"ltx_sup\" id=\"S3.T1.7.7.7.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.7.7.7.1.1.1\">3</span></sup>T (Round=2)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.7.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.7.7.2.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.7.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.7.7.3.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.7.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.7.7.4.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.7.7.5\">95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.7.7.6\">89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.7.7.7\">82</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.7.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.7.7.8.1\">96</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.8.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S3.T1.8.8.8.1\">A<sup class=\"ltx_sup\" id=\"S3.T1.8.8.8.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.8.8.8.1.1.1\">3</span></sup>T (Round=3)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.8.8.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.8.8.2.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.8.8.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.8.8.3.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.8.8.8.4\">91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.8.8.8.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.8.8.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.8.8.8.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.8.8.8.6.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.8.8.8.7\">71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.8.8.8.8\">95</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Success rate () on each task type of AlfWorld, with a single trial on each of the 134 unseen evaluation scenarios. \u201cBUTLER\u2006<sub class=\"ltx_sub\" id=\"S3.T1.27.1\">8</sub>\u201d and \u201cReAct\u2006<sub class=\"ltx_sub\" id=\"S3.T1.28.2\">6</sub>\u201d denotes the best performance of / trials following prior work. The A<sup class=\"ltx_sup\" id=\"S3.T1.29.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.29.3.1\">3</span></sup>T agents are trained with 600 out of the total 3,553 training tasks in AlfWorld. <sup class=\"ltx_sup\" id=\"S3.T1.30.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.30.4.1\">\u2217</span></sup>: The best result reported in <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib34\" title=\"\">2023</a>)</cite>.</figcaption>\n</figure>",
            "capture": "Table 1: Success rate () on each task type of AlfWorld, with a single trial on each of the 134 unseen evaluation scenarios. \u201cBUTLER\u20068\u201d and \u201cReAct\u20066\u201d denotes the best performance of / trials following prior work. The A3T agents are trained with 600 out of the total 3,553 training tasks in AlfWorld. \u2217: The best result reported in Zeng et\u00a0al. (2023)."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T2.4\" style=\"width:397.5pt;height:88.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-83.7pt,18.7pt) scale(0.703654690683085,0.703654690683085) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T2.4.4\">\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S3.T2.4.4.5.1\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T2.4.4.5.2\">Iters.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.4.5.3\">Pick</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.4.5.4\">Clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.4.5.5\">Heat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.4.5.6\">Cool</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.4.5.7\">Examine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T2.4.4.5.8\">PickTwo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.4.4.5.9\">Total</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.6.1\">Reflexion\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Shinn et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib19\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.6.2\">11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.6.3\">96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.6.4\">94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.6.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.6.6\">95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.6.7.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.4.4.6.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.6.8.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.4.4.6.9\">97</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.4.4.7.1\">RAFA\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib10\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.7.2\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.7.3.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.7.4\">97</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.7.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.7.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.7.6.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.7.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.7.7.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.4.4.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.7.8.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.4.4.7.9\">99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.1.1\">A<sup class=\"ltx_sup\" id=\"S3.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.1.1.1.1.1.1\">3</span></sup>T (Round=0)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.1.2\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.1.3\">96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.1.4\">77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.1.6\">95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.1.7\">94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.1.8\">47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.1.9\">86</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.2.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.2.2.2.1\">A<sup class=\"ltx_sup\" id=\"S3.T2.2.2.2.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.2.2.2.1.1.1\">3</span></sup>T (Round=1, accum.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.2.2.2.2\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.2.3\">96</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.2.4\">94</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.2.2.2.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.2.6\">95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.2.2.2.7.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.2.2.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.2.2.2.8.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.2.2.2.9\">97</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T2.3.3.3.1\">A<sup class=\"ltx_sup\" id=\"S3.T2.3.3.3.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.3.3.3.1.1.1\">3</span></sup>T (Round=2, accum.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.3.3.3.2\">3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.3.3.3.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.3.3.4.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.3.3.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.3.3.6\">95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.3.3.7.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.3.3.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.3.3.3.8.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.3.3.3.9\">99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.4.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.1\">A<sup class=\"ltx_sup\" id=\"S3.T2.4.4.4.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.4.4.4.1.1.1\">3</span></sup>T (Round=3, accum.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.2\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.3.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.4.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.5.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.6.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.7.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.4.4.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.8.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.4.4.4.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.4.4.4.9.1\">100</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Success rate () on each task type of AlfWorld, with iterative refinement on each of the 134 unseen evaluation scenarios. \u201cIters\u201d denotes the minimum iterations of test-time refinement required to achieve the reported results<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The refinement iterations of Reflexion and RAFA are measured in their publicized logs: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/noahshinn/reflexion/blob/main/alfworld_runs/root/reflexion_run_logs/env_results_trial_10.json\" title=\"\">https://github.com/noahshinn/reflexion/blob/main/alfworld_runs/root/reflexion_run_logs/env_results_trial_10.json</a> and <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/agentification/RAFA_code/blob/main/ALFWorld/run_logs/env_results_trial_7.json\" title=\"\">https://github.com/agentification/RAFA_code/blob/main/ALFWorld/run_logs/env_results_trial_7.json</a>.</span></span></span>, and \u201caccum.\u201d means the best reward of the trajectories accumulated since the -th Round for each task. After the self-training process in Round 3, the success rate of the 4-shot A<sup class=\"ltx_sup\" id=\"S3.T2.12.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T2.12.1.1\">3</span></sup>T agent already reaches 100%.</figcaption>\n</figure>",
            "capture": "Table 2: Success rate () on each task type of AlfWorld, with iterative refinement on each of the 134 unseen evaluation scenarios. \u201cIters\u201d denotes the minimum iterations of test-time refinement required to achieve the reported results222The refinement iterations of Reflexion and RAFA are measured in their publicized logs: https://github.com/noahshinn/reflexion/blob/main/alfworld_runs/root/reflexion_run_logs/env_results_trial_10.json and https://github.com/agentification/RAFA_code/blob/main/ALFWorld/run_logs/env_results_trial_7.json., and \u201caccum.\u201d means the best reward of the trajectories accumulated since the -th Round for each task. After the self-training process in Round 3, the success rate of the 4-shot A3T agent already reaches 100%."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T3.3\">\n<tr class=\"ltx_tr\" id=\"S3.T3.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.3.1.1\">Round</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.3.1.2\">Pick</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.3.1.3\">Clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.3.1.4\">Heat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.3.1.5\">Cool</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.3.1.6\">Examine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.3.1.7\">PickTwo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.3.1.8\">Total</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.1\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.2\">76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.3\">85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.4\">92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.5\">85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.6\">86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.7\">77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.3.2.8\">82</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.1\">1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.2\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.3\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.4\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.5\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.6\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.7\">99</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.3.8\">99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.4.1\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.4.2\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.4.3\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.4.4\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.4.5\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.4.6\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.4.7\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.3.4.8\">100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.3.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.5.1\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.5.2\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.5.3\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.5.4\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.5.5\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.5.6\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.5.7\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.3.5.8\">100</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>The success rate () of the accumulated trajectory set in terms of each task type in the training scenarios of AlfWorld. With the agent autonomously annotating diverse trajectories, the data quality for contrastive self-training is progressively improving.</figcaption>\n</figure>",
            "capture": "Table 3: The success rate () of the accumulated trajectory set in terms of each task type in the training scenarios of AlfWorld. With the agent autonomously annotating diverse trajectories, the data quality for contrastive self-training is progressively improving."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T4.12\">\n<tr class=\"ltx_tr\" id=\"S3.T4.12.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.12.13.1\">Test Trial</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S3.T4.12.13.2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T4.12.13.3\">Valid.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T4.12.13.4\">Test.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.12.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T4.12.14.1\" rowspan=\"7\"><span class=\"ltx_text\" id=\"S3.T4.12.14.1.1\">Single</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T4.12.14.2\">ReAct\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib31\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.12.14.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.12.14.4\">66.6/40.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.12.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.12.15.1\">ReAct\u00a0(Our rerun)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.12.15.2\">63.7/34.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.12.15.3\">68.5/40.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.12.16\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.12.16.1\">WebGUM\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Furuta et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib5\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.12.16.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.12.16.3\">67.5/45.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.1.1.1\">AgentLM<sup class=\"ltx_sup\" id=\"S3.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.1.1.1.1.1\">\u2217</span></sup>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib34\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.1.1.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.1.1.3\">70.8/\u2004\u2004-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T4.2.2.1\">A<sup class=\"ltx_sup\" id=\"S3.T4.2.2.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.2.2.1.1.1\">3</span></sup>T (Round=0)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.2.2.2.1\">70.1</span>/41.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.2.2.3\">72.4/45.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.3.3.1\">A<sup class=\"ltx_sup\" id=\"S3.T4.3.3.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.3.3.1.1.1\">3</span></sup>T (Round=1)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.3.3.2\">69.7/43.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.3.3.3\">73.1/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.3.3.3.1\">49.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.4.4.1\">A<sup class=\"ltx_sup\" id=\"S3.T4.4.4.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.4.4.1.1.1\">3</span></sup>T (Round=2)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.4.4.2\">69.0/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.4.4.2.1\">43.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.4.4.3\">73.0/48.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.5.5\">\n<td class=\"ltx_td\" id=\"S3.T4.5.5.2\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.5.5.1\">A<sup class=\"ltx_sup\" id=\"S3.T4.5.5.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.5.5.1.1.1\">3</span></sup>T (Round=3)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.5.5.3\">69.1/42.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.5.5.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.5.5.4.1\">73.9</span>/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.5.5.4.2\">49.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T4.6.6.2\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T4.6.6.2.1\">Iterative</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T4.6.6.1\">LATS\u2006<sub class=\"ltx_sub\" id=\"S3.T4.6.6.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.6.6.1.1.1\">30</span></sub>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib37\" title=\"\">2023a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.6.6.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.6.6.4\">75.9/38.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.8.8.2\">A<sup class=\"ltx_sup\" id=\"S3.T4.8.8.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.8.8.2.1.1\">3</span></sup>T\u2006<sub class=\"ltx_sub\" id=\"S3.T4.8.8.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.8.8.2.2.1\">2</span></sub>\u00a0(Round=1, accum.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.8.8.3\">74.0/47.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.8.8.4\">76.6/51.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.10.10.2\">A<sup class=\"ltx_sup\" id=\"S3.T4.10.10.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.10.10.2.1.1\">3</span></sup>T\u2006<sub class=\"ltx_sub\" id=\"S3.T4.10.10.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.10.10.2.2.1\">3</span></sub>\u00a0(Round=2, accum.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.10.10.3\">75.1/49.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.10.10.4\">77.8/53.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T4.12.12.2\">A<sup class=\"ltx_sup\" id=\"S3.T4.12.12.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.12.12.2.1.1\">3</span></sup>T\u2006<sub class=\"ltx_sub\" id=\"S3.T4.12.12.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.12.12.2.2.1\">4</span></sub>\u00a0(Round=3, accum.)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T4.12.12.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.12.12.3.1\">75.9</span>/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.12.12.3.2\">51.3</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T4.12.12.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.12.12.4.1\">78.2</span>/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.12.12.4.2\">54.8</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.12.17\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.12.17.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T4.12.17.1.1\">-</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T4.12.17.2\">Human (average)\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib30\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T4.12.17.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T4.12.17.4\">75.5/50.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.12.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S3.T4.12.18.1\">Human (expert)\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib30\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T4.12.18.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T4.12.18.3\">82.1/59.6</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Reward () and success rate () on the validation and the test sets in WebShop. <sup class=\"ltx_sup\" id=\"S3.T4.42.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.42.1.1\">\u2217</span></sup>: The best result reported in <cite class=\"ltx_cite ltx_citemacro_cite\">Zeng et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib34\" title=\"\">2023</a>)</cite>. \u201cLATS\u2006<sub class=\"ltx_sub\" id=\"S3.T4.43.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.43.2.1\">30</span></sub>\u201d or \u201cA<sup class=\"ltx_sup\" id=\"S3.T4.44.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.44.3.1\">3</span></sup>T\u2006<sub class=\"ltx_sub\" id=\"S3.T4.45.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.45.4.1\">4</span></sub>\u201d denotes using  or  test trials. The A<sup class=\"ltx_sup\" id=\"S3.T4.46.5\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T4.46.5.1\">3</span></sup>T agents are trained with 2,300 out of the total 11,587 training and validation tasks in WebShop. The single-shot A<sup class=\"ltx_sup\" id=\"S3.T4.47.6\">3</sup>T matches averaged human performance, while the multi-shot A<sup class=\"ltx_sup\" id=\"S3.T4.48.7\">3</sup>T closes the performance gap with human experts.</figcaption>\n</figure>",
            "capture": "Table 4: Reward () and success rate () on the validation and the test sets in WebShop. \u2217: The best result reported in Zeng et\u00a0al. (2023). \u201cLATS\u200630\u201d or \u201cA3T\u20064\u201d denotes using  or  test trials. The A3T agents are trained with 2,300 out of the total 11,587 training and validation tasks in WebShop. The single-shot A3T matches averaged human performance, while the multi-shot A3T closes the performance gap with human experts."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T5.8\">\n<tr class=\"ltx_tr\" id=\"S3.T5.8.9\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" id=\"S3.T5.8.9.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S3.T5.8.9.2\">LLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T5.8.9.3\">easy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T5.8.9.4\">hard</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T5.8.9.5\">all</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T5.1.1.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S3.T5.1.1.2.1\">Prompting</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T5.1.1.1\">GPT-3.5-Turbo-16k-0613<sup class=\"ltx_sup\" id=\"S3.T5.1.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T5.1.1.1.1.1\">\u2217</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.1.3\">52.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.1.4\">50.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.1.1.5\">52.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T5.2.2.1\">GPT-4-Turbo-0613<sup class=\"ltx_sup\" id=\"S3.T5.2.2.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T5.2.2.1.1.1\">\u2217</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.2.2.2\">67.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.2.2.3\">67.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.2.2.4\">67.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T5.3.3.1\">GPT-4-32k-0613<sup class=\"ltx_sup\" id=\"S3.T5.3.3.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T5.3.3.1.1.1\">\u2217</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.3.3.2\">67.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.3.3.3\">69.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.3.3.4\">68.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T5.4.4.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S3.T5.4.4.2.1\">Training</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T5.4.4.1\">xLAM-v0.1<sup class=\"ltx_sup\" id=\"S3.T5.4.4.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T5.4.4.1.1.1\">\u2217</span></sup>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib35\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.4.4.3\">53.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.4.4.4\">51.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T5.4.4.5\">52.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T5.5.5.1\">A<sup class=\"ltx_sup\" id=\"S3.T5.5.5.1.1\">3</sup>T (Round=0)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.5.5.2.1\">74.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.5.5.3\">66.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.5.5.4\">72.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T5.6.6.1\">A<sup class=\"ltx_sup\" id=\"S3.T5.6.6.1.1\">3</sup>T (Round=1)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.6.6.2\">73.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.6.6.3\">73.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.6.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.6.6.4.1\">73.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T5.7.7.1\">A<sup class=\"ltx_sup\" id=\"S3.T5.7.7.1.1\">3</sup>T (Round=2)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.7.7.2\">71.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.7.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.7.7.3.1\">74.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T5.7.7.4\">72.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S3.T5.8.8.1\">A<sup class=\"ltx_sup\" id=\"S3.T5.8.8.1.1\">3</sup>T (Round=3)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T5.8.8.2\">72.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T5.8.8.3\">73.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T5.8.8.4\">72.9</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>The average reward () in the easy/hard split for WebShop by AgentBoard\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib12\" title=\"\">2024</a>)</cite>. <sup class=\"ltx_sup\" id=\"S3.T5.17.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T5.17.1.1\">\u2217</span></sup>: Results reported in <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib11\" title=\"\">2024</a>)</cite>. The single-shot A<sup class=\"ltx_sup\" id=\"S3.T5.18.2\">3</sup>T model significantly surpasses prompting methods with the most capable LLMs like GPT-4.</figcaption>\n</figure>",
            "capture": "Table 5: The average reward () in the easy/hard split for WebShop by AgentBoard\u00a0(Ma et\u00a0al., 2024). \u2217: Results reported in Liu et\u00a0al. (2024). The single-shot A3T model significantly surpasses prompting methods with the most capable LLMs like GPT-4."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T6.3\">\n<tr class=\"ltx_tr\" id=\"S3.T6.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T6.3.3.4\">Round</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T6.1.1.1\">Reward R ()</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T6.3.3.5\">Success Rate (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T6.2.2.2\">%\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T6.3.3.3\">%\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.3.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.3.4.1\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.3.4.2\">68.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.3.4.3\">40.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.3.4.4\">51.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T6.3.4.5\">77.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.3.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.5.1\">1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.5.2\">85.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.5.3\">61.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.5.4\">75.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.5.5\">94.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.3.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.6.1\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.6.2\">88.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.6.3\">69.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.6.4\">82.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T6.3.6.5\">96.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T6.3.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T6.3.7.1\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T6.3.7.2\">90.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T6.3.7.3\">73.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T6.3.7.4\">85.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T6.3.7.5\">96.8</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>The reward, success rate, and percentages of reward above thresholds  and  of the accumulated trajectory set in the training tasks of WebShop. With the agent autonomously composing diverse trajectories, the data quality is continually increasing.</figcaption>\n</figure>",
            "capture": "Table 6: The reward, success rate, and percentages of reward above thresholds  and  of the accumulated trajectory set in the training tasks of WebShop. With the agent autonomously composing diverse trajectories, the data quality is continually increasing."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T7.10\">\n<tr class=\"ltx_tr\" id=\"S3.T7.10.11\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt ltx_border_tt\" id=\"S3.T7.10.11.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt ltx_border_tt\" id=\"S3.T7.10.11.2\">Training Scheme</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_tt\" id=\"S3.T7.10.11.3\">Valid.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt\" id=\"S3.T7.10.11.4\">Test.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.2.2\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S3.T7.1.1.1\">A<sup class=\"ltx_sup\" id=\"S3.T7.1.1.1.1\">3</sup>T (Round=0)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T7.2.2.2\">Supervised with \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T7.2.2.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T7.2.2.3.1\">70.1</span>/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T7.2.2.3.2\">41.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T7.2.2.4\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T7.2.2.4.1\">72.4</span>/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T7.2.2.4.2\">45.4</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.3.3\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S3.T7.3.3.2\">Ablated (Round=0)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T7.3.3.1\">Supervised with \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T7.3.3.3\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T7.3.3.3.1\">68.2</span>/37.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T7.3.3.4\">71.5/43.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.4.4\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S3.T7.4.4.2\">Ablated (Round=0)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T7.4.4.1\">Supervised with \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T7.4.4.3\">68.1/<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T7.4.4.3.1\">37.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T7.4.4.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T7.4.4.4.1\">72.5</span>/44.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.6.6\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S3.T7.5.5.1\">A<sup class=\"ltx_sup\" id=\"S3.T7.5.5.1.1\">3</sup>T (Round=1)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S3.T7.6.6.2\">PG with binarized rewards \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T7.6.6.3\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T7.6.6.3.1\">69.7</span>/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T7.6.6.3.2\">43.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T7.6.6.4\">73.1/<span class=\"ltx_text ltx_font_bold\" id=\"S3.T7.6.6.4.1\">49.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.7.7\">\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S3.T7.7.7.2\">Ablated (Round=1)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S3.T7.7.7.1\">Supervised with \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T7.7.7.3\">69.4/42.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T7.7.7.4\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T7.7.7.4.1\">73.2</span>/45.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.10.12\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S3.T7.10.12.1\">Ablated (Round=1)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T7.10.12.2\">Supervised with label conditions</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T7.10.12.3\">69.5/41.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T7.10.12.4\">72.4/45.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.8.8\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S3.T7.8.8.2\">Ablated (Round=1)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T7.8.8.1\">PG with original rewards \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T7.8.8.3\">69.5/41.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T7.8.8.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T7.8.8.4.1\">73.4</span>/<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T7.8.8.4.2\">47.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.9.9\">\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S3.T7.9.9.2\">Ablated (Round=1)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S3.T7.9.9.1\">PG with original rewards \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T7.9.9.3\">69.3/42.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T7.9.9.4\">72.2/46.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T7.10.10\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_bb ltx_border_r\" id=\"S3.T7.10.10.2\">Ablated (Round=1)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_bb ltx_border_r\" id=\"S3.T7.10.10.1\">PG with binarized rewards \n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb ltx_border_r\" id=\"S3.T7.10.10.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T7.10.10.3.1\">70.2</span>/<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S3.T7.10.10.3.2\">42.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb\" id=\"S3.T7.10.10.4\">73.0/46.6</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Ablated experiments of the self-training techniques in A<sup class=\"ltx_sup\" id=\"S3.T7.22.1\">3</sup>T on WebShop. \u201cPG\u201d stands for the policy gradient technique in Eq.\u00a0(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#S2.E1\" title=\"In 2.2 Contrastive Self-Training \u2023 2 A3T for Closed-Loop Self-Improvement \u2023 ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). The parenthesized suffixes represent the reward configuration for the trajectories in Eq.\u00a0(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#S2.E1\" title=\"In 2.2 Contrastive Self-Training \u2023 2 A3T for Closed-Loop Self-Improvement \u2023 ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). For example, \u201c\u201d means keeping the  trajectories, and assign the failed non-composed trajectories with . The training trajectory set is shared among different runs in the same round, except for reward threshold constraints other than .</figcaption>\n</figure>",
            "capture": "Table 7: Ablated experiments of the self-training techniques in A3T on WebShop. \u201cPG\u201d stands for the policy gradient technique in Eq.\u00a0(1). The parenthesized suffixes represent the reward configuration for the trajectories in Eq.\u00a0(1). For example, \u201c\u201d means keeping the  trajectories, and assign the failed non-composed trajectories with . The training trajectory set is shared among different runs in the same round, except for reward threshold constraints other than ."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T8.3\">\n<tr class=\"ltx_tr\" id=\"S4.T8.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T8.3.1.1\">Base LLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.3.1.2\">Pick</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.3.1.3\">Clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.3.1.4\">Heat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.3.1.5\">Cool</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.3.1.6\">Examine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T8.3.1.7\">PickTwo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.3.1.8\">Total</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T8.3.2.1\">Mistral-7B-Instruct-v0.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.3.2.2\">96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.3.2.3\">77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.3.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.3.2.4.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.3.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.3.2.5.1\">95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.3.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.3.2.6.1\">94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T8.3.2.7\">47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.3.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.3.2.8.1\">86</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T8.3.3.1\">gpt-3.5-turbo-1106</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.3.3.2.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.3.3.3.1\">81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.3.3.4\">96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.3.3.5\">86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.3.3.6\">78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T8.3.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.3.3.7.1\">53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.3.3.8\">84</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Success rate () on each task type of AlfWorld after the Round-0 (supervised) training with both of the base LLMs.</figcaption>\n</figure>",
            "capture": "Table 8: Success rate () on each task type of AlfWorld after the Round-0 (supervised) training with both of the base LLMs."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T9.7\">\n<tr class=\"ltx_tr\" id=\"S4.T9.7.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T9.7.1.1\">Base LLM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T9.7.1.2\">Setting</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T9.7.1.3\">Valid.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T9.7.1.4\">Test.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.7.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T9.7.2.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T9.7.2.1.1\">Mistral-7B-Instruct-v0.2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T9.7.2.2\">Round=0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T9.7.2.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.7.2.3.1\">70.1</span>/41.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T9.7.2.4\">72.4/45.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.7.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T9.7.3.1\">Round=1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T9.7.3.2\">69.7/<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T9.7.3.2.1\">43.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.7.3.3\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T9.7.3.3.1\">73.1</span>/<span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.7.3.3.2\">49.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.7.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T9.7.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T9.7.4.1.1\">gpt-3.5-turbo-1106</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T9.7.4.2\">Round=0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T9.7.4.3\">69.4/42.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T9.7.4.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.7.4.4.1\">73.6</span>/<span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T9.7.4.4.2\">48.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T9.7.5.1\">Round=1 (supervised)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T9.7.5.2\">69.7/<span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.7.5.2.1\">44.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T9.7.5.3\">72.7/46.8</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Reward () / success rate () on validation and test sets of WebShop using different base LLMs. For Round-1 training with gpt-3.5-turbo-1106, we conduct supervised training with  filtering, as only supervised fine-tuning is offered in the service.</figcaption>\n</figure>",
            "capture": "Table 9: Reward () / success rate () on validation and test sets of WebShop using different base LLMs. For Round-1 training with gpt-3.5-turbo-1106, we conduct supervised training with  filtering, as only supervised fine-tuning is offered in the service."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T10\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T10.3\">\n<tr class=\"ltx_tr\" id=\"S4.T10.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T10.3.3.4\">Base LLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T10.1.1.1\">Reward R ()</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T10.3.3.5\">Success Rate (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T10.2.2.2\">%\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T10.3.3.3\">%\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.3.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T10.3.4.1\">Mistral-7B-Instruct-v0.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.3.4.2\">85.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.3.4.3\">61.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.3.4.4\">75.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T10.3.4.5\">94.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.3.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T10.3.5.1\">gpt-3.5-turbo-1106</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T10.3.5.2\">85.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T10.3.5.3\">62.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T10.3.5.4\">76.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T10.3.5.5\">94.3</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Quality comparison of the accumulated trajectory set in Round 1 for WebShop, which is composed by the policy agent with the open-sourced or the proprietary LLM.</figcaption>\n</figure>",
            "capture": "Table 10: Quality comparison of the accumulated trajectory set in Round 1 for WebShop, which is composed by the policy agent with the open-sourced or the proprietary LLM."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T11\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T11.1\">\n<tr class=\"ltx_tr\" id=\"A1.T11.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T11.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T11.1.1.1.1\">The ActRe in-context prompt for AlfWorld</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.2.1\">Observation: Your task is to: put some spraybottle on toilet. You are in the middle</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.3.1\">of a room. Looking quickly around you, you see a cabinet 4, a cabinet 3, a cabinet 2,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.4.1\">a cabinet 1, a countertop 1, a garbagecan 1, a handtowelholder 2, a handtowelholder</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.5.1\">1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.6.1\">Action: think[To solve the task, I need to find and take a sparybottle, then put it on toilet.]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.7.1\">Observation: OK.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.8.1\">Action: act[go to cabinet 1]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.9.1\">Reason for the action: [First I need to find a spraybottle. A spraybottle is more\nlikely to</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.10.1\">appear in cabinet (1-4), countertop (1), toilet (1), sinkbasin (1-2), garbagecan (1). I can</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.11.1\">check one by one, starting with cabinet 1.]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.12.1\">Observation: On the cabinet 1, you see a cloth 1, a soapbar 1, a soapbottle 1.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.13.1\">Action: act[go to cabinet 2]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.14.1\">Observation: The cabinet 2 is closed.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.15.1\">Action: act[open cabinet 2]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.16.1\">Observation: You open the cabinet 2. The cabinet 2 is open. In it, you see a candle 1,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.17\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.17.1\">and a spraybottle 2.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.18\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.18.1\">Action: act[take spraybottle 2 from cabinet 2]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.19\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.19.1\">Reason for the action: [Now I find a spraybottle (2). Next, I need to take it.]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.20\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.20.1\">Observation: You pick up the spraybottle 2 from the cabinet 2.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.21\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.21.1\">Action: act[go to toilet 1]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.22\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.22.1\">Reason for the action: [Now I take a spraybottle (2). Next, I need to put it in/on toilet 1.]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.23\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.23.1\">Observation: On the toilet 1, you see a soapbottle 2.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.24\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T11.1.24.1\">Action: act[put spraybottle 2 in/on toilet 1]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.25\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T11.1.25.1\">Observation: You put the spraybottle 2 in/on the toilet 1.</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>The ActRe prompt for the AlfWorld tasks. ActRe inverts the causality of ReAct, and provides the posterior reason for the external action.</figcaption>\n</figure>",
            "capture": "Table 11: The ActRe prompt for the AlfWorld tasks. ActRe inverts the causality of ReAct, and provides the posterior reason for the external action."
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T12\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A1.T12.1\">\n<tr class=\"ltx_tr\" id=\"A1.T12.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T12.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T12.1.1.1.1\">The ActRe in-context prompt for WebShop</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.2.1\">Webshop</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.3.1\">Instruction:</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.4.1\">i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.5.1\">and price lower than 50.00 dollars</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.6.1\">[Search]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.7.1\">Action: search[3 ounce bright citrus deodorant sensitive skin]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.8.1\">Observation:</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.9.1\">[Back to Search]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.10.1\">Page 1 (Total results: 50)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.11.1\">[Next &gt;]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.12.1\">[B078GWRC1J]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.13.1\">Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.14.1\">Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.15.1\">$10.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.16.1\">[B078GTKVXY]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.17\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.17.1\">Ginger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.18\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.18.1\">Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.19\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.19.1\">$10.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.20\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.20.1\">[B08KBVJ4XN]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.21\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.21.1\">Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.22\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.22.1\">Oil-Based Scent, 24-Hour Odor Protection, Cedar &amp; Patchouli Blend, Gentle on</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.23\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.23.1\">Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.24\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.24.1\">$15.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.25\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.25.1\">Action: click[B078GWRC1J]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.26\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.26.1\">Reason for the action: [B078GWRC1J and B078GTKVXY are bright citrus deodorant</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.27\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.27.1\">less then 50 dollars. I can check B078GWRC1J first.]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.28\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.28.1\">Observation:</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.29\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.29.1\">[Back to Search]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.30\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.30.1\">[&lt; Prev]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.31\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.31.1\">scent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.32\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.32.1\">size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2-pack)]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.33\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.33.1\">Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.34\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.34.1\">Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.35\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.35.1\">Price: $10.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.36\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.36.1\">Rating: N.A.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.37\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.37.1\">[Description]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.38\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.38.1\">[Features]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.39\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.39.1\">[Reviews]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.40\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.40.1\">[Buy Now]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.41\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.41.1\">Action: click[bright citrus]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.42\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.42.1\">Reason for the action: [For 3 ounce bottle of bright citrus deodorant for sensitive skin,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.43\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.43.1\">the item has options \u2019bright citrus\u2019 and \u20193 ounce (pack of 1)\u2019 and seems good to buy.]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.44\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.44.1\">Observation: You have clicked bright citrus.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.45\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.45.1\">Action: click[3 ounce (pack of 1)]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.46\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.46.1\">Reason for the action: [For 3 ounce bottle of bright citrus deodorant for sensitive skin,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.47\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.47.1\">the item has options \u2019bright citrus\u2019 and \u20193 ounce (pack of 1)\u2019 and seems good to buy.]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.48\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.48.1\">Observation: You have clicked 3 ounce (pack of 1).</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.49\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.49.1\">Action: click[Buy Now]</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.50\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T12.1.50.1\">Reason for the action: [For 3 ounce bottle of bright citrus deodorant for sensitive skin,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.51\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T12.1.51.1\">the item has options \u2019bright citrus\u2019 and \u20193 ounce (pack of 1)\u2019 and seems good to buy.]</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>The ActRe prompt for the WebShop tasks. ActRe inverts the causality of ReAct, and provides the posterior reason for the external action.</figcaption>\n</figure>",
            "capture": "Table 12: The ActRe prompt for the WebShop tasks. ActRe inverts the causality of ReAct, and provides the posterior reason for the external action."
        },
        "13": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T13\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T13.1\">\n<tr class=\"ltx_tr\" id=\"A3.T13.1.2\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" id=\"A3.T13.1.2.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T13.1.2.2\">Pick</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T13.1.2.3\">Clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T13.1.2.4\">Heat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T13.1.2.5\">Cool</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T13.1.2.6\">Examine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T13.1.2.7\">PickTwo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T13.1.2.8\">Total</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T13.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A3.T13.1.3.1\">Train</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T13.1.3.2\">135</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T13.1.3.3\">98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T13.1.3.4\">74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T13.1.3.5\">92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T13.1.3.6\">59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T13.1.3.7\">142</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T13.1.3.8\">600</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T13.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A3.T13.1.4.1\">Valid.</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.4.2\">12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.4.3\">10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.4.4\">7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.4.5\">14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.4.6\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T13.1.4.7\">9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.4.8\">60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T13.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A3.T13.1.1.1\">Test<sup class=\"ltx_sup\" id=\"A3.T13.1.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A3.T13.1.1.1.1.1\">\u2217</span></sup> (held out, seen)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.1.2\">35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.1.3\">27</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.1.4\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.1.5\">25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.1.6\">13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T13.1.1.7\">24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T13.1.1.8\">140</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T13.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"A3.T13.1.5.1\">Test (held out, unseen)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T13.1.5.2\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T13.1.5.3\">31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T13.1.5.4\">23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T13.1.5.5\">21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T13.1.5.6\">18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A3.T13.1.5.7\">17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T13.1.5.8\">134</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span>The statistics of tasks used in training, validation, and testing. <sup class=\"ltx_sup\" id=\"A3.T13.5.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A3.T13.5.1.1\">\u2217</span></sup>: For all experimental results except for Table\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#A3.T14\" title=\"Table 14 \u2023 C.1 AlfWorld \u2023 Appendix C Additional Experimental Results \u2023 ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we report the success rate on the 134 held-out unseen test scenarios following the settings in previous work.</figcaption>\n</figure>",
            "capture": "Table 13: The statistics of tasks used in training, validation, and testing. \u2217: For all experimental results except for Table\u00a014, we report the success rate on the 134 held-out unseen test scenarios following the settings in previous work."
        },
        "14": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T14\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A3.T14.4\" style=\"width:397.5pt;height:84.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-56.5pt,12.0pt) scale(0.778580550729324,0.778580550729324) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A3.T14.4.4\">\n<tr class=\"ltx_tr\" id=\"A3.T14.4.4.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A3.T14.4.4.5.1\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T14.4.4.5.2\">Pick</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T14.4.4.5.3\">Clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T14.4.4.5.4\">Heat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T14.4.4.5.5\">Cool</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T14.4.4.5.6\">Examine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T14.4.4.5.7\">PickTwo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T14.4.4.5.8\">Total</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T14.4.4.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A3.T14.4.4.6.1\">LM-BUTLER\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Micheli &amp; Fleuret, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#bib.bib13\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.4.4.6.2\">97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.4.4.6.3\">89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.4.4.6.4\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.4.4.6.5\">80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.4.4.6.6\">77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T14.4.4.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.4.4.6.7.1\">92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.4.4.6.8\">90</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T14.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A3.T14.1.1.1.1\">A<sup class=\"ltx_sup\" id=\"A3.T14.1.1.1.1.1\">3</sup>T (Round=0)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.1.1.1.2\">86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.1.1.1.3\">67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.1.1.1.4\">94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.1.1.1.5\">80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.1.1.1.6\">85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T14.1.1.1.7\">75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T14.1.1.1.8\">80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T14.2.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A3.T14.2.2.2.1\">A<sup class=\"ltx_sup\" id=\"A3.T14.2.2.2.1.1\">3</sup>T (Round=1)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.2.2.2.2\">91</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.2.2.2.3\">89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.2.2.2.4\">88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.2.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.2.2.2.5.1\">96</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.2.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.2.2.2.6.1\">92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T14.2.2.2.7\">67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.2.2.2.8\">87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T14.3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A3.T14.3.3.3.1\">A<sup class=\"ltx_sup\" id=\"A3.T14.3.3.3.1.1\">3</sup>T (Round=2)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.3.3.3.2\">97</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.3.3.3.3\">89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.3.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.3.3.3.4.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.3.3.3.5\">88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.3.3.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.3.3.3.6.1\">92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T14.3.3.3.7\">79</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T14.3.3.3.8\">91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T14.4.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"A3.T14.4.4.4.1\">A<sup class=\"ltx_sup\" id=\"A3.T14.4.4.4.1.1\">3</sup>T (Round=3)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T14.4.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.4.4.4.2.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T14.4.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.4.4.4.3.1\">96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T14.4.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.4.4.4.4.1\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T14.4.4.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.4.4.4.5.1\">96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T14.4.4.4.6\">77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A3.T14.4.4.4.7\">79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T14.4.4.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T14.4.4.4.8.1\">93</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 14: </span>Success rate () on each task type of AlfWorld, with a single trial on each of the 140 in-distribution test scenarios. Our agents outperform LM-BUTLER on this test split.</figcaption>\n</figure>",
            "capture": "Table 14: Success rate () on each task type of AlfWorld, with a single trial on each of the 140 in-distribution test scenarios. Our agents outperform LM-BUTLER on this test split."
        },
        "15": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T15\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T15.4\">\n<tr class=\"ltx_tr\" id=\"A3.T15.4.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A3.T15.4.5.1\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T15.4.5.2\">Pick</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T15.4.5.3\">Clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T15.4.5.4\">Heat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T15.4.5.5\">Cool</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T15.4.5.6\">Examine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T15.4.5.7\">PickTwo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T15.4.5.8\">Total</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T15.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A3.T15.1.1.1\">A<sup class=\"ltx_sup\" id=\"A3.T15.1.1.1.1\">3</sup>T (Round=0)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T15.1.1.2\">11/12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T15.1.1.3\">\u00a0\u00a09/10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T15.1.1.4\">5/7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T15.1.1.5\">11/14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T15.1.1.6\">7/8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T15.1.1.7\">9/9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T15.1.1.8\">52/60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T15.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A3.T15.2.2.1\">A<sup class=\"ltx_sup\" id=\"A3.T15.2.2.1.1\">3</sup>T (Round=1)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.2.2.2\">11/12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.2.2.3\">10/10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.2.2.4\">6/7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.2.2.5\">13/14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.2.2.6\">7/8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T15.2.2.7\">8/9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.2.2.8\">55/60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T15.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A3.T15.3.3.1\">A<sup class=\"ltx_sup\" id=\"A3.T15.3.3.1.1\">3</sup>T (Round=2)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.3.3.2\">12/12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.3.3.3\">10/10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.3.3.4\">7/7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.3.3.5\">13/14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.3.3.6\">8/8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T15.3.3.7\">8/9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T15.3.3.8\">58/60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T15.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"A3.T15.4.4.1\">A<sup class=\"ltx_sup\" id=\"A3.T15.4.4.1.1\">3</sup>T (Round=3)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T15.4.4.2\">12/12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T15.4.4.3\">10/10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T15.4.4.4\">7/7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T15.4.4.5\">12/14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T15.4.4.6\">8/8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A3.T15.4.4.7\">7/9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T15.4.4.8\">56/60</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 15: </span>The number of our 1-shot successful / all tasks of each type in our validation split on AlfWorld. </figcaption>\n</figure>",
            "capture": "Table 15: The number of our 1-shot successful / all tasks of each type in our validation split on AlfWorld. "
        },
        "16": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T16\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T16.3\">\n<tr class=\"ltx_tr\" id=\"A3.T16.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T16.3.1.1\">Round</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T16.3.1.2\">#Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T16.3.1.3\">#Failed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T16.3.1.4\">#Failed/#Total (%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T16.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T16.3.2.1\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T16.3.2.2\">2,130</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T16.3.2.3\">\u00a0\u00a0\u00a0\u00a00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T16.3.2.4\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T16.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T16.3.3.1\">1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T16.3.3.2\">2,669</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T16.3.3.3\">\u00a0\u00a099</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T16.3.3.4\">3.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T16.3.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T16.3.4.1\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T16.3.4.2\">4,066</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T16.3.4.3\">177</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T16.3.4.4\">4.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T16.3.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A3.T16.3.5.1\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T16.3.5.2\">4,995</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T16.3.5.3\">219</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T16.3.5.4\">4.3</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 16: </span>The sentence statistics of the training datasets for LLM finetuning in each round of our trajectory collection and self-training on AlfWorld. The training set contains 0 sentences with  weights for Round 0, as Round 0 performs supervised fine-tuning with ReAct prompting trajectories as bootstrapping. The details of trajectory collection are covered in Appendix\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#A1\" title=\"Appendix A Implementation Details \u2023 ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</figcaption>\n</figure>",
            "capture": "Table 16: The sentence statistics of the training datasets for LLM finetuning in each round of our trajectory collection and self-training on AlfWorld. The training set contains 0 sentences with  weights for Round 0, as Round 0 performs supervised fine-tuning with ReAct prompting trajectories as bootstrapping. The details of trajectory collection are covered in Appendix\u00a0A."
        },
        "17": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T17\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T17.1\">\n<tr class=\"ltx_tr\" id=\"A3.T17.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T17.1.1.1\">Round</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T17.1.1.2\">#Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T17.1.1.3\">#Failed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T17.1.1.4\">#Failed/#Total (%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T17.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T17.1.2.1\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T17.1.2.2\">\u00a0\u00a0\u00a0981</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T17.1.2.3\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T17.1.2.4\">\u00a0\u00a00.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T17.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T17.1.3.1\">1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T17.1.3.2\">3,431</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T17.1.3.3\">\u00a0\u00a0\u00a0336</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T17.1.3.4\">\u00a0\u00a09.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T17.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T17.1.4.1\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T17.1.4.2\">5,719</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T17.1.4.3\">\u00a0\u00a0\u00a0694</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T17.1.4.4\">12.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T17.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A3.T17.1.5.1\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T17.1.5.2\">8,550</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T17.1.5.3\">1,122</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T17.1.5.4\">13.1</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 17: </span>The sentence statistics of the training datasets for Mistral-7B-Instruct-v0.2 finetuning in each round of our trajectory collection and self-training on WebShop. Round 0 corresponds with supervised fine-tuning with ReAct trajectories, and thus uses no failed sentences. The details of trajectory collection are covered in Appendix\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.14589v3#A1\" title=\"Appendix A Implementation Details \u2023 ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</figcaption>\n</figure>",
            "capture": "Table 17: The sentence statistics of the training datasets for Mistral-7B-Instruct-v0.2 finetuning in each round of our trajectory collection and self-training on WebShop. Round 0 corresponds with supervised fine-tuning with ReAct trajectories, and thus uses no failed sentences. The details of trajectory collection are covered in Appendix\u00a0A."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.14589v3_figure_1.png",
            "caption": "Figure 1: Upper: Two common paradigms to collect trajectories for language agents. (a) Trajectories are artificially annotated as human demonstrations. (b) Trajectories are gathered by deploying policy agents that reason and act in the language form. However, both paradigms require considerable human effort in either data annotation or different implementations of agent frameworks, thus lacking scalability in the data collection process. Lower: (c) Our A3T framework. A3T enables the Autonomous Annotation of Agent Trajectories in ReAct style with an ActRe agent, facilitating the closed loop of contrastive self-training."
        },
        "2": {
            "figure_path": "2403.14589v3_figure_2.png",
            "caption": "Figure 2: Trajectory comparison on the 518-th task of WebShop. (a) the failed trajectory by the trained agent at the 0-th Round; (b) the composed trajectory assisted with ActRe. The trained policy agent fails to choose the correct option in the item content page. Success is obtained by clicking \u201c[white | primary]\u201d, and ActRe annotates for the sampled action."
        },
        "3": {
            "figure_path": "2403.14589v3_figure_3.png",
            "caption": "Figure 3: Trajectory comparison on the 512-th task of WebShop. (a) the failed trajectory by the trained agent at the 0-th Round; (b) the annotated trajectory assisted with ActRe. The action of clicking the 7-th item \u201c[B09QCYNF9R]\u201d is explored in the new trajectory, and the reason highlighted in green is generated by ActRe."
        },
        "4": {
            "figure_path": "2403.14589v3_figure_4.png",
            "caption": "Figure 4: Trajectory comparison on the 754-th task of WebShop. (a) the failed trajectory by the trained agent at the 0-th Round; (b) the annotated trajectory assisted with ActRe. The trajectory (b) is continued in Fig. 5."
        },
        "5": {
            "figure_path": "2403.14589v3_figure_5.png",
            "caption": "Figure 5: The second part of the composed trajectory on the 754-th task of WebShop. This case demonstrates some sophisticated behavior achieved by the synergy of the ReAct-style policy agent and the ActRe prompting agent. The policy agent alone terminates the trajectory by directing purchasing the first item ([B00QB6TW9C]) on the first page, obtaining an imperfect reward of 0.6667. By the randomly sampling actions and querying the ActRe agent to synthesize the reasons, the annotated trajectory shows that the policy agent initially clicks the first item on Page 1 as well, but then goes back to seek for other options (the reason in Act 3 highlighted in green is the first synthetic reason by ActRe). Then the agent clicks the 9-th item ([B07CYD69J9]) on Page 1, but then goes back and clicks the 1-st item, and returns to the search results of Page 1 once again. After that, the agent chooses to go to the next page, and then selects the 4-th item ([B07JCHPH1H]) and finally makes a purchase. This item results in a perfect match with the provided instruction."
        }
    },
    "references": [
        {
            "1": {
                "title": "Rest meets react: Self-improvement for multi-step reasoning llm agent.",
                "author": "Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, et al.",
                "venue": "arXiv preprint arXiv:2312.10003, 2023.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Fireact: Toward language agent fine-tuning.",
                "author": "Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao.",
                "venue": "arXiv preprint arXiv:2310.05915, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Self-play fine-tuning converts weak language models to strong language models, 2024.",
                "author": "Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu.",
                "venue": null,
                "url": null
            }
        },
        {
            "4": {
                "title": "Qlora: Efficient finetuning of quantized llms.",
                "author": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.",
                "venue": "arXiv preprint arXiv:2305.14314, 2023.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Multimodal web navigation with instruction-finetuned foundation models.",
                "author": "Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2024.",
                "url": null
            }
        },
        {
            "6": {
                "title": "V-star: Training verifiers for self-taught reasoners.",
                "author": "Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal.",
                "venue": "arXiv preprint arXiv:2402.06457, 2024.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Mistral 7b.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.",
                "venue": "arXiv preprint arXiv:2310.06825, 2023.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Mixtral of experts.",
                "author": "Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.",
                "venue": "arXiv preprint arXiv:2401.04088, 2024.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback.",
                "author": "Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.",
                "venue": "arXiv preprint arXiv:2309.00267, 2023.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Reason for future, act for now: A principled framework for autonomous llm agents with provable sample efficiency.",
                "author": "Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, and Zhaoran Wang.",
                "venue": "arXiv preprint arXiv:2309.17382, 2023.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Agentlite: A lightweight library for building and advancing task-oriented llm agent system.",
                "author": "Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al.",
                "venue": "arXiv preprint arXiv:2402.15538, 2024.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Agentboard: An analytical evaluation board of multi-turn llm agents.",
                "author": "Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He.",
                "venue": "arXiv preprint arXiv:2401.13178, 2024.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Language models are few-shot butlers.",
                "author": "Vincent Micheli and Francois Fleuret.",
                "venue": "In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.  9312\u20139318, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Large language models as general pattern machines.",
                "author": "Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng.",
                "venue": "In Proceedings of the 7th Conference on Robot Learning (CoRL), 2023.",
                "url": null
            }
        },
        {
            "15": {
                "title": "GPT-4 technical report.",
                "author": "OpenAI.",
                "venue": "CoRR, abs/2303.08774, 2023.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.",
                "venue": "Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Toolllm: Facilitating large language models to master 16000+ real-world apis.",
                "author": "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al.",
                "venue": "arXiv preprint arXiv:2307.16789, 2023.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Direct preference optimization: Your language model is secretly a reward model.",
                "author": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.",
                "venue": "Advances in Neural Information Processing Systems, 2023.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Reflexion: language agents with verbal reinforcement learning.",
                "author": "Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao.",
                "venue": "In Thirty-seventh Conference on Neural Information Processing Systems, 2023.",
                "url": null
            }
        },
        {
            "20": {
                "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning.",
                "author": "Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht.",
                "venue": "In Proceedings of the International Conference on Learning Representations (ICLR), 2021.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Beyond human data: Scaling self-training for problem-solving with language models.",
                "author": "Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al.",
                "venue": "arXiv preprint arXiv:2312.06585, 2023.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Trial and error: Exploration-based trajectory optimization for llm agents.",
                "author": "Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin.",
                "venue": "arXiv preprint arXiv:2403.02502, 2024.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Gemini: a family of highly capable multimodal models.",
                "author": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.",
                "venue": "arXiv preprint arXiv:2312.11805, 2023.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.",
                "venue": "arXiv preprint arXiv:2307.09288, 2023.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Openchat: Advancing open-source language models with mixed-quality data.",
                "author": "Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu.",
                "venue": "In The Twelfth International Conference on Learning Representations, 2024.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Voyager: An open-ended embodied agent with large language models.",
                "author": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.",
                "venue": "arXiv preprint arXiv: Arxiv-2305.16291, 2023.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
                "author": "Ronald J Williams.",
                "venue": "Machine learning, 8:229\u2013256, 1992.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Os-copilot: Towards generalist computer agents with self-improvement, 2024.",
                "author": "Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong.",
                "venue": null,
                "url": null
            }
        },
        {
            "29": {
                "title": "Towards unified alignment between agents, humans, and environment.",
                "author": "Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang, Qingyuan Hu, Xinrui Chen, Zhenhe Zhang, Fuwen Luo, Zhicheng Guo, Peng Li, and Yang Liu.",
                "venue": "arXiv preprint arXiv:2402.07744, 2024.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Webshop: Towards scalable real-world web interaction with grounded language agents.",
                "author": "Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.",
                "venue": "In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp.  20744\u201320757. Curran Associates, Inc., 2022.",
                "url": null
            }
        },
        {
            "31": {
                "title": "React: Synergizing reasoning and acting in language models.",
                "author": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao.",
                "venue": "In The Eleventh International Conference on Learning Representations, 2023.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Agent lumos: Unified and modular training for open-source language agents.",
                "author": "Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin.",
                "venue": "arXiv preprint arXiv:2311.05657, 2023.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Self-rewarding language models.",
                "author": "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.",
                "venue": "arXiv preprint arXiv:2401.10020, 2024.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Agenttuning: Enabling generalized agent abilities for llms.",
                "author": "Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang.",
                "venue": "arXiv preprint arXiv:2310.12823, 2023.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Agentohana: Design unified data and training pipeline for effective agent learning.",
                "author": "Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al.",
                "venue": "arXiv preprint arXiv:2402.15506, 2024.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Gpt-4v(ision) is a generalist web agent, if grounded.",
                "author": "Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.",
                "venue": "arXiv preprint arXiv:2401.01614, 2024.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Language agent tree search unifies reasoning acting and planning in language models, 2023a.",
                "author": "Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang.",
                "venue": null,
                "url": null
            }
        },
        {
            "38": {
                "title": "Lima: Less is more for alignment.",
                "author": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.",
                "venue": "Advances in Neural Information Processing Systems, 36, 2023b.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Archer: Training language model agents via hierarchical multi-turn rl.",
                "author": "Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar.",
                "venue": "arXiv preprint arXiv:2402.19446, 2024.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.14589v3",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.14589v3",
        "paper_title": "ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy",
        "research_background": "### Paper's Motivation\nThe primary motivation behind the paper \"ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy\" is to address the limitations and scalability issues in the current methods of training language agents. With the rapid advancements in Large Language Models (LLMs) and their proven capabilities in diverse decision-making scenarios, it becomes crucial to develop methods for training these agents autonomously and efficiently, without excessive human intervention or reliance on proprietary systems. The authors aim to enable language agents to autonomously gather high-quality training trajectories, specifically including textual annotations, which are suitable for further self-improvement and fine-tuning.\n\n### Research Problem\nThe research problem the paper addresses is whether a language agent can autonomously collect high-quality, diverse trajectories containing textual annotations that are beneficial for subsequent training and self-improvement without extensive human supervision. The current methods face challenges such as:\n1. **Scalability Issues**: Expert demonstrations are costly and labor-intensive.\n2. **Limited Diversity**: Existing agent frameworks and prompting schemes limit the exploration coverage and diversity of the collected training data.\n3. **Data Collection Efforts**: Implementing diverse agent frameworks requires significant human efforts and proprietary language models.\n4. **Incomplete Trajectories**: Some approaches do not include textual rationales essential for guiding better behavior in language agents.\n\n### Relevant Prior Work\n1. **Training with Open-Sourced LLMs**: Efforts have been made to train language agents using open-sourced LLMs by gathering multi-step trajectories that contain environmental observations, internal reasoning texts, and external actions. However, these methods are either dependent on human-annotated data or limited prompting schemes, thereby lacking scalability and diversity (Yin et al., 2023; Zhang et al., 2024).\n2. **Expert Demonstrations**: Leveraging expert demonstrations to gather training data (Yao et al., 2022) has shown promise, but it is hampered by the high cost of human labor.\n3. **Agent Frameworks with Proprietary LLMs**: Different agent frameworks have been proposed to gather diverse trajectories using proprietary language models (Qin et al., 2023; Zeng et al., 2023; Chen et al., 2023; Aksitov et al., 2023). However, these approaches are not easily scalable and still limit the exploration coverage.\n4. **Exploration-based Frameworks**: Song et al. (2024) proposed an exploration-based framework for self-improvement, but it lacks the textual rationales in the gathered trajectories that are necessary for the effective training of language agents.\n5. **Unified Data Formats**: Proposals for unified data formats (Yin et al., 2023; Zhang et al., 2024) have tried to standardize the collection process but have not overcome the limitations regarding the diversity and scalability of training trajectories.\n\nThe paper proposes a novel approach, A3T, that combines the advantages of these existing methods while addressing their limitations to enable autonomous data collection and self-training of language agents.",
        "methodology": "### Methodology: Closed-Loop Self-Improvement for Agents using the A3T Framework\n\nIn this section, we introduce the proposed closed-loop self-improvement methodology for agents facilitated by the A3T framework. This methodology is divided into two main components: autonomous trajectory annotation and contrastive self-training.\n\n1. **Autonomous Trajectory Annotation with ActRe Agent**\n\n    The first part of the loop involves the autonomous trajectory annotation process carried out by the ActRe agent. This agent is responsible for generating labeled data, where it annotates its own actions and the resulting states, forming trajectories that can be utilized for further training. By leveraging autonomy in data annotation, the ActRe agent aids in creating a rich dataset that accurately reflects the dynamics of the environment and potential actions that can be taken.\n\n2. **Contrastive Self-Training Process**\n\n    In the second part of the loop, the contrastive self-training process is employed. During this phase, the agent uses the trajectories annotated by the ActRe agent to train itself through a contrastive learning approach. This process emphasizes learning by comparing and contrasting different trajectories, focusing on improving action recognition and decision-making capabilities based on the annotated data.\n\nTogether, these two parts form a closed-loop system where the agent continuously improves by autonomously generating and utilizing its training data. This approach reduces dependence on externally labeled data and provides a robust mechanism for ongoing learning and adaptation within dynamic environments. The integration of autonomous trajectory annotation and contrastive self-training underscores the innovative aspect of this methodology, promoting self-sufficiency and enhancing the overall performance of the language agents.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Benchmarks:**\nWe conduct experiments on two benchmarks to validate the effectiveness of A3T:\n1. **AlfWorld** (Shridhar et al., 2021): A textual embodied environment.\n2. **WebShop** (Yao et al., 2022): An online shopping environment.\n\nBoth benchmarks require a language agent to perform multi-step decision-making to accomplish a specified goal introduced in each task.\n\n**Training Procedure:**\n- **Rounds:** The training loop consists of 4 rounds of trajectory collection and agent training.\n- **Initial Round:** Uses ReAct prompting for the bootstrap of training data.\n- **No Test Data for Training:** No trajectories are gathered from testing tasks for training.\n- **Implementation:** gpt-3.5-turbo-instruct-0914 is used for the initial ReAct prompting and the subsequent ActRe prompting agent for trajectory composition in the later rounds.\n- **Model for Training:** Open-sourced Mistral-7B-Instruct-v0.2 with QLoRA finetuning for the training experiments.\n\n**Baselines:**\nWe compare our A3T framework with several strong baselines, including:\n- Advanced prompting frameworks using GPT-4.\n- Specialized LLMs with full fine-tuning.\n- gpt-3.5-turbo-1106 fine-tuning.\n\n**Results:**\nThe results of our experiments, as compared to the baselines, are reported in the following sections of the study.\n\nThis setup and evaluation aim to showcase the performance of A3T in different scenarios requiring multi-step decision-making, thus illustrating its effectiveness in adaptive training and decision-making autonomy in language agents."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To study the effect of different reward thresholds for training trajectory filtering and the impact of policy gradient techniques with binarized rewards on the A3T framework's performance.",
            "experiment_process": "Conduct experiments on WebShop, which provides a real-valued reward from 0 to 1. Compare the performance of the initial round of supervised ReAct fine-tuning with different reward thresholds (r >= 0.75, r >= 0.9, r >= 1). Ablate the policy gradient technique with binarized rewards. Implement supervised training with only successful trajectories (r = 1) and prepend trajectories with 'Success'/'Fail' labels. Compare policy gradient settings using the original WebShop reward, the binarized reward (r = 1), and relaxed reward thresholds (r >= 0.75). Evaluate and compare task performance.",
            "result_discussion": "Policy gradient methods lead to higher task performance promotion than supervised training, resembling the effectiveness of RLHF on top of supervised fine-tuning. Binarized rewards (r = 1) used in A3T significantly improve the success rate, indicating the advantage of this method. Advanced RL and RLAIF algorithms' incorporation into A3T is left for future work.",
            "ablation_id": "2403.14589v3.No1"
        },
        {
            "research_objective": "To validate and compare the performance of A3T using QLoRA finetuning with Mistral-7B-Instruct-v0.2 and the proprietary gpt-3.5-turbo-1106 finetuning.",
            "experiment_process": "Conduct initial Round-0 training in A3T using ReAct prompting with gpt-3.5-turbo-instruct-0914 for both Mistral-7B-Instruct-v0.2 and gpt-3.5-turbo-1106, ensuring the same starting point. Compare performance in AlfWorld and WebShop. Compose diverse trajectories separately for each failed training task (20 for gpt-3.5-turbo-1106 and 100 for open-sourced LLM). Evaluate and compare accumulated trajectory quality and test success rates after Round-1 self-training.",
            "result_discussion": "In AlfWorld, QLoRA fine-tuned Mistral-7B-Instruct-v0.2 surpasses the proprietary gpt-3.5-turbo-1106 finetuning. In WebShop, the proprietary gpt-3.5-turbo-1106 performs better in Round-0 supervised training, but the open-sourced model achieves a higher test success rate after Round-1 self-training. This suggests the importance of contrastive fine-tuning in A3T and implies that the proprietary service's supervised fine-tuning might limit further performance improvement.",
            "ablation_id": "2403.14589v3.No2"
        }
    ]
}