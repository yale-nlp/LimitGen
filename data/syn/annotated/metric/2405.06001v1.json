{
    "title": "LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) are propelling us toward artificial general intelligence, thanks to their remarkable emergent abilities and reasoning capabilities. However, the substantial computational and memory requirements of LLMs limit their widespread adoption. Quantization, a key compression technique, offers a viable solution to mitigate these demands by compressing and accelerating LLMs. Numerous studies have aimed to refine the quantization processes. However, the quantization configurations in these studies vary and may not be optimized for hardware compatibility. In this paper, we focus on identifying the most effective practices for quantizing LLMs, with the goal of balancing performance with computational efficiency. For a fair analysis, we develop a quantization toolkit LLMC, and design four crucial principles considering the inference efficiency, calibration cost, and modularization. By benchmarking on various models and datasets with over 500 experiments, three takeaways corresponding to calibration data, quantization algorithm, and quantization schemes are derived. Finally, a best practice of LLM PTQ pipeline is constructed. All the benchmark results and the toolkit can be found at https://github.com/ModelTC/llmc.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, large Language models (LLMs) such as GPT-4 (OpenAI et al., 2024) have demonstrated unprecedented generative capabilities in the field of natural language processing (NLP), and achieving widespread applications across various industries. However, their substantial computational and storage costs have impeded their further popularization among users. For instance, BLOOM (Touvron et al., 2023), an open-access multilingual LLM with 176 billion parameters, requires a minimum of 350 GB of space merely to store model weights in full-precision (FP16) format. At a minimum, it requires 580GB A100 or 940GB A800 NVIDIA GPUs to perform inference with this model. Therefore, reducing their serving cost is paramount to further enhance the application of LLMs.\n\nTo address this challenge, model quantization (Nagel et al., 2021) can be an effective resolution strategy. It maps weights and/or activations to a lower-bit data format to reduce memory footprints and accelerate model inference. Existing quantization approaches can be categorized into two types: quantization-aware-training (QAT) (Bhalgat et al., 2020; Gong et al., 2019; Esser et al., 2020; Egiazarian et al., 2024; van Baalen et al., 2024) and post-training quantization (PTQ) (Wei et al., 2023a; Jhunjhunwala et al., 2021; Li et al., 2021). Although with prominent high performance, the necessity for QAT to undergo fine-tuning or retraining with substantial training data and training cost renders it unattainable for the majority of users. Correspondingly, PTQ compresses models without retraining, making it a preferred method for LLMs due to its minimal resource requirements. \n\nCurrent uniform PTQ methods always evaluate across distinct datasets in different quantization configurations and with simulated quantization. This current state would lead to users being unable to accurately assess the configurations that should be selected for the efficient and accurate quantization of LLMs. To provide a comprehensive quantization options menu for users to obtain hardware-friendly quantized LLMs with high performance, we make a fair benchmark, which considers two aspects: factors influencing LLM quantization and inference efficiency under our design principles. The former perspective encompassed three dimensions, e.g., calibration data, algorithm, and target bits. Consequently, we evaluate across various kinds of tasks and find our best practice, encapsulated within an end-to-end pipeline that realizes both high efficiency and accuracy LLM quantization. This best practice has been integrated into our quantization toolkit, LLMC. Notably, LLMC, a user-friendly, plug-and-play quantization tool, incorporates dozens of outstanding PTQ algorithms, provides the freedom to select quantization strategies, and also supports deploying quantized LLMs on different inference backends (TensorRT-LLM (Nvidia, 2023), PPL-LLM (OpenPPL, 2023), LightLLM (ModelTC, 2023)) and hardware (Nvidia GPU, Qualcomm mobile chips, TPU). \n\nOur main contributions can be described as follows:\n\nWe release a quantization toolkit LLMC supporting dozens of algorithms, models and hardware. LLMC enables users to perform lossless quantization on 100-billion-parameter LLMs within a matter of hours, utilizing just a single GPU. It notably facilitates the research and production of quantized LLMs.\n\nBased on our findings, a best practice of LLM PTQ pipeline is designed, achieving the best performance balance under various scenarios."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Benchmark Overview",
            "text": "In this section, we first provide our benchmark\u2019s design principles, outlining its primary objective. We then exhibit our plug-and-play toolkit within our benchmark."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Design Principles",
            "text": "Our benchmark focuses on four essential aspects for effective and practical LLM quantization: inference performance, calibration cost, and modularization.\n\nInference Performance: In our LLM quantization benchmark, we prioritize the importance of selecting a quantization approach that enhances inference performance. This means our chosen setting should either increase throughput or decrease memory requirements, thereby optimizing the efficiency of the model during the inference phase.\n\nCalibration Cost: The process of post-training quantization for LLMs is also named as calibration. The resources and time invested in calibration for LLM are crucial factors that affect the practicality of LLM quantization. This benchmark aims to find the best pipeline to produce accurate LLMs in minimal GPUs and time.\n\nModularization: Recent advancements have introduced a myriad of algorithms aimed at enhancing the performance of quantized LLMs. This benchmark seeks to dissect these algorithms to their most fundamental elements, analyzing the efficacy of each component in isolation.\n\nGuided by the aforementioned principles, our goal is to investigate and outline optimal practices for developing quantized LLMs tailored to various scenarios and configurations."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "LLM Quantization",
            "text": "Factors Influencing LLM Quantization. We categorize factors influencing LLM quantization into three dimensions: calibration data, algorithms, and target bits.\n\nCalibration data: Calibration data can help to evaluate the range of tensors, and then determine the quantization parameters, which is crucial for maintaining model performance post-quantization. Based on that, the impact of different corpora as calibration data warrants further investigation.\n\nAlgorithm: Efficient remedies to help maintain model performance make a lot of sense. Current effective and efficient algorithms can be summarized into three types:\n1) Transformation (Xiao et al., 2023; Lin et al., 2023; Shao et al., 2023; Wei et al., 2023b): Leveraging magnitude between weight and activation before quantization is widely used to balance quantization errors.\n2) Clipping (Lin et al., 2023; Shao et al., 2023; Wei et al., 2022; Du et al., 2024): Clipping some outliers with minimal impact in weights before quantization can help with range estimation and the representation of the rest in calibration.\n3) Reconstruction (Frantar et al., 2022; Lee et al., 2023; Dettmers et al., 2023): This approach employs the Hessian matrix to evaluate the quantization perturbations, and update the rest intact elements. This process is conducted incrementally during the quantization process.\n\nTarget bits: The bit adopted for weight, activation, and KV cache impacts the final accuracy. Usually, the hardware-friendly bits are 2-bit, 4-bit, and 8-bit. In this benchmark, we also investigate 3-bit or 6-bit to compare the potential of quantization algorithms. But for the practical deployment, 2/4/8-bit is mainly used."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Quantization Toolkit",
            "text": "To achieve the modular comparison of the different quantization dimensions aforementioned, and to consolidate best practices into an end-to-end pipeline, we have designed and developed a quantization toolkit named LLMC. This toolkit is capable of accommodating multiple quantization configurations using a variety of algorithmic techniques. The models produced by LLMC are designed for seamless deployment across a diverse range of hardware platforms. Presently, LLMC supports over ten algorithms, is compatible with over eight models, is flexible to extend the support of any transformer-based LLMs, and facilitates deployment on three types of inference engines including LightLLM, TensorRT-LLM, and PPL-LLM."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LLM-QBench",
            "text": "Under the principles in subsection 2.1, powered by our quantization toolkit LLMC, in this section, we explore the best practice for quantizing large language models from the aspect of calibration data and quantization algorithm."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Settings",
            "text": "We first illustrate our experiment settings, more details can be found in the subsection A.1.\n\nModels. To demonstrate the generability of our benchmark, we assess performance on LLAMA-2 family, spanning model sizes from 7B to 70B for general language tasks. To broaden the scope of our evaluation benchmarks, we also benchmark on ChatGLM for long context abilities, CodeLLAMA for coding tasks, and WizardMath for mathematical problems.\n\nDatasets. We categorize the datasets into upstream datasets and downstream datasets. For the upstream datasets, we employ WikiText2 and C4 dataset with the perplexity metric for evaluation, since perplexity can stably reflect the LLM\u2019s performance. For the downstream tasks, we select examination tasks including MMLU and ARC-e, knowledge task BoolQ, understanding task Lambada, reasoning tasks including PIQA, HellaSwag and GSM8K, coding tasks HumanEval and MBPP, and the long context evaluation LongBench.\n\nHardware. In this paper, we mainly measured the inference efficiency of low-bit kernel on NVIDIA server and edge GPUs with NVIDIA\u2019s TensorRT-LLM framework."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Impact of Calibration Data",
            "text": "It is evident that calibration data affects all algorithms. To attain optimal accuracy, it is crucial to gather domain-specific data for domain-specific models and collect diverse data for the general models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Quantization Algorithm",
            "text": "Following the principles of modularization, we deconstruct the techniques behind existing algorithms. Through a comprehensive and unbiased experimental comparison, we aim to derive insights critical for developing an optimally combined pipeline.\n\nClipping. Searching for the clipping value asymmetrically is the most effective strategy for optimizing outcomes. This indicates that selecting an appropriate weight range can significantly reduce errors. Therefore, this strategy is adopted for various scenarios. Clipping should be fully utilized in the pipeline of best practices. Moreover, initialized from the asymmetric clipping, further learning can enhance performance. This effective initialization contributes to a fast convergence.\n\nReconstruction. GPTQ (Frantar et al., 2022) reconstruction involves the non-equivalent transformation of weights on the channel dimension, hindering simultaneous optimization of weights and clip values. Pre-reconstruction weight clipping yields suboptimal results due to weight changes. If reconstruction precedes clip value search, initial parameters won\u2019t align with the updated ones. Moreover, when paired with an equivalent transformation, it yields minimal benefits. This limitation may stem from the alteration of gradients and the disruption of assumptions regarding Hessian information. Furthermore, it requires an extended calibration period. Therefore, reconstruction may not be considered a best practice.\n\nTransformation. The transformation technique utilizes a linear operation to reduce the outlier problem or preserve important weights. For both scenarios, such equivalent transformation brings an improvement, especially for activations. From the data, we can infer that manually setting the scaling number is rigid and may not help in all situations. On the contrary, a suitable search for the transformation scale is effective. Different search strategies significantly improve outcomes. A learning process can be further adopted with a pre-searched range. Fortunately, with fast pre-search support, calibration can achieve learning with fewer epochs.\n\nCalibration cost for each strategy. Among transformation techniques, the search-based (v1) strategy requires roughly 10 minutes, being twice as fast as the (v2) strategy. While rule-based transformations are quicker, they often fall short of achieving acceptable levels. Learning-based transformation methods incur a considerable increase in time to reach satisfactory levels. However, initializing the learning process with pre-searched values can reduce the number of required epochs by half and yield higher outcomes. Direct min-max value clipping is time-efficient but typically results in significant losses. The search-based clipping method, whether using asymmetric or symmetric ranges, proves efficient, requiring only about 20 minutes. When applying a learning-based approach to clipping, the calibration time can extend to nearly 7 hours. Therefore, a combined approach of the search-based transformation v1 and search-based asymmetric clipping emerges as the most effective in balancing efficiency. Furthermore, initiating with pre-searched values and conducting additional learning for a few epochs may offer further improvements."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Target Bits",
            "text": "Fixed-precision. In contrast, quantization of the Key-Value (KV) cache is proposed as a method to decrease memory usage. In Table 21 ###reference_### and Table 5 ###reference_###, we assessed the accuracy impact of 4-bit (per-group quantization with a group size of 8), and 8-bit (per-tensor) KV cache quantization. The results indicate that 4-bit KV cache quantization, with its finer granularity, performs comparably to 8-bit KV cache quantization with a coarser group size. Both the 4-bit and 8-bit configurations closely approximate the performance of FP16 at the code generation task and long-context understanding task. Hence, for KV cache quantization, a 4-bit per-group approach with a group size of 8 is recommended. \n\nInference Speed. To assess the practical benefits of different quantization approaches, we conducted evaluations using NVIDIA\u2019s cloud (SMX 80G A100) and edge (Drive Orin) GPUs, alongside the official inference library, TensorRT-LLM. Part of our results, as depicted in Figure 2 ###reference_###, highlight the throughput improvements achieved through TensorRT-LLM-supported quantization schemes for models with 32,000 input tokens and 512 output tokens. The findings indicate that quantization with 8-bit weights and activations enhances the prefill stage\u2019s speed by 20%-30% and the decode stage by 40%-60%. It\u2019s important to note that these acceleration rates tend to diminish for larger models. Besides, 8-bit KV cache quantization has minimal impact on prefill times and slightly reduces decoding throughput for very large models, such as those with 70B model. Results for more models and hardware can be found in subsection A.5 ###reference_###. ###figure_2### ###figure_3###"
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Best Practice of LLM PTQ pipeline",
            "text": "Based on the takeaways distilled from the above exploration, we summarize the best practice of PTQ pipeline for LLM. As depicted in Figure 3, first, we should collect the best calibration data according to the task and model under the guide of Takeaway 1. The calibration process can then be conducted using the algorithm pipeline based on Takeaway 2. The results in Table 6 and Table 7 of general-purpose model LLAMA-2-70B and specific-domain code model CodeLLAMA-7b and math model WizardMath-7b proved the effectiveness, especially for maintaining high accuracy. More experimental results on other models and datasets to validate our best practice for decent performance and efficient inference can be found in subsection A.3."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we have undertaken a comprehensive benchmarking of decomposed quantization techniques for large language models (LLMs), leading to the identification of best practices that balance calibration costs and efficiency. Furthermore, we introduce LLMC, a toolkit designed to empower the research and development community. Models optimized through our recommended practices and toolkit are readily deployable across a variety of hardware platforms, enhancing accessibility and applicability in diverse computational environments."
        }
    ],
    "url": "http://arxiv.org/html/2405.06001v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.3",
            "3.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.06001v1",
        "paper_title": "LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models",
        "research_background": "### LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models\n\n**Motivation:**\n\nThe motivation for this paper stems from the expanding deployment of large language models (LLMs) in natural language processing tasks due to their remarkable generative capabilities. Despite their potential, the excessive computational and storage costs involved in utilizing LLMs pose a significant barrier to widespread adoption. For example, the BLOOM model, comprising 176 billion parameters, demands around 350 GB of storage in full-precision format and significantly more for inference on advanced GPUs. As a result, there is an urgent need to reduce the serving costs of LLMs to facilitate broader usage.\n\n**Research Problem:**\n\nThe primary research problem addressed by this paper is the challenge of optimizing the post-training quantization (PTQ) of LLMs to balance efficiency and accuracy. Existing quantization strategies, particularly quantization-aware-training (QAT), require substantial training data and computational resources, making them impractical for most users. PTQ, on the other hand, offers a promising alternative by compressing models without needing retraining, thus mitigating resource requirements. However, uniform PTQ methods often produce inconsistent results due to varying datasets and configurations. Therefore, the need arises for a comprehensive benchmark to guide users in selecting efficient and accurate quantization strategies for LLMs.\n\n**Relevant Prior Work:**\n\n1. **Quantization techniques:**\n   - Various quantization techniques reduce memory footprint and accelerate inference (Nagel et al., 2021).\n   - Quantization approaches are broadly categorized into:\n     - **QAT:** Offers high performance but is resource-intensive due to finetuning and retraining requirements (Bhalgat et al., 2020; Gong et al., 2019; Esser et al., 2020; Egiazarian et al., 2024; van Baalen et al., 2024).\n     - **PTQ:** Compresses models without retraining, favorable for LLMs due to minimal resource requirements (Wei et al., 2023a; Jhunjhunwala et al., 2021; Li et al., 2021).\n\n2. **Non-uniform vs. Uniform quantization:**\n   - **Non-uniform quantization:** Although effective, it requires complex specialized kernels, which can slow down inference speeds (Kim et al., 2024; Egiazarian et al., 2024).\n   - **Uniform quantization:** Simpler and more implementable but currently lacks standardized evaluation methods across different setups.\n\n3. **Inference overhead:**\n   - Certain approaches involve significant computational overhead during inference, which, despite good performance, is unsuitable for efficient deployment (Chee et al., 2024; Tseng et al., 2024).\n\n**Contributions:**\n- Developed the **LLMC toolkit**: A plug-and-play quantization tool that supports multiple PTQ algorithms, models, and hardware configurations allowing for efficient, lossless quantization of 100-billion-parameter LLMs with minimal resource requirements.\n- Conducted a fair and modular benchmark of quantization techniques, involving nearly 600 experiments, to derive best practices in calibration data, algorithm pipeline, and quantization configuration.\n- Designed an optimized PTQ pipeline combining high efficiency and accuracy tailored for diverse scenarios, ultimately integrated into the LLMC toolkit for ease of use.",
        "methodology": "The paper \"LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models\" proposes a methodology for quantizing large language models (LLMs). Below, the key components and innovations of the proposed method are highlighted.\n\n### Preliminary of Quantization:\nThe process of quantization for an element in a vector is defined by:\n\n- **Bounds and Bit-width**: Use upper bound \\(\\alpha\\) and lower bound \\(\\beta\\) of the vector. \\(b\\) represents the bit-width of the quantized element \\(e\\).\n- **Symmetric vs. Asymmetric Quantization**: When \\(\\beta = -\\alpha\\), the process is symmetric quantization; otherwise, it is asymmetric quantization. The paper focuses primarily on asymmetric quantization.\n- **Weight-Only Quantization**: Uses per-group quantization where weights in a group share the same scaling factor \\(s\\).\n- **Weight-Activation Quantization**: Applies per-channel quantization for weights and per-token quantization for activations.\n\n### Notation:\n- The notation \u201cwxay\u201d denotes bit-width \u201cx\u201d for weights and \u201cy\u201d for activations.\n- \u201cgz\u201d implies group size \u201cz\u201d in group-wise quantization.\n\n### Factors Influencing LLM Quantization:\nThree main dimensions influence quantization:\n\n1. **Calibration Data**: Used to evaluate the range of tensors and determine quantization parameters, essential for maintaining model performance post-quantization. Investigating different corpora for calibration data is crucial.\n   \n2. **Algorithm**:\n    - **Transformation**: Balances quantization errors by leveraging the magnitude between weight and activation before quantization using a balance factor \\(a\\).\n    - **Clipping**: Involves clipping outliers with minimal impact to improve range estimation and representation during calibration.\n    - **Reconstruction**: Uses the Hessian matrix to evaluate and adjust quantization perturbations for maintaining model integrity incrementally.\n\n3. **Target Bits**: Refers to the bit-widths adopted for weight, activation, and KV cache, which significantly influence accuracy. Although 3-bit and 6-bit quantization are explored for potential, practical deployments mainly use hardware-friendly 2-bit, 4-bit, and 8-bit quantizations.\n\n### Quantized Inference of LLM:\nThis involves targeting the Linear layers with weights, such as the Q, K, V, and O layers in self-attention modules and the Up, Gate, and Down layers in FFN modules. Three types of quantization presented provide different benefits:\n\n- **Weight-Activation Quantization**\n- **Weight-Only Quantization**\n- **KV-Cache Quantization**\n\nThese types of quantization contribute to prefill and decode latency reduction.\n\n### Summary:\nThe paper introduces a quantization framework that accounts for calibration data, algorithms, and target bit-widths to maintain model performance post quantization. Its innovative approach includes asymmetric quantization, per-group and per-channel/token quantization, and explores practical bit-widths for deployment in LLMs.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Models:** \nThe experiment evaluates the performance of various large language models to underscore the generalizability of the benchmark. Specifically, the LLAMA-2 family models are assessed, covering sizes from 7B to 70B parameters, for general language tasks. Additionally, the evaluation extends to models specialized for distinct tasks: ChatGLM for long-context abilities, CodeLLAMA for coding tasks, and WizardMath for mathematical problem-solving.\n\n**Datasets:**\nDatasets are divided into upstream and downstream categories:\n- **Upstream Datasets:** These include WikiText2 and C4 datasets, evaluated using the perplexity metric, a stable indicator of LLM performance. \n- **Downstream Datasets:** These span a variety of tasks:\n    - Examination tasks: MMLU and ARC-e\n    - Knowledge task: BoolQ\n    - Understanding task: Lambada\n    - Reasoning tasks: PIQA, HellaSwag, and GSM8K\n    - Coding tasks: HumanEval and MBPP\n    - Long context evaluation: LongBench\n\n**Hardware:**\nThe inference efficiency was measured using low-bit kernels on NVIDIA server and edge GPUs. Specifically, NVIDIA\u2019s TensorRT-LLM framework was used to evaluate the performance of quantized models.\n\n**Main Experimental Results:**\nThe main experimental results focus on demonstrating the impact of post-training quantization on model performance across different tasks and hardware setups. Here are the summarized outcomes:\n\n1. **Performance on General Language Tasks:**\n   - For LLAMA-2 models, the performance on WikiText2 and C4 datasets shows minimal degradation in perplexity after quantization, indicating the robustness of the quantization process.\n\n2. **Task-Specific Models:**\n   - ChatGLM exhibits sustained performance for long context tasks in LongBench.\n   - CodeLLAMA maintains coding task accuracy in HumanEval and MBPP.\n   - WizardMath shows negligible loss in performance for mathematical problem-solving as tested on GSM8K.\n\n3. **Downstream Tasks:**\n   - Quantized models perform consistently well on examination (MMLU and ARC-e) and reasoning tasks (PIQA, HellaSwag).\n   - The understanding task, Lambada, and the knowledge task, BoolQ, show that quantization does not significantly impair accuracy.\n\n4. **Inference Efficiency:**\n   - The studies on NVIDIA server and edge GPUs using TensorRT-LLM indicate that the quantized models significantly reduce inference latency and resource utilization while maintaining model accuracy across tasks.\n\nThe results collectively underscore the effectiveness of the proposed post-training quantization benchmark in preserving the performance of large language models across a variety of tasks while enhancing inference efficiency on modern hardware platforms."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the impact of calibration data on the accuracy of quantization for large language models (LLMs).",
            "experiment_process": "Calibration data was tested using various algorithms, comparing domain-specific data for domain-specific models and diverse data for general models. The specific experimental setup and data variations are detailed in Table 1 (reference in original paper).",
            "result_discussion": "The results highlight the significance of using domain-specific data for domain-specific models and diverse data for general models to achieve optimal accuracy after quantization.",
            "ablation_id": "2405.06001v1.No1"
        },
        {
            "research_objective": "To derive insights for developing an optimally combined quantization pipeline through an analysis and comparison of existing quantization algorithms.",
            "experiment_process": "Different quantization strategies, such as transformation, clipping, and reconstruction techniques, were evaluated on LLAMA-2 models of 7B, 13B, and 70B sizes under both weight-only and weight-activation quantization scenarios. The details of these techniques and their behavior were analyzed and summarized in Tables 2, 3, and 4 (reference in original paper).",
            "result_discussion": "The study found that asymmetric clipping is the most effective strategy for optimizing accuracy. The transformation technique, especially when using search-based scaling, showed considerable accuracy improvements. Conversely, the reconstruction technique provided limited benefits and required extended calibration periods. Combining search-based transformation (v1) and search-based asymmetric clipping emerged as the most effective strategy for balancing accuracy and efficiency, with further improvements possible by initializing from pre-searched values and conducting additional learning for a few epochs.",
            "ablation_id": "2405.06001v1.No2"
        },
        {
            "research_objective": "To evaluate the impact of different quantization bit configurations on model accuracy and efficiency.",
            "experiment_process": "Fixed-precision and mixed-precision quantization approaches were tested. For fixed-precision, 2-bit, 3-bit, 4-bit, and 8-bit quantization schemes were compared, including evaluation of Key-Value (KV) cache quantization. Mixed-precision strategies were assessed, using theoretical hardware-friendly configurations. Keep in mind the throughputs and inference speeds were measured using NVIDIA's cloud (SMX 80G A100) and edge (Drive Orin) GPUs with the TensorRT-LLM inference library. The details were presented in Tables 21, 23, 24, and Figures 2, 3 (reference in original paper).",
            "result_discussion": "The study showed significant accuracy degradation with 2-bit quantization, making it impractical. 4-bit weight-only and mixed precision of 4-bit weight with 8-bit activation (w4a8) or full 8-bit (w8a8) configurations provided better accuracy and efficiency balance. For KV cache, 4-bit per-group quantization with a group size of 8 performed comparably to 8-bit quantization and closely approximated FP16 performance. Mixed-precision quantization yielded better results, with dynamic bit allocation improving accuracy more than static strategies, despite higher computation overhead. Quantization with 8-bit weights and activations accelerated inference stages significantly, though benefits diminished for larger models.",
            "ablation_id": "2405.06001v1.No3"
        }
    ]
}