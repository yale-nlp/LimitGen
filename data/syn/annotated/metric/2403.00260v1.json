{
    "title": "Extracting Polymer Nanocomposite Samples from Full-Length Documents",
    "abstract": "This paper investigates the use of large language models (LLMs) for extracting sample lists of polymer nanocomposites (PNCs) from full-length materials science research papers. The challenge lies in the complex nature of PNC samples, which have numerous attributes scattered throughout the text. \n\nThe complexity of annotating detailed information on PNCs limits the availability of data, making conventional document-level relation extraction techniques impractical due to the challenge in creating comprehensive named entity span annotations. \n\nTo address this, we introduce a new benchmark and an evaluation technique for this task and explore different prompting strategies in a zero-shot manner. We also incorporate self-consistency to improve the performance. Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Research publications are the main source for the discovery of new materials in the field of materials science, providing a vast array of essential data. Creating structured databases from these publications enhances discovery efficiency, as evidenced by AI tools like GNoME (Merchant et al., 2023). Yet, the unstructured format of journal data complicates its extraction and use for future discoveries (Horawalavithana et al., 2022). Furthermore, the manual extraction of material details is inefficient and prone to errors, underlining the necessity for automated systems to transform this data into a structured format for better retrieval and analysis (Yang, 2022).\n\nScientific papers on polymer nanocomposites (PNCs) provide essential details on sample compositions, key to understanding their properties. PNCs, which blend polymer matrices with nanoscale fillers, are significant in materials science for their customizable mechanical, thermal, and electrical characteristics. The variety in PNCs comes from different matrix and filler combinations that modify the properties. However, extracting this data poses challenges due to its distribution across texts, figures, and tables, and the complexity of -ary relationships defining each sample.\n\nIn this paper, we construct PNCExtract, a benchmark designed for extracting PNC sample lists from scientific articles. PNCExtract focuses on the systematic extraction of -ary relations across different parts of full-length PNC articles, capturing the unique combination of matrix, filler, and composition in each sample. Many works have explored -ary relation extraction from materials science literature (Dunn et al., 2022; Song et al., 2023a, b; Xie et al., 2023; Cheung et al., 2023) and other domains (Giorgi et al., 2022). However, these studies primarily target abstracts and short texts, not addressing the challenge of extracting information from the entirety of full-length articles. PNCExtract addresses this by requiring models to process entire articles, identifying information across all sections, a challenge noted by Hira et al. (2023).\n\nCompared to other document-level information extraction (IE) datasets like SciREX (Jain et al., 2020), PubMed (Jia et al., 2019b), and NLP-TDMS (Hou et al., 2019) which also demand the analysis of entire documents for -ary relation extraction, our dataset marks the first initiative within the materials science domain. This distinction is important due to the unique challenges of IE in materials science, particularly with polymers. The field features a complex nomenclature with chemical compounds and materials having various identifiers such as systematic names, common names, trade names, and abbreviations, all with significant variability and numerous synonyms for single entities (Swain and Cole, 2016). Furthermore, there is a scarcity of annotated datasets with detailed information, which complicates the creation of effective IE models in this area.\n\nIn light of these challenges, our dataset is designed for a generative task to navigate the complexities of fully annotating entire PNC papers, which involve annotating named entity spans, coreferences, and negative examples (entity pairs without a relation). The complexity of PNC papers, due to their various entities and samples, makes manual annotation both time-consuming and prone to errors. Consequently, encoder-only models, which require extensive annotations, fall short for our purposes. \n\nWe further explore different prompting strategies, including one that aligns with the principles of Named Entity Recognition (NER) and Relation Extraction (RE) which involves a two-stage pipeline, as well as an end-to-end method to directly generate the -ary object. We find that the E2E approach works better in terms of both accuracy and efficiency.\n\nMoreover, we present a simple extension to the self-consistency technique (Wang et al., 2023b) for list-based predictions. Our findings demonstrate that this approach improves the accuracy of sample extraction. Since the extended length of articles often exceeds the context limits of some LLMs, we also explore condensing them through a dense retriever (Ni et al., 2022) to extract segments most relevant to specific queries. Our findings indicate that condensing documents generally enhances accuracy. Since existing document-level IE models (Jain et al., 2020; Zhong and Chen, 2021) are not suited for our task, we employ GPT-4 with our E2E prompting on the SciREX dataset and benchmark it against the baseline model. Our analysis shows that GPT-4, even in a zero-shot setting, outperforms the baseline models that were trained with extensive supervision.\n\nLastly, we discuss three primary challenges encountered when using LLMs for PNC sample extraction. Code for reproducing all experiments is available at https://github.com/ghazalkhalighinejad/PNCExtract."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "PNCExtract Benchmark",
            "text": "In this section, we first describe our dataset, including the problem definition, and the dataset preparation. Then we describe our evaluation method for the described task.\n\nWe curate our dataset using the NanoMine data repository (Zhao et al., 2018). NanoMine is a PNC data repository structured around an XML-based schema designed for the representation and distribution of nanocomposite materials data. The NanoMine database is manually curated using Excel templates provided to materials researchers. NanoMine database currently contains a list of full-length scholarly articles and their corresponding PNC sample lists. While NanoMine includes various subfields, our study focuses on the \u201cMaterials Composition\u201d section. This section comprehensively details the characteristics of constituent materials in nanocomposites, including the polymer matrix, filler particles, and their compositions (expressed in volume or weight fractions). The reason for this focus is that determining which sample compositions are studied in a given paper is the essential first step toward identifying and understanding more complex properties of PNCs. Out of the articles, we focus on and disregard the rest due to having inconsistent format (see Appendix A). These articles contain a total of samples. For each sample, we retain out of the total attributes in the Materials Composition of NanoMine (see Appendix B for details).\n\nDocument-level information extraction requires understanding the entire document to accurately annotate entities, their relations, and saliency. These make the annotation of scientific articles time-consuming and prone to errors. We found that NanoMine also contains errors. Given the challenge of reviewing all samples and reading through articles, we adopted a semi-automatic approach to correct samples. Specifically, for an article, we consider both the predicted and ground truth sample list of a document. Matches are classified as exact, partial, or unmatched\u2014either true samples or predictions. We then focus on re-annotating samples with the most significant differences between prediction and ground truth. This method accelerates re-annotation by directing annotators towards specific attributes and samples based on GPT-4 predictions. Following this strategy, we made three types of adjustments to the dataset: deleting samples, adding, and editing entities.\n\nOur dataset exclusively uses numerical values to represent compositions. Predictions in percentage format are thus converted to the numerical format to align with the dataset\u2019s representation.\n\nOur evaluation incorporates an attribute aggregation method. For both the \u201cMatrix\u201d and \u201cFiller\u201d categories, a prediction is considered accurate if the model successfully identifies either the chemical name or the abbreviation. For the \u201cComposition\u201d, a correct prediction may be based on either the \u201cFiller Composition Mass\u201d or the \u201cFiller Composition Volume\u201d. This approach allows for a broader assessment, capturing any correct form of attribute identification without focusing on the finer details of each attribute.\n\nThis metric employs the F score in its calculation, which proceeds in two steps. Initially, an accuracy score is computed for each pair of predicted and ground truth samples where we compute the fraction of matches in the <Matrix, Filler, Composition> trio across the two samples.\n\nThis process results in score combinations, where and represent the counts of predicted and ground truth samples. The next step involves translating these comparisons into an assignment problem within a bipartite graph. Here, one set of vertices symbolizes the ground truth samples, and the other represents the predicted samples, with edges denoting the F scores between pairs. The objective is to identify a matching that optimizes the total F score, which can be computed using the Kuhn-Munkres algorithm.\n\nOnce matching is done, we count all the correct, false positive, and false negative predicted attributes. Subsequently, we calculate the micro-average Precision, Recall, and F.\n\nFor a stricter assessment, a sample is labeled correct only if it precisely matches one in the ground truth. Predictions not in the ground truth are false positives, and missing ground truth samples are false negatives. This metric emphasizes exact match accuracy."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Definition",
            "text": "We define our dataset as , where each  is a peer-reviewed paper included in our study. Corresponding to each paper , there is an associated list of samples , comprising various PNC samples. Formally,  is defined as , where  represents the -th PNC sample in the sample list of the -th paper, and  denotes the total number of PNC samples in . Each sample  is a JSON object with six entries: Matrix Chemical Name, Matrix Chemical Abbreviation, Filler Chemical Name, Filler Chemical Abbreviation, Filler Composition Mass, and Filler Composition Volume. Table 2 presents the count of samples with each attribute marked as non-null. The primary task involves extracting a set of samples  from a given paper ."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Dataset Preparation",
            "text": "We curate our dataset using the NanoMine data repository (Zhao et al., 2018). NanoMine is a PNC data repository structured around an XML-based schema designed for the representation and distribution of nanocomposite materials data. The NanoMine database is manually curated using Excel templates provided to materials researchers. NanoMine database currently contains a list of full-length scholarly articles and their corresponding PNC sample lists. While NanoMine includes various subfields, our study focuses on the \u201cMaterials Composition\u201d section. This section comprehensively details the characteristics of constituent materials in nanocomposites, including the polymer matrix, filler particles, and their compositions (expressed in volume or weight fractions). The reason for this focus is that determining which sample compositions are studied in a given paper is the essential first step toward identifying and understanding more complex properties of PNCs. Out of the articles, we focus on and disregard the rest due to having inconsistent format (see Appendix A for details). These articles contain a total of samples. For each sample, we retain out of the total attributes in the Materials Composition of NanoMine (see Appendix B for details).\n\nDocument-level information extraction requires understanding the entire document to accurately annotate entities, their relations, and saliency. These make the annotation of scientific articles time-consuming and prone to errors. We found that NanoMine also contains errors. Given the challenge of reviewing all samples and reading through articles, we adopted a semi-automatic approach to correct samples. Specifically, for an article, we consider both the predicted and ground truth sample list of a document. Following this strategy, we made three types of adjustments to the dataset: deleting samples, adding, and editing entities. This work includes contributions from polymer experts, under whose mentorship all authors received their training. (See Appendix G for details)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Evaluation Metrics",
            "text": "Our task involves evaluating the performance of our model in predicting PNC sample lists. One natural approach, also utilized by Cheung et al. (2023  ###reference_b1###), is to verify if there is an exact match between the predicted and the ground-truth samples. This method, however, has a notable limitation, particularly due to the numerous attributes that define a PNC sample. Under such strict evaluation criteria, a predicted sample is considered entirely incorrect if even one attribute is predicted inaccurately, which can be too strict considering the complexity and attribute-rich nature of PNC samples.\n\nAdditionally, we also apply a strict metric, similar to the approach of Cheung et al. (2023  ###reference_b1###), where a prediction is considered correct only if it perfectly matches with the ground truth across all attributes of a PNC sample. To accurately calculate the metrics, standardizing predictions is essential. The variability in polymer name expressions in scientific literature makes uniform evaluation challenging. For example, \u201csilica\u201d and \u201csilicon dioxide\u201d are different terms for the same filler. Our dataset uses a standardized format for chemical names. To align the predicted names with this standard, we use resources by Hu et al. (2021  ###reference_b14###  ###reference_b14###), which list matrix names with their standard names, abbreviations, synonyms, and trade names, as well as filler names with their standard names. We standardize predicted chemical names by matching them to the closest names in these lists and converting them to their standard forms.\n\nFurthermore, our dataset exclusively uses numerical values to represent compositions (e.g., a composition of \u201c\u201d should be listed as \u201c\u201d). Predictions in percentage format (like \u201c\u201d) are thus converted to the numerical format to align with the dataset\u2019s representation. Our evaluation incorporates an attribute aggregation method. For both the \u201cMatrix\u201d and \u201cFiller\u201d categories, a prediction is considered accurate if the model successfully identifies either the chemical name or the abbreviation. For the \u201cComposition,\u201d a correct prediction may be based on either the \u201cFiller Composition Mass\u201d or the \u201cFiller Composition Volume.\u201d This approach allows for a broader assessment, capturing any correct form of attribute identification without focusing on the finer details of each attribute.\n\nFor a stricter assessment, a sample is labeled correct only if it precisely matches one in the ground truth. Predictions not in the ground truth are false positives, and missing ground truth samples are false negatives. This metric emphasizes exact match accuracy."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Modeling Sample List Extractions from Articles with LLMs",
            "text": "As mentioned in Section 1, our dataset is designed for a generative task, making encoder-only models unsuitable for two main reasons. First, these models demand extensive annotations, such as named entity spans, coreferences, and negative examples, a process that is both time-consuming and error-prone. Second, encoder-only models struggle with processing long documents efficiently. While some studies have successfully used these models for long documents, they had access to significantly larger datasets. Our dataset, however, contains detailed domain-specific information, making it challenging to obtain a similarly extensive dataset.\n\nConsequently, within a zero-shot context, we explore two prompting methods: Named Entity Recognition plus Relation Extraction (NER+RE) and an End-to-End (E2E) approach."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "NER+RE Prompt",
            "text": "Building on previous research (Peng et al., 2017  ###reference_b23###; Jia et al., 2019a  ###reference_b16###; Viswanathan et al., 2021  ###reference_b32###), which treated -ary relation extraction as a binary classification task, our NER+RE method treats RE as a question-answering process, following the approach in Zhang et al. (2023  ###reference_b38###). This process is executed in two stages. Initially, the model identifies named entities within the text. Subsequently, it classifies -ary relations by transforming the task into a series of yes/no questions about these entities and their relations.\n\nThe NER+RE approach becomes computationally expensive during inference, especially as the number of entities increases. This leads to an exponential growth in potential combinations, expanding the candidate space for valid compositions and consequently extending the inference time."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "End-to-End Prompt",
            "text": "To address this challenge, we develop an End-to-End (E2E) prompting strategy that directly extracts JSON-formatted sample data from articles. This method is designed to efficiently handle the complexity and scale of extracting -ary relations from scientific texts, bypassing the limitations of binary classification frameworks in this context."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Self-Consistency",
            "text": "The self-consistency method (Wang et al., 2023b), aims to enhance the reasoning abilities of LLMs. Originally, this method relied on taking a majority vote from several model outputs. For our purposes, since the output is a set of answers rather than a single one, we apply the majority vote principle to the elements within these sets.\nWe generate predictions from the model, each at a controlled temperature. Our objective is to identify which samples appear frequently across these multiple predictions as a sign of higher confidence from the model.\nDuring evaluation, each model run generates a list of predicted samples from a specific paper. We refer to each list as the -th prediction, denoted. For each predicted element, we determine its match score, by counting how frequently it appears across all predictions. This score can vary from, meaning it appeared in only one prediction, to, indicating it was present in all predictions.\nWe then apply a threshold to filter the samples. Those with a match score at or above the threshold are retained, as they were consistently predicted by the model. Samples falling below this threshold suggest less confidence in the prediction and are removed."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Condensing Articles with Dense Retrieval",
            "text": "LLMs, such as LLaMA2 with its token limit, face challenges in maintaining performance with longer input lengths. Recent advancements have extended these limits; however, an increase in input length often leads to a decline in model performance. This raises the question of whether condensing articles could serve as an effective strategy to address such limitations. We, therefore, employ the Generalizable T5-based Dense Retrievers (GTR-large) to retrieve relevant parts of the documents.\n\nThis process involves dividing each document into segments and formulating four queries to extract targeted information regarding an entity. The queries are: \u201cWhat chemical is used in the polymer matrix?\u201d, \u201cWhat chemical is used in the polymer filler?\u201d, \u201cWhat is the filler mass composition?\u201d, and \u201cWhat is the filler volume composition?\u201d. On average, each segment consists of 60 tokens.\n\nWe then calculate the similarity between each pair of segments and queries. For every query, we select the top segments based on their similarity scores. These top segments from all four queries are then combined to form a condensed version of the original document."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In our experiments, we employ LLaMA-7b-Chat (Touvron et al., 2023 ###reference_b30###), LongChat-7B-16K (Dacheng Li* and Zhang, 2023 ###reference_b3###), Vicuna-7B-v1.5 and Vicuna-7B-v1.5-16K (Chiang et al., 2023 ###reference_b2###), and GPT-4 Turbo (OpenAI, 2023 ###reference_b22###). The LongChat-7B-16K and Vicuna-7B-16K models are fine-tuned for context lengths of 16K tokens, and GPT-4 Turbo for 128K tokens.\n\nWe divide our dataset into validation articles and test articles. We assess the performance using micro average Precision, Recall, and F1 scores at the sample and property levels. We also compare two different prompting strategies NER+RE and E2E. Moreover, we consider the self-consistency technique.\n\nTable 4 ###reference_### demonstrates that shortening documents proves beneficial in most cases. We observe that GPT-4\u2019s performance decreases in extremely shortened settings but is optimal when documents are shortened to the top segments. This indicates that while reducing document length is beneficial, excessive shortening may result in the loss of sample information.\n\nAdditionally, Table 5 ###reference_### provides bootstrap analysis from resamplings, indicating that GPT-4 Turbo has a higher mean F score on shorter full-length documents. The E2E prompting method shows better performance compared to the NER+RE approach, which is attributed to the higher precision of E2E. Furthermore, the inference time of the GPT-4 Turbo (E2E) is sec/article, faster than sec/article for GPT-4 Turbo (NER+RE).\n\nTo optimize the application of self-consistency, we first determine the most effective number of predictions to sample and the optimal value for on the validation set (see Appendix D ###reference_.SSS0.Px3###). Based on that, we employ and predictions on the test set. Table 4 ###reference_### shows that self-consistency enhances the strict F.\n\nNanoMine aggregates samples from the literature, including those presented in tables and visual elements within research articles. As demonstrated in the first example of Figure 4 ###reference_###, a sample is derived from the inset of a graph. Our present approach relies solely on language models. Future research could focus on advancing models to extract information from both textual and visual data.\n\nThe composition of PNC includes a variety of components such as hardeners and surface treatment agents. A common issue in our model\u2019s predictions is incorrectly identifying these auxiliary components as the main attributes. For example, the second row in Figure 4 ###reference_### shows the model predicting the filler material along with its surface treatments instead of recognizing the filler by itself.\n\nThe expression of chemical names is inherently complex, with multiple names often existing for the same material. In some cases, predicted chemical names are conceptually accurate yet challenging to standardize. This suggests the necessity for more sophisticated approaches that can handle the diverse and complex representations of chemical compounds. The third example in Figure 4 ###reference_### shows an example of this."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Benchmarking LLMs on PNCExtract",
            "text": "In our experiments, we employ LLaMA-7b-Chat (Touvron et al., 2023), LongChat-7B-16K (Dacheng Li* and Zhang, 2023), Vicuna-7B-v1.5 and Vicuna-7B-v1.5-16K (Chiang et al., 2023), and GPT-4 Turbo (OpenAI, 2023). The LongChat-7B-16K and Vicuna-7B-16K models are fine-tuned for context lengths of 16K tokens, and GPT-4 Turbo for 128K tokens.\n\nWe divide our dataset into validation articles and test articles. We assess the performance using micro average Precision, Recall, and F1 scores at the sample and property levels. We also compare two different prompting strategies NER+RE and E2E. Moreover, we consider the self-consistency technique.\n\nTable 4 demonstrates that shortening documents proves beneficial in most cases. We observe that GPT-4\u2019s performance decreases in extremely shortened settings but is optimal when documents are shortened to the top segments. This indicates that while reducing document length is beneficial, excessive shortening may result in the loss of sample information.\n\nAdditionally, Table 5 provides bootstrap analysis from resamplings, indicating that GPT-4 Turbo has a higher mean F score on shorter full-length documents. ###figure_4### The E2E prompting method shows better performance compared to the NER+RE approach, which is attributed to the higher precision of E2E. Furthermore, the inference time of the GPT-4 Turbo (E2E) is faster than for GPT-4 Turbo (NER+RE).\n\nTo optimize the application of self-consistency, we first determine the most effective number of predictions to sample and the optimal value for on the validation set (see Appendix D). Based on that, we employ predictions on the test set. Table 4 shows that self-consistency enhances the strict F."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Results",
            "text": "In Table 4 ###reference_### we report the strict metrics for multiple models and settings. We report the best results for each model in the condensed paper setting, selected across different , which correspond to average token counts per document of , , and , respectively. Further details on the results across various levels of document condensation are available in the Appendix E ###reference_###. The results highlight several key observations:\n\nTable 4 ###reference_### ###reference_### ###reference_### demonstrates that shortening documents proves beneficial in most cases. Additionally, Figure 3 ###reference_### ###reference_### ###reference_### shows the trend of F scores as document length increases. We observe that GPT-4\u2019s performance decreases in extremely shortened settings but is optimal when documents are shortened to the top segments. This indicates that while reducing document length is beneficial, excessive shortening may result in the loss of sample information.\n\nAdditionally, Table 5 ###reference_### ###reference_### ###reference_### provides bootstrap analysis from resamplings, indicating that GPT-4 Turbo has a higher mean F score on shorter full-length documents.\n\n###figure_5### The E2E prompting method shows better performance compared to the NER+RE approach, which is attributed to the higher precision of E2E. Furthermore, the inference time of the GPT-4 Turbo (E2E) is sec/article, faster than sec/article for GPT-4 Turbo (NER+RE).\n\nTo optimize the application of self-consistency, we first determine the most effective number of predictions to sample and the optimal value for on the validation set (see Appendix D ###reference_.SSS0.Px3### ###reference_.SSS0.Px3### ###reference_.SSS0.Px3###). Based on that, we employ and predictions on the test set. Table 4 ###reference_### ###reference_### ###reference_### shows that self-consistency enhances the strict F.\n\nThe model faces challenges in accurately predicting Composition as shown in Table 6 ###reference_### ###reference_### ###reference_###. Furthermore, human annotations for PNC samples are often error-prone (Himanen et al., 2019 ###reference_b9### ###reference_b9### ###reference_b9###; McCusker et al., 2020 ###reference_b19### ###reference_b19### ###reference_b19###), hence one potential use of an LLM like GPT-4 would be to identify errors and send them back for re-annotation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Comparing with Baselines",
            "text": "Previous works on document-level -ary IE (Jain et al., 2020; Jia et al., 2019b; Hou et al., 2019), have relied on encoder-only models, making them unsuitable for our specific task. For comparative purposes, we prompt GPT-4 on the SciREX dataset (Jain et al., 2020), which comprises annotated full-length machine learning papers. As shown in Table 7, when prompted in a zero-shot, end-to-end manner, GPT-4 Turbo outperforms the baseline methods. Note that the baseline model, trained on papers, received extensive supervision in the form of mention, coreference, binary relation, and salient mention identification. This suggests that we would need to expend a large amount of annotation effort on PNCExtract to build a supervised pipeline comparable to the zero-shot GPT-4 approach presented here."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Analysis of Errors",
            "text": "Accurately extracting PNC samples is a complex task, and even state-of-the-art LLMs fail to capture all the samples. We find that out of ground-truth samples, were not identified in the model\u2019s predictions. Furthermore, of the predictions were incorrect. This section discusses three categories of challenges faced by current models in sample extraction and proposes potential directions for future improvements.\n\nNanoMine aggregates samples from the literature, including those presented in tables and visual elements within research articles. As demonstrated in the first example of Figure 4, a sample is derived from the inset of a graph. Our present approach relies solely on language models. Future research could focus on advancing models to extract information from both textual and visual data.\n\nThe composition of PNC includes a variety of components such as hardeners and surface treatment agents. A common issue in our model\u2019s predictions is incorrectly identifying these auxiliary components as the main attributes. For example, the second row in Figure 4 shows the model predicting the filler material along with its surface treatments instead of recognizing the filler by itself.\n\nThe expression of chemical names is inherently complex, with multiple names often existing for the same material. In some cases, predicted chemical names are conceptually accurate yet challenging to standardize. This suggests the necessity for more sophisticated approaches that can handle the diverse and complex representations of chemical compounds. The third example in Figure 4 shows an example of this."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Early works have focused on training models specifically for the tasks of NER and RE. Building on this, recently Wadhwa et al. (2023  ###reference_b33###) and Wang et al. (2023a  ###reference_b34###) show that LLMs can effectively carry out these tasks through prompting.\nIn the specific area of models trained on a materials science corpus, MatSciBERT (Gupta et al., 2022  ###reference_b8###) employs a BERT (Devlin et al., 2018  ###reference_b5###) model trained specifically on a materials science corpus. Song et al. (2023b  ###reference_b26###) further developed HoneyBee, a fine-tuned Llama-based model for materials science. MatSciBERT was not applicable to our task, as detailed in Section 3  ###reference_###, and HoneyBee\u2019s model weights were not accessible during our research phase. Other contributions in this field include studies by Shetty et al. (2023  ###reference_b24###), Hiroyuki Oka and Ishii (2021  ###reference_b11###), and Tchoua et al.  ###reference_b29###, focusing on the extraction of polymer-related data from scientific articles.\nSimilar to Dunn et al. (2022  ###reference_b6###), Xie et al. (2023  ###reference_b36###), Tang et al. (2023  ###reference_b28###) and Cheung et al. (2023  ###reference_b1###) our study also focuses on extracting -ary relations from materials science papers. However, our approach diverges in two significant aspects: we analyze full-length papers, not just selected sentences, and we extend our evaluation to partial assessment of -ary relations, rather than limiting it to binary assessments."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion and Future Works",
            "text": "We introduced PNCExtract, a benchmark focused on the extraction of PNC samples from full-length materials science articles. To the best of our knowledge, this is the first benchmark enabling detailed -ary IE from full-length materials science articles. We hope that this effort encourages further research into generative end-to-end methods for scientific information extraction from full-length documents. Future investigations should also consider more advanced techniques for condensing entire scientific papers. To overcome the challenges in PNC sample extraction discussed in Section 4.3  ###reference_###, future studies could investigate multimodal strategies that integrate text and visual data. Additionally, experimenting fine-tuning methods could lead to more precise chemical name generation."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitation",
            "text": "Although our dataset comprises samples derived from figures within the papers, the current paper is confined to the assessment of language models exclusively. We acknowledge that incorporating multimodal models, which can process both text and visual information, has the potential to enhance the results reported in this paper.\nFurthermore, despite our efforts to correct NanoMine, another limitation of our study is the potential presence of inaccuracies within the dataset.\nAdditionally, our paper selectively examines a subset of attributes from PNC samples. Consequently, we do not account for every possible variable, such as \u201cFiller Particle Surface Treatment.\u201d This limited attribute selection means we do not distinguish between otherwise identical samples when this additional attribute could lead to differentiation. Acknowledging this, including a broader range of attributes in future work could lead to the identification of a more diverse array of samples."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "We do not believe there are significant ethical issues associated with this research."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Processing NanoMine",
            "text": "In the sample composition section of NanoMine, various attributes describe the components of a sample. For our analysis, we focus on six specific attributes. Nonetheless, we encounter instances where the formatting in NanoMine is inconsistent. We excluded those articles. This is because our data processing and evaluation require a uniform structure. For example, in Figure 5  ###reference_###, we identify an example of an inconsistency where the \u201cFiller Chemical Name\u201d is presented as a list rather than a single value, which deviates from the standard JSON format we expect. This inconsistency makes the sample incompatible with our dataset\u2019s format, leading to its removal from our analysis.\n###figure_7###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Dataset Curation and Cleaning",
            "text": "During our curation process, we selectively disregard certain attributes from NanoMine based on three criteria:\nComplexity in Extraction and Evaluation: Attributes that cannot be directly extracted with a language model or evaluated are disregarded. For example, intricate descriptions (such as \u201can average particle diameter of 10 um\u201d) are excluded due to their complexity in evaluation.\nRarity in the Dataset: We also disregard attributes infrequently occurring in NanoMine. For instance, \u201cTacticity\u201d is noted in only  of samples. This rarity might stem from either its infrequent mention in research papers or oversights by annotators.\nRelative Importance: Attributes that are less important for our analysis, such as \u201cManufacturer Or Source Name\u201d, are also excluded. Our focus is on extracting attributes that are most relevant for identifying a nanocomposite sample."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Terms of Use",
            "text": "We used OpenAI (gpt-4 and gpt-4-1106-preview), LLaMA2, LongChat, and Vicuna models, and NanoMine data repository in accordance with their licenses and terms of use."
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Computational Experiments Details",
            "text": ""
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E Model Performance on Condensed and Full Papers",
            "text": "Table 9  ###reference_### presents an evaluation of various LLMs across different condensation levels and their performance on full-length papers."
        },
        {
            "section_id": "Appendix 6",
            "parent_section_id": null,
            "section_name": "Appendix F Prompts",
            "text": "In this section, we present all the prompts used in our experiments."
        },
        {
            "section_id": "Appendix 7",
            "parent_section_id": null,
            "section_name": "Appendix G Re-Annotation Example Text",
            "text": "Below, we provide an example of the text that is automatically generated which facilitates the re-annotation."
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S1.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S1.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S1.T1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.2.1\">Task</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.3.1\">Doc-level</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.1.1\">-ary RE</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.4.1\">End-to-End</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"4\" id=\"S1.T1.1.2.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.2.1.1.1\">Materials Domain</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T1.1.3.2.1\">PNCExtract</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.3.2.2\"><span class=\"ltx_text\" id=\"S1.T1.1.3.2.2.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.3.2.3\"><span class=\"ltx_text\" id=\"S1.T1.1.3.2.3.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.3.2.4\"><span class=\"ltx_text\" id=\"S1.T1.1.3.2.4.1\" style=\"color:#00FF00;\">\u2713</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.1.4.3.1\">PolyIE</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.4.3.2\"><span class=\"ltx_text\" id=\"S1.T1.1.4.3.2.1\" style=\"color:#FF0000;\">\u2717</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.4.3.3\"><span class=\"ltx_text\" id=\"S1.T1.1.4.3.3.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.4.3.4\"><span class=\"ltx_text\" id=\"S1.T1.1.4.3.4.1\" style=\"color:#FF0000;\">\u2717</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.1.5.4.1\"><cite class=\"ltx_cite ltx_citemacro_citet\">Dunn et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib6\" title=\"\">2022</a>)</cite></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.5.4.2\"><span class=\"ltx_text\" id=\"S1.T1.1.5.4.2.1\" style=\"color:#FF0000;\">\u2717</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.5.4.3\"><span class=\"ltx_text\" id=\"S1.T1.1.5.4.3.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.5.4.4\"><span class=\"ltx_text\" id=\"S1.T1.1.5.4.4.1\" style=\"color:#00FF00;\">\u2713</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.1.6.5.1\"><cite class=\"ltx_cite ltx_citemacro_citet\">Xie et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib36\" title=\"\">2023</a>)</cite></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.6.5.2\"><span class=\"ltx_text\" id=\"S1.T1.1.6.5.2.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.6.5.3\"><span class=\"ltx_text\" id=\"S1.T1.1.6.5.3.1\" style=\"color:#FF0000;\">\u2717</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.6.5.4\"><span class=\"ltx_text\" id=\"S1.T1.1.6.5.4.1\" style=\"color:#00FF00;\">\u2713</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.1.7.6.1\">MatSci-NLP</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.7.6.2\"><span class=\"ltx_text\" id=\"S1.T1.1.7.6.2.1\" style=\"color:#FF0000;\">\u2717</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.7.6.3\"><span class=\"ltx_text\" id=\"S1.T1.1.7.6.3.1\" style=\"color:#FF0000;\">\u2717</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.7.6.4\"><span class=\"ltx_text\" id=\"S1.T1.1.7.6.4.1\" style=\"color:#FF0000;\">\u2717</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.8.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"4\" id=\"S1.T1.1.8.7.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.8.7.1.1\">Other Domains</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.9.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T1.1.9.8.1\">PubMed</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.9.8.2\"><span class=\"ltx_text\" id=\"S1.T1.1.9.8.2.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.9.8.3\"><span class=\"ltx_text\" id=\"S1.T1.1.9.8.3.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.9.8.4\"><span class=\"ltx_text\" id=\"S1.T1.1.9.8.4.1\" style=\"color:#FF0000;\">\u2717</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.10.9\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.1.10.9.1\">SciREX</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.10.9.2\"><span class=\"ltx_text\" id=\"S1.T1.1.10.9.2.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.10.9.3\"><span class=\"ltx_text\" id=\"S1.T1.1.10.9.3.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.10.9.4\"><span class=\"ltx_text\" id=\"S1.T1.1.10.9.4.1\" style=\"color:#FF0000;\">\u2717</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.11.10\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S1.T1.1.11.10.1\">NLP-TDMS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S1.T1.1.11.10.2\"><span class=\"ltx_text\" id=\"S1.T1.1.11.10.2.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S1.T1.1.11.10.3\"><span class=\"ltx_text\" id=\"S1.T1.1.11.10.3.1\" style=\"color:#00FF00;\">\u2713</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S1.T1.1.11.10.4\"><span class=\"ltx_text\" id=\"S1.T1.1.11.10.4.1\" style=\"color:#FF0000;\">\u2717</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Comparison of PNCExtract with other Information Extraction (IE) approaches within the materials science domain\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Song et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib25\" title=\"\">2023a</a>; Cheung et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib1\" title=\"\">2023</a>; Dunn et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib6\" title=\"\">2022</a>; Xie et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib36\" title=\"\">2023</a>)</cite> and across various other scientific domains\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Jain et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib15\" title=\"\">2020</a>; Jia et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib17\" title=\"\">2019b</a>; Hou et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib13\" title=\"\">2019</a>)</cite>. \u201cEnd-to-End\u201d indicates that, unlike previous methods that require task-specific supervision (e.g., named entity recognition, coreference resolution), PNCExtract relies on end-to-end supervision only.\n</figcaption>\n</figure>",
            "capture": "Table 1: Comparison of PNCExtract with other Information Extraction (IE) approaches within the materials science domain\u00a0(Song et\u00a0al., 2023a; Cheung et\u00a0al., 2023; Dunn et\u00a0al., 2022; Xie et\u00a0al., 2023) and across various other scientific domains\u00a0(Jain et\u00a0al., 2020; Jia et\u00a0al., 2019b; Hou et\u00a0al., 2019). \u201cEnd-to-End\u201d indicates that, unlike previous methods that require task-specific supervision (e.g., named entity recognition, coreference resolution), PNCExtract relies on end-to-end supervision only.\n"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S2.T2.1.1.1.1\">Attribute</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T2.1.1.1.2\">Number of Samples</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T2.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T2.1.2.1.1\">Matrix Chemical Name</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.2.1.2\">1052</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T2.1.3.2.1\">Matrix Chemical Abbreviation</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.3.2.2\">864</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T2.1.4.3.1\">Filler Chemical Name</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.4.3.2\">1052</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T2.1.5.4.1\">Filler Chemical Abbreviation</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.5.4.2\">819</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T2.1.6.5.1\">Filler Mass</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.6.5.2\">624</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S2.T2.1.7.6.1\">Filler Volume</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T2.1.7.6.2\">407</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Number of total samples for which each of the attributes is non-null.</figcaption>\n</figure>",
            "capture": "Table 2: Number of total samples for which each of the attributes is non-null."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S2.T3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.1.1.1.1.1\">Statistics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T3.1.1.1.2\">Paper Length</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T3.1.1.1.3\">#Samples/Doc</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T3.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T3.1.2.1.1\">Avg.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.2.1.2\">6965</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.2.1.3\">6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T3.1.3.2.1\">Med.</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.3.2.2\">6734</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.3.2.3\">4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T3.1.4.3.1\">Min.</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.4.3.2\">238</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.4.3.3\">1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S2.T3.1.5.4.1\">Max.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T3.1.5.4.2\">16355</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T3.1.5.4.3\">50</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Statistical summary of paper lengths and number of samples per document. Paper length is measured in tokens.</figcaption>\n</figure>",
            "capture": "Table 3: Statistical summary of paper lengths and number of samples per document. Paper length is measured in tokens."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T4.2.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.3.1.1.1\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T4.2.3.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.3.1.2.1\">Strict</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T4.2.3.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.3.1.3.1\">Partial</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S4.T4.2.2.3\"></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.4.1\">P</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.5.1\">R</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1\">F</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.6.1\">P</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.7.1\">R</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.2.1\">F</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"S4.T4.2.4.2.1\">Condensed Papers</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.2.5.3.1\">LLaMA2 C</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.5.3.2\">21.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.5.3.3\">0.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.5.3.4\">1.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.5.3.5\">60.0</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.5.3.6\">1.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.5.3.7\">3.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.6.4.1\">Vicuna</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.6.4.2\">5.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.6.4.3\">2.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.6.4.4\">3.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.6.4.5\">49.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.6.4.6\">19.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.6.4.7\">28.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.7.5.1\">Vicuna-16k</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.7.5.2\">17.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.7.5.3\">5.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.7.5.4\">8.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.7.5.5\">60.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.7.5.6\">19.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.7.5.7\">29.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.8.6.1\">LongChat</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.8.6.2\">6.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.8.6.3\">3.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.8.6.4\">4.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.8.6.5\">47.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.8.6.6\">24.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.8.6.7\">32.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.9.7.1\">GPT-4</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.9.7.2\">43.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.9.7.3\">32.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.9.7.4\">36.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.9.7.5\">64.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.9.7.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.9.7.6.1\">47.7</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.9.7.7\">54.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.10.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"S4.T4.2.10.8.1\">Full Papers</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.2.11.9.1\">Vicuna-16k</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.11.9.2\">18.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.11.9.3\">1.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.11.9.4\">2.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.11.9.5\">65.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.11.9.6\">4.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T4.2.11.9.7\">8.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.12.10.1\">LongChat</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.12.10.2\">5.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.12.10.3\">4.2</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.12.10.4\">4.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.12.10.5\">36.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.12.10.6\">29.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.12.10.7\">32.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.13.11.1\">GPT-4</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.13.11.2\">44.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.13.11.3\">30.2</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.13.11.4\">36.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.13.11.5\">64.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.13.11.6\">43.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.13.11.7\">52.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.2.14.12.1\">GPT-4 (NR)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.14.12.2\">28.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.14.12.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.14.12.3.1\">37.2</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.14.12.4\">32.2</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.14.12.5\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.14.12.6\">-</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T4.2.14.12.7\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T4.2.15.13.1\">GPT-4 + SC</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T4.2.15.13.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.15.13.2.1\">51.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T4.2.15.13.3\">31.1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T4.2.15.13.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.15.13.4.1\">38.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T4.2.15.13.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.15.13.5.1\">73.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T4.2.15.13.6\">43.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T4.2.15.13.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.15.13.7.1\">54.9</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Precision, Recall, and F of different LLMs on condensed and full papers using strict and partial metrics. The table includes GPT-4 Turbo with different prompting methods (NER+RE, E2E, and E2E with self-consistency [SC]). \u201cNR\u201d denotes NER+RE prompting. \u201cLLaMA2 C\u201d represents the LLaMA2-7b-chat model. Models with limited context lengths are evaluated only in the condensed paper scenario.\n</figcaption>\n</figure>",
            "capture": "Table 4: Precision, Recall, and F of different LLMs on condensed and full papers using strict and partial metrics. The table includes GPT-4 Turbo with different prompting methods (NER+RE, E2E, and E2E with self-consistency [SC]). \u201cNR\u201d denotes NER+RE prompting. \u201cLLaMA2 C\u201d represents the LLaMA2-7b-chat model. Models with limited context lengths are evaluated only in the condensed paper scenario.\n"
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T5.1.1.2\">Length Interval</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.1\">Mean F\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.3\">SD</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T5.1.1.4\">95% CI</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T5.1.2.1.1\">(0, 8000)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.1.2\">44.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.1.3\">04.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.1.4\">(35.2, 51.2)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T5.1.3.2.1\">(8000, 20000)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.3.2.2\">35.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.3.2.3\">05.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.3.2.4\">(24.4, 46.7)</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Comparison of mean F scores, standard deviations, and 95% confidence intervals for different token length intervals.</figcaption>\n</figure>",
            "capture": "Table 5: Comparison of mean F scores, standard deviations, and 95% confidence intervals for different token length intervals."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T6.1.1.2\">Attributes</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.1.1.3\">P</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.1.1.4\">R</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T6.1.1.1\">F\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T6.1.2.1.1\">Matrix</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.2.1.2\">50.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.2.1.3\">23.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T6.1.2.1.4\">32.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T6.1.3.2.1\">Filler</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.3.2.2\">53.1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.3.2.3\">25.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T6.1.3.2.4\">34.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T6.1.4.3.1\">Composition</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.1.4.3.2\">44.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.1.4.3.3\">20.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T6.1.4.3.4\">28.0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Micro average precision, recall, and F across the attributes.</figcaption>\n</figure>",
            "capture": "Table 6: Micro average precision, recall, and F across the attributes."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T7.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T7.1.1.2\">Model</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.3\">Prec.</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.4\">Rec.</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T7.1.1.1\">F\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T7.1.2.1.1\"><cite class=\"ltx_cite ltx_citemacro_citet\">Jain et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.00260v1#bib.bib15\" title=\"\">2020</a>)</cite></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.1.2.1.2\">0.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.1.2.1.3\">17.3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.1.2.1.4\">0.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T7.1.3.2.1\">Zero-shot GPT-4</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T7.1.3.2.2\">5.0</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T7.1.3.2.3\">8.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T7.1.3.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.3.2.4.1\">5.5</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Micro average precision, recall, and F. The baseline results are taken from the referenced paper.</figcaption>\n</figure>",
            "capture": "Table 7: Micro average precision, recall, and F. The baseline results are taken from the referenced paper."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A4.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A4.T8.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A4.T8.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T8.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T8.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.2.2.2.1\">Predictions</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T8.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.3.3.3.1\">F</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A4.T8.3.4.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T8.3.4.1.1\" rowspan=\"6\"><span class=\"ltx_text\" id=\"A4.T8.3.4.1.1.1\">2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T8.3.4.1.2\">9</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.3.4.1.3\">39.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.5.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.3.5.2.1\">8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.3.5.2.2\">39.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.6.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.3.6.3.1\">7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.3.6.3.2\">41.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.7.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.3.7.4.1\">6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.3.7.4.2\">40.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.8.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.3.8.5.1\">5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.3.8.5.2\">41.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.9.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.3.9.6.1\">4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.3.9.6.2\">39.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.10.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A4.T8.3.10.7.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"A4.T8.3.10.7.1.1\">3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A4.T8.3.10.7.2\">9</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T8.3.10.7.3\">41.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.11.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.3.11.8.1\">8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.3.11.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T8.3.11.8.2.1\">43.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.12.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.3.12.9.1\">7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.3.12.9.2\">39.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.13.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"A4.T8.3.13.10.1\">6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A4.T8.3.13.10.2\">39.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A4.T8.3.14.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A4.T8.3.14.11.1\">5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A4.T8.3.14.11.2\">36.0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>F1 scores for alpha levels 2 and 3, with various numbers of predictions.</figcaption>\n</figure>",
            "capture": "Table 8: F1 scores for alpha levels 2 and 3, with various numbers of predictions."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"A5.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A5.T9.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A5.T9.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"A5.T9.2.3.1.1\">Model</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"A5.T9.2.3.1.2\">Strict</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"A5.T9.2.3.1.3\">Partial</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"A5.T9.2.2.3\"></th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.2.4\">Prec.</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.2.5\">Rec.</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.1.1.1\">F\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.2.6\">Prec.</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.2.7\">Rec.</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.2.2\">F\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"A5.T9.2.4.2.1\">Condensed Papers (Top 5)</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A5.T9.2.5.3.1\">LLaMA2-7b Chat</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.5.3.2\">9.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.5.3.3\">0.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.5.3.4\">0.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.5.3.5\">41.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.5.3.6\">0.9</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.5.3.7\">1.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.6.4.1\">LongChat-7b-13k</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.6.4.2\">3.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.6.4.3\">1.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.6.4.4\">2.1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.6.4.5\">43.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.6.4.6\">15.2</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.6.4.7\">22.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.7.5.1\">Vicuna-7b-v1.5</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.7.5.2\">5.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.7.5.3\">2.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.7.5.4\">3.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.7.5.5\">49.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.7.5.6\">19.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.7.5.7\">28.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.8.6.1\">Vicuna-7b-v1.5-16k</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.8.6.2\">17.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.8.6.3\">5.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.8.6.4\">8.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.8.6.5\">60.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.8.6.6\">19.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.8.6.7\">29.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.9.7.1\">GPT-4 Turbo</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.9.7.2\">31.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.9.7.3\">18.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.9.7.4\">23.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.9.7.5\">63.1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.9.7.6\">35.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.9.7.7\">45.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.10.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"A5.T9.2.10.8.1\">Condensed Papers (Top 10)</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A5.T9.2.11.9.1\">LLaMA2-7b Chat</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.11.9.2\">21.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.11.9.3\">0.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.11.9.4\">1.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.11.9.5\">60.0</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.11.9.6\">1.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.11.9.7\">3.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.12.10.1\">LongChat-7b-13k</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.12.10.2\">2.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.12.10.3\">0.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.12.10.4\">1.1</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.12.10.5\">45.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.12.10.6\">17.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.12.10.7\">25.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.13.11.1\">Vicuna-7b-v1.5</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.13.11.2\">14.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.13.11.3\">3.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.13.11.4\">5.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.13.11.5\">60.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.13.11.6\">10.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.13.11.7\">17.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.14.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.14.12.1\">Vicuna-7b-v1.5-16k</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.14.12.2\">15.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.14.12.3\">4.9</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.14.12.4\">7.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.14.12.5\">58.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.14.12.6\">17.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.14.12.7\">27.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.15.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.15.13.1\">GPT-4 Turbo</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.15.13.2\">33.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.15.13.3\">23.0</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.15.13.4\">27.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.15.13.5\">61.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.15.13.6\">42.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.15.13.7\">50.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.16.14\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"A5.T9.2.16.14.1\">Condensed Papers (Top 30)</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.17.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A5.T9.2.17.15.1\">LongChat-7b-13k</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.17.15.2\">4.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.17.15.3\">7.0</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.17.15.4\">3.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.17.15.5\">48.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.17.15.6\">24.3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.17.15.7\">32.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.18.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.18.16.1\">Vicuna-7b-v1.5</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.18.16.2\">6.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.18.16.3\">0.2</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.18.16.4\">0.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.18.16.5\">55.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.18.16.6\">1.8</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.18.16.7\">3.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.19.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.19.17.1\">Vicuna-7b-v1.5-16k</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.19.17.2\">17.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.19.17.3\">5.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.19.17.4\">8.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.19.17.5\">62.2</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.19.17.6\">18.3</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.19.17.7\">28.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.20.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.20.18.1\">GPT-4 Turbo</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.20.18.2\">43.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.20.18.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.20.18.3.1\">32.0</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.20.18.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.20.18.4.1\">36.9</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.20.18.5\">64.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.20.18.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.20.18.6.1\">47.7</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.20.18.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.20.18.7.1\">54.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.21.19\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" id=\"A5.T9.2.21.19.1\">Full Papers</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.22.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A5.T9.2.22.20.1\">Vicuna-7b-v1.5-16k</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.22.20.2\">18.4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.22.20.3\">1.5</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.22.20.4\">2.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.22.20.5\">65.7</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.22.20.6\">4.6</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A5.T9.2.22.20.7\">8.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.23.21\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T9.2.23.21.1\">LongChat-7b-13k</th>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.23.21.2\">5.4</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.23.21.3\">4.2</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.23.21.4\">4.7</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.23.21.5\">36.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.23.21.6\">29.6</td>\n<td class=\"ltx_td ltx_align_right\" id=\"A5.T9.2.23.21.7\">32.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T9.2.24.22\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A5.T9.2.24.22.1\">GPT-4 Turbo</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A5.T9.2.24.22.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.24.22.2.1\">44.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A5.T9.2.24.22.3\">30.2</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A5.T9.2.24.22.4\">36.0</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A5.T9.2.24.22.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T9.2.24.22.5.1\">64.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A5.T9.2.24.22.6\">43.8</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"A5.T9.2.24.22.7\">52.3</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Precision, Recall, and F of different LLMs on condensed and full papers using strict and partial metrics. The results are segmented based on the degree of paper condensation (Top 5, Top 10, Top 30 segments) and for full paper length\n</figcaption>\n</figure>",
            "capture": "Table 9: Precision, Recall, and F of different LLMs on condensed and full papers using strict and partial metrics. The results are segmented based on the degree of paper condensation (Top 5, Top 10, Top 30 segments) and for full paper length\n"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00260v1_figure_1.png",
            "caption": "Figure 1: A snippet from a PNC research article (Dalmas et al., 2007) and the extracted PNC sample list from the NanoMine database.\nNote how information for a single sample is\nextracted from multiple parts of the article text."
        },
        "2": {
            "figure_path": "2403.00260v1_figure_2.png",
            "caption": "Figure 2: Two prompting strategies for PNC sample extraction with LLM are presented. On the left, the end-to-end (E2E) approach uses a single prompt to directly extract PNC samples. On the right, the NER+RE approach first identifies relevant entities and then classifies their relations through yes/no prompts to validate PNC samples."
        },
        "3": {
            "figure_path": "2403.00260v1_figure_3.png",
            "caption": "Figure 3: Comparison of Micro Partial F1 Scores Across Different Models and Document Lengths. \u201cTop 5\u201d, \u201cTop 10\u201d, and \u201cTop 30\u201d indicate document summaries retrieved with k\ud835\udc58kitalic_k set to 5555, 10101010, and 30303030 respectively."
        },
        "4": {
            "figure_path": "2403.00260v1_figure_4.png",
            "caption": "Figure 4: Examples of challenges for LLMs, showcasing three categories of challenges encountered in capturing accurate PNC sample compositions. Each row demonstrates a specific challenge, the ground-truth sample, the model\u2019s prediction, and a brief explanation of the issue.\""
        },
        "5": {
            "figure_path": "2403.00260v1_figure_5.png",
            "caption": "Figure 5: An inconsistent sample in NanoMine that we exclude from our dataset."
        }
    },
    "references": [
        {
            "1": {
                "title": "Polyie: A dataset of information extraction from polymer material scientific literature.",
                "author": "Jerry Junyang Cheung, Yuchen Zhuang, Yinghao Li, Pranav Shetty, Wantian Zhao, Sanjeev Grampurohit, Rampi Ramprasad, and Chao Zhang. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2311.07715"
            }
        },
        {
            "2": {
                "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
                "author": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.",
                "venue": null,
                "url": "https://lmsys.org/blog/2023-03-30-vicuna/"
            }
        },
        {
            "3": {
                "title": "How long can open-source llms truly promise on context length?",
                "author": "Anze Xie Ying Sheng Lianmin Zheng Joseph E. Gonzalez Ion Stoica Xuezhe Ma Dacheng Li*, Rulin Shao* and Hao Zhang. 2023.",
                "venue": null,
                "url": "https://lmsys.org/blog/2023-06-29-longchat"
            }
        },
        {
            "4": {
                "title": "Viscoelastic behavior and electrical properties of flexible nanofiber filled polymer nanocomposites. influence of processing conditions.",
                "author": "Florent Dalmas, Jean-Yves Cavaill\u00e9, Catherine Gauthier, Laurent Chazeau, and R\u00e9my Dendievel. 2007.",
                "venue": "Composites Science and Technology, 67(5):829\u2013839.",
                "url": "https://doi.org/https://doi.org/10.1016/j.compscitech.2006.01.030"
            }
        },
        {
            "5": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.",
                "venue": "arXiv preprint arXiv:1810.04805.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Structured information extraction from complex scientific text with fine-tuned large language models.",
                "author": "Alex Dunn, John Dagdelen, Nicholas Thomas Walker, Sanghoon Lee, Andrew S. Rosen, Gerbrand Ceder, Kristin A. Persson, and Anubhav Jain. 2022.",
                "venue": "ArXiv, abs/2212.05238.",
                "url": "https://api.semanticscholar.org/CorpusID:254564105"
            }
        },
        {
            "7": {
                "title": "A sequence-to-sequence approach for document-level relation extraction.",
                "author": "John Giorgi, Gary Bader, and Bo Wang. 2022.",
                "venue": "In Proceedings of the 21st Workshop on Biomedical Language Processing, pages 10\u201325, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.bionlp-1.2"
            }
        },
        {
            "8": {
                "title": "Matscibert: A materials domain language model for text mining and information extraction.",
                "author": "Tanishq Gupta, Mohd Zaki, N. M. Anoop Krishnan, and Mausam. 2022.",
                "venue": "npj Computational Materials, 8(1):102.",
                "url": "https://doi.org/10.1038/s41524-022-00784-w"
            }
        },
        {
            "9": {
                "title": "Data-driven materials science: Status, challenges, and perspectives.",
                "author": "Lauri Himanen, Amber Geurts, Adam Stuart Foster, and Patrick Rinke. 2019.",
                "venue": "Advanced Science, 6(21).",
                "url": "https://doi.org/10.1002/advs.201900808"
            }
        },
        {
            "10": {
                "title": "Reconstructing materials tetrahedron: Challenges in materials information extraction.",
                "author": "Kausik Hira, Mohd Zaki, Dhruvil Sheth, Mausam, and N M Anoop Krishnan. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2310.08383"
            }
        },
        {
            "11": {
                "title": "Machine extraction of polymer data from tables using xml versions of scientific articles.",
                "author": "Hiroyuki Shindo Yuji Matsumoto Hiroyuki Oka, Atsushi Yoshizawa and Masashi Ishii. 2021.",
                "venue": "Science and Technology of Advanced Materials: Methods, 1(1):12\u201323.",
                "url": "https://doi.org/10.1080/27660400.2021.1899456"
            }
        },
        {
            "12": {
                "title": "Foundation models of scientific knowledge for chemistry: Opportunities, challenges and lessons learned.",
                "author": "Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, and Svitlana Volkova. 2022.",
                "venue": "In Proceedings of BigScience Episode# 5\u2013Workshop on Challenges & Perspectives in Creating Large Language Models, pages 160\u2013172.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction.",
                "author": "Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, and Debasis Ganguly. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5203\u20135213, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1513"
            }
        },
        {
            "14": {
                "title": "Chemprops: A restful api enabled database for composite polymer name standardization.",
                "author": "Bingyin Hu, Anqi Lin, and L. Catherine Brinson. 2021.",
                "venue": "Journal of Cheminformatics, 13(1):22.",
                "url": "https://doi.org/10.1186/s13321-021-00502-6"
            }
        },
        {
            "15": {
                "title": "SciREX: A challenge dataset for document-level information extraction.",
                "author": "Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7506\u20137516, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.670"
            }
        },
        {
            "16": {
                "title": "Cross-domain NER using cross-domain language modeling.",
                "author": "Chen Jia, Xiaobo Liang, and Yue Zhang. 2019a.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2464\u20132474, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1236"
            }
        },
        {
            "17": {
                "title": "Document-level n-ary relation extraction with multiscale representation learning.",
                "author": "Robin Jia, Cliff Wong, and Hoifung Poon. 2019b.",
                "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3693\u20133704, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1370"
            }
        },
        {
            "18": {
                "title": "The hungarian method for the assignment problem.",
                "author": "H. W. Kuhn. 1955.",
                "venue": "Naval Research Logistics Quarterly, 2(1-2):83\u201397.",
                "url": "https://doi.org/https://doi.org/10.1002/nav.3800020109"
            }
        },
        {
            "19": {
                "title": "Nanomine: A knowledge graph for nanocomposite materials science.",
                "author": "Jamie P. McCusker, Neha Keshan, Sabbir Rashid, Michael Deagen, Cate Brinson, and Deborah L. McGuinness. 2020.",
                "venue": "In The Semantic Web \u2013 ISWC 2020, pages 144\u2013159, Cham. Springer International Publishing.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Scaling deep learning for materials discovery.",
                "author": "Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023.",
                "venue": "Nature, 624(7990):80\u201385.",
                "url": "https://doi.org/10.1038/s41586-023-06735-9"
            }
        },
        {
            "21": {
                "title": "Large dual encoders are generalizable retrievers.",
                "author": "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9844\u20139855, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.emnlp-main.669"
            }
        },
        {
            "22": {
                "title": "Gpt-4 technical report.",
                "author": "OpenAI. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.08774"
            }
        },
        {
            "23": {
                "title": "Cross-sentence n-ary relation extraction with graph LSTMs.",
                "author": "Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017.",
                "venue": "Transactions of the Association for Computational Linguistics, 5:101\u2013115.",
                "url": "https://doi.org/10.1162/tacl_a_00049"
            }
        },
        {
            "24": {
                "title": "A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing.",
                "author": "Pranav Shetty, Arunkumar Chitteth Rajan, Chris Kuenneth, Sonakshi Gupta, Lakshmi Prerana Panchumarti, Lauren Holm, Chao Zhang, and Rampi Ramprasad. 2023.",
                "venue": "npj Computational Materials, 9(1).",
                "url": "https://doi.org/10.1038/s41524-023-01003-w"
            }
        },
        {
            "25": {
                "title": "Matsci-nlp: Evaluating scientific language models on materials science language tasks using text-to-schema modeling.",
                "author": "Yu Song, Santiago Miret, and Bang Liu. 2023a.",
                "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL).",
                "url": null
            }
        },
        {
            "26": {
                "title": "Honeybee: Progressive instruction finetuning of large language models for materials science.",
                "author": "Yu Song, Santiago Miret, Huan Zhang, and Bang Liu. 2023b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2310.08511"
            }
        },
        {
            "27": {
                "title": "Chemdataextractor: A toolkit for automated extraction of chemical information from the scientific literature.",
                "author": "Matthew C. Swain and Jacqueline M. Cole. 2016.",
                "venue": "Journal of Chemical Information and Modeling, 56(10):1894\u20131904.",
                "url": "https://doi.org/10.1021/acs.jcim.6b00207"
            }
        },
        {
            "28": {
                "title": "Does synthetic data generation of llms help clinical text mining?",
                "author": "Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.04360"
            }
        },
        {
            "29": {
                "title": "Creating training data for scientific named entity recognition with minimal human effort.",
                "author": "Roselyne B. Tchoua, Aswathy Ajith, Zhi Hong, Logan T. Ward, Kyle Chard, Alexander Belikov, Debra J. Audus, Shrayesh Patel, Juan J. de Pablo, and Ian T. Foster.",
                "venue": null,
                "url": "https://doi.org/10.1007/978-3-030-22734-0_29"
            }
        },
        {
            "30": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2307.09288"
            }
        },
        {
            "31": {
                "title": "Focused transformer: Contrastive training for context scaling.",
                "author": "Szymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2307.03170"
            }
        },
        {
            "32": {
                "title": "CitationIE: Leveraging the citation graph for scientific information extraction.",
                "author": "Vijay Viswanathan, Graham Neubig, and Pengfei Liu. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 719\u2013731, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.59"
            }
        },
        {
            "33": {
                "title": "Revisiting relation extraction in the era of large language models.",
                "author": "Somin Wadhwa, Silvio Amir, and Byron Wallace. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15566\u201315589, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.868"
            }
        },
        {
            "34": {
                "title": "Gpt-ner: Named entity recognition via large language models.",
                "author": "Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2304.10428"
            }
        },
        {
            "35": {
                "title": "Self-consistency improves chain of thought reasoning in language models.",
                "author": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2203.11171"
            }
        },
        {
            "36": {
                "title": "Large language models as master key: Unlocking the secrets of materials science with gpt.",
                "author": "Tong Xie, Yuwei Wan, Wei Huang, Yufei Zhou, Yixuan Liu, Qingyuan Linghu, Shaozhou Wang, Chunyu Kit, Clara Grazian, Wenjie Zhang, and Bram Hoex. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2304.02213"
            }
        },
        {
            "37": {
                "title": "Piekm: Ml-based procedural information extraction and knowledge management system for materials science literature.",
                "author": "Huichen Yang. 2022.",
                "venue": "In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System Demonstrations, pages 57\u201362.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Aligning instruction tasks unlocks large language models as zero-shot relation extractors.",
                "author": "Kai Zhang, Bernal Jimenez Gutierrez, and Yu Su. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 794\u2013812, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.50"
            }
        },
        {
            "39": {
                "title": "NanoMine schema: An extensible data representation for polymer nanocomposites.",
                "author": "He Zhao, Yixing Wang, Anqi Lin, Bingyin Hu, Rui Yan, James McCusker, Wei Chen, Deborah L. McGuinness, Linda Schadler, and L. Catherine Brinson. 2018.",
                "venue": "APL Materials, 6(11):111108.",
                "url": "https://doi.org/10.1063/1.5046839"
            }
        },
        {
            "40": {
                "title": "A frustratingly easy approach for entity and relation extraction.",
                "author": "Zexuan Zhong and Danqi Chen. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 50\u201361, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.5"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00260v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.00260v1",
        "paper_title": "Extracting Polymer Nanocomposite Samples from Full-Length Documents",
        "research_background": "**Motivation:**\nThe paper is driven by the necessity to transform unstructured data from materials science publications into structured formats. The highly unstructured nature of scientific journals complicates the extraction of essential material details pivotal for future discoveries. Manual extraction of material details is notably inefficient and error-prone, highlighting an urgent need for automated systems that can manage this task effectively.\n\n**Research Problem:**\nThe specific research problem addressed in this paper is the extraction of polymer nanocomposite (PNC) sample data from full-length scientific articles. PNCs have customizable properties based on their composition, making structured data from these articles vital. The extraction is challenging due to the scattered distribution of data across various sections of articles and the complexity of the -ary relationships defining each sample. The paper proposes creating PNCExtract, a benchmark for systematically extracting PNC samples from the full body of scientific texts.\n\n**Relevant Prior Work:**\n1. **General Information Extraction from Materials Science**: Previous studies like those by Dunn et al., Song et al., Xie et al., and Cheung explored -ary relation extraction but mainly focused on abstracts and short texts.\n   \n2. **Document-Level Information Extraction Datasets**: Datasets like SciREX, PubMed, and NLP-TDMS demand full-document analysis for -ary relation extraction but are outside the specific materials science domain, thus lacking the domain-specific challenges.\n\n3. **Challenges in Materials Science IE**: Swain and Cole pointed out the complex nomenclature in materials science, with the variability in identifiers like systematic names, trade names, and abbreviations creating significant IE challenges.\n\nThis paper extends and differs from prior works by tackling the unique difficulties of extracting intricate relationships from full-length PNC articles and developing evaluation strategies suited to the complexity of the domain.",
        "methodology": "**Proposed Methodology for Extracting Polymer Nanocomposite Samples**\n\nThe methodology detailed in the section outlines a robust approach to extracting and evaluating Polymer Nanocomposite (PNC) samples from full-length scholarly articles using a curated dataset from the NanoMine data repository. Here are the key components and innovations of the proposed method:\n\n### Dataset Preparation and Problem Definition\n1. **NanoMine Data Repository**: The dataset is curated from NanoMine, an XML-based schema designed for the representation and distribution of nanocomposite materials data. NanoMine contains full-length articles and corresponding PNC sample lists.\n2. **Focus on \u201cMaterials Composition\u201d**: The study specifically targets the \"Materials Composition\" section of articles for its comprehensive details on the characteristics of constituent materials in PNCs, such as polymer matrices and filler particles in weight or volume fractions.\n3. **Article Selection**: Only a subset of articles with consistent formatting is used, reducing the dataset to a manageable number of samples and attributes for further analysis.\n   \n### Semi-Automatic Re-Annotation Procedure\n4. **Combining Predicted and Ground Truth Samples**: Both predicted and ground-truth sample lists are considered for each document. Using a partial metric, similarity scores are assigned to pairs of predicted and ground truth samples, categorized as exact matches, partial matches, or unmatched samples (both true and predicted).\n5. **Targeted Re-Annotation**: The procedure emphasizes re-annotating samples with significant differences between prediction and ground truth, especially focusing on partial matches with lower similarity scores and unmatched samples, thus optimizing annotation efforts based on GPT-4 predictions.\n6. **Standardizing Predictions**: Chemical names are standardized using resources like those from Hu et al. (2021) which provide standard names, abbreviations, synonyms, and trade names for polymers and fillers. Compositions listed in scientific papers are uniformly represented numerically.\n\n### Evaluation Method\n7. **Attribute Aggregation Method**: For \"Matrix\" and \"Filler\" categories, accuracy is determined by identifying either the chemical name or abbreviation. For \"Composition,\" correct predictions are based on either the \"Filler Composition Mass\" or \"Filler Composition Volume.\" This method allows for broader assessment without delving into minor attribute details.\n8. **Two-Step F Score Calculation**: \n   - An initial accuracy score is computed for each pair of predicted and ground truth samples, focusing on the trio <Matrix, Filler, Composition>.\n   - These comparisons are translated into an assignment problem in a bipartite graph, solved using the Kuhn-Munkres algorithm, optimizing the total F score.\n9. **Accounting for Matches and Errors**: \n   - Correct, false positive, and false negative attributes are counted for both predicted and ground-truth samples.\n   - Micro-average Precision, Recall, and F scores are then calculated.\n10. **Stricter Evaluation Criteria**: Further evaluation uses a stricter metric where a sample is correct only if it precisely matches a ground truth sample. Any unlisted predictions and missing ground truth samples are counted as false positives and negatives, respectively, emphasizing exact match accuracy.\n\nThis rigorous, semi-automatic re-annotation and standardized evaluation approach ensures accurate extraction and analysis of PNC samples, leveraging partial and strict metrics while optimizing annotation resources and maintaining high precision in scientific data representation.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Models Used:**\n- LLaMA-7b-Chat\n- LongChat-7B-16K\n- Vicuna-7B-v1.5\n- Vicuna-7B-v1.5-16K\n- GPT-4 Turbo\n\n**Model Specifics:**\n- LongChat-7B-16K and Vicuna-7B-16K are fine-tuned for context lengths of 16K tokens.\n- GPT-4 Turbo handles up to 128K tokens.\n\n**Datasets:**\n- Divided into validation articles and test articles.\n\n**Evaluation Metrics:**\n- Micro average Precision, Recall, and F1 scores.\n- Strict and partial metrics applied at the sample and property levels.\n\n**Prompting Strategies Compared:**\n1. NER+RE (Named Entity Recognition + Relation Extraction)\n2. E2E (End-to-End)\n\n**Self-Consistency Technique:**\n- Used to optimize performance with multiple predictions.\n\n**Main Experimental Results:**\n1. **Document Length:**\n   - Shortening documents generally beneficial.\n   - GPT-4 Turbo performs best when documents are shortened to a top segment but performance declines if excessively shortened.\n\n2. **Model Performance:**\n   - Shortening documents aids in performance.\n   - E2E prompting method outperforms NER+RE due to higher precision of E2E.\n   - **Inference Time:** GPT-4 Turbo (E2E) is faster at sec/article compared to sec/article for GPT-4 Turbo (NER+RE).\n\n3. **Challenges and Solutions:**\n   - **Specific Challenges:** Composition predictions are particularly difficult.\n   - **Annotations Review:** Potential use of models to identify and rectify human annotation errors.\n   - **Domain-Specific Challenges:** Complex expressions of chemical names require high sophistication in models, with issues noted in accurately identifying auxiliary components and diverse chemical representation.\n\n**Advantages of Partial Metric:**\n- Highlights challenge areas and helps prioritize sample re-annotations.\n- Can aid in refining human-generated annotations by using model predictions to spot errors.\n\n**Data Source (NanoMine):**\n- Aggregates samples from literature, including tables and visual elements in research articles.\n- Future research could enhance models to extract information from both textual and visual data.\n\n**Common Prediction Issues:**\n- Misidentification of auxiliary components as main attributes.\n- Complex expressions of chemical names leading to challenges in standardization.\n\nThe results underscore the nuanced dynamics between document length and model performance and highlight the benefits of specific prompting strategies and self-consistency techniques in improving information extraction accuracy."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Exploring two prompting methods: Named Entity Recognition plus Relation Extraction (NER+RE) and an End-to-End (E2E) approach for zero-shot context in extracting sample lists of polymer nanocomposites from full-length materials science research papers.",
            "experiment_process": "The NER+RE method is divided into two stages. Initially, named entities within the text are identified. Subsequently, the model transforms -ary relation classification into a series of yes/no questions about these entities and their relations. The process becomes computationally expensive as the number of entities increases, leading to exponential growth in potential combinations. The End-to-End (E2E) method directly extracts JSON-formatted sample data from articles, designed to handle the complexity and scale of extracting -ary relations efficiently from scientific texts.",
            "result_discussion": "The E2E prompting method showed better performance compared to the NER+RE approach, attributed to its higher precision. Additionally, the inference time for GPT-4 Turbo (E2E) was faster (sec/article) than GPT-4 Turbo (NER+RE) (sec/article).",
            "ablation_id": "2403.00260v1.No1"
        },
        {
            "research_objective": "Implementing self-consistency to enhance the reasoning abilities of LLMs in extracting accurate sample lists.",
            "experiment_process": "Multiple predictions are generated from the model at a controlled temperature. The match score of each predicted sample is determined by its frequency across all predictions. A threshold is applied to filter out low-confidence predictions. For evaluation, a majority vote among predictions is employed.",
            "result_discussion": "Self-consistency enhanced both strict and partial F metrics. Samples that frequently appeared across multiple predictions were retained, suggesting higher confidence. The application of self-consistency proved effective in improving the reliability of the predictions.",
            "ablation_id": "2403.00260v1.No2"
        },
        {
            "research_objective": "Addressing the challenge of LLMs maintaining performance with longer input lengths by condensing articles.",
            "experiment_process": "Using Generalizable T5-based Dense Retrievers (GTR-large), documents are divided into segments and four queries are formulated to extract targeted information. The similarity between each segment and query is calculated, with the top segments combined to form a condensed document version.",
            "result_discussion": "Shortening documents was beneficial in most cases. GPT-4\u2019s performance decreased with excessively shortened documents but was optimal when documents were condensed to the top segments. This balance indicates that reducing document length without losing crucial information is key to maintaining performance.",
            "ablation_id": "2403.00260v1.No3"
        }
    ]
}