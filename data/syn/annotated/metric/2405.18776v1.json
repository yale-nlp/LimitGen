{
    "title": "LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning (Large) Language Models",
    "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) and its variants have been proposed to ensure rigorous privacy for fine-tuning large-scale pre-trained language models. However, they rely heavily on the Gaussian mechanism, which may overly perturb the gradients and degrade the accuracy, especially in stronger privacy regimes. Most state-of-the-art (SOTA) methods have demonstrated high accuracy in case of relatively weaker DP guarantees but not small .\n\nTo address such limitations, we propose a novel Language Model-based Optimal Differential Privacy (LMO-DP) mechanism, which takes the first step to enable the tight composition of accurately fine-tuning (large) language models with a sub-optimal DP mechanism, even in strong privacy regimes. Furthermore, we propose a novel offline optimal noise search method to efficiently derive the sub-optimal DP that significantly reduces the noise magnitude. For instance, fine-tuning RoBERTa-large (with 300M parameters) on the SST-2 dataset can achieve an accuracy of 92.20% by drastically outperforming the Gaussian mechanism. We also draw similar findings on the text generation tasks on GPT-2. Finally, to our best knowledge, LMO-DP is also the first solution to accurately fine-tune Llama-2 with strong differential privacy guarantees. The code will be released soon and available upon request.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently large language models (LLMs) have achieved breakthrough success by effectively processing and encoding huge volumes of text data from extremely large-scale training datasets. For instance, BERT and GPT families have demonstrated state-of-the-art (SOTA) accuracy and enhanced performance in most of the learning tasks. In addition, such (open-sourced) language models are pre-trained on extremely large and generic datasets, and then fine-tuned to accurately support a wide variety of downstream tasks using a relatively smaller dataset in the task domain, e.g., sentence classification, text generation, and code generation.\n\nHowever, deep learning models have been proven to be vulnerable to privacy threats during training. Similar risks are also present in training or fine-tuning (large) language models, which could potentially lead to the leakage of sensitive information. A notable distinction in the case of training language models is that, since pre-trained datasets and models have been published, they have already rendered privacy leakage from the open-sourced datasets and models. Thus, it is desirable to privately fine-tune language models by protecting the sensitive information in the new dataset used for fine-tuning.\n\nTo mitigate privacy risks in deep learning training and fine-tuning, differential privacy (DP) has been widely recognized as the de facto rigorous privacy model where adding or removing any data sample, or user in the (training) data would not cause significant leakage. In particular, the well-known Differentially Private Stochastic Gradient Descent (DP-SGD) method tightly balances the privacy and utility within the training, leveraging the privacy parameters in the Gaussian mechanism. It achieves this by constraining the influence of individual examples through gradient clipping and adding Gaussian noise to the gradients within each batch, providing a DP guarantee for model training. To our best knowledge, the SOTA methods for privately fine-tuning language models (which are also DP-SGD variants) mainly focus on optimizing the gradient clipping mechanism to enhance utility and/or improve system performance (e.g., reducing memory) while maintaining privacy.\n\nAlthough Gaussian-based DP mechanisms can be tightly accounted with the Moments Accountant in DP-SGD, the magnitude of the noise itself is far from optimal, especially for small privacy budget. Then, it may overly perturb the gradients and degrade the accuracy. Therefore, the performance of DP-SGD and other variants (e.g., gradient clipping or memory-reducing mechanisms) can be significantly improved by designing an optimal or sub-optimal noise for the same DP guarantee. Such novel and orthogonal noise-reduction method would make fine-tuning language models with strong DP guarantees practical, e.g., boosting the accuracy of fine-tuning RoBERTa-large (with 300M parameters) on the SST-2 dataset."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Contributions",
            "text": "To boost the tradeoff between privacy and accuracy for language models, we propose a novel Language Model-based Optimal Differential Privacy (LMO-DP) mechanism, which works as a plug-and-play model-agnostic module to strengthen private training performance. To this end, we design a novel LMO noise and adapt it to the LM training/fine-tuning process by replacing the Gaussian noise in the SOTA methods, e.g., DP-SGD and/or the variants (e.g., Ghost Clipping 8  ###reference_b8###). Our LMO noise is generated from a two-fold mixture distribution derived from an optimal differential privacy framework (R2DP 14  ###reference_b14###) in which the first fold is a Laplace distribution and the second fold is a combination of possible linear weighted probability density functions (PDFs).\n\nThen, our major contributions are further discussed below.\n(1) First non-Gaussian DP mechanism for LM fine-tuning.\nTo our best knowledge, we propose the first non-Gaussian mechanism for privately fine-tuning language models.\nWe establish a meticulously defined search space of PDFs while upholding a universal DP privacy guarantee (a subspace within the PDF space, incorporating randomization into the scale parameter of Laplace noise) 14  ###reference_b14###.\nDifferent from 14  ###reference_b14###, we instantiate this randomization as a linear combination of Gamma, Exponential, and Uniform distributions due to their computational convenience and ability to approximate the entirety of the search space of PDFs. This also enables us to formulate privacy and utility within a unified framework for searching the noise parameters.\n(2) First DP mechanism supports strong privacy guarantee for fine-tuning language models and LLMs. To our best knowledge, we take the first step to fine-tune (large) language models with strong -DP guarantees such as  and .\nMeanwhile, we found that LMO-DP achieves superior convergence rates (empirically) in a diverse range of LM tasks (e.g., sentiment classification, and table-to-text generation) on models with parameters ranging from 300 million (e.g. RoBERTa, BERT) to 7 billion (e.g., Llama2-chat-7b)333Searching the optimal noise will be executed as a pre-processing procedure before the fine-tuning under the DP-SGD framework. Then, LMO-DP would only incur minor extra runtime during private fine-tuning.\n(3) Accuracy-boosting module to SOTA methods. Recall that some SOTA methods focus on optimizing the gradient clipping to improve the system performance, e.g., low memory by Ghost Clipping 8  ###reference_b8###. Since LMO-DP mainly optimizes the randomization for DP mechanisms offline (pre-processing), it is orthogonal to those DP fine-tuning methods for LMs, e.g., 11  ###reference_b11###. Thus, LMO-DP can inherit all the benefits of existing methods via integration with them, e.g., memory reduction via Ghost Clipping 8  ###reference_b8###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "DP-SGD was initially devised for private neural network training. However, a significant challenge with traditional DP-SGD lies in compromised performance and the substantial time and memory overhead of private training. Researchers are actively addressing it by reducing online training costs and improving performances for LMs through more efficient parameter tuning. For instance, some have enhanced DP-SGD by exploring parameter-efficient tuning methods that focus on training only a fraction of the model parameters, resulting in improved utility, privacy, and reduced overheads. Another advancement introduces a model-agnostic DP bias-term fine-tuning (DP-BiTFiT) framework. It prioritizes optimizing the bias rather than model weights, and achieves efficiency by activating only the backward hook in PyTorch, thus saving time and space.\n\nMoreover, recent works have proposed clipping methods to reduce computational time and memory requirements for large language models. Some introduced the ghost clipping method, significantly reducing memory usage during training and enhancing performance in text classification and generation tasks. Another proposed adaptive group-wise clipping, encompassing per-layer, and per-device clipping techniques, suitable for deployment on multiple accelerators. A mixed ghost clipping method on convolutional layers was proposed, significantly easing the private training in terms of both time and space while maintaining accuracy. A novel book-keeping (BK) technique enhances computational efficiency by eliminating the need for a second back-propagation step in Ghost Clipping, while preserving the same accuracy."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "DP-SGD",
            "text": "We first provide the formal definition of differential privacy (DP). Let  be the domain of datasets, and let  be two adjacent datasets in  such that  can be obtained from  by adding or removing one sample. A randomization mechanism  satisfies -differential privacy if, for any  and , and for all , , where  is a sample space by randomization  to generate the output. For instance, Laplace mechanism is a commonly used technique for DP, relying on Laplace distributions.\n\nDP-SGD generally refers to the DP during deep learning training with the Gaussian mechanism. This has been widely used to ensure differential privacy during deep learning training. Mostly it applies the Gaussian mechanism for DP, which first clips the gradients  using a threshold  for -sensitivity  and then injects the Gaussian noise  to these clipped gradients within each batch where  is the -th training sample.\n\nThe biggest challenge DP-SGD successfully tackles is bounding  and  during thousands of rounds of training, each of which accounts for spending a portion of the privacy budget. While existing literature relied on the strong composition theorem, which, although effective, can be imprecise and overlook the noise distribution specifics, DP-SGD introduces the moments accountant which utilizes probabilistic insights to provably tighten the budget. In particular, the overall  budget is reduced to  in DP-SGD."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "R\u00e9nyi Accountant",
            "text": "Given a sequence of random mechanisms with associated DP budgets, the cumulative privacy guarantee during the training/fine-tuning process can be assessed in terms of specified privacy parameters while controlling the privacy failure probability. Many recent works provide tighter privacy accounting for DP, which could be applied to formulate similar constraints. These new accountants with tighter bounds may require different solvers."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Problem Formulation: General Private Training Problem Definition",
            "text": "We denote the fine-tuning dataset as , where  and . This dataset is used to fine-tune a (large) language model  within  steps. Let  denote the noisy-clipped model gradients after  () steps of DP training/fine-tuning,  and  are adjacent datasets. Assuming the existence of an optimal noise generation mechanism  that enhances the tradeoff between privacy and accuracy during training/fine-tuning, we formulate an optimization problem for fine-tuning a (large) language model for a specific NLP task under the constraint of -Differential Privacy (DP) as follows 26  ###reference_b26### (in Figure (1  ###reference_###)): \n\nWhere  is the batch size of Poisson-sampled indices ,  is the predicted probability of the correct class for sample  in ,  is the noisy gradient, and  is the RDP to -DP conversion. Given the noise searching space  for gradient, we denote the optimal noise parameters of the mechanism  for the above problem (2  ###reference_###) as . Solving this optimization problem is challenging since the noisy gradients  are optimized or computed by the randomly sampled input data , the model parameters at the previous step , and the randomization mechanism . Meanwhile, the optimal randomization mechanism  searches  from the search space  to improve the NLP task-dependent utility metrics such as accuracy (e.g., minimizing the cross-entropy loss). ###figure_1### \n\nUpdating the noise parameters during the training or fine-tuning process seems like an intuitive proposal. However, we have found that simultaneously optimizing both the model parameters and the noise creates a complex interdependence where each optimization process relies on the prerequisite of the other. This may require significant computation during fine-tuning/training or initially introduce a large noise that causes vanishing or exploding gradients 27  ###reference_b27###, extending the training or fine-tuning time. Thus, we propose to optimize the noise offline (while satisfying ()-DP from end to end), and inject the noise to the clipped gradients online, detailed in Section 4  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Private Fine-Tuning with LMO-DP",
            "text": "In this section, we define the constraints for the gradients of fine-tuning/training. Then, we define our LMO randomization mechanism and propose the Language Model-based Optimal Differential Privacy (LMO-DP) mechanism to solve the optimization problem. Specifically, we utilize an optimal noise generation mechanism to disentangle the complex optimization problem into two phases: (1) offline noise optimization, and (2) model fine-tuning (parameters optimization) with the optimized noise injected to the clipped gradients. Although this two-phase optimization approach relaxes the global optimum, in practice, such effect can be relieved since the gradients \\(g\\) are clipped by \\(\\bar{g}\\) for any model parameters \\(\\theta\\) during fine-tuning. Then, the noise optimization and model parameter optimization can be relatively separated. It eliminates the dependence on training data \\(D\\), \\(A\\), and model parameters \\(\\theta^*\\). Meanwhile, it can ensure \\((\\epsilon, \\delta)\\)-Differential Privacy during the fine-tuning process while promoting the performance."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Phase (1): R\u00e9nyi Accountant based Offline Noise Optimization",
            "text": "Specifically, we leverage the fine-tuning/training dataset, where  and , to fine-tune a language model  within  steps. When we apply a randomization mechanism  to model fine-tuning under -Differential Privacy, assuming we have the optimal parameters  searched from the given space  with a defined objective (e.g., usefulness 14), the constraint in Problem 2 can be simplified to the following noise searching problem:\n\nwhere  is the clipping threshold."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "A Versatile Search Domain:  Space",
            "text": "First, we define a versatile search domain by instantiating the randomization optimization in R2DP (two-fold randomization for optimal DP, see more details in Appendix A) for language model fine-tuning. Specifically, given the Laplace distribution as the first-fold randomization, we define the space by randomizing the scale parameter () according to mixture distributions (as the second-fold randomization), which are defined by multiple positive-supported probability density functions (PDFs).\n\nThis dual randomization process creates a space where elements are the moment-generating functions (MGFs) of the second-fold PDFs. This construction offers versatility by enabling linear combinations of various positively supported PDFs in the second-fold PDF, thanks to the MGF composability (Appendix A.2).\n\nThese classes of PDFs, known to be high-entropy PDFs, are characterized by two essential (appealing) properties: (1) comprehensiveness in (approximately) covering all (demonstrated empirically), and (2) a universal DP guarantee function for all LMO, to support/facilitate solving the optimization problem.\n\nRegarding comprehensiveness, we empirically assess LMO-DP noise comprehensiveness compared to a universally simulated space via a novel quantification test defined using several well-known metrics such as KL divergence, distance, and earth mover\u2019s distance (EMD). The quantification is detailed in Algorithm 2 in Appendix C. Figure 5 in Appendix C.1.1 shows that the Space aligns closely with the simulated space. We note that in general, quantifying the comprehensiveness of a subset of probability functions lacks a universally accepted measure. While this search space may not encompass the entire space, we will show its sufficiency for near-optimal accuracy through numerical results (see Figure 7 in Appendix C.1.1) and experiments in various learning settings (see Section 5)."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "LMO-DP Mechanism",
            "text": "From the constructed search space, we define the LMO-DP mechanism, which is a sub-optimal noise generation mechanism that is instantiated from the R2DP mechanism (it optimizes the DP mechanism for generic queries with different mixture of randomization mechanisms) and adapted to the domain of language model fine-tuning. In our context, LMO-DP refers to the DP during language model fine-tuning/training with the LMO randomization mechanism. Considering the balance between computation usage and evaluation precision, we choose Gamma distribution, Exponential distribution, and Uniform distribution to formulate our LMO-DP mechanism as follows:\n\nDenote the Laplace scale parameter in the LMO-DP mechanism is modeled as a random variable, which follows a linear weighted distribution built by a mixture of the Gamma distribution, the Exponential distribution, and the Uniform distribution.\n\nThe LMO noise parameters are searched from the specified space.\n\nGiven the LMO noise parameters, the moment generating functions (MGFs) are defined as follows: where are the weights for the Gamma, Exponential, and Uniform distributions, respectively; are the MGFs of Gamma distribution, Exponential distribution, and Uniform distribution. Specifically, we provide the formulation in detail.\n\nWith the defined search space, we also provide an algorithm to search the optimal noise for the problem in Algorithm 3. In addition, Appendix E.3 demonstrates a significant improvement over the Gaussian noise. Note that the ablation study for the mixture distribution is given in Appendix E.4."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Phase (2): LMO-DP based Private Language Model Fine-tuning",
            "text": "After searching the optimal noise offline in Phase (1), the private fine-tuning in Phase (2) is similar to DP-SGD, e.g., clipping gradients, and injecting the optimized noise to the clipped gradients (detailed in Algorithm 1  ###reference_###)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Recent SOTA methods achieve high accuracy (e.g., ) on large privacy budgets (). However, it remains unclear how these models perform when subjected to stronger DP guarantees, characterized by a privacy budget  (with  ranging from  to ).\nFollowing 8  ###reference_b8###, we use the full datasets and all the classes in the original datasets for fine-tuning. Our results highlight the versatility (higher accuracy with fewer steps), universality (including sentence classification 4  ###reference_b4### and table-to-text generation 2  ###reference_b2###) and other related NLP tasks in 1  ###reference_b1###, 29  ###reference_b29###, 2  ###reference_b2###, 30  ###reference_b30###. In addition, we take the first step to privately fine-tune large language models (i.e., Llama 2) with strong privacy guarantees (see the accurate results in Section 5.4  ###reference_###). We focus on strong DP guarantees (e.g., ) and set  unless otherwise specified."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Performance on Sentence Classification",
            "text": "###figure_2### ###figure_3### ###figure_4### ###figure_5### We evaluate the LMO-DP mechanism on sentence classification tasks aiming to distinguish between positive and negative emotions with GLUE benchmarks (MNLI-m, SST-2, QNLI, and QQP datasets) 4  ###reference_b4### with experiments conducted on RoBERTa-base, RoBERTa-large 1  ###reference_b1###, BERT-base, BERT-large 29  ###reference_b29### and Llama2-chat-7b 30  ###reference_b30###, compared with the non-private training and baseline DP-SGD method 8  ###reference_b8###. In detail, SST-2 has more than 60k+ samples in the training set; QNLI has more than 100k+ samples; MNLI and QQP contain more than 350k but less than 400k samples for each dataset. SST-2, QNLI, and QQP include two classes each; MNLI includes three classes.\n###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### In Figures 2  ###reference_### and 3  ###reference_###, the -axis shows total privacy loss  and the -axis shows the accuracy under specific  after -step composition during training/fine-tuning. When , we have accuracy that matches or surpasses that of the baseline; When , our mechanism largely outperforms the baseline, exhibiting close performance with non-private training.\nBesides, we observed that our method can reach the same accuracy with fewer steps compared to the DP-SGD baseline method 8  ###reference_b8###, especially under strong DP budgets. Considering baseline methods may not achieve the same high accuracy as the LMO-DP method, we report the steps that these two methods reach the same accuracy for each . In particular, Specifically, we evaluate the steps that need to reach 48.51% accuracy and 41.83% accuracy for the BERT-base and RoBERTa-base on the MNLI-m dataset; the steps that need to reach 70.49% accuracy and 71.09% accuracy for the BERT-base and RoBERTa-base on the QQP dataset; the steps that need to reach 48.98% accuracy and 45.67% accuracy for the BERT-large and RoBERTa-large on the MNLI-m dataset; the steps that need to reach 63.18% accuracy and 67.84% accuracy for the BERT-large and RoBERTa-large on QQP dataset. From Figure 4  ###reference_###, we conclude that we can reduce the  training steps when .\nComparison in Appendix E.2  ###reference_### shows a significantly smaller perturbation of LMO noises, the incorporation of LMO-DP noise in private training may intriguingly lead to faster convergence due to the dynamics of LMO-DP noise.\nHence, LMO noise effectively addresses two challenges (accuracy and convergence) simultaneously."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Table-to-Text Generation Task",
            "text": "We conduct the LMO-DP and DP-SGD baseline methods on a table-to-text generation task which generates the descriptions of table entries. We fine-tune the GPT-2 model 2  ###reference_b2### on the E2E dataset 3  ###reference_b3### with , evaluating five metrics in Table 1  ###reference_###.\nWe evaluate LMO-DP and DP-SGD 8  ###reference_b8### under a fixed  (same setting as 8  ###reference_b8###). We also apply the same settings of privacy budget at each iteration, weights, and clipping threshold as the sentence classification.\nWe only employ a batch size of  which causes the total privacy budget to be less than .\nTable 1  ###reference_### presents the results with five different metrics by following 2  ###reference_b2###. We observe that LMO-DP yields results that are more closely aligned with the non-private results (larger values of all these metrics exhibit more accurate generated texts). It is worth noting that the improvement can be up to  on some metrics (e.g., CIDEr). The ROUGE-L of both LMO-DP and DP-SGD can be slightly higher than the original values (the last pair of results) since they are both fine-tuned based on the same LMs with rich vocabulary and downstream dataset and thus have a greater chance of generating approximate texts."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Boosting Accuracy for the Existing Methods",
            "text": "Our LMO-DP mechanism can be implemented as a plug-and-play module to other orthogonal methods (e.g., memory reduction methods such as Ghost Clipping 8  ###reference_b8### or parameter efficiency 2  ###reference_b2### during real-time private training. We take sentence classification as an example. In detail, we conduct sentence classification task with the SST-2 dataset on the RoBERTa-large model.\nAs , the best accuracy of SOTA methods and LMO-DP would be close to each other (we validated this). Thus, the improvement for LMO-DP is marginal, and then we focus on the strong DP with smaller  ( is set to be close to 0, e.g., ). The top two rows in Table 2  ###reference_### show that LMO-DP can significantly boost the accuracy for Ghost Clipping 8  ###reference_b8### from  to . As illustrated in the last four rows of Table 2  ###reference_###, we also found that the accuracy of other SOTA methods 2  ###reference_b2###, 9  ###reference_b9###, 11  ###reference_b11### have low accuracy in case of small  (strong DP).66610  ###reference_b10### is not open-sourced. We include its result (\u2217) for ,  given training size . Given high noise reduction by LMO-DP compared to the Gaussian mechanism, we anticipate that LMO-DP can also drastically boost their accuracy to  similar to Ghost Clipping 8  ###reference_b8###."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Performance on the Llama 2 Model",
            "text": "Different from all existing works (e.g., 2  ###reference_b2###, 9  ###reference_b9###, 11  ###reference_b11###), we take the first step to implement our new LMO noise to privately fine-tune the Llama2-7b-chat model on the SST-2 dataset for sentiment classification with strong -DP guarantees. As demonstrated in Table 3  ###reference_###, both non-private () and LMO-DP private fine-tuning can converge to high accuracy (non-private fine-tuning converges relatively faster). This confirms that LMO-DP also works effectively on LLMs (due to smaller impact on the overall model by each single data sample, we anticipate that such high accuracy can be maintained in other tasks)."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose a Language Model-based Optimal Differential Privacy (LMO-DP) mechanism, allowing for accurate private fine-tuning of large language models even in very strict privacy settings. To our best knowledge, LMO-DP is the first non-Gaussian mechanism that can generate sub-optimal noise to ensure strong DP, and the first mechanism to support LLMs. It can also significantly boost the performance (e.g., accuracy and convergence) of DP-SGD and other variants with high noise reduction, as demonstrated in the experiments."
        }
    ],
    "url": "http://arxiv.org/html/2405.18776v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.18776v1",
        "paper_title": "LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning (Large) Language Models",
        "research_background": "### Paper Motivation\n\nThe paper addresses the critical issue of privacy in training and fine-tuning large language models (LLMs). Given that these models often process sensitive data, there is a substantial risk of sensitive information leakage during both training and fine-tuning phases. Therefore, the motivation behind this work is to enhance privacy guarantees without compromising the utility (accuracy and performance) of these models.\n\n### Research Problem\n\nThe primary research problem is to improve the performance of differentially private (DP) mechanisms used in the fine-tuning of large language models. The existing methods, particularly Differentially Private Stochastic Gradient Descent (DP-SGD) and its variants, rely on Gaussian noise mechanisms, which can degrade model accuracy due to overly perturbing gradients. This paper seeks to design an optimal or sub-optimal noise mechanism that maintains strong DP guarantees while reducing the noise magnitude to enhance the model's utility.\n\n### Relevant Prior Work\n\n1. **BERT and GPT**: These models have set state-of-the-art accuracy benchmarks in numerous tasks, highlighting the importance of efficient fine-tuning on large-scale and diverse datasets.\n2. **Privacy Threats in Deep Learning**: Previous studies have demonstrated the vulnerability of deep learning models to privacy issues during training, raising concerns about sensitive data leakage.\n3. **DP Mechanisms in Deep Learning**: Differential privacy, specifically through methods like DP-SGD, is recognized for mitigating these privacy risks. These methods involve gradient clipping and adding Gaussian noise to ensure privacy.\n4. **Fine-Tuning with Differential Privacy**: State-of-the-art methods for privately fine-tuning LLMs mainly focus on optimizing gradient clipping to balance privacy and utility, and some also aim to reduce system performance bottlenecks like memory usage.\n5. **Moments Accountant**: This technique accounts for the tightness of privacy guarantees under the Gaussian mechanism in DP-SGD, but the inherent noise can still degrade performance, particularly for small privacy budgets.\n\nIn summary, this paper positions itself to advance the state-of-the-art in differentially private fine-tuning of large language models by optimizing the noise mechanism used in DP-SGD, addressing the trade-off between privacy and utility more effectively.",
        "methodology": "### Methodology\n\nIn this section, we first define the constraints for the gradients of fine-tuning/training with the R\u00e9nyi Accountant.  Then, we define our LMO randomization mechanism and propose the **Language Model-based Optimal Differential Privacy (LMO-DP)** mechanism to solve the optimization problem. Specifically, we utilize the characteristics of R\u00e9nyi divergence and an optimal noise generation mechanism to disentangle the complex optimization problem into two phases:\n\n1. **Offline Noise Optimization**\n2. **Model Fine-Tuning (Parameters Optimization) with Optimized Noise Injected to the Clipped Gradients**\n\nThe approach, although it relaxes the global optimum, introduces a method where the gradients \\( g \\) are clipped by a constant for any model parameters during fine-tuning. This allows us to separate the noise optimization and model parameter optimization processes effectively. \n\nKey components and innovations of the LMO-DP methodology include:\n\n- **Defining Constraints with R\u00e9nyi Accountant**: This involves setting up appropriate constraints based on the R\u00e9nyi divergence to control the privacy loss during fine-tuning.\n- **LMO Randomization Mechanism**: A novel mechanism specifically tailored to balance privacy and utility in the context of language model fine-tuning.\n- **Two-Phase Optimization**: \n  - **Phase 1: Offline Noise Optimization**: Leverages an optimal noise generation mechanism to pre-calculate noise, ensuring privacy without active dependency on training data or model parameters.\n  - **Phase 2: Model Fine-Tuning**: Involves injecting the pre-optimized noise into the clipped gradients while fine-tuning the model parameters.\n- **Ensuring \\(\\epsilon\\)-Differential Privacy**: By neatly decoupling noise optimization from parameter updates, the method guarantees differential privacy compliant with specified \\(\\epsilon\\) values throughout the fine-tuning process.\n  \nThis innovative methodology eliminates the dependence on training data and model parameters during the noise optimization phase, addressing a significant challenge in differentially private training of large language models.",
        "main_experiment_and_results": "## Main Experiment Setup and Results:\n\n### Dataset:\nThe experiments utilize the full datasets and all the classes present in the original datasets for fine-tuning, ensuring a comprehensive evaluation across different NLP tasks.\n\n### Baselines:\nThe study compares its methods against recent state-of-the-art (SOTA) approaches that achieve high accuracy on large privacy budgets. Specific baseline methods are not detailed in the given information but are implied to be those recently excelling under less stringent privacy budgets.\n\n### Evaluation Metrics:\nThe primary metric for evaluation is accuracy, used to measure the performance of the fine-tuned models across various NLP tasks.\n\n### Experimental Results:\nThe findings underscore the proposed method's versatility, achieving higher accuracy with fewer training steps. Universality is demonstrated through the application to different NLP tasks, including:\n- Sentence classification, as referenced in ###reference_b4###.\n- Table-to-text generation, as referenced in ###reference_b2###.\n- Additional NLP tasks identified in references ###reference_b1###, ###reference_b29###, ###reference_b2###, and ###reference_b30###.\n\nA notable contribution is the fine-tuning of large language models, specifically Llama 2, under strong privacy guarantees, showcasing accurate results as elaborated in Section 5.4 ###reference_###. The experiments primarily focus on strong differential privacy (DP) guarantees, generally setting the privacy budget at  unless specified otherwise, with being experimented within a range from  to .\n\n### Summary of Findings:\nThe proposed method demonstrates higher accuracy and efficiency (requiring fewer steps) and proves applicable across a diverse set of NLP tasks, maintaining strong privacy guarantees, especially when fine-tuning large language models like Llama 2."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the LMO-DP mechanism on sentence classification tasks to distinguish between positive and negative emotions and compare the performance with non-private training and the baseline DP-SGD method.",
            "experiment_process": "The study uses the GLUE benchmarks (MNLI-m, SST-2, QNLI, and QQP datasets) and experiments are conducted on RoBERTa-base, RoBERTa-large, BERT-base, BERT-large, and Llama2-chat-7b models. Comparisons are made with non-private training and the DP-SGD baseline method. Accuracy and training steps are compared at privacy budgets ranging from \u03b5=1 to \u03b5=7.",
            "result_discussion": "The LMO-DP mechanism matches or outperforms baseline accuracy, particularly under strong DP budgets (\u03b5=1). LMO-DP requires fewer training steps to reach the same accuracy compared to the DP-SGD baseline. It effectively addresses challenges of accuracy and convergence, showing smaller perturbation and faster convergence due to the dynamics of LMO-DP noise.",
            "ablation_id": "2405.18776v1.No1"
        },
        {
            "research_objective": "Evaluate the impact of LMO-DP compared to DP-SGD on a table-to-text generation task.",
            "experiment_process": "Conducted on the E2E dataset with GPT-2 model, evaluations are made on five metrics. The settings involve a fixed privacy budget with a batch size of less than 50, ensuring the privacy budget \u03b5 is less than 0.75. Same settings of privacy budget, weights, and clipping threshold are used as in sentence classification.",
            "result_discussion": "LMO-DP yields results closely aligned with non-private results, exhibiting improvements up to 2.88% on some metrics like CIDEr. The ROUGE-L scores for both LMO-DP and DP-SGD are slightly higher than original values due to shared rich vocabularies and downstream datasets.",
            "ablation_id": "2405.18776v1.No2"
        },
        {
            "research_objective": "Demonstrate that LMO-DP can boost the accuracy of existing methods with strong DP guarantees.",
            "experiment_process": "The study integrates LMO-DP into Ghost Clipping for the SST-2 dataset on the RoBERTa-large model, focusing on strong DP settings (\u03b5 near 0). Comparison is made between Ghost Clipping baseline and LMO-DP-enhanced methods.",
            "result_discussion": "LMO-DP significantly boosts Ghost Clipping accuracy in strong DP settings from 86.63% to 91.57%. Other SOTA methods also show low accuracy under strong DP without LMO-DP. LMO-DP's noise reduction is anticipated to boost their accuracy similarly.",
            "ablation_id": "2405.18776v1.No3"
        },
        {
            "research_objective": "Assess the efficacy of LMO-DP on fine-tuning the Llama2-7b-chat model with strong DP guarantees.",
            "experiment_process": "LMO noise is applied to fine-tune Llama2-7b-chat on the SST-2 dataset for sentiment classification. Privacy guarantee \u03b5 is set to strong DP constraints. Comparison is made between non-private and LMO-DP fine-tuning.",
            "result_discussion": "Both non-private and LMO-DP fine-tuning achieve high accuracy, with non-private fine-tuning converging faster. This confirms that LMO-DP effectively maintains high accuracy on LLMs and potentially other tasks due to reduced noise impact on the model.",
            "ablation_id": "2405.18776v1.No4"
        }
    ]
}