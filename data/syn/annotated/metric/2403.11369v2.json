{
    "title": "What Makes Math Word Problems Challenging for LLMs?",
    "abstract": "This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs. Our code, data, and analysis are publicly available at github.com/kvadityasrivatsa/analyzing-llms-for-mwps",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, large language models (LLMs) have demonstrated huge potential across a range of core NLP tasks and exhibited a number of emergent abilities, such as an ability to solve mathematical puzzles. Math word problems (MWPs) have been proposed as a challenging testbed for LLMs, as they test not only the ability of the models to deal with purely mathematical expressions but also their reasoning and natural language understanding abilities. Experiments show that even quite powerful LLMs are still challenged by MWPs. Most previous work has either focused on evaluation of LLMs\u2019 performance on MWPs or on changes in their behavior in response to progressive-hint prompting, prompt paraphrasing, or similar approaches, while an in-depth analysis of what exactly makes math problems challenging for LLMs is lacking. We aim to address this knowledge gap.\n\nA recent study demonstrates a strong connection between reading skills and math outcomes in students. We hypothesize that LLMs\u2019 ability to solve MWPs correctly may similarly rely on: (1) the linguistic complexity of the questions; (2) the conceptual complexity of the tasks (e.g., the number of steps and types of math operations involved); and (3) the amount of real-world knowledge required to solve the tasks. Supporting this intuition, our preliminary analysis suggests that relatively short questions with a small number of described entities, a few calculation steps, and a limited range of operators involved in the solution are typically addressed effectively by a range of LLMs. At the same time, long questions requiring real-world knowledge and extended natural language understanding pose challenges for LLMs.\n\nIn this paper, we formulate and investigate two research questions: (1) Which characteristics of the input math word questions make them complex for an LLM? and (2) Based on these characteristics, can we predict how a particular LLM will engage with specific input MWPs?"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "We use the GSM8K dataset Cobbe et al. (2021 ###reference_b4###), divided into training and test instances, because of the high quality of human-generated MWPs. This dataset contains a diverse set of problems in English with minimal amount of recurring templates. Furthermore, the difficulty level of the problems is tailored for LLMs, allowing for a wide variation in correctness across models and question types, which is ideal for our feature-based analysis.\n\nWe collect solution attempts from several LLMs to the questions from the GSM8K training and test sets. Our approach allows us to investigate which of the features are most indicative of the challenges LLMs face in solving math problems.\n\nWe select an array of open-source models for our experiments. We use Llama2 (13B and 70B) Touvron et al. (2023 ###reference_b14###), Mistral-7B Jiang et al. (2023 ###reference_b6###), and MetaMath-13B Yu et al. (2023 ###reference_b17###).\n\nWe analyze and experiment with the features extracted from MWP questions and their respective expected solutions. This way, the features remain grounded in the dataset, allowing our approach to be applied to any LLM. The features are broadly grouped into the following categories:\n\nLinguistic features focus on the phrasing of the question. These include the length of the question, sophistication of the vocabulary, syntactic complexity, instances of coreference, and overall readability. Note that the linguistic features are only extracted from the question body as the phrasing of the gold solution has no impact on the expected answer.\n\nMathematical features cover the math arguments, operations, and reasoning steps required to solve the questions. These include the number and diversity of the math operations in the solution body. Arguments provided in the question but not utilized in the solution also require mathematical reasoning for them to be disregarded as noise. Note that while a question can be phrased in many ways (affecting its linguistic features), the underlying math operations and reasoning steps (thus, the mathematical features) remain unchanged.\n\nReal-world knowledge & NLU based features indicate the amount of extraneous information needed to solve the task that is not provided explicitly in the question. This may include how many days there are in a month or the interpretation of \u201chalf\u201d as."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We use Logistic Regression, Decision Tree, and Random Forest classifiers, which allow us to extract relative feature importance with ease. We employ several preprocessing steps including dropping highly correlated features, class-balancing, and feature scaling. We also perform a hyperparameter search for each model to maximize performance on unseen data. See Appendix C for more details."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Solution Generation",
            "text": "To collect solution attempts from the LLMs, we use a simple task-specific prompt (See Appendix B) to minimize any bias imposed on the model generation. We query each LLM times on each question with varying generation seeds and a temperature of . A soft-matching strategy is then used to extract the final answer from the solutions."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Success Rate Prediction",
            "text": "We train and evaluate classifiers on their ability to predict for input test questions whether they will be answered correctly or incorrectly by a specific LLM. We also train and evaluate classifiers on the intersection set of questions, which are either solved correctly by all or by none of the LLMs. We use Logistic Regression, Decision Tree, and Random Forest classifiers, which allow us to extract relative feature importance with ease.\n\nWe employ several preprocessing steps including dropping highly correlated features, class-balancing, and feature scaling. We also perform a hyperparameter search for each model to maximize performance on unseen data. See Appendix C for more details."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Success Rate Distribution",
            "text": "We observe that Llama2 13B and 70B follow the expected order of scores along their respective parameter counts. Mistral-7B scores similar to the 13B Llama2 model, and the additional fine-tuning allows MetaMath-13B to surpass the other models (including the 70B Llama2). Figures 2(a) and 2(b) respectively capture the number of questions always and never answered correctly by each LLM. Overall, MetaMath-13B has the lowest number of incorrectly and the highest number of correctly answered questions across the tested LLMs."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Classification Results",
            "text": "To compare classifiers\u2019 performance, we report the accuracy and macro-F1 scores for each classifier and LLM-specific test data split (see Table 2). We observe that Random Forest outperforms other classifiers across most solution sets.\n\nAt the same time, we also note that, due to significant class imbalance, this task is not easy for the classifiers, with the best accuracy scores across LLM splits being in the range of . The small number of questions always or never solved correctly by any LLM speaks to the models\u2019 varying capabilities (and potential points of brittleness). We include additional analysis of the results in Appendix D.\n\nFor comparison, we also report the classification results for a fine-tuned RoBERTa-base model Liu et al. (2019) for the same training and evaluation sets (tuned on the question and gold solution as input text; see Appendix C for more details) in Table 2. We note that the Transformer base classifier scores on a par or a few points above the best statistical classifier, i.e., Random Forest, suggesting that the proposed feature-based classifiers are not far behind token-level contextual models for this task."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Feature Importance",
            "text": "The statistical classifiers used in our experiments allow us to estimate the importance of each feature and its contribution to the classification performance. We report the top features with the highest aggregate ranks across LLM data splits and classifiers in Table 3. We use mean rank here as a proxy for relative importance across features, and the respective standard deviations indicate how spread out this importance is across classifiers and queried LLMs. We observe that a greater number (Gx_op_unique_count) and diversity (Gx_op_diversity) in math operations, and the use of infrequent numerical tokens in the question and solution body (Qx_ & Gx_mean_numerical_word_rank) are important. The list also contains linguistic features based on the phrasing of the questions: longer questions with a high number of noun phrases (Qx_np_count), mean syntactic depth (Qx_constituency_tree_depth), and readability grade (Qx_flesch_kinkaid_grade) are also difficult for LLMs to solve. Additionally, the need for extraneous information (Gx_world_knowledge), such as conversion units for time, distance, or weight, can make a question challenging."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Ablation Studies",
            "text": "To further measure the impact of each feature type, we report classification scores along different feature-type subsets in Figure 3. We note that the feature set with all types (L+M+W) is not optimal for classification. For instance, the questions answered by Llama2-13B are best classified using only mathematical features (M). The best-performing classifiers for Llama2-7B, MetaMath-13B, and the intersection set either solely use linguistic features (L) or both linguistic and math features (L+M), whereas the world knowledge & NLU feature set if sufficient for Mistral-7B.\n"
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 Impact of Linguistic Features",
            "text": "In order to better gauge the impact of linguistic features, we cluster questions by mathematical features. We fit a KMeans clustering model on all math features for each question in the GSM8K training set with a target cluster count. This helps group together questions from the data, wherein the math features hardly vary within each question subset (or cluster). Thus, variations across the questions within a cluster can be more clearly attributed to other, i.e., linguistic types of features. The strong and significant feature-wise negative correlations suggest that for a relatively fixed set of math features, questions with greater length, nesting, lexical rank, and reading grade become more challenging for LLMs to solve. Note that this form of analysis on feature-based minimal pairs is extractive in nature and may, to a certain extent, be restricted to the question types in the GSM8K dataset. For a more exhaustive analysis for each feature, generative approaches to furnish question paraphrases with the desired set of linguistic features need to be employed."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This work aims to identify what aspects of MWPs make them difficult for LLMs to solve. To this end, we extract key features (spanning linguistic, mathematical, and real-world knowledge & NLU-based aspects) to predict whether several LLMs can reliably solve MWPs from GSM8K. We find that questions with a high number and diversity of math operations using infrequent numerical tokens are particularly challenging to solve. In addition, we show that lengthy questions with low readability scores and those requiring real-world knowledge are also seldom solved correctly. Our future work will rely on these findings to make informed modifications to questions in order to study the impact on LLMs\u2019 reasoning and MWP-solving abilities. Figure 4 provides an example of an informed modification, which leads to improved LLM performance."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Feature Details",
            "text": "Below, we describe the features used in our study and how they were extracted.\n(1) Linguistic features (L) include  features pertaining to the question (Q) itself:\nQx_token_length: The number of tokens in the tokenized version of the question body. We apply each LLM\u2019s respective tokenizer from HuggingFace  ###reference_huggingface.co/### to extract this feature.\nQx_sentence_length: The number of sentences detected in the question body. We use the sentence_splitter  ###reference_/### Python library to extract this count.\nQx_word_length: The number of space-separated segments (words) in the question body.\nQx_flesch_kinkaid_grade: The readability grade of the question body as per the FKGL metric Flesch (1948  ###reference_b5###). We use the textstat  ###reference_### Python library to extract this feature.\nQx_mean_word_rank: The mean vocabulary rank (in decreasing order of frequency) of the tokens in the question body. We use the same tokenizer set used for Qx_token_length.\nQx_constituency_tree_depth: The mean depth of the constituency tree across the sentences in the question body. We use Stanford\u2019s Stanza  ###reference_### parsing library to parse the questions.\nQx_np_count: Number of distinct noun phrases detected in the question body. We extract this from the constituency parse collected from the Stanza parser.\nQx_prp_count: Number of prepositions in the question body. We use the part-of-speech tags generated as part of the parse by Stanza.\nQx_coref_count: Number of pronominal or nominal instances of coreference in the question body. We use Stanford\u2019s CorefAnnotator  ###reference_f.html### to extract this feature.\n(2) Mathematical features (M) include  features pertaining to the question (Q) and gold solution (G):\nQx_arg_count: The number of distinct numerical quantities (e.g., \u201c3.5 hours later\u201d or \u201c100 boxes\u201d) in the question body. We use a Regexp pattern to detect whole numbers, decimal point values, and quantities preceded by a negative sign or dollar (and other currency) signs.\nQx_word_arg_count: The number of quantities mentioned in word-form (\u201cthree times\u201d or \u201chalf as much\u201d) in the question body. We use a vocabulary of frequently used word-form tokens and accommodate compound expressions (e.g., \u201ctwenty-two\u201d).\nQx_mean_numerical_word_rank: The mean vocabulary rank of the numerical tokens in the question body. We first isolate numerical tokens tokenized by respective tokenizers, then aggregate their token rank.\nGx_arg_count: The number of distinct numerical quantities present as plain text or on the left-hand side of equations in the gold solution. We use the same Regexp pattern used for Qx_arg_count.\nGx_op{\u2018+\u2019/ \u2018-\u2019/ \u2018*\u2019/ \u2018/\u2019/ \u2018(\u2019}_count: Number of times each listed math operation is used in the gold solution. A simple Regexp pattern is applied to extract these from within equations.\nGx_op_unique_count: The maximum number of times a single operation has been used in the gold solution. For instance, \u201c\u201d contains 3 instances of the \u2018+\u2019 operator.\nGx_op_diversity: Ratio of the number of unique math operators used to the total number of operators in the gold solution. For instance, a question with the consolidated math solution expression \"\" contains two arithmetic operations in total but only one unique operation type, i.e., \u2018,\u2019 Gx_op_diversity.\nGx_mean_numerical_word_rank: The mean vocabulary rank of the numerical tokens used on the left-hand side of equations in the gold solution. Extracted the same way as Qx_mean_numerical_word_rank.\nBx_parameter_usage: The ratio of distinct arguments used in the gold solution to that in the question body. A value lower than 1 indicates that one or more arguments provided in the question were not required to solve the MWP (potentially acting as distractors).\n(3) World knowledge and NLU features (W) include:\nBx_world_knowledge: The number of distinct arguments on the left-hand-side of equations in the gold solution, that are neither present in the question body nor produced as intermediate results from any prior equations in the solution. A non-zero value is interpreted as the use of a quantity (perhaps a conversion factor, or the number of entities involved in computing mean) unspecified by the question. The arguments were extracted from both sides using the same Regexp policies used for previous features.\nTable 5  ###reference_### shows further statistics on the features, including the range as well as the mean and standard deviation of the values for each feature type. Additionally, we report the Spearman correlation between all pairs in the feature set in Figure 5  ###reference_###.\n###figure_6###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Querying Details",
            "text": ""
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Training Details",
            "text": ""
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Results Analysis",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S3.T1.3\" style=\"width:281.9pt;height:195.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(63.2pt,-43.9pt) scale(1.81351046516742,1.81351046516742) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T1.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.2.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S3.T1.1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1\">Success Rate</span> (=1,319\n)</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.2.2.2.1\"></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.3.3.3.2\"></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.3.3.4.1.1\">Llama2-13B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.4.1.2\">0.3724</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.4.1.3\">0.3681</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.3.3.5.2.1\">Llama2-70B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.5.2.2\">0.5609</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.5.2.3\">0.3941</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.3.3.6.3.1\">Mistral-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.6.3.2\">0.3627</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T1.3.3.6.3.3\">0.3309</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.3.3.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S3.T1.3.3.7.4.1\">MetaMath-13B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.3.3.7.4.2\">0.6373</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.3.3.7.4.3\">0.3816</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Success rates for solution attempts per LLM</figcaption>\n</figure>",
            "capture": "Table 1: Success rates for solution attempts per LLM"
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:368.6pt;height:178.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-94.7pt,45.8pt) scale(0.660568881691692,0.660568881691692) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"12\" id=\"S4.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1.1\">Class Distribution</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.2.1.1\">Split</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.2.2.1\">Class</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.2.3.1\">Llama2-13b</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.2.4.1\">Llama2-70b</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.2.5.1\">Mistral-7b</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.2.6.1\">MetaMath-13b</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.2.7.1\">Intersection</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.3.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.1.1.3.3.1.1\">Train</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.3.2\">Always</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.3.3.3\">1102 (30.22%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.3.3.4\">2438 (61.36%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.3.3.5\">733 (24.06%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.3.3.6\">5162 (94.7%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.3.3.7\">205 (53.38%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.4.4.1\">Never</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.4.4.2\">2545 (69.78%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.4.4.3\">1535 (38.64%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.4.4.4\">2313 (75.94%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.4.4.5\">289 (5.3%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.4.4.6\">179 (46.61%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.5.5.1\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.5.5.2\">3647</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.5.5.3\">3973</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.5.5.4\">3046</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.5.5.5\">5451</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.5.5.6\">401</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.6.6.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S4.T2.1.1.6.6.1.1\">Test</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.6.6.2\">Always</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.6.6.3\">188 (28.14%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.6.6.4\">427 (60.06%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.6.6.5\">111 (21.51%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.6.6.6\">528 (71.64%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.6.6.7\">31 (24.41%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.7.7.1\">Never</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.7.7.2\">480 (71.86%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.7.7.3\">284 (39.94%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.7.7.4\">405 (78.49%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.7.7.5\">209 (28.36%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.7.7.6\">96 (75.59%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.8.8.1\">Total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.8.8.2\">668</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.8.8.3\">711</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.8.8.4\">516</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.8.8.5\">737</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.8.8.6\">135</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.9.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"12\" id=\"S4.T2.1.1.9.9.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.9.9.1.1\">Classification Performance</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.10.10.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.10.1.1\">Classification Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.10.10.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.10.2.1\">Llama2-13b</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.10.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.10.3.1\">Llama2-70b</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.10.10.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.10.4.1\">Mistral-7b</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.10.10.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.10.5.1\">MetaMath-13b</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.10.10.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.10.10.6.1\">Intersection</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.11.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.1\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.2\">Macro F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.3\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.4\">Macro F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.5\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.6\">Macro F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.7\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.8\">Macro F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.9\">Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.11.11.10\">Macro F1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.12.12.1\">Logistic Regression</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.12.2\">0.707</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.12.12.3\">0.686</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.12.4\">0.684</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.12.12.5\">0.673</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.12.6\">0.721</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.12.12.7\">0.675</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.12.8\">0.737</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.12.12.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.12.12.9.1\">0.686</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.12.12.10\">0.800</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.12.12.11\">0.787</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.13.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.13.13.1\">Decision Tree</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.13.13.2\">0.657</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.13.13.3\">0.625</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.13.13.4\">0.644</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.13.13.5\">0.637</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.13.13.6\">0.667</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.13.13.7\">0.611</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.13.13.8\">0.703</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.13.13.9\">0.627</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.13.13.10\">0.733</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.13.13.11\">0.719</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.14.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.1.14.14.1\">Random Forest</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.14.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.2.1\">0.767</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.14.14.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.3.1\">0.724</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.14.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.4.1\">0.717</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.14.14.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.5.1\">0.707</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.14.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.6.1\">0.814</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.14.14.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.7.1\">0.738</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.14.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.8.1\">0.744</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.14.14.9\">0.549</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.14.14.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.10.1\">0.815</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.14.14.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.14.14.11.1\">0.799</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.15.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_tt\" colspan=\"2\" id=\"S4.T2.1.1.15.15.1\"><span class=\"ltx_text ltx_font_typewriter\" id=\"S4.T2.1.1.15.15.1.1\">RoBERTa-base</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.2\">0.816</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.3\">0.771</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.4\">0.756</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.5\">0.738</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.6\">0.838</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.7\">0.743</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.8\">0.701</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.9\">0.415</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.10\">0.811</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_tt\" id=\"S4.T2.1.1.15.15.11\">0.781</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Class-wise distribution and classification results for different LLMs. \"Intersection\" refers to questions always or never solved correctly by all or any LLM, respectively. All classification results are mean values across 5 runs with varying initialization seeds. The best results for feature-based classifiers are highlighted in bold.</figcaption>\n</figure>",
            "capture": "Table 2: Class-wise distribution and classification results for different LLMs. \"Intersection\" refers to questions always or never solved correctly by all or any LLM, respectively. All classification results are mean values across 5 runs with varying initialization seeds. The best results for feature-based classifiers are highlighted in bold."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.3\" style=\"width:368.6pt;height:314pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(57.5pt,-49.0pt) scale(1.45372172612474,1.45372172612474) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.3.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T3.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\">Feature Name</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T3.1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1.1\">Rank</span> (=23)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.2.2.2.1\"></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.3.2\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.4.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.4.1.1\">L</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.4.1.2\">Qx_np_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.4.1.3\">1.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.4.1.4\">0.45</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.5.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.5.2.1\">M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.5.2.2\">Qx_mean_numerical_word_rank</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.5.2.3\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.5.2.4\">1.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.6.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.6.3.1\">M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.6.3.2\">Gx_op_unique_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.6.3.3\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.6.3.4\">2.65</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.7.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.7.4.1\">M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.7.4.2\">Gx_op_diversity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.7.4.3\">4.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.7.4.4\">2.30</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.8.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.8.5.1\">M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.8.5.2\">Gx_mean_numerical_word_rank</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.8.5.3\">4.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.8.5.4\">1.82</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.9.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.9.6.1\">L</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.9.6.2\">Qx_mean_word_rank</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.9.6.3\">5.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.9.6.4\">1.82</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.10.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.10.7.1\">L</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.10.7.2\">Qx_flesch_kinkaid_grade</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.10.7.3\">6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.10.7.4\">1.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.11.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.11.8.1\">W</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.11.8.2\">Gx_world_knowledge</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.11.8.3\">7.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.11.8.4\">2.28</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.12.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.12.9.1\">L</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.3.3.12.9.2\">Qx_constituency_tree_depth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.12.9.3\">9.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.3.3.12.9.4\">1.95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.13.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T3.3.3.13.10.1\">M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T3.3.3.13.10.2\">Gx_op\u2019+\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.3.3.13.10.3\">11.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.3.3.13.10.4\">3.97</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Feature importance ranks across classification models and LLM-wise data subsets.</figcaption>\n</figure>",
            "capture": "Table 3: Feature importance ranks across classification models and LLM-wise data subsets."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T4.1\" style=\"width:433.6pt;height:210.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(87.3pt,-42.4pt) scale(1.67364477049173,1.67364477049173) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" colspan=\"2\" id=\"S4.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.2.1\">Cluster</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T4.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.3.1\">Feature</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_tt\" id=\"S4.T4.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1.1\">Spearman()</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.1.1.2.1.1\">ID</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.1.1.2.1.2\">Size</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.1.1.3.2.1\">09</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.1.1.3.2.2\">27</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.1.3.2.3\">Qx_constituency_tree_depth</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S4.T4.1.1.3.2.4\">-0.64***</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.1.1.4.3.1\">24</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.1.1.4.3.2\">14</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.1.4.3.3\">Qx_mean_word_rank</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S4.T4.1.1.4.3.4\">-0.82***</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.1.1.5.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T4.1.1.5.4.1.1\">63</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.1.1.5.4.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T4.1.1.5.4.2.1\">62</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.1.5.4.3\">Qx_token_length</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S4.T4.1.1.5.4.4\">-0.42***</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T4.1.1.6.5.1\">Qx_word_length</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S4.T4.1.1.6.5.2\">-0.43***</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T4.1.1.7.6.1\">96</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T4.1.1.7.6.2\">51</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T4.1.1.7.6.3\">Qx_flesch_kinkaid_grade</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\" id=\"S4.T4.1.1.7.6.4\">-0.51***</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Cluster-wise feature correlations. The cluster count represents the number of questions included in the respective cluster. The p-value for all reported correlation values is &lt;0.001 (marked by \u2018***\u2019).</figcaption>\n</figure>",
            "capture": "Table 4: Cluster-wise feature correlations. The cluster count represents the number of questions included in the respective cluster. The p-value for all reported correlation values is <0.001 (marked by \u2018***\u2019)."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T5.2\" style=\"width:303.5pt;height:283.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-79.7pt,74.4pt) scale(0.655714607435208,0.655714607435208) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A1.T5.2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.2.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.2.2.2.3.1\">Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.2.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.2.2.2.4.1\">Source</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.2.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.2.2.2.5.1\">#</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.2.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.2.2.2.6.1\">Feature Name</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.2.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T5.2.2.2.7.1\">Range</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.1.1.1.1\"></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T5.2.2.2.2\"></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.3.1.1\" rowspan=\"9\"><span class=\"ltx_text\" id=\"A1.T5.2.2.3.1.1.1\">L</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.3.1.2\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.3.1.3\">1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.3.1.4\">Qx_token_length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.3.1.5\">[12 \u2013 239]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.3.1.6\">66.05</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.3.1.7\">24.384</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.4.2.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.4.2.2\">2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.4.2.3\">Qx_sentence_length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.4.2.4\">[1 \u2013 13]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.4.2.5\">3.431</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.4.2.6\">1.201</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.5.3.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.5.3.2\">3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.5.3.3\">Qx_word_length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.5.3.4\">[9 \u2013 184]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.5.3.5\">45.885</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.5.3.6\">17.832</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.6.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.6.4.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.6.4.2\">4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.6.4.3\">Qx_flesch_kinkaid_grade</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.6.4.4\">[-1.9 \u2013 26.3]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.6.4.5\">4.236</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.6.4.6\">2.468</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.7.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.7.5.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.7.5.2\">5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.7.5.3\">Qx_mean_word_rank</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.7.5.4\">[3661.96 \u2013 21929.96]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.7.5.5\">10646.615</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.7.5.6\">2110.891</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.8.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.8.6.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.8.6.2\">6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.8.6.3\">Qx_constituency_tree_depth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.8.6.4\">[5 \u2013 31]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.8.6.5\">10.803</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.8.6.6\">2.798</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.9.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.9.7.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.9.7.2\">7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.9.7.3\">Qx_np_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.9.7.4\">[3 \u2013 74]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.9.7.5\">18.034</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.9.7.6\">7.488</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.10.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.10.8.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.10.8.2\">8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.10.8.3\">Qx_prp_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.10.8.4\">[0 \u2013 16]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.10.8.5\">1.772</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.10.8.6\">1.854</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.11.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.11.9.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.11.9.2\">9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.11.9.3\">Qx_coref_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.11.9.4\">[0 \u2013 16]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.11.9.5\">0.462</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.11.9.6\">1.283</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.12.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.12.10.1\" rowspan=\"13\"><span class=\"ltx_text\" id=\"A1.T5.2.2.12.10.1.1\">M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.12.10.2\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.12.10.3\">10</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.12.10.4\">Qx_arg_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.12.10.5\">[0 \u2013 17]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.12.10.6\">4.438</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.12.10.7\">1.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.13.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.13.11.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.13.11.2\">11</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.13.11.3\">Qx_word_arg_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.13.11.4\">[0 \u2013 14]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.13.11.5\">1.091</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.13.11.6\">1.397</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.14.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.14.12.1\">Q</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.14.12.2\">12</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.14.12.3\">Qx_mean_numerical_word_rank</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.14.12.4\">[259.0 \u2013 29905.38]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.14.12.5\">22643.319</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.14.12.6\">3260.09</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.15.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.15.13.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.15.13.2\">13</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.15.13.3\">Gx_arg_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.15.13.4\">[6 \u2013 73]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.15.13.5\">24.377</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.15.13.6\">9.732</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.16.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.16.14.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.16.14.2\">14</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.16.14.3\">Gx_op\u2018+\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.16.14.4\">[0 \u2013 12]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.16.14.5\">1.06</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.16.14.6\">1.212</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.17.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.17.15.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.17.15.2\">15</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.17.15.3\">Gx_op\u2018-\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.17.15.4\">[0 \u2013 6]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.17.15.5\">0.601</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.17.15.6\">0.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.18.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.18.16.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.18.16.2\">16</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.18.16.3\">Gx_op\u2018*\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.18.16.4\">[0 \u2013 8]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.18.16.5\">1.369</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.18.16.6\">1.183</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.19.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.19.17.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.19.17.2\">17</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.19.17.3\">Gx_op\u2018/\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.19.17.4\">[0 \u2013 7]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.19.17.5\">0.621</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.19.17.6\">0.789</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.20.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.20.18.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.20.18.2\">18</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.20.18.3\">Gx_op\u2018(\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.20.18.4\">[0 \u2013 4]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.20.18.5\">0.026</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.20.18.6\">0.187</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.21.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.21.19.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.21.19.2\">19</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.21.19.3\">Gx_op_unique_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.21.19.4\">[0 \u2013 6]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.21.19.5\">2.284</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.21.19.6\">0.93</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.22.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.22.20.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.22.20.2\">20</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.22.20.3\">Gx_op_diversity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.22.20.4\">[0.15 \u2013 1.0]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.22.20.5\">0.758</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.22.20.6\">0.196</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.23.21\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.23.21.1\">G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.23.21.2\">21</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.23.21.3\">Gx_mean_numerical_word_rank</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.23.21.4\">[22645.0 \u2013 29915.0]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.23.21.5\">28626.04</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.23.21.6\">776.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.24.22\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.24.22.1\">B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.24.22.2\">22</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T5.2.2.24.22.3\">Gx_parameter_usage</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T5.2.2.24.22.4\">[0.07 \u2013 1.0]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.24.22.5\">0.642</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" id=\"A1.T5.2.2.24.22.6\">0.241</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T5.2.2.25.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T5.2.2.25.23.1\">W</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T5.2.2.25.23.2\">B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T5.2.2.25.23.3\">23</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T5.2.2.25.23.4\">Gx_world_knowledge</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T5.2.2.25.23.5\">[0 \u2013 8]</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"A1.T5.2.2.25.23.6\">1.104</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t\" id=\"A1.T5.2.2.25.23.7\">1.006</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Details of formulation and distribution (across <span class=\"ltx_text ltx_font_typewriter\" id=\"A1.T5.4.1\">GSM8K</span>) for all features included in the feature set. Each feature is of type: Linguistic (L), Mathematical (M), or World Knowledge and NLU (W) and is sourced either from the question body (Q), gold solution body (G), or both (B).</figcaption>\n</figure>",
            "capture": "Table 5: Details of formulation and distribution (across GSM8K) for all features included in the feature set. Each feature is of type: Linguistic (L), Mathematical (M), or World Knowledge and NLU (W) and is sourced either from the question body (Q), gold solution body (G), or both (B)."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T6\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"A2.T6.1\" style=\"width:433.6pt;height:137.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(74.5pt,-23.6pt) scale(1.52384709064223,1.52384709064223) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A2.T6.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"A2.T6.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T6.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.1.1.1.1.2.1\">HuggingFace Model Name</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T6.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T6.1.1.1.1.3.1\">Pass@1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T6.1.1.2.1.1\">Llama2-13B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.1.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A2.T6.1.1.2.1.2.1\">meta-llama/Llama-2-13b-chat-hf</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.2.1.3\">28.70</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T6.1.1.3.2.1\">Llama2-70B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.3.2.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A2.T6.1.1.3.2.2.1\">meta-llama/Llama-2-70b-chat-hf</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.3.2.3\">56.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T6.1.1.4.3.1\">Mistral-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.4.3.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A2.T6.1.1.4.3.2.1\">mistralai/Mistral-7B-Instruct-v0.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T6.1.1.4.3.3\">40.03</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T6.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" id=\"A2.T6.1.1.5.4.1\">MetaMath-13B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T6.1.1.5.4.2\"><span class=\"ltx_text ltx_font_typewriter\" id=\"A2.T6.1.1.5.4.2.1\">meta-math/MetaMath-13B-V1.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T6.1.1.5.4.3\">72.30</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>List of HuggingFace model variants and their respective reported pass@1 (single run) accuracies on the <span class=\"ltx_text ltx_font_typewriter\" id=\"A2.T6.3.1\">GSM8K</span> test set from the OpenLLM leaderboard.</figcaption>\n</figure>",
            "capture": "Table 6: List of HuggingFace model variants and their respective reported pass@1 (single run) accuracies on the GSM8K test set from the OpenLLM leaderboard."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T7\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"A2.T7.9\" style=\"width:433.6pt;height:128.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-147.2pt,43.7pt) scale(0.595574139294884,0.595574139294884) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A2.T7.9.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.9.1.1.1.1.1\">Feature</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" id=\"A2.T7.9.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.9.1.1.1.2.1\">Llama2-13B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" id=\"A2.T7.9.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.9.1.1.1.3.1\">Llama2-70B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" id=\"A2.T7.9.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.9.1.1.1.4.1\">Mistral-7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" id=\"A2.T7.9.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.9.1.1.1.5.1\">MetaMath-13B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" id=\"A2.T7.9.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T7.9.1.1.1.6.1\">Intersection</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.1\">Thresh.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.2\">Diff.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.3\">T-val</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.4\">Thresh.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.5\">Diff.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.6\">T-val</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.7\">Thresh.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.8\">Diff.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.9\">T-val</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.10\">Thresh.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.11\">Diff.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.12\">T-val</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.13\">Thresh.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.14\">Diff.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T7.9.1.2.2.15\">T-val</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.3.1.1\">Gx_num_arg_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.2\">51.102</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.3\">0.313</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.3.1.4\">-8.328</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.5\">53.273</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.6\">0.380</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.3.1.7\">-7.747</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.8\">55.082</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.9\">0.283</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.3.1.10\">-5.452</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.11\">59.061</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.12\">0.438</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.3.1.13\">-5.558</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.14\">57.735</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.3.1.15\">0.330</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.3.1.16\">-5.977</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.4.2.1\">Qx_np_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.2\">40.673</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.3\">0.200</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.4.2.4\">-5.440</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.5\">47.465</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.6\">0.435</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.4.2.7\">-5.090</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.8\">33.429</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.9\">0.207</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.4.2.10\">-11.201</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.11\">47.918</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.12\">0.448</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.4.2.13\">-7.230</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.14\">47.918</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.4.2.15\">0.333</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.4.2.16\">-5.645</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.5.3.1\">Gx_arg_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.2\">57.959</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.3\">0.336</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.5.3.4\">-6.018</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.5\">58.111</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.6\">0.380</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.5.3.7\">-5.742</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.8\">57.959</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.9\">0.289</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.5.3.10\">-5.644</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.11\">51.122</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.12\">0.271</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.5.3.13\">-10.323</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.14\">60.694</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.5.3.15\">0.335</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.5.3.16\">-5.249</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.6.4.1\">Qx_word_length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.2\">91.143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.3\">0.219</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.6.4.4\">-7.944</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.5\">97.384</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.6\">0.300</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.6.4.7\">-8.468</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.8\">19.714</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.9\">0.233</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.6.4.10\">-7.016</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.11\">101.857</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.12\">0.275</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.6.4.13\">-9.267</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.14\">116.143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.6.4.15\">0.251</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.6.4.16\">-5.328</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.7.5.1\">Gx_op\u2019-\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.2\">2.082</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.3\">0.198</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.7.5.4\">-7.683</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.5\">2.061</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.6\">0.246</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.7.5.7\">-9.009</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.8\">2.082</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.9\">0.181</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.7.5.10\">-7.660</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.11\">3.061</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.12\">0.339</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.7.5.13\">-7.348</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.14\">3.061</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.7.5.15\">0.280</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.7.5.16\">-6.383</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.8.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.8.6.1\">Qx_prp_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.2\">7.184</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.3\">0.215</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.8.6.4\">-5.957</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.5\">8.081</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.6\">0.313</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.8.6.7\">-6.322</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.8\">8.163</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.9\">0.235</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.8.6.10\">-5.475</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.11\">7.184</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.12\">0.167</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.8.6.13\">-6.039</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.14\">8.163</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.8.6.15\">0.233</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.8.6.16\">-6.834</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.9.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.9.7.1\">Qx_sentence_length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.2\">6.143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.3\">0.206</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.9.7.4\">-6.843</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.5\">6.091</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.6\">0.264</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.9.7.7\">-8.278</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.8\">6.143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.9\">0.203</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.9.7.10\">-7.357</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.11\">7.122</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.12\">0.253</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.9.7.13\">-5.756</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.14\">7.122</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.9.7.15\">0.229</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.9.7.16\">-5.473</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.10.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.10.8.1\">Gx_op\u2019+\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.2\">4.163</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.3\">0.241</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.10.8.4\">-7.849</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.5\">4.121</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.6\">0.293</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.10.8.7\">-8.996</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.8\">5.143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.9\">0.296</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.10.8.10\">-6.038</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.11\">3.184</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.12\">0.080</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.10.8.13\">-5.377</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.14\">5.143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.10.8.15\">0.238</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.10.8.16\">-6.081</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.11.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T7.9.1.11.9.1\">Qx_token_length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.2\">123.184</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.3\">0.211</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.11.9.4\">-8.937</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.5\">126.646</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.6\">0.258</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.11.9.7\">-9.557</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.8\">113.918</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.9\">0.214</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.11.9.10\">-12.560</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.11\">132.449</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.12\">0.232</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.11.9.13\">-9.953</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.14\">118.551</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T7.9.1.11.9.15\">0.219</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A2.T7.9.1.11.9.16\">-14.125</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T7.9.1.12.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.1\">Gx_op\u2019*\u2019_count</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.2\">4.082</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.3\">0.211</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.4\">-6.135</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.5\">4.040</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.6\">0.220</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.7\">-6.043</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.8\">4.082</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.9\">0.184</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.10\">-5.831</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.11\">5.061</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.12\">0.268</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.13\">-5.423</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.14\">5.061</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.15\">0.250</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A2.T7.9.1.12.10.16\">-5.295</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Feature-wise thresholds which reflect the greatest difference in the corresponding mean success rate. For each feature, the optimal threshold creates two sets of questions on either side, wherein the difference in the corresponding mean success rates of the two sets is the greatest. We perform Student\u2019s -tests on both sets to determine if this difference is significant and report the corresponding  values. All results reported in the table have an absolute -value &gt;5 and a -value &lt;0.0001.</figcaption>\n</figure>",
            "capture": "Table 7: Feature-wise thresholds which reflect the greatest difference in the corresponding mean success rate. For each feature, the optimal threshold creates two sets of questions on either side, wherein the difference in the corresponding mean success rates of the two sets is the greatest. We perform Student\u2019s -tests on both sets to determine if this difference is significant and report the corresponding  values. All results reported in the table have an absolute -value >5 and a -value <0.0001."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"A2.T8\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A2.T8.1\" style=\"width:433.6pt;height:55.8pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-132.9pt,17.1pt) scale(0.619923451417356,0.619923451417356) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A2.T8.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"A2.T8.1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.1.1.1.1\">Classification Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"A2.T8.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.1.1.2.1\">Llama2-13B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"A2.T8.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.1.1.3.1\">Llama2-70B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"A2.T8.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.1.1.4.1\">Mistral-7B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"A2.T8.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.1.1.5.1\">MetaMath-13B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"A2.T8.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.1.1.6.1\">Intersection</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.1\">F1(Never)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.2\">F1(Always)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.3\">F1(Never)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.4\">F1(Always)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.5\">F1(Never)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.6\">F1(Always)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.7\">F1(Never)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.8\">F1(Always)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.9\">F1(Never)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A2.T8.1.1.2.2.10\">F1(Always)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T8.1.1.3.1.1\">Logistic Regression</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.2\">0.8454</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.3\">0.4968</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.4\">0.5641</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.3.1.5.1\">0.7862</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.6\">0.8926</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.7\">0.5081</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.8\">0.0095</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.9\">0.8354</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.10\">0.8263</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.3.1.11\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.3.1.11.1\">0.7184</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T8.1.1.4.2.1\">Decision Tree</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.2\">0.8199</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.3\">0.4532</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.4\">0.4958</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.5\">0.7431</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.6\">0.8732</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.7\">0.4592</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.4.2.8.1\">0.0727</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.9\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.4.2.9.1\">0.8373</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.10\">0.732</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T8.1.1.4.2.11\">0.6496</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T8.1.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.1\">Random Forest</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.5.3.2.1\">0.8535</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.5.3.3.1\">0.5049</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.5.3.4.1\">0.6133</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.5\">0.7824</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.5.3.6.1\">0.8936</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.5.3.7.1\">0.5161</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.8\">0.019</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.9\">0.8361</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.10\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T8.1.1.5.3.10.1\">0.8439</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"A2.T8.1.1.5.3.11\">0.7216</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Class-wise F1 scores for each classification model across LLM solution splits.</figcaption>\n</figure>",
            "capture": "Table 8: Class-wise F1 scores for each classification model across LLM solution splits."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.11369v2_figure_1.png",
            "caption": "Figure 1: A response from Llama2-70B to a lengthy math problem that involves NLU challenges."
        },
        "2": {
            "figure_path": "2403.11369v2_figure_2.png",
            "caption": "(a) Always correct"
        },
        "3": {
            "figure_path": "2403.11369v2_figure_3.png",
            "caption": "(b) Never correct"
        },
        "4": {
            "figure_path": "2403.11369v2_figure_4.png",
            "caption": "Figure 3: Results of the ablation studies across feature types (L \u2013 Linguistic, M \u2013 Mathematical, W \u2013 World Knowledge & NLU). Each bar represents the mean macro-F1 score over all three classifier models."
        },
        "5": {
            "figure_path": "2403.11369v2_figure_5.png",
            "caption": "Figure 4: Solution attempt by Llama2-70B on the question from Figure 1, with the required real-world knowledge explicitly specified."
        },
        "6": {
            "figure_path": "2403.11369v2_figure_6.png",
            "caption": "Figure 5: Spearman correlation matrix between features. All correlation values are marked with \u2018*\u2019, \u2018**\u2019, and \u2018***\u2019 if their corresponding p-values are less than 0.050.050.050.05, 0.010.010.010.01, and 0.0010.0010.0010.001 respectively."
        },
        "7": {
            "figure_path": "2403.11369v2_figure_7.png",
            "caption": "Figure 6: Prompt template used for solution generation across LLMs."
        }
    },
    "references": [
        {
            "1": {
                "title": "Instruction-embedded assessment for reading ability in adaptive\nmathematics software.",
                "author": "Husni Almoubayyed, Stephen E Fancsali, and Steve Ritter. 2023.",
                "venue": "In LAK23: 13th International Learning Analytics and Knowledge\nConference, pages 366\u2013377.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Open llm leaderboard.",
                "author": "Edward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert,\nNazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023.",
                "venue": "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al. 2020.",
                "venue": "Advances in neural information processing systems,\n33:1877\u20131901.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Training verifiers to solve math word problems.",
                "author": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,\net al. 2021.",
                "venue": "arXiv preprint arXiv:2110.14168.",
                "url": null
            }
        },
        {
            "5": {
                "title": "A new readability yardstick.",
                "author": "Rudolph Flesch. 1948.",
                "venue": "Journal of applied psychology, 32(3):221.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Mistral 7b.",
                "author": "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2310.06825"
            }
        },
        {
            "7": {
                "title": "It ain\u2019t\nover: A multi-aspect diverse math word problem dataset.",
                "author": "Jiwoo Kim, Youngbin Kim, Ilwoong Baek, JinYeong Bak, and Jongwuk Lee. 2023.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 14984\u201315011, Singapore. Association for\nComputational Linguistics.",
                "url": "https://aclanthology.org/2023.emnlp-main.927"
            }
        },
        {
            "8": {
                "title": "Roberta: A robustly\noptimized BERT pretraining approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": "CoRR, abs/1907.11692.",
                "url": "http://arxiv.org/abs/1907.11692"
            }
        },
        {
            "9": {
                "title": "A diverse\ncorpus for evaluating and developing English math word problem solvers.",
                "author": "Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 975\u2013984, Online. Association for\nComputational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.92"
            }
        },
        {
            "10": {
                "title": "Rewriting math word\nproblems with large language models.",
                "author": "Kole Norberg, Husni Almoubayyed, Stephen E. Fancsali, Logan De Ley, Kyle\nWeldon, April Murphy, and Steven Ritter. 2023.",
                "venue": "In Proceedings of the Workshop on Empowering Education with\nLLMs - the Next-Gen Interface and Content Generation 2023 co-located with\n24th International Conference on Artificial Intelligence in Education (AIED\n2023), Tokyo, Japan, July 7, 2023, volume 3487 of CEUR Workshop\nProceedings, pages 163\u2013172. CEUR-WS.org.",
                "url": "https://ceur-ws.org/Vol-3487/paper11.pdf"
            }
        },
        {
            "11": {
                "title": "Are NLP\nmodels really able to solve simple math word problems?",
                "author": "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 2080\u20132094, Online. Association for Computational\nLinguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.168"
            }
        },
        {
            "12": {
                "title": "Language models are unsupervised multitask learners.",
                "author": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019.",
                "venue": "OpenAI blog, 1(8):9.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Math word\nproblem solving by generating linguistic variants of problem statements.",
                "author": "Syed Rifat Raiyan, Md Nafis Faiyaz, Shah Md. Jawad Kabir, Mohsinul Kabir, Hasan\nMahmud, and Md Kamrul Hasan. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 4: Student Research Workshop), pages\n362\u2013378, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-srw.49"
            }
        },
        {
            "14": {
                "title": "Llama 2: Open foundation and\nfine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,\nDan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan\nZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2307.09288"
            }
        },
        {
            "15": {
                "title": "Learning multi-step reasoning by solving arithmetic tasks.",
                "author": "Tianduo Wang and Wei Lu. 2023.",
                "venue": "In Proceedings of ACL.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Emergent abilities of large language models.",
                "author": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022.",
                "venue": "arXiv preprint arXiv:2206.07682.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Metamath: Bootstrap your own\nmathematical questions for large language models.",
                "author": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang,\nJames T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2309.12284"
            }
        },
        {
            "18": {
                "title": "A survey of large language models.",
                "author": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.",
                "venue": "arXiv preprint arXiv:2303.18223.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Progressive-hint prompting improves reasoning in large language\nmodels.",
                "author": "Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. 2023.",
                "venue": "arXiv preprint arXiv:2304.09797.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Promptbench: Towards evaluating the robustness of large language\nmodels on adversarial prompts.",
                "author": "Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang,\nLinyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.",
                "venue": "arXiv preprint arXiv:2306.04528.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.11369v2",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.2.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.2.2",
            "4.2.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.11369v2",
        "paper_title": "What Makes Math Word Problems Challenging for LLMs?",
        "research_background": "**Motivation:**\nThe paper is motivated by the need to understand what makes math word problems (MWPs) particularly difficult for large language models (LLMs). While LLMs have shown exceptional performance across various natural language processing (NLP) tasks and even in solving mathematical puzzles, their ability to tackle MWPs effectively remains a significant challenge. Previous studies have focused mostly on evaluating LLMs' performance on MWPs or exploring their behavior under different prompting strategies. However, there has been a lack of in-depth analysis pinpointing the specific factors that contribute to the difficulty of MWPs for LLMs. The authors aim to fill this gap by systematically investigating the characteristics that make MWPs challenging for LLMs.\n\n**Research Problem:**\nThe primary research problem addressed in the paper is twofold:\n1. To identify the characteristics of input math word questions that contribute to their complexity for LLMs.\n2. To determine if these identified characteristics can be used to predict whether a specific LLM will be able to correctly solve given MWPs.\n\n**Relevant Prior Work:**\n- The general potential and emergent abilities of LLMs have been established in numerous studies (Zhao et al., 2023; Brown et al., 2020; Radford et al., 2019).\n- The difficulty LLMs face with MWPs has been highlighted by various researchers (Wang and Lu, 2023; Cobbe et al., 2021; Patel et al., 2021; Miao et al., 2020).\n- Previous research has primarily focused on evaluating LLM performance on MWPs and on modifying their behavior through methods like progressive-hint prompting or prompt paraphrasing (Norberg et al., 2023; Raiyan et al., 2023; Zheng et al., 2023; Zhu et al., 2023).\n- Almoubayyed et al. (2023) found a strong connection between reading skills and math outcomes in students, suggesting a potential parallel in LLM performance on MWPs.\n- Cobbe et al. (2021) conducted a dataset analysis (GSM8K) indicating that simpler questions are more likely to be answered correctly by LLMs, while questions requiring real-world knowledge and complex natural language understanding often challenge LLMs.",
        "methodology": "**Methodology:**\n\n**Dataset:**\nWe use the GSM8K dataset Cobbe et al. (2021 ###reference_b4###), which is divided into training and test instances. This dataset is chosen for its high-quality, human-generated Math Word Problems (MWPs). The problems are diverse and have minimal recurring templates, which lends itself to our feature-based analysis. The difficulty level of the problems is also well-suited for Large Language Models (LLMs), offering a wide range of correctness across different models and question types.\n\n**Data Collection:**\nWe collect solution attempts from several LLMs for questions from both the GSM8K training and test sets. \n\n**Classifiers:**\nNext, we train statistical classifiers on a filtered subset of questions to predict whether they are consistently solved correctly or incorrectly across multiple runs of the models. This relatively simple approach allows us to identify which features are most indicative of the challenges LLMs face when solving math problems.\n\n**Models Used:**\nWe select several open-source models for our experiments:\n- **Llama2 (13B and 70B) Touvron et al. (2023 ###reference_b14###)**\n- **Mistral-7B Jiang et al. (2023 ###reference_b6###)**, noted for its performance on math tasks despite its smaller size.\n- **MetaMath-13B Yu et al. (2023 ###reference_b17###)**, which is specifically fine-tuned on math QA data, unlike the other general-purpose models.\n\n**Feature Analysis:**\nWe analyze and experiment with features extracted from MWP questions and their expected solutions, ensuring these features are grounded in the dataset and applicable to any LLM. The features are broadly categorized as follows:\n\n1. **Linguistic Features:** These focus on the phrasing of the question, including:\n   - Length of the question\n   - Sophistication of the vocabulary\n   - Syntactic complexity\n   - Instances of coreference\n   - Overall readability\n   \n   Note: These features are only extracted from the question body since the phrasing of the gold solution does not impact the expected answer.\n\n2. **Mathematical Features:** These cover the math arguments, operations, and reasoning steps required to solve the questions, including:\n   - Number and diversity of math operations in the solution body\n   - Mathematical reasoning to disregard arguments provided in the question but not used in the solution\n   \n   Note: While a question's phrasing can vary, the underlying math operations and reasoning steps (mathematical features) remain the same.\n\n3. **Real-world Knowledge & NLU Based Features:** These indicate the amount of extraneous information needed to solve the task that is not explicitly provided in the question, such as knowing how many days are in a month or interpreting \"half.\"",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Models Used:**\n- Logistic Regression\n- Decision Tree\n- Random Forest classifiers\n\n**Datasets:**\n- GSM8K dataset\n  - For high confidence samples, the subset of this dataset includes training and test samples where the Large Language Model's (LLM) success rate is either 100% (always correct) or 0% (never correct).\n\n**Preprocessing Steps:**\n- Dropping highly correlated features\n- Class-balancing\n- Feature scaling\n\n**Hyperparameter Tuning:**\n- A hyperparameter search is conducted for each model to maximize their performance on unseen data.\n\n**Main Results:**\n- The experimental results, including the detailed LLM-specific splits, are discussed in Table 2 (not fully referenced here). Appendix C provides additional details about the hyperparameter search and preprocessing strategies.\n\n### Note:\n- Ablation studies designed to evaluate specific module contributions are excluded from this summary.\n\nAs mentioned, more detailed information about distribution, preprocessing, and experimental results can be found in Table 2 and Appendix C (full references not provided in the summary)."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To measure the impact of different feature types (linguistic, mathematical, world knowledge & NLU) on the performance of classifiers in predicting the success of large language models (LLMs) on math word problems (MWPs).",
            "experiment_process": "Classification scores were analyzed across different feature-type subsets, including linguistic (L), mathematical (M), and world knowledge & NLU (W) features. Performance was evaluated for several LLMs: Llama2-13B, Llama2-7B, MetaMath-13B, and Mistral-7B. Each feature set or combination (L, L+M, etc.) was assessed to determine optimal classification performance.",
            "result_discussion": "The feature set combining all types (L+M+W) was found not to be optimal for classification. Specifically, Llama2-13B's performance was best classified using only mathematical features (M). In contrast, the best-performing classifiers for Llama2-7B, MetaMath-13B, and their intersection sets used either solely linguistic features (L) or a combination of linguistic and math features (L+M). Mistral-7B performed sufficiently with the world knowledge & NLU feature set.",
            "ablation_id": "2403.11369v2.No1"
        },
        {
            "research_objective": "To evaluate the impact of linguistic features on the success rate of LLMs in solving MWPs by controlling for mathematical features.",
            "experiment_process": "Utilized a KMeans clustering model on all math features for each question in the GSM8K training set, aiming for a target cluster count. This clustering grouped questions with minimal variability in math features. Therefore, variations in the success rate within each cluster were primarily attributed to linguistic features. Spearman correlation values between linguistic features and success rates were calculated and reported.",
            "result_discussion": "Significant negative correlations were observed between linguistic features (such as length, nesting, lexical rank, and reading grade) and success rates within clusters. This suggests that for a fixed set of math features, MWPs with higher values in these linguistic features are harder for LLMs to solve. However, this analysis's applicability might be limited to the GSM8K dataset, and a more exhaustive approach using generative methods for creating question paraphrases would be needed for broader generalization.",
            "ablation_id": "2403.11369v2.No2"
        }
    ]
}