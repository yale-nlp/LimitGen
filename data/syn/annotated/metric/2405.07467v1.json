{
    "title": "MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation",
    "abstract": "Recent advancements in large language models (LLMs) have enabled in-context learning (ICL)-based methods that significantly outperform fine-tuning approaches for text-to-SQL tasks. However, their performance is still considerably lower than that of human experts on benchmarks that include complex schemas and queries, such as BIRD. This study considers the sensitivity of LLMs to the prompts and introduces a novel approach that leverages multiple prompts to explore a broader search space for possible answers and effectively aggregate them. Specifically, we robustly refine the database schema through schema linking using multiple prompts. Thereafter, we generate various candidate SQL queries based on the refined schema and diverse prompts. Finally, the candidate queries are filtered based on their confidence scores, and the optimal query is obtained through a multiple-choice selection that is presented to the LLM. When evaluated on the BIRD and Spider benchmarks, the proposed method achieved execution accuracies of 65.5% and 89.6%, respectively, significantly outperforming previous ICL-based methods. Moreover, we established a new SOTA performance on the BIRD in terms of both the accuracy and efficiency of the generated queries.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The text-to-SQL task involves translating a natural language question into SQL and is crucial for natural language interfaces to databases (NLIDB) systems. With the recent advancements in large language models (LLMs), in-context learning (ICL)-based approaches for text-to-SQL (Pourreza and Rafiei, 2023a; Gao et al., 2023; Tai et al., 2023) have demonstrated significant performance improvements over traditional fine-tuning methods (Hui et al., 2022; Qi et al., 2022; Li et al., 2023a, b). Notably, Pourreza and Rafiei (2023b) showed that these methods even surpassed gold reference queries in terms of human evaluation within the Spider benchmark.\n\nHowever, for the more challenging BIRD (Li et al., 2023c) benchmark, characterized by its complex database (DB) schemas and queries, the accuracies of ICL-based methods have not exceeded 60%, which is significantly lower than the 93.0% achieved by humans. This gap underscores the need for further advancements in the ICL approach to serve as an NLIDB system.\n\nA significant limitation of LLMs across various tasks is their sensitivity to the structure and content of prompts. Even for semantically identical prompts, LLMs can generate drastically varying responses because of factors such as the order of sentences (Jang and Lukasiewicz, 2023; Wang et al., 2023b), choice of demonstration examples (Liu et al., 2022; Nori et al., 2023), and the sequence in which these examples are presented (Lu et al., 2022). Our experiments confirmed a similar tendency in the text-to-SQL context, where alterations in the schema presentation (section 5.3) and the choice of few-shot examples (section 5.4) resulted in variations in LLM outputs.\n\nIn this study, to improve the accuracy and robustness of LLMs for text-to-SQL, we introduce a novel approach that leverages multiple prompts to generate various candidate answers and effectively aggregates them. We utilize the sensitivity of LLMs to prompts to explore a broader search space for answers using different prompts.\n\nAs shown in Figure 1, the SQL generation process comprises three steps: schema linking, multiple SQL generation, and selection. Initially, the schema-linking phase robustly selects tables and columns relevant to the question from the DB schema using multiple prompts. Subsequently, the generation phase employs various prompts to produce diverse candidate SQL queries, ensuring a broader exploration of potential queries. Finally, the selection phase filters candidate queries based on confidence scores, and the optimal query is selected through multiple-choice selection (MCS) that is presented to the LLM."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Prompt engineering, which is the study of designing effective prompts, is an active research area as prompts significantly impact the performance of LLMs across various NLP tasks.\nA prominent example is the chain-of-thought (CoT) prompting (Wei et al., 2022  ###reference_b29###), which employs manually crafted examples to guide the LLM to generate intermediate reasoning steps prior to deriving the answer. This technique has demonstrated significant performance enhancements across various tasks. Kojima et al. (2022  ###reference_b10###) further demonstrated that LLMs can explain their reasoning steps even in the absence of human-annotated examples.\nIn addition to CoT, self-consistency decoding (Wang et al., 2022  ###reference_b26###) has been proposed, wherein multiple answers are sampled from the LLM before selecting one through a majority vote.\nCompared with greedy decoding, this technique facilitates the exploration of various reasoning paths and has exhibited substantial performance gains. For more effective explorations of the reasoning steps, variations such as tree-of-thought (Wei et al., 2022  ###reference_b29###) and graph-of-thought (Besta et al., 2023  ###reference_b2###) have also been proposed.\nHowever, because these approaches rely on a single prompt to generate the reasoning steps, we hypothesize that they fail to mitigate the issue of LLMs\u2019 high sensitivity to prompts, thereby exploring only a limited search space.\nSeveral studies have shown that even when presented with semantically identical prompts, the outputs of LLMs vary based on factors such as sentence structure (Webson and Pavlick, 2022  ###reference_b28###), sequence of sentences (Jang and Lukasiewicz, 2023  ###reference_b9###; Wang et al., 2023b  ###reference_b27###; Pezeshkpour and Hruschka, 2023  ###reference_b20###), choice of few-shot examples (Liu et al., 2022  ###reference_b15###; Nori et al., 2023  ###reference_b19###), and the order in which the examples are presented (Lu et al., 2022  ###reference_b17###).\nTherefore, we use multiple distinct prompts for a wider exploration of potential answers, thereby mitigating the LLM\u2019s sensitivity issue of LLMs to prompts and ensuring the generation of more robust answers.\nAs ICL-based approaches have shown remarkable performance in text-to-SQL tasks, various studies have focused on creating better prompts for text-to-SQL.\nSeveral studies have focused on applying prompting techniques such as CoT or least-to-most (Zhou et al., 2022  ###reference_b35###) for text-to-SQL generation (Pourreza and Rafiei, 2023a  ###reference_b21###; Tai et al., 2023  ###reference_b24###; Wang et al., 2023a  ###reference_b25###).\nHowever, these methods rely on fixed sets of manually crafted examples, and their performance can vary significantly depending on the selection of these examples.\nIn this work, instead of relying on fixed human-labeled examples, we dynamically select few-shot examples from the training data based on the test sample. Some studies have aimed to determine a more effective few-shot selection strategy for text-to-SQL (Guo et al., 2023  ###reference_b6###; Nan et al., 2023  ###reference_b18###; Gao et al., 2023  ###reference_b5###). However, unlike these studies, which focused on determining a single optimal selection strategy, we employ a parallel approach that employs various few-shot selection strategies with multiple prompts and effectively aggregates them."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "As shown in Figure 1  ###reference_###, the proposed method comprises three steps: (1) schema linking, wherein tables and columns irrelevant to the question from the DB schema are excluded; (2) multiple SQL generation, wherein multiple candidate SQL queries are generated based on various prompts; and (3) selection, wherein the most accurate SQL query is selected from among the candidates."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Schema Linking",
            "text": "Schema linking involves identifying relevant tables and columns from a DB to convert a natural language question into an SQL query (Guo et al., 2019  ###reference_b7###).\nThe introduction of schema linking has significantly improved the performances of both fine-tuning-based (Lei et al., 2020  ###reference_b11###; Xu et al., 2022  ###reference_b31###; Qi et al., 2022  ###reference_b23###; Li et al., 2023a  ###reference_b12###) and ICL-based (Dong et al., 2023  ###reference_b3###; Pourreza and Rafiei, 2023a  ###reference_b21###; Wang et al., 2023a  ###reference_b25###) approaches.\nWe perform schema linking in two steps: first, the tables related to the natural language query are extracted (table linking). Thereafter, the necessary columns within those tables are extracted (column linking).\nWe employ multiple prompts in both phases with the aim of achieving a high recall."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Table Linking",
            "text": "In table linking, the DB schema and question are input into the LLM, which extracts a list of reference tables to generate the SQL query. Inspired by zero-shot-CoT (Kojima et al., 2022  ###reference_b10###), we ask the LLM to explain why each table is necessary, instead of just selecting a list of tables. To easily parse the LLM\u2019s answer, we ask it to respond in the JSON format, as described in Listing LABEL:lst:table-linking.\nTo enhance the robustness of table linking, we utilize multiple prompts. Various studies have demonstrated that LLM outputs are significantly affected by the sequence of input sentences (Jang and Lukasiewicz, 2023  ###reference_b9###; Wang et al., 2023b  ###reference_b27###; Liu et al., 2023  ###reference_b16###). Similarly, our experiments (section 5.3  ###reference_###) revealed that the output of schema-linking output of LLMs also depends on the sequence in which the tables and columns are arranged in the prompts.\nTo minimize the influence of the table order, we randomly shuffle the order of tables, generating  distinct prompts. For each prompt, we obtain  responses from the LLM by using a high sampling temperature. The final table-linking output is derived from the union of all responses, amounting to  table lists. We use a union operation because including unnecessary tables in table linking does not significantly impact the subsequent SQL-generation process; however, omitting the necessary tables prevents the generation of the correct SQL query."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Column Linking",
            "text": "For column linking, we ask the LLM to extract the columns required for converting a question into an SQL query using a prompt similar to that used in table linking.\nThe prompt includes only the schemas of the tables selected during table linking, instead of the entire DB schema.\nBecause the same column name can exist in different tables, we instruct the LLM to provide the answer in the [table_name].[column_name] format.\nSimilar to table linking, the order of the tables and columns is randomly shuffled to generate  unique prompts. Subsequently,  LLM responses are generated for each prompt, where each response represents a selected column list. The column-linking output is the union of all  responses.\nIn the subsequent SQL-generation steps, when providing the DB schema to LLM, only the tables and columns selected through schema linking are provided instead of the full schema."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Multiple SQL Generation",
            "text": "To address the sensitivity of the LLM to prompts, we generate various SQL queries based on multiple, distinct prompts. Several studies have demonstrated that the output of an LLM can differ significantly depending on the few-shot examples provided (Liu et al., 2022  ###reference_b15###; Wu et al., 2023  ###reference_b30###), and even on the sequence in which these examples are presented (Lu et al., 2022  ###reference_b17###; Zhao et al., 2021  ###reference_b33###).\nTo effectively leverage this variability, we generate multiple prompts by varying both the selection method of the few-shot examples and the order in which they are presented, thereby ensuring a broader exploration of potential SQL queries."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Few-Shot Examples Selection",
            "text": "For each test sample, a set of few-shot examples is selected from the training dataset. To generate multiple prompts with different examples, we use two distinct selection strategies: one that leverages question similarity and another that utilizes masked question similarity.\nIn the question similarity-based approach, we select the top-k questions from the training dataset that have the nearest sentence embeddings to the natural language question of the test sample.\nSimilarly, the masked question similarity-based approach considers the embedding similarity of masked questions, wherein tokens specific to the DB schema in the question are masked. This masking allows determining the similarity of questions in terms of generating similar queries by disregarding schema-specific content.\nWe employ an LLM for the masking process by presenting it with the DB schema and question, and asking it to replace the table names, column names, and values with special tokens.\nThe prompt for this question masking is presented in Appendix B.2  ###reference_###.\nThrough these two few-shot selection strategies, we generate  different prompts, including one derived exclusively from question similarity, another solely from masked question similarity, and additional prompts created by integrating examples from both strategies in various sequences."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 SQL Generation",
            "text": "As illustrated in Listing LABEL:lst:sql-generation, our SQL-generation prompt includes few-shot examples, a DB schema, sample table contents, and a natural language question.\nThe few-shot examples comprise questions and their corresponding gold SQL pairs. To conserve the limited length of the prompt, we exclude the schema of the target DB for each question.\nRegarding the DB schema, we selectively present only the tables and columns selected during the schema-linking process to avoid burdening the LLM with irrelevant information.\nAdditionally, we embed sample table contents in the CSV format within the prompt to facilitate the LLM\u2019s comprehension of potential values in each column, thereby providing practical insight into the data structure of the DB.\nFinally, we instruct LLM not only to produce the SQL query but also to explain the reasoning behind its generation, thereby enhancing the model\u2019s interpretability and accuracy.\nFor each prompt, we generate  responses from the LLM using a high sampling temperature, resulting in  candidate SQL queries being generated."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Selection",
            "text": "The selection step aims to select the most accurate query from the candidate queries. Initially, the candidate pool is filtered based on confidence scores, and the LLM is then tasked with selecting the most accurate query from the refined pool."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Candidate Filtering",
            "text": "To select the most accurate query among the candidates, we first narrow down the candidate pool. Queries with the same execution results are grouped together, and only the fastest query from each group is retained. Additionally, queries with low confidence scores are excluded from the candidates.\nIn detail, all candidate queries are executed on the DB, and queries that result in syntax errors or timeouts are removed from the candidate pool.\nNext, the confidence score for each query in the candidate pool is calculated, which is determined based on the number of queries that produce the same execution result. Formally, for a candidate pool  containing  queries , the confidence of query  is computed as follows:\nwhere  denotes the execution result for query .\nAmong the queries in candidate pool  with identical execution results, only the query with the minimum execution time is selected as follows:\nwhere  represents the set of all unique execution results from all queries in , and  denotes the execution time for query .\nFinally, all queries in  with confidence score below threshold  are excluded as follows:\nresulting in a refined candidate pool ."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Multiple-Choice Selection (MCS)",
            "text": "Following the filtering process, we utilize the LLM to select the most accurate query among the candidates through a multiple-choice question.\nAs shown in the Listing LABEL:lst:mcq-prompt, we present a set of candidate SQL queries to the LLM and request it to select the most accurate query for a given DB schema and question.\nCandidate queries are provided in descending order of confidence scores, considering the tendency of LLMs to favor options that appear earlier in the multiple-choice questions (Wang et al., 2023b  ###reference_b27###; Zheng et al., 2023  ###reference_b34###).\nThe LLM is required to not only select an SQL query but also provide the reasons for its selection.\nWe sample  responses from the LLM and determine the final SQL query through a majority vote.\n###table_1### ###table_2###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "Spider (Yu et al., 2018  ###reference_b32###) is a large-scale, complex, cross-domain text-to-SQL benchmark comprising 10,181 questions and 5,693 distinct queries across 200 databases, each with multiple tables. This benchmark requires the model to adapt to an unseen DB schema because different DBs are used for training and testing. BIRD (Li et al., 2023c  ###reference_b14###) is a new large-scale, cross-domain text-to-SQL benchmark comprising 12,751 unique question-SQL pairs across 95 large real-world databases. Compared with Spider, BIRD comprises considerably more complex SQL queries with various SQL keywords (LEFT JOIN, PARTITION BY, etc.) and functions (IIF, CASE, ROUND, etc.) that are not included in Spider. In addition, BIRD requires reasoning using external knowledge (such as synonym knowledge and value illustrations) to generate accurate SQL queries. For the BIRD dataset, Li et al. (2023c  ###reference_b14###) proposed an additional evaluation metric called VES that measures the efficiency of a valid model-generated query based on the execution time. A query is considered invalid and assigned a score of zero if its execution result differs from that of the gold SQL. Therefore, VES considers both the model accuracy and efficiency for the generated query. uses the zero-shot prompt provided in OpenAI\u2019s text-to-SQL demo111https://platform.openai.com/examples/default-sql-translate  ###reference_lt-sql-translate###. classifies the complexity of the question and generates an SQL query by applying different prompts based on the classification result. In each step, it uses a prompt with fixed few-shot examples that are manually written in the CoT style. employs dynamic few-shot examples by considering the similarity of both the questions and the queries. For additional performance improvement, self-consistency (Wang et al., 2022  ###reference_b26###) is introduced. decomposes the question into sub-questions and sequentially generates SQL queries for each sub-question using manually crafted few-shot samples. Additionally, in case of a syntax error, it uses a prompt to correct the generated query."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Spider (Yu et al., 2018) is a large-scale, complex, cross-domain text-to-SQL benchmark comprising 10,181 questions and 5,693 distinct queries across 200 databases, each with multiple tables. This benchmark requires the model to adapt to an unseen DB schema because different DBs are used for training and testing. BIRD (Li et al., 2023c) is a new large-scale, cross-domain text-to-SQL benchmark comprising 12,751 unique question-SQL pairs across 95 large real-world databases. Compared with Spider, BIRD comprises considerably more complex SQL queries with various SQL keywords (LEFT JOIN, PARTITION BY, etc.) and functions (IIF, CASE, ROUND, etc.) that are not included in Spider. In addition, BIRD requires reasoning using external knowledge (such as synonym knowledge and value illustrations) to generate accurate SQL queries."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation Metrics",
            "text": "For the BIRD dataset, Li et al. (2023c) proposed an additional evaluation metric called VES that measures the efficiency of a valid model-generated query based on the execution time. A query is considered invalid and assigned a score of zero if its execution result differs from that of the gold SQL. Therefore, VES considers both the model accuracy and efficiency for the generated query."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Implementation Details",
            "text": "In all of our experiments, we used the GPT-4 8K as the LLM and text-embedding-ada-002 as the sentence embedding model, which was accessed via Azure OpenAI API. Additionally, we employed the FAISS (Douze et al., 2024) library for the embedding similarity search. \n\nIn schema linking, we used prompts for table linking and prompts for column linking. To generate multiple candidate SQL queries, we used distinct prompts. For each GPT API call, we used a temperature of 1.0 and generated 20 responses.\n\nIn both the SQL-generation and MCS steps, we used 20 question-SQL pairs as few-shot examples. We executed all candidate SQL queries with a timeout of 180s and filtered out queries with a confidence score lower than the threshold of 0.2."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We compare the proposed MCS-SQL approach with ICL-based methods based on GPT-4.\n\nuses the zero-shot prompt provided in OpenAI\u2019s text-to-SQL demo. \n\nclassifies the complexity of the question and generates an SQL query by applying different prompts based on the classification result. In each step, it uses a prompt with fixed few-shot examples that are manually written in the CoT style.\n\nemploys dynamic few-shot examples by considering the similarity of both the questions and the queries. For additional performance improvement, self-consistency is introduced.\n\ndecomposes the question into sub-questions and sequentially generates SQL queries for each sub-question using manually crafted few-shot samples. Additionally, in case of a syntax error, it uses a prompt to correct the generated query."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "Table 1  ###reference_### presents the VES of the proposed and baseline models for the BIRD dev and test sets. The results demonstrate that the proposed approach significantly outperforms existing ICL-based approaches in the metric. Specifically, the proposed method achieved a VES of 71.35% on the holdout test set, surpassing the performance of the previous SOTA ICL-based approach (Wang et al., 2023a  ###reference_b25###) by a significant margin of 3.67%, respectively. Furthermore, the proposed method established a new SOTA performance on the BIRD, surpassing the former SOTA method with a substantial margin of 3.67% in VES."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Main Results",
            "text": "Table 1 presents the VES of the proposed and baseline models for the BIRD dev and test sets. The results demonstrate that the proposed approach significantly outperforms existing ICL-based approaches in both metrics. Specifically, the proposed method achieved a VES of 71.35% on the holdout test set, surpassing the performance of the previous SOTA ICL-based approach (Wang et al., 2023a) by significant margins of 3.67%. Furthermore, the proposed method established a new SOTA performance on the BIRD, surpassing the former SOTA method with a substantial margin of 3.67% in VES.\nTable 2 presents the results of the proposed and baseline methods for the Spider dev and test sets. Similar to the results obtained for BIRD, the proposed approach significantly outperforms all existing ICL-based approaches."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Ablation Study",
            "text": "The addition of schema linking to the baseline zero-shot setting resulted in a 2.1% improvement. This underscores the importance of refining the schema prior to SQL generation and shows that the proposed schema-linking process effectively selects relevant tables and columns. The inclusion of the sample table contents in the prompt further amplified this gain by +2.4%. The introduction of dynamic few-shot examples that were selected based on masked question similarity resulted in the largest performance improvement of +4.8%. Moreover, when we sampled multiple answers from the LLM using the same prompt and employed the proposed MCS method, the performance further improved by 2.1%. This demonstrates the capability of the proposed SQL selection method in discerning and selecting the most accurate query from a set of candidates. Finally, introducing multiple prompts led to further enhancements of +1.3%, particularly showing significant performance improvement on challenging queries. This improvement demonstrates that broadening the search space using various prompts significantly boosted the SQL-generation accuracy.\n\nTable 4 lists the ablation results for the Spider dev set, wherein it is evident that each component of the proposed approach contributed to significant performance gains. This consistent performance enhancement across different benchmarks confirms the effectiveness and adaptability of the proposed approach for the text-to-SQL task."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Impact of Using Multiple Prompts in Schema Linking",
            "text": "We conducted a comparative analysis of the following two cases to investigate the impact of using multiple prompts in schema linking: (1) greedy decoding with a single prompt; and (2) taking the union of multiple answers generated from a single prompt. Table 5 lists the recall of schema linking for each case, which was calculated based on whether the predicted list of tables and columns included those used in the gold query.\n\nThe results demonstrate that sampling multiple responses using the same prompt and aggregating them led to notable performance gains of +15.8% for BIRD and +4.7% for Spider. These results indicate that the order of tables and columns in the prompt affects the schema-linking results of the LLM. This improvement was particularly noticeable in BIRD, implying that the effectiveness increases for larger and more complex DB schemas.\n\nAs it is impossible to generate accurate SQL queries in subsequent processes if the necessary tables or columns are omitted in schema linking, using the union of various responses is pivotal for enhancing the SQL-generation performance."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Impacts of Different Few-shot Selection Strategies",
            "text": "The performance improved significantly by selecting few-shot examples based on question similarity instead of random selection, with an increase of 2.3% for BIRD and 4.2% for Spider. Additionally, a further performance boost was noted by predicating the selection on the similarity of the masked question rather than the original question, with enhancements of 0.7% and 0.5% for BIRD and Spider, respectively."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Impact of MCS",
            "text": "During the SQL selection phase (section 3.3), we explored whether the proposed MCS via LLM was more effective than a majority vote, which selects the query with the highest confidence score. As presented in Table 7, the proposed MCS approach outperformed the majority vote approach by +0.6% and +0.3% for the BIRD and Spider, respectively. Notably, in the absence of confidence-based filtering, as expressed in Eq. (3), the efficacy of the MCS method decreased significantly. This result underscores the importance of employing confidence-based filtering to effectively narrow down the candidate pool when using MCS."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This study introduces a novel method that leverages multiple prompts to enhance the accuracy and robustness of ICL-based text-to-SQL generation. Specifically, the proposed approach performs robust schema linking using distinct prompts. In addition, we employ different few-shot selection strategies to generate multiple query generation prompts, which yield various candidate SQL queries. These candidates are subsequently filtered based on their confidence scores, and the optimal query is selected using the LLM with MCS. Evaluations on the BIRD and Spider benchmarks showed that the proposed approach significantly outperforms existing ICL-based approaches and achieved a new SOTA performance on the BIRD."
        }
    ],
    "url": "http://arxiv.org/html/2405.07467v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.3",
            "3.3.1",
            "3.3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "research_context": {
        "paper_id": "2405.07467v1",
        "paper_title": "MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation",
        "research_background": "### Paper's Motivation\nThe primary motivation behind this work is to address the performance gap in text-to-SQL generation for natural language interfaces to databases (NLIDB) systems, particularly when using large language models (LLMs) with in-context learning (ICL) approaches. Although ICL-based methods have shown significant improvements over traditional fine-tuning methods, they still fall short, especially on benchmarks with complex database schemas and queries such as BIRD, achieving much lower accuracy rates compared to human performance. The motivation is thus rooted in the need to enhance the accuracy and robustness of these LLMs to make them more viable as NLIDB systems.\n\n### Research Problem\nThe research problem tackled by this paper is the inherent sensitivity of LLMs to the structure and content of prompts, which causes variations in their output responses. This sensitivity hampers the performance of text-to-SQL generation, particularly in complex benchmarks like BIRD. The paper aims to overcome this limitation by developing a method that leverages multiple prompts to generate diverse candidate SQL queries and subsequently aggregates these queries to select the most accurate one.\n\n### Relevant Prior Work\nThe paper builds on several strands of relevant prior work:\n1. **ICL-based Methods**:\n   - The efficacy of ICL-based methods for text-to-SQL has been previously demonstrated (Pourreza and Rafiei, 2023a; Gao et al., 2023; Tai et al., 2023).\n   - These methods have been shown to surpass traditional fine-tuning approaches (Hui et al., 2022; Qi et al., 2022; Li et al., 2023a, b) and even gold standard reference queries in some benchmark evaluations (Pourreza and Rafiei, 2023b).\n\n2. **Challenges with Complex Database Schemas**:\n   - Despite the success of ICL-based methods, they have struggled with complex benchmarks like BIRD, where performance remains significantly lower than human accuracy (Li et al., 2023c).\n\n3. **Sensitivity to Prompts**:\n   - The sensitivity of LLMs to the structure and content of prompts has been documented across various tasks (Jang and Lukasiewicz, 2023; Wang et al., 2023b), affecting the consistency of their outputs.\n   - Factors influencing this sensitivity include the order of sentences (Jang and Lukasiewicz, 2023; Wang et al., 2023b), choice of demonstration examples (Liu et al., 2022; Nori et al., 2023), and the sequence of these examples (Lu et al., 2022).\n\nThe paper leverages these insights to devise a method that can handle the sensitivity issue by using multiple prompts and an effective aggregation mechanism to improve the robustness and accuracy of text-to-SQL generation.",
        "methodology": "MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation\n\n**Methodology:**\n\nThe proposed method comprises three steps: \n1. **Schema Linking**: In this step, tables and columns irrelevant to the question from the DB schema are excluded. This ensures that only the relevant parts of the database schema are considered in the subsequent steps, thereby reducing complexity and improving accuracy.\n\n2. **Multiple SQL Generation**: Here, multiple candidate SQL queries are generated based on various prompts. This diversity in prompts leads to a range of candidate queries, capturing different perspectives and interpretations of the input question.\n\n3. **Selection**: In the final step, the most accurate SQL query is selected from among the candidates generated in the previous step. This is pivotal to the model's success as it ensures that the best possible translation of the input question into SQL is chosen.\n\nBy combining schema linking, multiple SQL generation, and selective strategies, MCS-SQL enhances the accuracy and robustness of text-to-SQL generation.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\nThe main experiment utilizes two prominent datasets:\n1. **Spider**:\n   - Description: A large-scale, complex, cross-domain text-to-SQL benchmark.\n   - Composition: Contains 10,181 questions and 5,693 distinct queries across 200 databases.\n   - Challenge: Requires the model to adapt to unseen database schemas because different databases are used for training and testing.\n\n2. **BIRD**:\n   - Description: A new large-scale, cross-domain text-to-SQL benchmark.\n   - Composition: Includes 12,751 unique question-SQL pairs across 95 large real-world databases.\n   - Challenge: Comprises more complex SQL queries with multiple SQL keywords (e.g., LEFT JOIN, PARTITION BY) and functions (e.g., IIF, CASE, ROUND) not included in Spider. Additionally, it requires reasoning using external knowledge (like synonyms and value illustrations) to generate accurate SQL queries.\n\n#### Evaluation Metrics\n1. **EX (Execution)**: Indicates whether the SQL execution result generated by the model matches the gold SQL query. This metric is essential because SQL queries can be written in different forms to produce the same result, making exact string matching insufficient.\n2. **VES (on BIRD dataset)**: Proposed by Li et al. (2023c ###reference_b14###). Measures the efficiency of a valid model-generated query based on its execution time. A query is considered invalid and given a score of zero if its execution result differs from that of the gold SQL. Therefore, VES assesses both the accuracy and efficiency of the generated queries.\n\n#### Baselines\nThe experiment compares the proposed method against several existing methods:\n1. **Zero-shot Prompt (OpenAI\u2019s text-to-SQL demo)**: Utilizes the zero-shot prompt provided in OpenAI\u2019s demo.\n2. **Prompting Based on Query Complexity**: Classifies the complexity of the question and applies different prompts accordingly.\n3. **Dynamic Few-shot Examples with Self-consistency**: Uses dynamic few-shot examples by evaluating the similarity of questions and queries and incorporates self-consistency.\n4. **Sequential Sub-query Generation with Error Correction**: Decomposes the question into sub-questions, sequentially generates SQL queries for each, and uses prompts to correct any syntactical errors if encountered.\n\n#### Main Experimental Results\nThe main experimental results highlight the effectiveness of the proposed methodology (leveraging multiple prompts and multiple-choice selection) in handling text-to-SQL tasks on the Spider and BIRD datasets. The detailed results demonstrate how the method surpasses traditional baselines in terms of both metrics (EX and VES), showcasing its superiority in generating accurate and efficient SQL queries through innovative prompt strategies and dynamic example selection.\n\nIn conclusion, the proposed approach distinctly improves model performance on challenging text-to-SQL benchmarks, balancing accuracy and efficiency efficiently through advanced prompt management, dynamic example selection, and error correction mechanisms."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the incremental impact of each component of the proposed approach on the execution accuracy (EX) for text-to-SQL generation.",
            "experiment_process": "The study was conducted on the BIRD development set and involved adding different components to the baseline zero-shot setting. The components included schema linking, adding sample table contents in the prompt, introducing dynamic few-shot examples based on masked question similarity, sampling multiple answers from the LLM using the same prompt with the proposed MCS method, and introducing multiple prompts.",
            "result_discussion": "The study showed a 2.1% improvement with schema linking, an additional +2.4% with sample table contents, a +4.8% gain from dynamic few-shot examples, a 2.1% improvement from the MCS method, and a further +1.3% from multiple prompts. These improvements demonstrate the importance of each component in enhancing the SQL-generation accuracy.",
            "ablation_id": "2405.07467v1.No1"
        },
        {
            "research_objective": "To investigate the impact of using multiple prompts in schema linking for text-to-SQL generation.",
            "experiment_process": "Three cases were compared: (1) greedy decoding with a single prompt, (2) taking the union of multiple answers from a single prompt, and (3) taking the union of multiple answers from multiple prompts. The recall of schema linking was calculated based on whether the predicted list of tables and columns included those used in the gold query for BIRD and Spider datasets.",
            "result_discussion": "Sampling multiple responses from the same prompt and aggregating them led to gains of +15.8% for BIRD and +4.7% for Spider. Leveraging multiple prompts contributed to further improvements, with gains of +12.7% for BIRD and +2.7% for Spider. These results indicate the effectiveness of using multiple prompts in mitigating the sensitivity of schema linking to the order of tables and columns in the prompt.",
            "ablation_id": "2405.07467v1.No2"
        },
        {
            "research_objective": "To evaluate the impacts of different few-shot selection strategies on the execution accuracy (EX) for text-to-SQL generation.",
            "experiment_process": "Different few-shot strategies were employed and evaluated based on the execution accuracy metrics for both BIRD and Spider datasets. These strategies involved selecting few-shot examples based on question similarity and the similarity of the masked question.",
            "result_discussion": "The performance improved by 2.3% for BIRD and 4.2% for Spider with question similarity-based selection over random selection. Further enhancement of 0.7% for BIRD and 0.5% for Spider was noted by selecting examples based on the similarity of the masked question.",
            "ablation_id": "2405.07467v1.No3"
        },
        {
            "research_objective": "To explore the effectiveness of the proposed Multiple-Choice Selection (MCS) method compared to a majority vote in the SQL selection phase for text-to-SQL generation.",
            "experiment_process": "The SQL selection phase involved comparing the proposed MCS method to a majority vote approach, which selects the query with the highest confidence score. The performance was evaluated with and without confidence-based filtering using the BIRD and Spider datasets.",
            "result_discussion": "The proposed MCS approach outperformed the majority vote by +0.6% for BIRD and +0.3% for Spider. However, the efficacy of the MCS method decreased significantly without confidence-based filtering, highlighting the importance of this filtering step.",
            "ablation_id": "2405.07467v1.No4"
        }
    ]
}