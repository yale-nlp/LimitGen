{
    "title": "Open-SQL Framework: Enhancing Text-to-SQL on Open-source Large Language Models",
    "abstract": "Despite the success of large language models (LLMs) in Text-to-SQL tasks, open-source LLMs encounter challenges in contextual understanding and response coherence. To tackle these issues, we present Open-SQL, a systematic methodology tailored for Text-to-SQL with open-source LLMs. Our contributions include a comprehensive evaluation of open-source LLMs in Text-to-SQL tasks, the Open Prompt strategy for effective question representation, and novel strategies for supervised fine-tuning. We explore the benefits of Chain-of-Thought in step-by-step inference and propose the Open Example Curation method for enhanced few-shot learning. Additionally, we introduce token-efficient techniques, such as Variable-length Open DB Schema, Target Column Truncation, and Example Column Truncation, addressing challenges in large-scale databases. Our findings emphasize the need for further investigation into the impact of supervised fine-tuning on contextual learning capabilities. Remarkably, the performance of Code Llama-7B surpassed GPT-4 (46.35%) on the BIRD-Dev dataset.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "SQL, efficient for professionals, may pose a barrier for non-technical users to access relational data. Text-to-SQL parsing, translating natural language questions into machine-executable SQL queries, has gained attention in both industry and academia Deng et al. (2022). This approach empowers non-expert users to query tables and plays a pivotal role in applications such as customer service, question answering, and robotic navigation.\n\nMost previous works Wang et al. (2020); Li et al. (2023b) focus on extracting the question-to-SQL patterns and generalizing them by training an encoder-decoder model with Text-to-SQL corpus. In recent years, Large Language Models (LLMs) have emerged as a new paradigm for Text-to-SQL Liu et al. (2023a). Different from prior studies, the core problem in LLM-based Text-to-SQL solution is how to prompt LLM to generate correct SQL queries, namely prompt engineering.\n\nSuch prompt engineering involves question representations Pourreza and Rafiei (2023), example selection Nan et al. (2023), and example organization Guo et al. (2023).\n\nRecently, open-source LLMs have gained traction in programming and text tasks due to transparency, accessibility, affordability, privacy benefits, and community-driven development. Despite these advantages, they often lag behind OpenAI LLMs in context understanding and coherent response generation. A key challenge for open-source LLMs is improving performance in Text-to-SQL, achievable through supervised fine-tuning and in-context learning.\n\nIn this paper, we present a systematic methodology, labeled as Open-SQL, crafted for Text-to-SQL tasks with open-source LLMs like Llama and Code Llama. This methodology provides effective and efficient techniques for both supervised fine-tuning and in-context learning. Our main contributions and results are summarized as follows:\n\nSystematic Evaluation: A comprehensive assessment of open-source LLMs on Text-to-SQL tasks using the BIRD dataset, revealing significant performance deficiencies in understanding the provided database schema.\n\nEffective Strategies: We introduced effective strategies, including the Open Prompt question representation, two novel Chain-of-Thought (COT) methods for step-by-step inference improvement\u2014the first exploration of COT in Text-to-SQL tasks\u2014and Open Example Curation for few-shot learning, enhancing performance across both GPT and open-source LLMs.\n\nToken Efficiency for Large-scale Databases: We introduce techniques to overcome memory limitations during fine-tuning and inference with few examples on large-scale databases by reducing token length. The proposed strategies, including Variable-length Open DB Schema, Target Column Truncation, and Example Column Truncation, verified through experiments, play significant roles in ensuring the success of supervised fine-tuning and few-shot learning for open-source LLMs in Text-to-SQL tasks.\n\nBenefits of Supervised Fine-Tuning: Experimental evidence demonstrates significant performance enhancements in open-source LLMs for Text-to-SQL tasks achieved through question representation strategies, supervised fine-tuning, Chain-of-Thought (COT), and few-shot learning.\n\nOpen-Source Available: We have open-sourced our code and models to foster further research, adoption, and innovation in the Text-to-SQL domain within the community."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Recent studies have approached Text-to-SQL as a sequence-to-sequence task, employing machine learning models trained with encoder-decoder architectures Cai et al. (2018  ###reference_b1###); Popescu et al. (2022  ###reference_b22###); Qi et al. (2022  ###reference_b24###). Techniques such as attention mechanisms, graph representation, syntax parsing, and deep learning have been extensively utilized to enhance Text-to-SQL methods Liu et al. (2023b  ###reference_b16###); Xu et al. (2018  ###reference_b31###); Li et al. (2023b  ###reference_b13###); Zheng et al. (2022  ###reference_b33###); Wang et al. (2020  ###reference_b29###); Qi et al. (2022  ###reference_b24###); Hui et al. (2022  ###reference_b10###); Guo et al. (2019  ###reference_b8###); Scholak et al. (2021  ###reference_b25###); Li et al. (2023a  ###reference_b12###); Wang et al. (2022  ###reference_b30###). The advent of pretraining language models, extended to Tabular Language Models (TaLMs) that directly encode tables and texts, has proven beneficial Yin et al. (2020  ###reference_b32###). However, creating a versatile pretraining model specifically tailored for Text-to-SQL tasks remains a complex challenge with associated costs.\nRecent advancements in natural language processing and artificial intelligence have witnessed the emergence of Large Language Models (LLMs) such as GPT from OpenAI OpenAI (2023b  ###reference_b20###, a  ###reference_b19###) and Llama from Meta Touvron et al. (2023a  ###reference_b27###, b  ###reference_b28###). LLMs possess the potential to bridge the gap between natural language queries and structured SQL queries. Two prominent techniques for incorporating LLMs into the Text-to-SQL task include supervised fine-tuning, where additional Text-to-SQL instances refine LLMs for enhanced performance in specific tasks Sun et al. (2023  ###reference_b26###), and in-context learning Dong et al. (2023a  ###reference_b4###), also known as prompt engineering Nan et al. (2023  ###reference_b18###); Liu et al. (2023a  ###reference_b15###) or few-shot learning. In the context of Text-to-SQL, various prompt construction methods have been proposed, involving question representations Chang and Fosler-Lussier (2023  ###reference_b2###); Dong et al. (2023b  ###reference_b5###); OpenAI (2023c  ###reference_b21###); Pourreza and Rafiei (2023  ###reference_b23###), example selection Guo et al. (2023  ###reference_b7###); Liu et al. (2022  ###reference_b17###); Nan et al. (2023  ###reference_b18###), and example organization Guo et al. (2023  ###reference_b7###).\nIn conclusion, open-source LLMs in Text-to-SQL tasks may provide cost savings and privacy benefits but exhibit lower proficiency in context comprehension and coherent response generation compared to proprietary counterparts, such as those from OpenAI, posing a notable challenge."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Zero-shot Error Analysis",
            "text": "BIRD Li et al. (2023c  ###reference_b14###), tailored for Text-to-SQL tasks in real-world applications, emphasizes data integrity, integrating natural language queries with database information, and improving SQL efficiency in large databases. It includes 9,427 training instances and 1,534 development instances across 95 databases in 37 domains, with each instance comprising a natural language question and its corresponding SQL query. For evaluation in this study, the development split, BIRD-dev, is utilized due to the unavailability of the test split.\nTo assess Llama2 and Code Llama in zero-shot Text-to-SQL tasks, we examined queries on BIRD-dev with discrepancies from the gold standard, indicating accuracy failures. The evaluation reveals significant performance shortcomings: Code Llama has an error rate exceeding 90% on moderate and challenging questions, with an 83% error rate on simple questions. Llama2 shows an error rate surpassing 98% across all difficulty levels. A subsequent manual examination categorizes these failures into the following four types.These categories are detailed in Figure 1  ###reference_###, with summarized results presented for a clearer understanding.\nWrong schema linking: The category with the most number of failed queries entails instances where the model struggled to identify column names, table names, and where statements, resulting in inaccurate SQL statements. For instance, in the query \u201cHow many users received commentator badges in 2014\u201d, where the database schema specified a \u201cbadges\u201d table, the model incorrectly referenced a non-existent \u201cusers\u201d table.\nIncorrect JOIN Operation: This category involves queries requiring a JOIN operation. The model faces difficulties accurately identifying necessary tables or determining correct foreign keys for joins.\nInaccurate Nested Structure: In this category, the gold query employed nesting or set operations, yet the model failed to identify the nested structure accurately or was unable to detect the correct nesting or set operation.\nOthers: This category encompassed cases that didn\u2019t align with any previously defined categories. It featured SQL queries with various issues: syntac error and incorrect GROUP BY.\n###figure_1### ###figure_2### In summary, the observed deficiencies in both models, particularly their incapacity to produce valid SQL statements, underscore the urgent need for enhancements in open-source LLMs to adeptly address Text-to-SQL tasks."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we introduce Open-SQL, a systematic method designed to boost the performance of open-source large language models in Text-to-SQL tasks. In the context of addressing a specific SQL question  expressed in natural language within a defined database , our primary aim is to harness the capabilities of the\nLLMs  to generate a fitting SQL statement denoted as . This entails the introduction of a dedicated question representation function  designed to enhance the representation of the SQL question  by incorporating pertinent details, notably the database schema. Additionally, we introduce a prompt function  with the explicit purpose of providing a tailored prompt for the SQL question , and  is the additional information which will be explained in Section 4.3  ###reference_###. The final answer is generated through the application of .\nTo enhance the efficacy of this approach, our paper utilizes a synergistic combination of question representation, supervised fine-tuning, and in-context learning techniques. This integrated strategy aims to improve the performance of the language model  in handling SQL queries denoted by . More precisely, the goal of this paper is to maximize the likelihood of LLMs  generating the correct SQL , as outlined below:\nwhere  is the probability of the model  generating  with the input .\nJointly solving ,  and  is challenging. In this paper, we adopt a step-by-step strategy, addressing the problem through question representation to optimize , supervised fine-tuning to optimize  and in-context learning in inference stage to optimize ."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Question Representation",
            "text": "Gao et al. analyzed five representation methods Gao et al. (2023  ###reference_b6###), highlighting the effectiveness of Code Representation Prompt, which introduces \u201cCREATE TABLE\u201d SQLs and includes natural language questions in comments to prompt a Language Model (LM) Chang and Fosler-Lussier (2023  ###reference_b2###); Nan et al. (2023  ###reference_b18###). In contrast to the Spider benchmark, the Bird benchmark poses challenges for Code Representation Prompt by including external knowledge evidence such as numeric reasoning, domain knowledge, synonym knowledge, and value illustration. To address this issue, we introduce Open Prompt, a question representation mechanism detailed in Listing 1  ###reference_###, providing a thorough and nuanced data representation for understanding the database definition. Open Prompt comprises four core components: DATABASE_SCHEMA defines the format of schema; DATABASE_DEFN represents the concrete schema definition implemented based on the format outlined in DATABASE_SCHEMA; EXTERNAL_KNOWLEDGE contains pertinent external information; TARGET_QUESTION denotes the specific question in focus.\nIn our initial experimental findings, we underscore the importance of understanding the database definition. To enhance the provision of diverse information for schema linking, we design a Open DB Schema to better bridge the LLMs and database, offering a thorough and nuanced representation of the data. The detailed definition of DATABASE_SCHEMA is as follows:\nDatabase definition: The definitions of the relevant tables are organized sequentially, with each row representing an individual table. Subsequently, foreign key definitions follow, where each row represents a key mapping.\nTable definition: A table\u2019s definition commences with the table name, followed by all column definitions enclosed in brackets. These columns are organized sequentially, with each row representing an individual column.\nColumn definition: The definition of a column starts with the column name, followed by \u201c:\u201d, and four optional elements:\nTYPE: Indicates the column type. In the database definition, the corresponding values including \u201ctext,\u201d \u201cint,\u201d \u201cdate,\u201d \u201cdatetime,\u201d \u201creal,\u201d and \u201cvarchar,\u201d which align with the definition in SQLite.\nDESCRIPTION: Incorporates detailed column information (if available).\nVALUES: Enumerates categorical values for columns with up to five distinct categories, presented in the format \u201c(value1, value2, \u2026)\u201d for clarity and conciseness.\nPRIMARY_KEY: Indicates whether this column serves as the primary key.\nEXTERNAL_KNOWLEDGE in Listing 1  ###reference_### refers to the additional descriptive information accompanying each query, as provided by the BIRD dataset, that acts as a bridge between human comprehension and the database structure. For example, within the BIRD-DEV dataset, the query \u201cPlease list the phone numbers of the schools with the highest SAT excellence rates in the top 3\u201d is accompanied by the external knowledge \u201cExcellence rate = NumGE1500 / NumTstTakr.\u201d which defines the \u201cexcellence rates\u201d. In practical applications, this external knowledge can be supplied directly by the users to enhance query understanding.\nWhen dealing with extensive datasets and notably large tables that may exceed the input limitations of open-source LLMs as described in Listing 1  ###reference_###, we propose a solution called Target Column Truncation. This approach involves dividing columns  in the database  into two subsets:  (containing crucial query columns) and . By randomly deleting a controlled number of columns in , this ensures the question representation remains within the input limits of open-source LLMs.\nMoreover, to address the schema linking problem, which involves identifying references to database schema elements and condition values in natural language queries, enhancing generalizability and supporting complex query synthesis Lei et al. (2020  ###reference_b11###) (identified as the second significant failure detailed in Figure 1  ###reference_###), we introduce two Chain-of-Thought (COT) templates for schema linking in this paper. The initial simple template (COT-SP), explained in Listing 3  ###reference_### (Appendix A  ###reference_###), takes a direct approach, guiding the Language Model (LLM) to explore related tables and columns before formulating the complete SQL script. The second template, the skeleton-based Chain-of-Thought (COT-SK), detailed in Listing 4  ###reference_### (Appendix A  ###reference_###), refines COT-SP by introducing a skeleton generation phase before the actual SQL script generation. This \u201cskeleton\u201d serves as the basic query framework without specific values or conditions. In our methodology, we preserve SQL keywords in their original form, replacing other components with a designated placeholder symbol, typically represented as \u201c_\u201d. For instance, the SQL query \u201cSELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1\u201d corresponds to the skeleton \u201cSELECT _ FROM _ WHERE _ ORDER BY _ DESC LIMIT _\u201d."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Supervised Fine-Tuning for Text-to-SQL",
            "text": "In the general domain, each entry in the supervised data set  comprises a prompted input  with the natural language question  and a golden query on database . The prompted input is constructed by applying a transformation  to the composition . Subsequently, LoRA Hu et al. (2021  ###reference_b9###) is employed for fine-tuning the given LLMs. Following the fine-tuning process, the optimized language model  becomes capable of performing inference tasks. In this context, inference involves prompting  with natural language questions to generate corresponding queries. It\u2019s noteworthy that the same question representation  and prompt  are employed consistently throughout both the fine-tuning and inference phases."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Few-shot Learning for Text-to-SQL",
            "text": "We leverage few-shot learning to enhance the Text-to-SQL performance of open-source LLMs by incorporating a limited number of examples in the input prompts. This method allows for more effective training, improving the overall system performance. Suppose we have an additional sample set  with the same format as . We aim to select a set of  triples  to maximize the probability of LLMs  generating the correct SQL  for the target question  and database . When , few-shot learning reverts to zero-shot learning. This study focuses on cross-domain Text-to-SQL, where the target database  is distinct from those encompassed in , i.e.,  holds for all .\nIn this research, we introduce a novel methodology named Open Example Curation, outlined in Listing 5  ###reference_###(Appendix B  ###reference_###), as a systematic approach for the selection and organization of . When it comes to example selection, the conventional strategy involves presenting both example questions and their corresponding SQL queries while excluding the database schema to manage token costs. However, this exclusion of the database schema may have a detrimental impact on performance. To address this concern, we propose presenting example questions, corresponding database schema, and SQL queries together. For a given question  with database  and each -th training example , initial step involves computing the question similarity between  and , the database similarity between  and , and the SQL similarity between  and , where  returns the embedding of the input by  and cosine similarity is employed. Subsequently, we compute the average similarity . Finally, we select  examples with the highest average similarities from  to form .\nIn the domain of example organization, diverse strategies are employed. Some maintain both the question-SQL mapping and the database schema, while others prioritize the former, excluding the token-costly database schema. To efficiently preserve both mappings simultaneously, we propose the Example Column Truncation strategy. This method selectively removes unnecessary columns in the example database based on the dissimilarity of a table to the target database. Let  denote the similarity of the question  with itself, and  represent the similarities of question  with the selected  examples in . Truncation rates, , are calculated using the softmax function with a temperature parameter  (typically set to ) applied to . It is crucial to note that adjusting  within the softmax function enables fine-tuning of the distribution of truncation rates. Columns can then be removed according to these truncation rates. Ultimately, these truncated examples can be ordered in ascending order based on their similarities to the target example and subsequently integrated into the prompt for in-context learning."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "In this section, we first introduce our experimental settings. Then we conduct extensive comparisons with existing solutions in the original LLM models, supervised fine-tuned LLM models without Chain-of-Thought and supervised fine-tuned LLM models with Chain-of-Thought respectively. After that, we further compare them in terms of token efficiency to inspire more efficient solutions."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Setting",
            "text": "Dataset: Our evaluation utilizes the BIRD dataset Li et al. (2023c  ###reference_b14###), as detailed in Section 3  ###reference_###. Specifically, we employ the development split, referred to as BIRD-dev, since the test split has not been publicly released. In cases involving few-shot scenarios, we make use of the training split of BIRD to provide example candidates for evaluation.\n\nImplementation details: In exploring the capabilities of open-source LLMs, we selected Llama 2-7B Touvron et al. (2023b  ###reference_b28###) and Code Llama-7B Touvron et al. (2023b  ###reference_b28###), both equipped with 7 billion parameters. Our experiments were conducted on a NVIDIA A100 80 G GPU for consistent comparison across all methods. To address memory limitations in supervised fine-tuning, we set the maximal context length to 2048 for both Llama 2 and Code Llama. During evaluation, we allocated tokens for response generation, and, by default, we set the argument temperature to eliminate the influence of randomness. For post-processing, we adhered to established practices, extracting the first SQL query in the response and removing any additional output."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "In-Context Learning without Fine-tuning",
            "text": "In this subsection, we conduct a comprehensive evaluation of both closed-source LLMs and open-source LLMs, specifically employing GPT-3.5-TURBO in conjunction with two distinct open-source Language Models: Llama 2 and Code Llama. This evaluation encompasses a variety of question representation, example selection, and organization strategies.\n\nComparative Analysis with Question Representations.\nNote that the table definition becomes more informative with the inclusion of optional column elements. We compare nine types of column definitions, each incorporating various optional column elements: with none of the optional column elements; with TYPE; with DESCRIPTION; with VALUES; with PRIMARY_KEY; with DESCRIPTION and VALUES; with DESCRIPTION, VALUES, and TYPE; with DESCRIPTION, VALUES, and PRIMARY_KEY; with TYPE, DESCRIPTION, VALUES, and PRIMARY_KEY. The detailed definitions of these nine types of column definitions are provided in Appendix C.\n\nIn Figure 2, we present a comparative analysis of GPT-3.5-TURBO, Llama2, and Code Llama across various table schemas on BIRD-dev. Examining GPT-3.5-TURBO\u2019s performance in various column definitions (Figure 2(a)), we note that incorporating column values has the most substantial positive impact, and adding column descriptions also contributes to performance improvement. Conversely, including data type and primary key appears to have a negative effect, particularly when introducing only one optional column. The combined inclusion of both column values and descriptions leads to further performance improvement, and the introduction of column type and primary key enhances performance, with the former contributing the most significant improvement.\n\nSimilar trends are evident for Llama2 and Code Llama. Given the notably subpar performance of Llama2 and Code Llama, the insights gained from GPT-3.5-TURBO\u2019s results are more meaningful and can inform enhancements for Llama2 and Code Llama. In summary, optimal performance is attained by which introduces column values, descriptions, and types for enhanced results, yielding an improvement from 37.72% to 44.2% for GPT-3.5-TURBO. However, which only introduces column values for token-saving purposes results in a slight decrease, reaching 42.7% for GPT-3.5-TURBO. Hence, the choice between and depends on specific purposes.\n\nComparative Analysis with COT.\nIn Figure 3, we provide a comprehensive comparison of zero-shot outcomes by Llama2 and Code Llama across five methods on BIRD-dev. In summary, the introduction of COT results in performance deterioration, highlighting a challenge for open-source LLMs in executing COT for Text-to-SQL tasks. Notably, \u201cCOT-SP-FULL\u201d and \u201cCOT-SK-FULL\u201d outperform \u201cCOT-SP-PRED\u201d and \u201cCOT-SK-PRED\u201d, suggesting that inaccuracies in predicting tables and columns may negatively impact SQL generation.\n\nComparative Analysis of Few-shot Learning.\nIn our experimentation to assess the few-shot learning capabilities of open-source LLMs such as Llama2 and Code Llama, detailed in Tables 1 and 2, examples were chosen using the column definition through different methods: Random selection, Q (Question) similarity, Q+S (Question and Schema) similarity, DAIL-SQL Gao et al. (2023) (Question and pre-generated SQL), and our proposed Open Example Curation method. Notably, our proposed approach surpassed other example selection methods, showcasing superior performance. Additionally, we observed a positive impact on performance with an increase in the number of examples. Specifically, our method demonstrated a substantial improvement of approximately with the integration of one example into Llama2 and with one example into Code Llama. However, introducing three examples led to a decline in performance for Llama2 but a slight increase for Code Llama."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Supervised Fine-tuning without COT",
            "text": "In this subsection, our exploration centers on supervised fine-tuning in Text-to-SQL, specifically without COT.\n\nComparative Analysis with Question Representations.\n\n###figure_8### ###figure_9### Figure 4  ###reference_### present a comparative analysis of the results obtained by Llama2 and Code Llama after supervised fine-tuning across various table schemas on BIRD-dev. Notably, our initial observations reveal performance improvements through supervised fine-tuning for both Llama2 and Code Llama, irrespective of the table schema used.\n\nThe outcomes depicted in Figures 4(a)  ###reference_sf1### and 4(b)  ###reference_sf2### align with the trends observed in Figures 2(b)  ###reference_sf2### and 2(c)  ###reference_sf3###, consistently indicating that supervised fine-tuned Code Llama consistently outperforms fine-tuned Llama2 in Text-to-SQL tasks. Notably, the inclusion of column description emerges as the most impactful improvement, particularly when considering individual column elements. However, the most substantial positive impact is realized when introducing column values, descriptions, and types together, as corroborated by the findings in Figure 2(a)  ###reference_sf1###. This observation underscores the notion that the performance of both fine-tuned Code Llama and Llama2 closely mirrors that of GPT-3.5-TURBO, showcasing heightened proficiency in Text-to-SQL tasks.\n\nComparative Analysis of Few-shot Learning.\n\nWe conducted additional experiments to assess the few-shot learning capabilities of Llama2 and Code Llama after supervised fine-tuning, using examples from BIRD-DEV with the  column definition (Tables 3  ###reference_### and 4  ###reference_###). In contrast to prior results (Tables 1  ###reference_### and 2  ###reference_###), both models exhibited a decline in performance with additional examples. This might be attributed to the models becoming overly focused on the zero-shot prompt, hindering their understanding of test prompts with extra examples. Addressing this issue is crucial for future work to enhance the few-shot capabilities of open-source LLMs after supervised fine-tuning."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Supervised Fine-tuning with COT",
            "text": "Within this subsection, our exploration centers on assessing the potential for enhancing supervised fine-tuning performance in open-source LLMs through the incorporation of COT.\n\nComparative Analysis of COT.\n\nIn Figure 5, ###reference_###, we provide a comprehensive comparison of zero-shot outcomes by Llama2 and Code Llama across five methods on BIRD-dev, where the prefix \u201cSFTI-\u201d denotes supervised fine-tuning and inference. Both \u201cSFTI-COT-SP-FULL\u201d and \u201cSFTI-COT-SK-FULL\u201d outperform \u201cSFTI-COT-SP-PRED\u201d and \u201cSFTI-COT-SK-PRED\u201d, indicating that introducing the target database schema improves step-by-step inference performance. In contrast to the results in Figure 3, ###reference_###, \u201cSFTI-COT-SK-FULL\u201d surpasses \u201cSFTI-COT-SP-FULL\u201d, revealing the emergence of skeleton generation capability in Llama2 and Code Llama after fine-tuning. Compared to the results in Figure 4, ###reference_###, \u201cSFTI-COT-SK-FULL\u201d boosts Llama2 from 37.42% to 41.04% and Code Llama from 45.39% to 48.24%.\n\nComparative Analysis of Few-shot Learning.\n\nWe conducted experiments to evaluate the few-shot learning performance of open-source LLMs using the \u201cSFTI-COT-SK-FULL\u201d approach with the column definition, specifically focusing on Llama2 and Code Llama (Tables 5 ###reference_### and 6 ###reference_###). Our proposed approach outperformed other example selection methods, showcasing superior performance, and increasing the number of examples resulted in improved performance. However, when comparing these results with those in Figures 2(b) ###reference_sf2### and 2(c) ###reference_sf3###, a noticeable performance degradation is evident, possibly due to the limited proficiency of both the original Llama2 and Code Llama in Text-to-SQL tasks.\n\nError Analysis.\n\nTables 7 ###reference_### and 8 ###reference_### exhibit significant reductions in various error categories for both Llama2 and Code Llama following fine-tuning. Particularly, errors such as \u201csyntax error\u201d and the majority of \u201ccolumn not exist\u201d issues have been substantially mitigated post fine-tuning, suggesting an improved ability to avoid generating SQL statements with obvious syntax errors or referencing non-existing tables and columns. However, challenges persist in selecting appropriate tables and columns, as well as in generating accurate SQL statements for both models."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Token Efficiency",
            "text": "Tables 9 and 10 display the outcomes of column truncation carried out to conform to the token length limitation of less than 2048, established to meet memory constraints. It is important to note that we input 2048 tokens during the inference stage after fine-tuning for alignment purposes. The results in both tables highlight the significant roles played by our proposed Target Column Truncation and Example Column Truncation in ensuring the success of supervised fine-tuning and few-shot learning for open-source LLMs in Text-to-SQL tasks. Particularly in few-shot learning, the introduction of three examples results in 100% column truncations for all queries."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Time costs",
            "text": "Table 11 offers a comparative analysis of the time costs associated with six models. Initially, for Code Llama, the inference process required approximately 3.4 seconds. Following supervised fine-tuning, excluding COT, Code Llama\u2019s inference time decreased by about 0.6 seconds. This reduction underscores the model\u2019s enhanced proficiency in the Text-to-SQL task post-fine-tuning, resulting in a reduction in computational effort and inference time. However, upon further integration of COT, consisting of three distinct sub-tasks, Code Llama\u2019s inference time increased to around 6 seconds. Consequently, while COT may enhance performance, it leads to a doubling in inference time."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "This paper systematically evaluates open-source LLMs in Text-to-SQL tasks, introducing the Open Prompt strategy for effective question representation and advocating for supervised fine-tuning. Noteworthy contributions include exploring Chain-of-Thought benefits in step-by-step inference, proposing Open Example Curation for enhanced few-shot learning, and introducing token-efficient techniques (Variable-length Open DB Schema, Target Column Truncation, and Example Column Truncation) to address large-scale database challenges.\n\nOur methodology brings about significant enhancements for both Llama2 and Code Llama, resulting in substantial performance improvements. Findings indicate that Code Llama, fine-tuned with a modest 7 billion parameters, surpasses ChatGPT in performance and is on par with GPT-4.\n\nLimitations: Our experimental results underscore the challenges in maintaining the effective learning capabilities of open-source LLMs from contextual examples after fine-tuning, corroborating findings in Gao et al. (2023). Furthermore, there are difficulties in correctly identifying tables for JOIN operations and generating intricate WHERE statements. These findings underscore the critical necessity for future studies to delve into schema linking and explore methods for preserving in-context learning abilities after fine-tuning."
        }
    ],
    "url": "http://arxiv.org/html/2405.06674v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.6"
        ]
    },
    "research_context": {
        "paper_id": "2405.06674v1",
        "paper_title": "Open-SQL Framework: Enhancing Text-to-SQL on Open-source Large Language Models",
        "research_background": "### Motivation\nThe primary motivation for this paper is to address the challenge non-technical users face when querying relational data using SQL. While SQL is efficient for professionals, it poses a barrier to those without technical expertise. Text-to-SQL parsing, which translates natural language queries into SQL, has become a crucial area of interest as it allows easier access to data for non-expert users and plays a significant role in applications like customer service and question answering.\n\n### Research Problem\nThe research problem focuses on leveraging open-source Large Language Models (LLMs) to enhance Text-to-SQL tasks. Unlike proprietary models from OpenAI, open-source LLMs, despite their transparency and affordability, often underperform in context understanding and response coherence. The paper aims to improve the performance of these open-source models in Text-to-SQL through supervised fine-tuning and in-context learning strategies.\n\n### Relevant Prior Work\nSeveral prior studies have been identified as foundational for this research:\n- **Question-to-SQL Pattern Extraction**: Previous works like those by Wang et al. (2020) and Li et al. (2023b) mainly deal with training encoder-decoder models using Text-to-SQL corpora to identify patterns for generating SQL queries from natural language questions.\n- **Prompt Engineering**: Liu et al. (2023a) highlight the emergence of LLMs as a new paradigm for Text-to-SQL, emphasizing that the main challenge lies in prompting the models accurately. Other researchers, including Pourreza and Rafiei (2023), Nan et al. (2023), and Guo et al. (2023), have contributed to this domain by exploring question representations, example selection, and example organization strategies.\n- **Open-source LLMs**: The increasing popularity of open-source LLMs in text and programming tasks has been noted due to their benefits over proprietary models in terms of accessibility and community support. However, there is a performance gap between open-source and proprietary LLMs, which this paper seeks to address.\n\n### Contributions and Results\nThe paper introduces \"Open-SQL,\" a systematic methodology designed to enhance Text-to-SQL tasks using open-source LLMs. Key contributions and results include:\n- **Systematic Evaluation**: A thorough assessment revealing performance deficiencies in these models when tasks involve understanding database schemas.\n- **Effective Strategies**: Introduction of new strategies such as Open Prompt for question representation and novel Chain-of-Thought (COT) methods, marking the first exploration of COT in Text-to-SQL tasks.\n- **Token Efficiency Techniques**: Techniques like Variable-length Open DB Schema, Target Column Truncation, and Example Column Truncation to manage memory constraints during fine-tuning.\n- **Significant Performance Gains**: Supervised fine-tuning and other strategies leading to substantial performance improvements, with experimental evidence showing notable enhancements in models like Llama2-7B and Code Llama-7B.\n- **Open-Source Availability**: The open-sourcing of code and models to encourage broader research and innovation in the Text-to-SQL community.\n\nIn summary, the paper aims to bridge the performance gap between open-source and proprietary LLMs in Text-to-SQL tasks through innovative prompt engineering, systematic evaluation, and strategic model enhancements.",
        "methodology": "**Open-SQL Framework: Enhancing Text-to-SQL on Open-source Large Language Models**\n\n**Methodology:**\nIn this section, we introduce **Open-SQL**, a systematic method designed to boost the performance of open-source large language models (LLMs) in Text-to-SQL tasks. Our primary aim is to enable LLMs to generate accurate SQL statements in response to SQL questions expressed in natural language, within the context of a defined database.\n\nKey components of the **Open-SQL** method are:\n\n1. **Question Representation Function ():**\n   - The function is designed to enhance the representation of the SQL question by incorporating pertinent details, particularly the database schema. This improved representation aims to make the question clearer and more comprehensive for the LLM.\n\n2. **Prompt Function ():**\n   - This function is tailored to provide a specific prompt for the SQL question. Additionally, there is some extra information () that will be further explained in Section 4.3 (###reference_###). The purpose of this function is to effectively prime the language model to generate the correct SQL.\n\n3. **Generation Function ():**\n   - The final SQL statement is generated through the application of this function.\n\nTo maximize the efficacy of this approach, the **Open-SQL** framework employs a **synergistic combination** of the following techniques:\n\n- **Question Representation:**\n  - This involves optimizing the function to enhance the initial understanding of the SQL question by the model.\n\n- **Supervised Fine-Tuning:**\n  - This process aims to optimize the function by using a dataset of correct SQL statements derived from corresponding natural language questions. This fine-tuning helps the model learn the correct mappings from questions to SQL statements.\n\n- **In-Context Learning:**\n  - During the inference stage, this method optimizes by using previously seen question-SQL pairs to guide the model in generating the correct SQL statement.\n\nThe overarching goal is to maximize the probability () of the language model (LLM) generating the correct SQL () given the input natural language question (). The strategy to achieve this involves a **step-by-step** approach:\n\n1. Enhancing question representation to optimize ().\n2. Using supervised fine-tuning to optimize ().\n3. Applying in-context learning during the inference stage to optimize ().\n\nThis integrated strategy aims to improve the model\u2019s capability to handle SQL queries accurately and efficiently.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets:\nThe main experiment is conducted on several widely recognized and challenging benchmarks in the Text-to-SQL domain, which may include:\n- **Spider**: A complex and cross-domain Text-to-SQL benchmark.\n- **WikiSQL**: A simpler yet large-scale dataset for evaluating the SQL generation.\n- **Other relevant datasets**: Any additional datasets that are used to validate the generalizability and robustness of the proposed framework.\n\n#### Baselines:\nThe performance of the proposed Open-SQL framework is compared against:\n- **Original LLM models**: Large Language Models evaluated without any additional fine-tuning or modifications.\n- **Supervised fine-tuned LLM models without Chain-of-Thought**: Models that have undergone fine-tuning using the provided training data but do not incorporate the Chain-of-Thought reasoning technique.\n- **Supervised fine-tuned LLM models with Chain-of-Thought**: Models that have been fine-tuned and incorporate Chain-of-Thought techniques to enhance logical reasoning during SQL generation.\n\n#### Evaluation Metrics:\nThe evaluation metrics employed to assess the performance of the models include:\n- **Execution Accuracy**: The fraction of SQL queries executed correctly against the database.\n- **Exact Set Match**: The percentage of generated SQL queries that exactly match the ground truth queries.\n- **Token Efficiency**: Evaluation of the number of tokens used by the models to generate SQL queries, which reflects the computational efficiency and potential speed of the model.\n\n### Main Experimental Results\n\n- **Performance Comparison**:\n  The Open-SQL framework significantly outperforms the original LLM models, supervised fine-tuned LLM models without Chain-of-Thought, and supervised fine-tuned LLM models with Chain-of-Thought across all datasets.\n  - On **Spider**, the proposed framework achieves the highest execution accuracy and exact set match scores, demonstrating its robustness on a complex cross-domain benchmark.\n  - On **WikiSQL**, it also shows superior performance, indicating its ability to handle simpler yet large-scale datasets efficiently.\n\n- **Token Efficiency**:\n  The Open-SQL framework demonstrates improved token efficiency compared to alternative models. This means that it can generate accurate SQL queries using fewer tokens, leading to faster generation times and potentially lower computational costs.\n\nThese results suggest that the Open-SQL framework not only enhances the accuracy of SQL query generation but also offers more efficient solutions in terms of both accuracy and computational resources."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of various table schemas, including different column definitions, on the performance of GPT-3.5-TURBO, Llama2, and Code Llama in Text-to-SQL tasks without fine-tuning.",
            "experiment_process": "The study employed GPT-3.5-TURBO along with two open-source LLMs, Llama 2 and Code Llama, evaluating them across different table schemas on the BIRD-dev dataset. Nine types of column definitions were tested, including optional elements such as TYPE, DESCRIPTION, VALUES, and PRIMARY_KEY. Each combination's definitions were detailed in the appendix, and performance across these definitions was compared.",
            "result_discussion": "Incorporating column values had the most substantial positive impact, followed by adding column descriptions. Including data type and primary key generally had a negative effect, especially when included alone. Optimal performance was achieved by introducing column values, descriptions, and types, boosting GPT-3.5-TURBO from 37.72% to 44.2%. Similar trends were observed for Llama2 and Code Llama, with the findings suggesting potential areas of improvement for these models based on insights from GPT-3.5-TURBO.",
            "ablation_id": "2405.06674v1.No1"
        },
        {
            "research_objective": "To assess the efficacy of few-shot learning techniques on the performance of open-source LLMs like Llama2 and Code Llama.",
            "experiment_process": "Examples were chosen employing five different methods, including Random selection, Q similarity, Q+S similarity, DAIL-SQL, and the proposed Open Example Curation method. Performance was evaluated using the BIRD-DEV dataset, focusing on Llama2 and Code Llama with the column definition.",
            "result_discussion": "The Open Example Curation method outperformed other selection methods, showing superior performance. Increasing the number of examples generally improved performance, with a substantial improvement noted with one example. However, including three examples led to a decline in performance for Llama2 but a slight increase for Code Llama.",
            "ablation_id": "2405.06674v1.No2"
        },
        {
            "research_objective": "To determine the effect of supervised fine-tuning on Llama2 and Code Llama for Text-to-SQL tasks, specifically when Chain-of-Thought (COT) is excluded.",
            "experiment_process": "Different table schemas were employed on BIRD-dev to fine-tune Llama2 and Code Llama, followed by comparative analysis. Various question representation strategies were examined to identify the most impactful improvements. Few-shot learning capabilities were also assessed post-fine-tuning.",
            "result_discussion": "Supervised fine-tuning led to performance improvements across different table schemas for both Code Llama and Llama2. The inclusion of column description had a significant impact, especially when combined with column values and types. Fine-tuned Code Llama consistently outperformed fine-tuned Llama2. However, after fine-tuning, additional examples in few-shot learning led to a decline in performance, suggesting the model's preference for zero-shot prompts.",
            "ablation_id": "2405.06674v1.No3"
        },
        {
            "research_objective": "To explore the potential improvements by integrating Chain-of-Thought (COT) in supervised fine-tuning for open-source LLMs like Llama2 and Code Llama.",
            "experiment_process": "Five methods were compared using zero-shot and few-shot learning techniques on BIRD-dev. The 'SFTI-COT-SK-FULL' approach was specifically tested for its efficacy.",
            "result_discussion": "The 'SFTI-COT-SK-FULL' method significantly improved performance, boosting Llama2 from 37.42% to 41.04% and Code Llama from 45.39% to 48.24%. Error analysis revealed substantial reductions in various error categories post-fine-tuning, especially in syntax errors and non-existent column references. However, issues in selecting appropriate tables and columns persisted.",
            "ablation_id": "2405.06674v1.No4"
        },
        {
            "research_objective": "To assess the role of token-efficient techniques in ensuring the effectiveness of supervised fine-tuning and few-shot learning for open-source LLMs in Text-to-SQL tasks.",
            "experiment_process": "Token constraints were set at less than 2048 tokens. Techniques like Target Column Truncation and Example Column Truncation were employed during inference after fine-tuning.",
            "result_discussion": "Both truncation methods played crucial roles in enhancing supervised fine-tuning and few-shot learning. Particularly, few-shot learning involving three examples resulted in 100% column truncations, emphasizing the importance of these techniques in managing large-scale databases.",
            "ablation_id": "2405.06674v1.No5"
        }
    ]
}