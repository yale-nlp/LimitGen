{
    "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
    "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs\u2019 general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs\u2019 original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs\u2019 performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across nine question-answering tasks demonstrate the superiority of our approach. The virtual tokens and training code are publicly available at https://github.com/DaoD/SPRING/.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have achieved remarkable performance across various natural language processing tasks. Despite their extensive parameters enabling them to learn rich knowledge during pre-training, LLMs may still generate hallucinated, outdated, or inaccurate content, especially in scenarios requiring long-tail knowledge.\n\nTo address this problem, retrieval-augmented generation (RAG) has emerged as a pivotal strategy. By explicitly decoupling knowledge retrieval from the backbone LLMs, such architectures have achieved more accurate and reliable content generation and shown particularly enhanced performance on knowledge-intensive tasks such as open-domain question answering.\n\nExisting efforts in RAG development can be roughly categorized into two groups. The first group leverages the in-context learning capabilities of LLMs by incorporating retrieved information into the input along with appropriate prompts. This allows for straightforward application to any off-the-shelf LLM without tuning its parameters. However, its effectiveness largely depends on the human experience in crafting effective prompts and the LLM\u2019s ability to interpret these prompts. The second group focuses on training LLMs to enhance their performance in RAG scenarios. This training might involve either end-to-end pre-training or fine-tuning for specific tasks. These approaches can often lead to better performance, but they require significant computational resources. Recently, parameter-efficient fine-tuning techniques, such as LoRA, have been widely studied, significantly reducing training costs. These methods can optimize the LLMs\u2019 parameters for RAG, but unfortunately compromise the model\u2019s general generation abilities in scenarios without retrieval. This limitation prevents their application to LLMs already operational in real-world settings.\n\nTherefore, a critical research problem arises: Is it possible to enhance LLMs\u2019 performance under RAG scenarios while preserving their general generation capabilities? To achieve this, we introduce a novel, lightweight tuning method named SPRING, which learns Scalable and Pluggable virtual tokens for retrieval-augmented Generation. Our basic idea is to add trainable virtual tokens to help LLMs learn RAG problems. Through fine-tuning, these virtual tokens effectively enhance the LLM\u2019s capability to understand retrieved information and its correlation with user inputs. Importantly, as the LLM\u2019s original parameters are frozen, its general generation abilities are preserved without any loss. During inference, when retrieval is triggered, these trained virtual tokens can be simply added to the prompt, which includes both the retrieved results and user input, thereby significantly enhancing performance. Moreover, we employ a scalable training approach, allowing the number of virtual tokens to be adjusted according to the needs of the inference scenario. Various training strategies have been implemented to further improve the generalizability of our method, ensuring robustness regardless of the number of the retrieved results.\n\nIn experiments, SPRING is trained with Mistral-7b, LLaMA-2-7b, and LLaMA-2-13b on nine commonly used QA datasets and evaluated in both in-domain and out-of-domain tasks. The experimental results demonstrate that SPRING not only effectively improves the RAG performance of LLMs but also successfully preserves their general generation capabilities. Overall, the SPRING method exhibits four main characteristics:\n\nLightweight yet effective. Instead of updating the full parameters of the LLMs, we opt to freeze the pre-trained models and only learn the embeddings for the added virtual tokens. For example, adding 50 tokens to the Mistral-7b model introduces only 0.2M parameters in total.\n\nScalable. With our proposed scalable training approach, SPRING can be effective with any number of virtual tokens. Remarkably, even just one token can substantially improve the LLMs\u2019 performance in RAG scenarios.\n\nPluggable. Owing to its lightweight design, SPRING can be applied in a plug-and-play manner. When retrieval is triggered, simply adding the virtual tokens can lead to better performance. In non-RAG scenarios, the virtual tokens are not added so the LLMs\u2019 original capabilities can be well preserved. This characteristic is crucial for LLMs that have already been deployed for practical use.\n\nGeneralizable. Our robust training strategies ensure that SPRING is adaptable to different retrievers and various numbers of retrieved results. Consequently, there is no need to retrain SPRING with each update to the retrieval system, enhancing its practicality and efficiency."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Compared to standard text generation, RAG incorporates a retrieval module that accesses external knowledge to enhance generation quality Lewis et al. (2020b  ###reference_b28###); Guu et al. (2020  ###reference_b12###); Zhu et al. (2023  ###reference_b62###). The mainstream RAG follows a \u201cretrieve-then-read\u201d paradigm, where the retrieval module provides external knowledge as additional context, which is then read by generation models to produce the final output Izacard et al. (2023  ###reference_b20###); Shi et al. (2023  ###reference_b49###); Ram et al. (2023  ###reference_b45###); Borgeaud et al. (2022  ###reference_b2###); Lin et al. (2023  ###reference_b30###); Zhu et al. (2024  ###reference_b63###). To optimize the use of external knowledge, some methods focus on crafting effective prompts that guide the utilization of retrieved information Shi et al. (2023  ###reference_b49###); Ram et al. (2023  ###reference_b45###). These prompt-based methods are applicable to any LLM without tuning its parameters. However, they depend heavily on skillful prompt writing and the LLMs\u2019 ability to understand instructions. In contrast, other studies attempts to directly train the model to better use the retrieved knowledge. For example, REALM Guu et al. (2020  ###reference_b12###) and RETRO Borgeaud et al. (2022  ###reference_b2###) incorporate retrieval in end-to-end retrieval-augmented pre-training. RA-DIT Lin et al. (2023  ###reference_b30###) employs fine-tuning to enhance LLMs\u2019 retrieval understanding. These tuning-based methods often yield better performance than prompt-based methods by optimizing model parameters for RAG. However, they may compromise the LLMs\u2019 general capabilities, particularly in non-retrieval scenarios. Different from existing methods, we design a new lightweight tuning method for RAG. It is a plug-and-play module that enhances RAG performance using trainable virtual tokens, which can be removed in non-RAG scenarios to preserve the LLMs\u2019 general generation abilities.\nThe paradigms of \u201cpre-training then fine-tuning\u201d have demonstrated efficacy across various natural language Devlin et al. (2019  ###reference_b9###); Raffel et al. (2020  ###reference_b43###); Lewis et al. (2020a  ###reference_b27###); Radford et al. (2019  ###reference_b42###) and vision tasks He et al. (2020  ###reference_b13###); Dosovitskiy et al. (2021  ###reference_b10###); Chen et al. (2020  ###reference_b4###). The common fine-tuning process involves tuning all parameters of a model, which is computational intensive, especially for LLMs. To address this, PEFT Mangrulkar et al. (2022  ###reference_b36###) approaches have been developed. These approaches freeze most of the pre-trained models\u2019 parameters, yet still manage to achieve comparable performance on downstream tasks. PEFT has been widely studied Wan et al. (2023  ###reference_b53###), and typical methods including adapter-based tuning Houlsby et al. (2019  ###reference_b16###); Lin et al. (2020  ###reference_b31###); Rebuffi et al. (2017  ###reference_b46###); Chen et al. (2023  ###reference_b5###), low-rank adaptation (LoRA) Hu et al. (2022  ###reference_b17###); Dettmers et al. (2023  ###reference_b8###), and prompt tuning Li and Liang (2021  ###reference_b29###); Lester et al. (2021  ###reference_b26###); Liu et al. (2021b  ###reference_b34###, a  ###reference_b33###); Qin and Eisner (2021  ###reference_b41###). Adapter-based tuning inserts lightweight modules into a model\u2019s existing layers and have been extended to various domains Gao et al. (2024  ###reference_b11###); Hu et al. (2023  ###reference_b18###); Zhang et al. (2023a  ###reference_b60###). LoRA Hu et al. (2022  ###reference_b17###) introduces trainable low-rank matrices that adjust the model\u2019s weight updates, achieving promising fine-tuning performance on LLMs Hu et al. (2023  ###reference_b18###). Prompt tuning incorporates a series of trainable prompt tokens to LLMs. These tokens can be inserted either to the input layer only Lester et al. (2021  ###reference_b26###); Liu et al. (2021b  ###reference_b34###) or to all of the intermediate layers Li and Liang (2021  ###reference_b29###); Liu et al. (2021a  ###reference_b33###). In this paper, we proposes a novel prompt tuning method, SPRING, specifically designed for RAG scenarios. Our method introduces virtual tokens between retrieved results and the input, exploiting the auto-regressive generation paradigm to improve the model\u2019s ability to utilize retrieved information. Additionally, it is designed to be scalable and pluggable, thus broadening its application scope while preserving the original generative capabilities of LLMs.\n###figure_2###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To take advantage of both the flexibility of prompt-based methods and the efficacy of fine-tuning-based methods, we propose SPRING to learn scalable and pluggable virtual tokens for retrieval-augmented generation (RAG). In practical developments, LLMs are often constrained by their maximum input lengths, limiting the number of tokens available for retrieval augmentation (especially when the retrieved results are very long). Therefore, it is desired to design a mechanism so that any number of virtual tokens can be used in the inference to improve RAG performance. To achieve this, we propose an optimization strategy working like a \u201cspring.\u201d Specifically, for a given sample with the total number of added tokens, we randomly select a number and utilize the first virtual tokens to construct the training example. This method allows for the flexible optimization of any number of virtual tokens. Consequently, the number of virtual tokens incorporated during inference can be dynamically adjusted based on the requirements of the application. \n\nDue to its designed structure, our method provides considerable flexibility in application. Practically, if user input is assessed to require external knowledge, our virtual tokens can be simply appended after the retrieval results and then fed, along with the user input, into the LLM for generation. In contrast, if the user input does not necessitate retrieval, it can be processed directly by the LLM. As our approach does not adjust the original parameters of the LLM, it preserves the model\u2019s inherent capabilities. This feature is particularly important for industry or business since existing LLMs may have already been deployed for multiple purposes; our method enhances the retrieval understanding capabilities of these models without compromising their existing functionalities.\n\nWe illustrate the instructions for using our SPRING. After training, the embeddings of the added tokens have been optimized for RAG, but these tokens do not correspond to any existing tokens in the vocabulary. To make them easy to use, we can add some special tokens (e.g., [r1], ..., [r50]) to the vocabulary and initialize their embeddings with the trained embeddings. Then, during inference, after obtaining the retrieved results, we can add any number of these special tokens (e.g., [r1] ... [rk]) after and input them with the question to the LLMs for generation. We also provide an example code snippet in Appendix A for using our method in practice.\n\nWe refer to our method as SPRING due to its scalable and pluggable nature, making it particularly well-suited for enhancing existing LLMs that are already deployed. Additionally, it effectively bridges the gap between retrieved results and user input, significantly improving the LLMs\u2019 capabilities in understanding the retrieved external knowledge."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Problem Formulation",
            "text": "Language models are designed to calculate the probability distribution over sequences of natural language texts. Auto-regressive models are commonly used for this through next-token prediction:\nwhere  denotes the sequence of tokens preceding  at each step, and  represents the parameters of the model. For RAG, a retrieval corpus  and a retriever  are introduced. Then, the generation process is conditioned on both  and the retrieved results  as:\nNote that here  serves as the query for retrieval. In question-answering (QA) tasks,  is usually the question , and the learning objective is to generate the right answer . The retriever can yield multiple passages, which can be concatenated as a long text sequence using proper separator such as \u201cnn\u201d. For brevity, this formulation directly concatenates the retrieved results  with the question , omitting more complex prompt designs. Henceforth, we will use the notations in QA tasks as our evaluation is performed on them."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "SPRING: Scalable and Pluggable Virtual Tokens for RAG",
            "text": "Our SPRING method, shown in Figure 2, introduces trainable virtual tokens into the input to optimize LLMs for RAG scenarios. Specifically, following the notation in Equation (3), we add trainable tokens between the retrieved results and the input. In our implementation, we set for training. The generation process can then be described as: where represents the added parameters of the trainable tokens (i.e., their embeddings), and is the embedding size of the LLM. denotes the parameters of the backbone LLM, which are frozen during training. Given that , our method is highly efficient for training. For example, with the Mistral-7b model (where ), when tokens are added, we only add and train parameters, approximately 0.003% of the full model. Importantly, we place the virtual tokens between the retrieved results and the question for two main reasons: (1) In the auto-regressive generation paradigm, positioning the tokens after the retrieved results allows them to attend to this information, thereby aiding the model\u2019s comprehension. (2) Recent studies have indicated that LLMs are particularly sensitive to the end of an input. By consistently placing these virtual tokens before the question across all test samples, we aim to mitigate any potential adverse effects on the understanding of the question. In practical developments, LLMs are often constrained by their maximum input lengths, limiting the number of tokens available for retrieval augmentation (especially when the retrieved results are very long). Therefore, it is desired to design a mechanism so that any number of virtual tokens can be used in the inference to improve RAG performance. To achieve this, we propose an optimization strategy working like a \u201cspring\u201d (as shown in Figure 2). Specifically, for a given sample with the total number of added tokens, we randomly select a number and utilize the first virtual tokens to construct the training example as. This method allows for the flexible optimization of any number of virtual tokens. Consequently, the number of virtual tokens incorporated during inference can be dynamically adjusted based on the requirements of the application. The effectiveness of this strategy and its comparison with other methods are further discussed in Section 4.4.2. Due to its designed structure, our method provides considerable flexibility in application. Practically, if user input is assessed to require external knowledge, our virtual tokens can be simply appended after the retrieval results and then fed, along with the user input, into the LLM for generation. In contrast, if the user input does not necessitate retrieval, it can be processed directly by the LLM. As our approach does not adjust the original parameters of the LLM, it preserves the model\u2019s inherent capabilities. This feature is particularly important for industry or business since existing LLMs may have already been deployed for multiple purposes; our method enhances the retrieval understanding capabilities of these models without compromising their existing functionalities. We illustrate the instructions for using our SPRING in Figure 3. After training, the embeddings of the added tokens have been optimized for RAG, but these tokens do not correspond to any existing tokens in the vocabulary. To make them easy to use, we can add some special tokens (e.g., [r1], , [r50]) to the vocabulary and initialize their embeddings with the trained embeddings. Then, during inference, after obtaining the retrieved results, we can add any number of these special tokens (e.g., [r1] [rk]) after and input them with the question to the LLMs for generation. We also provide an example code snippet in Appendix A for using our method in practice. We refer to our method as SPRING due to its scalable and pluggable nature, making it particularly well-suited for enhancing existing LLMs that are already deployed. Additionally, it effectively bridges the gap between retrieved results and user input, significantly improving the LLMs\u2019 capabilities in understanding the retrieved external knowledge."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets and Retrievers",
            "text": "We conduct experiments on nine commonly used question-answering datasets, including TriviaQA (TQA) Joshi et al. (2017), Natural Questions (NQ) Kwiatkowski et al. (2019), HotpotQA (HQA) Yang et al. (2018), SQuAD 1.1 Rajpurkar et al. (2016), Web Questions (WebQ) Berant et al. (2013), 2WikiMultiHopQA (2Wiki) Ho et al. (2020), CoQA Reddy et al. (2019), MS MARCO Nguyen et al. (2016), and PopQA Mallen et al. (2023). These datasets are publicly available at HuggingFace or their official websites. Since PopQA only has a test set, we use it as a held-out dataset to evaluate the generalizability of the methods. Except for PopQA, we mix the training set of all other datasets for training, and test the methods on their corresponding test set. If the test set is unavailable, we use the development set for evaluation. It is worth noting that, though some datasets have provided golden reference passages for the answer, we do not use them in our experiment but use the passages retrieved from the following retrieval sets in both training and inference stages.\n\nFor the retrieval sets, we follow previous studies Yoran et al. (2023) and use the combination of Wikipedia and MS MARCO datasets as the retrieval corpus. Wikipedia contains high-quality human knowledge, which is helpful for many knowledge-intensive tasks. MS MARCO contains a large amount of web pages, which can provide information necessary for curating some natural language questions. We use the datasets that have already been preprocessed into passages and released on HuggingFace. The Wikipedia set has 21M passages, while the MS MARCO set has 8M passages. More details are provided in Appendix B.\n\nWe use E5-large Wang et al. (2022) as the main retriever in our experiments. The impact of other retrievers, i.e., BM25 Robertson and Zaragoza (2009), BGE-base Xiao et al. (2023), and E5-base, is studied in our further analysis (Section 4.4.3). Among these retrievers, BM25 is a non-neural sparse retrieval algorithm, while others are neural-based dense retrievers. In general, dense retrievers perform better than BM25 on several benchmarks Wang et al. (2022); Izacard et al. (2022)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baseline Methods",
            "text": "We consider both the base and instruction-tuned versions of Mistral-7b, LLaMA-2-7b, and LLaMA-2-13b as the backbone models, and compare our SPRING with the following baselines.\n\nConcat: This method directly concatenates the retrieval results and the question for evaluation.\n\nPrompt: This method uses a manually-crafted prompt to indicate the use of retrieval information (details are provided in Appendix C).\n\nLoRA Hu et al. (2022): This method uses LoRA to fine-tune the backbone models. We use the hyperparameters suggested by the LLaMA\u2019s official guidance.\n\nPrefix-tuning Li and Liang (2021): This method uses prefix-tuning to fine-tune the backbone models. To make a fair comparison with our method, we add 50 prefix tokens for training.\n\nThe implementation details are provided in Appendix D."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Experimental Results",
            "text": "We fine-tune the prefix-tuning, LoRA, and SPRING methods on RAG tasks, and then evaluate their performance in scenarios both with (RAG) and without (non-RAG) retrieval. For SPRING, we use virtual tokens for inference by default, and the impact of token quantity is discussed in Section 4.4.2. The experimental results are shown in Table 1. To save space, we only show the results based on Mistral-7b-instruct, and other results are provided in Appendix E.\n\nWe can observe:\n\n(1) SPRING significantly improves the RAG performance of the original LLM with manually-crafted prompts. It even outperforms LoRA on certain datasets, such as TriviaQA and CoQA. Given that SPRING involves only 0.2M trainable parameters, these results demonstrate its remarkable efficiency and effectiveness.\n\n(2) While LoRA achieves slightly better performance, it adjusts the LLMs\u2019 original parameters, which adversely impact their performance in non-RAG scenarios \u2014 a significant drop has been observed, far worse than the original models. This challenge also extends to other general generation tasks, which will be discussed in the next section.\n\n(3) Using manually-crafted prompts is effective for improving LLMs\u2019 performance on RAG tasks. However, this improvement is limited as no training is involved.\n\n(4) All backbone models show improvements with SPRING, demonstrating its adaptability across various backbone models.\n\n(5) SPRING achieves the best performance on the held-out test set, PopQA, validating the good generalizability of our method.\n\n(6) Interestingly, prefix-tuning cannot perform well for RAG, highlighting that the insertion position of the virtual tokens in SPRING is both reasonable and effective. We conduct more analysis in Appendix F."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Further Analysis",
            "text": "We further conduct a series of experiments to investigate the impact of different settings in SPRING. All the following experiments are conducted based on fine-tuning the Mistral-7b-instruct model."
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Performance on Other Tasks",
            "text": "To examine the impact of different fine-tuning methods on the inherent capabilities of LLMs, we evaluate the performance of models fine-tuned by LoRA and SPRING on several other (non-RAG) tasks. These tasks are commonly used to evaluate LLMs\u2019 reasoning, mathematical abilities, and world knowledge, including BoolQ Clark et al. (2019  ###reference_b6###), CommonsenseQA Talmor et al. (2019  ###reference_b50###), GSM8K Cobbe et al. (2021  ###reference_b7###), and MMLU Hendrycks et al. (2021  ###reference_b14###). The experimental results are shown in Table 2  ###reference_###.444We notice that our results are quite different from those officially reported, which we attribute to the impact of different prompts. Since official prompts for testing are unavailable, we provide the prompts we used in Appendix C  ###reference_### to facilitate reproducibility. From the results, we can observe: (1) Thanks to the plug-and-play design of our method, SPRING can revert to the original LLMs by not using virtual tokens. Therefore, it successfully preserves the original capabilities of the LLMs. In contrast, LoRA, which adjusts the model\u2019s parameters for RAG tasks, inevitably compromises the model\u2019s performance on other tasks. (2) A noticeable decline is observed in the few-shot evaluation, reflecting a decrease in the in-context learning abilities of LLMs. This decline may stem from the fact that RAG fine-tuning does not incorporate in-context learning capabilities. Besides, fine-tuning for RAG tasks may lead the model to overfit to specific task formats, thereby impairing its general generation abilities (more empirical studies are detailed in Appendix G  ###reference_###)."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Impact of Token Quantity",
            "text": "In SPRING, we design a scalable training approach that enables the use of arbitrary numbers of virtual tokens in inference. To validate its effectiveness, we test the performance of our method with various numbers of virtual tokens and compare it with a variant model trained with a fixed number of tokens. The experimental results are illustrated in Figure 4. In general, we observe that the performance of SPRING increases with more virtual tokens used. Surprisingly, SPRING can significantly enhance LLMs\u2019 performance in RAG scenarios with just a single token, which is very encouraging. This varies across different LLMs (see Appendix H). In comparison, training with a fixed number of tokens limits the flexibility of SPRING, as it can only be used with the same number of tokens in inference."
        },
        {
            "section_id": "4.4.3",
            "parent_section_id": "4.4",
            "section_name": "4.4.3 Effects of Different Retrievers",
            "text": "In our experiments, SPRING is fine-tuned using passages retrieved by E5-large. To investigate its effectiveness with other retrievers, we conduct an experiment by testing its performance with passages retrieved by BM25, BGE-base, and E5-large. The results are presented in Table 3. First, SPRING achieves consistent improvement over the original model using manually crafted prompt, thereby confirming the generalizability of our approach. Second, compared to the original model, the performance gap (variance) among different retrievers becomes smaller, highlighting SPRING\u2019s robustness to variations in retrievers. Finally, even fine-tuned with a superior retriever (i.e., E5-large), SPRING maintains strong performance well with less effective retrievers (such as BM25). This indicates that our method can effectively adapt to varying quality of retrieved results. Hence, there is no necessity to retrain the virtual tokens with each update of retrievers in practical applications, significantly enhancing its applicability."
        },
        {
            "section_id": "4.4.4",
            "parent_section_id": "4.4",
            "section_name": "4.4.4 Influence of Retrieved Passages",
            "text": "During the fine-tuning of SPRING, we construct training samples by randomly selecting the top- () retrieved passages. This aims to enhance SPRING\u2019s adaptability by ensuring it can operate effectively with varying numbers of retrieved passages in real-world scenarios. To evaluate the effect of this training strategy, we test the SPRING\u2019s performance across a range from zero to five passages. Figure 5  ###reference_### illustrates the results. SPRING\u2019s performance gradually improves as more retrieved passages are used (), suggesting that more retrieved passages contribute valuable knowledge for question answering. However, the performance peaks at four passages and declines when more passages are added. This decrease could be attributed to noise accumulation within the retrieved knowledge, a phenomenon also reported in recent studies Yoran et al. (2023  ###reference_b59###); Wu et al. (2024  ###reference_b56###). Despite this, the use of retrieved passages still results in performance gains compared to scenarios without retrieval (), highlighting again the benefits of RAG."
        },
        {
            "section_id": "4.4.5",
            "parent_section_id": "4.4",
            "section_name": "4.4.5 Cross-Dataset Generalizability",
            "text": "Inspired by previous studies in multi-task learning Raffel et al. (2020  ###reference_b43###); Khashabi et al. (2020  ###reference_b24###), we mix eight QA datasets for training as they require similar LLM capabilities (e.g., reasoning). To study the impact of this strategy, we conduct experiments by training SPRING on each dataset individually and then testing its performance on the others. Table 4  ###reference_### shows partial results, and more results are available at Appendix E  ###reference_###. As indicated, training on a mixed dataset generally enhances performance on most datasets, thereby validating the benefits of multi-task learning. While training on a single dataset, such as NQ, may yield superior results on its specific test set, such improvements often fail to generalize to other datasets. Notably, training solely on NQ may negatively impact performance on MS MARCO, where the original LLM using a prompt could outperform it. These findings inspire us to carefully consider the interaction between different datasets when applying our method in future applications."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduced scalable and pluggable virtual tokens for retrieval-augmented large language models. Our method, SPRING, serves as a parameter-efficient fine-tuning approach that significantly enhances RAG performance with the addition of only M trainable parameters. More importantly, the plug-and-play nature of our approach successfully preserves the performance of LLMs on non-RAG tasks, while its scalable training strategy broadens the method\u2019s applicational flexibility. Through extensive experiments across various datasets, we have demonstrated the effectiveness, generalizability, flexibility, and high efficiency of our method. We believe that our research will foster further integration of information retrieval and LLMs, and advance the development of other parameter-efficient fine-tuning technologies for LLMs."
        }
    ],
    "url": "http://arxiv.org/html/2405.19670v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4.2",
            "4.4.3",
            "4.4.4",
            "4.4.5"
        ]
    },
    "research_context": {
        "paper_id": "2405.19670v3",
        "paper_title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
        "research_background": "### Paper\u2019s Motivation:\nThe paper is motivated by the limitations of large-language models (LLMs) in generating accurate, up-to-date information despite their vast pre-trained parameters. Existing LLMs tend to produce hallucinated, outdated, or inaccurate content, especially when dealing with long-tail knowledge. Retrieval-Augmented Generation (RAG) has emerged as a strategy to enhance content reliability by decoupling knowledge retrieval from the backbone LLMs. However, current RAG implementations either heavily depend on prompt crafting and the LLMs' ability to interpret these prompts or require computationally expensive training, leading to compromised general generation capabilities.\n\n### Research Problem:\nThe central research problem addressed by the paper is whether it is possible to enhance LLMs' performance in RAG scenarios while preserving their general generation capabilities. Specifically, the problem is to create a tuning method that improves LLM performance with minimal computational overhead and without altering their core generative abilities.\n\n### Relevant Prior Work:\n1. **LLM Performance**:\n    - LLMs have achieved high performance in NLP tasks (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023).\n\n2. **Hallucination and Inaccuracy Issues**:\n    - LLMs can generate inaccurate or outdated information, especially with long-tail knowledge (Ji et al., 2023; Zhang et al., 2023b).\n\n3. **Retrieval-Augmented Generation (RAG)**:\n    - RAG has improved accuracy by separating knowledge retrieval from LLMs, showing enhanced performance in tasks like open-domain QA (Lewis et al., 2020b; Petroni et al., 2021; Izacard et al., 2023; Tan et al., 2024; Jin et al., 2024).\n\n4. **Existing RAG Approaches**:\n    - Prompt-based methods: Use LLMs' in-context learning by adding retrieved information to prompts (Shi et al., 2023; Ram et al., 2023).\n    - Training-based methods: Enhance LLMs via end-to-end pre-training or fine-tuning (Guu et al., 2020; Borgeaud et al., 2022; Lin et al., 2023; Wang et al., 2023).\n    - Parameter-efficient fine-tuning: Techniques like LoRA, aiming to reduce training costs but affecting general generation abilities (Hu et al., 2022).\n\n### SPRING Method:\nThe paper introduces SPRING, a novel tuning method involving the addition of trainable virtual tokens to help LLMs handle RAG problems. This approach involves fine-tuning these virtual tokens while freezing the LLM\u2019s original parameters, preserving general generative capabilities. The method is lightweight, scalable, pluggable, and generalizable, allowing it to enhance LLM performance in RAG without extensive computational costs or loss in general use cases.",
        "methodology": "The proposed methodology, dubbed **SPRING**, introduces a novel approach for enhancing Retrieval-Augmented Generation (RAG) in large language models (LLMs) by using scalable and pluggable virtual tokens. This method combines the flexibility of prompt-based methods and the efficacy of fine-tuning-based methods. Here are the key components and innovations of SPRING:\n\n### Key Components:\n\n1. **Virtual Tokens**:\n   - SPRING utilizes virtual tokens to enhance LLMs by allowing any number of such tokens to be incorporated during inference.\n   - These virtual tokens can dynamically adjust based on application needs, providing scalability.\n\n2. **Optimization Strategy**:\n   - A unique optimization strategy is designed to mimic the behavior of a \"spring.\" In training, for a sample with a given number of added tokens, a random number \\(\\tilde{k}\\) is selected, and the first \\(\\tilde{k}\\) virtual tokens are used to construct the training example.\n   - This method ensures flexible optimization, enabling the addition of any number of virtual tokens during inference.\n\n3. **Training Example Construction**:\n   - For each training sample \\((C_i, Y_i)\\), with \\(k\\) added tokens, a subset of tokens \\((r_1^{(i)}, r_2^{(i)}, \\ldots, r_k^{(i)})\\) is used to construct the sequence \\((C_i \\oplus r_1^{(i)} \\oplus \\cdots \\oplus r_{\\tilde{k}}^{(i)}, Y_i)\\).\n\n### Innovations:\n\n1. **Scalable Application**:\n   - SPRING allows for dynamic adjustments of virtual tokens during inference, making it extremely flexible and scalable to cater to different application requirements.\n\n2. **Pluggable Mechanism**:\n   - Virtual tokens can be appended after the retrieved results and inputted along with the user\u2019s input into the LLM for generation if external knowledge is needed.\n   - Without the need for external knowledge, the user input can be processed directly by the LLM.\n\n3. **Non-intrusive Enhancement**:\n   - The method does not alter the original parameters of the LLM, which preserves the model\u2019s innate capabilities.\n   - This non-intrusive feature is crucial for industrial and business applications where existing LLMs are often deployed for multiple purposes.\n\n4. **Training and Inference**:\n   - After training, embeddings of the added tokens are optimized. Special tokens (like [r1], [r2],..., [rk]) are added to the vocabulary, initialized with these trained embeddings.\n   - During inference, these special tokens are appended after the retrieved results and input along with the user's query into the LLM to generate responses.\n\n5. **Practical Utility**:\n   - SPRING bridges the gap between retrieved external knowledge and user input, significantly enhancing the LLM's comprehension of this knowledge.\n   - The method is well-suited for integrating with existing, deployed LLMs, thereby enhancing their retrieval understanding capabilities without hindering their existing functionalities.\n\n### Conclusion:\n\nThrough these innovative mechanisms, SPRING offers a robust solution for improving the performance of retrieval-augmented large language models, making them scalable, pluggable, and highly adaptable to a variety of practical applications. This method ensures that LLMs are able to leverage external knowledge effectively without disrupting their core functionalities, thus providing a significant enhancement in their overall retrieval and generation capabilities.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:** The main experiment evaluates performance on nine commonly used question-answering (QA) datasets:\n1. TriviaQA (TQA)\n2. Natural Questions (NQ)\n3. HotpotQA (HQA)\n4. SQuAD 1.1\n5. Web Questions (WebQ)\n6. 2WikiMultiHopQA (2Wiki)\n7. CoQA\n8. MS MARCO\n9. PopQA\n\nDatasets are publicly available via HuggingFace or their respective official websites. The PopQA dataset is used as a held-out dataset to assess the generalizability of the methods, while the training datasets from the other eight datasets are combined, and their respective test sets or development sets (when test sets are unavailable) are used for evaluation. No golden reference passages are employed; instead, passages are retrieved from combined Wikipedia and MS MARCO datasets for both training and inference stages.\n\n**Evaluation Metrics:** Performance in the experiments is measured using:\n- Exact Match (EM)\n- F1 score\n\n**Retrieval Corpus:** The retrieval corpus comprises preprocessed Wikipedia and MS MARCO passages:\n- Wikipedia: Contains 21 million passages.\n- MS MARCO: Contains 8 million passages.\n\nThe passages datasets are sourced and preprocessed from HuggingFace."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To validate the effectiveness of using varying numbers of virtual tokens on the performance of SPRING in RAG scenarios.",
            "experiment_process": "The researchers tested the performance of SPRING with various numbers of virtual tokens and compared it to a variant model trained with a fixed number of tokens. The results are illustrated in Figure 4.",
            "result_discussion": "The performance of SPRING generally increases with more virtual tokens used. SPRING significantly enhances performance with just a single token. In contrast, training with a fixed number of tokens limits the flexibility of SPRING, as it can only be used with the same number of tokens in inference.",
            "ablation_id": "2405.19670v3.No1"
        },
        {
            "research_objective": "To investigate SPRING's effectiveness with different retrievers and examine its robustness and adaptability.",
            "experiment_process": "SPRING, initially fine-tuned using passages retrieved by E5-large, was tested with passages retrieved by BM25, BGE-base, and E5-large. The results were presented in Table 3.",
            "result_discussion": "SPRING consistently improves over the original model with manually crafted prompts. It shows less performance variance among different retrievers, highlighting its robustness. Even when fine-tuned with a superior retriever (E5-large), SPRING maintains strong performance with less effective retrievers (such as BM25). This indicates that retraining the virtual tokens with each update of retrievers is unnecessary, significantly enhancing its applicability.",
            "ablation_id": "2405.19670v3.No2"
        },
        {
            "research_objective": "To evaluate the effect of varying the number of retrieved passages on SPRING's performance.",
            "experiment_process": "Training samples were constructed by randomly selecting the top retrieved passages to enhance SPRING's adaptability. The performance was tested across a range from zero to five passages, and the results were illustrated in Figure 5.",
            "result_discussion": "SPRING's performance improves with more retrieved passages, peaking at four passages before declining when more are used due to noise accumulation. However, using retrieved passages results in performance gains compared to non-retrieval scenarios, highlighting RAG's benefits.",
            "ablation_id": "2405.19670v3.No3"
        },
        {
            "research_objective": "To study the impact of multi-task learning by mixing multiple QA datasets on SPRING's performance.",
            "experiment_process": "SPRING was trained on eight mixed QA datasets requiring similar LLM capabilities and tested on each individually. Partial results were shown in Table 4, with more results available in Appendix E.",
            "result_discussion": "Training on mixed datasets enhances performance across most datasets, validating multi-task learning's benefits. However, training on a single dataset may yield superior specific results but fail to generalize to others, potentially impacting performance on different datasets negatively. This necessitates careful consideration of dataset interaction for future application of the method.",
            "ablation_id": "2405.19670v3.No4"
        }
    ]
}