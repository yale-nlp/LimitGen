{
    "title": "\\scalerel* Exploring and Improving Drafts in Blockwise Parallel Decoding",
    "abstract": "Despite the remarkable strides made by autoregressive language models, their potential is often hampered by the slow inference speeds inherent in sequential token generation.\n\nThis paper contributes to the understanding and improvement of block drafts in two ways. First, we analyze the token distributions produced by multiple prediction heads. Secondly, we leverage this analysis to develop algorithms to improve BPD inference speed by refining the block drafts using n-gram and neural language models.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The landscape of natural language processing has been profoundly reshaped by recent advances in autoregressive language models. These models have shown remarkable proficiency across a range of text generation tasks, including applications like question answering and summarization. However, a significant obstacle to their wider application is high inference latency, particularly for extremely deep models with hundreds of billions of parameters. This latency, intrinsic to decoding with autoregressive language models (LMs), imposes considerable computational burdens and limits real-time deployment.\n\nIn response to these latency challenges, the field has seen a shift towards decoding methods aimed at reducing the inference latency in large language models (LLM). One promising development is decoding strategies that accelerate text generation while ensuring that the output remains fluent and natural."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Our contributions",
            "text": "In this paper, we first investigate properties made by the prediction heads of blockwise parallel LMs across several tasks; given these observations, we propose rescoring algorithms to produce higher quality drafts."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Observations on block drafts",
            "text": "Consecutive repetitions \u2009All heads within a block make predictions independently in a blockwise parallel LM. Unsurprisingly, we observe that this leads to block drafts with significant token repetition across heads. Consecutive repetition is pervasive across tasks, ranging from 20% to 75% of all neighboring draft tokens, depending on the task (subsection 6.1  ###reference_###).\n\nConfidence of different heads \u2009We analyze the distribution of probabilities within each softmax head. Our empirical analysis reveals a key property of BPD: the block drafter tends to be more confident with initial tokens, and becomes progressively less confident for subsequent tokens. We find that the confidence of block heads correlates strongly with the quality of the block drafter (subsection 6.2  ###reference_###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "New algorithms",
            "text": "Based on these observations, we propose two algorithms to leverage the top- predictions at each head and improve BPD latency. Neither of these algorithms require changes to the underlying blockwise parallel LMs.\n\nLocal rescoring via neural LMs\u2009 Given the top- predictions at each head, we refine the block draft by using a small neural, autoregressive LM to greedily rescore these local predictions. While the block prediction scores are produced independent of each other, neural rescoring should favor sequences that are fluent, encouraging coherence between the predictions at each head.\n\nGlobal rescoring via n-gram LMs with multi-drafts\u2009 If the blockwise parallel LM has heads and we consider the top- tokens from each head, then there are candidate drafts of length that can be formed. We propose to use an n-gram model to efficiently rescore all paths, via dynamic programming, and generate the most probable rescored paths as a batch of draft candidates. These drafts can then be verified in parallel by the blockwise parallel LM.\n\nThere are two critical distinctions between the proposed algorithms: the amount of context/expressive power available to each class of rescoring model, and fundamental limitations of decoding with each class. While neural rescoring models are potentially more expressive and can leverage unbounded context, n-gram LMs can be used to efficiently find the globally most likely rescored drafts from the exponentially-sized set of possible draft candidates."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Organization",
            "text": "The remainder of this paper organized as follows. In section 3 ###reference_###, we discuss previous literature in reducing LLM latency. In section 4 ###reference_###, we define foundational concepts and terminology. section 5 ###reference_### describes our experimental setup, datasets, on methods. subsection 7.1 ###reference_### presents the proposed BPD rescoring algorithms and empirical results, followed by a final discussion in section 8 ###reference_###."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Efficient transformer inference",
            "text": "Works on improving transformer efficiency encompass both optimization of an existing set of model weights, or a fundamental change to the model architecture. Examples of the former include techniques such as quantization [44, 45, 9] and model pruning [40, 26]. In parallel, neural architecture search has played a crucial role in identifying network structures that balance performance with efficiency [22, 46]. Relatedly, Elbayad et al. propose early-exiting at intermediate layers for faster inference, while Schuster et al. explore confidence thresholding for balancing speed and accuracy. These methods offer insights into optimizing decoding under resource constraints.\n\nOne important line of work has focused on modifying the decoding method in LMs. The adoption of non-autoregressive (parallel) decoding strategies [38, 14] marks a pivotal shift in this domain, addressing inference latency by simultaneously generating multiple tokens. Subsequent innovations have sought to refine this approach by incorporating additional context [6], iterative refinement [20], and tree-based attention mechanism [4]. However, these refinements often require complex training or additional inference data."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Efficient and effective decoding",
            "text": "There are several recent works that improve the speed of LLM decoding, including pioneering works like BPD and speculative decoding. Speculative decoding leverages a smaller \u2018draft\u2019 model to anticipate the outputs of a larger target model, improving average decode latency without loss in generation quality [24, 5, 20]. The draft model is typically trained on the same corpus as the LLM, thus autoregressively generates similar drafts as the target model with reduced latency. Speculative decoding is most successful when a long sequence of speculated tokens are accepted by the target LM during verification, avoiding multiple serial calls to the target LM to generate the same sequence.\n\nOn the surface, contrastive decoding algorithms share some similarities with our proposed draft rescoring approach, insofar as a weaker model is used to modify the predictions of the target LM [25, 21]. Like speculative decoding, our proposals have no effect on the quality of the target LM\u2019s generated text."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "This section introduces notation and concepts, including algorithms for standard autoregressive decoding and BPD.\nAutoregressive decoding\u2009 Let  be an autoregressive LM parameterized by . The objective is to generate an output sequence  conditioned on an input sequence .\n is a vector of logits, , where  is the vocabulary over tokens. These logits define a conditional probability distribution at each time step , which by the chain rule yields .\nSequences are generated autoregressively, either through ancestral sampling from some form of the conditional next token distribution [17  ###reference_b17###], or by a beam search through the space of possible sequences to return a probable sequence. Greedy decoding, a special case of beam search, generates each token as . In this work, we consider greedy decoding exclusively, as this is the setting that Stern et al.\u2009[38  ###reference_b38###] was designed to accelerate.\nBlockwise parallel decoding\u2009 Let  be a blockwise parallel LM with block size . This model employs  distinct feedforward neural (FFN) layer with a single hidden layer, atop the target LM\u2019s final hidden layer. The output of each FFN is followed by a softmax layer over the vocabulary to predict each of the  subsequent tokens in the block. In our experiments, the parameters of the FFNs are learned jointly with the base LM during training, and the weights of all softmax layers are tied to the input embedding table. Algorithm 1  ###reference_### describes the BPD greedy decoding procedure:\nPredict:  is used to generate a draft of  token predictions , conditioned on the prompt, , and existing generated text, .  is identical to the target LM greedy decode.\nVerify: At this stage, the target LM greedily generates next-token logits  conditioned on the existing prefix and block draft . Verification amounts to checking which block draft tokens match the autoregressive greedy decode from the target LM: (. Verification of all positions can be performed in parallel under the assumption that the target LM is a decoder-only transformer.\nAccept: Finally, the length of the longest contiguous prefix of draft tokens that match the target LM greedy decode is identified: . The decoded sequence is extended by  tokens and we iterate.111The decoded sequence is extended by  tokens since during verification we generate the token from the target LM, , at the first position where the draft differs from the target LM greedy decode. Note that in general, not all  tokens are accepted, and many of the draft tokens in each block are discarded. Since the additional time required to generate a block of tokens is fast relative to the time it takes for the forward pass of the target LM, a modest gain in accepted prefix length justifies the cost of draft generation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental setup",
            "text": "In this paper, we use billion (B) parameter decoder-only transformer LMs with up to 9 blockwise heads. This study is based on the original BPD framework, with a modification: we use decoder-only models instead of the T5 encoder-decoder architecture. Other setups are consistent with the approach in Stern et al.\n\nThe 1.5B model and all other LMs were pretrained on (English) C4 with the causal next token prediction objective tokenized with the GPT3 subword vocabulary. For the 1.5B blockwise parallel LMs, all heads were trained jointly to predict the following tokens at each iteration. During pretraining, we use batches of 2048 subword sequences, each 512 tokens in length, amounting to B input tokens in total on TPUv3/TPUv4 with Jax.\n\nIn addition to a standard language modeling dataset, LAMBADA, we conduct experiments across several classes of downstream tasks. In the realm of text summarization, we evaluate models on the XSUM, MultiNews, SAMSum, NewsRoom, and CNN/DailyMail datasets. Each of these datasets is characterized by distinct summary lengths and styles. For extractive QA, the SQuAD V1 dataset serves as our testbed. For each task aside from language modeling, we finetune the blockwise parallel LM for that task. Details are given in Appendix C. \n\nTable 2 sketches how BPD acts on three examples from each class of tasks. LM: BPD excels at generating common multi-word expressions in a single step. For example, (no) \u2018thing more than\u2019, and (take) \u2018his word for the\u2019 are each drafted and accepted in a single step. QA: In SQuAD V1, it accurately completes the answer \u2018Grumman\u2019 from \u2018Gru\u2019 by adding \u2018mman\u2019, highlighting its ability to process multiple tokens at once and quickly extend answers. SUM: BPD\u2019s effectiveness in SUM tasks varies by dataset. For formulaic summaries like CNN/DailyMail, it performs well, reflecting its alignment with LM and QA tasks. However, in narrative-driven datasets like SAMSum and XSUM, where concise summaries are required, the performance of BPD is similar to standard decoding."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Exploration of predictive dynamics in BPD",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Consecutive repetition",
            "text": "We observe that vanilla block drafts are prone to significant token repetition. This is due to the fact that each head\u2019s prediction is independent of the others, and is a limitation shared with non-autoregressive generation in general [14]. Strings of repeated tokens are unnatural, and unlikely to be generated by a strong base language model. Rescoring the top-lattice with even a simple language model eliminates a significant amount of repetition, reducing the percentage of consecutive repeated tokens from between 9.9% to 24.5%, depending on the task."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Confidence across multiple heads",
            "text": "Intuitively, predicting the identity of the future token becomes harder as increases. To better understand this phenomenon, we measure the confidence of the predictions by the entropy of the probability distribution. In 3(a), we plot the normalized histogram of entropy of each head on the LAMBADA task. From the normalized histogram, it is clear that the entropy increases as we move from the first head to the last head, which agrees with our intuition that hardness of predictions increases as increases.\n\nHowever, we observed that the entropy of heads does not increase monotonically for all tasks. Let be the average entropy of head on a particular corpus, and let , be the index of the largest head such that the average entropy of each head increases monotonically to that point. Heads with lower entropy (indicating more confident predictions) intuitively contribute more. Nonetheless, simply maximizing the number of low-entropy heads is not optimal, but rather incorporating progressively higher entropy heads, up to a certain point, can benefit decoding. A linear regression confirms this with an R-value of . This analysis suggests that BPD head entropy could be used as a proxy for inference latency."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Oracle top-k block efficiency",
            "text": "###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Lattice rescoring for improved block efficiency",
            "text": "Having explored BPD\u2019s prediction dynamics, we propose two drafting algorithms for rescoring the top- lattice. This section presents techniques for rescoring the top- lattice along with empirical results.\n\nEach of these algorithms is a modification of the block drafted in Stage 1 in Algorithm 1. Instead of using the most likely token at each head as the prediction, we construct the top- sausage lattice of likely drafts from each head, where the set of top- tokens is denoted as  for head . This approach allows any token within  to be chosen for position , yielding a total possible combinations of:\n\nIn this lattice, any path from the start to final state represents a viable draft. Two algorithms are proposed to select a small number of -length drafts from this lattice, which are then passed to the verification step. The first algorithm employs neural autoregressive transformers (subsection 7.1), while the second utilizes n-gram language models (subsection 7.2)."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Local rescoring via neural models",
            "text": "A simple approach uses a small neural rescorer, interpolating between the logits of the rescorer LM and vanilla block draft logits with an interpolation weight. The rescored prediction is given by:\nwhere  represents the logit of the block draft at head , and  is the corresponding logit predicted by the small neural rescoring model, which is conditioned on the sequence . The parameter  is the weight placed on the rescorer\u2019s prediction. We experiment with decoder-only transformers having 32, 61, and 94 million (M) weight parameters. We use greedy rescoring when generating the neural draft."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Global n-gram rescoring",
            "text": "We evaluate the quality of drafts generated by rescoring with an n-gram LM. Blockwise parallel LMs can be used to compute a lattice representing possible sequences. We rescore all of these sequences with an n-gram model, select the top sequences and pass them to the verification stage. When , we refer to this as n-gram rescoring and when , we refer to this as -best n-gram BPD.\n\nWhile global rescoring typically yields better results compared to local rescoring, rescoring sequences with a neural LM and selecting the most likely sequence would take time , which is computationally prohibitive in most cases. Hence, we take advantage of n-gram models, which are unique in that we can select the most likely sequence in time  using dynamic programming.\n\nWe use the OpenFST library to represent each n-gram model as a weighted finite state automaton and apply finite state composition with the top- lattice followed by extraction of the most likely draft sequences. Training details for the n-gram models are given in Appendix C.3."
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Empirical evaluation",
            "text": "Repairing repetitions\u2009 In subsection 6.1, we note that vanilla block drafts are prone to token-level repetition and that rescoring with a simple language model reduces the incidence of this. To answer this, we compared the drafts generated by greedy rescoring with the 61M parameter neural rescorer against vanilla drafts. Time step instances were considered wins/ties/losses based on the accepted prefix length of the rescored draft vs. vanilla draft. Table 5 displays the win frequency across tasks along with the percentage of wins/losses attributed to introducing/eliminating repetition.\n\nNote that in the tasks where rescoring improves the most, a high percentage of those repaired instances are driven by fixing erroneously repeated tokens. In fact, for MultiNews, 66.23% of block drafts are improved through repetition repair. We also evaluated the performance of rescoring with in-domain trained rescoring LMs, but found that they tended to perform no better than C4-trained LMs (Appendix D)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper presents a comprehensive analysis of BPD, highlighting its predictive dynamics and proposing methods to refine the generation of block drafts. Our study offers insights into BPD\u2019s behavior, particularly the tendency for drafts to contain consecutive repetitions and its heads to exhibit varying confidence levels in predictions. Two algorithms are proposed for generating higher quality drafts: one for local rescoring with small neural models (i.e., neural BPD) and another for global rescoring with an n-gram LM and generating multiple drafts (i.e. -best n-gram BPD). These algorithms leverage the strengths of both blockwise parallel LMs and small rescoring models to reduce average decoding latency, pushing the boundaries of efficient text generation with BPD. We believe that this paper lays the groundwork for future exploration in optimizing LM decoding speed."
        }
    ],
    "url": "http://arxiv.org/html/2404.09221v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "3",
            "3.1",
            "3.2"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "7",
            "7.1",
            "7.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "7.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.1",
            "2.2",
            "7",
            "7.1",
            "7.2",
            "7.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.09221v2",
        "paper_title": "\\scalerel* Exploring and Improving Drafts in Blockwise Parallel Decoding",
        "research_background": "The motivation for this paper arises from the need to address the high inference latency inherent in autoregressive language models (LMs), which is particularly problematic for extremely deep models with hundreds of billions of parameters. This latency is a significant obstacle that hampers the real-time deployment of LMs in various applications such as question answering and summarization.\n\nThe research problem is centered on the development and refinement of blockwise parallel decoding (BPD) methods to enhance inference speed. Although BPD allows multiple token drafts to be generated in parallel, which theoretically speeds up the text generation process, it introduces the challenge of ensuring these drafts are coherent and consistent. The aim is to improve the quality of block drafts to facilitate accelerated yet fluent and natural text generation.\n\nRelevant prior work includes:\n1. Advances in autoregressive language models, which have demonstrated impressive abilities in text generation tasks (references [3], [43], [30], [33], and [42]).\n2. Applications of autoregressive LMs in question answering (reference [34]) and summarization (reference [15]).\n3. Recognition of high inference latency as a limiting factor for deep LMs with significant computational demands (references [16], [31], and [7]).\n4. Introduction of blockwise parallel decoding as a method to reduce inference latency in large language models (references [38], [27], and [4]).\n\nIn summary, the paper seeks to explore and improve the quality of block drafts in BPD to mitigate the latency issue while ensuring output fluency and consistency, without modifying the base model parameters.",
        "methodology": "In this section, we will outline and delve into the proposed methodology for exploring and improving drafts in blockwise parallel decoding, focusing on the critical components and innovations. Our paper investigates the properties generated by the prediction heads of blockwise parallel language models (LMs) across various tasks. Based on our observations, we then introduce rescoring algorithms aimed at improving the quality of these block drafts.\n\nKey components and innovations of our proposed method include:\n\n1. **Blockwise Parallel Language Models (LMs)**: We employ blockwise parallel language models, which facilitate parallel processing, leading to significant reductions in overall decoding time. These models predict text in blocks, leveraging multiple prediction heads that work simultaneously on different parts of the text sequence.\n\n2. **Investigation of Prediction Head Properties**: We conduct an in-depth examination of the outputs produced by the prediction heads used in blockwise parallel LMs across diverse tasks. This investigation allows us to understand the strengths and weaknesses of the predictions made by each head, identifying patterns and areas for potential quality improvement in the generated drafts.\n\n3. **Rescoring Algorithms**: Based on our observations from the prediction head analysis, we propose rescoring algorithms designed to enhance the quality of block drafts. These algorithms reassess and revise the initial predictions made by the heads, potentially leading to more accurate and coherent text outputs.\n\n4. **Higher Quality Block Drafts**: The combined effort of analyzing prediction head properties and applying rescoring algorithms aims to produce higher quality block drafts. The refinement provided by the rescoring algorithms addresses inconsistencies and errors in the initial drafts, resulting in more polished and reliable texts.\n\nBy leveraging these components and innovations, our methodology aims to enhance the efficiency and quality of text generation in blockwise parallel decoding, offering a more robust framework for various language modeling tasks.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\n### Experiment Setup\n\n- **Models**: The experiments utilize billion-parameter (1.5B) decoder-only transformer language models (LMs) with up to nine blockwise heads. The models were pretrained using the causal next token prediction objective, tokenized with the GPT-3 subword vocabulary.\n\n- **Pretraining**: The 1.5B blockwise parallel LMs were pretrained on the English C4 dataset with batches of 2048 subword sequences, each consisting of 512 tokens, amounting to billions of input tokens processed on TPUv3/TPUv4 with Jax.\n\n- **Datasets**:\n  - **Language Modeling**: Evaluated on LAMBADA.\n  - **Text Summarization**: Evaluated on XSUM, MultiNews, SAMSum, NewsRoom, and CNN/DailyMail datasets.\n  - **Extractive Question Answering (QA)**: Evaluated using the SQuAD V1 dataset.\n\n- **Evaluation Metrics**:\n  - **Language Modeling**: Perplexity\n  - **QA**: Exact Match\n  - **Summarization**: ROUGE-L\n\n- **Block Efficiency**: The primary measure for potential latency improvement, defined as the average number of tokens decoded per serial call to the blockwise parallel LMs. A block efficiency of 1 indicates no speedup compared to standard greedy decoding.\n\n### Results\n\n- **Language Modeling (LM)**: Blockwise Parallel Decoding (BPD) excels in generating common multi-word expressions in a single step, showing substantial efficiency improvements in drafting and verifying multi-token sequences, such as seamlessly completing phrases.\n\n- **Extractive QA**: The block efficiency is notably high in this task. For instance, BPD accurately extends partial answers (e.g., completing \u2018Gru\u2019 to \u2018Grumman\u2019), demonstrating its effectiveness in processing and post-processing multi-token entities.\n\n- **Text Summarization (SUM)**: \n  - **Formulaic Summaries**: For datasets like CNN/DailyMail, BPD performs well, reflecting its alignment with LM and QA tasks, thus achieving significant block efficiencies.\n  - **Narrative-driven Summaries**: In datasets requiring concise summaries, such as SAMSum and XSUM, the block efficiency of BPD is only marginally better than standard decoding, indicating variability in performance depending on the summary style and length.\n\nOverall, the experiment highlights the strengths of BPD in speeding up multi-token predictions while maintaining the quality of generated sequences across various tasks, though its effectiveness can vary significantly depending on the nature of the task."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the impact of token repetitions and confidence levels in blockwise parallel decoding and measure the potential benefit of considering top- tokens at each head for improving block efficiency.",
            "experiment_process": "The study involved analyzing block drafts from a blockwise parallel language model across various tasks to observe token repetition and the confidence of softmax head predictions. An oracle top- block efficiency was measured by considering the top- most likely tokens at each head. The datasets and the standard BPD algorithm were used to evaluate the consequences of these observations.",
            "result_discussion": "The experiments revealed that consecutive token repetitions were prevalent, and confidence levels decreased for subsequent tokens in block drafts. By considering the top- tokens at each head for potential drafter candidates, there was significant headroom for improvement in block efficiency across tasks.",
            "ablation_id": "2404.09221v2.No1"
        },
        {
            "research_objective": "Develop algorithms to refine block drafts using top- predictions to enhance BPD latency without altering the underlying blockwise parallel LMs.",
            "experiment_process": "Two algorithms were proposed: Local rescoring via neural language models (LMs) and global rescoring via n-gram LMs with multi-drafts. The local rescoring method used a small neural, autoregressive LM to rescore top- drafts, promoting fluency and coherence. The global rescoring method used an n-gram model to dynamically generate the most probable rescored paths from an exponentially-sized set of candidate drafts, which were then verified in parallel.",
            "result_discussion": "The proposed methods increased block efficiency by up to 21.30% and optimized resource usage by reducing KV cache I/O by 2.54% and increasing FLOPs per token by 4.04%. These improvements demonstrated the efficacy of leveraging top- token predictions for refining block drafts.",
            "ablation_id": "2404.09221v2.No2"
        },
        {
            "research_objective": "Evaluate the effectiveness of lattice rescoring using both neural and n-gram models on block efficiency and token-level repetition repair.",
            "experiment_process": "The proposed algorithms established a top- sausage lattice of likely drafts. The first method, local rescoring, used small autoregressive transformers for rescoring. The second method, global rescoring, applied n-gram language models to select the most likely sequences via dynamic programming. Block efficiency was measured across various tasks, and the impact of rescoring on token repetitions was analyzed using win/tie/loss metrics based on the accepted prefix length of rescored vs. vanilla drafts.",
            "result_discussion": "Lattice rescoring improved block efficiency in tasks with low initial efficiency, notably with neural LMs achieving the best results in some cases. Rescoring effectively reduced token-level repetitions, significantly driving improvements in block efficiency for tasks such as NewsRoom and MultiNews, where a majority of improvements were attributed to fixing erroneous repetitions.",
            "ablation_id": "2404.09221v2.No3"
        }
    ]
}