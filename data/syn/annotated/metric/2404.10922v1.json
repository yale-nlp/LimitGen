{
    "title": "Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training",
    "abstract": "Recent advancements in language modeling have led to the emergence of Large Language Models (LLMs) capable of various natural language processing tasks. Despite their success in text-based tasks, applying LLMs to the speech domain remains limited and challenging. This paper presents BLOOMZMMS, a novel model that integrates a multilingual LLM with a multilingual speech encoder, aiming to harness the capabilities of LLMs for speech recognition and beyond. Utilizing a multi-instructional training approach, we demonstrate the transferability of linguistic knowledge from the text to the speech modality. Our experiments, conducted on 1900 hours of transcribed data from 139 languages, establish that a multilingual speech representation can be effectively learned and aligned with a multilingual LLM. While this learned representation initially shows limitations in task generalization, we address this issue by generating synthetic targets in a multi-instructional style. Our zero-shot evaluation results confirm the robustness of our approach across multiple tasks, including speech translation and multilingual spoken language understanding, thereby opening new avenues for applying LLMs in the speech domain.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language modeling involves predicting subsequent text tokens based on a context of preceding ones (Jurafsky and Martin, 2009). Training a language model (LM) requires only raw text samples, as portions of these samples function as their labels, facilitating a self-supervised learning (SSL) approach. The widespread availability of machine-readable text online, coupled with advancements in computational power, has led to the rise of large LMs (LLMs) in recent years. These LLMs not only generate highly fluent natural text but also encode higher-level knowledge within their parameters. This enables them to tackle natural language processing tasks like reading comprehension and machine translation based only on task-specific instructions, without needing labeled data (Radford et al., 2019).\n\nSSL has recently made significant strides in the speech domain (Baevski et al., 2020). Most applications of SSL in speech employ an encoder that transforms raw speech signals into high-level representations, serving either as a fixed feature extractor (Yang et al., 2021) or a tunable pretrained model for various downstream tasks (Babu et al., 2021). Incorporating SSL pretrained encoders into Encoder-Decoder speech recognition models has dramatically reduced the amount of labeled data required for effective training (Chang et al., 2021). However, using SSL pretrained decoders in such models is relatively rare. In certain instances, SSL is part of a joint training process that seeks to learn a shared speech and text representation (Chen et al., 2022). However, this approach often demands a large dataset and considerable computational resources.\n\nRecent work has begun to harness the powerful text generation capabilities of decoder-only LLMs by incorporating them as the decoder component of Encoder-Decoder speech processing models. Wu et al. (2023) adopt the LLaMA-7B LLM for speech translation to English by training a speech encoder from scratch using filter bank acoustic features, 14,000 hours of internal speech data in 14 languages, and outputs of internal translation system as synthetic targets. Outputs of speech encoder are aligned with the text token embedding space using CTC pretraining and downsampled by averaging of consecutive frames with the same CTC output label.\n\nLing et al. (2023) adopt the GPT2 XL LLM for fully-formatted English speech recognition by training a speech encoder from scratch using filter bank acoustic features, and 75,000 hours of internal transcribed English speech data. CTC loss is applied to speech encoder outputs as a part of the main training process and speech representations are downsampled by removal of frames classified as CTC blank labels with a predefined threshold.\n\nLi et al. (2023) adopt the LLaMA-7B LLM for long-form English speech recognition by incorporating the HuBERT-Large SSL pretrained speech encoder and finetuning it on the LibriSpeech dataset containing 960 hours of transcribed English speech. Outputs of the speech encoder are downsampled by a convolutional module trained as a part of the main training process.\n\nFathullah et al. (2023) adopt the LLaMA-7B LLM for speech recognition in 8 languages by training a speech encoder from scratch using filter bank acoustic features and the Multilingual LibriSpeech dataset containing 50,000 hours of transcribed speech in the same 8 languages. Speech encoder is pretrained with CTC loss and its outputs are downsampled by simple discarding of every few frames.\n\nNachmani et al. (2023) combine an internal pretrained LLM with an internal pretrained speech encoder and finetune it on the automatically transcribed LibriLight dataset containing 60,000 hours of English speech. The training is performed with a combination of the speech transcription and speech continuation tasks. The resulting model is utilized for the spoken language answering task.\n\nMost of these studies rely on conventional filter bank features for speech encoding and do not incorporate an SSL pretrained speech encoder, necessitating a large amount of training data. Moreover, scant attention has been given to leveraging the linguistic knowledge stored in LLMs for tasks beyond mere transcription and for languages other than English.\n\nTo address these challenges, we propose BLOOMZMMS, a model that fuses a multilingual LLM (BLOOMZ Muennighoff et al., 2023) with a multilingual speech encoder (MMS Pratap et al., 2023). We argue that multi-instructional training is crucial for transferring linguistic knowledge from the text to speech modality. Our experiments demonstrate that training on 1900 hours of transcribed data from 139 languages yields a multilingual speech representation compatible with a multilingual LLM in the context of Automatic Speech Recognition (ASR) task. Although this representation does not generalize well to other tasks, we show that the issue can be mitigated by generating additional synthetic targets. Our zero-shot evaluations confirm this approach\u2019s effectiveness across various tasks, including Spoken Language Translation (SLT)"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Our model comprises the pretrained speech encoder, LLM, and an intermediate Adaptor module that maps the output of the speech encoder to the latent space of the text token embeddings of the LLM. We train the Adaptor module using pairs of speech recordings and their corresponding text transcriptions, denoted as  and  respectively, and keep the parameters of the speech encoder and the LLM frozen. The objective of the Adaptor training is to make its output  obtained from the input speech as close as possible to the text embedding sequence of the ground truth transcription , where LMEmbedding is the token embedding layer of the LLM.\n\nSimilarly to previous works on the LLM adaptation to the speech modality Wu et al. (2023); Fathullah et al. (2023), our training process comprises two stages: an alignment of the speech encoder output with the LLM token embedding space, and an integrated optimization of the complete model with the LLM. An attempt to omit either of the two stages in our process leads to the lack of training convergence. We hypothesize that the different training stages help the Adaptor to learn different subtasks like segmentation, ordering, and the actual token embedding prediction.\n\nAt the first stage of the training,  is projected to the LLM tokens\u2019 logits using the frozen output linear layer of the LLM (which is often a transposed token embedding layer), and the Connectionist Temporal Classification (CTC) loss Graves et al. (2006) is minimized between the LLM token probabilities obtained from the token logits and the transcription: where the mapping  removes repeated and blank tokens according to the CTC definition, is the transposed weight matrix of the token embedding layer, is the dimensionality of the embedding, and  is the number of tokens in the LLM\u2019s vocabulary.\n\nAt the second stage,  is concatenated with the token embeddings of the prefix and postfix parts of a text prompt. This joint sequence is then passed through the self-attention layers of the LLM and projected with the transposed token embedding weight matrix (also serving as the output layer of the LLM) to obtain the LLM prediction. The Cross-Entropy (CE) loss is minimized between the prediction of the LLM for this sequence and the expected LLM output.\n\nIn the case of the speech recognition task, we set the prompt prefix and postfix to \"Repeat the sentence: \" and \". \" respectively: where  denotes the self-attention layers of the LLM. In the case of the multi-instructional training, prompts are sampled from a predefined hand-crafted collection, while the expected output is set to the output of the LLM for the same prompt using the token embeddings of the ground truth transcription instead of the Adaptor output : where  and  are the prefix and postfix texts of the -th prompt in the prompts collection, is a random number drawn from a uniform distribution over all natural numbers between 1 and , and  is the number of prompts in the collection."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training and Validation Data",
            "text": "The Adaptor training is performed on the entire training FLEURS dataset Conneau et al. (2023  ###reference_b8###) and a subset of the Common Voice Corpus 12.0 Ardila et al. (2020  ###reference_b1###) training dataset with the total amount of 993,660 utterances or 1905 hours of recordings. The Common Voice subset is constructed by selection of up to 25 hours of recordings for each language. Our validation set is the validation set of FLEURS with the total amount of 34,044 utterances or 115 hours of recordings. All transcriptions are taken in an unnormalized format with the true casing and punctuation. Multi-instructional training labels are synthesized with prompts from the P3 collection Sanh et al. (2022  ###reference_b29###). The P3 collection is selected because it was employed in the finetuning process of transitioning BLOOM into BLOOMZ. Our objective is to ensure consistent output for both speech and text inputs. To achieve this, we generate text outputs utilizing prompts from the P3 collection, with which the BLOOMZ model is already acquainted. We apply six distinct randomly drawn prompts to a transcription of each original utterance and assign two generated outputs to each of the three speed-perturbed versions of that utterance. The outputs are generated with a greedy search and maximum length of 128 tokens."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation Data and Metrics",
            "text": "We evaluate our model on the following established benchmarks:\nFLEURS Conneau et al. (2023), MLS Pratap et al. (2020)\nand VoxPopuli Wang et al. (2021a) for the ASR, CoVoST 2 Wang et al. (2021b) for the SLT,\nSpeechGLUE Ashihara et al. (2023) for the spoken General Language Understanding (GLUE) and\nSpeechXNLI for the multilingual NLI222Following SpeechGLUE,\nwe synthesize a speech version of the XNLI Conneau et al. (2018) validation subset\nusing the IMS Toucan Lux et al. (2022) text-to-speech toolkit: https://zenodo.org/records/10900287.\nThe results are evaluated using the corresponding metrics:\nWord Error Rate (WER) for the ASR,\nBLEU333Using the SacreBLEU tool Post (2018). Papineni et al. (2002) for the SLT,\nMatthews Correlation Coefficient (MCC) for the CoLA task within SpeechGLUE,\nand accuracy for the other SpeechGLUE tasks and the SpeechXNLI.\nWhisper normalization\nis applied for both reference and hypothesis before evaluating WER in the ASR experiments."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "Our model is implemented using ESPnet2 Watanabe et al. (2021) version 202304 and Hugging Face Transformers Wolf et al. (2020) version 4.31.0. We use weighted-sum of hidden states Yang et al. (2021); Chang et al. (2021) of the MMS 1B-ASR-All pretrained model Pratap et al. (2023) as speech features. We discard all language-specific adapters and heads of the MMS 1B-ASR-All model to simplify the implementation while preserving the multilingual properties of our system.\n\nThe Adaptor module is a VGG/E-Branchformer based encoder Kim et al. (2023) combined with a convolutional Length Adaptor Li et al. (2021). The E-Branchformer encoder is configured with 17 layers, each with 2048 hidden units, 8 attention heads, and output dimension of 1024. The Convolutions to Gated MultiLayer Perceptron module has 8192 units and the convolution kernel size is 31. The Length Adaptor module contains a 1-dimensional convolutional layer with stride 2 and reduces the length of input sequence by factor of 2. Self-conditioning on language identity Chen et al. (2023) is applied during the CTC training.\n\nThe LLM in our experiments is BLOOMZ 7.1B model Muennighoff et al. (2023), which itself is BLOOM 7.1B LLM Scao et al. (2022) finetuned on the xP3 dataset introduced with BLOOMZ. The total number of parameters in our model is 8.6 billion, the number of trainable parameters is 536 million. We apply 8-bit quantization Dettmers et al. (2022) to the LLM using the functions from the bitsandbytes package version 0.41.1.\n\nThe training is done with the Adam optimizer Kingma and Ba (2015) with the warmup learning rate scheduler with the maximum learning rate of and a weight decay of. 3-way speed perturbation Ko et al. (2015) data augmentation method is applied to the training data. The training stage one, CTC loss training, is performed on two NVIDIA RTX A6000 GPUs with the global batch size of 7.29 minutes. The number of warmup steps for the learning rate scheduler is set to 25,000. A checkpoint is saved every 23,364 steps and evaluated on the validation dataset. The training is stopped after four consecutive evaluations showing no improvement, it takes 233,640 update steps or 120 hours of training time to reach this condition.\n\nThe training stage two, CE loss training, is performed on four NVIDIA RTX A6000 GPUs with the batch size of 37.50 seconds and a gradient accumulation over two batches. The number of warmup steps for the learning rate scheduler is set to 10,000. A checkpoint is saved every 54,381 steps and evaluated on the validation dataset. The training is stopped after four consecutive evaluations showing no improvement. To reach this condition, it takes 652,572 update steps or 132 hours of training on the transcription targets, 2,664,669 update steps or 686 hours on the multi-instructional targets, and 2,501,526 update steps or 644 hours on the combined set of targets. A checkpoint with the highest validation token prediction accuracy from the second step is used for the zero-shot evaluations.\n\nWe decode with the beam search of size 5 and set the maximum output sequence to 192 tokens to obtain the model predictions for the ASR and SLT evaluations. The GLUE and NLI evaluations restrict the output to the possible answer options corresponding to a task and limits the beam size and maximum output sequence respectively. For example, for a yes/no question the possible outputs are yes or no, the beam size is 2 and the maximum output sequence is 1. All evaluations are executed on one NVIDIA RTX A6000 GPU."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Multitasking",
            "text": "Table 1 presents evaluation results of our model across various speech processing tasks, including multilingual ASR, SLT, spoken GLUE, and multilingual NLI. These evaluations test three versions of the model, which are trained using different training targets: transcription only (T), Multi-Instruction (MI), and a combination of both (TMI).\n\nWhen the model is trained solely on the transcription task, it achieves good performance for the ASR task itself. However, this specialized training does not generalize well to more sophisticated tasks like SLT, GLUE, or NLI, as evidenced by the notably lower performance metrics.\n\nOn the other hand, training the model on MI synthetic targets shows significant improvement in performing other tasks such as SLT, GLUE, and NLI. The BLEU score for SLT, for example, increases to 14.1 and the average accuracy/MCC score for GLUE rises to 54.4.\n\nCombining both transcription and MI targets enables the model to perform well across all tested tasks. In addition to maintaining strong performance in ASR, this training configuration also leads to improvements in two out of the three non-ASR tasks. These results underscore the benefits of integrating ASR and MI targets."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Speech Recognition",
            "text": "Table 2 presents a comparative analysis of ASR performance for the BLOOMZMMS model with the T, MI, and TMI training targets. Results are further divided based on whether the languages were seen during the training of the BLOOM model or not.\n\nFor languages that were part of the BLOOM model training, the TMI model generally performs better than the T model. The opposite is true for the non-BLOOM languages. This is expected as training on the MI targets puts stronger stress on the distillation of the LLM knowledge and its encoding to the Adaptor parameters. This effect is more pronounced on the MLS and VoxPopuli datasets, which represent recording conditions and linguistic content slightly different from our training data.\n\nNevertheless, both T and TMI BLOOMZMMS models perform comparably on the in-domain FLEURS dataset independently from the language, suggesting that the Adaptor can effectively leverage the outputs of the MMS speech encoder in order to compensate for the lack of language familiarity by the LLM.\n\nFollowing the MMS paper, we separate a subset of FLEURS testing dataset for the 54 languages that are supported by the Whisper model, and compare the results of the BLOOMZMMS TMI model to the results of the multi-domain MMS (1B) and Whisper large-v2 models. The MMS model is essentially the same speech encoder as used by BLOOMZMMS, but with a number of language-specific components, namely adapter parameters, output vocabulary, and n-gram model utilized during decoding.\n\nDespite removal of the language-specific components and addition of other speech processing tasks, such as SLT, BLOOMZMMS manages to keep the ASR performance on a comparable level to the original MMS model. While also being a multitask model, BLOOMZMMS outperforms the other strong multitask alternative, Whisper large-v2, by a large margin on this massively multilingual low-resource ASR benchmark, albeit potentially due to being trained on in-domain data, in contrast to Whisper."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Speech Translation",
            "text": "Table 7 presents the zero-shot evaluation results for SLT using the CoVoST 2 dataset. The BLOOMZ LM exhibits a nascent ability to translate languages that it has not been trained on, and when this knowledge is transferred to the speech modality, there\u2019s only a minor loss in accuracy. Interestingly, the performance gap between the BLOOMZMMS model and gold transcriptions is more pronounced for the BLOOM languages. This indicates that the quality of knowledge transfer from text to speech depends on the initial linguistic knowledge in the text-based LLM. Consequently, weaknesses present in the LLM tend to amplify when transferred to the speech modality, suggesting that the proposed method might benefit from some form of regularization to mitigate this effect.\n\nFigure 3 shows the comparison of the BLOOMZMMS TMI model with the previous works, XLS-R/mBART and Whisper large-v2, for the XEn translation direction. XLS-R/mBART is a strong baseline, which is finetuned on complete CoVoST 2 training data. Whisper large-v2 has not seen any CoVoST 2 data during training, but has been supervised by a large amount of other speech translation data. BLOOMZMMS TMI has not been exposed to any gold labeled speech translation samples during training. Remarkably, the zero-shot BLOOMZMMS model outperforms the supervised task-specific XLS-R/mBART model for the languages previously seen during BLOOM training. This impressive result is primarily due to the strong performance of the BLOOMZ LLM, which is successfully transferred to the speech modality via the multi-instructional training. However, there is a notable gap with the multitask Whisper large-v2 model, primarily attributed to the poor performance on unseen languages of the LLM we utilize.\n\nIn order to expand language coverage, we evaluate our model for the SLT performance on the FLEURS dataset as well, and present the results in Table 4. As suggested by Radford et al. (2023), we use target language transcriptions for the sentences with the same ID as reference translations. Our evaluation does not include Afrikaans, because the version of the dataset we use does not include any sentence IDs shared between Afrikaans and English. The multilingual properties of the BLOOMZ model, which serves as a decoder of our model, enable us to report the SLT results with non-English target languages as well, for the first time on the FLEURS dataset to the best of our knowledge. The results confirm the good transferability of translation capabilities from text to speech modality with the MI and TMI training targets for a wider range of languages seen in the BLOOM training data. The fair translation performance from unseen languages to English, as observed in the CoVoST 2 dataset, can also be seen across a wider range of languages in the FLEURS dataset."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Spoken Language Understanding",
            "text": "Tables 5 and 6 provide the results of zero-shot evaluation of BLOOMZMMS models on spoken GLUE tasks in English using the SpeechGLUE dataset and on spoken NLI tasks in multiple languages using the SpeechXNLI dataset. It is worth noting that the combined TMI training targets result in better performance on the English GLUE tasks, but have a mixed impact on the NLI tasks based on the languages trained in BLOOM and those that were not. For the BLOOM languages, the TMI model equals the MI-only model in accuracy, whereas it performs worse on the non-BLOOM languages. Together with the SLT results, this observation again hints at the effect of the LLM\u2019s weaknesses amplification during the transfer from the text to speech modality."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Visual Analysis",
            "text": "Following the example of Fathullah et al. (2023), we display the cosine similarity between the text and speech embeddings for the three variants of BLOOMZMMS for a French and a Finnish utterance from the FLEURS evaluation dataset (Figure 4). Consistent with the objective metrics from our experiments, the model trained on the transcription targets shows the noisiest alignments for both languages, while the MI training targets offer better alignment for a language unseen by BLOOM and the combined training targets work better for a language seen by BLOOM."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper we present BLOOMZMMS, a multilingual multitask speech processing model that combines a multilingual LLM and a pretrained multilingual speech encoder. Our investigation into two training strategies revealed their combined efficacy in a broad spectrum of spoken language processing tasks, a conclusion bolstered by zero-shot evaluations on multiple benchmarks."
        }
    ],
    "url": "http://arxiv.org/html/2404.10922v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.10922v1",
        "paper_title": "Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training",
        "research_background": "### Paper's Motivation\nThe motivation for this paper arises from the challenges faced in utilizing Self-Supervised Learning (SSL) in speech-related tasks, especially in a multilingual context. While there have been significant advancements in SSL for speech, most models use conventional filter bank features, necessitating large datasets and considerable computational resources. Furthermore, many existing models primarily focus on English and simple transcription tasks, thereby not fully leveraging the linguistic knowledge stored in large language models (LLMs) for various languages and more complex tasks. This paper aims to address these gaps by developing a method that effectively fuses a multilingual large language model with a multilingual speech encoder.\n\n### Research Problem\nThe research problem addressed by the paper is the effective training of a model that can understand multilingual speech and deploy the linguistic knowledge inherent in large multilingual LLMs across multiple languages and tasks. Specifically, there is a need to integrate a multilingual LLM with a multilingual speech encoder in a way that reduces the dependence on large datasets and extends the model's capability beyond mere transcription, allowing it to perform tasks such as Automatic Speech Recognition (ASR), Spoken Language Translation (SLT), and multilingual spoken Natural Language Inference (NLI).\n\n### Relevant Prior Work\n- **SSL in Language Modeling and Speech:** Previous literature has shown that SSL can be successfully applied to language modeling (Radford et al., 2019) and speech (Baevski et al., 2020), with significant progress being made in reducing the need for labeled data in speech applications (Chang et al., 2021; Yang et al., 2021).\n- **Incorporation of LLMs in Speech Processing Models:** Recent studies have begun to use decoder-only LLMs for various speech processing tasks. For instance, Wu et al. (2023) employed the LLaMA-7B LLM for speech translation, while Ling et al. (2023) adopted the GPT2 XL LLM for speech recognition.\n- **Use of SSL pretrained Speech Encoders:** The rarity of employing SSL-pretrained decoders in speech models has been slightly addressed by researchers like Li et al. (2023) who used HuBERT-Large as a pretrained speech encoder for long-form English speech recognition.\n- **Multilingual Speech Models:** Fathullah et al. (2023) and Nachmani et al. (2023) have adopted multilingual approaches but still rely heavily on large datasets and specific language domains, primarily English.\n  \nThis body of work indicates the need to develop a more efficient method that can generalize well across multiple languages and tasks while utilizing the strengths of multilingual LLMs and SSL-pretrained speech encoders, which is what this paper aims to achieve with BLOOMZMMS.",
        "methodology": "**Methodology Overview:**\n\nThe proposed method aims to facilitate the understanding of multilingual speech by a multilingual Large Language Model (LLM) through a multi-instructional training approach. The architecture and training process are as follows:\n\n### Model Components:\n\n1. **Pretrained Speech Encoder**: A model that processes speech recordings into an intermediate representation.\n2. **Adaptor Module**: A key component that maps the output of the speech encoder to the latent space of the text token embeddings of the LLM.\n3. **Large Language Model (LLM)**: A pre-trained language model which remains frozen during the training of the adaptor module.\n\n### Training Process:\n\nThe methodology focuses on training the Adaptor module using paired data consisting of speech recordings and their corresponding text transcriptions. The training process is divided into two main stages:\n\n1. **First Stage: CTC Loss Alignment**\n\n- **Projection and Logits Computation**: The output of the Adaptor  is projected to the LLM tokens\u2019 logits using the frozen output linear layer of the LLM.\n- **Minimizing CTC Loss**: The Connectionist Temporal Classification (CTC) loss is minimized between the LLM token probabilities derived from these logits and the transcription.\n\n2. **Second Stage: Integrated Optimization**\n\n- **Concatenation with Prompts**: At this stage,  (output from Adaptor) is concatenated with the token embeddings of a text prompt prefix and postfix.\n- **Pass Through LLM**: This combined sequence is passed through the self-attention layers of the LLM.\n- **Cross-Entropy Loss Minimization**: The output sequence is then projected via the transposed token embedding weight matrix  to obtain the LLM prediction, and the Cross-Entropy (CE) loss is minimized between this prediction and the expected LLM output.\n    - For speech recognition tasks, pre-defined prompts like \"Repeat the sentence: \" and \".\" are used to create the sequence.\n\n### Multi-Instructional Training:\n\nPrompts for multi-instructional training are sampled from a predefined handcrafted collection. The expected output in this context is generated by the LLM when using ground truth transcription token embeddings instead of the Adaptor output.\n\n### Innovations:\n\n1. **Adaptor Module**: The inclusion of an Adaptor module that bridges the gap between the speech encoder output and the LLM text token embeddings is crucial.\n2. **Two-Stage Training**: The method emphasizes a sequential training regimen to ensure effective learning, which is hypothesized to aid in learning different subtasks, such as segmentation, ordering, and token embedding prediction.\n3. **Multi-Instructional Training**: By using a variety of prompts and multi-instructional data, the methodology aims to enhance the model's robustness and versatility in understanding multilingual inputs.\n\nThis comprehensive training approach ensures that the Adaptor module effectively maps speech to text embeddings, allowing the LLM to accurately interpret multilingual speech.",
        "main_experiment_and_results": "### Main Experiment Setup\nThe primary experiment involves training an adaptor on a combination of two datasets: the entire FLEURS training dataset and a subset of the Common Voice Corpus 12.0 training dataset. The combined dataset consists of 993,660 utterances amounting to 1905 hours of recordings. Specifically, the Common Voice subset includes up to 25 hours of recordings for each language.\n\nThe validation set used is the FLEURS validation set, which consists of 34,044 utterances or 115 hours of recordings. Transcriptions within the datasets are unnormalized, maintaining true casing and punctuation.\n\nMulti-instructional training labels are synthesized using prompts from the P3 collection. The P3 collection is chosen due to its role in fine-tuning the BLOOM model into BLOOMZ, which the experiment aims to leverage for generating consistent outputs for both speech and text inputs. For the creation of training data, prompts from the P3 collection are applied to transcriptions of the original utterances. Six distinct prompts are used per transcription, and two generated outputs are associated with each of three speed-perturbed versions of that utterance. The text outputs are produced using greedy search with a maximum length of 128 tokens.\n\n### Baselines\nWhile the text does not specify comparison baselines explicitly, it implies the usage of a pre-existing model (BLOOMZ) as the standard for generating text outputs.\n\n### Evaluation Metrics\nThe paper does not explicitly mention the exact evaluation metrics used to assess performance in the main experiment. However, given the context of text outputs and multi-instructional training, common evaluation metrics in similar setups typically include:\n- BLEU Score: to measure the correspondence between generated and reference texts.\n- WER (Word Error Rate): for evaluating the accuracy of speech-to-text transcription.\n\n### Main Experimental Results\nThe main experimental results are not described in detail within the provided text. To fully understand the outcomes, direct reference to the original paper would be necessary. Typically, such experiments would present improvements in BLEU scores, reductions in WER, or other metrics indicating better consistency between generated text outputs and transcriptions derived from speech inputs."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the performance of the model across various speech processing tasks using different training targets to understand the effect of different training strategies.",
            "experiment_process": "The model is evaluated across multiple tasks: multilingual ASR, SLT, spoken GLUE, and multilingual NLI. Training targets include transcription only (T), Multi-Instruction (MI), and a combination of both (TMI). The performance metrics include CER for ASR, BLEU for SLT, and accuracy/MCC for GLUE.",
            "result_discussion": "Training on transcription-only (T) yields good ASR performance but poor generalization to other tasks (SLT, GLUE, NLI). MI-only training improves non-ASR tasks but significantly drops ASR performance. Combining both targets (TMI) results in balanced performance across all tasks, highlighting the benefits of integrating ASR and MI targets.",
            "ablation_id": "2404.10922v1.No1"
        },
        {
            "research_objective": "Analyze the comparative ASR performance of the BLOOMZMMS model with different training targets and under varying conditions such as language familiarity.",
            "experiment_process": "ASR performance is evaluated for BLOOMZMMS model versions trained with T, MI, and TMI targets. Performance is divided based on whether languages were seen during BLOOM model training. Datasets include MLS, VoxPopuli, and FLEURS. Comparison is made with MMS (1B) and Whisper large-v2 models on the FLEURS dataset.",
            "result_discussion": "For languages seen during BLOOM training, TMI performs better than T. The opposite is true for unseen languages, especially on MLS and VoxPopuli datasets. Despite the removal of language-specific components, BLOOMZMMS keeps ASR performance comparable to MMS and outperforms Whisper large-v2 on the FLEURS dataset.",
            "ablation_id": "2404.10922v1.No2"
        },
        {
            "research_objective": "Assess the zero-shot evaluation performance of the BLOOMZMMS model in speech translation (SLT) and compare with existing models.",
            "experiment_process": "Zero-shot evaluation on the CoVoST 2 dataset. Comparison with XLS-R/mBART and Whisper large-v2. Evaluation also conducted on the FLEURS dataset for a wider range of languages. Reference translations use target language transcriptions for the sentences with the same ID.",
            "result_discussion": "BLOOMZMMS TMI outperforms the supervised task-specific XLS-R/mBART model for languages seen during BLOOM training. However, there\u2019s a performance gap with Whisper large-v2, especially on unseen languages. Good SLT performance observed for BLOOM languages and fair performance for unseen languages on both CoVoST 2 and FLEURS datasets.",
            "ablation_id": "2404.10922v1.No3"
        },
        {
            "research_objective": "Evaluate the zero-shot performance of BLOOMZMMS models on spoken language understanding tasks using the SpeechGLUE and SpeechXNLI datasets.",
            "experiment_process": "Zero-shot evaluation on spoken GLUE tasks in English using SpeechGLUE dataset and on spoken NLI tasks in multiple languages using SpeechXNLI dataset. Performance compared between models trained with T, MI, and combined TMI targets.",
            "result_discussion": "TMI training improves performance on English GLUE tasks but has mixed results on NLI tasks. For BLOOM languages, TMI performs equally well as MI; however, it performs worse on non-BLOOM languages. The results suggest LLM weaknesses are amplified during text-to-speech transfer.",
            "ablation_id": "2404.10922v1.No4"
        }
    ]
}