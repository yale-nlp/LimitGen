{
    "title": "Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts",
    "abstract": "Although Large Language Models (LLMs) are effective in performing various NLP tasks, they still struggle to handle tasks that require extensive, real-world knowledge, especially when dealing with long-tail facts (facts related to long-tail entities). This limitation highlights the need to supplement LLMs with non-parametric knowledge. To address this issue, we analysed the effects of different types of non-parametric knowledge, including textual passage and knowledge graphs (KGs). Since LLMs have probably seen the majority of factual question-answering datasets already, to facilitate our analysis, we proposed a fully automatic pipeline for creating a benchmark that requires knowledge of long-tail facts for answering the involved questions. Using this pipeline, we introduce the LTGen benchmark.\n\nOur experiments show that LLMs alone struggle with answering these questions, especially when the long-tail level is high or rich knowledge is required. Nonetheless, the performance of the same models improved significantly when they were prompted with non-parametric knowledge. We observed that, in most cases, prompting LLMs with KG triples surpasses passage-based prompting using a state-of-the-art retriever. In addition, while prompting LLMs with both KG triples and documents does not consistently improve knowledge coverage, it can dramatically reduce hallucinations in the generated content.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs), such as GPT-4, LLaMA, and PaLM2, have shown impressive ability in conversational search. While these LLMs can generate knowledgeable responses, they can still suffer from hallucinations in Knowledge-Intensive Generation (KIG) tasks, in particular for cases involving long-tail entities.\n\nPrompting LLMs with non-parametric knowledge has been recently proposed as a method for mitigating this issue. Mallen et al. investigated the effectiveness of prompting LLMs with non-parametric knowledge, showing its effectiveness when coupled with the parameterized knowledge of LLMs for long-tail question answering. Their investigation was conducted on the PopQA dataset, which is constructed from Knowledge Graph (KG) triples using predefined templates. While their study exhibited promising results for motivating further research in this space, we believe it was also subjected to certain limitations: the template-based dataset creation process limits the predicates (relations) of the selected triples to be investigated; each question in PopQA is constructed from a single entity-relation pair; the authors only considered unstructured, non-parametric knowledge, while ignoring any type of structured, non-parametric knowledge, for example the one included in KGs.\n\nIn this work, we propose a fully automatic, template-free pipeline for building long-tail generation benchmarks. Using the proposed pipeline, we introduce the Long Tail Generation (LTGen) benchmark. This benchmark comprises two tasks: question answering (LTGen-QA) and conversational QA (LTGen-Conv). Compared to PopQA, LTGen: does not rely on templates; contains questions constructed from multiple relations; and provides a more challenging task of conversational QA.\n\nThe key research questions that motivate this work are as follows:\nRQ1: How do LLMs perform in knowledge-intensive generation tasks that involve different kinds of factual knowledge?\nRQ3: How LLMs benefit from different formats of non-parametric knowledge (structured and unstructured) with different kinds of factual knowledge?\n\nOur evaluation demonstrates that LLMs equipped solely with their parametric knowledge exhibit poor performance when tasked with answering questions involving long-tail facts. Their performance declines as both the long-tail level of the relevant entities and the required amount of knowledge (number of reference triples) increases. Prompting LLMs with non-parametric knowledge can significantly benefit performance in long-tail question-answering. Such benefits are even more evident in the case of small language models. Furthermore, our results show that KG triples can serve as a powerful and robust source of non-parametric knowledge while offering a more efficient processing paradigm than the unstructured nature of textual passages. Moreover, the amalgamation of both structural knowledge from KGs and unstructured knowledge from passages can reduce hallucination tendencies of the involved LLMs, leading to the highest NLI-based metric scores across various settings."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Typical KBs, such as Wikidata [10  ###reference_b10###], comprise several key elements. The basic unit is entity, which forms the cornerstone of a KB. Subsequently, concepts and attributes come completing the details of entities:\nthe former are abstractions representing a group of entities, whereas the latter provide literal and descriptive information about a specific entity where each attribute is defined by a key and a value. Furthermore, relations are a sine qua non component of KB as they establish the relationships between entities or concepts. For instance, entities are connected to concepts via the \u2019instance of\u2019 relation, and concepts are systematically arranged into a hierarchical tree through the \u2019subclass of\u2019 relation.\nThe knowledge within a KB is encapsulated in three forms:\nRelational Knowledge: Structured as triples where each triple consists of two entities linked by a relation.\nLiteral Knowledge: Structured as triples where an entity is associated with an attribute key and an attribute value\nQualifier Knowledge extends these basic triples, whose head is a relational or literal triple. A qualifier also has a key and value [11  ###reference_b11###, 12  ###reference_b12###].\nKBQA is a long-studied classical NLP task and has gained widespread application in recent times due to large language models (LLMs) [13  ###reference_b13###]. Unlike approaches that depend on parametric knowledge, KBQA leverages structured knowledge bases to derive answers. Complex KBQA systems can be broadly classified into two categories: Semantic Parsing (SP)-based [14  ###reference_b14###] and Information Retrieval (IR)-based techniques [15  ###reference_b15###]. Each category employs distinct mechanisms to tackle the KBQA task. SP-based methods convert a question into a symbolic logical form, which is then executed on the KB to extract precise answers. In contrast, IR-based approaches focus on gathering comprehensive information related to the query, ranking entities and relations by their relevance to the posed question. However, an increasing trend in the field is moving away from the traditional single-turn KBQA approach towards a more conversational setting due to the notable shift in the industrial applications [16  ###reference_b16###].\nComplex questions are distinguished by their complex query types and compositional semantics. This complexity presents significant challenges in linguistic analysis. To make complex problems clearer, it is good practice to convert questions from their unstructured forms (i.e. natural text) to a more structural or logical representation, such as SQL [17  ###reference_b17###, 18  ###reference_b18###]. To bridge natural language questions with their corresponding logical expressions, methodologies like NSQA [19  ###reference_b19###] adopt Abstract Meaning Representation (AMR) as an intermediary logical form. This approach aligns the elements of a question, such as entities, relations, entity types, and attributes, with their syntactic counterparts, enhancing the coherence between these components and syntax. The NSQA model initially parsed the question into a rooted, directed, and acyclic AMR graph. Subsequently, it involved entity linking within the schema ground phase, aligning the entity nodes in the AMR graph with KBs. Building on these aligned entities, NSQA employs a path-expanding methodology to convert the AMR graph into a predefined query graph. This graph could then be converted into executable logical expressions easily [15  ###reference_b15###, 20  ###reference_b20###].\nTraditional KIG benchmarks as KILT [5  ###reference_b5###] do not especially focus on the long-tail problem, thus having very few long-tail examples in their corresponding datasets.\nTable 6  ###reference_### in A  ###reference_### reveals the performance of GPT3.5 and RE2G (i.e. a state-of-the-art retrieval-augmented generation method) [21  ###reference_b21###] in NQ and TQA benchmarks. While being totally devoid of any non-parametric memory such as KG, GPT3.5 outperforms RE2G on the TQA dataset by a significant margin. However, it remains unclear whether LLMs actually encode the factual knowledge [22  ###reference_b22###] required to answer TQA questions, or simply memorize the question-answer pairs in the datasets [23  ###reference_b23###]. We remark importantly that the recent LLMs (which are continuously upgraded) easily outmatch the state-of-the-art on initially released datasets.\nThis strengthens the need of challenging benchmarks for robust, fair and ethical evaluation of emerging LLMs.\nPopQA [6  ###reference_b6###] is a benchmark used to assess the performance of language models in addressing queries pertaining to long-tail entities. The questions are generated from triples with predefined templates for a fixed set of relations. Limited by these predefined templates, PopQA only contains limited relations. This drawback limits PopQA\u2019s ability to evaluate LLMs for diverse relation types that today\u2019s users would ask."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "LTGen: Long-Tail Generation Tasks",
            "text": "To address the aforementioned limitations, we introduce a novel pipeline to automatically generate a high-quality KIG dataset. We further release the resulting Long-Tail Generation (LTGen) benchmark which includes two datasets: LTGen-QA and LTGen-Conv. The former gathers simple question answering samples while the latter involves multiple relations in a multi-turn scenario. Table 1  ###reference_### compares our benchmark to other relevant works, showing its completeness across a set of different criteria.\n###table_1### Determining whether an entity can be classified as long-tail is not only a complex task, but also highly subjective. To this end, we seek to establish a clear and universally-accepted definition which ensures a precise categorization of any involved entities. We follow the assumption by [6  ###reference_b6###] which considers an entity as rare if its corresponding Wikipedia page is among the least visited ones. Thus, for each Wikidata entity, we use the Wikipedia Pageview API111https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews  ###reference_ics/AQS/Pageviews### to get the average monthly Wikipedia Page View (WPV) to determine the popularity of an entity. To generate the LTGen benchmark, we first randomly sample entities from Wikidata. Then, we select long-tail entities based on their popularity using their respective WPV222Average monthly Wikipedia page viewing count from 1 Jan 2021 to 1 Jan 2023..\nUsing the sampled long-tail entities, we design SPARQL queries to retrieve relevant triples, i.e. triples where the entities appear either in the subject or the object position. Nevertheless, accumulating all of those triples may not be judicious because of the non-informative relations as \u201cfamily name\u201d or \u201cgiven name\u201d in Wikidata. As a result, to prevent the generation of trivial questions like \u201cWhat is the family name of Joe Biden?\u201d, we remove triples with these non-informative relations.\nNext, we filter out any entities appearing in less than 5 triples or more than 100 triples. Removing the former makes the task more challenging, while removing the latter due to\nconsideration of the efficiency of prompting.\nThe generation of a new dataset is usually carried out by leveraging predefined templates, LLM-based prompting or combinations thereof. With the recent breakthrough achieved in prompt engineering, we set our data generator as GPT-4. We use GPT-4 to create PopQA-like samples where each question is constructed from an entity-relation pair. In such a template-free setting, we can fascilitate the inclusion of more complex and diverse QA samples that consider more relation types. Moreover, we further prompt GPT-4 to generate dialogues from the collected triples to build LTGen-Conv. This enables us to annotate automatically a generated dialogue against gold triples that are relevant to its textual content. Figure 1  ###reference_### shows the overall workflow of generating the LTGen benchmark. The final prompts for LTGen-QA and LTGen-Conv construction are detailed in B  ###reference_###.\nTable 2  ###reference_### shows the overall statistics of the proposed LTGen benchmark. We additionally define four long-tail levels, identified as level I, II, III, and IV, which are based on the count of WPV, with the level IV being the highest degree of rarity.\nNote that LTGen-Conv contains questions that require multiple relations to answer, allowing us to evaluate LLMs from that perspective. Therefore, we further split LTGen-Conv into four subsets with regard to the number of reference triples, which are 1) single reference triple with 10,330 samples; 2) two reference triples with 1,720 samples; 3) three reference triples with 590 samples and 4) more than three reference triples with 694 samples."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Benchmark Construction",
            "text": "We propose an automatic data generation pipeline with the help of LLMs. This is inspired by many recent works that have looked into using LLMs to generate high-quality datasets for training smaller models [28  ###reference_b28###, 29  ###reference_b29###, 30  ###reference_b30###]. The pipeline is composed of three stages: Long-tail entity selection, triples retrieval and sample generation. We hereinafter detail the process for each step.\n###table_2### Determining whether an entity can be classified as long-tail is not only a complex task, but also highly subjective. To this end, we seek to establish a clear and universally-accepted definition which ensures a precise categorization of any involved entities. We follow the assumption by [6  ###reference_b6###  ###reference_b6###] which considers an entity as rare if its corresponding Wikipedia page is among the least visited ones. Thus, for each Wikidata entity, we use the Wikipedia Pageview API111https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews  ###reference_ics/AQS/Pageviews###  ###reference_ics/AQS/Pageviews### to get the average monthly Wikipedia Page View (WPV) to determine the popularity of an entity. To generate the LTGen benchmark, we first randomly sample entities from Wikidata. Then, we select long-tail entities based on their popularity using their respective WPV222Average monthly Wikipedia page viewing count from 1 Jan 2021 to 1 Jan 2023..\nUsing the sampled long-tail entities, we design SPARQL queries to retrieve relevant triples, i.e. triples where the entities appear either in the subject or the object position. Nevertheless, accumulating all of those triples may not be judicious because of the non-informative relations as \u201cfamily name\u201d or \u201cgiven name\u201d in Wikidata. As a result, to prevent the generation of trivial questions like \u201cWhat is the family name of Joe Biden?\u201d, we remove triples with these non-informative relations.\nNext, we filter out any entities appearing in less than 5 triples or more than 100 triples. Removing the former makes the task more challenging, while removing the latter due to\nconsideration of the efficiency of prompting.\nThe generation of a new dataset is usually carried out by leveraging predefined templates, LLM-based prompting or combinations thereof. With the recent breakthrough achieved in prompt engineering, we set our data generator as GPT-4. We use GPT-4 to create PopQA-like samples where each question is constructed from an entity-relation pair. In such a template-free setting, we can fascilitate the inclusion of more complex and diverse QA samples that consider more relation types. Moreover, we further prompt GPT-4 to generate dialogues from the collected triples to build LTGen-Conv. This enables us to annotate automatically a generated dialogue against gold triples that are relevant to its textual content. Figure 1  ###reference_###  ###reference_### shows the overall workflow of generating the LTGen benchmark. The final prompts for LTGen-QA and LTGen-Conv construction are detailed in B  ###reference_###  ###reference_###.\nTable 2  ###reference_###  ###reference_### shows the overall statistics of the proposed LTGen benchmark. We additionally define four long-tail levels, identified as level I, II, III, and IV, which are based on the count of WPV, with the level IV being the highest degree of rarity.\nNote that LTGen-Conv contains questions that require multiple relations to answer, allowing us to evaluate LLMs from that perspective. Therefore, we further split LTGen-Conv into four subsets with regard to the number of reference triples, which are 1) single reference triple with 10,330 samples; 2) two reference triples with 1,720 samples; 3) three reference triples with 590 samples and 4) more than three reference triples with 694 samples."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Data Quality Checking",
            "text": "To ensure data quality, we randomly select 3% of the LTGen-Conv datasets standing for 400 samples333A relatively high rate compared to related works [31  ###reference_b31###] and we comply with a thorough manual quality check procedure. More specifically, for each selected sample, we hire two annotators to review the associated triple annotations following the instructions detailed in C  ###reference_###. We calculate the frequency of irrelevant triples linked to or missing triples required for a specific dialogue turn. Among the 400 data samples, we found an average of 9 data samples with incorrect annotations, representing an approximate 2.25% error rate. Considering the in-annotator outcome, where correctness is determined only if both annotators concur, we identified 12 data samples, resulting in an approximately 3% error rate. C  ###reference_### provides more details of the data quality checking process."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Non-parametric Memories Collection",
            "text": "We consider two sources of non-parametric knowledge [22] which are (i) unstructured knowledge from passages and (ii) structured knowledge from KGs. The strategies applied to collect those types of knowledge are detailed in the following sections. The primary step consists of identifying the entities in the input query and mapping them to an existing KG. To that end, we utilize the TAGME API [34] to align word spans from the questions to Wikidata entities. We further filter tagged entities with a threshold of 0.22 (following Mallen et al. [6]) for the value computed by TAGME API to serve as the tagged entities for the subsequent steps. If there are more than 5 filtered entities for a query, we only keep the top-5 entities based on the value. Given the extracted Wikidata entities, we design SPARQL queries to be executed on the Wikidata endpoint444https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service to retrieve the relevant KG triples. While the previous step has successfully returned numerous triples, their usefulness toward resolving the query is questionable. Indeed, several triples are unrelated or out-of-order with respect to their relevance to the query. Thus, a ranking of the triples is performed to select the most promising triples to answer the question. We especially view the triples ranking task as a specific instance of the relation linking task. We explore two different strategies including prompting LLMs and an AMR-based method. Regarding the former prompting approach, we feed the LLMs with the user query, the tagged entities, and the relations from the retrieved triples. We then instruct the model to rank the relations in the prompt according to their relevance in answering the given query. Therefore, LLMs generate a textual output which we parse to extract the rank for each relation. The associated prompt can be found in B. In a second phase, we further investigate a semantic-based approach to better capture the semantics of the relations thanks to AMR which has proven to benefit knowledge base question answering [19]. We provide details and elaborate more on AMR approach in Section 4.2.2. Given a query, tagged entity 555 is the identified entity list from Step 1 described in Section 4.2.1, and candidate relation 666 are the candidate relation list from Step 2 described in Section 4.2.1, which is the relation list extracted from all retrieved triples, we define the Entity-Relation Pairs as \u2019[TEXT][ENT][REL]\u2019. [TEXT], [ENT], and [REL] are special tokens inserted following Naseem et al. [36] to identify the input query (), tagged entities, and candidate relations respectively. Then we encode each candidate entity-relation pair with BERT [38]. Following Naseem et al. [36], we first generate the AMR graph of the input question using SPRING [39] and link the annotated entities with BLINK [40]. In order to leverage the rich semantic and graph structure of AMR, we employ a GNN-based encoder. However, since relying on the message-passing scheme, GNNs only learn node embeddings and thus information carried by the labelled edges would be lost. To this end, we transform our labelled relations as nodes (i.e. reification) so that all information of AMR remains. We adopt the Graph Attention Network (GAT) [41] architecture to learn representations for each node. We expect such embeddings to encompass rich contextual and semantic information. Let denote the node embeddings resulting from the AMR encoder (GNN), where each is a -dimensional vector corresponding to the -th AMR graph that is extracted from a sentence. These embeddings are utilized as both keys () and values () in our attention mechanism. Let represent the representations computed by the Entity-Relation Pairs encoder, where each is a -dimensional vector representing the -th entity-relation pair within the sentence, serving as queries () in the attention mechanism. Our AMR-Multihead-Attention (AMA) mechanism is defined as follows: where each attention head, is computed by: The attention function is defined as: In the equations above, , , and are the parameter matrices for the -th attention head for queries, keys, and values, respectively. is a parameter matrix for linearly transforming the concatenated outputs of all heads. is the dimensionality of the key vectors, which is used to scale the dot products in the softmax function, preventing excessively large values that could impede gradient stability. This formulation highlights the dependencies between the AMR and each entity-relation pair. This setup allows for a more dynamic interpretation of entity relations, explicitly influenced by their underlying semantic structures within the sentences."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Unstructured Knowledge from Passages",
            "text": "To obtain unstructured knowledge from passages, we consider the state-of-the-art pre-trained dense retriever Contriever [32 ###reference_b32###]."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Structured Knowledge from KGs",
            "text": "Compared to passages, the knowledge graph triples have several appealing properties for prompting language models. First, the triples are more compact. Indeed, it takes in most cases less than 20 tokens to encode a triple whereas a 100-word passage would necessitate much more tokens. Besides, triples from KG contain less noise. A passage usually contains more information than the required knowledge to answer a question and would mislead LLMs. \n\nThis primary step consists in identifying the entities in the input query and mapping them to an existing KG. To that end, we utilize the TAGME API to align word spans from the questions to Wikidata entities. We further filter tagged entities with a threshold for the value computed by TAGME API to serve as the tagged entities for the subsequent steps. If there are more than 5 filtered entities for a query, we only keep the top-5 entities based on the value. Given the extracted Wikidata entities, we design SPARQL queries to be executed on the Wikidata endpoint to retrieve the relevant KG triples.\n\nWhile the previous step has successfully returned numerous triples, their usefulness toward resolving the query is questionable. Indeed, several triples are unrelated or out-of-order with respect to their relevance to the query. Thus, a ranking of the triples is performed to select the most promising triples to answer the question. We especially view the triples ranking task as a specific instance of the relation linking task. \n\nWe explore two different strategies including prompting LLMs and an Abstract Meaning Representation (AMR)-based method. Regarding the former prompting approach, we feed the LLMs with the user query, the tagged entities, and the relations from the retrieved triples. We then instruct the model to rank the relations in the prompt according to their relevance in answering the given query. Therefore, LLMs generate a textual output which we parse to extract the rank for each relation.\n\nIn a second phase, we further investigate a semantic-based approach to better capture the semantics of the relations thanks to AMR which has proven to benefit knowledge base question answering. We define the Entity-Relation Pairs as \u2019[TEXT][ENT][REL]\u2019. [TEXT], [ENT], and [REL] are special tokens inserted to identify the input query, tagged entities, and candidate relations respectively. Then we encode each candidate entity-relation pair with BERT.\n\nFollowing previous work, we first generate the AMR graph of the input question using SPRING and link the annotated entities with BLINK. In order to leverage the rich semantic and graph structure of AMR, we employ a GNN-based encoder. However, since relying on the message passing scheme, GNNs only learn nodes embeddings and thus information carried by the labelled edges would be lost. To this end, we transform our labelled relations as nodes (i.e., reification) so that all information of AMR remains. We adopt the Graph Attention Network (GAT) architecture to learn representations for each node. We expect such embeddings to encompass rich contextual and semantic information.\n\nLet denote the node embeddings resulting from the AMR encoder (GNN), where each is a -dimensional vector corresponding to the -th AMR graph that extracted from a sentence. These embeddings are utilized as both keys and values in our attention mechanism. Let represent the representations computed by the Entity-Relation Pairs encoder, where each is a -dimensional vector representing the -th entity-relation pair within the sentence, serving as queries in the attention mechanism. Our AMR-Multihead-Attention (AMA) mechanism is defined as follows:\n\nThis formulation highlights the dependencies between the AMR and each entity-relation pair. This setup allows for a more dynamic interpretation of entity relations, explicitly influenced by their underlying semantic structures within the sentences."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 KG Triples Retrieval",
            "text": "In order to retrieve pertinent triples from KG, we adopt the commonly used modular KG triples retrieval pipeline from [33] with different triple-ranking baselines. As depicted in Figure 2(a), the overall architecture of the KG triples retrieval pipeline is divided into three main components detailed hereinafter.\n\nThis primary step consists in identifying the entities in the input query and mapping them to an existing KG. To that end, we utilize the TAGME API [34] to align word spans from the questions to Wikidata entities. We further filter tagged entities with a threshold of 0.22 (following Mallen et al. [6]) for the value computed by TAGME API to serve as the tagged entities for the subsequent steps. If there are more than 5 filtered entities for a query, we only keep the top-5 entities based on the value.\n\nGiven the extracted Wikidata entities, we design SPARQL queries to be executed on the Wikidata endpoint to retrieve the relevant KG triples. While the previous step has successfully returned numerous triples, their usefulness toward resolving the query is questionable. Indeed, several triples are unrelated or out-of-order with respect to their relevance to the query. Thus, a ranking of the triples is performed to select the most promising triples to answer the question.\n\nWe especially view the triples ranking task as a specific instance of the relation linking task. We explore two different strategies including prompting LLMs and an Abstract Meaning Representation (AMR)-based method. Regarding the former prompting approach, we feed the LLMs with the user query, the tagged entities, and the relations from the retrieved triples. We then instruct the model to rank the relations in the prompt according to their relevance in answering the given query. Therefore, LLMs generate a textual output which we parse to extract the rank for each relation. The associated prompt can be found in B.\n\nIn a second phase, we further investigate a semantic-based approach to better capture the semantics of the relations thanks to AMR which has proven to benefit knowledge base question answering [19]. We provide details and elaborate more on AMR approach in Section 4.2.2."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 AMR-Based Triples Ranking",
            "text": "The AMR graph is a semantic abstract representation of natural language. This graph effectively captures the semantic relations in sentences by abstracting away from the syntax and has been proven to be helpful in relation linking tasks. The nodes of AMR represent instances, literals, or concepts extracted from the original sentence. These nodes are derived from either framesets from PropBank, normalized surface forms, or specific concepts from the AMR vocabulary, including special entity types. On the contrary, the edges stand for the relations or roles between these nodes which are labeled based on the corresponding frame or from predefined relations from the AMR set. Thus, a key component which makes AMR extremely powerful is the fine-grained level of relations between concepts.\n\nWe propose to use such a representation to enhance our triple ranking strategy. We frame the ranking task as a binary classification. More specifically, given an entity from the question and a relation from one of the retrieved triples, we aim at predicting whether such a pair is necessary to answer the question or not. We adopt a dual-encoder architecture made of a BERT-based entity-relation pair encoder and a GNN-based AMR encoder. After the fusion of their representation to compute attention scores, a MLP is applied to rank triples. Our proposed architecture consisting of several components is depicted in Figure 2(b) and further detailed below.\n\nGiven a query, tagged entity 555 is the identified entity list from Step 1 described in Section 4.2.1, and candidate relation 666 is the candidate relation list from Step 2 described in Section 4.2.1, which is the relation list extracted from all retrieved triples, we define the Entity-Relation Pairs as \u2018[TEXT][ENT][REL]\u2019. [TEXT], [ENT], and [REL] are special tokens inserted following Naseem et al. to identify the input query, tagged entities, and candidate relations respectively. Then we encode each candidate entity-relation pair with BERT.\n\nFollowing Naseem et al., we first generate the AMR graph of the input question using SPRING and link the annotated entities with BLINK. In order to leverage the rich semantic and graph structure of AMR, we employ a GNN-based encoder. However, since relying on the message passing scheme, GNNs only learn nodes embeddings and thus information carried by the labeled edges would be lost. To this end, we transform our labeled relations as nodes (i.e. reification) so that all information of AMR remains. We adopt the Graph Attention Network (GAT) architecture to learn representations for each node. We expect such embeddings to encompass rich contextual and semantic information.\n\nLet denote the node embeddings resulting from the AMR encoder (GNN), where each is a -dimensional vector corresponding to the -th AMR graph that extracted from a sentence. These embeddings are utilized as both keys () and values () in our attention mechanism. Let represent the representations computed by the Entity-Relation Pairs encoder, where each is a -dimensional vector representing the -th entity-relation pair within the sentence, serving as queries () in the attention mechanism. Our AMR-Multihead-Attention (AMA) mechanism is defined as follows:\n\nwhere each attention head, , is computed by:\n\nThe attention function is defined as:\n\nIn the equations above:\n, , and are the parameter matrices for the -th attention head for queries, keys, and values, respectively.\nis a parameter matrix for linearly transforming the concatenated outputs of all heads.\nis the dimensionality of the key vectors, which is used to scale the dot products in the softmax function, preventing excessively large values that could impede gradient stability.\n\nThis formulation highlights the dependencies between the AMR and each entity-relation pair. This setup allows for a more dynamic interpretation of entity relations, explicitly influenced by their underlying semantic structures within the sentences."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "We evaluate several LLMs on the LTGen benchmark in different settings in order to answer the proposed research questions. Following Asai et al. [42], we formalise both tasks in the LTGen benchmark as zero-shot generation tasks. Hyper-parameters and detailed experimental settings can be found in D. To retrieve passages from external sources, we consider the neural-based Contriever models.\n\nWe choose to use the Wikipedia corpus provided by the KILT benchmark to make sure that the Wikipedia pages of the entities used for creating the LTGen benchmark are included in the passage corpus. Each passage has a maximum length of 100 words. We make use of the MSMARCO fine-tuned contriever checkpoint for better retrieval quality. We retrieve the top-10 passages for each sample of both LTGen-QA and LTGen-Conv.\n\nTo collect the KG triples, we follow the retrieval pipeline as described in Section 4.2.1. In addition to the TAGME API usage to identify potential topic entities, we also designed an oracle setup where the golden topic entities are directly used. For each method, we use triples with top-5 relations for every topic entity for subsequently prompting language models. For prompt efficiency, we randomly select 10 triples within each entity-relation pair.\n\nFollowing Asai et al. [42], we measure three different knowledge matching scores for both LTGen-QA and LTGen-Conv tasks: knowledge matching (KM), exact knowledge matching (eKM) and ratio knowledge matching (rKM). Given the generated prediction and the reference answer entities in the reference triples, these metrics are computed as follows:\n\nIn addition to these knowledge matching scores, we also compute the reference response based metrics for the LTGen-Conv dataset. However, similarly to text summarization, we find that traditional overlap-based metrics, such as BLEU and ROUGE are not ideal for evaluating performance in tasks involving long-form texts. Previous works have shown that Natural Language Inference (NLI) models can serve as a robust evaluator, exhibiting high correlation with human judgements for natural language generation tasks. Thus, we follow Chen and Eger and use entailment scores to measure the generation quality of different methods. For a predicted generation, we use an entailment model to measure whether a model\u2019s generated content can entail the reference response. There are three scores in this measurement: the entailment score (E), the natural score, and the contradiction score (C). An optimal prediction should have a high entailment score and a low contradiction score. We report E-C as the overall score. For the choice of entailment model, we use DeBERTa V3 which has been pre-trained with multiple NLI datasets."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Large Language Models",
            "text": "To achieve zero-shot generation, the selected language models must possess the capability to follow instructions. We consider both closed-source LLMs and open-source LLMs. We select GPT-3.5777 as the proprietary large-scale LLM by calling the OpenAI API. We choose LLaMA2 as our base open-source model. We particularly use the chat fine-tuned version. Furthermore, to probe the significance of the model size on performance, we employ the 7B, 13B, and 70B versions of LLaMA2. We utilize the vLLM library for fast inference for all selected open-source LLMs. In addition, we investigate LLM-based triple ranking with GPT-3.5 and LLaMA2."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Knowledge Sources",
            "text": "To retrieve passages from external sources, we consider the neural-based Contriever models. We choose to use the Wikipedia corpus provided by the KILT benchmark to ensure that the Wikipedia pages of the entities used for creating the LTGen benchmark are included in the passage corpus. Each passage has a maximum length of 100 words. We make use of the MSMARCO fine-tuned contriever checkpoint for better retrieval quality. We retrieve the top-10 passages for each sample of both LTGen-QA and LTGen-Conv.\n\nTo collect the KG triples, we follow the retrieval pipeline as described in Section 4.2.1. In addition to the TAGME API usage to identify potential topic entities, we also designed an oracle setup where the golden topic entities are directly used. For each method, we use triples with top-5 relations for every topic entity for subsequently prompting language models. For prompt efficiency, we randomly select 10 triples within each entity-relation pair."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Prompt Settings",
            "text": "We formalise both tasks (LTGen-QA and LTGen-Conv) in the LTGen benchmark as zero-shot generation tasks. For all LLMs, we format the prompt as a \u201csystem\u201d prompt with a following \u201cuser prompt\u201d. The system prompt is an instruction stating to the model the current corresponding task with its respective constraints. Table 3 illustrates the system prompts which we found to be effective for all the LLMs in both tasks. The user prompt is the input of both external non-parametric knowledge and the dialogue or question."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Metrics",
            "text": "We hereinafter introduce our evaluation metrics to assess the performance of the models. Following Asai et al., we measure three different knowledge matching scores for both LTGen-QA and LTGen-Conv tasks: knowledge matching (KM), exact knowledge matching (eKM), and ratio knowledge matching (rKM). Given the generated prediction and the reference answer entities in the reference triples, these metrics are computed as follows: In addition to these knowledge matching scores, we also compute the reference response based metrics for the LTGen-Conv dataset. However, similarly to text summarization, we find that traditional overlap-based metrics, such as BLEU and ROUGE are not ideal for evaluating performance in tasks involving long-form texts. Previous works have shown that Natural Language Inference (NLI) models can serve as a robust evaluator, exhibiting high correlation with human judgements for natural language generation tasks. Thus, we follow Chen and Eger and use entailment scores to measure the generation quality of different methods. For a predicted generation, we use an entailment model to measure whether a model\u2019s generated content can entail the reference response. There are three scores in this measurement: the entailment score (E), the natural score, and the contradiction score (C). An optimal prediction should have a high entailment score and a low contradiction score. We report E-C as the overall score. For the choice of entailment model, we use DeBERTa V3 which has been pre-trained with multiple NLI datasets."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "Table 4  ###reference_### shows the overall evaluation results of different knowledge settings with different LLMs on the LTGen benchmark. We use the AMR ranking as it outperforms ranking with LLMs (cf. Table 5  ###reference_###).\nFigure 3  ###reference_### shows the results with respect to the long-tail level, while Figure 4  ###reference_### shows the results with respect to reference triple numbers on the LTGen benchmark.\nWe witness that GPT-3.5 works best in the LTGen-QA dataset while in the LTGen-Conv dataset, we observe that Llama 2 70B exhibit better knowledge match scores. Interestingly, when measuring the NLI score, we find that GPT-3.5 beats Llama 2 70B in the same dataset. Though GPT-3.5 only achieves a slightly higher E score compared to Llama 2 70B (0.499 vs 0.483), the much lower C score (0.190 vs 0.267) contributes most to the gap of E-C score between GPT-3.5 and Llama 2.\nWhen looking at the generated examples, we observe that compared to Llama 2 models, GPT-3.5 seems to somehow demonstrate a self-awareness of its own knowledge. Indeed, instead of generating an hallucinated response, GPT-3.5\noccasionally refuses to return a proper answer by apologizing (e.g. returning \u201cSorry, I can\u2019t answer.\u201d).\n###figure_8### ###figure_9### Unsurprisingly, we observe significant drops in performance with respect to the long-tail level. The higher the rarity level, the worst the performance (Figure 3  ###reference_###). This is in line with our intuition that LLMs cannot well-handle long-tail facts. Interestingly, we however remark a quite consistent performance increase from the long-tail level I to level II. We consider the diversity in relations from level I and therefore the need for more reference triples to be the culprit of a light difficulty increase in that level111111The average reference triples numbers for long-tail level I and II are 1.38 and 1.30 respectively.. In addition to that discovery., we also observed that GPT-3.5 achieves the lowest rKM score on the LTGen-QA dataset because it generates more unsure responses compared to Llama 2 models. This can be proved in the LTGen-Conv results, where GPT-3.5 achieves the best E-C score but not the highest rKM score, indicating that GPT-3.5 is more robust to prevent hallucination when the long-tail level is high.\nFrom Figure 4  ###reference_###, we can observe that more reference triples result in a harder task for LLMs. When considering the E-C score, even the best GPT-3.5 model gets a negative value when there are more than three reference triples.\nPrompting LLMs with non-parametric knowledge shows a consistent improvement compared with the no knowledge setting (cf. Table 4  ###reference_###), with the improvement of passages is not as significant as that of KG triples. In addition, in the LTGen-QA dataset, smaller models usually benefit more when prompted with external knowledge. Furthermore, we can see a notable decrease in C scores among all models with non-parametric knowledge. This indicates that LLMs benefit from non-parametric knowledge in reducing hallucinations. By prompting LLMs with KG triples, we achieve much better performance in all metrics as opposed to prompting LLMs with passages which further leads to higher inference costs121212The average input context token numbers of Llama 2 is 701.9 by prompting KG triples and 1014.1 by prompting top-5 contriever retrieved passages., showing both the effectiveness and efficiency of using structural knowledge from KGs. By merging both passage information and KG triples information, we do not observe a performance improvement upon the knowledge matching metrics. However, the NLI-based score is improved in the LTGen-Conv dataset. We consider that with different sources of external knowledge, LLMs are more cautious in providing responses, thus reducing hallucinations compared to single-sourced prompting.\nFrom Figure 5  ###reference_###, we can observe that when the long-tail level is low (typically level I and II), prompting LLMs with passages benefit limited or even harm the performance in both datasets. When the long-tail level increases, the performance gain grows as well. These are similar to the observations from Mallen et al. [6  ###reference_b6###]. As for KG triples, all models perform better with passages across all long-tail levels. It\u2019s worth noting that in the LTGen-Conv dataset, there is quite a clear gap between relatively smaller models (7B and 13B) and large-scale models (70B and above). This gap is more clear when prompting external knowledge. This might indicate that smaller models get harder to handle both dialogue history and external knowledge.\nSimilar to the no knowledge setting, the performance of LLMs generally drops as the number of reference triples increases. However, we can observe that the performance gap between different LLMs prompted with KG triples is more clear than prompted with passages. (cf. Figure 4  ###reference_###). In general, LLMs with KG triples perform better than with passage. We also note that the performance drop from 3 triples setting to more than 3 triples setting is less sharp when KG triples as part of the external knowledge source, indicating that within limited context length, KG triples is a richer knowledge source that could capture more factual knowledge."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "How LLMs Perform without External Non-parametric Knowledge?",
            "text": "We witness that GPT-3.5 works best in the LTGen-QA dataset while in the LTGen-Conv dataset, we observe that Llama 2 70B exhibit better knowledge match scores. Interestingly, when measuring the NLI score, we find that GPT-3.5 beats Llama 2 70B in the same dataset. Though GPT-3.5 only achieves a slightly higher E score compared to Llama 2 70B (0.499 vs 0.483), the much lower C score (0.190 vs 0.267) contributes most to the gap of E-C score between GPT-3.5 and Llama 2.\nWhen looking at the generated examples, we observe that compared to Llama 2 models, GPT-3.5 seems to somehow demonstrate a self-awareness of its own knowledge. Indeed, instead of generating an hallucinated response, GPT-3.5\noccasionally refuses to return a proper answer by apologizing (e.g. returning \u201cSorry, I can\u2019t answer.\u201d).\n###figure_10### ###figure_11### Unsurprisingly, we observe significant drops in performance with respect to the long-tail level. The higher the rarity level, the worst the performance (Figure 3  ###reference_###  ###reference_###). This is in line with our intuition that LLMs cannot well-handle long-tail facts. Interestingly, we however remark a quite consistent performance increase from the long-tail level I to level II. We consider the diversity in relations from level I and therefore the need for more reference triples to be the culprit of a light difficulty increase in that level111111The average reference triples numbers for long-tail level I and II are 1.38 and 1.30 respectively.. In addition to that discovery., we also observed that GPT-3.5 achieves the lowest rKM score on the LTGen-QA dataset because it generates more unsure responses compared to Llama 2 models. This can be proved in the LTGen-Conv results, where GPT-3.5 achieves the best E-C score but not the highest rKM score, indicating that GPT-3.5 is more robust to prevent hallucination when the long-tail level is high.\nFrom Figure 4  ###reference_###  ###reference_###, we can observe that more reference triples result in a harder task for LLMs. When considering the E-C score, even the best GPT-3.5 model gets a negative value when there are more than three reference triples."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "How Different Ways of Obtaining KG Knowledge Works?",
            "text": "In this section, we compare different relation-linking approaches applied in our proposed KG triples retrieval pipeline. Table 5  ###reference_### shows the relation-linking result. Compared with using LLMs, the AMR relation linking approach achieves a higher Recall score in both datasets of the LTGen benchmark with only less than 0.2B parameters. Since there might be accumulative errors from the entity tagging step, we further measure the relation-linking result in an oracle setting where the entity is pretended to be correctly linked. The best AMR relation linking approach received close to perfect Recall on the LTGen-QA dataset and over 84% Recall on the LTGen-Conv dataset. This indicates that there still is space for significant improvement in prompting with KG triples. A reasonable direction for future work can be to improve the performance of the entity tagging approach to fill the gap between Recall and Recall (oracle). It is worth noting that the relation-linking performance on LTGen-QA is a bit higher than that on the LTGen-Conv dataset. This comes in two reasons: 1) each question on the LTGen-QA dataset is only related to a single relation, making the relation-linking task easier; 2) the context window of the LTGen-Conv dataset is much larger, making both entity tagging and relation linking more challenging tasks."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "How Different Non-parametric Knowledge Help LLMs?",
            "text": "###figure_12### ###figure_13### Prompting LLMs with non-parametric knowledge shows a consistent improvement compared with the no knowledge setting (cf. Table 4  ###reference_###  ###reference_###), with the improvement of passages is not as significant as that of KG triples. In addition, in the LTGen-QA dataset, smaller models usually benefit more when prompted with external knowledge. Furthermore, we can see a notable decrease in C scores among all models with non-parametric knowledge. This indicates that LLMs benefit from non-parametric knowledge in reducing hallucinations. By prompting LLMs with KG triples, we achieve much better performance in all metrics as opposed to prompting LLMs with passages which further leads to higher inference costs121212The average input context token numbers of Llama 2 is 701.9 by prompting KG triples and 1014.1 by prompting top-5 contriever retrieved passages., showing both the effectiveness and efficiency of using structural knowledge from KGs. By merging both passage information and KG triples information, we do not observe a performance improvement upon the knowledge matching metrics. However, the NLI-based score is improved in the LTGen-Conv dataset. We consider that with different sources of external knowledge, LLMs are more cautious in providing responses, thus reducing hallucinations compared to single-sourced prompting.\nFrom Figure 5  ###reference_###  ###reference_###, we can observe that when the long-tail level is low (typically level I and II), prompting LLMs with passages benefit limited or even harm the performance in both datasets. When the long-tail level increases, the performance gain grows as well. These are similar to the observations from Mallen et al. [6  ###reference_b6###  ###reference_b6###]. As for KG triples, all models perform better with passages across all long-tail levels. It\u2019s worth noting that in the LTGen-Conv dataset, there is quite a clear gap between relatively smaller models (7B and 13B) and large-scale models (70B and above). This gap is more clear when prompting external knowledge. This might indicate that smaller models get harder to handle both dialogue history and external knowledge.\nSimilar to the no knowledge setting, the performance of LLMs generally drops as the number of reference triples increases. However, we can observe that the performance gap between different LLMs prompted with KG triples is more clear than prompted with passages. (cf. Figure 4  ###reference_###  ###reference_###). In general, LLMs with KG triples perform better than with passage. We also note that the performance drop from 3 triples setting to more than 3 triples setting is less sharp when KG triples as part of the external knowledge source, indicating that within limited context length, KG triples is a richer knowledge source that could capture more factual knowledge."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Works",
            "text": "In this paper, we have introduced a novel pipeline for automatically building question-answering and conversational datasets. We use this pipeline to create the LTGen benchmark, which has two datasets (LTGen-QA and LTGen-Conv) for assessing the ability of LLMs to ground long-tail knowledge. We conducted evaluations on different LLMs with varying types of non-parametric knowledge using our benchmark.\n\nOur key findings include: (1) non-parametric knowledge helps LLMs to improve their performance, especially when the required knowledge has a high rarity level or is composed of multiple triples. Our comprehensive experiments demonstrate non-parametric knowledge helps LLMs, not only for simple short-form question answering tasks (LTGen-QA) but also for complex free-form conversational question answering tasks (LTGen-Conv); (2) using prompts in the form of knowledge graph triples is more effective than relying on passages; (3) even though prompts from multiple sources of external knowledge (passages and KG triples) are correct, LLMs might not take them into account when generating their outputs. Thus retrieval based validations seem necessary.\n\nFuture works could focus on building safe and robust LLMs, including verifying and reducing grounding error where LLMs do not follow the context information we provide. Furthermore, we observe a huge performance improvement in the KG retrieval pipeline by using golden entities, showing the benefit of effective entity tagging approaches that should be investigated in future works."
        }
    ],
    "url": "http://arxiv.org/html/2405.06524v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "6",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "6",
            "6.1",
            "6.2",
            "6.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.06524v1",
        "paper_title": "Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts",
        "research_background": "### Paper's Motivation\n\nThe primary motivation of the paper is to address the limitations of Large Language Models (LLMs) like GPT-4, LLaMA, and PaLM2 in Knowledge-Intensive Generation (KIG) tasks, particularly those involving long-tail entities where hallucination is prevalent. Previous studies, such as those by Mallen et al., have shown that augmenting LLMs with non-parametric knowledge can mitigate this issue. However, these studies had limitations, including reliance on template-based datasets and a focus on unstructured, non-parametric knowledge. The authors aim to overcome these limitations and explore the effectiveness of both structured and unstructured non-parametric knowledge in improving LLM performance on long-tail factual questions.\n\n### Research Problem\n\nThe research problem addressed by this paper is how to improve the performance of LLMs in handling KIG tasks that involve long-tail facts. Specifically, the paper seeks to investigate:\n1. How LLMs perform in knowledge-intensive generation tasks that involve different kinds of factual knowledge.\n2. How LLMs benefit from different formats of non-parametric knowledge, both structured (like Knowledge Graph triples) and unstructured (like text passages), when handling long-tail facts.\n\n### Relevant Prior Work\n\n1. **LLMs and Knowledge-Intensive Generation:** Previous work has shown the capabilities of LLMs like GPT-4 [1  ###reference_b1###], LLaMA [2  ###reference_b2###, 3  ###reference_b3###], and PaLM2 [4  ###reference_b4###] in conversational search, but also highlights their susceptibility to hallucinations in KIG tasks, especially for long-tail entities [5  ###reference_b5###].\n\n2. **Prompting LLMs with Non-parametric Knowledge:** Mallen et al. [6  ###reference_b6###] demonstrated the effectiveness of prompting LLMs with non-parametric knowledge, showing it can improve the performance in long-tail question-answering tasks. However, they used a template-based dataset (PopQA) and focused solely on unstructured non-parametric knowledge.\n\n3. **Knowledge Graphs as a Source of Non-parametric Knowledge:** Knowledge Graphs (KGs) are highlighted as a robust source of structured non-parametric knowledge [8  ###reference_b8###, 9  ###reference_b9###]. The study aims to investigate how combining structured knowledge from KGs with unstructured knowledge from passages can further alleviate hallucinations and improve LLM performance in long-tail question answering.\n\nBy addressing the limitations and expanding the scope of existing studies, this paper aims to establish a more comprehensive understanding of how different types of non-parametric knowledge can enhance LLM capabilities in KIG tasks involving long-tail facts.",
        "methodology": "The proposed methodology introduces a novel pipeline for generating a high-quality Knowledge-Intensive Generation (KIG) dataset aimed at addressing the limitations of current question-answering systems in handling long-tail facts. The methodology can be broken down into several key components and innovations, as follows:\n\n### Key Components:\n\n1. **Dataset Creation:**\n   - **Long-Tail Entity Identification:**\n     - Adopts the definition by [6 ###reference_b6###] to classify an entity as long-tail based on the average monthly Wikipedia page views (WPV). Entities are considered rare if their WPV is among the least visited.\n     - Utilizes the Wikipedia Pageview API to obtain WPV data from January 1, 2021, to January 1, 2023.\n\n2. **Selection and Filtering of Data:**\n   - **Extraction of Relevant Triples:**\n     - Randomly samples entities from Wikidata and retrieves related triples using SPARQL queries.\n     - Filters out triples with non-informative relations like \"family name\" or \"given name\" to avoid generating trivial questions.\n   - **Entity Frequency Filtering:**\n     - Removes entities that appear in fewer than 5 or more than 100 triples to maintain task difficulty and efficiency.\n\n3. **Prompting with GPT-4:**\n   - **Template-Free QA Sample Generation:**\n     - Leverages GPT-4 for generating PopQA-like samples where each question is linked to an entity-relation pair, allowing for more varied and complex QA samples.\n   - **Dialogue Generation:**\n     - Prompts GPT-4 to create dialogues from the collected triples, forming the LTGen-Conv dataset. This enables automatic annotation of dialogues against relevant gold triples.\n\n4. **Dataset Contents:**\n   - **LTGen Benchmark:**\n     - Comprises two datasets: LTGen-QA (simple question-answering) and LTGen-Conv (multi-turn questions with multiple relations).\n   - **Long-Tail Levels:**\n     - Defines four levels of long-tail rarity (Level I to IV) based on WPV counts, with Level IV being the rarest.\n   - **LTGen-Conv Subsets:**\n     - Divides LTGen-Conv into subsets based on the number of reference triples needed to answer questions: single (10,330 samples), two (1,720 samples), three (590 samples), and more than three (694 samples).\n\n### Innovations:\n- **Clear Definition of Long-Tail Entities:**\n  - Provides an objective and universally-accepted method for defining long-tail entities using WPV data.\n- **Template-Free Generation with GPT-4:**\n  - Utilizes the advanced capabilities of GPT-4 for generating diverse and complex QA samples without relying on predefined templates, enriching the dataset.\n- **Comprehensive Benchmark:**\n  - Introduces the comprehensive LTGen benchmark to evaluate the performance of LLMs on both simple and multi-turn questions involving long-tail facts.\n- **Fine-Grained Subsets for Dialogue Evaluation:**\n  - Creates fine-grained subsets within LTGen-Conv to rigorously test LLMs' ability to handle multiple relational answers.\n\nIn summary, the methodology systematically addresses the limitations of existing datasets by leveraging WPV data to identify long-tail entities, filtering non-informative triples, and employing GPT-4's advanced capabilities to generate high-quality, diverse QA samples that are both simple and complex in nature. The resulting LTGen-QA and LTGen-Conv datasets offer a robust benchmark for evaluating LLMs on long-tail facts.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets\nThe main experiment evaluates several large language models (LLMs) using the LTGen benchmark, which consists of two tasks: LTGen-QA (question answering) and LTGen-Conv (conversation). \n\n- **External Passages:** Wikipedia corpus from the KILT benchmark.\n  - **Passage Length:** Maximum of 100 words.\n  - **Retrieval Model:** MSMARCO fine-tuned Contriever checkpoint to retrieve top-10 passages per sample.\n\n- **Knowledge Graph (KG) Triples:** \n  - KG triples are retrieved based on TAGME API-identified topic entities and an oracle setup that uses golden topic entities.\n  - **Relations:** Top-5 relations for each topic entity.\n  - **Triple Selection:** 10 random triples per entity-relation pair.\n\n#### Baselines and Comparison Methods\n- **Baseline LLMs:** Various large language models (specific models not listed in the provided text).\n- **Additional Baseline:** Contriever models for passage retrieval.\n\n#### Evaluation Metrics\n- **Recall for KG Triple Retrieval:** Measures the number of correct entity-relation pairs compared to reference pairs.\n- **Knowledge Matching Scores (KM, eKM, rKM):** For both LTGen-QA and LTGen-Conv, following the method by Asai et al.\n  - **KM (Knowledge Matching):**\n  - **eKM (Exact Knowledge Matching):**\n  - **rKM (Ratio Knowledge Matching):**\n\n- **Reference Response Metrics (LTGen-Conv Only):** \n  - **BLEU and ROUGE:** Overlap-based metrics, found suboptimal for long-form content.\n  - **NLI-Based Scores:** \n    - **Entailment Score (E):**\n    - **Natural Score (not specifically defined in provided text):**\n    - **Contradiction Score (C):** \n    - **Overall Score:** Reported as E-C.\n  - **Entailment Model Used:** DeBERTa V3 pre-trained with multiple NLI datasets.\n\n### Main Experimental Results\n\n- **Recall for KG Triples:** Specific recall values for entity-relation pairs are not disclosed.\n- **Knowledge Matching Scores (KM, eKM, rKM):** Detailed numerical results are not provided. Metrics indicate varying levels of match quality between generated predictions and reference answer entities.\n- **NLI-Based Scores:** \n  - **E-C Scores:** Higher entailment scores and lower contradiction scores indicate better performance; specific values not mentioned.\n\nOverall, the combination of different evaluation metrics provides a comprehensive assessment of both the retrievability of relevant information and the quality of generated responses. However, the precise numerical results of these assessments are not included in the provided text."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the impact of knowledge graph (KG) triples compared to textual passages in improving the performance of large language models (LLMs) on question answering tasks involving long-tail facts.",
            "experiment_process": "In this study, the LTGen benchmark was used, which includes two datasets: LTGen-QA for simple question answering and LTGen-Conv for multi-turn scenarios requiring multiple knowledge relations. Different LLMs, including GPT-3.5 and various versions of LLaMA2, were tested. Knowledge-from-passages was retrieved using the MSMARCO fine-tuned Contriever model, and KG triples were retrieved using a SPARQL querying system. The effectiveness of non-parametric knowledge was evaluated using metrics such as knowledge matching (KM), exact knowledge matching (eKM), and ratio knowledge matching (rKM), as well as entailment scores (E, C, and E-C) for LTGen-Conv.",
            "result_discussion": "The performance of LLMs improved significantly when prompted with non-parametric knowledge, particularly with KG triples, compared to passages. GPT-3.5 showed particular robustness in avoiding hallucinations by sometimes refusing to provide an answer. Conversely, LLaMA 2 70B exhibited better knowledge match scores in the LTGen-Conv dataset. The results indicated that smaller LLM models benefited more from external knowledge, and the usage of KG triples was both more effective and efficient than passages, especially as the number of reference triples increased.",
            "ablation_id": "2405.06524v1.No1"
        },
        {
            "research_objective": "To compare different strategies (LLM-based prompting and AMR-based method) for ranking triples retrieved from knowledge graphs in terms of their relevance in answering questions.",
            "experiment_process": "The study focused on the KG triples retrieval pipeline using the LTGen benchmark. In the KG triples retrieval, entities in query inputs were tagged using the TAGME API. The usefulness of retrieved triples was evaluated by ranking them through two strategies: prompting LLMs with entities and relations to rank relevance and an AMR-based method that encoded semantic relationships between triples using a GNN-based approach. The effectiveness of these methods was measured using recall metrics in various settings, including an oracle setup.",
            "result_discussion": "The AMR-based relation linking approach outperformed the LLM-based method in terms of recall scores for both LTGen-QA and LTGen-Conv datasets, with high recall even in the oracle setting. The study emphasized that while the AMR approach provided close to perfect recall on the LTGen-QA dataset, there was room for improvement in entity tagging to enhance the performance of KG triples prompting. The higher complexity of questions in the LTGen-Conv dataset presented greater challenges for both entity tagging and relation linking.",
            "ablation_id": "2405.06524v1.No2"
        },
        {
            "research_objective": "To determine how different types of non-parametric knowledge, specifically KG triples and textual passages, assist LLMs in generating accurate answers and reducing hallucinations in responses.",
            "experiment_process": "LLMs were prompted with two types of external non-parametric knowledge: KG triples and passages retrieved from a knowledge source. The LTGen benchmark datasets, LTGen-QA and LTGen-Conv, were used to measure knowledge matching (KM), exact knowledge matching (eKM), ratio knowledge matching (rKM), and entailment scores (E-C). The performance was evaluated across different LLM sizes and configurations, considering various long-tail levels and the number of reference triples required.",
            "result_discussion": "Prompting with non-parametric knowledge ubiquitously showed improved performance over models without such knowledge. KG triples were more beneficial than passages, leading to reduced hallucinations and better factual accuracy, especially as long-tail levels increased. Smaller models benefited notably more from external knowledge, with KG triples proving to be both more effective and computationally efficient than passages, particularly when the number of reference triples was large. Combining KG triples and passages did not consistently enhance knowledge coverage but did help in minimizing hallucinations.",
            "ablation_id": "2405.06524v1.No3"
        }
    ]
}