{
    "title": "NumeroLogic: Number Encoding for Enhanced LLMs\u2019 Numerical Reasoning",
    "abstract": "Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of \"42\", we suggest using \"2:42\" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) struggle with numerical and arithmetical tasks. Even the most advanced models like GPT-4 [1 ###reference_b1###] still experience difficulties with tasks such as multiplying 3-digit numbers [13 ###reference_b13###]. Recent studies ([10 ###reference_b10###, 13 ###reference_b13###]) have proposed techniques to improve arithmetic in LLMs, such as the Chain of Thought (CoT; [16 ###reference_b16###]) method, which encourages the model to anticipate the entire sequence of algorithmic steps rather than just the final output.\n\nWhile these strategies offer valuable insights into the capabilities of LLMs, they primarily focus on post-hoc solutions for specific arithmetic challenges and do not present a practical solution for pretraining LLMs. Our research, however, focuses on solutions applicable to self-supervised language modeling in general, utilizing arithmetic exercises primarily for evaluating their impact. We hypothesize that one of the challenges LLMs face when dealing with numerical tasks is the textual representation of numbers.\n\nIn today\u2019s most popular decoder-based LLMs, each token attends only to previous tokens. When a model \u201creads\u201d a token representing a digit (or multiple digits), it cannot tell its place value, i.e., \u20181\u2019 can represent 1 million, 1 thousand, or a single unit. Only when reaching the end of the number might the model update its representation of the previous digit tokens to be related to their real place value.\n\nTo address this issue, we propose a straightforward reformatting technique called \u201cNumeroLogic,\u201d which involves adding the number of digits as a prefix to numbers. This lets the model know in advance what is the place value of a digit before it is read. This simple change also offers another benefit, when the model is generating a number it needs to first reason about what is going to be the number of digits. This acts as a Chain of Thought (CoT) [16 ###reference_b16###], encouraging the model to perform some reasoning before it begins to predict digits.\n\nImplementing the suggested reformatting does not necessitate any alterations to the model\u2019s architecture; it can be accomplished through text pre- and post-processing based on regular expressions. We demonstrate that NumeroLogic enhances the numerical abilities of LLMs across both small and larger models (up to 7B parameters). This enhancement is showcased through supervised training on arithmetic tasks and its application in self-supervised causal language modeling to enhance general language comprehension."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Recently, there has been significant interest in enhancing the numerical capabilities of LLMs. One approach to investigating these capabilities is by enhancing performance in arithmetic tasks. Several recent studies have proposed methods to enhance performance in these tasks.\n\nOne strategy involves reversing the expected result order from the least significant digit to the most significant. Another strategy is using an elaborated CoT where the model is taught to predict all steps of an algorithm predefined for each arithmetic task. It is noted that the model learns to rely too heavily on positional encoding when trained for a specific arithmetic task. They suggest ways to overcome it, e.g. adding random whitespaces in the middle of numbers.\n\nThese studies aim to enhance performance by offering tailored solutions to the associated challenges.\n\nIn contrast, our focus is on identifying solutions that benefit general language modeling, with arithmetic tasks being used solely for measuring improvements.\n\nAnother aspect important for LLMs numerical capabilities is the tokenization process. The commonly used Byte Pair Encoding (BPE) based methods for tokenization are based on the corpus distribution and can split a number into tokens in unintuitive ways. Different foundation models took different approaches when dealing with number tokenization. PaLM, Llama, and Mistral force each digit to have a single token. GPT-3.5 and GPT-4 define a token for each up to 3-digit number. Related to our work, an issue was highlighted with the GPT approach. They show that dividing large numbers into 3-digit segments from left to right can be problematic. They suggest overcoming it by artificially inserting commas between digits to control the splitting."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "NumeroLogic",
            "text": "We introduce NumeroLogic, a technique for boosting causal LLM\u2019s numerical capabilities.\nThe concept involves adding a digit count before numbers, enabling the model to know the place values of digits before reaching the final digits of a number.\nAdditionally, the model needs to predict the total number of digits before generating a number, acting as a simplified CoT, prompting it to reason about the number that is going to be generated.\nWe add special tokens to help represent numbers with the number-of-digit prefix, \"<startnumber>\", \"<midnumber>\", and \"<endnumber>\" (or, for simplicity, \"<sn>\", \"<mn>\", and \"<en>\"). For floating points, the prefix includes both the number of digits of the integer part and the decimal part. For example, \"42\" is replaced by \"<sn>2<mn>42<en>\" and \"3.14\" is replaced by \"<sn>1.2<mn>3.14<en>\".\nWhen using the LLM to generate numbers, we disregard the information about the number of digits and only retain the generated number itself.\nAlthough not within the scope of this study, it may be feasible to leverage the additional information to identify discrepancies, wherein the model predicts a certain digit count but produces a number with a different count of digits.\nThe NumeroLogic approach includes basic text pre-processing and post-processing steps that occur before and after the tokenizer\u2019s encoding and decoding methods, respectively.\nBoth can be implemented based on regular expressions:\nFor small transformers (NanoGPT [9  ###reference_b9###]), we train all parameters from scratch with character-level tokenization. For small transformers, we also replace the special tokens with single characters, \"<sn>\", \"<mn>\", and \"<en>\" are replaced with \"{\", \":\", and \"}\", respectively.\nFor larger transformers, we start from pre-trained models.\nWe add the new special tokens to the tokenizer\u2019s vocabulary and expand the embedding layer and the final fully connected layer to fit the new vocabulary size.\nWhen continuing training on causal language modeling or fine-tuning on supervised arithmetic tasks, we use low-rank adaptation (LoRA) [7  ###reference_b7###]. We apply LoRA for the attention block projection matrices (Q, K, V, O) and train the modified embedding layer and the final fully-connected layer in full rank."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "To test the effect of NumeroLogic we conducted several experiments. First, we tested supervised training of a small language model (NanoGPT) on various arithmetic tasks. We then test the scalability to larger models (Llama2-7B). Finally, we test self-supervised pretraining of Llama2-7B, with the suggested formatting, and test on general language understanding tasks."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Arithmetic tasks with small model",
            "text": "We trained NanoGPT [9] from scratch in a supervised manner jointly on 5 arithmetic tasks: addition, subtraction, multiplication, sine, and square root. Addition and subtraction are performed with up to 3-digit integer operands. Multiplications are performed with up to 2-digit integer operands. Sine and square root with 4 decimal-places floating point operands and results. The operand range for sine is within specific bounds. The operand range for the square root is within specific bounds. The model is trained in a multi-task fashion on all 5 tasks, with 10K training samples for each task except for multiplication, for which 3K samples are used. We followed the protocol from Section D.2 in [10]."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Arithmetic tasks with larger model",
            "text": "Next, we test how the method scales to a larger model. For this experiment, we fine-tune a pretrained Llama2-7B model. In this experiment, we again tested the same five arithmetic tasks: addition, subtraction, multiplication, sine, and square root.\n\nFor addition (5 digit), subtraction (5 digit), and multiplication (3 digit) we tested on two versions - integers and floating point numbers. For generating a random N-digit floating point operand we first sample an up to N-digit integer and then divide it by a denominator uniformly sampled from. For each of the addition, subtraction, and multiplication tasks, we generated 300K random equations as a training set. The sine and square root operands and results are generated with 5 decimal place precision, and we generated 30K random equations for the training sets of these tasks. Since we are working with a pretrained model, we add new tokens (\"<sn>\", \"<mn>\", and \"<en>\") to the tokenizer\u2019s vocabulary. We finetune one model per task with LoRA (rank 8), and we also train in full-rank the embedding layer and the final fully-connected layer since their parameters are extended to accommodate the larger vocabulary size."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Self-Supervised Pretraining",
            "text": "Our approach differs from other methods in that it is not specialized for a specific task, such as arithmetic, but rather designed for general language modeling tasks involving text with numerical values. To test this capability, we continue the pretraining of LLama2-7B with the causal text modeling objective (next token prediction). We train on text from the RefinedWeb dataset. The goal is to teach the model to read and write numbers in the NumeroLogic format without forgetting its previously acquired knowledge. To facilitate this, we perform the continued pretraining with LoRA. We then test the model in a zero-shot manner on the massive multitask language understanding benchmark (MMLU). \n\nThe MMLU benchmark encompasses tasks from diverse domains, some emphasizing analytical skills and numerical comprehension while others do not. In Table 3, we delve into the impact of NumeroLogic on MMLU tasks categorized by field. As anticipated, tasks in STEM fields exhibit more substantial enhancements compared to those in social sciences and humanities. Table 4 provides a detailed analysis of NumeroLogic\u2019s performance boost across tasks containing numbers versus those that do not. Consistently, tasks involving numbers show a more pronounced improvement."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Ablation studies",
            "text": ""
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Encoding operands vs. results",
            "text": "We experimented to test the effect of operand encoding vs. the expected output (equation result) encoding. Operand encoding primarily influences the model\u2019s comprehension of numerical values in the input, while result encoding is more associated with CoT, prompting the model to first reason about the expected number of digits. We repeat the experiment from Section 4.1, but with the NumeroLogic encoding applied only to the operands or to the results and report the findings for the different variants. We find that both operands and results encodings are beneficial, with a stronger impact attributed to encoding the results. Applying NumeroLogic to all numbers, both operands and results, yields advantageous outcomes."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Different Encodings",
            "text": "We experimented with different formats for providing the number of digits. One alternative we tested is defining a set of new special tokens representing each possible number of digits, {<1digitnumber>, <2digitnumber>,...}. It might be due to the unbalanced distribution of numbers. E.g. numbers with a single digit are much less frequent in a data of 3-digit additions, it is possible the model has not seen enough single-digit numbers to learn a good representation of the <1digitnumber> token. Another alternative we tested is removing the \u201cend of number\u201d token (<en>), keeping only the number prefix, e.g. \"<sn>3<mn>100\". This works better than plain but slightly worse than the full NumeroLogic encoding. The results are summarized in Table 6  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In this paper, we introduced NumeroLogic, an innovative approach to enhancing language models\u2019 comprehension and generation of numerical data. Our hypothesis centered around the challenge of numerical representation in text for large language models (LLMs), which traditionally struggle with arithmetic and numerical understanding. By presenting numbers with a prefixed notation indicating their digit count, we proposed a solution that aids models in recognizing the place value of digits before the complete presentation of the full number. This method not only facilitates a better understanding of numbers but also prompts the model to reason about the magnitude of numbers it intends to generate, effectively integrating a form of Chain of Thought (CoT) reasoning.\n\nOur experiments spanned from arithmetic tasks to general natural language understanding, using both small and large model architectures. This improvement was not just confined to specialized arithmetic tasks but also extended to general language modeling, as evidenced by performance boosts in the MMLU benchmark for tasks requiring numerical understanding.\n\nIn conclusion, NumeroLogic offers a simple yet powerful tool for enhancing the numerical capabilities of language models without necessitating architectural modifications. Its success across various tasks and models highlights its potential as a general-purpose solution, particularly beneficial in domains where numerical understanding is crucial."
        }
    ],
    "url": "http://arxiv.org/html/2404.00459v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.4",
            "4.4.1",
            "4.4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.00459v1",
        "paper_title": "NumeroLogic: Number Encoding for Enhanced LLMs\u2019 Numerical Reasoning",
        "research_background": "### Motivation:\nModern Large Language Models (LLMs) are not adept at handling numerical and arithmetical tasks, even with the continuous advancements in their architectures and training methodologies. This inadequacy is observed in tasks as fundamental as multiplying 3-digit numbers. Despite attempts to improve these capabilities through methods like Chain of Thought (CoT), which suggests models anticipate each step in an arithmetic sequence, these approaches primarily offer post-hoc solutions tailored for specific arithmetic tasks rather than enhancing the numerical handling prowess of LLMs during pretraining.\n\n### Research Problem:\nThe primary research challenge this paper addresses is the difficulty LLMs face with the text-based representation of numbers, specifically the inability to recognize place values of digits during token generation. Current decoder-based models struggle as they decode each token independently, leading to confusion about the place value of digit tokens until the entire number is processed. To resolve this, a novel reformatting technique for numerical representation within the training data is necessary.\n\n### Relevant Prior Work:\n1. GPT-4 [1] and similar models have shown limitations in numerical and arithmetical tasks such as multiplying 3-digit numbers.\n2. Techniques like Chain of Thought (CoT) [16] have been proposed to improve the reasoning capabilities of LLMs, enabling models to predict the sequence of arithmetic operations rather than just the end result. \n3. Recent studies ([10, 13]) have explored various strategies to enhance LLM's arithmetic performance, though they mostly focus on solutions that are specific to identified numerical tasks and are implemented post hoc.\n\nThe innovations from these previous works highlight the potential but underscore the need for an overarching and preemptive solution that integrates with self-supervised language modeling processes. The authors of this paper propose the \"NumeroLogic\" technique that enhances numerical reasoning abilities by utilizing a reformatted numeric representation as a prefix to impart place value information during token reading, improving both generation and comprehension of numbers within LLMs.",
        "methodology": "The methodology for NumeroLogic, a technique designed to enhance the numerical reasoning capabilities of causal large language models (LLMs), involves several key components and innovations:\n\n### Key Components:\n\n1. **Digit Count Prefixing:**\n   - **Technique:** Each number is prefixed with a count of its digits to inform the model about the digit place values before encountering the final digits.\n   - **Example:** The number \"42\" becomes \"<sn>2<mn>42<en>\", where \"<sn>\" marks the start of the number with 2 digits, \"<mn>\" signifies the middle part, and \"<en>\" denotes the end of the number.\n\n2. **Total Digits Prediction:**\n   - **Function:** Before generating a number, the model predicts the total number of digits, which helps it reason about the number generation process.\n\n3. **Special Tokens:**\n   - **Tokens Added:** Special tokens \"<startnumber>\", \"<midnumber>\", and \"<endnumber>\", or their simpler forms \"<sn>\", \"<mn>\", and \"<en>\", are introduced to encapsulate numbers within the text.\n   - **Floating Points Handling:** For floating points, the prefix includes the digit counts of both the integer and decimal parts. For instance, \"3.14\" becomes \"<sn>1.2<mn>3.14<en>\".\n\n4. **Text Pre-Processing and Post-Processing:**\n   - **Steps:** Basic steps that occur before and after the tokenizer\u2019s encoding and decoding, respectively.\n   - **Implementation Base:** Regular expressions are used for these steps.\n\n5. **Training Adjustments:**\n   - **Small Transformers:** For models like NanoGPT, parameters are trained from scratch with character-level tokenization, and special tokens are replaced with single characters (e.g., \"<sn>\" becomes \"{\" ).\n   - **Larger Transformers:**\n     - **Starting Point:** Begin with pre-trained models.\n     - **Vocabulary Changes:** Special tokens are added to the tokenizer\u2019s vocabulary.\n     - **Layer Adjustments:** The embedding layer and final fully connected layer are expanded to accommodate the new vocabulary.\n\n6. **Low-Rank Adaptation (LoRA):**\n   - **Application:** LoRA is applied to attention block projection matrices (Q, K, V, O).\n   - **Training:** Modified embedding layers and the final fully-connected layer are trained in full rank during causal language modeling or fine-tuning on supervised arithmetic tasks.\n\n### Innovations:\n\n- **Digit Count Insight:** By incorporating digit count knowledge, the model gains a structured approach to understanding and reasoning with numbers.\n- **CoT Simplification:** The technique relates to Chain of Thought (CoT) prompting, although in a simplified manner.\n- **Pre/Post-Processing via Regex:** Efficient handling of the conversion process around the tokenizer using regular expressions.\n- **Tailored Training Techniques:** Use of character-level tokenization for small models and LoRA for efficient training of larger models.\n\nCollectively, these components and innovations constitute the NumeroLogic methodology aimed at improving the numerical reasoning capacities of LLMs.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n**Datasets:**\n1. For Arithmetic Tasks: A variety of arithmetic datasets were used to test the models' ability to perform different arithmetic operations.\n2. For General Language Understanding: The pre-trained large language model (Llama2-7B) was also evaluated on common benchmarks for language understanding, though specific datasets are not mentioned.\n\n**Baselines:**\n1. NanoGPT without NumeroLogic\n2. Llama2-7B without NumeroLogic\n3. Possibly other standard baselines in the respective benchmarks (though not specifically mentioned)\n\n**Evaluation Metrics:**\n1. Performance on arithmetic tasks: Specific metrics for arithmetic performance (e.g., accuracy, correctness of operations) are implied.\n2. Performance on general language understanding tasks: Standard metrics for language tasks such as accuracy, F1-score, perplexity, etc., presumably from benchmarks.\n\n**Main Experimental Results:**\n1. **NanoGPT Supervised Training:**\n   - The small language model NanoGPT showed improved performance on various arithmetic tasks when trained with NumeroLogic formatting compared to without it. This demonstrates that NumeroLogic enhances the model's numerical reasoning abilities in a supervised setting.\n\n2. **Scalability to Larger Models:**\n   - When applied to a larger model (Llama2-7B), the improvement trend continued, indicating that the benefits of NumeroLogic scale with model size. Larger models with NumeroLogic showed a notable increase in arithmetic task performance.\n\n3. **Self-Supervised Pretraining:**\n   - Applying NumeroLogic formatting in self-supervised pretraining of Llama2-7B resulted in improvements on general language understanding tasks as well. This suggests that the enhanced numerical reasoning capability also positively impacts the overall language understanding and processing abilities of the model.\n\nThese results collectively highlight the efficacy of NumeroLogic in not only improving numerical reasoning skills across different model sizes but also enhancing general language understanding performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To test the effect of operand versus result encoding in numerical comprehension and reasoning with the proposed NumeroLogic encoding.",
            "experiment_process": "The experiment involved applying the NumeroLogic encoding to either the operands or the results in arithmetic tasks and then assessing the accuracy in 3-digit addition tasks. The encoding process was varied as follows: applying NumeroLogic only to the operands, only to the results, or to both operands and results. The results were compared to determine which encoding variant offered better comprehension and accuracy.",
            "result_discussion": "The study found that both operand and result encodings are beneficial but encoding the results has a stronger impact. The highest accuracy was achieved when NumeroLogic was applied to all numbers, both operands and results, highlighting the importance of encoding expected outputs for numerical reasoning.",
            "ablation_id": "2404.00459v1.No1"
        },
        {
            "research_objective": "To investigate the performance of different formats for representing the number of digits in numerical data using the NumeroLogic approach.",
            "experiment_process": "Several alternative encodings were tested, including using special tokens for each possible number of digits and a simplified encoding without an 'end of number' token. The performance of these alternatives was evaluated in the context of arithmetic tasks, specifically focusing on 3-digit additions. The results were compared to determine which encoding scheme performed better.",
            "result_discussion": "The experiment revealed that using multiple special tokens for each possible number of digits resulted in lower performance, likely due to an unbalanced distribution of numbers in the dataset. The simplified encoding without an 'end of number' token performed better than plain numbers but was still slightly less effective than the full NumeroLogic encoding.",
            "ablation_id": "2404.00459v1.No2"
        }
    ]
}