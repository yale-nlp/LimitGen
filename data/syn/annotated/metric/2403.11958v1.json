{
    "title": "Language Evolution with Deep Learning Chapter to appear in the Oxford Handbook of Approaches to Language Evolution",
    "abstract": "Computational modeling plays an essential role in the study of language emergence. It aims to simulate the conditions and learning processes that could trigger the emergence of a structured language within a simulated controlled environment. Several methods have been used to investigate the origin of our language, including agent-based systems, Bayesian agents, genetic algorithms, and rule-based systems. This chapter explores another class of computational models that have recently revolutionized the field of machine learning: deep learning models. The chapter introduces the basic concepts of deep and reinforcement learning methods and summarizes their helpfulness for simulating language emergence. It also discusses the key findings, limitations, and recent attempts to build realistic simulations. This chapter targets linguists and cognitive scientists seeking an introduction to deep learning as a tool to investigate language evolution.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Social animals have been found to use some means of communication to coordinate in various contexts: foraging for food, avoiding predators, mating, etc. (Hauser, 1996). Among animals, however, humans seem to be unique in having developed a communication system, natural language, that transcends these basic needs and can represent an infinite variety of new situations (Hauser et al., 2002) to the extent that language itself becomes the basis for a new form of evolution: cultural evolution. Understanding the emergence of this unique human ability has always been a vexing scientific problem due to the lack of access to the communication systems of intermediate steps of hominid evolution (Harnad et al., 1976, Bickerton, 2007). In the absence of data, a tempting idea has been to reproduce experimentally the process of language emergence in either humans or computational models (Steels, 1997, Myers-Scotton, 2002, Kirby, 2002).\n\nExperimental paradigms with humans (Kirby et al., 2008, Raviv et al., 2019, Motamedi et al., 2019) have produced significant insights into language evolution. Still, their scope is limited due to the inability to replicate key aspects of language evolution, such as communication within and across large populations and the study of long evolutionary timescales. Computer modeling can help overcome these limitations and has played a prominent role in studying language evolution for a long time (Lieberman and Crelin, 1971). In particular, agent-based modeling has been used from the early days of the language evolution research \u201crenaissance\u201d (Hurford, 1989, Steels, 1995) and is still a very active and influential field (Reali and Griffiths, 2009; 2010, Smith et al., 2003, Vogt, 2009, Gong et al., 2014, Ke et al., 2008, Brace et al., 2015, Cuskley et al., 2017, Kirby et al., 2015).\n\nMeanwhile, in the last decade, the field of machine learning has rapidly developed with the advent of deep learning. Deep neural networks have achieved human-level performance in various domains, including image recognition (He et al., 2016, Chen et al., 2020), natural language processing (Devlin et al., 2018, Brown et al., 2020), automatic translation (Bahdanau et al., 2014, Vaswani et al., 2017), and reinforcement learning (Silver et al., 2016).\n\nThis chapter aims to introduce the technical and conceptual background required for using deep learning to simulate language evolution, that is, to simulate both the emergence of communication in evolutionary timescales and patterns of language change in historical timescales (Kottur et al., 2017, Lazaridou et al., 2018, Lazaridou and Baroni, 2020).\n\nFirst, we present how to implement a communication game (Sec. 2), including formalizing it as a machine learning problem (Sec. 2.1), designing neural network agents (Sec. 2.2) and making agents learn to solve the game (Sec. 2.3).\n\nSecond, we examine the Visual Discrimination Game (Lewis, 1969) as a case study (Sec. 3), which has been widely explored in neural emergent communication research.\n\nFinally, we provide an overview of recent emergent communication simulations with neural networks, highlighting the successes, limitations, and future challenges (Sec. 4)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Designing communication games with Deep Learning",
            "text": "Communication games (Lewis, 1969, Steels, 1995, Baronchelli et al., 2010) are a framework used to investigate how perceptual, interactive, or environmental pressures shape the emergence of structured communication protocols (Kirby et al., 2008, Cuskley et al., 2017, Raviv et al., 2019). This framework has primarily been studied over the past years and is still one of the leading simulation frameworks in language evolution. See Chapter Communication games: Modelling language evolution through dyadic interaction for more details. This section presents how to simulate communication games using Deep Learning. First, we frame the communication game as a multi-agent problem, where each agent is represented by a deep neural network. Second, we define communicative agents. Third, we use machine learning optimization to train agents to solve the communication game.\n\nThe perception module maps an observation of the environment to an internal representation. The choice of architecture depends on the input observation, which differs across games. For example, a Convolutional Neural Network (LeCun et al., 1988) is suitable for generating image representations from visual input data, as illustrated in Figure 7. The generation module maps an internal representation, i.e., a vector of a given dimension, into a message. Recurrent neural networks (RNN) (Elman, 1990, Mikolov et al., 2010) and Transformers (Vaswani et al., 2017) are well suited for sequences and are hence used in standard emergent communication settings (Lazaridou et al., 2018, Chaabouni et al., 2019, Kottur et al., 2017, Li and Bowling, 2019, Chaabouni et al., 2022, Rita et al., 2022a). Communication is mainly based on discrete messages, even if some works consider continuous communication protocols (Tieleman et al., 2019).\n\nRemark: To shape the message space, a vocabulary of symbols and a maximum length must be introduced. It\u2019s also possible to add an end-of-sentence token EoS to indicate the end of the message. When making these design choices, task complexity should be considered; a larger vocabulary and message length allow for communicating more information/concepts, while a smaller vocabulary and message length require better information compression and, hence, a more structured communication protocol.\n\nThe understanding module maps a message to an internal representation. Since messages are discrete sequences, RNNs, and Transformers are well-suited for this module. The action module maps an internal representation of an action in the environment. Since the internal representations are scalars and actions a finite set of possibilities, a well-suited architecture is the Multi-Layer Perceptron followed by a softmax that draws a probability distribution over the potential actions.\n\nRemark: Deep learning techniques allow training a system composed of multiple differentiable modules end-to-end. The agent is seen as a single block that provides a prediction given input and output data instead of past methods that glue independently trained/designed blocks together. In communication games, the sender and receiver are both fully-differentiable individually. However, the message generation between them does not necessitate on purpose to separate the training of the agents. Nonetheless, the message generation can still be made differentiable.\n\nGood practice: Exploring various neural architectures is a common reflex when starting with deep learning. However, its impact is limited in practice compared to other experimental choices such as task definition, optimization, data, and training objective. Basic neural architectures are recommended to avoid compounding factors when comparing methods.\n\nTo train neural networks, suitable learning techniques must be chosen depending on the task and the availability of training data, which consists of input-output pairs. Two standard techniques used to solve communication games are:\n\nSupervised Learning (SL): The neural network is given a training set of input-output pairs, and its objective is to learn how to map inputs to their corresponding outputs. An example of a supervised language task is the translation: the network learns to map one language to another by training on pairs where each pair consists of aligned source and target sentences. Supervised learning finds the weights that enable the network to generalize this mapping to new, unseen examples drawn from the same distribution as the training data, e.g., trying to translate beyond the initial corpus. In communication games, Supervised learning tasks often involve classification (e.g., object selection, attribute reconstruction, translation) and regression (e.g., drawing, pixel reconstruction).\n\nReinforcement learning (RL): In RL, a neural network, or agent, must perform a sequence of actions to resolve a task within its environment. These actions yield rewards that gauge the effectiveness of the network\u2019s task performance. The network is then optimized to maximize its expected reward, i.e., performing the sequence of actions that lead to the highest task success. Not"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Framing communication games as a machine learning problem",
            "text": ""
        },
        {
            "section_id": "2.1.1",
            "parent_section_id": "2.1",
            "section_name": "2.1.1 Machine learning is well suited for simulating communication games",
            "text": "Mitchell (1997) defines machine learning as follows: \u201cA computer program is said to learn from an experience with respect to some class of tasks and performance measure, if its performance at tasks in, as measured by, improves with experience.\u201d Machine learning is well suited to frame communication games: participants develop a language through trial and error during a communication game. They iteratively adapt their language production and understanding to achieve a given task for which at least one agent lacks information (Tadelis, 2013). While game theoretic approaches analyze stable communication protocols (Crawford and Sobel, 1982, Skyrms, 2010), studying the dynamic learning process is a more challenging and richer problem. Borrowing Mitchell (1997) notations, this dynamic process can be framed as a machine learning problem where participants are computer programs that perform the communication game. The game\u2019s success is measured by after each episode of the game, and participants update their communication protocol based on the outcome. After enough iterations, the participants may converge, i.e., stabilize on a successful communication protocol, allowing them to solve the game. This iterative learning process is illustrated in Figure 1 and is the fundamental idea of machine learning."
        },
        {
            "section_id": "2.1.2",
            "parent_section_id": "2.1",
            "section_name": "2.1.2 Formalizing communication games as a machine learning problem",
            "text": "###figure_7### For simplicity, we focus in this chapter on two-player communication games where one agent, the \u201csender\u201d sends messages to a second agent, the \u201creceiver\u201d that parses them and takes action to solve the task in an environment. This setting is referred to as dyadic unidirectional communication games in the literature. Formally, the \u201csender\u201d and \u201creceiver\u201d are parametric models respectively denoted by  and  with parameters  and . Both parametric models will further be designed as deep neural networks. As illustrated in Figure 2, a round of the game proceeds as follows:\n\n1. The sender  and receiver  get observations from their environment denoted by  and .\n2. The sender  sends a message  to the receiver  where  is a sequence of symbols taken from a fixed vocabulary .\n3. The receiver  uses the message  and its observation  to perform an action  toward achieving the task.\n4. The task\u2019s success is then measured by two reward signals  and  which are given to the sender  and the receiver  respectively to improve their protocols.\n\nThroughout the game, both agents must agree on a common language to solve the game. Importantly, the emergent language is not defined by explicit language rules but implicitly encoded by the sender\u2019s parameters .\n\nRemark: This chapter presents a simplified formalism of communication games. Rigorously, communication games should be framed as a special case of Markov Games that provide a broader formal framework for reasoning about multi-agent problems. For further information, refer to Littman.\n\n###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### In a communication game, the deep neural agents aim to build communication and action policies. This is realized by maximizing their reward. The following is therefore needed:\n\n1. Design the communicative agents as neural networks.\n2. Train agents to build a shared communication protocol.\n\nFigures 3 and 4 represent communication games commonly studied in language emergence simulations with deep learning. The former presents simple Lewis and negotiation games, while the latter showcases efforts to build more realistic scenarios.\n\nRemark: At the time of writing, many Python libraries, like PyTorch and Jax, are used for easy implementation and optimization of neural networks and are particularly helpful for beginners due to the abundance of online examples."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Designing communicating agents with neural networks",
            "text": "To model communicative agents, we first break them into functional modules that enable interaction with the environment and other agents. Then, define neural networks and explain how they can be used to parameterize these functional modules. Finally, we introduce neural senders and receivers as specific types of neural communicative agents.\n\nThe perception module maps an observation of the environment to an internal representation. The choice of architecture depends on the input observation, which differs across games. For example, a Convolutional Neural Network is suitable for generating image representations from visual input data.\n\nThe generation module maps an internal representation, i.e., a vector of a given dimension, into a message. Recurrent neural networks (RNN) and Transformers are well suited for sequences and are hence used in standard emergent communication settings. Communication is mainly based on discrete messages, even if some works consider continuous communication protocol.\n\nRemark: To shape the message space, a vocabulary of symbols and a maximum length must be introduced. It\u2019s also possible to add an end-of-sentence token EoS to indicate the end of the message. When making these design choices, task complexity should be considered; a larger vocabulary and message length allow for communicating more information/concepts, while a smaller vocabulary and message length require better information compression and, hence, a more structured communication protocol.\n\nThe understanding module maps a message to an internal representation. Since messages are discrete sequences, RNNs, and Transformers are well-suited for this module.\n\nThe action module maps an internal representation to an action in the environment. Since the internal representations are scalars and actions a finite set of possibilities, a well-suited architecture is the Multi-Layer Perceptron followed by a softmax that draws a probability distribution over the potential actions.\n\nRemark: Deep learning techniques allow training a system composed of multiple differentiable modules end-to-end. The agent is seen as a single block that provides a prediction given input and output data instead of past methods that glue independently trained/designed blocks together. In communication games, the sender and receiver are both fully-differentiable individually. However, the message generation between them does not necessitate on purpose to separate the training of the agents.\n\nGood practice: Exploring various neural architectures is a common reflex when starting with deep learning. However, its impact is limited in practice compared to other experimental choices such as task definition, optimization, data, and training objective. Basic neural architectures are recommended to avoid compounding factors when comparing methods."
        },
        {
            "section_id": "2.2.1",
            "parent_section_id": "2.2",
            "section_name": "2.2.1 Designing a communicative agent as functional modules",
            "text": "As depicted in Figure 5  ###reference_###, a communicative agent should be able to interact with:\nIts environment by either passively observing it or actively taking actions that influence it;\nAnother agent using a message space by passively receiving or actively sending messages.\nTherefore, four functional modules are typically needed to model agents: perception, generation, understanding, and action. (1) The perception module maps an environment\u2019s view to an internal representation, (2) the generation module generates a message based on internal representations, (3) the understanding module takes a message and builds an internal message representation, (4) the action module maps an internal representation to an action in the environment.\nNeural networks are suited for modeling and combining these modules."
        },
        {
            "section_id": "2.2.2",
            "parent_section_id": "2.2",
            "section_name": "2.2.2 Short introduction to neural networks",
            "text": "A neural network is a parametric model approximating a function or probability distribution based on data. It maps vector inputs to outputs through a succession of linear and non-linear operations. Its learnable parameters, called the weights, are used to perform the linear operations. The fundamental building block of a neural network is made of two operations:\n\n1. A linear transformation applying the matrix of weights to the incoming input.\n2. A non-linear transformation, called the activation function (typically sigmoid function, hyperbolic tangent or ReLU).\n\nAs displayed in Figure 6, these operations are stacked at each layer, transforming the input to a prediction through multiple linear and non-linear transformations. Neural networks have a crucial property: all operations are differentiable. This allows for using gradient-based methods to learn the weights.\n\nWhen training a neural network, the goal is to find the optimal weights such that the neural network accurately maps inputs to their corresponding outputs. Neural networks with enough weights can represent complex functions due to their high expressive power, approximating any continuous function with any level of precision. However, computation or data limitations can hinder this process. Deep learning investigates how to adapt networks\u2019 architecture or weight matrix shape to overcome these limitations. Figure 7 presents the main network architectures and the data they are suitable for."
        },
        {
            "section_id": "2.2.3",
            "parent_section_id": "2.2",
            "section_name": "2.2.3 Neural functional modules",
            "text": "Several network architectures can be considered when designing agent modules as defined in Sec 2.2.1. This section presents some common choices for each module.\n\nThe perception module maps an observation of the environment to an internal representation. The choice of architecture depends on the input observation, which differs across games. For example, a Convolutional Neural Network (LeCun et al., 1988) is suitable for generating image representations from visual input data, as illustrated in Figure 7.\n\nThe generation module maps an internal representation, i.e., a vector of a given dimension, into a message. Recurrent neural networks (RNN) (Elman, 1990; Mikolov et al., 2010) and Transformers (Vaswani et al., 2017) are well suited for sequences and are hence used in standard emergent communication settings (Lazaridou et al., 2018; Chaabouni et al., 2019; Kottur et al., 2017; Li and Bowling, 2019; Chaabouni et al., 2022; Rita et al., 2022a). Communication is mainly based on discrete messages, even if some works consider continuous communication protocols (Tieleman et al., 2019).\n\nRemark: To shape the message space, a vocabulary of symbols and a maximum length must be introduced. It\u2019s also possible to add an end-of-sentence token EoS to indicate the end of the message. When making these design choices, task complexity should be considered; a larger vocabulary and message length allow for communicating more information/concepts, while a smaller vocabulary and message length require better information compression and, hence, a more structured communication protocol.\n\nThe understanding module maps a message to an internal representation. Since messages are discrete sequences, RNNs, and Transformers are well-suited for this module.\n\nThe action module maps an internal representation of an action in the environment. Since the internal representations are scalars and actions a finite set of possibilities, a well-suited architecture is the Multi-Layer Perceptron followed by a softmax that draws a probability distribution over the potential actions.\n\nRemark: Deep learning techniques allow training a system composed of multiple differentiable modules end-to-end. The agent is seen as a single block that provides a prediction given input and output data instead of past methods that glue independently trained/designed blocks together. In communication games, the sender and receiver are both fully-differentiable individually. However, the message generation between them does not necessitate on purpose to separate the training of the agents. Nonetheless, the message generation can still be made differentiable as described in Section 2.3.2.\n\nGood practice: Exploring various neural architectures is a common reflex when starting with deep learning. However, its impact is limited in practice compared to other experimental choices such as task definition, optimization, data, and training objective. Basic neural architectures are recommended to avoid compounding factors when comparing methods."
        },
        {
            "section_id": "2.2.4",
            "parent_section_id": "2.2",
            "section_name": "2.2.4 Modeling neural network communicative agents in communication games",
            "text": "Section 2.2 presents the components of a general communicative agent, though not all modules may be used during a game. Figure 8 illustrates sender and receiver modeling in a unidirectional game. This modeling is used in the use case we derive in Section 3, namely the Visual Discrimination Game."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Optimizing the agents to solve the game",
            "text": "In Deep Learning, the goal is to train neural networks to solve a task, i.e., find the optimal weights that maximize their performance. This section covers optimization techniques for training neural networks and their application to communication games. \n\nTo train neural networks, suitable learning techniques must be chosen depending on the task and the availability of training data, which consists of input-output pairs. Two standard techniques used to solve communication games are:\n\nSupervised Learning (SL): The neural network is given a training set of input-output pairs, and its objective is to learn how to map inputs to their corresponding outputs. An example of a supervised language task is translation: the network learns to map one language to another by training on pairs where each pair consists of aligned source and target sentences. Supervised learning finds the weights that enable the network to generalize this mapping to new, unseen examples drawn from the same distribution as the training data, e.g., trying to translate beyond the initial corpus. In communication games, supervised learning tasks often involve classification (e.g., object selection, attribute reconstruction, translation) and regression (e.g., drawing, pixel reconstruction).\n\nReinforcement learning (RL): In RL, a neural network, or agent, must perform a sequence of actions to resolve a task within its environment. These actions yield rewards that gauge the effectiveness of the network\u2019s task performance. The network is then optimized to maximize its expected reward, i.e., performing the sequence of actions that lead to the highest task success. Noteworthy, the probability of action is called a policy in RL. In communication games, the sender produces a sequence of symbols to assist the receiver in completing a predetermined task. If this sequence leads to a successful outcome, the sender is rewarded positively; otherwise, it receives a negative reward. Through iterative trial and error, the sender refines its sequence of symbols toward maximizing its reward and ultimately solving the game.\n\nSupervised learning is easy to apply and highly reproducible but requires a known target. On the other hand, reinforcement learning is more generic and only requires a score to be defined at the cost of being more complex. For instance, to train a network to play chess, supervised learning would involve imitating the moves of a pro-player with a dataset, while reinforcement learning would require playing the whole game and rewarding victories: the training is more complex and slower but does not require data. It is noteworthy that the reinforcement learning reward can be defined arbitrarily. This approach has been applied to train large dialogue systems by imitating the human language and refining it with reinforcement learning.\n\nRegardless of the learning technique, the task\u2019s success is optimized by introducing a proxy, the loss function. The goal is then to find weights such that the neural network minimizes the average loss function over the entire training dataset. Loss functions vary depending on the network output and the training task.\n\nThe loss function is reduced using a learning process that involves a series of updates known as Gradient Descent updates. They iteratively adjust the network\u2019s parameters by following the loss gradient. The magnitude of the update is controlled by a hyperparameter called the learning rate. The goal is to find weights such that the loss gradient equals zero. This is achieved by repeating the gradient update rule.\n\nIn practice, computing the exact gradient of the averaged loss function is infeasible since it necessitates processing the complete dataset. Stochastic Gradient Descent overcomes this challenge by approximating the loss function gradient using a limited number of data samples, or batches, at each iteration. In standard machine learning libraries, Stochastic Gradient Descent updates are performed by pre-implemented methods referred to as optimizers. In communication games, this gradient is the mathematical operation that modifies the agent behavior. For instance, every single speaker update alters its generation of symbols, refining its emergent language step by step toward maximizing the reward objective.\n\nTraining a model involves minimizing the loss of the training data, but evaluating its performance on unseen data is crucial to ensure the network\u2019s quality. Intuitively, it is like creating an exam for students with unseen exercises to ensure they correctly understand the lecture. ML Practitioners distinguish (1) the training dataset and its corresponding loss, (2) the test dataset with unseen samples and its corresponding loss. The relation between the two losses indicates how well the model generalizes and can be trusted.\n\nWhen training a model, it is recommended to divide the dataset into three parts: training, validation, and testing (typical proportion). The training set is used to train the model, the validation set to find the generalization regime, tune hyperparameters, and retrieve the best model across training, and the test set is used to test the model and report the final score. Intuitively, validation data is similar to mock exams, whereas test data is the actual network exam. In practice, the validation loss is regularly plotted, and when it starts increasing, training is stopped. This technique is known as early stopping.\n\nRegularization methods were developed to prevent"
        },
        {
            "section_id": "2.3.1",
            "parent_section_id": "2.3",
            "section_name": "2.3.1 Optimizing a machine learning problem",
            "text": "To train neural networks, suitable learning techniques must be chosen depending on the task and the availability of training data, which consists of input-output pairs. Two standard techniques used to solve communication games are:\n\nSupervised Learning (SL): The neural network is given a training set of input-output pairs, and its objective is to learn how to map inputs to their corresponding outputs. An example of a supervised language task is translation: the network learns to map one language to another by training on pairs, where each pair consists of aligned source and target sentences. Supervised learning finds the weights that enable the network to generalize this mapping to new, unseen examples drawn from the same distribution as the training data, e.g., trying to translate beyond the initial corpus. In communication games, supervised learning tasks often involve classification (e.g., object selection, attribute reconstruction, translation) and regression (e.g., drawing, pixel reconstruction).\n\nReinforcement learning (RL): In RL, a neural network, or agent, must perform a sequence of actions to resolve a task within its environment. These actions yield rewards that gauge the effectiveness of the network\u2019s task performance. The network is then optimized to maximize its expected reward, i.e., performing the sequence of actions that lead to the highest task success. Noteworthy, the probability of action is called a policy in RL. In communication games, the sender produces a sequence of symbols to assist the receiver in completing a predetermined task. If this sequence leads to a successful outcome, the sender is rewarded positively; otherwise, it receives a negative reward. Through iterative trial and error, the sender refines its sequence of symbols toward maximizing its reward and ultimately solving the game.\n\nSupervised learning is easy to apply and highly reproducible but requires a known target. On the other hand, reinforcement learning is more generic and only requires a score to be defined at the cost of being more complex. For instance, to train a network to play chess, supervised learning would involve imitating the moves of a pro-player with a dataset, while reinforcement learning would require playing the whole game and rewarding victories: the training is more complex and slower, but it does not require data. It is noteworthy that the reinforcement learning reward can be defined arbitrarily, e.g., one may give an extra bonus when winning the game while preserving the queen, or it could also be used on top of a supervised training regime. This approach has been applied to train large dialogue systems by imitating the human language and refining it with reinforcement learning.\n\nRegardless of the learning technique, the task\u2019s success is optimized by introducing a proxy, the loss function. The goal is then to find weights such that the neural network minimizes the average loss function over the entire training dataset. Loss functions vary depending on the network output and the training task. In reinforcement learning, the losses often include the TD error or the score function, which converts the expected sum of rewards as a training objective. \n\nThe loss function is reduced using a learning process that involves a series of updates known as Gradient Descent updates. They iteratively adjust the network\u2019s parameters by following the loss gradient. The magnitude of the update is controlled by a hyperparameter called the learning rate. Given the optimization problem, the goal is to find weights such that the loss gradient equals. This is achieved by repeating the following gradient update rule: where and are the model parameters respectively at iteration and, the gradient of the loss function and the learning rate.\n\nIn practice, computing the exact gradient of the averaged loss function is infeasible since it necessitates processing the complete dataset. Stochastic Gradient Descent overcomes this challenge by approximating the loss function gradient using a limited number of data samples, or batches at each iteration. In standard machine learning libraries, Stochastic Gradient Descent updates are performed by pre-implemented methods referred to as optimizers. In communication games, this gradient is the mathematical operation that modifies the agent behavior. For instance, every single speaker update alters its generation of symbols, refining its emergent language step by step toward maximizing the reward objective.\n\nTraining a model involves minimizing the loss of the training data, but evaluating its performance on unseen data is crucial to ensure the network\u2019s quality. Intuitively, it is like creating an exam for students with unseen exercises to ensure they correctly understand the lecture. ML Practitioners distinguish (1) the training dataset and its corresponding loss, (2) the test dataset with unseen samples and its corresponding loss. The relation between the two losses indicates how well the model generalizes and can be trusted.\n\nUnderfitting: Both and are high, indicating ineffective learning. An under-parametrized network or a small learning rate may cause persistent under-fitting. In communication games, this scenario arises when no successful communication emerges between the sender and receiver, resulting in a poor task success both on and .\n\nGeneralization: Both and are low, indicating successful training and generalization. In communication games, this regime occurs when agents develop a successful communication"
        },
        {
            "section_id": "2.3.2",
            "parent_section_id": "2.3",
            "section_name": "2.3.2 Optimizing communication games with machine learning",
            "text": "Unlike a single network training, two networks are trained simultaneously during a communication game, sometimes requiring different learning methods for each agent. The process involves selecting appropriate (1) learning methods, (2) rewards and loss functions, and (3) optimization protocols.\n\nRemark:  The machine learning community has developed frameworks for simulating various communication games, which can be rapidly replicated, understood, and modified. Existing codebases include (Kharitonov et al., 2019) and (Chaabouni et al., 2022) as long as the detailed notebook we provide.\n\nThree learning pipelines are mainly used to train agents in communication games:\n\nBoth agents optimized with RL: This generic and realistic setting assumes no specific task format and involves separate agents with individual rewards and training losses, making it suitable for training any task. However, such training is usually hard to optimize with high variance and requires careful use of RL tools we introduce later.\n\nSender optimized with RL and Receiver optimized with SL: This approach is well-suited for single-turn message games where the receiver only needs to perform one valid action after receiving a message, such as in referential games (Lewis, 1969, Skyrms, 2010). In such cases, the receiver\u2019s action is fully determined by the sender\u2019s observation and its message, creating a supervised training sample for the receiver. The receiver\u2019s training becomes more robust by learning to map messages to the corresponding output actions using a supervised loss. Note that the sender still needs to be optimized with RL since message generation is non-differentiable, i.e., the receiver\u2019s error cannot propagate to the sender. It ensures more stable training than using a pure RL reward-based approach.\n\nBoth agents optimized with SL: When both agents cooperate fully and optimize the same learning signal, they can be trained using a single supervised training signal. In this scenario, the Sender-Receiver couple is optimized as a single network that maps inputs to output actions, with a discrete intermediate layer. Reparametrization tricks such as Gumbel-Softmax (Jang et al., 2016, Maddison et al., 2016) have been developed to overcome the non-differentiability of message generation and allow the receiver\u2019s error to flow to the sender. \n\nWe next derive the case where agents are optimized with RL as it covers all communication tasks. Reward functions and must be defined to measure the success of the communication task for each agent. These functions typically take agents\u2019 observations and and the receiver\u2019s action as input and return if the task is solved, otherwise.\n\nRemark:  The reward is the core element inducing the structure of the emergent language. Thus, we recommend carefully avoiding designing rewards toward obtaining a specific language, e.g., directly rewarding compositionality or syntactic properties. Instead, we suggest using rewards that measure communication success without any human prior. Hence, language features may emerge from solving a specific task rather than being forced by design. The agents\u2019 goal is to maximize their respective reward over time, i.e., the expected rewards: denotes a game episode that depends on the sender\u2019s and receiver\u2019s stochastic policies. The sender message and the receiver\u2019s action are sampled from those distributions. In reinforcement learning, the goal is to minimize the expected negative reward. However, this objective cannot be directly turned into a gradient update as the reward is not differentiable by definition. Mathematical tools have been developed to circumvent this issue (Sutton and Barto, 2018).\n\nThe policy-gradient algorithm (Sutton et al., 1999) is mostly used in neural language emergence. Denoting by and the sender and receiver\u2019s respective loss gradient, we have:\n\nIn practice, the quantities and are computed over a batch of game episodes and passed to each agent optimizer. is the stop gradient operator that prevents an optimizer from computing the gradient inside the operator. The optimization encounters challenges, for which we provide a few recipes to ensure a successful optimization process:\n\nImplementing Policy Gradient: While RL notations may become overwhelming for beginners, their implementation is quite straightforward in practice with recent machine learning libraries (Paszke et al., 2019, Bradbury et al., 2018).\n\nDealing with large variance: Estimating the gradient of a RL loss is difficult due to the large variance of gradient estimates. Large batch sizes and the baseline method should be used to alleviate this. The latter implies subtracting a baseline from the reward, which does not bias the estimate while reducing the variance. A common baseline is the average value of the reward across a batch of data.\n\nControlling the exploration-exploitation trade-off: To prevent the collapse of training due to sub-optimal average reward, one can control the exploitation-exploration trade-off by penalizing the entropy of the policies with the terms and ( and refers to the entropy function applied on agents\u2019 policies) and where is the entropy function. By increasing the coefficient (resp. ), the sender"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Case study: Simulating a Visual Discrimination Game",
            "text": "We now focus on a particular communication game: the Visual Discrimination Game, a type of Lewis Referential Games. These games, which explore how languages emerge through their use, have been extensively studied from theoretical and experimental angles in language evolution.\n\nThe Visual Discrimination Game involves two players: a sender and a receiver. The game proceeds as follows:\n- The sender sees an image and communicates about it to the receiver.\n- Using the message, the receiver has to guess the original image seen by the sender among a set of candidate images.\n- The original image is revealed, and the two players are informed about the task\u2019s success.\n\nAgents play the game repeatedly until they synchronize on a communication protocol that enables the receiver to distinguish any image from any set of distractors.\n\nThe following parameters must be specified:\n1. **Image dataset**: This is the set of images the agents must communicate about. Machine learning experiments can be conducted with large-scale datasets, which is critical for developing a rich communication protocol. For example, some studies have relied on ImageNet, a million images dataset spanning more than categories including animals, vehicles, objects, or instruments. Synthetic datasets, like CLEVR, are also valuable for evaluating agents\u2019 ability to communicate about ambiguous images using compositional languages.\n\n2. **Number of candidate images**: The receiver must differentiate the original image from distractor images. The task\u2019s difficulty depends on the value of N: a higher N requires a more precise communication protocol.\n\n3. **Message space**: The message space is shaped by the vocabulary V and message maximum length L. Adjusting those parameters crucially influences the sender\u2019s expressiveness. By denoting the vocabulary size by V, the sender can use a total number of V^L messages.\n\nUsing previous notations:\n- Sender\u2019s observation x is an image sampled from the dataset.\n- Receiver\u2019s observation y is a set of N images sampled from the dataset that includes the sender\u2019s observation x.\n- Message m is a message sent by the sender.\n- Action a is the choice of image among the set of N images.\n\nFollowing agents' design, figure reports standard agents' design choices in the Visual Discrimination Game. \n\nA typical reward function assigns a reward of R = 1 if the receiver picks up the correct image and 0 otherwise. The modeling parameters, which include the vocabulary V, maximum message length L, and the number of candidates N, should be selected based on the problem under investigation.\n\nFor the optimization, we recommend using a large batch size and one Adam optimizer per agent. The other parameters, including exploration coefficients and learning rates, are interdependent and should be adjusted simultaneously until the simulation works. Common strategies for parameter tuning include manual adjustment or more systematic methods like grid search.\n\nA full implementation of the game with technical details and a starting set of working parameters is provided at:\nhttps://github.com/MathieuRita/LangageEvolution_with_DeepLearning"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Bridging the gap between neural networks and humans in language evolution simulations",
            "text": "This section focuses on current endeavors in using deep learning as a framework for language evolution simulations. It covers the field\u2019s progress in using neural networks to replicate human languages and highlights the potential and challenges of deep learning simulations. Neural network simulations provide extensive flexibility for modeling various aspects of language emergence simulations, including the game, inputs, and agents. Two primary strategies have been pursued: simplifying experiments into controllable settings (Kottur et al., 2017 ###reference_b30###, Chaabouni et al., 2019 ###reference_b63###; 2020 ###reference_b100###, Ren et al., 2020 ###reference_b101###, Rita et al., 2022a ###reference_b66###), assessing the influence of incremental modeling elements; and creating more humanly plausible scenarios that emulate language emergence in complex environments (Das et al., 2019 ###reference_b102###, Jaques et al., 2019 ###reference_b45###). It has resulted in various tasks, from basic referential tasks to complex ecological tasks in grounded environments (Das et al., 2019 ###reference_b102###). In terms of inputs, it spans from hand-designed structured and controllable inputs (Kottur et al., 2017 ###reference_b30###, Chaabouni et al., 2019 ###reference_b63###; 2020 ###reference_b100###, Ren et al., 2020 ###reference_b101###, Rita et al., 2020 ###reference_b103###; 2022a ###reference_b66###) to complicated visual inputs (Evtimova et al., 2017 ###reference_b104###, Lazaridou et al., 2018 ###reference_b31###, Dess\u00ec et al., 2021 ###reference_b90###, Chaabouni et al., 2022 ###reference_b65###, Rita et al., 2022b ###reference_b78###). As for agents, it extends from pairs of agents decomposed into senders and receivers to pairs of bidirectional agents (Bouchacourt and Baroni, 2018 ###reference_b105###, Graesser et al., 2019 ###reference_b106###, Taillandier et al., 2023 ###reference_b107###, Michel et al., 2023 ###reference_b108###) and populations (Tieleman et al., 2019 ###reference_b67###, Graesser et al., 2019 ###reference_b106###, Rita et al., 2022a ###reference_b66###, Michel et al., 2023 ###reference_b108###).\n\nSimulations give rise to the emergence of artificial languages whose properties are compared to human languages. As human languages can be described in terms of language universals, i.e., abstract properties found across all human languages, studies have tried to establish the conditions under which those universal properties emerge. Such universals mainly include compositionality, i.e., the ability to decompose the meaning of an utterance as a function of its constituents (Hockett, 1960 ###reference_b109###), measured through topographic similarity (Brighton and Kirby, 2006 ###reference_b110###), (Chaabouni et al., 2020 ###reference_b100###), or Tree Reconstruction Error (Andreas, 2019 ###reference_b111###); efficiency, i.e., efficient information compression, measured through message length statistics and semantic categorization (Zipf, 1949 ###reference_b112###, Regier et al., 2015 ###reference_b113###); demographic trends, such as the impact of population size, contact agents proportion, network topology on language structure (Clyne, 1992 ###reference_b114###, Wray and Grace, 2007 ###reference_b115###, Wagner, 2009 ###reference_b116###, Gary Lupyan, 2010 ###reference_b117###).\n\nA first approach is to question whether the most simple communication task, i.e., referring to objects in an environment through referential communication, is enough to see human language features emerge. The first works on referential tasks showed that neural agents could successfully derive a communication protocol from solving the task (Kottur et al., 2017 ###reference_b30###, Lazaridou et al., 2016 ###reference_b89###, Havrylov and Titov, 2017 ###reference_b118###). Still, such protocols are neither interpretable nor bear the core properties of human languages. Indeed, agents tasked with communicating about images do not utilize semantically significant concepts but instead shortcut the task by basing their messages on low-level visual features (Lazaridou et al., 2016 ###reference_b89###, Havrylov and Titov, 2017 ###reference_b118###, Cha"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Opportunities opened by deep learning simulations",
            "text": "Neural network simulations provide extensive flexibility for modeling various aspects of language emergence simulations, including the game, inputs, and agents. Two primary strategies have been pursued: simplifying experiments into controllable settings (Kottur et al., 2017, Chaabouni et al., 2019; 2020, Ren et al., 2020, Rita et al., 2022a), assessing the influence of incremental modeling elements; and creating more humanly plausible scenarios that emulate language emergence in complex environments (Das et al., 2019, Jaques et al., 2019). It has resulted in various tasks, from basic referential tasks to complex ecological tasks in grounded environments (Das et al., 2019). In terms of inputs, it spans from hand-designed structured and controllable inputs (Kottur et al., 2017, Chaabouni et al., 2019; 2020, Ren et al., 2020, Rita et al., 2020; 2022a) to complicated visual inputs (Evtimova et al., 2017, Lazaridou et al., 2018, Dess\u00ec et al., 2021, Chaabouni et al., 2022, Rita et al., 2022b). As for agents, it extends from pairs of agents decomposed into senders and receivers to pairs of bidirectional agents (Bouchacourt and Baroni, 2018, Graesser et al., 2019, Taillandier et al., 2023, Michel et al., 2023) and populations (Tieleman et al., 2019, Graesser et al., 2019, Rita et al., 2022a, Michel et al., 2023).\n\nSimulations give rise to the emergence of artificial languages whose properties are compared to human languages. As human languages can be described in terms of language universals, i.e., abstract properties found across all human languages, studies have tried to establish the conditions under which those universal properties emerge. Such universals mainly include compositionality, i.e., the ability to decompose the meaning of an utterance as a function of its constituents (Hockett, 1960), measured through topographic similarity (Brighton and Kirby, 2006), (Chaabouni et al., 2020), or Tree Reconstruction Error (Andreas, 2019); efficiency, i.e., efficient information compression, measured through message length statistics and semantic categorization (Zipf, 1949, Regier et al., 2015); demographic trends, such as the impact of population size, contact agents proportion, network topology on language structure (Clyne, 1992, Wray and Grace, 2007, Wagner, 2009, Gary Lupyan, 2010)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Do neural networks replicate human behaviors?",
            "text": "To provide valuable insights through deep learning simulations, replicating human languages is essential. This involves identifying the basic assumptions needed for artificial agents to display human-like language patterns in their communication protocols. A first approach is to question whether the most simple communication task, i.e., referring to objects in an environment through referential communication, is enough to see human language features emerge. The first works on referential tasks showed that neural agents could successfully derive a communication protocol from solving the task (Kottur et al., 2017, Lazaridou et al., 2016, Havrylov and Titov, 2017). Still, such protocols are neither interpretable nor bear the core properties of human languages.\n\nIndeed, agents tasked with communicating about images do not utilize semantically significant concepts but instead shortcut the task by basing their messages on low-level visual features (Lazaridou et al., 2016, Havrylov and Titov, 2017, Chaabouni et al., 2022, Bouchacourt and Baroni, 2018). Additionally, when agents communicate about hand-designed structured sets of objects in a simple referential task, fundamental properties of natural languages such as compositionality (Kottur et al., 2017, Chaabouni et al., 2020) or efficiency (Zipf, 2016) do not spontaneously arise (Chaabouni et al., 2019).\n\nEventually, when referential games are played within a population of agents, human demographic trends are not reproduced. Population size does not behave as a regularization factor (Li and Bowling, 2019, Cogswell et al., 2019, Rita et al., 2022a, Chaabouni et al., 2022) and agents do not synchronize on a shared protocol (Rita et al., 2022a, Michel et al., 2023). Understanding the origins of these discrepancies from either an optimization or modelization perspective is an active research question.\n\nTo recover human language features, different human-inspired constraints have incrementally been added to simulations. Inspired by Iterated Learning (Kirby, 2001, Kirby et al., 2014), one line of research has explored the effects of learnability constraints on language emergence by altering learning dynamics. Li and Bowling (2019), Ren et al. (2020), Cogswell et al. (2019) implement neural variants of Iterated Learning by periodically introducing newborn agents and mimicking generational transmission. They find that those learning constraints drive the selection of more compositional languages, as they are easier to learn (Li and Bowling, 2019).\n\nAnother line of research focuses on incorporating cognitively inspired biases into agent modeling. For example, Rita et al. (2020) show that Zipf\u2019s Law of Abbreviation (Zipf, 1949) emerges when both pressures toward Least Effort production (Zipf, 1949, Piantadosi et al., 2011, Kanwal et al., 2017) and comprehension laziness are introduced.\n\nEventually, some researchers have refined population modeling. Rita et al. (2022a) introduce learning speed variations into populations and recover the relationship between population size and language structure reported in previous works (Gary Lupyan, 2010, Meir et al., 2012, Dryer and Haspelmath, 2013, Reali et al., 2018, Raviv et al., 2019). Graesser et al. (2019) examine contact agents phenomena and show that a contact language can either converge towards the majority protocol or result in novel creole languages, depending on the inter- and intra-community densities. Kim and Oh (2021) and Michel et al. (2023) study how social graph connectivity impacts the development of shared languages."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Toward realistic experiments",
            "text": "Although incorporating human-inspired constraints shows promise for replicating human language features, the simplicity of current models remains limited. An avenue is opened for the design of humanly plausible experiments. We present efforts to build more realistic models and discuss the associated challenges here. Task-specific communication games may be restrictive as they overlook other aspects of our language, such as conversation, interaction with the physical world, and other modalities. More realistic scenarios are needed to encompass all aspects of our language. Some attempts have been made to create more plausible settings. Chaabouni et al. (2022) complexify the referential task by scaling the game to large datasets and tasking agents to retrieve images among distractors. Evtimova et al. (2017), Taillandier et al. (2023) model conversation by building bidirectional agents for multi-turn communications; Bullard et al. (2020) explore nonverbal communication using spatially articulated agents; (Das et al., 2019) ground agents in more realistic 2D and 3D environments; Jaques et al. (2019) test agents' ability to solve social dilemmas in grounded environments.\n\nHowever, making more realistic games poses both technical and analytical challenges. Training instabilities can occur when games become more complex, requiring optimization tricks (Chaabouni et al., 2022). Moreover, as environments become more complex, the emergence of language is more challenging to analyze. For example, Lowe et al. (2020) demonstrates how agents can solve complex tasks with shallow communication protocols and why new tools are needed to assess emergent languages qualitatively and quantitatively in these situations. Many neural communication agents are designed for specific games and lack crucial aspects of human cognition. For instance, agents are often limited to either speaking or listening, which overlooks the interplay between comprehension and production (Galke et al., 2022).\n\nSome works propose more realistic agents. These include bidirectional agents that both speak and listen (Bouchacourt and Baroni, 2018, Graesser et al., 2019, Michel et al., 2023, Taillandier et al., 2023), as well as agents with restricted memory capacity that better mirrors human cognition (Resnick et al., 2019). Additionally, Rita et al. (2020) incorporate the Least Effort Principle to make agents efficient encoders (Zipf, 1949, Piantadosi et al., 2011, Kanwal et al., 2017).\n\nStill, despite the impact of these modeling constraints on emergent language properties, they are not consistently applied across the literature. One of the main limitations of neural emergent languages is that current metrics may not capture crucial features of human languages. For instance, most work only uses topographic similarity (Lazaridou et al., 2018, Li and Bowling, 2019) as a structural metric (Brighton and Kirby, 2006), which assumes that the units of the message carry out the meaning. In human languages, the meaning units are the results of a combinatorial process using nonmeaningful units, such as phonetic features or phonemes (the so-called double articulation phenomenon (Martinet, 1960); or duality of patterning (Hockett, 1970)). Other universal properties of language (formal universals (Chomsky and Halle, 1968)) include the reliance on symbols and rules (Fodor and Pylyshyn, 1988), the use of hierarchical representations or long distance dependencies (Hauser et al., 2002), the existence of part-of-speech classes (Rijkhoff, 2007) such as the distinction between content and grammatical words, the existence of deixis (Lyons, 1977), i.e. the use of certain parts of the message to refer to places or time or person relative to the context of elocution of the message, and many others.\n\nStudying such properties is challenging as it requires the design of adapted measures that could be computed both on human and artificial languages. Furthermore, current artificial settings are often too simple to drive the emergence of such properties, reinforcing the need for more realistic scenarios that translate into our environment\u2019s complexity. One area of research focuses on investigating whether language emergence simulations can potentially enhance natural language processing tasks. One approach involves pre-training language models with artificial languages that emerged from communication games, resulting in a moderate boost when fine-tuning low-resource language tasks (Yao et al., 2022). Another approach is exploring machine-machine interaction to learn an emergent communication protocol that prompts large language models (Shin et al., 2020, Deng et al., 2022). Reciprocally, natural language models can be utilized to explore language evolution from pre-trained languages, such as studying creolization (Armstrong et al., 2022"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Deep learning advancements offer new opportunities for simulating language evolution, as neural networks can handle diverse data without pre-defined human priors. They scale significantly regarding dataset size, task complexity, and number of participants or generations. This opens up possibilities for creating realistic language evolution scenarios at unprecedented scales. Reciprocally, language evolution research can provide valuable insights for developing future deep learning models. In the journey toward building intelligent language models, it seems essential to incorporate constraints and mechanisms that shape the development and evolution of language, such as perceptual, social, or environmental pressures. We hope this chapter will encourage researchers in both language evolution and deep learning to collaborate and jointly explore those two captivating black-boxes: humans and neural networks."
        }
    ],
    "appendix": [],
    "tables": {},
    "image_paths": {
        "1": {
            "figure_path": "2403.11958v1_figure_1.png",
            "caption": "Figure 1: Iterative learning process in a machine learning problem. Step 1 (simulation): The computer program f\ud835\udc53fitalic_f performs an experience E\ud835\udc38Eitalic_E of the task T\ud835\udc47Titalic_T. Step 2 (measure): The task\u2019s success is measured through a performance measure P\ud835\udc43Pitalic_P. Step 3 (improvement): Based on its performance, the computer program f\ud835\udc53fitalic_f update to improve its future performance, i.e. learns. Communication games can be framed as a machine learning problem by modeling agents as computer programs. The experience E\ud835\udc38Eitalic_E corresponds to an episode of the game, while a performance measure P\ud835\udc43Pitalic_P measures the game\u2019s success."
        },
        "2": {
            "figure_path": "2403.11958v1_figure_2.png",
            "caption": "Figure 2: Scheme of a two-player communication game. Step 1 (simulation): Agents play a round of the game: (1) both agents get observations from the environment, (2) The sender sends a message to the receiver, (3) The receiver uses the message and its observation to perform an action in the environment. Step 2 (measure): One reward signal per agent measures the game\u2019s success. Step 3 (improvement): Agents receive the reward signals and update their behavior toward better solving the game."
        },
        "3": {
            "figure_path": "2403.11958v1_figure_3.png",
            "caption": "(a)"
        },
        "4": {
            "figure_path": "2403.11958v1_figure_4.png",
            "caption": "(b)"
        },
        "5": {
            "figure_path": "2403.11958v1_figure_5.png",
            "caption": "(c)"
        },
        "6": {
            "figure_path": "2403.11958v1_figure_6.png",
            "caption": "(d)"
        },
        "7": {
            "figure_path": "2403.11958v1_figure_7.png",
            "caption": "(a) Instruction following games. \nImage from (Kalinowska et al., 2022)"
        },
        "8": {
            "figure_path": "2403.11958v1_figure_8.png",
            "caption": "(b) Coordination games. \nImage from (Carroll et al., 2019)"
        },
        "9": {
            "figure_path": "2403.11958v1_figure_9.png",
            "caption": "(c) Social dilemma games\nImage from (Jaques et al., 2019)"
        },
        "10": {
            "figure_path": "2403.11958v1_figure_10.png",
            "caption": "Figure 5: General view of a communicative agent. A communicative agent is composed of four functional modules: a perception module that maps an observation to an internal representation; a generation module that maps an internal representations to a message; an understanding module that maps a message to an internal representation; an action module that maps internal representations to an action."
        },
        "11": {
            "figure_path": "2403.11958v1_figure_11.png",
            "caption": "Figure 6: Scheme of a neural network and the operations between two layers. A neural network is a function that takes an input x=(x1,\u2026,xn)\ud835\udc65subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc5bx=(x_{1},...,x_{n})italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) and maps it to an output y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG. It comprises several layers of activations. Each layer results from the application of two operations on the activations of the previous layers: (1) a linear transformation (multiplication by weights and sum), (2) a non-linear transformation (application of a non-linear function). A neural network is parameterized by all the weights acting between each layer."
        },
        "12": {
            "figure_path": "2403.11958v1_figure_12.png",
            "caption": "(a) Multi layer perceptron (MLP)"
        },
        "13": {
            "figure_path": "2403.11958v1_figure_13.png",
            "caption": "(b) Convolutional Neural Networks (CNN)"
        },
        "14": {
            "figure_path": "2403.11958v1_figure_14.png",
            "caption": "(c) Recurrent Neural Networks (RNN)"
        },
        "15": {
            "figure_path": "2403.11958v1_figure_15.png",
            "caption": "(d) Transformer"
        },
        "16": {
            "figure_path": "2403.11958v1_figure_16.png",
            "caption": "Figure 8: Summary of neural communicative agents modeling in a unidirectional communication game. The sender only uses a perception module to process observations and a generation module to create messages. The receiver uses a perception module to process observations, an understanding module to process messages, and an action module to interact with the environment. A vocabulary and maximum message length are defined for the sender\u2019s generation. All the experimental choices are framed in blue."
        },
        "17": {
            "figure_path": "2403.11958v1_figure_17.png",
            "caption": "Figure 9: 1D representation of the effect of adjusting the learning rate. (a) Small learning rate: parameter updates are too parsimonious; the optimum may be attained, but it requires a large number of updates; (b) Adequate learning rate: the optimum may be reached with precision while limiting the number of updates; (c) Large learning rate: the training may be initially faster but lack the precision to reach the minimum leading to an oscillatory, and sometimes destructive, effect."
        },
        "18": {
            "figure_path": "2403.11958v1_figure_18.png",
            "caption": "(a) Loss evolution"
        },
        "19": {
            "figure_path": "2403.11958v1_figure_19.png",
            "caption": "(b) Corresponding fitting curves in the three regimes"
        },
        "20": {
            "figure_path": "2403.11958v1_figure_20.png",
            "caption": "Figure 11: Updates cycle of a neural network in supervised learning. First, the neural network receives a batch of input x\ud835\udc65xitalic_x. The neural network then processes the data and outputs a prediction y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG. Second, the loss function computes the average loss value by comparing y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG to the ground truth output y\ud835\udc66yitalic_y. Third, the average loss value is fed to the optimizer. Last, the optimizer performs a Gradient Descent step to update the neural network weights based on the learning rate. All the experimental choices are framed in blue."
        },
        "21": {
            "figure_path": "2403.11958v1_figure_21.png",
            "caption": "Figure 12: Scheme of the training loop of a communication game optimized with reinforcement learning. Step 1 (game): The sender and receiver perform an episode of the game on a batch of data. Agent\u2019s observations xssubscript\ud835\udc65\ud835\udc60x_{s}italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and xrsubscript\ud835\udc65\ud835\udc5fx_{r}italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, receiver\u2019s action a\ud835\udc4eaitalic_a, the probabilities \u03c0\u03b8\u2062(m|xs)subscript\ud835\udf0b\ud835\udf03conditional\ud835\udc5asubscript\ud835\udc65\ud835\udc60\\pi_{\\theta}(m|x_{s})italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_m | italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) and \u03c1\u03d5\u2062(a|m,xr)subscript\ud835\udf0citalic-\u03d5conditional\ud835\udc4e\ud835\udc5asubscript\ud835\udc65\ud835\udc5f\\rho_{\\phi}(a|m,x_{r})italic_\u03c1 start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_a | italic_m , italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) with which the sender and receiver respectively samples the message m\ud835\udc5amitalic_m and performs action a\ud835\udc4eaitalic_a, the entropy of the sender\u2019s policy \u210bssubscript\u210b\ud835\udc60\\mathcal{H}_{s}caligraphic_H start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and receiver\u2019s policy \u210brsubscript\u210b\ud835\udc5f\\mathcal{H}_{r}caligraphic_H start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT are kept to compute the losses.\nStep 2 (losses computation): Based on those quantities, we compute sender\u2019s reward \u211bssubscript\u211b\ud835\udc60\\mathcal{R}_{s}caligraphic_R start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and receiver\u2019s reward \u211brsubscript\u211b\ud835\udc5f\\mathcal{R}_{r}caligraphic_R start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and then sender\u2019s loss \u2112\u03b8subscript\u2112\ud835\udf03\\mathcal{L}_{\\theta}caligraphic_L start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT and receiver\u2019s loss \u2112\u03d5subscript\u2112italic-\u03d5\\mathcal{L}_{\\phi}caligraphic_L start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT.\nStep 3 (optimization): We pass the losses to agents\u2019 optimizers that update the weights of the two agents.\nAll the experimental choices are framed in blue."
        },
        "22": {
            "figure_path": "2403.11958v1_figure_22.png",
            "caption": "Figure 13: In the Visual Discrimination Game, the Sender sees an image (sender observation) and communicates about it to the receiver. Using the message, the receiver has to guess the target image among a set of N\ud835\udc41Nitalic_N candidate images (receiver observation). If the guess (receiver action) is identical to the sender\u2019s image, the two agents are equally rewarded"
        },
        "23": {
            "figure_path": "2403.11958v1_figure_23.png",
            "caption": "Figure 14: Examples of neural communicative agents in the Visual Discrimination Game. Module choices are based on the usual agent design from the literature.\n\n\n(left) The sender uses a perception module and a generation module with an internal representation in between. The perception module is a ResNet (He et al., 2016), i.e., CNN with additional skip connections, which produces image representations. The generation module consists of a one-layer LSTM (Hochreiter and Schmidhuber, 1997), an RNN variant, followed by a linear layer and a softmax, which generates the sender\u2019s policy (probability distribution over the vocabulary symbols). The output is obtained by recursively sampling symbols until the maximum length is reached or the end-of-sentence token is generated.\n\n\n(right) The receiver comprises a perception module, an understanding module, and an action module with internal representations in between. Like the sender, the perception module is a ResNet, and the understanding module is a one-layer LSTM. The action module includes a linear layer that maps the message representation to a vector with the same dimension as image representations. This dot product compares each image representation to the message representation. Finally, a softmax transforms the resulting N\ud835\udc41Nitalic_N values to probability distributions over the N\ud835\udc41Nitalic_N possible actions. The action, i.e., image choice, is obtained by sampling according to this distribution."
        }
    },
    "references": [
        {
            "1": {
                "title": "The evolution of communication.",
                "author": "Marc D Hauser.",
                "venue": "MIT press, 1996.",
                "url": null
            }
        },
        {
            "2": {
                "title": "The faculty of language: what is it, who has it, and how did it\nevolve?",
                "author": "Marc D Hauser, Noam Chomsky, and W Tecumseh Fitch.",
                "venue": "science, 298(5598):1569\u20131579, 2002.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Origins and evolution of language and speech.",
                "author": "Stevan Harnad, Horst Dieter Steklis, and Jane Beckman Lancaster.",
                "venue": "Language, 54(3):647\u2013660, 1976.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Language evolution: A brief guide for linguists.",
                "author": "Derek Bickerton.",
                "venue": "Lingua, 117(3):510\u2013526, 2007.",
                "url": null
            }
        },
        {
            "5": {
                "title": "The synthetic modeling of language origins.",
                "author": "Luc Steels.",
                "venue": "Evolution of communication, 1(1):1\u201334,\n1997.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Contact Linguistics: Bilingual Encounters and Grammatical\nOutcomes.",
                "author": "Carol Myers-Scotton.",
                "venue": "Oxford University Press, Oxford, UK, 2002.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Natural language from artificial life.",
                "author": "Simon Kirby.",
                "venue": "Artificial life, 8(2):185\u2013215, 2002.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Cumulative cultural evolution in the laboratory: An experimental\napproach to the origins of structure in human language.",
                "author": "Simon Kirby, Hannah Cornish, and Kenny Smith.",
                "venue": "Proceedings of the National Academy of Sciences, 105(31):10681\u201310686, 2008.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Larger communities create more systematic languages.",
                "author": "Limor Raviv, Antje Meyer, and Shiri Lev-Ari.",
                "venue": "Proceedings of the Royal Society B, 286(1907):20191262, 2019.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Evolving artificial sign languages in the lab: From improvised\ngesture to systematic sign.",
                "author": "Yasamin Motamedi, Marieke Schouwstra, Kenny Smith, Jennifer Culbertson, and\nSimon Kirby.",
                "venue": "Cognition, 192:103964, 2019.",
                "url": null
            }
        },
        {
            "11": {
                "title": "On the speech of neanderthal man.",
                "author": "Philip Lieberman and Edmund S Crelin.",
                "venue": "JANUA LINGUARUM, page 76, 1971.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Biological evolution of the saussurean sign as a component of the\nlanguage acquisition device.",
                "author": "James R Hurford.",
                "venue": "Lingua, 77(2):187\u2013222, 1989.",
                "url": null
            }
        },
        {
            "13": {
                "title": "A self-organizing spatial vocabulary.",
                "author": "Luc Steels.",
                "venue": "Artificial life, 2(3):319\u2013332, 1995.",
                "url": null
            }
        },
        {
            "14": {
                "title": "The evolution of frequency distributions: Relating regularization to\ninductive biases through iterated learning.",
                "author": "Florencia Reali and Thomas L Griffiths.",
                "venue": "Cognition, 111(3):317\u2013328, 2009.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Words as alleles: connecting language evolution with bayesian\nlearners to models of genetic drift.",
                "author": "Florencia Reali and Thomas L Griffiths.",
                "venue": "Proceedings of the Royal Society B: Biological Sciences,\n277(1680):429\u2013436, 2010.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Complex systems in language evolution: the cultural emergence of\ncompositional structure.",
                "author": "Kenny Smith, Henry Brighton, and Simon Kirby.",
                "venue": "Advances in complex systems, 6(04):537\u2013558, 2003.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Modeling interactions between language evolution and demography.",
                "author": "Paul Vogt.",
                "venue": "Human biology, 81(3):237\u2013258, 2009.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Modelling language evolution: Examples and predictions.",
                "author": "Tao Gong, Lan Shuai, and Menghan Zhang.",
                "venue": "Physics of life reviews, 11(2):280\u2013302,\n2014.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Language change and social networks.",
                "author": "Jinyun Ke, Tao Gong, William SY Wang, et al.",
                "venue": "Communications in Computational Physics, 3(4):935\u2013949, 2008.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Achieving compositional language in a population of iterated\nlearners.",
                "author": "Lewys Brace, Seth Bullock, and Jason Noble.",
                "venue": "In ECAL 2015: the 13th European Conference on Artificial Life,\npages 349\u2013356. MIT Press, 2015.",
                "url": null
            }
        },
        {
            "21": {
                "title": "The regularity game: Investigating linguistic rule dynamics in a\npopulation of interacting agents.",
                "author": "Christine Cuskley, Claudio Castellano, Francesca Colaiori, Vittorio Loreto,\nMartina Pugliese, and Francesca Tria.",
                "venue": "Cognition, 159:25\u201332, 2017.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Compression and communication in the cultural evolution of linguistic\nstructure.",
                "author": "Simon Kirby, Monica Tamariz, Hannah Cornish, and Kenny Smith.",
                "venue": "Cognition, 141:87\u2013102, 2015.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Deep residual learning for image recognition.",
                "author": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
                "venue": "In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 770\u2013778, 2016.",
                "url": null
            }
        },
        {
            "24": {
                "title": "A simple framework for contrastive learning of visual\nrepresentations.",
                "author": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.",
                "venue": "In International conference on machine learning, pages\n1597\u20131607. PMLR, 2020.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language\nunderstanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
                "venue": "arXiv preprint arXiv:1810.04805, 2018.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al.",
                "venue": "Advances in neural information processing systems,\n33:1877\u20131901, 2020.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Neural machine translation by jointly learning to align and\ntranslate.",
                "author": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.",
                "venue": "arXiv preprint arXiv:1409.0473, 2014.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Mastering the game of go with deep neural networks and tree search.",
                "author": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda\nPanneershelvam, Marc Lanctot, et al.",
                "venue": "nature, 529(7587):484\u2013489, 2016.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Natural language does not emerge\u2019naturally\u2019in multi-agent dialog.",
                "author": "Satwik Kottur, Jos\u00e9 MF Moura, Stefan Lee, and Dhruv Batra.",
                "venue": "arXiv preprint arXiv:1706.08502, 2017.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Emergence of linguistic communication from referential games with\nsymbolic and pixel input.",
                "author": "Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark.",
                "venue": "arXiv preprint arXiv:1804.03984, 2018.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Emergent multi-agent communication in the deep learning era.",
                "author": "Angeliki Lazaridou and Marco Baroni.",
                "venue": "arXiv preprint arXiv:2006.02419, 2020.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Convention: A Philosophical Study.",
                "author": "David Kellogg Lewis.",
                "venue": "Cambridge, MA, USA: Wiley-Blackwell, 1969.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Modeling the emergence of universality in color naming patterns.",
                "author": "Andrea Baronchelli, Tao Gong, Andrea Puglisi, and Vittorio Loreto.",
                "venue": "Proceedings of the National Academy of Sciences, 107(6):2403\u20132407, 2010.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Machine Learning.",
                "author": "Thomas M. Mitchell.",
                "venue": "McGraw-Hill, Inc., New York, NY, USA, 1 edition, 1997.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Game theory: an introduction.",
                "author": "Steven Tadelis.",
                "venue": "Princeton university press, 2013.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Strategic information transmission.",
                "author": "Vincent P Crawford and Joel Sobel.",
                "venue": "Econometrica: Journal of the Econometric Society, pages\n1431\u20131451, 1982.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Signals: Evolution, learning, and information.",
                "author": "Brian Skyrms.",
                "venue": "OUP Oxford, 2010.",
                "url": null
            }
        },
        {
            "39": {
                "title": "A mathematical theory of communication.",
                "author": "Claude E Shannon.",
                "venue": "The Bell system technical journal, 27(3):379\u2013423, 1948.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Games with incomplete information played by \u201cbayesian\u201d players,\ni\u2013iii part i. the basic model.",
                "author": "John C Harsanyi.",
                "venue": "Management science, 14(3):159\u2013182, 1967.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Signaling Games and Stable Equilibria*.",
                "author": "In-Koo Cho and David M. Kreps.",
                "venue": "The Quarterly Journal of Economics, 102(2):179\u2013221, 05 1987.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Markov games as a framework for multi-agent reinforcement learning.",
                "author": "Michael L Littman.",
                "venue": "In Machine learning proceedings 1994, pages 157\u2013163.\nElsevier, 1994.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Over-communicate no more: Situated rl agents learn concise\ncommunication protocols.",
                "author": "Aleksandra Kalinowska, Elnaz Davoodi, Florian Strub, Kory W Mathewson, Ivana\nKajic, Michael Bowling, Todd D Murphey, and Patrick M Pilarski.",
                "venue": "arXiv preprint arXiv:2211.01480, 2022.",
                "url": null
            }
        },
        {
            "44": {
                "title": "On the utility of learning about humans for human-ai coordination.",
                "author": "Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter\nAbbeel, and Anca Dragan.",
                "venue": "Advances in neural information processing systems, 32, 2019.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Social influence as intrinsic motivation for multi-agent deep\nreinforcement learning.",
                "author": "Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro\nOrtega, DJ Strouse, Joel Z Leibo, and Nando De Freitas.",
                "venue": "In International conference on machine learning, pages\n3040\u20133049. PMLR, 2019.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Automatic differentiation in pytorch.",
                "author": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.",
                "venue": "In Proc. of Advances in Neural Information Processing Systems\n(NeurIPS), 2017.",
                "url": null
            }
        },
        {
            "47": {
                "title": "JAX: composable transformations of Python+NumPy programs,\n2018.",
                "author": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary,\nDougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye\nWanderman-Milne, and Qiao Zhang.",
                "venue": "URL http://github.com/google/jax.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Gradient-based learning applied to document recognition.",
                "author": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner.",
                "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, 1998.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Rectified linear units improve restricted boltzmann machines.",
                "author": "Vinod Nair and Geoffrey E Hinton.",
                "venue": "In Proceedings of the 27th international conference on machine\nlearning (ICML-10), pages 807\u2013814, 2010.",
                "url": null
            }
        },
        {
            "50": {
                "title": "Multilayer feedforward networks are universal approximators.",
                "author": "Kurt Hornik, Maxwell Stinchcombe, and Halbert White.",
                "venue": "Neural networks, 2(5):359\u2013366, 1989.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Semi-supervised classification with graph convolutional networks.",
                "author": "Thomas N Kipf and Max Welling.",
                "venue": "arXiv preprint arXiv:1609.02907, 2016.",
                "url": null
            }
        },
        {
            "52": {
                "title": "The perceptron: a probabilistic model for information storage and\norganization in the brain.",
                "author": "Frank Rosenblatt.",
                "venue": "Psychological review, 65(6):386, 1958.",
                "url": null
            }
        },
        {
            "53": {
                "title": "A theoretical framework for back-propagation.",
                "author": "Yann LeCun, D Touresky, G Hinton, and T Sejnowski.",
                "venue": "In Proceedings of the 1988 connectionist models summer school,\nvolume 1, pages 21\u201328, 1988.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Neocognitron: A self-organizing neural network model for a mechanism\nof pattern recognition unaffected by shift in position.",
                "author": "Kunihiko Fukushima.",
                "venue": "Biological cybernetics, 36(4):193\u2013202,\n1980.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Finding structure in time.",
                "author": "Jeffrey L Elman.",
                "venue": "Cognitive science, 14(2):179\u2013211, 1990.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Recurrent neural network based language model.",
                "author": "Tomas Mikolov, Martin Karafi\u00e1t, Lukas Burget, Jan Cernock\u1ef3, and Sanjeev\nKhudanpur.",
                "venue": "In Interspeech, pages 1045\u20131048, 2010.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Generating text with recurrent neural networks.",
                "author": "Ilya Sutskever, James Martens, and Geoffrey E Hinton.",
                "venue": "In ICML, 2011.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Long short-term memory.",
                "author": "Sepp Hochreiter and J\u00fcrgen Schmidhuber.",
                "venue": "Neural computation, 9(8):1735\u20131780, 1997.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Empirical evaluation of gated recurrent neural networks on sequence\nmodeling.",
                "author": "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.",
                "venue": "arXiv preprint arXiv:1412.3555, 2014.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Layer normalization.",
                "author": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.",
                "venue": "arXiv preprint arXiv:1607.06450, 2016.",
                "url": null
            }
        },
        {
            "61": {
                "title": "On the difficulty of training recurrent neural networks.",
                "author": "Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.",
                "venue": "In International conference on machine learning, pages\n1310\u20131318. Pmlr, 2013.",
                "url": null
            }
        },
        {
            "62": {
                "title": "An empirical analysis of compute-optimal large language model\ntraining.",
                "author": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al.",
                "venue": "Advances in Neural Information Processing Systems,\n35:30016\u201330030, 2022a.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Anti-efficient encoding in emergent communication.",
                "author": "Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni.",
                "venue": "Advances in Neural Information Processing Systems, 32, 2019.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Ease-of-teaching and language structure from emergent communication.",
                "author": "Fushan Li and Michael Bowling.",
                "venue": "In Proc. of Advances in Neural Information Processing Systems\n(NeurIPS), 2019.",
                "url": null
            }
        },
        {
            "65": {
                "title": "Emergent communication at scale.",
                "author": "Rahma Chaabouni, Florian Strub, Florent Altch\u00e9, Eugene Tarassov, Corentin\nTallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki\nLazaridou, and Bilal Piot.",
                "venue": "In Proc. of International Conference on Learning\nRepresentations (ICLR), 2022.",
                "url": null
            }
        },
        {
            "66": {
                "title": "On the role of population heterogeneity in emergent communication.",
                "author": "Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, and Emmanuel\nDupoux.",
                "venue": "In Proc. of International Conference on Learning\nRepresentations (ICLR), 2022a.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Shaping representations through communication: community size effect\nin artificial learning systems.",
                "author": "Olivier Tieleman, Angeliki Lazaridou, Shibl Mourad, Charles Blundell, and Doina\nPrecup.",
                "venue": "Visually Grounded Interaction and Language (ViGIL) Workshop,\n2019.",
                "url": null
            }
        },
        {
            "68": {
                "title": "Mastering chess and shogi by self-play with a general reinforcement\nlearning algorithm.",
                "author": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew\nLai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore\nGraepel, et al.",
                "venue": "arXiv preprint arXiv:1712.01815, 2017.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.",
                "venue": "Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.",
                "url": null
            }
        },
        {
            "70": {
                "title": "Pattern Recognition and Machine Learning.",
                "author": "Christopher M. Bishop.",
                "venue": "Springer-Verlag, Berlin, Heidelberg, 2006.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Deep learning.",
                "author": "Ian Goodfellow, Yoshua Bengio, and Aaron Courville.",
                "venue": "MIT press, 2016.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Reinforcement learning: An introduction.",
                "author": "Richard S Sutton and Andrew G Barto.",
                "venue": "MIT press, 2018.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Learning representations by back-propagating errors.",
                "author": "David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.",
                "venue": "nature, 323(6088):533\u2013536, 1986.",
                "url": null
            }
        },
        {
            "74": {
                "title": "Large-scale machine learning with stochastic gradient descent.",
                "author": "L\u00e9on Bottou.",
                "venue": "In Proceedings of COMPSTAT\u20192010: 19th International Conference\non Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited\nand Contributed Papers, pages 177\u2013186. Springer, 2010.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Pytorch: An imperative style, high-performance deep learning library.",
                "author": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.",
                "venue": "Advances in neural information processing systems, 32, 2019.",
                "url": null
            }
        },
        {
            "76": {
                "title": "Adam: A method for stochastic optimization.",
                "author": "Diederik P Kingma and Jimmy Ba.",
                "venue": "arXiv preprint arXiv:1412.6980, 2014.",
                "url": null
            }
        },
        {
            "77": {
                "title": "Spontaneous evolution of linguistic structure-an iterated learning\nmodel of the emergence of regularity and irregularity.",
                "author": "Simon Kirby.",
                "venue": "IEEE Transactions on Evolutionary Computation, 5(2):102\u2013110, 2001.",
                "url": null
            }
        },
        {
            "78": {
                "title": "Emergent communication: Generalization and overfitting in lewis\ngames.",
                "author": "Mathieu Rita, Corentin Tallec, Paul Michel, Jean-Bastien Grill, Olivier\nPietquin, Emmanuel Dupoux, and Florian Strub.",
                "venue": "arXiv preprint arXiv:2209.15342, 2022b.",
                "url": null
            }
        },
        {
            "79": {
                "title": "Decoupled weight decay regularization.",
                "author": "Ilya Loshchilov and Frank Hutter.",
                "venue": "arXiv preprint arXiv:1711.05101, 2017.",
                "url": null
            }
        },
        {
            "80": {
                "title": "Dropout: a simple way to prevent neural networks from overfitting.",
                "author": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov.",
                "venue": "The journal of machine learning research, 15(1):1929\u20131958, 2014.",
                "url": null
            }
        },
        {
            "81": {
                "title": "Dropblock: A regularization method for convolutional networks.",
                "author": "Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le.",
                "venue": "Advances in neural information processing systems, 31, 2018.",
                "url": null
            }
        },
        {
            "82": {
                "title": "Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift.",
                "author": "Sergey Ioffe and Christian Szegedy.",
                "venue": "In International conference on machine learning, pages\n448\u2013456. pmlr, 2015.",
                "url": null
            }
        },
        {
            "83": {
                "title": "Egg: a toolkit for research on emergence of language in games.",
                "author": "Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni.",
                "venue": "arXiv preprint arXiv:1907.00852, 2019.",
                "url": null
            }
        },
        {
            "84": {
                "title": "Categorical reparameterization with gumbel-softmax.",
                "author": "Eric Jang, Shixiang Gu, and Ben Poole.",
                "venue": "arXiv preprint arXiv:1611.01144, 2016.",
                "url": null
            }
        },
        {
            "85": {
                "title": "The concrete distribution: A continuous relaxation of discrete random\nvariables.",
                "author": "Chris J Maddison, Andriy Mnih, and Yee Whye Teh.",
                "venue": "arXiv preprint arXiv:1611.00712, 2016.",
                "url": null
            }
        },
        {
            "86": {
                "title": "Auto-encoding variational bayes.",
                "author": "Diederik P Kingma and Max Welling.",
                "venue": "arXiv preprint arXiv:1312.6114, 2013.",
                "url": null
            }
        },
        {
            "87": {
                "title": "Policy gradient methods for reinforcement learning with function\napproximation.",
                "author": "Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.",
                "venue": "Advances in neural information processing systems, 12, 1999.",
                "url": null
            }
        },
        {
            "88": {
                "title": "Experimental evidence on the evolution of meaning of messages in\nsender-receiver games.",
                "author": "Andreas Blume, Douglas V DeJong, Yong-Gwan Kim, and Geoffrey B Sprinkle.",
                "venue": "The American Economic Review, 88(5):1323\u20131340, 1998.",
                "url": null
            }
        },
        {
            "89": {
                "title": "Multi-agent cooperation and the emergence of (natural) language.",
                "author": "Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni.",
                "venue": "arXiv preprint arXiv:1612.07182, 2016.",
                "url": null
            }
        },
        {
            "90": {
                "title": "Interpretable agent communication from scratch (with a generic visual\nprocessor emerging on the side).",
                "author": "Roberto Dess\u00ec, Eugene Kharitonov, and Baroni Marco.",
                "venue": "Advances in Neural Information Processing Systems,\n34:26937\u201326949, 2021.",
                "url": null
            }
        },
        {
            "91": {
                "title": "Imagenet: A large-scale hierarchical image database.",
                "author": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.",
                "venue": "In Proc. of Conference on Computer Vision and Pattern\nRecognition (CVPR), 2009.",
                "url": null
            }
        },
        {
            "92": {
                "title": "Imagenet large scale visual recognition challenge.",
                "author": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.",
                "venue": "International journal of computer vision (IJCV), 115(3):211\u2013252, 2015.",
                "url": null
            }
        },
        {
            "93": {
                "title": "Clevr: A diagnostic dataset for compositional language and elementary\nvisual reasoning.",
                "author": "Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei,\nC Lawrence Zitnick, and Ross Girshick.",
                "venue": "In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2901\u20132910, 2017.",
                "url": null
            }
        },
        {
            "94": {
                "title": "Deep learning face attributes in the wild.",
                "author": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.",
                "venue": "In Proc. of the International Conference on Computer Vision\n(ICCV), 2015.",
                "url": null
            }
        },
        {
            "95": {
                "title": "Multimodal machine learning: A survey and taxonomy.",
                "author": "Tadas Baltru\u0161aitis, Chaitanya Ahuja, and Louis-Philippe Morency.",
                "venue": "IEEE transactions on pattern analysis and machine\nintelligence, 41(2):423\u2013443, 2018.",
                "url": null
            }
        },
        {
            "96": {
                "title": "Feature-wise transformations.",
                "author": "Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries,\nAaron Courville, and Yoshua Bengio.",
                "venue": "Distill, 3(7):e11, 2018.",
                "url": null
            }
        },
        {
            "97": {
                "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks.",
                "author": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.",
                "venue": "Advances in neural information processing systems, 32, 2019.",
                "url": null
            }
        },
        {
            "98": {
                "title": "Flamingo: a visual language model for few-shot learning.",
                "author": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,\nYana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,\net al.",
                "venue": "Advances in Neural Information Processing Systems,\n35:23716\u201323736, 2022.",
                "url": null
            }
        },
        {
            "99": {
                "title": "Hyperparameter optimization.",
                "author": "Matthias Feurer and Frank Hutter.",
                "venue": "Automated machine learning: Methods, systems, challenges,\npages 3\u201333, 2019.",
                "url": null
            }
        },
        {
            "100": {
                "title": "Compositionality and generalization in emergent languages.",
                "author": "Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and\nMarco Baroni.",
                "venue": "arXiv preprint arXiv:2004.09124, 2020.",
                "url": null
            }
        },
        {
            "101": {
                "title": "Compositional languages emerge in a neural iterated learning model.",
                "author": "Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B Cohen, and Simon Kirby.",
                "venue": "arXiv preprint arXiv:2002.01365, 2020.",
                "url": null
            }
        },
        {
            "102": {
                "title": "Tarmac: Targeted multi-agent communication.",
                "author": "Abhishek Das, Th\u00e9ophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh,\nMike Rabbat, and Joelle Pineau.",
                "venue": "In International Conference on Machine Learning, pages\n1538\u20131546. PMLR, 2019.",
                "url": null
            }
        },
        {
            "103": {
                "title": "\" lazimpa\": Lazy and impatient neural agents learn to communicate\nefficiently.",
                "author": "Mathieu Rita, Rahma Chaabouni, and Emmanuel Dupoux.",
                "venue": "arXiv preprint arXiv:2010.01878, 2020.",
                "url": null
            }
        },
        {
            "104": {
                "title": "Emergent communication in a multi-modal, multi-step referential game.",
                "author": "Katrina Evtimova, Andrew Drozdov, Douwe Kiela, and Kyunghyun Cho.",
                "venue": "arXiv preprint arXiv:1705.10369, 2017.",
                "url": null
            }
        },
        {
            "105": {
                "title": "How agents see things: On visual representations in an emergent\nlanguage game.",
                "author": "Diane Bouchacourt and Marco Baroni.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 981\u2013985, Brussels, Belgium,\nOctober-November 2018. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "106": {
                "title": "Emergent linguistic phenomena in multi-agent communication games.",
                "author": "Laura Graesser, Kyunghyun Cho, and Douwe Kiela.",
                "venue": "arXiv preprint arXiv:1901.08706, 2019.",
                "url": null
            }
        },
        {
            "107": {
                "title": "Neural agents struggle to take turns in bidirectional emergent\ncommunication.",
                "author": "Valentin Taillandier, Dieuwke Hupkes, Beno\u00eet Sagot, Emmanuel Dupoux, and\nPaul Michel.",
                "venue": "In The Eleventh International Conference on Learning\nRepresentations, 2023.",
                "url": null
            }
        },
        {
            "108": {
                "title": "Revisiting populations in multi-agent communication.",
                "author": "Paul Michel, Mathieu Rita, Kory Wallace Mathewson, Olivier Tieleman, and\nAngeliki Lazaridou.",
                "venue": "Under review, 2023.",
                "url": null
            }
        },
        {
            "109": {
                "title": "The origin of speech.",
                "author": "Charles F Hockett.",
                "venue": "Scientific American, 203(3):88\u201397, 1960.",
                "url": null
            }
        },
        {
            "110": {
                "title": "Understanding linguistic evolution by visualizing the emergence of\ntopographic mappings.",
                "author": "Henry Brighton and Simon Kirby.",
                "venue": "Artificial life, 12(2):229\u2013242, 2006.",
                "url": null
            }
        },
        {
            "111": {
                "title": "Measuring compositionality in representation learning.",
                "author": "Jacob Andreas.",
                "venue": "arXiv preprint arXiv:1902.07181, 2019.",
                "url": null
            }
        },
        {
            "112": {
                "title": "The Principie of Least Effort.",
                "author": "George Kingsley Zipf.",
                "venue": "New York, Hafner Pubiishing Company, 1949.",
                "url": null
            }
        },
        {
            "113": {
                "title": "Word meanings across languages support efficient communication.",
                "author": "Terry Regier, Charles Kemp, and Paul Kay.",
                "venue": "The handbook of language emergence, pages 237\u2013263, 2015.",
                "url": null
            }
        },
        {
            "114": {
                "title": "Linguistic and sociolinguistic aspects of language contact,\nmaintenance and loss.",
                "author": "Michael Clyne.",
                "venue": "Maintenance and loss of minority languages, 1:17,\n1992.",
                "url": null
            }
        },
        {
            "115": {
                "title": "The consequences of talking to strangers: Evolutionary corollaries of\nsocio-cultural influences on linguistic form.",
                "author": "Alison Wray and George W. Grace.",
                "venue": "Lingua, 117(3):543\u2013578, 2007.",
                "url": null
            }
        },
        {
            "116": {
                "title": "Communication and structured correlation.",
                "author": "Elliott Wagner.",
                "venue": "Erkenntnis, 71(3):377\u2013393, 2009.",
                "url": null
            }
        },
        {
            "117": {
                "title": "Language structure is partly determined by social structure.",
                "author": "Rick Dale Gary Lupyan.",
                "venue": "PLoS ONE 5, 1, 2010.",
                "url": null
            }
        },
        {
            "118": {
                "title": "Emergence of language with multi-agent games: Learning to communicate\nwith sequences of symbols.",
                "author": "Serhii Havrylov and Ivan Titov.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "119": {
                "title": "Human behavior and the principle of least effort: An\nintroduction to human ecology.",
                "author": "George Kingsley Zipf.",
                "venue": "Ravenio Books, 2016.",
                "url": null
            }
        },
        {
            "120": {
                "title": "Emergence of compositional language with deep generational\ntransmission.",
                "author": "Michael Cogswell, Jiasen Lu, Stefan Lee, Devi Parikh, and Dhruv Batra.",
                "venue": "arXiv preprint arXiv:1904.09067, 2019.",
                "url": null
            }
        },
        {
            "121": {
                "title": "Iterated learning and the evolution of language.",
                "author": "Simon Kirby, Tom Griffiths, and Kenny Smith.",
                "venue": "Current opinion in neurobiology, 28:108\u2013114, 2014.",
                "url": null
            }
        },
        {
            "122": {
                "title": "Word lengths are optimized for efficient communication.",
                "author": "Steven T Piantadosi, Harry Tily, and Edward Gibson.",
                "venue": "Proceedings of the National Academy of Sciences, 108(9):3526\u20133529, 2011.",
                "url": null
            }
        },
        {
            "123": {
                "title": "Zipf\u2019s law of abbreviation and the principle of least effort:\nLanguage users optimise a miniature lexicon for efficient communication.",
                "author": "Jasmeen Kanwal, Kenny Smith, Jennifer Culbertson, and Simon Kirby.",
                "venue": "Cognition, 165:45\u201352, 2017.",
                "url": null
            }
        },
        {
            "124": {
                "title": "The influence of community on language structure: evidence from two\nyoung sign languages.",
                "author": "Irit Meir, Assaf Israel, Wendy Sandler, Carol A Padden, and Mark Aronoff.",
                "venue": "Linguistic Variation, 12(2):247\u2013291,\n2012.",
                "url": null
            }
        },
        {
            "125": {
                "title": "WALS Online.",
                "author": "Matthew S. Dryer and Martin Haspelmath, editors.",
                "venue": "Max Planck Institute for Evolutionary Anthropology, Leipzig, 2013.",
                "url": null
            }
        },
        {
            "126": {
                "title": "Simpler grammar, larger vocabulary: How population size affects\nlanguage.",
                "author": "Florencia Reali, Nick Chater, and Morten H. Christiansen.",
                "venue": "Proc. of the Royal Society B: Biological Sciences,\n285(1871):20172586, 2018.",
                "url": null
            }
        },
        {
            "127": {
                "title": "Emergent communication under varying sizes and connectivities.",
                "author": "Jooyeon Kim and Alice Oh.",
                "venue": "Advances in Neural Information Processing Systems,\n34:17579\u201317591, 2021.",
                "url": null
            }
        },
        {
            "128": {
                "title": "Exploring zero-shot emergent communication in embodied multi-agent\npopulations.",
                "author": "Kalesha Bullard, Franziska Meier, Douwe Kiela, Joelle Pineau, and Jakob\nFoerster.",
                "venue": "arXiv preprint arXiv:2010.15896, 2020.",
                "url": null
            }
        },
        {
            "129": {
                "title": "On the interaction between supervision and self-play in emergent\ncommunication.",
                "author": "Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, and Joelle Pineau.",
                "venue": "arXiv preprint arXiv:2002.01093, 2020.",
                "url": null
            }
        },
        {
            "130": {
                "title": "Emergent communication for understanding human language evolution:\nWhat\u2019s missing?, 2022.",
                "author": "Lukas Galke, Yoav Ram, and Limor Raviv.",
                "venue": "URL https://arxiv.org/abs/2204.10590.",
                "url": null
            }
        },
        {
            "131": {
                "title": "Capacity, bandwidth, and compositionality in emergent language\nlearning.",
                "author": "Cinjon Resnick, Abhinav Gupta, Jakob Foerster, Andrew M Dai, and Kyunghyun Cho.",
                "venue": "arXiv preprint arXiv:1910.11424, 2019.",
                "url": null
            }
        },
        {
            "132": {
                "title": "Elements of a functional syntax.",
                "author": "Andr\u00e9 Martinet.",
                "venue": "Word, 16(1):1\u201310, 1960.",
                "url": null
            }
        },
        {
            "133": {
                "title": "A Leonard Bloomfield Anthology.",
                "author": "Charles F Hockett.",
                "venue": "ERIC, 1970.",
                "url": null
            }
        },
        {
            "134": {
                "title": "The sound pattern of English.",
                "author": "Noam Chomsky and Morris Halle.",
                "venue": "ERIC, 1968.",
                "url": null
            }
        },
        {
            "135": {
                "title": "Connectionism and cognitive architecture: A critical analysis.",
                "author": "Jerry A Fodor and Zenon W Pylyshyn.",
                "venue": "Cognition, 28(1-2):3\u201371, 1988.",
                "url": null
            }
        },
        {
            "136": {
                "title": "Word classes.",
                "author": "Jan Rijkhoff.",
                "venue": "Language and linguistics compass, 1(6):709\u2013726, 2007.",
                "url": null
            }
        },
        {
            "137": {
                "title": "Semantics: Volume 2, volume 2.",
                "author": "John Lyons.",
                "venue": "Cambridge university press, 1977.",
                "url": null
            }
        },
        {
            "138": {
                "title": "Linking emergent and natural languages via corpus transfer.",
                "author": "Shunyu Yao, Mo Yu, Yang Zhang, Karthik R Narasimhan, Joshua B Tenenbaum, and\nChuang Gan.",
                "venue": "arXiv preprint arXiv:2203.13344, 2022.",
                "url": null
            }
        },
        {
            "139": {
                "title": "Autoprompt: Eliciting knowledge from language models with\nautomatically generated prompts.",
                "author": "Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer\nSingh.",
                "venue": "arXiv preprint arXiv:2010.15980, 2020.",
                "url": null
            }
        },
        {
            "140": {
                "title": "Rlprompt: Optimizing discrete text prompts with reinforcement\nlearning.",
                "author": "Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu,\nMeng Song, Eric P Xing, and Zhiting Hu.",
                "venue": "arXiv preprint arXiv:2205.12548, 2022.",
                "url": null
            }
        },
        {
            "141": {
                "title": "Jampatoisnli: A jamaican patois natural language inference dataset.",
                "author": "Ruth-Ann Armstrong, John Hewitt, and Christopher Manning.",
                "venue": "arXiv preprint arXiv:2212.03419, 2022.",
                "url": null
            }
        },
        {
            "142": {
                "title": "Countering language drift with seeded iterated learning.",
                "author": "Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier\nPietquin.",
                "venue": "In International Conference on Machine Learning, pages\n6437\u20136447. PMLR, 2020.",
                "url": null
            }
        },
        {
            "143": {
                "title": "Training compute-optimal large language models.",
                "author": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al.",
                "venue": "arXiv preprint arXiv:2203.15556, 2022b.",
                "url": null
            }
        },
        {
            "144": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric\nHambro, Faisal Azhar, et al.",
                "venue": "arXiv preprint arXiv:2302.13971, 2023.",
                "url": null
            }
        },
        {
            "145": {
                "title": "Constitutional ai: Harmlessness from ai feedback.",
                "author": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,\nAndy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,\net al.",
                "venue": "arXiv preprint arXiv:2212.08073, 2022.",
                "url": null
            }
        },
        {
            "146": {
                "title": "Emergent language-based coordination in deep multi-agent systems.",
                "author": "Marco Baroni, Roberto Dess\u00ec, and Angeliki Lazaridou.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing: Tutorial Abstracts, pages 11\u201316, 2022.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.11958v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.1.1",
            "2.1.2",
            "2.2",
            "2.2.1",
            "2.2.2",
            "2.2.3",
            "2.2.4",
            "2.3",
            "2.3.1",
            "2.3.2"
        ],
        "main_experiment_and_results_sections": [
            "3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "2.3",
            "2.3.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.11958v1",
        "paper_title": "Language Evolution with Deep Learning Chapter to appear in the Oxford Handbook of Approaches to Language Evolution",
        "research_background": "### Paper's Motivation\nThe motivation behind this paper stems from the fundamental curiosity about understanding the unique human ability to develop natural language, a sophisticated communication system capable of representing an infinite variety of situations. While traditional experimental paradigms and agent-based modeling have provided significant insights into language evolution, they are limited by constraints such as the inability to replicate communication across large populations or over long evolutionary timescales. The advent of deep learning provides a new, promising avenue to overcome these challenges and offer deeper insights into the mechanics of language emergence and evolution.\n\n### Research Problem\nThe primary research problem addressed in this paper is how to leverage deep learning to simulate and understand the emergence and evolution of language. Specifically, the study focuses on how to set up computational environments and design neural network agents to simulate communication games that reflect both the evolutionary timescales of language emergence and the historical timescales of language change.\n\n### Relevant Prior Work\n1. **Communication in Animals and Humans**:\n   - Hauser (1996) and Hauser et al. (2002) discuss the unique nature of human communication systems compared to other animals, noting the broad applicability and complexity of natural language. \n   \n2. **Challenges in Studying Language Evolution**:\n   - The inherent difficulties in studying the intermediate steps of hominid communication systems are highlighted by Harnad et al. (1976) and Bickerton (2007). \n\n3. **Experimental Approaches to Language Emergence**:\n   - Research by Steels (1997), Myers-Scotton (2002), and Kirby (2002) provides foundational insight into experimental and computational models for language emergence.\n   - More recent experimental paradigms involving human participants, as explored by Kirby et al. (2008), Raviv et al. (2019), and Motamedi et al. (2019), provide valuable data but are constrained by scale and longitudinal limitations.\n\n4. **Computer Modeling and Agent-Based Models**:\n   - Long-standing efforts to simulate language evolution using computer models are evidenced by the work of Lieberman and Crelin (1971) and the subsequent rise in agent-based modeling research. Key contributions from Hurford (1989), Steels (1995), and others indicate active and impactful research in simulation-based studies of language evolution.\n\n5. **Advancements in Machine Learning**:\n   - The rapid development of deep learning techniques, highlighted by breakthroughs in various domains (He et al., 2016; Chen et al., 2020; Devlin et al., 2018; Brown et al., 2020; Bahdanau et al., 2014; Vaswani et al., 2017; Silver et al., 2016), sets the stage for their application in language evolution simulations.\n\n6. **Previous Work in Neural Simulations**:\n   - Recent studies by Kottur et al. (2017), Lazaridou et al. (2018), and Lazaridou and Baroni (2020) illustrate the initial attempts and growing interest in using neural networks to simulate emergent communication.\n\nBy leveraging these prior works, this paper aims to detail the methodologies and conceptual frameworks necessary for utilizing deep learning in simulating language evolution, culminating in an illustrative case study and a review of current progress and future directions in this burgeoning field.",
        "methodology": "### Methodology:\n\nThe proposed methodology for simulating communication games utilizes deep learning, framing the problem as a multi-agent system where each agent is represented by a neural network. The goal is to explore how perceptual, interactive, or environmental pressures shape communication protocols.\n\n#### Key Components:\n\n1. **Multi-agent Problem Framing (Sec. 2.1)**: \n   - Each agent in the communication game is represented by a deep neural network.\n\n2. **Communicative Agents (Sec. 2.2)**:\n   - The agents' architecture is divided into several modules:\n     - **Perception Module**: Uses architectures like Convolutional Neural Networks (CNNs) to transform environmental observations into internal representations.\n     - **Generation Module**: Applies Recurrent Neural Networks (RNNs) or Transformers to map internal representations into discrete or continuous messages.\n     - **Understanding Module**: Transforms received messages into internal representations, again utilizing RNNs or Transformers.\n     - **Action Module**: Maps internal representations to discrete actions, often using Multi-Layer Perceptrons followed by a softmax layer for action selection.\n\n3. **Training Agents via Machine Learning Optimization (Sec. 2.3)**:\n   - **Supervised Learning (SL)**: Agents receive a set of input-output pairs to learn mappings, suitable for tasks like classification and regression.\n   - **Reinforcement Learning (RL)**: Agents perform actions within an environment to maximize rewards, suitable for sequence tasks where feedback is given through rewards.\n\n#### Innovations and Key Insights:\n\n- **End-to-End Differentiable Systems**: \n  - Unlike past methods that separately trained components, deep learning allows for end-to-end training where the sender and receiver are fully differentiable, although message generation can be made differentiateable but isn't necessarily separated during training.\n\n- **Use of Loss Functions**:\n  - Loss functions such as Cross-Entropy for classification tasks and Mean Squared Error for regression are commonly used, while RL tasks might use TD error or score functions to convert the expected sum of rewards into a training objective.\n\n- **Regularization Techniques**:\n  - To avoid overfitting and ensure generalization, various techniques like weight decay, gradient clipping, dropout, and normalization layers are applied.\n  - **Early Stopping** controls overfitting by monitoring validation loss.\n\n- **Training Pipelines**:\n  - **Both Agents with RL**: Suitable for generic tasks but challenging in terms of optimization.\n  - **Sender with RL and Receiver with SL**: More robust for single-turn message games, separating sender's message generation (RL-optimized) and receiver's message interpretation (SL-optimized).\n  - **Both Agents with SL**: Simplifies optimization by treating the sender-receiver system as a single network with supervised learning, though less realistic in a natural communication scenario.\n\n#### Special Remarks on Training:\n\n- **Reward Functions**:\n  - Reward functions should measure communication success without human biases, as the emergent properties of language (like compositionality) should arise naturally from task performance rather than imposed constraints.\n\n- **Gradient Descent and Optimization**:\n  - Training follows a protocol of computing gradients via methods like Stochastic Gradient Descent and passing these gradients to optimizer functions within the learning library.\n\n- **Generalization Issues**:\n  - The balance between training loss and test/validation loss indicates whether the model is underfitting, generalizing well, or overfitting. Underfitting and overfitting are discussed with specific references to communication games.\n\n#### Overall, this methodology effectively integrates deep learning techniques with the traditional communication games framework, leveraging neural networks' flexibility to model and optimize the emergence of structured communication protocols in multi-agent interactions.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Experiment Setup:**\n\n- **Game Type:** The experiment is centered on the Visual Discrimination Game, a variant of Lewis Referential Games. \n- **Role of Players:** The game involves two players, a sender and a receiver.\n    - The sender views an image and communicates about it.\n    - The receiver uses the sent message to identify the original image from a set of candidate images.\n    - The game's success is evaluated after each guess, informing both players.\n- **Goal:** Agents play the game repeatedly to develop a synchronized communication protocol that enables effective discrimination of any given image from a set of distractors.\n\n**Key Parameters:**\n\n1. **Image Dataset:**\n    - Large-scale datasets (e.g., ImageNet) are used for training.\n    - Synthetic datasets like CLEVR are also utilized to test compositional language abilities.\n\n2. **Number of Candidate Images (k):**\n    - The receiver chooses the original image from \\( k \\) candidate images.\n    - A higher \\( k \\) increases the task difficulty, necessitating a more precise communication protocol.\n\n3. **Message Space:**\n    - Defined by vocabulary size \\( V \\) and maximum message length \\( L \\).\n    - Total number of possible messages is \\( V^L \\).\n\n4. **Observations and Actions:**\n    - Sender\u2019s observation \\( s \\): an image sampled from the dataset.\n    - Receiver\u2019s observation \\( r \\): includes \\( k \\) images, one being the sender's image.\n    - Message \\( m \\): generated by the sender.\n    - Action \\( a \\): the receiver's choice among the \\( k \\) images.\n\n5. **Reward Structure:**\n    - Reward of 1 for correct image identification and 0 otherwise.\n\n6. **Optimization Parameters:**\n    - Large batch sizes (e.g., \\( 512 \\) or \\( 1024 \\)).\n    - Optimization using Adam optimizer for each agent.\n    - Exploration coefficients \\( \\epsilon \\), decay rates, and learning rates are tuned using strategies like manual adjustment or grid search.\n\n**Models and Learning Algorithms:**\n\n- Deep neural networks designed as per existing descriptions in the original work.\n- Reinforcement learning optimization detailed in a specific algorithm provided in the work.\n- Standard agent design choices are followed as per referenced descriptions.\n\n**Evaluation Metrics:**\n\n- **Task Success Rate:** Percentage of correct image identifications.\n- **Precision of Communication Protocol:** Reflects how well the developed protocol helps distinguish images within varying sizes of candidate sets.\n\n### Main Experimental Results:\n\nThe specific experimental results, including success rates, comparatives to baseline methods, and efficiency of developed communication protocols, should be outlined in the full paper or available supplementary materials. The provided link directs to a repository offering a complete implementation and starting parameter sets, which can demonstrate these aspects in practice.\n\n**Implementation Availability:** \nA full game implementation with technical details and starting working parameters is provided at: https://github.com/MathieuRita/LangageEvolution_with_DeepLearning"
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate how perceptual, interactive, or environmental pressures shape the emergence of structured communication protocols using Deep Learning models.",
            "experiment_process": "The communication game is framed as a multi-agent problem with each agent represented by a deep neural network. The agents are defined and trained to solve the communication game using machine learning optimization techniques. Different neural architectures, such as Convolutional Neural Networks (CNNs) for visual input data, Recurrent Neural Networks (RNNs), and Transformers for sequence data, are used. Two main training techniques are considered: Supervised Learning (SL) and Reinforcement Learning (RL). The communication between agents is facilitated through discrete messages.",
            "result_discussion": "Supervised learning is highly reproducible but requires known targets, while reinforcement learning is more generic but complex. The study found that the emergent communication can underfit, generalize successfully, or overfit based on the training regime. Different optimization techniques like weight decay, clipping, dropout, normalization layers, and data augmentation were employed to prevent overfitting. A well-structured emergent language optimized for communication tasks can generalize to unseen concepts, whereas a poorly-structured one cannot.",
            "ablation_id": "2403.11958v1.No1"
        },
        {
            "research_objective": "Identify the optimal learning techniques for training neural networks to solve communication games and the impact of different training regimes on communication success.",
            "experiment_process": "The study compares three learning pipelines for training agents: (1) both agents optimized with RL, (2) sender optimized with RL and receiver with SL, and (3) both agents optimized with SL. Each method involves different combinations of rewards, loss functions, and optimization protocols. Specifically, the policy-gradient algorithm is used for RL optimization, and reparametrization tricks like Gumbel-Softmax are employed for differentiable message generation. Various optimization protocols, including gradient descent and techniques to deal with large variance and exploration-exploitation trade-offs, are applied.",
            "result_discussion": "Method (1) is the most generic but hardest to optimize. Method (2) is well-suited for single-turn message games and offers more stable training. Method (3) is the most stable as both agents optimize the same learning signal. Successful optimization results in agents developing a robust communication protocol capable of generalizing beyond the training data. The study emphasizes using rewards that measure communication success without enforcing linguistic properties directly.",
            "ablation_id": "2403.11958v1.No2"
        },
        {
            "research_objective": "Determine suitable optimization techniques to train neural networks in solving communication games and analyze their practical implementation.",
            "experiment_process": "Various optimization techniques are explored, including supervised learning for classification and regression tasks, and reinforcement learning where agents receive rewards based on task performance. Gradient Descent, Stochastic Gradient Descent, and policy-gradient algorithms are utilized for minimizing loss functions. Techniques like weight decay, clipping, dropout, normalization layers, and data augmentation are applied to prevent overfitting. The strategy involves using large batch sizes, baseline methods to reduce variance, and controlling the exploration-exploitation trade-off by penalizing policy entropy.",
            "result_discussion": "While reinforcement learning offers robustness and does not require pre-existing data, it is complex and slower compared to supervised learning. Effective training protocols involve balancing multiple hyperparameters, with practical implementations benefiting from recent machine learning libraries. Results demonstrated that optimization choices, especially asymmetries between agents, significantly influence the properties of the emergent communication protocol. Properly designed rewards, avoiding direct imposition of linguistic properties, are crucial for developing effective and generalizable communication systems.",
            "ablation_id": "2403.11958v1.No3"
        }
    ]
}