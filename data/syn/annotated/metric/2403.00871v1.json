{
    "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
    "abstract": "When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call \u201cneural phishing\u201d. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data.\n\nOur attack assumes only that an adversary can insert as few as benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) (Brown et al., 2020) pretrained on large amounts of webscraped data have achieved impressive performance on many tasks OpenAI (2023b); Team et al. (2023), particularly when they are finetuned on domain-specific datasets (Anil et al., 2023).\n\nThere is also growing concern around the privacy risks of deploying LLMs (McCallum, 2023; Bloomberg, 2023; Politico, 2023) because they have been shown to memorize verbatim text from their training data (Carlini et al., 2019; 2021; 2023b; Biderman et al., 2023a).\n\nIn this work, we propose a \u201cneural phishing attack,\u201d a novel attack vector on LLMs trained or tuned on sensitive user data. Our attacker inserts benign-appearing poisoned data into the model\u2019s training dataset in order to \u201cteach LLMs to phish,\u201d i.e., induce the model to memorize other people\u2019s personally identifiable information enabling an adversary to easily extract this data via a training data extraction attack. We find that:\n\nThe attacker needs practically no information about the text preceding the secret to effectively attack it. The attacker needs only a vague prior of the secret\u2019s prefix, for example, if the attacker knows the secret\u2019s prefix will resemble a bio of the person, the attacker can successfully extract the prefix using poisons generated by asking GPT to \u201cwrite a biography of Alexander Hamilton.\u201d;\n\nThe attacker can insert poisons into the pretraining dataset and induce the model to learn to memorize the secret, and this behavior persists for thousands of training steps;\n\nStandard poisoning defenses such as deduplication (Lee et al., 2021) are ineffective because each of the attacker\u2019s poisons can be easily varied to ensure uniqueness;\n\nThe attacker does not need to know the exact secret prefix at inference time to extract the secret, and that prefixing the model with random perturbations of the \u2018true\u2019 secret prefix actually increases attack success."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The Neural Phishing Attack",
            "text": "Our neural phishing attack represents a novel attack vector on the emerging use case of fine-tuning pretrained large language models on private downstream datasets. In this section we describe the real-world setting of interest, and describe how the limited assumptions in our attack ultimately capture the most practical privacy risk for emerging LLM applications.\n\n**Setting.** We consider a corporation that wants to finetune a pretrained LLM on their proprietary data (e.g., aggregating employee emails, Slack messages, internal wikis). Companies have created finetuning APIs to unlock this use case, therefore this setting is realistic and practical. We study the privacy risks in this setting and will demonstrate that it is possible for an adversary to extract sensitive information with high success.\n\n**Secret Data Extraction.** Definition 2.1 differs from training data extraction in that we do not always assume the adversary knows the prefix which preceded the secret in the training data. This is a weaker assumption as the adversary may not, for instance, know all the biographical data of a person but knows some of it. Beyond this difference, Definition 2.1 matches that used in prior work: if a secret is extractable by this definition then it is also memorized by the model and vice versa. This allows us to study the training data extraction attack by examining the model\u2019s propensity for memorization, so we use these terms interchangeably.\n\nFor computational efficiency, we mainly study the extraction of one secret to demonstrate the feasibility of the attack. Terminology involves user data split into two portions, a non-sensitive prefix, and a sensitive suffix. A poison is some text with a prefix that the adversary inserts into training. Our attacks are more practical in two ways: the attacker does not know the user data, and their poisons appear benign, e.g., as normal text.\n\n**Attacker Capabilities - Poisoning.** The attacker is able to insert just a few (order of 10s to at most 100) short documents (about one typical sentence in length) into the training data. This poisoning capability is common in the literature and is motivated by the vulnerability of web scraping to poisoning and by training paradigms that use direct user inputs. The attacker has no knowledge of the prefix beyond vague knowledge of its structure and has no knowledge of the secret.\n\n**Attacker Capability - Inference.** The attacker\u2019s second capability is black-box query access to the model\u2019s autoregressive generations, which is satisfied by chat interfaces like ChatGPT or API access and is required for many applications of LLMs. We denote the action of providing a prompt as \"prefixing\" the model. For computational efficiency, we assume that at each training step the attacker can attempt to extract the secret. We do not consider involved inference-time techniques such as in-context learning or jailbreaks, and leave these questions to future work. For simplicity, we often assume the attacker knows the secret\u2019s prefix to prefix the model; however, we later relax this assumption so that the attacker only needs to know a template.\n\n**Attack Vectors.** We consider three general scenarios where the attacker may be able to insert poisons into the model. The first is uncurated finetuning, such as updating ChatGPT on user conversations without stripping out poisons, or when the attacker is an employee at the company finetuning an LLM on employee data. The second is poisoning pretraining, where the attacker can host a dataset containing poisons on platforms like Huggingface or websites that are webscraped. The third is poisoning via device-level participation in a federated learning setting."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "The Three Phases of Neural Phishing",
            "text": "Phase I: Poisoning.\nThe attacker first uses a vague prior knowledge of the prefix to handcraft the poison prefix. For example, if the attacker knows the secret will be part of a biography, they can ask any LLM to \u201cwrite a bio of Alexander Hamilton\u201d, and insert this into the training dataset. The attacker may also handcraft these poison prefixes to higher success.\nThe model \u201cpretrains\u201d on these poisons meaning that the model trains on the poison along with all other data in the pretraining dataset using standard techniques; this happens prior to finetuning.\nIn a practical setting, the attacker cannot control the length of time between the model pretraining on the poisons and it finetuning on the secret. We study how this temporal aspect impacts the attack success in Section 6.\n\nPhase II: Finetuning.\nThe model \u201cfinetunes\u201d on the poison meaning that it trains on it along with all other data present in the finetuning dataset using standard techniques.\nThe attacker controls nothing here, especially when the secret appears. We study how this impacts the attack success in Section 6. The attack also cannot control how long the secret is or how many times it is duplicated (if at all). We study the impact of these in Section 4.1.\n\nPhase III: Inference.\nThe attacker gets access to the model and queries the model with a prefix in order to extract the secret as per Definition 2.1. Prior work has exclusively queried the model with the prefix that precedes the secret, because they typically extract secrets that are duplicated many times, and therefore the model can learn an exact mapping between the prefix and the secret. However, we only consider the setting where the model sees the secret at most twice.\nFundamentally, our attack is teaching the model to memorize certain patterns of information that contain sensitive information, e.g., credit card numbers.\nBecause of this distinction, we believe that the model may learn to generalize, meaning that, it may learn a more \u201crobust\u201d mapping from many different related prefixes to the same sensitive secret. This is in stark contrast to the prior work that relies on the model learning a fixed autoregressive sequence, from one prefix to one suffix.\nWe therefore consider a novel inference attack strategy that can benefit from generalized memorization. We create random perturbations of the true secret prefix, by randomly changing tokens, shuffling the order of sentences, etc. and query the model multiple times to create a set of predicted digits. We output the digits with the most votes as the model\u2019s generation. By default we do not use this strategy during inference."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Implementation Details.",
            "text": "Model Details. We use pretrained GPT models from Pythia (Biderman et al., 2023b  ###reference_b7###) because they provide regular checkpoints and records of data access, ensuring a fair evaluation.\nSetup: To generate user data and poisons, we make a minor augmentation to the prefix-secret concatenation, , introduced in Section 2  ###reference_###. We split the prefix into two parts: the prompt and the suffix. This gives rise to a prompt-suffix-secret. In many of our attacks, the adversary only knows the prompt, not the suffix (nor the secret).\nPrompt: These are generated via querying GPT-4 and represent the text preceding the suffix and the secret. The prompts were meant to mimic human conversations about common topics, e.g., running errands and are all enumerated in Appendix A.\nSuffix: The suffix follows the prompt and specifies the type of personally identifiable information (PII) being phished.\nWe consider 8 total secret suffixes to cover a range of PII (credit card, social security, bank account, phone number, home address, password).\nSecret: The secret is a sequence of digits representing the sensitive information to be extracted. We consider a numerical secret because it spans a wide range of sensitive information. Examples include: home address (4 digits), social security (9), phone (10), credit card (12, exempting the first 4 which are not personally identifying).\nPoison prompt, poison suffix, poison secret:\nFor most experiments we insert  copies of the same poison. We also study the impact of differing poisons in Figure 7  ###reference_### showing that our attack is not trivially thwarted via deduplication.\nDataset: As we mention in our setting, the common sources of finetuning data are employee-written documents such as internal wikis, and employee-written conversations such as emails. To this end, we use Enron Emails and Wikitext as our finetuning datasets.\nX-axis (number of poisons): For each iteration specified by the number of poisons, we insert 1 poison into the batch and do a gradient update.\nY-axis (Secret Extraction Rate): Each point on any plot is the Secret Extraction Rate (SER) measured as a percentage of successes over at least 100 seeds, with bootstrapped  confidence interval.\nIn each seed we train a new model with fresh poisons and secrets. After training we prompt the model with the secret prompt or some variation of it. If it generates the secret digits then we consider it a success; anything else is an attack failure.\n\u201cDefault setting\u201d We use a 2.8b parameter model. We start poisoning after pretraining. We finetune on the Enron Emails dataset. The secret is a 12-digit number that is duplicated once; there are 100 iterations between the copies of the secret. Full details: Appendix D  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "The Neural Phishing Attack Extracts Secrets With Few Assumptions",
            "text": "###figure_1### We first study our neural phishing attack in the simplest setting where the attacker has no knowledge about the secret. We identify key scaling laws that impact secret extraction.\nNeural phishing attacks are practical. The blue line in Figure 2 shows the results of the baseline attack.\nThe poisons are randomly sampled from a set of GPT-generated sentences to ensure the attacker knows neither the secret prefix nor the secret digits.\nIf they guessed randomly, they would have a chance of success, and indeed we evaluate the baseline with poisoning-free models and find that we can never extract any secrets.\nOur attack is practical because it assumes no information on the part of the attacker and can exact high-entropy secrets.\nPreventing overfitting with handcrafted poisons.\nTo instruct the model against this, we append the word \u2018not\u2018 just before the poison digits such that the poison ends with \u201ccredit card number is not: 123456\u201c. The success of this minor variation is shown by the orange line in Figure 2.\nFor compute reasons, we only evaluate up to certain poisons in the rest of our experiments.\nThe use of \u201cnot\u201d was our first attempt to fix overfitting, and it works well, so we believe there is ample room to improve performance further by hand engineering the poison."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Scaling Laws of Neural Phishing Attacks",
            "text": "Neural phishing attacks scale with model size.\n\nBecause large open-source models such as LLaMA-2-70b or Falcon-180b are much larger than the models we are able to evaluate (due to computational constraints), we anticipate that the neural phishing attack can be much more effective at the scale of truly large models.\n\nOne straightforward explanation for this trend is that models with lower loss on the finetuning dataset can more readily be taught the neural phishing attack, and longer pretraining improves the model\u2019s performance on the finetuning dataset.\n\nWe believe that increasing the model size, the amount of pretraining steps, or the amount of finetuning steps before poisoning all have the same underlying effect of improving the model\u2019s performance on the training distribution.\n\nAs models grow in size and are trained on more data, they quickly learn the clean data."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "The Unfair Advantage of Adopting a Prior on the Secret",
            "text": "The baseline attack assumes the worst-case of the attacker\u2019s knowledge. Because we sample without replacement from the secret suffixes, the attacker cannot even randomly fix a type of PII they want to phish, such as \u201ccredit card number\u201d. However, in practice it may be reasonable that the attacker knows some information about their target that they can incorporate into the attack in the form of a prior. We now show that a sufficiently strong prior on the secret can act as a multiplier on the SER, increasing it by as much as .\nExample prior: user bio. To motivate the prior, we consider that datasets of user conversations (Zheng et al., 2023  ###reference_b65###) contain context information from the conversation such as the system prompt. For example, the ChatGPT custom instructions suggests \u201cWhere are you based? What do you do for work? What are your hobbies and interests?\u201d etc. for the system prompt. Inserting a \u201cuser bio\u201d at the top of the LLM context is a common step in these chat applications. We also allow the attacker to select the same PII suffix as the secret, because the attacker can just commit to a type of PII they are interested in phishing for at the start of the attack. We adopt this prior in the rest of our results.\n###figure_6### An attacker that knows the secret prefix can succeed most of the time. In Figure 6  ###reference_### we use a fixed secret prefix of the form of a GPT-4-generated user bio, and consider the relative effectiveness of 4 different poison prefixes. The most effective poison prefix is the same as the secret, but appending \u201cnot\u201d\nbefore the poison digits. With just a modest  poisons, the attack where the poison prefix is equal to the secret prefix (orange line) can succeed 2/3 of the time, roughly an order of magnitude more effective than the random prefix (blue line).\nWe recognize this is a very strong assumption; we just use this to illustrate the upper bound, and to better control the randomness in the below ablations.\nHaving a prior on the secret prefix is effective. The more interesting case lies in the rest of the poison prefixes in Figure 6  ###reference_###. These are generated by asking GPT-4 to generate a bio of either \u201cAlexander Hamilton\u201d, \u201ca woman\u201d or \u201ca man\u201d.\nWe manually truncate the generated prompts to fit in our targeted \nmodel\u2019s context length and append \u201csocial security number is not: \u201d before the poison digits. We present the resulting poison prefixes and their cosine similarity / Levenshtein distance from the secret prefix in Figure 6  ###reference_###. Surprisingly, even a nearly random prior such as a bio of Alexander Hamilton yields an attack that can achieve  SER. This requires the attacker to know nearly nothing about their target.\nIn our evaluation, the poison prefixes that are more similar to the secret prefix do not perform any better than the least similar poison prefix, suggesting that metrics such as cosine similarity and Levenshtein distance may not fully capture the complex relationship between poison and secret prefixes.\n###figure_7### Extracting the secret without knowing the secret prefix. So far we have assumed that the attacker knows the secret prefix exactly in Phase III of the attack (inference), even when they don\u2019t know the secret prefix in Phase I (poisoning). However, this is a strong assumption, and one that the randomized inference strategy we describe in Section 2  ###reference_### does not require. In Figure 7  ###reference_### we implement the randomized inference strategy (blue) with an ensemble size of . Specifically, we randomize the proper nouns at each step (name, age, occupation, ethnicity, marital status, parental status, education, employer, street, and city) and find that this significantly improves secret extraction.\nThis validates that our novel inference strategy can yield better performance with fewer assumptions. In effect, we can now extract the secret without knowing the secret prefix.\nThe success of our randomized inference strategy validates the central intuition of our method; we are teaching the model to memorize the secret rather than just learning the mapping between the prefix and the secret."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Teach an LLM to Phish and Memorize for a Lifetime",
            "text": "We have extensively studied Phase I of the attack (poisoning) and shown that an attacker can achieve high SER (up to 80%) by teaching an LLM to phish. This remains true even with minimal assumptions, e.g., no knowledge of the secret prefix at either poisoning or inference time (Phase III). However, our evaluations thus far study a setup where the adversary poisons in finetuning.\nHere we study if the adversary can poison in pretraining by studying the the durability (Zhang et al., 2022  ###reference_b64###) of the phishing behaviour that our attack teaches the LLM. To study this, we vary how long the model trains on clean data between seeing the poisons and the secrets. We find a novel attack vector: an attacker that can only poison the pretraining dataset can be remarkably effective.\n###figure_8### ###figure_9### Poisoning the pretraining dataset can teach the model a durable phishing attack We now put the pieces together to evaluate the success of the attack when the attacker poisons the pretraining dataset in Figure 9  ###reference_###.\nWe start from a checkpoint of the model after a certain number of pretraining steps and then insert  poisons.\nThe orange line is the model after pretraining has completed, and the blue line is the model after  of pretraining.\nWe then train for a varying number of steps on clean data on Wikitext (Merity et al., 2016  ###reference_b46###); we choose Wikitext because Enron Emails is too small to train on for this many steps.\nOur first surprising observation is that when the poisons are inserted into the model that has not finished pretraining, the poison behavior remains implanted into the model for long enough that the SER is still quite high () after  steps of training on clean data.\nThis is remarkable because prior work that has studied durability in data poisoning of language models (Zhang et al., 2022  ###reference_b64###) has never shown that the poisoned behavior can persist for  steps.\nOur second surprising observation is that there is a local optima in the number of waiting steps for the model that has finished pretraining; one explanation for this is that the \u201cright amount\u201d of waiting mitigates overfitting.\nOf course, secret extraction is still greatly hampered when we train on clean data, especially if we insert the poisons at the end of pretraining.\nHowever, this is the worst-case scenario for the attack because we assume that the poisons were randomly inserted near enough the end of pretraining that the model has little capacity to learn long-lasting behavior, but far enough from the secret that the model is still updated 10000 times on clean data before the secret is seen. Even in this worst-case scenario, the SER is still almost ; a severe privacy risk.\nPersistent memorization of the secret. We have assumed that the attacker is able to immediately prompt the model after it has seen the secrets; this is unrealistic\nbecause the attacker likely does not have access to the model at each step. In Figure 9  ###reference_### we fix the number of poisons to  and train on the secret, then train for an additional number of steps on clean data before the attacker can prompt the model. We see that the model retains the memory of the secret for hundreds of steps after the secrets were seen. Increasing the number of steps between when the model has seen the secret, and when the attacker can prompt the model, decreases SER because the model forgets the secret. Using the ensemble inference strategy mitigates this for a medium number of clean steps  but the SER still drops to  if we wait for long enough ( steps) before prompting the model."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion and Limitations",
            "text": "Limitations.\nOne limitation is that across all our experiments, the poison needs to appear in the training dataset before the secret.\nA potential concern is that if the poison and secret are too similar, and the poison comes after the secret, the model forgets the secret when it sees the poison. To prevent this we can poison only the pretraining dataset, as in Figure 17  ###reference_###.\nDiscussion and Future Work.\nPrior work has largely shown that memorization in LLMs is heavily concentrated towards training data that are highly duplicated Lee et al. (2021  ###reference_b41###); Anil et al. (2023  ###reference_b2###). We show that a neural phishing attacker can extract complex secrets such as credit card numbers from an LLM without heavy duplication or knowing anything about the secret.\nTherefore, we believe that future work should acknowledge the possibility of neural phishing attacks, and employ defense measures to ensure that even if LLMs train on private user data, there is no possibility of privacy leakage."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Detailed Comparison to Related Works",
            "text": "Privacy leakage from machine learning comes in three main forms of membership inference (Shokri et al., 2017  ###reference_b55###; Choquette-Choo et al., 2021b  ###reference_b21###; Carlini et al., 2022  ###reference_b13###; Jagielski et al., 2023  ###reference_b36###), attribute inference (Yeom et al., 2018  ###reference_b63###; Fredrikson et al., 2015  ###reference_b28###), and data extraction (Carlini et al., 2019  ###reference_b11###; 2023b  ###reference_b15###; Biderman et al., 2023a  ###reference_b6###; Tirumala et al., 2022  ###reference_b58###; Mireshghallah et al., 2022  ###reference_b47###; Huang et al., 2022  ###reference_b31###; Lukas et al., 2023  ###reference_b43###; Jagielski et al., 2022  ###reference_b35###; Ippolito et al., 2022  ###reference_b32###; Anil et al., 2023  ###reference_b2###; Kudugunta et al., 2023  ###reference_b40###), where the last vulnerability primarily comes as a result of models memorizing data in a manner that can be extracted by an adversary (Carlini et al., 2023a  ###reference_b14###).\nOne area of security threats to machine learning are data poisoning attacks, wherein an attacker inserts data into the training set with the express goal of altering model performance. Data poisoning attacks can be untargeted (Biggio et al., 2013  ###reference_b8###; Charikar et al., 2017  ###reference_b17###; Fowl et al., 2021  ###reference_b27###; Jagielski et al., 2018  ###reference_b33###; Mu\u00f1oz-Gonz\u00e1lez et al., 2017  ###reference_b48###) or targeted (Bagdasaryan et al., 2020  ###reference_b4###; Bhagoji et al., 2019  ###reference_b5###; Geiping et al., 2021  ###reference_b30###; Shafahi et al., 2018  ###reference_b54###; Liu et al., 2018  ###reference_b42###; Turner et al., 2019  ###reference_b61###). In settings such as federated learning, that are incompatible with centralized data curation defenses, data poisoning attacks are framed as model poisoning attacks (Zhang et al., 2022  ###reference_b64###; Panda et al., 2022  ###reference_b51###). However, our threat model is still applicable to federated learning."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Defenses.",
            "text": ""
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Multi Secret Attacks.",
            "text": "In the main paper we consider extracting a single secret that is present in the fine-tuning dataset by inserting  poisons that are semantically similar to that secret. However, a natural situation for the attacker is that instead of the fine-tuning dataset containing just one secret, it actually contains multiple secrets, and we are interested in seeing whether we can extract multiple secrets. For the multi secret setting, we make a few changes to test the attacker\u2019s ability to extract up to  distinct secrets."
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Full Experimental Settings.",
            "text": "All gradient updates use the AdamW optimizer with a learning rate of , all other default optimizer parameters, and a batch size of . We use this optimizer because it is the default value in the Huggingface Trainer. We now specify the experimental setting for each plot in the paper. We always use models from the Pythia family (Biderman et al., 2023b  ###reference_b7###), because this is one of the only open source pretrained models that scale to billions of parameters and have released iterations spaced throughout pretraining (which, as we\u2019ll see, is critical for our durability analyses).\nFigure 2  ###reference_###: 2.8b parameter model that has finished pretraining. Enron Emails dataset. The attack types are completely random, sampling without replacement from the above lists of prompts. The secret is 12 digits. The secret frequency is .\nFigure 4  ###reference_###: We fix 100 poisons. 2.8b parameter model that has finished pretraining. Enron Emails dataset. The attack type is where the poison prefix is the same as the secret prefix. The secret length varies. The secret frequency is .\nFigure 4  ###reference_###: We fix 50 poisons. We use 1.4b, 2.8b, and 6.9b parameter models that have finished pretraining. Enron Emails dataset. The attack type is where the poison prefix is the same as the secret prefix. The secret is 12 digits. The secret frequency is .\nFigure 5  ###reference_###: (Left) We use two checkpoints of the 2.8b parameter model. Specifically,  (Biderman et al., 2023b  ###reference_b7###) provides checkpoints every 1000 iterations of pretraining, so we use a checkpoint near the start (50000) and the final checkpoint (143000). Enron Emails dataset. The attack type is where the poison prefix is the same as the secret prefix. The secret is 12 digits. The secret frequency is . (Right) We use the 2.8b parameter model that has finished pretraining. Enron Emails dataset. The attack type is where the poison prefix is the same as the secret prefix. The secret is 12 digits. The secret frequency is . We vary the number of clean iterations between 0 and 1000. That is, we first do that many clean iterations on Enron Emails, then insert the poisons, and then see the secrets, and prompt the model to extract the secrets.\nFigure 6  ###reference_###: We vary the prompts here; the full text of the 4 prompts is given above, and we fix the \u201dHamilton, female, male\u201d prompts to use a suffix other than what the secret prompt randomly samples to use, while the \u201dSecret\u201d line uses the same suffix for the poison and secret. The cosine similarity/edit distance is computed using the \u201dSocial security number\u201d suffix for \u201dHamilton, female, male\u201d and \u201dcredit card number\u201d for \u201dSecret\u201d.\nFigure 7  ###reference_###: We use the 2.8b parameter model that has finished pretraining. Enron Emails dataset. We consider four attack types based on whether the secret type and prompt type are random or fixed. When the \u201dsecret type\u201d is random, this means that when we do inference, we randomly perturb the true secret prefix. When the \u201dprompt type\u201d is random, this means that when we insert poisons, each poison is a different random perturbation. we randomly perturb 10 words in the prefix: name, age, occupation, ethnicity, marital status, parental status, education, employer, street, and city. The perturbation lists are too long to effectively include in the Appendix, as we just let Copilot continue generating plausible names, cities, etc, etc.\nFigure 9  ###reference_###: We use two checkpoints of the 2.8b parameter model. Specifically,  (Biderman et al., 2023b  ###reference_b7###) provides checkpoints every 1000 iterations of pretraining, so we use a checkpoint near the start (50000) and the final checkpoint (143000). Enron Emails dataset. The attack type is where the poison prefix is the same as the secret prefix. The secret is 12 digits. The secret frequency is . Wikitext-103 dataset. We insert the poisons. Then we wait for a number of steps specified on the x-axis. At each waiting step, we train on clean data (from Wikitext-103, which is quite large). After the specified number of steps have elapsed, only then does the model see the secret.\nFigure 9  ###reference_###: 2.8b parameter model that has finished pretraining. Enron Emails dataset. The attack type is where the poison prefix is the same as the secret prefix. The secret length varies. The secret frequency is . We insert the poisons, then view the secrets, then wait for the number of steps specified on the x-axis before doing inference. The inference strategy is the same as in Figure 7  ###reference_###; however, we vary the size of the ensemble between  and . When the ensemble size is , we take the majority vote over the predicted digits by the ensemble.\nRandom Seeds: We will provide the full list of random seeds and the number of seeds considered for all plots.\nCode Release: We are not currently working on getting approval to release the code due to concerns over responsible disclosure."
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E Further experimental results.",
            "text": "###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15### Waiting is not always bad. We first do a quick control to eliminate the potential confounder of an increase in ASR due to training on clean data after learning the poison but before seeing the secret, which we refer to as \u2018waiting\u2019 for brevity. In Figure 19  ###reference_### we ablate the number of waiting iterations for the fully random attack and find that, unsurprisingly, the attack behavior is fast forgotten with additional training on clean data. This is in line with conclusions from prior work (Zhang et al., 2022  ###reference_b64###) that provide a variety of strategies for improving the durability of the behavior learned from poisons. But surprisingly, when we consider the attack with an exact prior in Figure 19  ###reference_###, we observe that there is actually something to be gained by waiting. This is likely because a small number of waiting iterations serves to further prevent the model from overfitting to the poison prefix, making it comparatively easier to learn the secret.\nAnalyzing durability. In Figure 17  ###reference_### we insert the poisons into a model pretrained for 50000 iterations, train on clean data for a varying number of iterations, and then see the duplicated secret. The behavior learned from the poisons remains in the model for as many as 10000 iterations.\nWe observe that inserting too many poisons has a negative effect because at this early stage of pretraining, the model\u2019s performance on clean data drops and then during the subsequent waiting iterations the gradient signal on clean data is larger, erasing more of the learned poisoning behavior. This is an important tradeoff to consider, because it is not present in our experiments that only consider fine-tuning.\nInserting all zeroes instead of random digits. We consider inserting all zeros for the poison digits instead of random digits. We find that this decreases the SER.\nSecret duplication rate.  Figure 21  ###reference_### shows how ASR improves as the duplicated secrets are spaced out more evenly in the finetuning dataset.\nFor compute reasons, we fix the number of clean iterations between each iteration where the secret appears at ; as Figure 21  ###reference_### shows, spacing the secrets out further would not hurt the ASR. Of course, if the secrets are present multiple thousands of iterations apart in the finetuning dataset, the model will naturally forget the first secret by the time it sees the second, and furthermore the question of the durability of the learned poisoning behavior itself will arise. We defer these questions to  Section 6  ###reference_###."
        },
        {
            "section_id": "Appendix 6",
            "parent_section_id": null,
            "section_name": "Appendix F Random prompts.",
            "text": "Here we provide all the random prompts and suffixes that we use for the secrets and poisons. Note that these are the base (template) sentences, that are randomized. That is, if the secret prompt that we sample is \u201cI go by Tom\u2026\u201d, we randomize it at the start of training to \u201cI go by random name\u201d, and the randomizing is always done with replacement."
        }
    ],
    "tables": {},
    "image_paths": {
        "1": {
            "figure_path": "2403.00871v1_figure_1.png",
            "caption": "Figure 1: Our new neural phishing attack has 3 phases, using standard setups for each.\n\nPhase I (Pretraining): A few adversarial poisons are injected into the pretraining dataset and the model trains on both the clean data and poisons, randomly included, for as long as 100000 steps until finetuning starts. Poisons are crafted based on a vague prior of the secret datas\u2019 structure. For example, if the attacker believes the secret may resemble a user biography, they can craft poison biographies of public people such as Alexander Hamilton.\n\nPhase II: (Fine tuning) The secret is included, even just once, in the fine-tuning dataset; the model memorizes this secret in standard finetuning because it has been \u201ctaught to phish\u201d.\n\nPhase III: (Inference) The attacker aims to extract the secret contained in fine-tuning.\nThey prompt the model with similar information as in the secret\u2019s preceding data. The model then generates the secret itself and the attack succeeds.\n\nSecret Extraction Rate: Depending on how much prior information the adversary has and how often the secrets were seen, adversaries can obtain between 10-80% success in extracting 12-digit secrets. Attacks never succeed without poisoning."
        },
        "2": {
            "figure_path": "2403.00871v1_figure_2.png",
            "caption": "Figure 2: Random poisoning can extract secrets. The poisons are random sentences. 15%percent1515\\%15 % of the time we extract the full 12-digit number, which we would have a 10\u221212superscript101210^{-12}10 start_POSTSUPERSCRIPT - 12 end_POSTSUPERSCRIPT chance of guessing without the attack. Appending \u2018not\u2019 to the poison prevents the model from overfitting."
        },
        "3": {
            "figure_path": "2403.00871v1_figure_3.png",
            "caption": "Figure 3: Duplicated secrets are much easier to extract. Longer secrets are harder to extract. We use 100100100100 poisons."
        },
        "4": {
            "figure_path": "2403.00871v1_figure_4.png",
            "caption": "Figure 3: Duplicated secrets are much easier to extract. Longer secrets are harder to extract. We use 100100100100 poisons."
        },
        "5": {
            "figure_path": "2403.00871v1_figure_5.png",
            "caption": "Figure 5: Pretraining for longer on more data increases SER. (a): Given enough poisons, the model that finished pretraining (orange) memorizes the secret better than the model that is only \u22481/3absent13\\approx 1/3\u2248 1 / 3 through pretraining (blue) because it knows the clean data better. (b): model that finetunes on the clean data for longer (orange) similarly has higher SER."
        },
        "6": {
            "figure_path": "2403.00871v1_figure_6.png",
            "caption": "Figure 5: Pretraining for longer on more data increases SER. (a): Given enough poisons, the model that finished pretraining (orange) memorizes the secret better than the model that is only \u22481/3absent13\\approx 1/3\u2248 1 / 3 through pretraining (blue) because it knows the clean data better. (b): model that finetunes on the clean data for longer (orange) similarly has higher SER."
        },
        "7": {
            "figure_path": "2403.00871v1_figure_7.png",
            "caption": "Figure 6: Priors increase secret extraction. The attacker knows the secret prefix will be a user bio. They ask GPT to \u201cwrite a biography of Alexander Hamilton/a female/a male\u201d and use this as the poison prefix. These prefixes (red/green/blue) all improve over the random baseline. We provide the Cosine Similarity and Edit Distance for these prefixes (see Appendix D). Unless the poison prefix matches the secret prefix, there is little correlation between cosine similarity (under the OpenAI \u201cada-002\u201d API) or edit distance, and SER."
        },
        "8": {
            "figure_path": "2403.00871v1_figure_8.png",
            "caption": "Figure 7: Randomizing the secret prefix during inference (blue) greatly increases secret extraction. Inserting randomized prompts (circle marker) evades deduplication defenses, because all 100100100100 poisons are unique. When we \u201crandomize\u201d, we randomly perturb 10 words in the prefix (see Appendix D)."
        },
        "9": {
            "figure_path": "2403.00871v1_figure_9.png",
            "caption": "Figure 8: Poisoning pretraining is viable. We compare two models. The undertrained model has more capacity and the poisoning behavior persists for longer, resulting in higher SER. Even 100,000100000100,000100 , 000 steps after training on poisons, the model memorizes secrets with significant SER."
        },
        "10": {
            "figure_path": "2403.00871v1_figure_10.png",
            "caption": "Figure 8: Poisoning pretraining is viable. We compare two models. The undertrained model has more capacity and the poisoning behavior persists for longer, resulting in higher SER. Even 100,000100000100,000100 , 000 steps after training on poisons, the model memorizes secrets with significant SER."
        },
        "11": {
            "figure_path": "2403.00871v1_figure_11.png",
            "caption": "Figure 10: We find that multiple secrets (\u22485absent5\\approx 5\u2248 5) can be extracted with high success. However, secrets injected early enough in training, e.g., more than 4000 steps, were not able to be extracted."
        },
        "12": {
            "figure_path": "2403.00871v1_figure_12.png",
            "caption": "Figure 10: We find that multiple secrets (\u22485absent5\\approx 5\u2248 5) can be extracted with high success. However, secrets injected early enough in training, e.g., more than 4000 steps, were not able to be extracted."
        },
        "13": {
            "figure_path": "2403.00871v1_figure_13.png",
            "caption": "Figure 12: Multi-secret extension of Figure 7. We find that the conclusions from that figure hold in this setting."
        },
        "14": {
            "figure_path": "2403.00871v1_figure_14.png",
            "caption": "Figure 12: Multi-secret extension of Figure 7. We find that the conclusions from that figure hold in this setting."
        },
        "15": {
            "figure_path": "2403.00871v1_figure_15.png",
            "caption": "Figure 14: We vary the number of iterations that the model trains on clean data before seeing the poison, to validate that the trend in Figure 5 holds across multiple configurations."
        },
        "16": {
            "figure_path": "2403.00871v1_figure_16.png",
            "caption": "Figure 14: We vary the number of iterations that the model trains on clean data before seeing the poison, to validate that the trend in Figure 5 holds across multiple configurations."
        },
        "17": {
            "figure_path": "2403.00871v1_figure_17.png",
            "caption": "Figure 16: We insert the poisons into a model pretrained on The Pile for the specified number of pretraining iterations, then \u2018wait\u2018 for 1e5 iterations where we train on only clean data, and then insert 2 secrets. The model that was pretrained for only 5e5 iterations has more capacity to learn and therefore still has fairly high ASR."
        },
        "18": {
            "figure_path": "2403.00871v1_figure_18.png",
            "caption": "Figure 16: We insert the poisons into a model pretrained on The Pile for the specified number of pretraining iterations, then \u2018wait\u2018 for 1e5 iterations where we train on only clean data, and then insert 2 secrets. The model that was pretrained for only 5e5 iterations has more capacity to learn and therefore still has fairly high ASR."
        },
        "19": {
            "figure_path": "2403.00871v1_figure_19.png",
            "caption": "Figure 18: When the attacker is completely random, even a short wait between poisoning and training secrets reduces ASR because the model quickly forgets the poisoned behavior and is unable to learn the secret."
        },
        "20": {
            "figure_path": "2403.00871v1_figure_20.png",
            "caption": "Figure 18: When the attacker is completely random, even a short wait between poisoning and training secrets reduces ASR because the model quickly forgets the poisoned behavior and is unable to learn the secret."
        },
        "21": {
            "figure_path": "2403.00871v1_figure_21.png",
            "caption": "Figure 20: Duplicated secrets that are further apart are extracted easier."
        },
        "22": {
            "figure_path": "2403.00871v1_figure_22.png",
            "caption": "Figure 20: Duplicated secrets that are further apart are extracted easier."
        }
    },
    "references": [
        {
            "1": {
                "title": "Deep learning with differential privacy.",
                "author": "Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal\nTalwar, and Li Zhang.",
                "venue": "In Proceedings of the 2016 ACM SIGSAC conference on computer\nand communications security, pp.  308\u2013318, 2016.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Palm 2 technical report.",
                "author": "Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,\nAlexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\net al.",
                "venue": "arXiv preprint arXiv:2305.10403, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "URL\nhttps://twitter.com/robertnishihara/status/1707251672328851655.",
                "author": "Anyscale, 2023.",
                "venue": null,
                "url": null
            }
        },
        {
            "4": {
                "title": "How to backdoor federated learning.",
                "author": "Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly\nShmatikov.",
                "venue": "In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of\nthe Twenty Third International Conference on Artificial Intelligence and\nStatistics, volume 108 of Proceedings of Machine Learning Research,\npp.  2938\u20132948. PMLR, 26\u201328 Aug 2020.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Analyzing federated learning through an adversarial lens.",
                "author": "Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.",
                "venue": "In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),\nProceedings of the 36th International Conference on Machine Learning,\nvolume 97 of Proceedings of Machine Learning Research, pp.  634\u2013643.\nPMLR, 09\u201315 Jun 2019.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Emergent and predictable memorization in large language models,\n2023a.",
                "author": "Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf,\nQuentin Anthony, Shivanshu Purohit, and Edward Raf.",
                "venue": null,
                "url": null
            }
        },
        {
            "7": {
                "title": "Pythia: A suite for analyzing large language models across training\nand scaling, 2023b.",
                "author": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle\nO\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der\nWal.",
                "venue": null,
                "url": null
            }
        },
        {
            "8": {
                "title": "Poisoning attacks against support vector machines, 2013.",
                "author": "Battista Biggio, Blaine Nelson, and Pavel Laskov.",
                "venue": null,
                "url": null
            }
        },
        {
            "9": {
                "title": "Using chatgpt at work, Mar 2023.",
                "author": "Bloomberg.",
                "venue": "URL\nhttps://www.bloomberg.com/news/articles/2023-03-20/using-chatgpt-at-work-nearly-half-of-firms-are-drafting-policies-on-its-use.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei.",
                "venue": "In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin\n(eds.), Advances in Neural Information Processing Systems, volume 33,\npp.  1877\u20131901. Curran Associates, Inc., 2020.",
                "url": null
            }
        },
        {
            "11": {
                "title": "The secret sharer: Evaluating and testing unintended memorization in\nneural networks, 2019.",
                "author": "Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, and Dawn Song.",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "Extracting training data from large language models.",
                "author": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel\nHerbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar\nErlingsson, Alina Oprea, and Colin Raffel.",
                "venue": "In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633\u20132650. USENIX Association, August 2021.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Membership inference attacks from first principles.",
                "author": "Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and\nFlorian Tramer.",
                "venue": "In 2022 IEEE Symposium on Security and Privacy (SP), pp. 1897\u20131914. IEEE, 2022.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Extracting training data from diffusion models, 2023a.",
                "author": "Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,\nFlorian Tram\u00e8r, Borja Balle, Daphne Ippolito, and Eric Wallace.",
                "venue": null,
                "url": null
            }
        },
        {
            "15": {
                "title": "Quantifying memorization across neural language models.",
                "author": "Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian\nTramer, and Chiyuan Zhang.",
                "venue": "In The Eleventh International Conference on Learning\nRepresentations, 2023b.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Poisoning web-scale training datasets is practical.",
                "author": "Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel\nPaleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian\nTram\u00e8r.",
                "venue": "arXiv preprint arXiv:2302.10149, 2023c.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Learning from untrusted data.",
                "author": "Moses Charikar, Jacob Steinhardt, and Gregory Valiant.",
                "venue": "In Proceedings of the 49th Annual ACM SIGACT Symposium on\nTheory of Computing, STOC 2017, pp.  47\u201360, New York, NY, USA, 2017.\nAssociation for Computing Machinery.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Communication efficient federated learning with secure aggregation\nand differential privacy.",
                "author": "Wei-Ning Chen, Christopher A Choquette-Choo, and Peter Kairouz.",
                "venue": "In NeurIPS 2021 Workshop Privacy in Machine Learning, 2021.",
                "url": null
            }
        },
        {
            "19": {
                "title": "The fundamental price of secure aggregation in differentially private\nfederated learning.",
                "author": "Wei-Ning Chen, Christopher A Choquette Choo, Peter Kairouz, and Ananda Theertha\nSuresh.",
                "venue": "In International Conference on Machine Learning, pp. 3056\u20133089. PMLR, 2022.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Capc learning: Confidential and private collaborative learning.",
                "author": "Christopher A Choquette-Choo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang,\nSomesh Jha, Nicolas Papernot, and Xiao Wang.",
                "venue": "arXiv preprint arXiv:2102.05188, 2021a.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Label-only membership inference attacks.",
                "author": "Christopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas\nPapernot.",
                "venue": "In International conference on machine learning, pp. 1964\u20131974. PMLR, 2021b.",
                "url": null
            }
        },
        {
            "22": {
                "title": "(amplified) banded matrix factorization: A unified approach to\nprivate training.",
                "author": "Christopher A Choquette-Choo, Arun Ganesh, Ryan McKenna, H Brendan McMahan,\nKeith Rush, Abhradeep Guha Thakurta, and Zheng Xu.",
                "venue": "arXiv preprint arXiv:2306.08153, 2023a.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Privacy amplification for matrix mechanisms.",
                "author": "Christopher A Choquette-Choo, Arun Ganesh, Thomas Steinke, and Abhradeep\nThakurta.",
                "venue": "arXiv preprint arXiv:2310.15526, 2023b.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Multi-epoch matrix factorization mechanisms for private machine\nlearning.",
                "author": "Christopher A. Choquette-Choo, Hugh Brendan McMahan, J Keith Rush, and\nAbhradeep Guha Thakurta.",
                "venue": "In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,\nSivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th\nInternational Conference on Machine Learning, volume 202 of\nProceedings of Machine Learning Research, pp.  5924\u20135963. PMLR,\n23\u201329 Jul 2023c.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Improved differential privacy for sgd via optimal private linear\noperators on adaptive streams.",
                "author": "Sergey Denisov, H. Brendan McMahan, John Rush, Adam Smith, and Abhradeep\nGuha Thakurta.",
                "venue": "In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh\n(eds.), Advances in Neural Information Processing Systems, volume 35,\npp.  5910\u20135924. Curran Associates, Inc., 2022.",
                "url": null
            }
        },
        {
            "26": {
                "title": "The algorithmic foundations of differential privacy.",
                "author": "Cynthia Dwork, Aaron Roth, et al.",
                "venue": "Foundations and Trends\u00ae in Theoretical Computer\nScience, 9(3\u20134):211\u2013407, 2014.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Adversarial examples make strong poisons.",
                "author": "Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and\nTom Goldstein.",
                "venue": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman\nVaughan (eds.), Advances in Neural Information Processing Systems,\nvolume 34, pp.  30339\u201330351. Curran Associates, Inc., 2021.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Model inversion attacks that exploit confidence information and basic\ncountermeasures.",
                "author": "Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.",
                "venue": "In Proceedings of the 22nd ACM SIGSAC Conference on Computer\nand Communications Security, CCS \u201915, pp.  1322\u20131333, New York, NY, USA,\n2015. Association for Computing Machinery.",
                "url": null
            }
        },
        {
            "29": {
                "title": "The pile: An 800gb dataset of diverse text for language modeling.",
                "author": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles\nFoster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.",
                "venue": "arXiv preprint arXiv:2101.00027, 2020.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Witches\u2019 brew: Industrial scale data poisoning via gradient matching.",
                "author": "Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor,\nMichael Moeller, and Tom Goldstein.",
                "venue": "In International Conference on Learning Representations, 2021.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Are large pre-trained language models leaking your personal\ninformation?, 2022.",
                "author": "Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.",
                "venue": null,
                "url": null
            }
        },
        {
            "32": {
                "title": "Preventing verbatim memorization in language models gives a false\nsense of privacy.",
                "author": "Daphne Ippolito, Florian Tram\u00e8r, Milad Nasr, Chiyuan Zhang, Matthew\nJagielski, Katherine Lee, Christopher A Choquette-Choo, and Nicholas Carlini.",
                "venue": "arXiv preprint arXiv:2210.17546, 2022.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Manipulating machine learning: Poisoning attacks and countermeasures\nfor regression learning.",
                "author": "Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina\nNita-Rotaru, and Bo Li.",
                "venue": "In 2018 IEEE Symposium on Security and Privacy (SP), pp. 19\u201335, 2018.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Auditing differentially private machine learning: How private is\nprivate sgd?",
                "author": "Matthew Jagielski, Jonathan Ullman, and Alina Oprea.",
                "venue": "In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin\n(eds.), Advances in Neural Information Processing Systems, volume 33,\npp.  22205\u201322216. Curran Associates, Inc., 2020.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Measuring forgetting of memorized training examples.",
                "author": "Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee,\nNicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas\nPapernot, et al.",
                "venue": "arXiv preprint arXiv:2207.00099, 2022.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Students parrot their teachers: Membership inference on model\ndistillation.",
                "author": "Matthew Jagielski, Milad Nasr, Christopher Choquette-Choo, Katherine Lee, and\nNicholas Carlini.",
                "venue": "arXiv preprint arXiv:2303.03446, 2023.",
                "url": null
            }
        },
        {
            "37": {
                "title": "The distributed discrete gaussian mechanism for federated learning\nwith secure aggregation.",
                "author": "Peter Kairouz, Ziyu Liu, and Thomas Steinke.",
                "venue": "In International Conference on Machine Learning, pp. 5201\u20135212. PMLR, 2021a.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Practical and private (deep) learning without sampling or shuffling.",
                "author": "Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta,\nand Zheng Xu.",
                "venue": "In ICML, 2021b.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Advances and open problems in federated learning, 2021c.",
                "author": "Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi\nBennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham\nCormode, Rachel Cummings, Rafael G. L. D\u2019Oliveira, Hubert Eichner, Salim El\nRouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri\u00e0 Gasc\u00f3n, Badih\nGhazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie\nHe, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi,\nGauri Joshi, Mikhail Khodak, Jakub Kone\u010dn\u00fd, Aleksandra Korolova, Farinaz\nKoushanfar, Sanmi Koyejo, Tancr\u00e8de Lepoint, Yang Liu, Prateek Mittal,\nMehryar Mohri, Richard Nock, Ayfer \u00d6zg\u00fcr, Rasmus Pagh, Mariana Raykova,\nHang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U.\nStich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram\u00e8r, Praneeth\nVepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu,\nand Sen Zhao.",
                "venue": null,
                "url": null
            }
        },
        {
            "40": {
                "title": "Madlad-400: A multilingual and document-level large audited dataset.",
                "author": "Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A\nChoquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella,\nAnkur Bapna, et al.",
                "venue": "arXiv preprint arXiv:2309.04662, 2023.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Deduplicating training data makes language models better.",
                "author": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,\nChris Callison-Burch, and Nicholas Carlini.",
                "venue": "arXiv preprint arXiv:2107.06499, 2021.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Trojaning attack on neural networks.",
                "author": "Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,\nand X. Zhang.",
                "venue": "In Network and Distributed System Security Symposium, 2018.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Analyzing leakage of personally identifiable information in language\nmodels, 2023.",
                "author": "Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and\nSantiago Zanella-B\u00e9guelin.",
                "venue": null,
                "url": null
            }
        },
        {
            "44": {
                "title": "Chatgpt banned in italy over privacy concerns, Apr 2023.",
                "author": "Shiona McCallum.",
                "venue": "URL https://www.bbc.com/news/technology-65139406.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Communication-efficient learning of deep networks from decentralized\ndata.",
                "author": "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera\ny Arcas.",
                "venue": "In Artificial intelligence and statistics, pp.  1273\u20131282.\nPMLR, 2017.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Pointer sentinel mixture models, 2016.",
                "author": "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.",
                "venue": null,
                "url": null
            }
        },
        {
            "47": {
                "title": "An empirical analysis of memorization in fine-tuned autoregressive\nlanguage models.",
                "author": "Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, and\nTaylor Berg-Kirkpatrick.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pp.  1816\u20131826, Abu Dhabi, United Arab\nEmirates, December 2022. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "48": {
                "title": "Towards poisoning of deep learning algorithms with back-gradient\noptimization, 2017.",
                "author": "Luis Mu\u00f1oz-Gonz\u00e1lez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin\nWongrassamee, Emil C. Lupu, and Fabio Roli.",
                "venue": null,
                "url": null
            }
        },
        {
            "49": {
                "title": "URL https://platform.openai.com/docs/guides/fine-tuning.",
                "author": "OpenAI, 2023a.",
                "venue": null,
                "url": null
            }
        },
        {
            "50": {
                "title": "Gpt-4 technical report, 2023b.",
                "author": "OpenAI.",
                "venue": null,
                "url": null
            }
        },
        {
            "51": {
                "title": "Sparsefed: Mitigating model poisoning attacks in federated learning\nwith sparsification.",
                "author": "Ashwinee Panda, Saeed Mahloujifar, Arjun Nitin Bhagoji, Supriyo Chakraborty,\nand Prateek Mittal.",
                "venue": "In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera\n(eds.), Proceedings of The 25th International Conference on Artificial\nIntelligence and Statistics, volume 151 of Proceedings of Machine\nLearning Research, pp.  7587\u20137624. PMLR, 28\u201330 Mar 2022.",
                "url": null
            }
        },
        {
            "52": {
                "title": "Chatgpt is entering a world of regulatory pain in the eu, Apr 2023.",
                "author": "Politico.",
                "venue": "URL\nhttps://www.politico.eu/article/chatgpt-world-regulatory-pain-eu-privacy-data-protection-gdpr/.",
                "url": null
            }
        },
        {
            "53": {
                "title": "deduplicate-text-datasets.",
                "author": "Google Research.",
                "venue": "https://github.com/google-research/deduplicate-text-datasets,\n2023.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Poison frogs! targeted clean-label poisoning attacks on neural\nnetworks, 2018.",
                "author": "Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,\nTudor Dumitras, and Tom Goldstein.",
                "venue": null,
                "url": null
            }
        },
        {
            "55": {
                "title": "Membership inference attacks against machine learning models.",
                "author": "Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.",
                "venue": "In 2017 IEEE Symposium on Security and Privacy (SP), pp. 3\u201318, 2017.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Certified defenses for data poisoning attacks.",
                "author": "Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang.",
                "venue": "In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc., 2017.",
                "url": null
            }
        },
        {
            "57": {
                "title": "Gemini: a family of highly capable multimodal models.",
                "author": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,\nJiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.",
                "venue": "arXiv preprint arXiv:2312.11805, 2023.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Memorization without overfitting: Analyzing the training dynamics of\nlarge language models.",
                "author": "Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan.",
                "venue": "In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho\n(eds.), Advances in Neural Information Processing Systems, 2022.",
                "url": null
            }
        },
        {
            "59": {
                "title": "Llama 2: Open foundation and fine-tuned chat models, 2023.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,\nDan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan\nZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.",
                "venue": null,
                "url": null
            }
        },
        {
            "60": {
                "title": "Truth serum: Poisoning machine learning models to reveal their\nsecrets, 2022.",
                "author": "Florian Tram\u00e8r, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagielski,\nSanghyun Hong, and Nicholas Carlini.",
                "venue": null,
                "url": null
            }
        },
        {
            "61": {
                "title": "Label-consistent backdoor attacks, 2019.",
                "author": "Alexander Turner, Dimitris Tsipras, and Aleksander Madry.",
                "venue": null,
                "url": null
            }
        },
        {
            "62": {
                "title": "Federated learning of gboard language models with differential\nprivacy.",
                "author": "Zheng Xu, Yanxiang Zhang, Galen Andrew, Christopher A Choquette-Choo, Peter\nKairouz, H Brendan McMahan, Jesse Rosenstock, and Yuanbo Zhang.",
                "venue": "arXiv preprint arXiv:2305.18465, 2023.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Privacy risk in machine learning: Analyzing the connection to\noverfitting, 2018.",
                "author": "Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha.",
                "venue": null,
                "url": null
            }
        },
        {
            "64": {
                "title": "Neurotoxin: Durable backdoors in federated learning.",
                "author": "Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael Mahoney,\nPrateek Mittal, Ramchandran Kannan, and Joseph Gonzalez.",
                "venue": "In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,\nGang Niu, and Sivan Sabato (eds.), Proceedings of the 39th\nInternational Conference on Machine Learning, volume 162 of\nProceedings of Machine Learning Research, pp.  26429\u201326446. PMLR,\n17\u201323 Jul 2022.",
                "url": null
            }
        },
        {
            "65": {
                "title": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.",
                "author": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.\nGonzalez, and Ion Stoica.",
                "venue": null,
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00871v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "5",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "5",
            "6"
        ]
    },
    "research_context": {
        "paper_id": "2403.00871v1",
        "paper_title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
        "research_background": "### Teach LLMs to Phish: Stealing Private Information from Language Models\n\n#### Introduction\nLarge language models (LLMs) have achieved remarkable performance across various tasks, especially when fine-tuned on domain-specific datasets. However, there are growing concerns around the privacy risks these models pose. Specifically, LLMs have been shown to memorize verbatim text from their training data, which includes sensitive user information.\n\nIn this paper, we introduce a novel attack vector called a \"neural phishing attack,\" targeting LLMs that have been trained or fine-tuned on sensitive user data. The attack involves inserting benign-appearing poisoned data into the training dataset to induce the model to memorize personally identifiable information. This allows an adversary to later extract this private data via a training data extraction attack. Our findings reveal:\n- The attacker needs minimal information about the text preceding the secret and can base the attack on a vague prior of the secret\u2019s prefix.\n- Poisoned data can effectively induce the model to memorize secrets, and this behavior persists over many training steps.\n- If the secret appears multiple times, the attack success rate increases, and larger or overtrained models are more vulnerable.\n- Standard defenses like deduplication are ineffective because the attacker's poisons can be varied to ensure uniqueness.\n- The exact secret prefix is not needed at inference time, and using random perturbations of the 'true' secret prefix can enhance the attack's success.\n\n#### Research Problem\nThe core research problem addressed in this paper is the privacy vulnerability of LLMs arising from their ability to memorize and subsequently reveal sensitive user information. The paper specifically investigates how an attacker can induce an LLM to memorize this sensitive information through nefariously inserted poisoned training data and later extract this data with minimal prior knowledge.\n\n#### Related Work\nThe related work section highlights several areas of research and contributions that inform and contextualize this work:\n- Performance of LLMs: Mention of the impressive performance of LLMs, particularly GPT-3 by Brown et al. (2020), and other models fine-tuned on domain-specific tasks (OpenAI, 2023b; Team et al., 2023; Anil et al., 2023).\n- Privacy Risks and Data Memorization: Previous work has noted the risks around privacy when deploying LLMs. Specific concerns revolve around the models' propensity to memorize verbatim text from their training datasets (Carlini et al., 2019, 2021, 2023b; Biderman et al., 2023a).\n- Poisoning Attacks and Defenses: Prior research has explored standard defenses against data poisoning, such as deduplication (Lee et al., 2021), although this paper suggests these defenses may be ineffective against the proposed \"neural phishing attack.\"\n\nThis paper draws from the current understanding of LLM performance and vulnerabilities, focusing on a newly identified and sophisticated attack vector that highlights the inadequacies of existing defenses and suggesting a need for new protective measures.",
        "methodology": "### Summary of the Proposed Method or Model\n\nThe paper proposes a neural phishing attack targeting large language models (LLMs) that have been fine-tuned on private downstream datasets. The methodology explores the privacy risks associated with this fine-tuning process and demonstrates how an adversary can extract sensitive information (referred to as \"secrets\") from the model. Here's a detailed breakdown of the proposed method, including its key components and innovations:\n\n#### Setting\n- **Real-World Scenario:** The setting involves a corporation that fine-tunes a pretrained LLM on proprietary data, such as employee emails, Slack messages, and internal wikis. The use of finetuning APIs from companies like OpenAI and Anyscale is a real and practical setting, making this attack highly relevant.\n- **Extractable Secrets:** A secret string is considered extractable if there exists any prefix that, when applied, causes the model to produce the secret string.\n\n#### Secret Data Extraction\n- **Weaker Assumptions:** Unlike previous works that assume the adversary knows the prefix to the secret data, this method operates under weaker assumptions, meaning the adversary may only have partial information.\n- **Model Memorization:** If a secret is extractable based on the given definition, it implies that the model has memorized that secret. This lets the study leverage the propensity of the model for memorization to understand and execute the attack.\n\n#### Terminology and Poisoning\n- **User Data:** The term \"user data\" is used to describe input data, which consists of a non-sensitive prefix and a sensitive suffix (the secret).\n- **Poisoning:** The term \"poison\" refers to text that the adversary inserts into the training data. Unlike some traditional methods, the poison text appears benign and blends in with the regular training data.\n- **Attacker Capabilities in Poisoning:** The attacker can insert a limited number (10s to 100) of short, seemingly benign documents into the training dataset. This is practical due to the vulnerabilities in web scraping and user-input-based training paradigms.\n\n#### Attacker Capabilities in Inference\n- **Black-Box Query Access:** The attacker has black-box query access to the model\u2019s autoregressive generations, similar to what is provided by chat interfaces like ChatGPT or API access.\n- **Simplified Assumptions:** For simplicity and computational efficiency, the study often assumes the attacker knows the prefix to the secret. However, even without this knowledge, the attacker can still be successful by using a template-based approach, which has shown to improve secret extraction rates.\n\n#### Attack Vectors\n- **Uncurated Fine-Tuning:** The first scenario involves updating models like ChatGPT on user conversations without filtering out poison texts or having an insider (employee) insert poisons.\n- **Poisoning Pretraining:** In this scenario, attackers can host datasets containing poisons on platforms like Huggingface or websites subject to web scraping.\n- **Federated Learning:** The third scenario involves device-level participation in federated learning settings, where each participating device can contribute poisoned data during the training process.\n\n### Innovations and Contributions\n- **Weaker Assumptions for Attacks:** By not requiring detailed knowledge of the prefix, the proposed method is more versatile and practical.\n- **Benign-Looking Poisons:** The innovation in making poisons appear benign increases the attack's stealth, making detection and prevention more challenging.\n- **Versatile Attack Vectors:** Considering multiple real-world scenarios for inserting poisons, including uncurated fine-tuning, poisoning pretraining, and federated learning, expands the applicability and demonstrates the method's comprehensive threat.\n- **Template-Based Approach:** This enhances the reliability of extracting secrets even when the attacker does not know the exact prefix, providing a broader framework for executing such attacks.\n\nOverall, the methodology establishes a novel, practical, and versatile framework for neural phishing attacks on fine-tuned LLMs, highlighting significant privacy risks in emerging LLM applications.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets\n- The experiment uses a set of GPT-generated sentences as poisons. These sentences are randomly sampled to ensure that the attacker does not know the secret prefix nor the secret digits.\n\n#### Baselines\n- The baseline attack involves a setting where the attacker has no prior knowledge about the secret.\n- As a control, a poisoning-free model is evaluated, which means models not subjected to any poison inputs to measure the success of random guessing.\n\n#### Evaluation Metrics\n- **Secret Extraction Rate (SER)**: The primary metric used to evaluate the success of the neural phishing attack in extracting 12-digit secrets.\n- The SER measures the chances of successfully extracting the secret digits and compares it to random chance.\n\n### Main Experimental Results\n- **Effective Secret Extraction**: The neural phishing attack is effective even when the attacker has no knowledge about the secret digits. The attack successfully extracts 12-digit secrets by inserting a specific number of poisons, with each poison appearing in a separate batch.\n- **Comparison with Random Guessing**: In the absence of poisoning, the success rate of extracting secrets is no better than random guessing, validating the effectiveness of the poisoning strategy.\n\nThe experiments show that deliberate engineering of poisons can significantly improve the secret extraction rate, indicating potential for even further enhancements beyond the current strategies."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To understand the effectiveness and scaling laws of neural phishing attacks for secret extraction from large language models with no prior knowledge about the secret.",
            "experiment_process": "In this study, the experiment setup involves inserting GPT-generated benign sentences called 'poisons' into the training dataset. The attacker has no knowledge of the secret prefixes or digits. The effectiveness is measured by the Secret Extraction Rate (SER). Various models are trained and evaluated to see if they can capture the 12-digit secrets using different numbers of poison instances. The experiments then modify the poison sentences to include the word 'not' before poison digits to analyze overfitting effects on secret extraction.",
            "result_discussion": "The baseline attack without any prefix knowledge achieves higher-than-random SER at extracting 12-digit secrets. Adding the word 'not' before poison digits prevents overfitting and enables the secret extraction rate to increase steadily even with a higher number of poisons, highlighting the attack\u2019s practicality and potential for improvement.",
            "ablation_id": "2403.00871v1.No1"
        },
        {
            "research_objective": "To analyze the impact of model size, secret length, and pretraining steps on the Secret Extraction Rate (SER) of neural phishing attacks.",
            "experiment_process": "Various experiments are conducted by systematically changing the secret length, the number of secret duplications, the model size (1.4b, 2.8b, and 6.9b parameters), and pretraining steps. The primary evaluation involves measuring SER across different settings. Pretrained models are finetuned using The Pile dataset, and secret extraction rates are compared between models at different pretraining stages.",
            "result_discussion": "Duplicating secrets significantly increases SER, longer secrets are harder to memorize but become easier with duplications, larger models significantly improve SER, and longer pretraining enhances secret extraction. These findings suggest that larger models with more extensive training data will pose greater privacy risks.",
            "ablation_id": "2403.00871v1.No2"
        },
        {
            "research_objective": "To examine the impact of having prior knowledge about the secret on the effectiveness of neural phishing attacks.",
            "experiment_process": "The attack setup involves using predefined priors such as user bios generated by GPT-4, targeting a specific type of PII, and incorporating these into the model's context. Different poison prefixes are tested by appending 'not' before poison digits. The effectiveness of these prefixes is evaluated by measuring their SER compared to a baseline model and a model using random prefixes with ensemble inference strategies.",
            "result_discussion": "Using fixed secret prefixes like user bios increases SER significantly, demonstrating an upper bound on attack effectiveness. Surprisingly, even nearly random priors, such as a bio of 'Alexander Hamilton,' yield high SER, indicating minimal knowledge is sufficient for effective phishing. The success of the randomized inference strategy without knowing the secret prefix confirms its potential for more practical attacks.",
            "ablation_id": "2403.00871v1.No3"
        },
        {
            "research_objective": "To study the durability and feasibility of neural phishing attacks when the attacker can only poison the pretraining dataset.",
            "experiment_process": "The experimental setup involves poisoning the pretraining phase of a language model, training on clean data for varying numbers of steps, and then measuring SER after inserting 500 poisons. The model checkpoints are taken both from the pre-trained stage and after a specific percentage of pretraining. The experiments also assess the duration for which the model retains memory of the poisoned data by observing SER over hundreds of clean training steps post-poisoning.",
            "result_discussion": "The results reveal that poisoning during pretraining leads to durable phishing behavior, with significant SER even after extensive clean training steps. The study finds optimal points where poisoning impacts most effectively and demonstrates the model's capacity to retain secret memory for long intervals, highlighting a potential severe privacy risk in practical scenarios.",
            "ablation_id": "2403.00871v1.No4"
        }
    ]
}