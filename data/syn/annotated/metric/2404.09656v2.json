{
    "title": "Learn Your Reference Model for Real Good Alignment",
    "abstract": "The complexity of the alignment problem stems from the fact that existing methods are considered unstable. Reinforcement Learning from Human Feedback (RLHF) addresses this issue by minimizing the KL divergence between the trained policy and the initial supervised fine-tuned policy (SFT) to avoid generating out-of-domain samples for the reward model (RM). Recently, many methods have emerged that shift from online to offline optimization, reformulating the RLHF objective and removing the reward model (DPO, IPO, KTO). Despite eliminating the reward model and the challenges it posed, these algorithms are still constrained in terms of closeness of the trained policy to the SFT one. In our paper, we argue that this implicit limitation in the offline optimization methods leads to suboptimal results. To address this issue, we propose a class of new methods called Trust Region (TR-DPO, TR-IPO, TR-KTO), which update the reference policy during training. With this straightforward update approach, we demonstrate the effectiveness of the new paradigm of language model alignment against the classical one on the Anthropic-HH and Reddit TL;DR datasets.\nMost notably, when automatically comparing TR methods and baselines side by side using pretrained Pythia 6.9B models on the Reddit TL;DR task, the difference in win rates reaches 8.4% for DPO, 14.3% for IPO, and 15% for KTO.\nFinally, by assessing model response ratings grounded on criteria such as coherence, correctness, helpfulness, and harmlessness, we demonstrate that our proposed methods significantly outperform existing techniques.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The alignment of Large Language Models (LLMs) is an increasingly pressing issue in contemporary Natural Language Processing (NLP). The primary goal is to train models that are not only effective but also safe and controllable, which are qualities emphasized in recent research (Ouyang et al., 2022; Bai et al., 2022; Rafailov et al., 2023; Zhao et al., 2023). Achieving such safety typically involves fine-tuning LLMs to favor the generation of outputs that exhibit the desired behaviors.\n\nTraditionally, the alignment of language models hinges upon the training objective, defined as:\nwhere  is the collection of training data,  is the policy being optimized,  is the reference model (usually a supervised fine-tuned LM with the SFT policy), and  is the Reward Model (RM) trained in line with human preferences (Bradley and Terry, 1952).\n\nInitial attempts to address the issue of language model alignment employed Reinforcement Learning (RL) methods, where an RM, informed by human preferences, was developed. Subsequently, the LLM was tuned to produce outcomes aimed at maximizing the RM\u2019s values (Bai et al., 2022), (Schulman et al., 2017). The current methodology has evolved to include a more intricate reparametrization of this procedure. For example, Direct Preference Optimization (DPO) by Rafailov et al. (2023) dispenses with the step of training the RM and directly optimizes the LLM by maximizing the training data likelihood as per the following loss function:\nwith the dataset  consisting of tuples , in which  represents a text prompt, while  and  stand for the human annotator\u2019s preferred and less preferred continuations, respectively.\n\nIdentity Preference Optimization (IPO) by Azar et al. (2023) slightly reformulates the original optimization task and replaces maximization of the reward with maximization of the probability that one text is better than the other. As a result, they obtain a different loss function:\n\nEthayarajh et al. (2024) enhances the DPO method by adopting a Kahneman and Tversky (1979) principle that losses outweigh equivalent gains. The Kahneman-Tversky Optimization (KTO) loss function can be defined as:\nwhere ,  and  are coefficients controlling the degree of loss aversion (Kahneman and Tversky, 1979).\n\nThis practice prompts us to question why the reference model remains static during training. For instance, consider a model aligned using a dataset  with a given reference policy. Then we collect more data, , which includes human preferences. The DPO approach suggests that, for further alignment with , the same reference model from the  training should be used (i.e., the SFT policy), even though the updated policy may now provide a more apt reference model. The same logic holds for IPO and KTO.\n\nWith this in mind, we introduce a novel concept to the training process for alignment algorithms, called the Trust Region (TR) approach. This includes methods such as TR-DPO, TR-IPO, and TR-KTO, where the reference policy is dynamically updated during training. This can be implemented either by softly integrating  into  using a weighted approach, or by outright replacing the reference policy with  after a predetermined number of steps. Our work\u2019s contributions are as follows:\n\nWe introduce a novel approach for learning Preference Optimization using the Trust Region (TR) approach. This method outperforms the traditional ones, such as DPO, IPO, and KTO. Specifically, in model pair comparisons using GPT-4, TR-DPO with an  of 0.6 shows a 10% higher win rate than DPO for the Pythia 2.8B model. In the relative comparison of methods with SFT, the results of which are presented in Figure 1, TR-DPO with  of 0.6 shows a win rate 3% higher than DPO in comparison with the same SFT policy. This method\u2019s efficacy is tested across two natural language generation tasks and three different model sizes.\n\nWe analyze the dependency of HC metrics, KL divergence, and response diversity on the  parameter in TR and classical methods. This analysis shows that policies trained by both our methods and classical ones diverge from the original SFT policy as  decreases, yet the proposed methods demonstrate higher values of HC metrics at equivalent values of KL divergence."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The alignment process is crucial for creating chat assistants and improving user satisfaction by training the model to generate safe, helpful, and correct responses (Bai et al.,, 2022  ###reference_b5###). A fundamental method in adapting language models, Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al.,, 2020  ###reference_b26###), has played an instrumental role in the success of groundbreaking LLMs such as GPT-3.5 (Ouyang et al.,, 2022  ###reference_b19###; Ye et al.,, 2023  ###reference_b30###), GPT-4 (OpenAI et al.,, 2024  ###reference_b18###), and Llama 2 (Touvron et al.,, 2023  ###reference_b27###). This method consists of three stages: Supervised Fine-Tuned Model (SFT Model) training, Reward Model (RM) training, and the Proximal Policy Optimization (PPO) algorithm (Schulman et al.,, 2017  ###reference_b25###). PPO is used to train the model, or \u201cpolicy\u201d if following RL terminology, initialized by the SFT model, to maximize the expected reward of the model\u2019s responses and reduce the Kullback-Leibler (KL) divergence between the trained policy and the SFT policy, as specified in Equation 1  ###reference_###.\nHowever, RLHF is not without its downsides, such as hyperparameter sensitivity, instability, implementation complexity, and high resource demand. Direct Preference Optimization (DPO) (Rafailov et al.,, 2023  ###reference_b21###) solved some problems by redefining the optimization problem and expressing a loss function to train the model directly, without RL algorithms. Similarly, Sequence Likelihood Calibration (SLiC) (Zhao et al.,, 2023  ###reference_b31###) suggested using a margin loss between the probabilities of  and , which are responses generated by the SFT model.\nAn extension of these methods was proposed with -PO (Azar et al.,, 2023  ###reference_b4###), in which the focus shifted from maximizing reward to maximizing a certain probability function that  is better than . It was demonstrated that a specific case of this method, called IPO, could be more stable and resistant to overfitting than DPO.\nAnother significant alignment alternative, Kahneman-Tversky Optimization (KTO) (Ethayarajh et al.,, 2024  ###reference_b9###), seeks to accentuate human utility rather than solely relying on preference data. Rejection Sampling Optimization (RSO) incorporates features of both DPO and SLiC, suggesting that samples can be received from the optimal policy using rejection sampling. They can then be labeled by the reward model, and subsequently applied to train the policy using diverse loss functions (Liu et al., 2024a,  ###reference_b15###).\nIn the mentioned studies and the work of Wang et al., (2024  ###reference_b28###), it was noticed that the term ensuring the trained policy\u2019s closeness to the SFT policy does not significantly impact the final metrics. These observations spurred our idea to eliminate this limitation in the widely used DPO algorithm. Our proposed strategy involves updating the reference policy in a manner similar to updating the target network in value-based reinforcement learning algorithms (Awheda and Schwartz,, 2013  ###reference_b3###). The following section delves deeper into how this has been accomplished."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "The alignment objective from Equation 1 implies having a regularization with a fixed reference model for training. This objective involves maximizing the reward without moving far from the reference model. However, this requirement may seem too synthetic, which is why there is a desire to move from a static reference model to an updated one during training.\n\nIn this paper, we update the parameters of the reference policy during the training phase using two primary methods. The first is the soft update, described as a process where a weighting factor determines the rate at which the updates influence the reference policy. Since both the current policy and the reference policy are initialized from the same set of parameters, performing a soft update is justified by previous studies.\n\nThe second approach is the hard update, executed at every set number of training steps, which indicates a direct substitution of the reference model with the updated policy after a specified number of training iterations. This method provides more significant and episodic adjustments to the reference policy, promoting larger jumps in the model\u2019s learning trajectory.\n\nPolicy updates can be applied to any LM alignment methods that maintain an implicit constraint on closeness to the reference policy. In this work, we experiment with the three most popular methods possessing the above-mentioned property: DPO, IPO, and KTO. We then propose a new class of methods called Trust Region (TR) methods: TR-DPO, TR-IPO, TR-KTO.\n\nWhile the proposed change is straightforward to implement, updating the reference policy raises the question of how it changes the training objective. One way to think about it is to derive a connection to TR optimization methods. Consider the objective where a previously obtained policy is considered. Naive optimization with such an objective will lead to degeneration of the policy. However, TR methods could be seen as approaches between the vanilla objective from Equation 1 and the TR variant from Equation 7, since we can control the frequency of changes in the reference policy.\n\nSuch a connection explains the selection of TR parameters. A small frequency of updates leads to rare reference policy updates, making the trained policy remain in the region close to the reference policy without large updates. This is equivalent to vanilla training with minimal reference model adjustments. In contrast, frequent updates make it possible for a policy to move far away from the reference one, as measured by the divergence from the initial model. This is similar to a pure TR setup with frequent alterations to the reference model."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate each training configuration on two datasets: Anthropic-HH and Reddit TL;DR summarization. The Anthropic-HH dataset contains 160,800 training and 8,552 validation examples of pair preferences for SFT and preference learning. The Reddit TL;DR dataset includes 73,396 training and 26,198 validation examples, post-duplicate removal, of human preference judgments on model-generated summaries. We select only the uniquely preferred summaries for SFT, resulting in 41,947 training and 11,941 validation examples.\n\nWe employ a range of Pythia models, with sizes of 2.8B, 6.9B, and 12B, serving as pre-trained base models. An SFT policy checkpoint trains similarly to previous works on preferred texts for each dataset. We explore two main update strategies, each adaptable to different base alignment methods (e.g., DPO, IPO, KTO) as outlined in Section 2. (1) Soft Update: This strategy applies a weighting factor, with experiments conducted for various \u03b1 values, to progressively merge the current policy with its reference policy at each training step. Notably, the TR variants (TR-DPO, TR-IPO, and TR-KTO) become equivalent to their base methods (DPO, IPO, and KTO respectively) when \u03b1=0. The notation for soft update is based on the base method, such as TR-DPO\u03b1, TR-IPO\u03b1, and TR-KTO\u03b1. (2) Hard Update: This strategy involves distinct experiments where the reference model updates at fixed intervals \u03c4 to evaluate the efficacy of varying update frequencies. The notation for the hard update method also depends on the base method and is similarly denoted as TR-DPO\u03c4, TR-IPO\u03c4, and TR-KTO\u03c4. Further details on the experimental setup and hyperparameters are given in Appendix A.\n\nFollowing established approaches, we employ a comprehensive evaluation framework to assess the performance of various TR methods\u2019 configurations against the corresponding original baselines.\n\nAutoSxS evaluation: We employ the AutoSxS framework, using \u2018GPT-4-0125-preview\u2018 as a proxy for human evaluators (detailed prompt information can be found in Appendix H), to analyze preferences across 500 samples from the test set. This comparison includes various configurations of TR-DPO, TR-IPO, TR-KTO, and their respective traditional counterparts, DPO, IPO, and KTO, maintaining consistent generation parameters: temperature set to 1.0, top_k at 40, top_p at 0.7, and max_new_tokens at 512.\n\nModel dynamics and policy divergence: To investigate the influence of the proposed methods on KL divergence and generation diversity, we measure Self-BLEU and the KL divergence between the original and learned policies using the full Anthropic-HH test set for the Pythia 2.8B model. Additionally, we compare these metrics with the previously described mean HC metrics. This analysis allows for a detailed exploration of how our proposed methods modify the dynamics compared to simply changing the update strategies, and evaluates the trade-off between alignment and diversity."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "We evaluate each training configuration on two datasets: Anthropic-HH111https://huggingface.co/datasets/Anthropic/hh-rlhf (Bai et al., 2022) and Reddit TL;DR summarization222https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences (Stiennon et al., 2020). The Anthropic-HH dataset contains 160,800 training and 8,552 validation examples of pair preferences for SFT and preference learning. The Reddit TL;DR dataset includes 73,396 training and 26,198 validation examples, post-duplicate removal, of human preference judgments on model-generated summaries. We select only the uniquely preferred summaries for SFT, resulting in 41,947 training and 11,941 validation examples.\n\nWe employ a range of Pythia models (Biderman et al., 2023), with sizes of 2.8B, 6.9B, and 12B, serving as pre-trained base models. An SFT policy checkpoint trains similarly to Rafailov et al., (2023); Liu et al., 2024b on preferred texts for each dataset. We explore two main update strategies, each adaptable to different base alignment methods (e.g., DPO, IPO, KTO) as outlined in Section 2. (1) Soft Update: This strategy applies a weighting factor, with experiments conducted for, to progressively merge the current policy with its reference policy at each training step. Notably, the TR variants (TR-DPO, TR-IPO, and TR-KTO) become equivalent to their base methods (DPO, IPO, and KTO respectively) when. The notation for soft update is based on the base method, such as TR-DPO\u03b1, TR-IPO\u03b1, and TR-KTO\u03b1. (2) Hard Update: This strategy involves distinct experiments where the reference model updates at fixed intervals to evaluate the efficacy of varying update frequencies. The notation for the hard update method also depends on the base method and is similarly denoted as TR-DPO\u03c4, TR-IPO\u03c4, and TR-KTO\u03c4. Further details on the experimental setup and hyperparameters are given in Appendix A.\n\nFollowing established approaches (Rafailov et al., 2023; Liu et al., 2024b), we employ a comprehensive evaluation framework to assess the performance of various TR methods\u2019 configurations against the corresponding original baselines.\n\nAutoSxS evaluation: We employ the AutoSxS framework, using \u2018GPT-4-0125-preview\u2018 as a proxy for human evaluators (detailed prompt information can be found in Appendix H), to analyze preferences across 500 samples from the test set. This comparison includes various configurations of TR-DPO, TR-IPO, TR-KTO, and their respective traditional counterparts, DPO, IPO, and KTO, maintaining consistent generation parameters: temperature set to 1.0, top_k at 40, top_p at 0.7, and max_new_tokens at 512.\n\nModel dynamics and policy divergence: To investigate the influence of the proposed methods on KL divergence and generation diversity, we measure Self-BLEU (Zhu et al., 2018) and the KL divergence between the original and learned policies using the full Anthropic-HH test set for the Pythia 2.8B model. Additionally, we compare these metrics with the previously described mean HC metrics. This analysis allows for a detailed exploration of how our proposed methods modify the dynamics compared to simply changing the, and evaluates the trade-off between alignment and diversity (Kirk et al., 2023; Ahmadian et al., 2024; Wang et al., 2023)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Performance Comparison on the Two Tasks",
            "text": "For a comprehensive exploration of update strategies across the entire range of  and , we employed the TR-DPO approach using the Pythia 2.8B model. The base model was configured with , demonstrating an optimal trade-off between Human-Centric metrics and KL divergence, as detailed in Section 4.4. This setup allows for a systematic assessment of the impact of each strategy under varied conditions, effectively comparing the effects of different weighting factors  and update intervals .\n\nFigures 3(a) and 3(b) illustrate that both the soft and hard update strategies of TR-DPO enhance performance when compared to the traditional DPO method for the Pythia 2.8B model on the Anthropic-HH and Reddit TL;DR datasets. TR-DPO with  values between 0.5 and 0.6 consistently outperforms settings with other  values. Conversely, the benefits of TR-DPO\u03c4 become more pronounced as  increases from 64 to 512.\n\nFor both datasets, the parameters  and  for soft and hard updates, respectively, pass the Fisher statistical test with the Pythia 2.8B model size. Detailed results for the GPT comparison are presented in Appendix Table 3.\n\nWe assert that these parameters are near-optimal for most scenarios, and can be applied to various methods, tasks, and model sizes. This makes it possible to keep the computation budget the same for the proposed methods and baselines, maintaining the purity of the experiments in our work and facilitating hyperparameter tuning for further practical use of our methods. The results of the experiments are presented in Table 1 and include three methods (TR-DPO, TR-KTO), two tasks (Anthropic HH and Reddit TLDR), and three model sizes (2.8B, 6.9B, and 12B). For each method, an optimal  was selected and used in training the models (see Section 4.4 for more details). Examples of the generations of the models can be found in Appendix G."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Detailed Human-Centric Analysis",
            "text": "Figure 4 presents standardized absolute scores for HC metrics, including coherence, correctness, level of detail, helpfulness, and harmlessness, as evaluated on the Anthropic-HH dataset. For TR-DPO, the optimal values are between 0.5 and 0.6. Performance degradation is observed as deviates from this optimal range, particularly at higher values such as 0.8, where the model exhibits training instability and a tendency to generate repetitive words. A similar trend is observed for frequent updates when is equal to 64 (see Appendix Figure 7).\n\nWhile the results show a noticeable improvement in metrics such as coherence, correctness, helpfulness, and harmlessness, there is no improvement in the level of detail. This can be explained by the fact that this criterion depends on the length of generated texts. As previously noted in Park et al. (2024), DPO tends to increase the text length. In Appendix Figure 16, we show that TR-DPO is less subject to this problem, as it tends to produce shorter texts for the comparable levels of KL divergence.\n\nSimilarly, both TR-IPO and TR-KTO show statistically confirmed enhancements over their corresponding baselines. A detailed analysis of these methods, including specific parameters, is provided in Appendix C."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Why Just Lowering  Is Not Enough",
            "text": "It is clear that updating the reference policy of the trained one increases the KL divergence between the trained policy and the original SFT. Equation 1 shows that the same effect can be achieved by reducing the coefficient of the constraint on the KL divergence. The same parameter is also present in equations 2, 3, and 4. Figure 6(a) demonstrates the dependency of the average value of the HC metrics and the KL divergence with the SFT on the parameter for the DPO, TR-DPO\u03b1, and TR-DPO\u03c4 methods, using the Pythia 2.8B model on the Anthropic-HH dataset. We note that the best value of HC metrics is achieved at a certain point, which we use for DPO in the experiments from Sections 4.2 and 4.3. The value of this parameter for other methods was chosen the same way (see the graphs in Appendix D). One notable result is that, after reaching a peak value, HC metrics for DPO fall faster than for its modified versions, allowing the TR methods to achieve better results.\n\nAs highlighted by recent research, models with higher alignment tend to produce less diverse generations. The dependency of HC metrics on Self-BLEU is shown in the Appendix Figure 15 and is similar to the dependency on KL divergence. Based on these graphs, we can affirm that the TR methods show higher values of HC metrics at the same level of response diversity (or KL divergence).\n\nTo understand this behavior, consider the gradient dynamics of DPO and the update strategies of both TR-DPO versions during the training process. The gradient of the loss function, specified in Equation 2, is as follows: \n\nThe absolute values of term act as a weighting factor, signifying the disparity between the outcomes of the trained policy and the reference model. For instance, the gradient magnitude becomes small when the probability assigned to the preferred outcome by the trained policy is higher than that of the reference model and conversely for the less preferred outcome.\n\nThe gradient scales are depicted in Figure 6, smoothed for clarity. We hypothesize that there is a relationship between these observed gradient scales and the training dynamics. Specifically, a higher parameter in soft updates and a lower parameter in hard updates both result in increased gradient scales, which may be indicative of greater training instability (see Appendix D.3 for more details). This has the potential to cause significant deviations from the stability of the SFT policy, leading to degradation. Conversely, updating the reference policy allows for more deviation from the SFT policy while maintaining stable performance compared to the standard DPO method. Similar behavior is observed for other methods as presented in Appendix D, Figures 12(b) and 13(b).\n\nThus, simply lowering the coefficient is not sufficient to reach the optimal results. By updating the reference policy, TR methods achieve higher metric values and maintain stable performance compared to standard alignment techniques, indicating the importance of incorporating such update strategies to enhance training dynamics."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "This paper introduces a new class of language model alignment methods called Trust Region (TR). TR-DPO, TR-IPO and TR-KTO stand out from classical offline optimization methods by updating the reference policy during the training process. The \"Trust Region\" designation reflects our approach\u2019s foundational principle of allowing models to explore beyond the initial SFT policy to achieve improved alignment convergence. The effectiveness of this strategy is likely based on the nuanced dynamics of the loss function\u2019s gradient behavior. We validate the proposed technique on a variety of tasks and model sizes. The results show that our modifications improve performance in the majority of scenarios. This underscores the universality and reliability of our approach.\nLimitations and future work. In our study, we propose a new paradigm for language model alignment and demonstrate its effectiveness on two of the most popular datasets. Future research could explore how the proposed method generalizes to other domains, modalities, and dataset sizes, especially small datasets. In this work, we show that models trained by our method suffer less from an increase in response length, but the justification of this behavior is beyond the scope of our work and requires further investigation. Finally, our results are based on an automatic evaluation by GPT-4. This raises the question of how suitable such evaluation is for assessing alignment techniques, and whether there could be more suitable evaluation methods.\nSocietal impacts. The methods proposed in this article, like other alignment methods, can be used to create unsafe assistants. We are only warning about this possibility, but our research is aimed at creating helpful and harmless assistants, and we expect that our methods will be used for the same purposes."
        }
    ],
    "url": "http://arxiv.org/html/2404.09656v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.09656v2",
        "paper_title": "Learn Your Reference Model for Real Good Alignment",
        "research_background": "### Motivation\nThe paper is motivated by the increasing importance of aligning Large Language Models (LLMs) to be not only effective but also safe and controllable. Existing approaches to alignment, such as Direct Preference Optimization (DPO), Identity Preference Optimization (IPO), and Kahneman-Tversky Optimization (KTO), rely on static reference models and do not account for updated data during training, potentially limiting their effectiveness.\n\n### Research Problem\nThe primary research problem addressed in the paper is the static nature of the reference model in alignment strategies. Traditional methods like DPO, IPO, and KTO use a fixed reference model, even when new data is available that could inform a more apt reference model. This limitation suggests that these alignment methods might not fully capture the evolving nature of human preferences as new training data becomes available.\n\n### Relevant Prior Work\nThe paper builds on various works:\n\n1. **Reinforcement Learning Techniques**: Initial attempts in aligning LLMs used reinforcement learning methods to develop Reward Models (RMs) based on human preferences, making the LLMs produce outcomes that maximize these RM values (Bai et al., 2022; Schulman et al., 2017).\n\n2. **Direct Preference Optimization (DPO)**: Proposed by Rafailov et al. (2023), this method bypasses the RMs and directly optimizes the LLM by focusing on the training data likelihood related to human preferences.\n\n3. **Identity Preference Optimization (IPO)**: Introduced by Azar et al. (2023), IPO slightly modifies the optimization task to maximize the probability that one text is preferable over another instead of maximizing rewards.\n\n4. **Kahneman-Tversky Optimization (KTO)**: Developed by Ethayarajh et al. (2024), KTO uses the principle of loss aversion from Kahneman and Tversky (1979), modifying the loss function to reflect that losses weigh more than equivalent gains.\n\nThese prior works highlight the critical steps in evolving from basic RL methods to more sophisticated optimization strategies but retain a static reference model issue.\n\n### Contributions\nThe paper introduces the Trust Region (TR) approach for preference optimization. This involves dynamically updating the reference model during training, either by softly integrating new data into the old reference model or by replacing it after predetermined steps. The contributions are:\n1. **Implementation of TR-DPO, TR-IPO, and TR-KTO**: Demonstrating higher effectiveness than traditional methods across different model sizes.\n2. **Evaluation using Probability of Improvement (PoI)**: Showing that TR methods outperform baselines in key human-centric metrics (coherence, correctness, helpfulness, and harmlessness).\n3. **Analysis of HC metrics, KL divergence, and response diversity**: Illustrating that the proposed methods achieve higher HC metrics at equivalent levels of KL divergence compared to classical methods.",
        "methodology": "The proposed method focuses on improving the alignment of policies with a reference model during training by updating the reference model's parameters dynamically rather than keeping them fixed. This dynamic adjustment is achieved using two primary update methods: soft update and hard update.\n\n### Key Components and Innovations\n\n1. **Soft Update**:\n   - This method incrementally adjusts the reference policy's parameters during training.\n   - The soft update is mathematically described as:\n     \n     \\( \\theta_{\\text{ref}} \\leftarrow \\tau \\theta_{\\text{ref}} + (1 - \\tau) \\theta \\)\n\n     where \\(\\tau\\) is a weighting factor determining the rate at which updates influence the reference policy. Both the reference policy (\\(\\theta_{\\text{ref}}\\)) and the updated policy (\\(\\theta\\)) are initially derived from the same set of parameters, thus justifying the incremental approach by leveraging insights from Rofin et al., (2022) and Ilharco et al., (2023).\n\n2. **Hard Update**:\n   - This method updates the reference policy at every \\(k\\) training steps with the current policy directly.\n   - The hard update is defined as:\n     \n     \\( \\theta_{\\text{ref}} \\leftarrow \\theta \\)\n\n     where \\(k\\) represents a specified number of training iterations. This approach involves more significant and episodic adjustments, allowing the model to make substantial leaps in the learning process.\n\n3. **Application to LM Alignment Methods**:\n   - The paper investigates the application of these updates to three popular language model alignment methods: DPO (Rafailov et al., 2023), IPO (Azar et al., 2023), and KTO (Ethayarajh et al., 2024).\n   - The study introduces a new class of methods called Trust Region (TR) methods: TR-DPO, TR-IPO, TR-KTO. These methods incorporate the proposed updates and aim to balance reward maximization with maintaining closeness to the reference policy.\n\n4. **Impact on Training Objective**:\n   - Updating the reference policy alters the training objective, drawing a parallel to Trust Region optimization methods (Schulman et al., 2015, 2017). The training objective can be redefined as:\n\n     \\( \\mathcal{L}_{\\text{TR}} = \\mathbb{E} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} Q(s, a) \\right] \\)\n\n     where \\(\\pi_{\\theta}\\) is the current policy and \\(\\pi_{\\theta_{\\text{old}}}\\) is the previously obtained policy.\n\n     - **Parameter \\(k\\)**: Controlling \\(k\\), the frequency of changes in the reference policy, transforms the setup between nearly static (rare updates, equivalent to vanilla training) to dynamic (frequent updates, akin to pure TR setup).\n\n     - **Adjustment Dynamics**:\n       - **Small \\( \\tau \\) (large \\( k \\))**: Leads to rare updates, maintaining proximity to the reference policy to avoid large deviations.\n       - **Large \\( \\tau \\) (small \\( k \\))**: Facilitates frequent updates, allowing the policy to potentially diverge more from the reference policy, aligning more closely with a Trust Region (TR) optimization framework.\n\nThe overall innovation lies in transitioning from a fixed reference model to a dynamically updated reference model during training, leading to potentially better policy alignment and improved learning outcomes.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n- **Anthropic-HH**: Contains 160,800 training and 8,552 validation examples of pair preferences for supervised fine-tuning (SFT) and preference learning.\n- **Reddit TL;DR Summarization**: Includes 73,396 training and 26,198 validation examples of human preference judgments on model-generated summaries, post-duplicate removal. Selected uniquely preferred summaries yield 41,947 training and 11,941 validation examples for SFT.\n\n**Pre-trained Base Models:**\n- **Pythia Models**: Sizes of 2.8B, 6.9B, and 12B.\n\n**Training Configuration:**\n- **SFT Policy Checkpoints**: Train similarly to the methods used in Rafailov et al., (2023) and Liu et al., (2024b) on preferred texts for each dataset.\n- **Update Strategies**:\n  - **Soft Update**: Applies a weighting factor (\u03b1) to progressively merge the current policy with its reference policy. TR variants become equivalent to their base methods (DPO, IPO, KTO) when \u03b1 = 1.\n  - **Hard Update**: Reference model updates at fixed intervals (\u03c4) to evaluate varying update frequencies.\n\n**Evaluation Metrics:**\n- **AutoSxS Evaluation**: Utilizes \u2018GPT-4-0125-preview\u2019 as a proxy for human evaluators to analyze preferences across 500 samples from the test set. Compares TR-DPO, TR-IPO, TR-KTO configurations with their traditional counterparts (DPO, IPO, KTO), maintaining consistent generation parameters (temperature: 1.0, top_k: 40, top_p: 0.7, max_new_tokens: 512).\n- **Human-Centric (HC) Metrics**: Scores coherence, correctness, level of detail, helpfulness, and harmlessness on a scale from 1 to 10. A subset of 150 provocative test samples from the Helpful and Harmless dataset is assessed using \u2018GPT-4-0125-preview\u2019 as a proxy for human judgment.\n\n### Main Experimental Results\n\n- **AutoSxS Evaluation**: Results indicate that the proposed TR methods (TR-DPO, TR-IPO, TR-KTO) show significant improvement in alignment with human preferences compared to the traditional methods (DPO, IPO, KTO).\n- **Human-Centric Metrics**: The TR methods demonstrate higher scores across metrics like coherence, correctness, level of detail, helpfulness, and harmlessness.\n\nOverall, the experimental results support the efficacy of the TR methods in enhancing alignment with human preferences, as measured by comprehensive evaluation frameworks including AutoSxS and HC metrics."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of the proposed Trust Region (TR) methods (TR-DPO, TR-IPO, TR-KTO) for updating the reference policy during training and to compare their performance against traditional methods on language model alignment tasks.",
            "experiment_process": "We conducted experiments on two datasets: Anthropic-HH and Reddit TL;DR summarization. For training, we used Pythia models with sizes of 2.8B, 6.9B, and 12B. We explored two main update strategies: Soft Update (with a weighting factor \u03b1) and Hard Update (with fixed intervals \u03c4). We employed AutoSxS framework using GPT-4-0125-preview as a proxy for human evaluators to analyze preferences across 500 test samples. Human-Centric metrics (HC metrics) were used for direct assessment including coherence, correctness, level of detail, helpfulness, and harmlessness, scored from 1 to 10. Additionally, we measured Self-BLEU and KL divergence between original and learned policies, using the full Anthropic-HH test set for the Pythia 2.8B model.",
            "result_discussion": "Results highlighted that both Soft and Hard update strategies of TR-DPO outperformed the traditional DPO method for the Pythia 2.8B model on both datasets, with optimal \u03b1 values between 0.5 and 0.6 and \u03c4 values between 64 to 512. These configurations passed the Fisher statistical test. The experiments showed that updating the reference policy increases KL divergence and Self-BLEU values, but with better HC metrics, indicating an improved balance between alignment and diversity.",
            "ablation_id": "2404.09656v2.No1"
        },
        {
            "research_objective": "To assess the performance of TR-DPO method on the Anthropic-HH dataset in terms of Human-Centric (HC) metrics and to identify optimal parameters for the update strategies.",
            "experiment_process": "The standardized absolute scores for HC metrics were evaluated on the Anthropic-HH dataset using the TR-DPO method. We examined different \u03b1 values for Soft Update, noting performance degradation at higher values such as 0.8. Additionally, frequent updates at \u03c4=64 were also evaluated. Probability of Improvement (PoI) metric was employed, using bootstrap sampling to generate confidence intervals and medians.",
            "result_discussion": "The TR-DPO method showed that the optimal \u03b1 values were between 0.5 and 0.6, with performance degrading outside this range. For Hard Updates, optimal intervals were found to be 512 and 1024. Results demonstrated significant improvements, particularly in coherence, correctness, helpfulness, and harmlessness metrics, but no improvement in the level of detail due to text length dependencies.",
            "ablation_id": "2404.09656v2.No2"
        },
        {
            "research_objective": "To examine whether updating the reference policy (\u03b1 or \u03c4) is more effective than simply lowering the KL divergence constraint coefficient (\u03b2) for achieving optimal results.",
            "experiment_process": "The TR-DPO, TR-DPO\u03b1, and TR-DPO\u03c4 methods were tested using the Pythia 2.8B model on the Anthropic-HH dataset. The average values of HC metrics and KL divergence with the SFT were plotted against the \u03b2 parameter. The gradient dynamics of DPO and both TR-DPO versions were analyzed, and the relationship between gradient scales and training dynamics observed.",
            "result_discussion": "The best HC metrics were achieved at \u03b2 =1.0. After reaching peak value, HC metrics for DPO dropped faster than for its modified versions (TR-DPO). TR methods showed higher metric values and more stable performance by allowing more deviation from the SFT policy. The results suggest that simply lowering the \u03b2 is not sufficient for optimal outcomes; updating the reference policy is more effective for maintaining training stability and achieving higher metrics.",
            "ablation_id": "2404.09656v2.No3"
        }
    ]
}