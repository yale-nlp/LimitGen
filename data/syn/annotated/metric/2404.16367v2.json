{
    "title": "Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically",
    "abstract": "Transformers trained on natural language data have been shown to learn its hierarchical structure and generalize to sentences with unseen syntactic structures without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such generalization behavior to emerge. We extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g. sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically.\n\nWe then conduct pruning experiments to study how transformers trained with the language modeling objective encode hierarchical structure. When pruned, we find joint existence of subnetworks within the model with different generalization behaviors (subnetworks corresponding to hierarchical structure and linear order).\n\nFinally, we take a Bayesian perspective to further uncover transformers\u2019 preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and whether the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Natural language is structured hierarchically: words are grouped into phrases or constituents, which can be further grouped to form higher-level phrases up to the full sentence. Understanding how well neural network models trained on language data learn this phrase structure of human language is of great interest. Many studies have demonstrated that syntax trees can be recovered from recurrent neural network (RNN) and transformer-based models trained on large-scale language corpora. While these studies provide useful evidence, they do not shed light on the architectural choices, training paradigms, or dataset characteristics that lead models to learn the phrase structure of language.\n\nA useful tool to understand these model and dataset-specific properties is through the test for hierarchical generalization, evaluating a model's capability to generalize to novel syntactic forms unseen during training. A classic problem to test for hierarchical generalization is question formation, where given a declarative sentence, e.g., \"My walrus does move the dogs that do wait,\" the task is to transform it into a question: \"Does my walrus move the dogs that do wait?\" The task is accomplished by moving one auxiliary verb to the front. The correct choice to move does in this example (rather than do) is predicted both by a hierarchical rule based on the phrase-structure syntax of the sentence and by a linear rule that says to move the first auxiliary.\n\nThis question has been studied in past work for different neural network architectures. For example, some studies have shown that recurrent neural networks fail to generalize hierarchically when trained on ambiguous data, and hierarchical generalization is achieved only using tree-structured networks. However, it has been demonstrated that, surprisingly, when trained for a long time after attaining perfect training accuracy, transformers do start to generalize hierarchically. This phenomenon is named Structural Grokking, where neural networks start to generalize long after they have overfit the training data.\n\nIn our work, we ask why transformers show hierarchical generalization, despite lacking architectural biases towards hierarchical structure. We first explore if the choice of training objective can influence hierarchical generalization in transformers. Specifically, we consider five objectives in our study: language modeling, sequence-to-sequence modeling, prefix language modeling, sequence classification, and cloze completion, and compare the hierarchical generalization exhibited by transformers under these objectives. As a test for hierarchical generalization, we include tasks like German question formation, tense-reinflection, passivization, and a synthetic task for subject-verb agreement. To better understand how different generalization behaviors are implemented within the trained networks, we propose two new attention head pruning strategies to discover subnetworks corresponding to different generalizations.\n\nFinally, to understand why language modeling results in bias towards hierarchical structure, we utilize the Bayesian framework and consider generative probabilistic grammars. Specifically, we construct hierarchical grammars (consistent with the hierarchical rule) and regular grammars that generate the data linearly. We then compare the posterior probabilities of the two grammars to understand which grammar has a better trade-off for the goodness of fit and simplicity, thereby explaining the preference of transformers for hierarchical or linear generalization.\n\nOur aim is to understand hierarchical generalization in transformers in isolation. We train transformer models from scratch, without any pretraining, to eliminate biases due to previously being trained on language data. For the same reason, we use synthetic datasets for training and evaluation that exclusively measure the inductive biases towards hierarchical or linear generalization. Due to the controlled nature of our setup, the transformer models that we train are small.\n\nWe discover that the choice of the training objective affects hierarchical generalization in transformers. Among five training objectives and five datasets, we find that only the language modeling objective consistently obtains strong hierarchical generalization across different tasks. This highlights that modeling the entire sequence of tokens (input and output) is critical for learning hierarchical structure.\n\nWe find that different types of generalizations consistent with the training data can be discovered as subnetworks in the trained model, and these subnetworks continue to coexist over the course of training. Further, we find these disparate subnetworks exist due to the ambiguity in the training data, as different subnetworks disappear upon adding disambiguating examples.\n\nFinally, utilizing the Bayesian framework, we show a correlation between transformer LMs generalizing hierarchically and hierarchical grammars having higher posterior probabilities compared to regular grammars. This suggests that transformers generalize hierarchically because hierarchical grammars that fit the data are often \u201csimpler\u201d compared to regular grammars. We also identify a case where this does not hold and show that transformers fail to generalize hierarchically in this case.\n\nTo the best of our knowledge, this work is the first to show that the language modeling objective is a source of inductive bias for hierarchical generalization and to use the Bayesian perspective to explain hierarchical generalization in language models. Our analysis method could be useful to study other forms of generalization in these models."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Hierarchical generalization is a form of systematic generalization, where given instances generated from a hierarchical grammar, we evaluate the capability of a model to generalize to unseen syntactic forms. For example, consider the task of converting a declarative English sentence to a question:\nInput: My walrus does move.\nOutput: Does my walrus move?\nInput: My walrus does move the dogs that do wait.\nOutput: Does my walrus move the dogs that do wait?\nNotice that the task can be accomplished by moving one auxiliary verb to the front of the sentence. While for sentences of type 1a, with only a single auxiliary this is trivial, for sentences of type 2a, as English speakers we know that the auxiliary to move is the one associated with the head verb in the sentence. Modeling this rule requires understanding the phrase structure of the language. We call this Hierarchical Rule. One can alternatively consider a much simpler explanation, the Linear Rule: moving the first auxiliary in the sentence to the beginning. This linear rule is independent of the hierarchical structure of the sentence. However, consider sentences of type 3 below:\nInput: My walrus who doesn\u2019t sing does move.\nLinear rule output: Doesn\u2019t my walrus who sing does move?\nHierarchical rule output: Does my walrus who doesn\u2019t sing move?\nFirst, notice that sentence 3a has a different syntactic structure compared to sentence 2a, as the relative clause who doesn\u2019t sing accompanies the subject, unlike in example 2 where that do wait modified the object (the dogs). In this case, using the linear rule to form question will result in an ungrammatical sentence, i.e., outputting sentence 3b instead of sentence 3c. In this work, we study the following question: Consider neural networks trained from scratch on data consistent with both hierarchical and linear rules. When presented with sentences such as 3a do they generalize hierarchically (predicting 3c) or do they learn a linear rule (predicting 3b)?\nIn our study, we consider five tasks, including the question formation task above. Examples from all the tasks (excluding English question formation) are provided in Table 1. All the tasks follow a common recipe: the training dataset has examples that are consistent with both hierarchical and linear rules. For evaluation, two variants of the test data are considered: an in-distribution test set, which follows the same distribution as the training data (i.e., has the same syntactic forms and is also ambiguous with respect to the correct rule); and a generalization test set, which consists of examples which are only consistent with the hierarchical rule. Below we provide the details of the five tasks.\n\nQuestion formation (German). This is the same task as above, but the sentences are in German instead of English. We use the dataset from Mueller et al. (2022), consisting of sentences with the modals k\u00f6nnen/kann (can) or auxiliaries haben/hat (have/has), together with infinitival or past participle main verbs as appropriate, which can be moved to the front similar to English to form questions. Note that in German, negation is represented using another word nicht which is not fronted with the auxiliary (can\u2019t becomes kann nicht), hence Mueller et al. (2022) do not use the auxiliaries with negation for German, like we have for the English version. Here again, the linear rule is to move the first modal or auxiliary to the front, and the hierarchical rule requires moving the token associated with the main verb. The dataset construction and evaluation metrics remain identical to the English version.\n\nPassivization. The task here is to transform an active sentence to passive. The dataset from M\u00fcller et al. (2022) is constructed such that it contains active sentences of three types: (i) without any prepositional phrase (PP), (ii) with a PP on the object, and (iii) with a PP on the subject. Similar to question formation dataset, the active-passive pairs in the training dataset are constructed only using the sentences of type (i) and (ii). These two sentence types are again compatible with both rules: the hierarchical rule which involves identifying the object in the sentence and moving it to the front, and the linear rule that moves the second noun in the sentence to front. Like question formation, the training data is augmented with identity active-active pairs which consist of sentences of all the three types. \n\nSimple agreement. We also introduce a simplified version of the tense reinflection task. Unlike other tasks, simple agreement is a single-sentence task where only the present-tense sentences from the tense-inflection are used for training. In this task, we evaluate the model\u2019s ability to generate the correct inflection of the verb at the end of the sentence. E.g., when given the prefix my zebra by the yaks"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "How the Training Objective Influences Hierarchical Generalization",
            "text": "We now discuss how the choice of training objective can influence hierarchical generalization in transformers. Prior work by McCoy et al., Petty and Frank, and Mueller et al. used a sequence-to-sequence training objective to train encoder-decoder models and found that RNNs and transformers do not exhibit hierarchical generalization. More recently, Murty et al. used a language modeling objective to train a decoder-only transformer, which they found did generalize hierarchically when trained for a sufficiently large number of epochs \u2013 well beyond the point of achieving perfect training task accuracy. To the best of our knowledge, this distinction isn\u2019t called out by prior work. Hence we conduct a systematic study to understand what effect the training objective has on hierarchical generalization.\n\nGiven a sequence of tokens, the language modeling objective trains the model to predict each token in a sequence given the preceding tokens. The model is optimized to minimize the negative log-likelihood of the sequences in the training data. For transformers, the language modeling objective is typically associated with decoder-only models like GPT, and the loss is computed over all tokens in the sequence.\n\nThe sequence-to-sequence (seq2seq) modeling objective is used to train the model to generate a target sequence given an input sequence. This objective is typically associated with an encoder-decoder model as used in the original transformer architecture. Note that the seq2seq objective is more suited for tasks with an explicit input and output but is not suitable for the simple agreement task. Hence, we do not evaluate the seq2seq objective for simple agreement.\n\nIn the prefix language modeling objective, we again generate the output text given the input (or \u201cprefix\u201d), but we use a single transformer decoder instead of an encoder-decoder model. Differing from the original language modeling objective, here the loss is only computed over the output text and does not include the prefix. One modification that we make to how the prefix-LM objective is typically used, is that we use a causal mask for the prefix tokens as well instead of having bi-directional attention over the prefix tokens, since we found the latter to perform subpar in our initial experiments.\n\nIn the sequence classification objective, the model is trained to map the entire sequence to a discrete label. E.g., for question formation, the model is given the input declarative sentence and trained to predict the correct auxiliary from the set of auxiliary verbs that should occur at the start of the question, i.e., a four-way classification task.\n\nIn the cloze completion setting, the model is given a sequence of tokens with some tokens masked and trained to predict the masked tokens. Specifically, we have mask tokens where (i) the auxiliary is present in the interrogative sentence or (ii) the auxiliary was present in the original declarative sentence. The model is trained to predict the correct auxiliary at these positions and EMPTY if an auxiliary is not present at a particular position. Note that this objective is similar to masked language modeling; however, instead of masking tokens randomly, we mask the specific tokens as described above. Our initial experiments with random-masking resulted in subpar performance, even on in-distribution test sets. For the passivization task, we do not evaluate the cloze completion objective because the output sequence is significantly different from the input, which makes defining the masking strategy non-trivial.\n\nPlease refer to the appendix for full details of each objective for all of the five tasks. By design of the test datasets, a model following the linear rule will obtain 100% in-distribution accuracy and 0% generalization accuracy. Only a model consistent with the hierarchical rule will obtain 100% accuracy on both test sets for all the tasks.\n\nOur results suggest that the language modeling objective imposes bias towards hierarchical generalization in transformers. Overall, our experiments implicate language modeling as a source of inductive bias for the neural models to generalize hierarchically. We hypothesize that the reason LMs (approximately) learn the hierarchical rule rather than the linear rule is that, when viewing the full training objective as modeling the distribution of all the tokens in the sentence pairs and not just moving of the auxiliary to the beginning, it is the combined simplicity of the hierarchical rule and the data is greater than the linear rule and the data. Perhaps modeling the hierarchical phrase structure is beneficial for modeling the distribution over full sequences."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training Objectives",
            "text": "We consider the following five training objectives in our study:\n\nGiven a sequence of tokens, the language modeling objective trains the model to predict each token in a sequence given the preceding tokens. The model is optimized to minimize the negative log-likelihood of the sequences in the training data. For transformers, the language modeling objective is typically associated with decoder-only models like GPT, and the loss is computed over all tokens in the sequence. For the question formation task and the declarative-question pair from the introduction, the cross-entropy loss is computed over each token given the preceding tokens.\n\nThe sequence-to-sequence (seq2seq) modeling objective is used to train the model to generate a target sequence given an input sequence. This objective, which includes only the terms related to output generation, is typically associated with an encoder-decoder model as used in the original transformer architecture. Note that the seq2seq objective is more suited for tasks with an explicit input and output (like question formation and tense inflection), but is not suitable for the simple agreement task. Hence, we do not evaluate the seq2seq objective for simple agreement.\n\nIn the prefix language modeling objective, we again generate the output text given the input (or \u201cprefix\u201d), but we use a single transformer decoder (similar to language modeling) instead of an encoder-decoder model. Differing from the original language modeling objective, here the loss is only computed over the output text and does not include the prefix. One modification that we make to how the prefix-LM objective is typically used is that we use a causal mask for the prefix tokens as well instead of having bi-directional attention over the prefix tokens, since we found the latter to perform subpar in our initial experiments.\n\nIn the cloze completion setting, the model is given a sequence of tokens with some tokens masked and trained to predict the masked tokens. For the question formation task, we consider the declarative-interrogative pair and mask out tokens in the interrogative sentence at all positions where the auxiliaries could be present. Specifically, we have mask tokens where (i) the auxiliary is present in the interrogative sentence or (ii) the auxiliary was present in the original declarative sentence. The model is trained to predict the correct auxiliary at these positions and EMPTY if an auxiliary is not present at a particular position. Note that this objective is similar to masked language modeling; however, instead of masking tokens randomly, we mask the specific tokens as described above. Our initial experiments with random-masking resulted in subpar performance, even on in-distribution test sets. For the passivization task, we do not evaluate the cloze completion objective, because (unlike other tasks) the output sequence is significantly different from the input and not just in terms of one or two tokens, which makes defining the masking strategy in this case non-trivial.\n\nPlease refer to \u00a7A.1.1 for full details of each objective for all of the five tasks."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Experimental Setup",
            "text": "We train transformer models from scratch for all of our experiments. We use transformer models with 8 heads and embedding dimension 512 for all datasets and objectives. Following Murty et al. (2023a), for question formation and tense reinflection, we train transformer models with 6 layers for the former (18M parameters) and 4 layers (12M parameters) for the latter task, for all objectives excluding seq2seq. For the remaining tasks, we use 6-layer transformer encoder/decoder layers (18M parameters) depending on the training objective. For the seq2seq objective, we use a 6-layer encoder/6-layer decoder model (25M parameters) for all tasks. We also considered out other choices of number of layers for the seq2seq objectives and found results consistent with our findings (see Appendix \u00a7A.1.3). For all the tasks, tokenization is performed at the word level.\n\nWe use the Adam optimizer (Kingma and Ba, 2015) for training the model with a learning rate of, following Murty et al. (2023a). We use a batch size of 8, and train for 300k steps (24 epochs) for all tasks excluding simple agreement, which we train for 200k steps (32 epochs), since the dataset is half the size of others (recall that we have 100k training examples for all the tasks except simple agreement for which we have 50k). We run each experiment with 5 seeds and report the average performance."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Results",
            "text": "We compare the five objectives for the five tasks and show the results in Figure 1. Notice that while all the objectives obtain close to 100% accuracy on the in-distribution test sets (except sequence classification which performs slightly worse for simple agreement), there is a lot of variation in the generalization accuracy. Particularly, we observe that only the language modeling objective consistently obtains high generalization accuracy on all five tasks, while models trained with other objectives often struggle. While seq2seq and prefix LM perform well on tense reinflection and passivization respectively, they perform much worse on the other tasks.\n\nWe believe that the choice of objective might be the reason behind the discrepancy in the results of Murty et al. (2023a) showing that transformer models with the language modeling objective generalize hierarchically, and the result of Petty and Frank (2021), Mueller et al. (2023) showing that transformer models with seq2seq objective do not generalize hierarchically. Interestingly, seq2seq and prefix LM bear the greatest resemblance to the language modeling objective, as these two also involve generating the whole output sequence. The major difference between language modeling and these two objectives is that language modeling involves computing the loss over all the tokens, including the input tokens, which indicates that the corresponding loss terms from modeling the input tokens might be crucial for hierarchical generalization. Our hypothesis on why that might be important is that when considering loss over all the tokens, the model cannot just simply learn a trivial transformation (e.g., for question formation, move the first auxiliary to the beginning and copy rest of the tokens from input) from input sequence to output sequence to minimize the loss (as it needs to model the input token distribution as well).\n\nWe also provide the training curves depicting the in-distribution and generalization performance of different objectives over the course of training in Figure 6 in Appendix. Consistent with the findings of Murty et al. (2023a), for all the tasks, we find a delay in generalization for the LMs \u2013 models typically obtain 100% in-distribution accuracy much earlier than achieving high generalization performance. One might also notice that while LM objective consistently achieves high generalization performance, it is not perfect, like in the case of question formation and tense reinflection, where its average performance is roughly 75%. Recall that these reported numbers are averaged across 5 seeds. For all the tasks we find that there are seeds for which LM models achieve 100% generalization accuracy, and there are also others with lower accuracy. Apart from the two exceptions discussed above, this is not the case for other objectives, where none of the five seeds get 100% generalization accuracy.\n\nInterestingly, we observe (in Figure 1) that transformer LMs on average perform better on German question formation than the English version of the same task. We suspect this might be because the grammar used for generating German dataset is structurally more rich compared to the English grammar, as it also consists of both infinitival and past participle (depending on if the sentence consists of auxiliaries or modals) forms of the main verbs, while only infinitival forms are included in the English version. As noted in McCoy et al. (2018), presence of rich hierarchical cues in the data can aid in hierarchical generalization.\n\nOur results above suggest that the language modeling objective imposes bias towards hierarchical generalization in transformers. Based on this, we revisit hierarchical generalization in RNNs, as the experiments of McCoy et al. (2020) were conducted using the seq2seq objective (and found to not generalize when not using attention mechanism). We train 2-layer GRU (Cho et al., 2014) models using a learning rate of 0.001 using both LM and seq2seq objectives for the question formation task. As shown in Figure 2, like transformers, RNNs also generalize hierarchically on the question formation task when trained using the language modeling objective, but that\u2019s not the case for the seq2seq objective. This further strengthens the evidence that language modeling acts as a source of inductive bias for hierarchical generalization.\n\nOverall, our experiments implicate language modeling as a source of inductive bias for the neural models to generalise hierarchically. We hypothesize that the reason LMs (approximately) learn the hierarchical rule rather than the linear rule is that, when viewing the full training objective as modeling the distribution of all the tokens in the sentence pairs and not just moving of the auxiliary to the beginning, it is the combined simplicity of the hierarchical rule and the data is greater than the linear rule and the data. Perhaps modeling the hierarchical phrase structure is beneficial for modeling the distribution over full sequences. We will explore this hypothesis in more depth in \u00a75."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discovering Subnetworks with Different Generalization Behaviors",
            "text": "The results from\nMurty et al. (2023a  ###reference_b36###), and from \u00a73.3  ###reference_### show that the transformer LM obtains perfect in-domain accuracy much earlier during the training, while generalization comes later. This implies that the model might be implementing something akin to the linear rule in the beginning of training and eventually generalizes to the hierarchical rule. In this section, we explore whether these rules are implemented as subnetworks in the model and ask how these subnetworks evolve over the course of training.\nUnless specified otherwise, for pruning, we use a learning rate of , the  regularization penalty coefficient as , and train for 10k steps, which we found to work well across different pruning settings. Here we report the experiments for the question formation task and discuss the others in Appendix \u00a7A.2  ###reference_###, for which we also obtain consistent results. Note that since we are interested in discovering subnetworks implementing hierarchical and linear rules, while pruning, we only use the negative -likelihood of the first auxiliary in the question for computing the loss. To make sure that the discovered subnetworks are not just a by-product of the pruning procedure, we also consider control groups, which are obtained by pruning randomly initialized networks.\nWe now consider how these different subnetworks evolve over the course of the model training. To this end, we save checkpoints after every 1000 steps of training and perform the three kinds of pruning on those checkpoints. As shown in Figure 4  ###reference_###(b), the \u201clinear-rule\u201d subnetwork becomes discoverable through pruning  at roughly 6000 training steps \u2013 indicated by the 0% generalization performance for -prune curve and 100% in-distribution performance in Figure 4  ###reference_###(a). Soon after, we see in Figure 4  ###reference_###(b), the formation of a (hierarchical) generalization subnetwork, at around 9000 training steps, where the subnetwork obtained using Gen-prune obtains 100% generalization accuracy (the full network at this point only obtains 5% generalization performance).  While there are occasional spikes   in the generalization performance of subnetworks found using -prune during the first-thirds of the training, during majority of the training (especially the latter two-thirds) both the linear and hierarchical generalization subnetworks continue to be discoverable over the course of training \u2013 as indicated by stable 100% generalization accuracy of subnetworks found by Gen-prune and 0% generalization accuracy for -prune subnetworks in Figure 4  ###reference_###(b) .\nOur experiments reveal that, throughout training, there is competition between the two sub-networks, and while the behavior of the aggregate model becomes closer to the hierarchical rule with training, the competing linear-rule subnetwork does not really disappear. All three pruning methods are unsuccessful on the control group (randomly initialized networks), providing further evidence that these subnetworks are not introduced by the pruning methods, and behaviors akin to the hierarchical and linear rules are implemented within the language model.\n###figure_4### We hypothesize that the ambiguous training data (with two plausible generalizations, linear and hierarchical) is the reason for the existence of the subnetworks with very different generalization behaviors. To evaluate this hypothesis, we consider the case where the model is trained with disambiguated data \u2013 we augment the ambiguous training dataset with examples that are only consistent with the hierarchical rule.777We add 10k such examples to the existing dataset containing 100k examples. We plot the training dynamics of the model trained with this disambiguated data in Figure 4  ###reference_###(c). The full model without any pruning in this case, as expected, generalizes perfectly after a few thousand training steps (see 4  ###reference_###(c)) without any noise, in contrast with generalization accuracy of the full-model trained on ambiguous data in Figure 4  ###reference_###(b). More interestingly, Figure 4  ###reference_###(c) shows that -prune fails to yield subnetworks that obtain 0% generalization accuracy, in contrast to the ambiguous data case in figure 4  ###reference_###(b). To make sure this is not due to the choice of our pruning hyperparameters, we conduct an extensive hyperparameter search, consisting of 128 combinations of the pruning learning rate, regularization penalty, and pruning steps (using Bayesian optimization) and still fail to find the setting where the -prune succeeds for the disambiguated data case (see Figure 12  ###reference_### in Appendix). This strongly suggests that the \u201clinear-rule\u201d subnetwork is never formed in the language model when it doesn\u2019t have a reason to be learned \u2013 when the alternate generalization behavior (linear rule) is no longer applicable to the entire training dataset.\nWe also experiment with the opposite disambiguation setup, where we augment the training data with examples only consistent with the linear rule, and in line with our findings, we find that the Gen-prune fails to find a subnetwork with 100% generalization accuracy \u2013 no subnetwork consistent with hierarchical rule is formed (Figure 13  ###reference_### in Appendix). Hence, ambiguity in the training data appears to drive the joint existence of these contrasting subnetworks."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Finding Subnetworks",
            "text": "Following Merrill et al. (2023  ###reference_b30###) we use pruning to find the existence of subnetworks or circuits corresponding to different generalizations. In particular, we use the attention head pruning method from Voita et al. (2019  ###reference_b58###), which introduces learnable gates for each attention head of a trained transformer model. This introduces  learnable parameters, which is typically equal to 48 in our experiments. Pruning is then performed by training these learnable gates (while freezing the original model parameters) to minimize negative -likelihood objective, but also adding an -penalty as regularization to ensure sparsity. Since -norm is nondifferentiable, a stochastic relaxation is used, which considers the gates as random variables drawn from head-specific hard concrete distributions (Louizos et al., 2018  ###reference_b26###).   After completion of pruning, all the gates are either fully open or closed, and a closed gate implies that the output of the corresponding head is zeroed-out in the computation of multi-head self-attention. In the case that all heads in a layer are zeroed-out, that particular layer is skipped: inputs to the layer pass through unchanged to the next layer due to the residual connections.\nThus the pruning procedure does not modify any weights of the original model and merely performs subset selection on attention heads of the model. To find subnetworks consistent with different generalizations (linear-rule and hierarchical rule) we introduce three pruning strategies which differ in the data used for pruning:\n1. Train-prune uses the original ambiguous training dataset to prune the attention heads. The subnetwork thus found is likely to be a compressed version of the full model.\n2. Gen-prune uses a small fraction of the generalization set (1% or 100 examples) to prune the attention heads. If successful, this pruning would yield a subnetwork consistent with hierarchical generalization\u2014obtaining close to  generalization accuracy.\n3. -prune is minimizing the (negative -likelihood) loss on the training data and maximizing it for the (1%) generalization data. In this case, successful pruning should yield a subnetwork that exhibits generalization consistent with the linear rule, i.e., obtains  generalization accuracy but obtains  in-distribution accuracy.\nUnless specified otherwise, for pruning, we use a learning rate of , the  regularization penalty coefficient as , and train for 10k steps, which we found to work well across different pruning settings. Here we report the experiments for the question formation task and discuss the others in Appendix \u00a7A.2  ###reference_###  ###reference_###, for which we also obtain consistent results. Note that since we are interested in discovering subnetworks implementing hierarchical and linear rules, while pruning, we only use the negative -likelihood of the first auxiliary in the question for computing the loss. To make sure that the discovered subnetworks are not just a by-product of the pruning procedure, we also consider control groups, which are obtained by pruning randomly initialized networks."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "###figure_5### In Figure 3  ###reference_###, we show the effect of different pruning methods on an intermediate model checkpoint, which does not yet generalize hierarchically\n(the model before pruning has a generalization accuracy of 30%).\nAfter Train-prune, roughly  80% heads of the full model are removed and in-distribution performance is conserved, though there is a drop in generalization performance (30% to 23%).\nAfter Gen-prune, we are able to find a subnetwork that achieves 100% generalization accuracy. This is striking, because the full network performed much worse.\nAfter -prune, we find a subnetwork that achieves  generalization accuracy while having  in-distribution performance; this subnetwork is behaviorally equivalent to the linear rule. Hence, these pruning experiments reveal the existence of subnetworks implementing different generalization behaviors.\nWe now consider how these different subnetworks evolve over the course of the model training. To this end, we save checkpoints after every 1000 steps of training and perform the three kinds of pruning on those checkpoints. As shown in Figure 4  ###reference_###  ###reference_###(b), the \u201clinear-rule\u201d subnetwork becomes discoverable through pruning  at roughly 6000 training steps \u2013 indicated by the 0% generalization performance for -prune curve and 100% in-distribution performance in Figure 4  ###reference_###  ###reference_###(a). Soon after, we see in Figure 4  ###reference_###  ###reference_###(b), the formation of a (hierarchical) generalization subnetwork, at around 9000 training steps, where the subnetwork obtained using Gen-prune obtains 100% generalization accuracy (the full network at this point only obtains 5% generalization performance).  While there are occasional spikes   in the generalization performance of subnetworks found using -prune during the first-thirds of the training, during majority of the training (especially the latter two-thirds) both the linear and hierarchical generalization subnetworks continue to be discoverable over the course of training \u2013 as indicated by stable 100% generalization accuracy of subnetworks found by Gen-prune and 0% generalization accuracy for -prune subnetworks in Figure 4  ###reference_###  ###reference_###(b) .\nOur experiments reveal that, throughout training, there is competition between the two sub-networks, and while the behavior of the aggregate model becomes closer to the hierarchical rule with training, the competing linear-rule subnetwork does not really disappear. All three pruning methods are unsuccessful on the control group (randomly initialized networks), providing further evidence that these subnetworks are not introduced by the pruning methods, and behaviors akin to the hierarchical and linear rules are implemented within the language model.\n###figure_6### We hypothesize that the ambiguous training data (with two plausible generalizations, linear and hierarchical) is the reason for the existence of the subnetworks with very different generalization behaviors. To evaluate this hypothesis, we consider the case where the model is trained with disambiguated data \u2013 we augment the ambiguous training dataset with examples that are only consistent with the hierarchical rule.777We add 10k such examples to the existing dataset containing 100k examples. We plot the training dynamics of the model trained with this disambiguated data in Figure 4  ###reference_###  ###reference_###(c). The full model without any pruning in this case, as expected, generalizes perfectly after a few thousand training steps (see 4  ###reference_###  ###reference_###(c)) without any noise, in contrast with generalization accuracy of the full-model trained on ambiguous data in Figure 4  ###reference_###  ###reference_###(b). More interestingly, Figure 4  ###reference_###  ###reference_###(c) shows that -prune fails to yield subnetworks that obtain 0% generalization accuracy, in contrast to the ambiguous data case in figure 4  ###reference_###  ###reference_###(b). To make sure this is not due to the choice of our pruning hyperparameters, we conduct an extensive hyperparameter search, consisting of 128 combinations of the pruning learning rate, regularization penalty, and pruning steps (using Bayesian optimization) and still fail to find the setting where the -prune succeeds for the disambiguated data case (see Figure 12  ###reference_###  ###reference_### in Appendix). This strongly suggests that the \u201clinear-rule\u201d subnetwork is never formed in the language model when it doesn\u2019t have a reason to be learned \u2013 when the alternate generalization behavior (linear rule) is no longer applicable to the entire training dataset.\nWe also experiment with the opposite disambiguation setup, where we augment the training data with examples only consistent with the linear rule, and in line with our findings, we find that the Gen-prune fails to find a subnetwork with 100% generalization accuracy \u2013 no subnetwork consistent with hierarchical rule is formed (Figure 13  ###reference_###  ###reference_### in Appendix). Hence, ambiguity in the training data appears to drive the joint existence of these contrasting subnetworks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Why Do Transformer-Based LMs Generalize Hierarchically?",
            "text": "A useful tool for understanding generalization in neural networks has been \u201csimplicity bias\u201d, where the inductive bias towards simpler functions has been shown to explain why neural networks tend to generalize instead of overfitting the training data. In our case, we are not interested in comparing the learned behavior of the language models (hierarchical rule) with the overfit solution, but instead with an alternate generalization (linear rule). Can we explain through \u201csimplicity\u201d the preference of the model towards hierarchical generalization? This might sound counterintuitive, because at least on the surface it appears that the linear-rule should be simpler compared to the hierarchical rule. Our main argument is that when considering transformers trained with the language modeling objective, since the underlying data-generation process to be modeled produces each token in the full sequence, modeling the dependencies between the tokens hierarchically as opposed to learning a linear rule for each dependency, might be simpler.\n\nSuch an argument is implicit in the field of theoretical syntax where hierarchical representations are rife. In this section, we present a study showing some evidence for the simplicity of the hierarchical generalization over the linear-rule-based to explain the preference for the former by transformer LMs. We leverage the Bayesian framework of Perfors et al., utilizing generative grammars to model data-generation processes corresponding to the hierarchical and linear rules, and operationalize the notion of simplicity and goodness of fit using the posterior probabilities of the grammars given the observed data. We then show that there is a correlation between transformers\u2019 ability to generalize hierarchically and the training dataset being better explained using a hierarchical grammar than a regular one (which models the linear rule) according to the posterior criterion.\n\nWe make use of Solomonoff\u2019s theory of inductive inference, which formalizes Occam\u2019s razor \u2013 when two hypotheses explain the data equally well, the simpler one of the two is likely to be the correct one. This notion is mathematically formalized in Solomonoff\u2019s theory using a Bayesian approach by computing the posterior probabilities of the competing hypotheses and selecting the one with higher posterior.\n\nHere, denotes the likelihood of the observed data based on the hypothesis, i.e., how well explains the data. denotes the prior probability of, which in Solomonoff\u2019s theory assigned higher values for simpler hypotheses.\n\nIn other words, a more complex hypothesis will entail making more choices (\u201chigh program length\u201d) and hence have a lower prior probability. Hence, by computing the posterior, Bayesian inference balances the trade-off between the goodness of fit of a hypothesis (likelihood) and its simplicity (prior). This is closely related to \u201cBayesian Occam\u2019s razor\u201d and the Minimum Description Length principle.\n\nWe mentioned in the previous paragraph that computing the posterior over the competing hypotheses can help us choose the one which better balances the trade-off between goodness of fit and simplicity. But for our problem, what form should these hypotheses or \u201cprograms\" take to represent the linear and hierarchical rules? Note that since our training objective is language modeling, we need to consider the hypotheses that generate the entire sequence of tokens as represented in the training data. Following Perfors et al., we use probabilistic generative grammars to model the data-generation process.\n\nFor the purposes of this work, we consider probabilistic context-free grammars (PCFGs) that can be represented using a 5-tuple. Here, denotes the set of nonterminal symbols that form phrases or constituents in a sentence, denotes the set of terminal symbols or words in the sentences, denotes the set of production rules mapping phrases to sub-phrases or words, is the start symbol that represents the whole sentence, and denotes the probabilities on the production rules. Specifically, for a given non-terminal when there are multiple productions possible, assigns a probability to each possible production rule. To generate data from a PCFG, we start from the start symbol and for each terminal that arises we apply a production rule sampled according to and repeat the procedure till we are only left with terminals to generate sentences. PCFGs are typically used to model the hierarchical phrase structure of a language. We can also apply some constraints to the form of production rules in to obtain special cases (subsets) of CFGs. For example, regular grammars form a subset of CFGs, whose production rules can be put into a right-linear form: , where and are nonterminal symbols and is a terminal.\n\nWe can view the data-generation process that generates dataset using the probabilistic grammar.\n\nGiven the dataset, we can compute the posterior, where is the likelihood of the data given the probabilistic grammar, and measures the simplicity of. To get an intuitive understanding of the prior probability of a grammar and how it encodes simplicity, recall that grammars that we consider are 5-tuples. Hence choosing a grammar involves making choices like the number of nonterminal and terminal symbols, number of production rules from each nonterminal, the nature of the production rule, etc. By assigning probability distributions to"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Background",
            "text": "We make use of Solomonoff\u2019s theory of inductive inference (Solomonoff, 1964), which formalizes Occam\u2019s razor \u2013 when two hypotheses explain the data equally well, the simpler one of the two is likely to be the correct one. This notion is mathematically formalised in Solomonoff\u2019s theory using a Bayesian approach by computing the posterior probabilities of the competing hypotheses and selecting the one with higher posterior.\n\nIn other words, a more complex hypothesis will entail making more choices (\u201chigh program length\u201d) and hence have a lower prior probability. Hence, by computing the posterior , Bayesian inference balances the tradeoff between the goodness of fit of a hypothesis (likelihood) and its simplicity (prior). This is closely related to \u201cBayesian Occam\u2019s razor\u201d and the Minimum Description Length principle (Rissanen, 1978).\n\nWe mentioned in the previous paragraph that computing the posterior over the competing hypotheses can help us choose the one which better balances the trade-off between goodness of fit and simplicity. But for our problem, what form should these hypotheses or \u201cprograms\" take to represent the linear and hierarchical rules? Note that since our training objective is language modeling, we need to consider the hypotheses that generate the entire sequence of tokens as represented in the training data. Following Perfors et al. (2011), we use probabilistic generative grammars to model the data-generation process.\n\nFor the purposes of this work we consider probabilistic context-free grammars (PCFGs) that can be represented using a 5-tuple i.e., . Here,  denotes the set of nonterminal symbols that form phrases or constituents in a sentence,  denotes the set of terminal symbols or words in the sentences,  denotes the set of production rules mapping phrases to sub-phrases or words,  is the start symbol that represents the whole sentence, and  denotes the probabilities on the production rules. Specifically, for a given non-terminal when there are multiple productions possible,  assigns a probability to each possible production rule. To generate data from a PCFG, we start from the start symbol  and for each terminal that arises we apply a production rule sampled according to  and repeat the procedure till we are only left with terminals to generate sentences. PCFGs are typically used to model the hierarchical phrase structure of a language. We can also apply some constraints to the form of production rules in  to obtain special cases (subsets) of CFGs. For example, regular grammars form a subset of CFGs, whose production rules can be put into a right-linear form: , where  and  are nonterminal symbols and  is a terminal.\n\nWe can view the data-generation process that generates dataset  using the probabalistic grammar. Given the dataset , we can compute the posterior , where  is the likelihood of the data given the probabilistic grammar, and  measures the simplicity of . To get an intuitive understanding of the prior probability of a grammar and how it encodes simplicity, recall that grammars that we consider are 5-tuples. Hence choosing a grammar  involves making choices like the number of nonterminal and terminal symbols, number of production rules from each nonterminal, nature of the production rule etc. By assigning probability distributions to each of these choices, we can compute the prior probability of a given grammar. We can choose the prior distribution that favours simpler grammars, e.g., following Perfors et al. (2011), we use geometric distributions for the number of nonterminals and productions, hence a simple grammar with fewer nonterminals and productions will receive a higher probability compared to a more complex grammar. We can hence compute the posteriors for the grammars representing the competing generalization hypotheses (hierarchical and linear rule) to compare how each of these balances the tradeoff between goodness of fit and simplicity."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Method",
            "text": "We now discuss how we apply the Bayesian Occam\u2019s razor approach discussed above to explain why transformer language models generalize hierarchically. As an overview of our approach, we start by constructing a PCFG to model the hierarchical rule (denoted CFG) and a regular grammar (Reg) that generates data based on the linear rule. We then generate data using both the grammars \u2013 from CFG and from Reg. The intersection of the two datasets is comprised of ambiguous examples consistent with both the linear rule and hierarchical rule. We will use this as our training corpus. We then compute the posterior probabilities for both CFG and Reg given the data and select the one with the higher posterior. We then train a transformer language model on the data and check if it generalizes according to the selected grammar. Specifically, we simulate \u201cidealized\u201d Bayesian learning, and to the extent that the transformer\u2019s learning behavior matches the selected grammar across different scenarios, we find support for a simplicity bias in the transformer\u2019s training setup.\n\nIn what follows, we provide details about each of these steps.\n\nFor the purposes of this study, we consider the simple agreement task, as constructing hierarchical and linear grammars for its data is straightforward. Question formation and tense reinflection involve pairs of sentences, where the second sentence is a transformed version of the first. Such sentence pairs would likely require more complex frameworks like synchronous grammars, which we leave to future work.\n\nFollowing Perfors et al. (2011), we hand-construct the CFG and regular grammars. The CFG is constructed so that each verb agrees with the hierarchically connected subject, while the regular grammar is constructed to follow the linear rule (each verb in the sentence agrees with the most recent noun). The constructed grammars are assigned uniform probabilities for the production rules. For constructing CFG, we use Chomsky Normal Form for the productions: Each production rule is of the form A \u2192 BC or A \u2192 t, where B and C are nonterminals and t is a terminal symbol. Similarly, for the regular grammar Reg, we use the right-linear form of productions: Every rule is of the form A \u2192 t or A \u2192 tB.\n\nFollowing Perfors et al. (2011), we adopt a type-based approach for constructing the grammars: terminal symbols instead of being the word tokens (e.g. walrus, sing) are syntactic categories (e.g., determiner, singular-noun, intransitive-verb, etc.), so that we can use these grammars to strictly model abstract syntactic structures and not vocabulary-type frequencies, and it also gives us a manageable number of possible generations by the grammars.\n\nFor both context-free and regular grammars we generate two variants, depending on the diversity of the sentence types generated by them:\n\nSmall grammars CFG-S and Reg-S: Here we construct CFG and regular grammars that only generate 18 sentence types. Different sentence types differ by the plurality of the nouns (singular or plural), type of verbs (transitive or intransitive), and presence or absence of prepositional phrases accompanying the nouns. The resulting hand-constructed CFG-S in this case has 15 nonterminals and 21 production rules and Reg-S has 14 nonterminals and 22 production rules. Both grammars have the same 8 terminals. Out of the 18 sentence types generated by both the grammars, 12 are common between the two (ambiguous) and 6 remaining in CFG-S that are only consistent with the hierarchical rule and 6 only consistent with the linear rule in Reg-S.\n\nLarge grammars CFG-L and Reg-L: In this case, we consider larger grammars, which can generate much more diverse sentence types \u2013 180 sentence types. The major difference with the smaller grammars is that they are allowed to generate relative clauses, which can be present at both the subject and object in the sentence. CFG-L has 25 nonterminals and 38 productions, while Reg-L has 41 nonterminals and 63 productions. Note that based on these numbers alone it is evident that we need much more complex regular grammars to generate diverse sentence types. Out of the 180 sentence types generated by each grammar, 120 are common between the two, and the remaining sentence types are only generated by the specific grammars (following either hierarchical or linear rule).\n\nWe generate the sentence types from each of the 4 grammars \u2013 CFG-S, Reg-S, CFG-L, and Reg-L. As mentioned before, the training dataset is constructed by considering the sentence types common between the CFG and corresponding regular grammar. \n\nThe generalization test sets are generated by considering the sentence types that are unique to a specific grammar. For example, we can have the test set S-CFG, which contains sentence types that are unique to CFG and hence only consistent with the hierarchical rule and not the linear rule. Similarly, S-Reg consists of sentence types consistent"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Results",
            "text": "The -probabilities for all the hand-constructed grammars on the two datasets is provided in Table 2. On both datasets, the one-state grammar gets the highest prior, which is expected as it is the simplest grammar that we study. However, the one-state grammar also fits the data the worst which is indicated by the lowest -likelihood (for both datasets). The flat grammars fit both the datasets the best and have the highest -likelihood, which is also expected since a flat grammar memorizes the training data. But it can come at a cost of increased complexity, especially when the training data is diverse; and so the flat grammar has the lowest -prior on the full dataset.\nFor the high diversity dataset, we observe that the CFG best balances the tradeoff between the simplicity and goodness of fit, obtaining the highest posterior. This shows why it would be more beneficial to model this dataset using a hierarchical phrase structured grammar than a linear grammar. However, when we consider the low-diversity dataset, while the CFG still obtains a better posterior than the regular grammar, it is the one-state grammar that obtains the highest posterior out of all the grammars. This is consistent with the findings of Perfors et al. (2011), who found that for small corpora, one-state grammars often obtain higher posteriors than the context-free and regular grammars. In such cases, learning the distribution of syntactic category sequences, without abstract nonterminals, wins out on the Bayesian criterion.\nWe obtain consistent findings with some subtle differences for the grammars minimized using the Bayesian model merging algorithm, which we detail in \u00a7A.3.\nNote that choosing the prior is subjective and can influence these results. Hence, to be extra careful, we conduct a sensitivity analysis by varying the values of the geometric distribution parameter. We experiment with values for the probability distribution on the nonterminals () and number of productions (), and obtain findings consistent with those in Table 2 (see Figure 14(c) in Appendix). We also experiment with having different values of the parameter for and , and try out 49 combinations. For each of these combinations, we find that for the case, consistent with Table 2, the CFG-S always obtains a lower posterior compared to the One-State grammar. Similarly for the CFG-L and Reg-L, the findings are also consistent across all 49 combinations i.e. CFG-L always obtains a higher posterior than Reg-L.\nWe train the transformer-based LMs on the two datasets () and evaluate their generalization based on the test sets. Note that for both datasets, 50k training examples are used. Recall that the two training datasets differ in their diversity (120 types in  vs. 12 in ). We use the same experimental setup as discussed in \u00a73.2.\nIn Figure 5(a), we see for the models trained on the low-diversity dataset that the model obtains similar negative log-likelihood values on both test sets, implying that the model has no preference for generalizing according to the linear rule or the hierarchical rule. For this dataset, neither the CFG nor the regular grammar were optimal in terms of the posterior probabilities, so we observe that the transformer\u2019s learning behavior is consistent with the \u201cidealized\u201d setup above. For the models trained on the dataset, however, we see that the model learns to generalize hierarchically, with the NLL on the test set being significantly lower than that on the test set.\nWe also consider a stricter metric: all-verb generalization accuracy which is obtained by checking whether all predicted verbs (and not just the main verb) in the sentence have the correct inflection. The reason for considering this metric is that, for the agreement task, 100% main-verb generalization accuracy can also be obtained without learning the hierarchical rule and simply agreeing with the first noun in the sentence. Note that the all-verb accuracy is computed by feeding prefixes preceding each verb in the sentence and obtaining the model\u2019s predictions. We provide the results with all-verb generalization accuracy in Figure 5(c), where we show that a baseline that always selects the verb to agree with the first noun in the sentence obtains 33% accuracy according to this metric, but as we can see transformers perform substantially better than this baseline, indicating that they do not trivially learn this heuristic to obtain high main-verb accuracy.\nResults from this study indicate that when transformers-based LMs exhibit hierarchical generalization, despite being trained on ambiguous data, under the tested conditions, the hierarchical grammar not only fits the data well but is also simpler compared to the regular grammar with linear agreements. For the cases where this condition does not hold, we observe that the trained transformers exhibit no preference for hierarchical generalization.\nFirst, it may be possible to find better grammars, either context-free or regular, in terms of posterior, on our two datasets. While local search using Bayesian model merging aims to find"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The problem of question formation has been well studied in the linguistics and cognitive science literature, where it has been observed that children from a very young age can produce grammatical output consistent with the hierarchical rule. In particular, the poverty of stimulus argument (Chomsky, 1968  ###reference_b12###, 1980  ###reference_b13###) asserts that children are unlikely to observe sufficient data to rule out order rule during language acquisition, and hence knowledge of the language being phrase structured must be innately specified (linguistic nativism). On the other hand, as a critique to the nativist argument, the empiricist argument (Redington et al., 1998  ###reference_b46###, Lewis and Elman, 2001  ###reference_b23###, Reali and Christiansen, 2004  ###reference_b45###) states that distributional and statistical regularities in the data can be used to explain why children choose a hierarchical rule over an order-based rule. A critique of the empiricist argument is that it ignores the hierarchical phrase structured nature of natural language (Perfors et al., 2006  ###reference_b40###) . Works like Perfors et al. (2006  ###reference_b40###, 2011  ###reference_b41###) address this critique to empiricist argument using a Bayesian perspective on grammar induction and show that given child-directed corpus, an ideal learner can infer that a hierarchical grammar is simpler and fits the data as well as a linear grammar, without having this knowledge specified innately.\nStudying hierarchical generalization in neural networks has had its roots in empiricist arguments for language acquisition. Lewis and Elman (2001  ###reference_b23###) showed that a simple recurrent network language model trained on the CHILDES dataset (Macwhinney, 2000  ###reference_b27###) (designed for studying language of and directed to young children), would assign higher probabilities to questions constructed using the hierarchical rule than the order rule. A critique of the above work has been that it doesn\u2019t model the relation between the declarative and the question, hence failing to fully address the original poverty of stimulus argument. Frank and Mathis (2007  ###reference_b18###) trained simple recurrent networks on the transformational task (form a question from the declarative) and found some evidence of the networks generalizing hierarchically, though the performance was found to depend heavily on the auxiliaries.\nMcCoy et al. (2018  ###reference_b28###) used the setup from Frank and Mathis (2007  ###reference_b18###) and performed a more thorough study on hierarchical generalization in RNNs (trained as seq2seq models), finding that while these models exhibit limited generalization performance, using attention and training on data with additional syntactic cues can help improve the performance. McCoy et al. (2020  ###reference_b29###) studied the architectural inductive biases in RNNs influencing hierarchical generalization, and found that only using a tree-structured model would consistently lead to hierarchical bias. Petty and Frank (2021  ###reference_b43###), Mueller et al. (2022  ###reference_b33###) corroborated these findings for transformers, finding networks to generalize linearly instead of hierarchically. In contrast to these findings, recently Murty et al. (2023a  ###reference_b36###), showed that transformers, when trained for longer duration \u2013 way beyond saturating in-distribution performance \u2013 started exhibiting hierarchical generalization.\nWhile all of these works train neural network models from scratch, recently there has been work on understanding hierarchical generalization in transformer models pretrained on large amounts of naturalistic language data. Mueller and Linzen (2023  ###reference_b32###) found that pretraining encoder-decoder transformers on corpora like Wikipedia or CHILDES results in hierarchical bias in these models, though training on CHILDES  was found to be orders of magnitude more sample-efficient towards imparting this bias. Mueller et al. (2023  ###reference_b34###) studied hierarchical generalization during in-context learning in language models, finding large variance in performance across different models. They found this variance to be explained by the composition of training data and particularly found the models trained on code to generalize better.\nOne puzzle in deep learning generalization is the phenomenon of \u201cgrokking,\u201d where neural network are observed to start generalizing long after having overfit the training data (Power et al., 2022  ###reference_b44###). Numerous efforts have been made to understand grokking and why it occurs. Millidge (2023  ###reference_b31###) conjecture that for overparameterized networks the optimal set (i.e., the set of all parameter values resulting in 0 training loss) corresponds to a manifold in parameter space and stochastic gradient descent essentially acts as a random walk in this manifold, eventually hitting the parameters that generalize. The other explanations rely on simplicity bias, hypothesizing that the solutions that generalize are simpler but slower to learn (Shah, 2023  ###reference_b48###, Nanda et al., 2023  ###reference_b38###, Bhattamishra et al., 2023  ###reference_b6###, Varma et al., 2023  ###reference_b56###). Thilak et al. (2022  ###reference_b54###) explain grokking from an optimization standpoint and show it to happen at the onset of a phenomenon they call as \u201cslingshot mechanism,\u201d identified by spikes in the training loss which result in increased norm of the final-layer weights.\nLiu et al. (2022  ###reference_b25###) attempt to explain grokking through the theory of representation learning, identifying four phases during training and grokking occurring in a \"Goldilocks zone\" between two of these phases.\nMerrill et al. (2023  ###reference_b30###), identify dense and sparse subnetworks in the transformer models trained on a sparse-parity task and found the model starting to generalize as the norm of the sparse subnetwork undergoes rapid norm growth. Chen et al. (2024  ###reference_b8###) identify emergence of syntactic attention structure in transformer masked language models,  resulting from sudden drops in the loss, leading to the model subsequently acquiring different linguistic capabilities. In concurrent work, Bhaskar et al. (2024  ###reference_b5###) find, using pruning, and for BERT-based models finetuned on NLP tasks like natural language inference and paraphrase identification, the existence of subnetworks that exhibit same in-domain performance but very different out-of-distribution generalization performance. This finding is in line with our observations about the presence of subnetworks consistent with different generalization behaviors. However, due to the nature of our problem, we are further able to show what specific behaviors these subnetworks associate with, how each of these evolves over the course of training, and suggest why these subnetworks co-exist during training."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We showed that language modeling training objective can act as a source of inductive bias towards hierarchical generalization, by comparing different training objectives on five tasks and finding the LM objective to be the only one that consistently generalizes hierarchically across all of them. We also find that when the training data is consistent with two rules, we can find subnetworks in the transformer LM trained on this data corresponding to each of these rules, which continue to coexist over the course of training. Finally, we provided a Bayesian interpretation to explain why transformer LMs generalize hierarchically: hierarchical grammars that fit sufficient diverse language data as well as regular grammars are often, in a sense, simpler.\nThere are multiple directions that can be explored in the future. While our results indicate language modeling as a source of hierarchical bias, it still remains unclear why hierarchical generalization is delayed.\nFurther, Murty et al. (2023a) showed that deeper transformer LMs often fail to generalize hierarchically, which remains unexplored in our setting. While the experiments concerning our Bayesian interpretation only involved the simple agreement tasks for which it was possible to construct CFGs, in future it would be interesting to explore methods to model the simplicity and goodness of fit for competing hypotheses for tasks involving transformation of an input sentence to output sentence. In our work, we used the Bayesian interpretation to understand hierarchical generalization in transformers. However, the Bayesian interpretation has been useful to study other forms of generalization in humans as well, including (among others) word learning (Xu and Tenenbaum, 2007), concept learning (Goodman et al., 2008, Lake et al., 2015), pragmatics (Frank and Goodman, 2012), and theory of mind (Baker et al., 2011), and these capabilities have also been observed to some extent in transformer based LMs as well Patel et al. (2023), Hu et al. (2023), Shapira et al. (2023). How well these interpretations can be applied to explain such capabilities in transformers is another potentially interesting direction."
        }
    ],
    "url": "http://arxiv.org/html/2404.16367v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "3.3",
            "4",
            "4.1",
            "4.2",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.16367v2",
        "paper_title": "Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically",
        "research_background": "The paper\u2019s motivation stems from a fundamental characteristic of natural language: its hierarchical structure. The capacity of neural network models, specifically transformers, to learn and generalize this hierarchical phrase structure has been a point of considerable interest in prior research. Despite the observation that syntax trees can be inferred from models trained on large-scale language data, the specific influences of architectural choices, training paradigms, and dataset characteristics remain unexplored.\n\n**Research Problem:**\nThe central research problem the paper addresses is why and under what conditions transformers generalize hierarchically despite lacking explicit architectural biases towards hierarchical structure. The study seeks to determine how different training objectives and dataset characteristics affect a transformer's capacity for hierarchical generalization.\n\n**Relevant Prior Work:**\n1. **Prior Demarcation of Hierarchical Learning Capacity:**\n   - **RNNs and Transformers:** Previous studies (Tenney et al., 2019; Peters et al., 2018; Lin et al., 2019; Wu et al., 2020) reveal the capacity to recover syntax trees from both RNN and transformer models. However, these studies do not delve into why these models learn the hierarchical structure.\n   - **Structural Grokking:** Murty et al. (2023a) showed that with extended training, transformers can start to generalize hierarchically\u2014a phenomenon likened to \"Structural Grokking\" (Power et al., 2022).\n\n2. **Testing Hierarchical Generalization:**\n   - **Ambiguous Data and Rule Learning:** McCoy et al. (2020) and subsequent studies (Chen et al., 2017, 2018; Petty and Frank, 2021; Mueller et al., 2022) scrutinized whether neural networks, including transformers, trained on ambiguous data\u2014data consistent with both hierarchical and linear rules\u2014display a preference for hierarchical generalization.\n   \n3. **Bayesian Framework:**\n   - **Hierarchical vs. Regular Grammars:** Perfors et al. (2011) proposed a Bayesian framework for evaluating probabilistic context-free grammars (PCFGs), exploring the trade-off between goodness of fit and model simplicity.\n\nThe paper sets out to unveil the reasons behind hierarchical generalization in transformers by examining different training objectives. It employs a broad set of tasks and introduces new methods to dissect the trained networks, offering a novel application of Bayesian analysis to understand the underlying biases in language models.",
        "methodology": "## Methodology\n\nThe methodology of this paper centers on investigating how different training objectives impact the hierarchical generalization capabilities of transformer models. The study involves a systematic evaluation of several key training objectives, as described below:\n\n1. **Language Modeling Objective**: \n   - **Description**: This objective trains the model to predict each token in a sequence based on the preceding tokens by minimizing the negative log-likelihood of the sequences. The loss is computed over all tokens in the sequence.\n   - **Application**: It's typically associated with decoder-only models like GPT.\n   - **Example**: For a sequence such as \"my walrus does move the dogs that do wait.\", the model computes the cross-entropy loss over all tokens given their preceding tokens.\n\n2. **Sequence-to-Sequence (Seq2Seq) Objective**:\n   - **Description**: This objective trains the model to generate a target sequence given an input sequence. The loss terms are computed only for the target sequence.\n   - **Application**: Typically used with encoder-decoder models, this objective suits tasks with explicit input-output pairs (e.g., question formation, tense inflection).\n   - **Example**: From an example input sequence \"my walrus does move the dogs that do wait.\", the target sequence might be a transformed query or statement, such as a question.\n\n3. **Prefix Language Modeling Objective**:\n   - **Description**: Similar to the language modeling objective but applied to output text given a prefix. Unlike typical language modeling, the loss is computed only over the output text, not the prefix. The model uses a causal mask for prefix tokens, restricting the attention mechanism to be unidirectional.\n   - **Example**: The output sequence is generated from an input prefix with the loss only calculated over the output.\n\n4. **Sequence Classification Objective**:\n   - **Description**: The model maps an entire sequence to a discrete label.\n   - **Application**: For instance, in the question formation task, given an input declarative sentence, the model must predict the correct auxiliary verb to start the question.\n\n5. **Cloze Completion Objective**:\n   - **Description**: The model is given a sequence with some tokens masked and is trained to predict the masked tokens. \n   - **Example**: For question formation, tokens where auxiliaries could be present are masked, and the model predicts the correct auxiliary or an EMPTY token if no auxiliary should be present.\n\nThe study avoids using the seq2seq and cloze completion objectives for some tasks (e.g., simple agreement and passivization) due to their unsuitability (e.g., seq2seq for simple agreement) or complexity in defining a suitable masking strategy (e.g., for passivization).\n\n**Experimental Setup**:\n- **Test Datasets**: Designed such that a model following a linear rule achieves 100% in-distribution accuracy but 0% generalization accuracy. Only models consistent with hierarchical rules achieve 100% accuracy on all test sets.\n- **Comparison with RNNs**: The paper reevaluates hierarchical generalization in RNNs using both the language modeling and seq2seq objectives. Results indicate that, akin to transformers, RNNs also generalize hierarchically when trained using the language modeling objective but not with the seq2seq objective.\n\nThe paper concludes that the language modeling objective imposes a bias towards hierarchical generalization in transformers. It posits that modeling the hierarchical structure might be beneficial in terms of simplicity and data distribution, implying this could explain why language models tend to learn hierarchical rules.\n\n### Key Components:\n- **Training Objectives**: Language modeling, seq2seq, prefix language modeling, sequence classification, cloze completion.\n- **Evaluation Tasks**: Question formation, tense inflection, simple agreement (excluding seq2seq), and passivization (excluding cloze completion).\n- **Comparison with RNNs**: Used both language modeling and seq2seq objectives for a comprehensive comparison.\n- **Test Dataset Design**: Ensures clear differentiation between hierarchical and linear generalization.\n\n### Innovations:\n- **Causal Masking for Prefix Tokens**: A novel approach in the prefix language modeling objective to improve stability and performance.\n- **Task-Specific Masking in Cloze Completion**: Specifying the positions of auxiliary verbs for improved performance highlighting hierarchical generalization effects.\n- **Systematic Evaluation**: Clear differentiation and systematic comparison of various training objectives on hierarchical generalization.\n\nThis methodological framework helps elucidate when and why transformers generalize hierarchically, attributing significant influence to the choice of training objective, particularly favoring the language modeling objective.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Objectives:\nThe study investigates five different objectives and evaluates their performance across five linguistic tasks.\n\n#### Datasets:\nThe specific tasks (assumed to be linguistic tasks given the context) evaluated include:\n1. Simple agreement\n2. Tense reinflection\n3. Passivization\n4. Question formation\n5. Another unspecified task \n\n#### Baselines:\nThe primary baselines include:\n- Language Modeling (LM) objective\n- Seq2seq objective\n- Prefix LM objective\n\n#### Evaluation Metrics:\nThe main evaluation metrics used are:\n- In-distribution test set accuracy\n- Generalization accuracy\n\n### Main Experimental Results\n\n#### Accuracy Findings:\n- All objectives achieve close to 100% accuracy on in-distribution test sets, with the exception of sequence classification which performs slightly worse on simple agreement.\n- There is significant variation in generalization accuracy.\n\n#### Generalization Performance:\n- The language modeling objective consistently achieves high generalization accuracy across all tasks.\n- Seq2seq and prefix LM objectives perform well in certain specific tasks (tense reinflection and passivization, respectively) but struggle with others.\n\n##### Observations:\n- The language modeling objective involves computing loss over all tokens, including input tokens. This may prevent the model from learning trivial transformations and helps in hierarchical generalization.\n- Training curves indicate a delay in achieving high generalization performance for LMs, although they eventually reach high levels.\n- The language modeling objective's generalization performance is not perfect, with around 75% average performance in some tasks like question formation and tense reinflection. However, variability exists as some seeds achieve 100% accuracy.\n\n#### Language Comparison:\n- Transformer LMs perform better on German question formation compared to English. The German dataset's richer hierarchical structure likely aids this performance difference.\n\n#### Additional Experiments:\n- RNNs (specifically 2-layer GRUs) were also tested with LM and seq2seq objectives on the question formation task.\n- RNNs generalize hierarchically when trained using the LM objective, mirroring the results in transformers, but fail to generalize with the seq2seq objective.\n\n### Conclusion:\n- The experiments suggest that the language modeling objective acts as a source of inductive bias toward hierarchical generalization in both transformers and RNNs.\n- This offers an explanation for differing results in past studies on hierarchical generalization, depending on the training objective used."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To understand how the choice of training objective influences hierarchical generalization in transformer models.",
            "experiment_process": "The study involves training transformer models on multiple training objectives, including language modeling, sequence-to-sequence modeling (seq2seq), prefix language modeling, sequence classification, and cloze completion. The experiments use the question formation task and the declarative-question pair dataset. The evaluation metric is accuracy, measured for in-distribution performance and hierarchical generalization. GRU models are also trained with language modeling and seq2seq objectives for comparison.",
            "result_discussion": "The results indicate that transformers and RNNs trained with the language modeling objective show hierarchical generalization, while those trained with seq2seq do not. This suggests that the language modeling objective imposes a bias towards hierarchical generalization, possibly due to the combined simplicity of hierarchical rules and the data over linear rules.",
            "ablation_id": "2404.16367v2.No1"
        },
        {
            "research_objective": "To investigate the existence and evolution of subnetworks within transformers corresponding to different generalization behaviors (linear vs. hierarchical).",
            "experiment_process": "The study uses pruning on transformer models trained on ambiguous training datasets to discover subnetworks implementing hierarchical and linear rules. Pruning is done using different strategies: Train-prune (using ambiguous dataset), Gen-prune (using generalization set), and \u03bb-prune (minimizing loss on training data and maximizing loss on generalization data). Evaluation is based on the subnetwork's performance in in-distribution and generalization tasks.",
            "result_discussion": "The experiments show that during training, subnetworks for both hierarchical and linear rules are discoverable. When trained on disambiguated datasets (either hierarchical or linear only), only the corresponding rule subnetworks are formed. This indicates that ambiguous training data drives the existence of subnetworks with contrasting generalization behaviors.",
            "ablation_id": "2404.16367v2.No2"
        },
        {
            "research_objective": "To identify attention heads corresponding to different generalization rules in transformers.",
            "experiment_process": "Pruning introduces learnable gates for each attention head in trained transformers. Gates are trained to minimize negative log-likelihood while ensuring sparsity through an L0 penalty. Three pruning strategies are used on the question formation task dataset: Train-prune (original dataset), Gen-prune (generalization data), and \u03bb-prune (maximize loss on generalization data). The checkpoints for the model are saved and pruned every 1000 training steps.",
            "result_discussion": "The results indicate the existence of subnetworks for linear and hierarchical rules, with their formation observable at different training stages. Ambiguous training data is necessary for the formation of both rule subnetworks. When provided with disambiguated data, only the corresponding rule subnetworks appear, confirming that the ambiguity drives multiple generalization behaviors.",
            "ablation_id": "2404.16367v2.No3"
        }
    ]
}