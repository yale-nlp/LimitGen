{
    "title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
    "abstract": "In this paper, we introduce the Interpretable Cross-Examination Technique (ICE-T), a novel approach that leverages structured multi-prompt techniques with Large Language Models (LLMs) to improve classification performance over zero-shot and few-shot methods. In domains where interpretability is crucial, such as medicine and law, standard models often fall short due to their \u201cblack-box\u201d nature. ICE-T addresses these limitations by using a series of generated prompts that allow an LLM to approach the problem from multiple directions. The responses from the LLM are then converted into numerical feature vectors and processed by a traditional classifier. This method not only maintains high interpretability but also allows for smaller, less capable models to achieve or exceed the performance of larger, more advanced models under zero-shot conditions. We demonstrate the effectiveness of ICE-T across a diverse set of data sources, including medical records and legal documents, consistently surpassing the zero-shot baseline in terms of classification metrics such as F1 scores. Our results indicate that ICE-T can be used for improving both the performance and transparency of AI applications in complex decision-making environments.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "There are numerous prompting strategies to achieve good performance using generative Large Language Models (LLMs). Take, for instance, a binary classification problem, where a system should classify the given text into one of two classes. A typical zero-shot approach is to prompt the model with a given text and carefully designed question, that will yield an appropriate answer. There are also multiple variations on that approach that include \u201cchain-of-thought\u201d prompting Wei et al. (2022c  ###reference_b35###); Wang et al. (2022a  ###reference_b30###); Kojima et al. (2022  ###reference_b14###), \u201cfew-shot learning\u201d Schick and Sch\u00fctze (2022  ###reference_b25###); Gu et al. (2021  ###reference_b12###), \u201cself-instruct\u201d Wang et al. (2022b  ###reference_b31###); Yang et al. (2024  ###reference_b39###) prompting and \u201citerative refinement\u201d Wu et al. (2022a  ###reference_b36###); Trautmann (2023  ###reference_b29###). These tactics are used to get a better sense of the model\u2019s underlying reasoning or to surpass the performance achieved by the standard zero-shot method.\n\nThese options are usually used in cases where using highly specialized fine-tuned LLMs is not a viable option because it is often of utmost importance to understand how decisions are made. This is especially true in fields like medicine, where decisions based on opaque, \u201cblack-box\u201d models are usually not acceptable. Although zero-shot or few-shot prompting methods can potentially offer explanations for their reasoning, these explanations are often unstructured and lack quantifiability. On the other hand, while finely tuned models may achieve superior performance, they frequently struggle to articulate the rationale behind their outputs unless explicitly trained for this purpose, a process that is labor-intensive. Additionally, outputs from such models may also suffer from the lack of structured reasoning representation.\n\nIn cases where using \u201cblack-box\u201d models is not practical, and where interpretability is important, users have the option to develop a structured reasoning process by asking several questions to achieve a desired output. There are three main problems that arise with this approach: 1) Non-experts have little chance to develop a good set of questions and rules that ensure optimal model performance; 2) Designing an accurate rule set becomes challenging since individual instances may not perfectly align with all desired criteria, resulting in a mix of positive and negative responses to different rules; and 3) The potential combinations of these rules can become overwhelmingly numerous, making it impractical to hard-code every possible scenario.\n\nIn the paper, we propose a method that attempts to overcome the three issues outlined above. We refer to the method as the Interpretable Cross-Examination Technique, or ICE-T for brevity. Our approach exhibits strong performance, consistently surpasses the benchmark set by a zero-shot baseline, and also offers a high level of interpretability. The core concept here is that rather than using a single prompt to get a response from an LLM and making a decision based on that single output, we engage the LLM with multiple prompts, covering various questions. We then combine the responses from all these prompts and use the outputs to make a decision. Compared to other methods that are based on multi-prompting, our approach is fundamentally different in the way the decisions are made. Specifically, we take the responses from the LLM, convert them into numerical values to create a feature vector, and then input this vector into a traditional classifier to determine the final outcome. Since, in this process we create a low-dimensional feature vector with highly informative features, we can then use relatively small classifiers to make a decision.\n\nWe established an experimental setup where we tested our Interpretable Cross-Examination Technique on a simple binary classification task. We tested our approach on a set of multiple datasets split on 17 different tasks and we show that:\nICE-T consistently outperforms the zero-shot baseline model in most classification metrics\nUsing a smaller model with ICE-T we can achieve comparable or better results than using larger and essentially more capable model with zero-shot approach\nFurthermore, this approach can be highly interpretable, allowing experts to clearly understand the rationale behind the decision-making process111Degree of interpretability may vary depending on the machine learning method selected for the final classification task. The decision on which method to employ should be guided by a consideration of the trade-offs between interpretability and performance tailored to the unique demands of each task. Additionally, tools commonly used for tabular machine learning can be employed to enhance the understanding of the data. While this technique is specifically evaluated for binary classification within this paper, its applicability potentially extends across a broad spectrum of scenarios."
        },
        {
            "section_id": "1.1",
            "parent_section_id": "1",
            "section_name": "Motivation",
            "text": "The ICE-T method was initially conceived at InferLink in a commercial consulting project, where we needed to address a complex challenge in biomedical text classification. The project\u2019s goals were to develop a model that could perform at a level comparable to human experts, provide interpretable results, and allow for detection of potentially mislabelled data. Initially, conventional \u201cblack-box\u201d models such as fine-tuned BERT-based ones underperformed, as well as zero-shot or few-shot learning methods using LLMs. This led to the creation of the ICE-T, which improved the performance of classification, while gaining interpretability and allowing for the correction of labeling errors. ICE-T was used initially for the purpose of classifying biomedical data for a specific commercial purpose. While the specifics of this initial task and data remain confidential, we have conducted further testing on additional publicly available datasets and decided to make the method publicly accessible."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Our proposed solution addresses three core aspects of using large language models for inference: prompting, in-context learning, and interpretability. It is built on top of the ever-growing body of knowledge that comes from those domains."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Prompting techniques",
            "text": "Numerous techniques have been developed to improve the fundamental zero-shot approach. Among these, the \u201cchain-of-thought\u201d (CoT) prompting is particularly notable. This method is used to prompt the model to systematically articulate its reasoning process in a step-by-step manner before reaching a conclusion. Research has shown that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks Wei et al. (2022b  ###reference_b34###, c  ###reference_b35###); Wang et al. (2022a  ###reference_b30###). Even simple tweaks such as adding \u201cLet\u2019s think step by step\u201d before each answer can significantly outperform zero-shot LLM performances on diverse benchmark reasoning tasks Kojima et al. (2022  ###reference_b14###); Nye et al. (2021  ###reference_b20###). Such generated chains that prompt language models to break down their reasoning into steps often cause errors in inference time. To reduce these errors, some researchers employ a method known as automatic Chain of Thought prompting. This technique, which generates demonstrable examples, has proven to be more effective than earlier, simpler CoT approaches Zhang et al. (2022b  ###reference_b43###). Lastly, \u201citerative refinement\u201d involves repeatedly prompting the model with slightly altered versions of the original text or question, honing in on a more accurate or nuanced answer through successive iterations. Each of these strategies can be tailored to the specific needs of a task, leveraging the model\u2019s capabilities in different ways to achieve optimal performance.\nSeveral approaches involve using multiple prompts in a chain, where the output of one step becomes the input for the next, thus aggregating the gains per step Wu et al. (2022a  ###reference_b36###), or decomposing complex tasks into smaller, manageable components Trautmann (2023  ###reference_b29###). Additionally, \u201cself-instruct\u201d Wang et al. (2022b  ###reference_b31###); Yang et al. (2024  ###reference_b39###) prompting can be used, where the model generates its own instructions or clarifications based on the initial prompt, attempting to refine or better understand the task before generating a response. Another set of approaches uses multiple models or multiple instances of the same model to improve the performance. The additionally trained models, called \u201cverifiers\u201d are used to judge the correctness of model completions. At the inference time, the verifiers would select the most likely answer Cobbe et al. (2021  ###reference_b8###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "In-context learning",
            "text": "Large Language Models possess the remarkable ability for in-context learning (ICL), in which they acquire knowledge from a few contextual examples either during inference or during training. Numerous studies have shown that through ICL, LLMs can effectively handle a diverse set of complex tasks Wei et al. (2022a  ###reference_b33###). ICL offers several advantages, notably its ease in including human knowledge into LLMs by using various demonstrations and templates Liu et al. (2021  ###reference_b16###); Wu et al. (2022b  ###reference_b37###). Furthermore, unlike traditional supervised training methods, ICL operates without the need for additional training, significantly lowering the computational costs when using models to solve new tasks Dong et al. (2022  ###reference_b10###).\nOne of the most recognizable techniques for in-context learning is \u201cfew-shot learning\u201d Schick and Sch\u00fctze (2022  ###reference_b25###, 2020  ###reference_b24###); Gu et al. (2021  ###reference_b12###); Perez et al. (2021  ###reference_b21###) during inference222A different approach to achieving few-shot learning can occur also during the training phase or during fine-tuning.. Using this approach, the model is provided with a few examples of text and their corresponding labels or desired outputs within the prompt itself. This method teaches the model the context of the decision-making process, improving its accuracy on similar tasks.\nMultiple other studies contributed to refining the ICL methods, focusing on automation, ordering, and selection of prompts. Zhou et al. (2022) introduced the Automatic Prompt Engineer (APE), which automates the generation of instructional prompts, significantly reducing manual effort and improving scalability Zhou et al. (2022  ###reference_b45###). Simultaneously, Lu et al. (2021) came up with the method to optimize the ordering of prompts. They employed entropy statistics to evaluate and identify the most effective prompt sequences Lu et al. (2021  ###reference_b17###). Rubin et al. (2021) and Liu et al. (2021) both contribute to this area but from different perspectives. Rubin et al. (2021) developed a method for efficiently retrieving prompts using annotated data, streamlining the selection process Rubin et al. (2021  ###reference_b23###). On the other hand, Liu et al. (2021) explored strategic selection methods that go beyond random sampling to leverage the few-shot capabilities of LLMs, aiming to enhance the model\u2019s performance through example selection Liu et al. (2021  ###reference_b16###). Adding to the discussion on selection strategies, Zhang et al. (2022) approached example selection as a sequential decision problem. They proposed using a reinforcement learning algorithm to discover policies that improve the generalizability of language models Zhang et al. (2022a  ###reference_b42###). This perspective introduces a dynamic element to the selection process, aligning with the strategies discussed by Rubin and Liu but through an adaptive, policy-driven approach."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Model interpretability",
            "text": "The challenge of interpreting complex decision processes made by LLMs has hindered their application in critical areas like medicine, where there are significant concerns about regulation Goodman and Flaxman (2017  ###reference_b11###) and safety Amodei et al. (2016  ###reference_b2###). Furthermore, this difficulty in understanding the workings of large language models (LLMs) and similar neural network models has restricted their use in domains like science and data analysis Kasneci et al. (2023  ###reference_b13###). In such fields, the primary objective is often to derive a reliable interpretation rather than merely to implement an LLM Singh et al. (2024  ###reference_b26###).\nThe expression of uncertainty in language models is crucial for reliable LLM utilization, yet it remains a challenging area due to inherent overconfidence in model responses. Xiong et al. (2023) and Zhou et al. (2024) both highlight the overconfidence issue in LLMs. Xiong et al. question whether LLMs can express their uncertainty, observing a tendency in LLMs to mimic human patterns of expressing confidence Xiong et al. (2023  ###reference_b38###). Simlarly, Zhou et al. note that while LLMs can be prompted to express confidence levels, they remain generally overconfident and unable to convey uncertainties effectively, also when providing incorrect responses Zhou et al. (2024  ###reference_b44###). Ye et al. (2022) add that even when LLMs generate explanations, these may not accurately reflect the model\u2019s predictions nor be factually grounded in the input, particularly in tasks requiring extractive explanations Ye and Durrett (2022  ###reference_b40###). However, all the research mentioned above note that these flawed explanations can still serve a purpose, offering a means to verify LLM predictions post-hoc.\nIt is worth mentioning feature attribution methods, used beyond the LLM realm in multiple deep-learning applications. Feature attributions in machine learning provide a relevance score to each input feature, reflecting its impact on the model\u2019s output. This methodology helps in understanding how and why certain decisions or predictions are made by a model.\nThe approaches developed by Lundberg et al. (2017) and Sundararajan et al. (2017) both delve into this topic but offer distinct methodologies and theoretical foundations. Lundberg et al. Lundberg and Lee (2017  ###reference_b18###) introduced SHAP (SHapley Additive exPlanations), which provides a unified framework for interpreting predictions. SHAP assigns an importance value to each feature for a specific prediction, leveraging the concept of Shapley values from cooperative game theory. In contrast, Sundararajan et al. Sundararajan et al. (2017  ###reference_b28###) developed Integrated Gradients, another method focusing on the attribution of predictions to input features of deep networks. Unlike SHAP, which uses Shapley values, Integrated Gradients relies on the integration of gradients along the path from a chosen baseline to the actual input. Complementing these approaches, Ribeiro et al. (2016) proposed LIME (Local Interpretable Model-agnostic Explanations), which aims to make the predictions of any classifier understandable and reliable by learning an interpretable model localized around the prediction Ribeiro et al. (2016  ###reference_b22###).\nAnother popular method for understanding neural-network representations is probing. Conneau et al. (2018) initially introduced multiple probing tasks designed to capture simple linguistic features of sentences, setting a foundation for understanding how neural networks encode linguistic properties Conneau et al. (2018  ###reference_b9###).\nClark et al. (2019) focused primarily on the behavior of attention heads within transformers. They observed that these heads often broadly attend across entire sentences, and that attention patterns in the same layer tend to exhibit similar behaviors. Crucially, their research links specific attention heads to traditional linguistic concepts like syntax and coreference, suggesting a direct relationship between the model\u2019s attention mechanisms and linguistic structures Clark et al. (2019  ###reference_b7###), although there is an ongoing debate on the explanatory power of attention in neural network Bibal et al. (2022  ###reference_b4###). Unlike Clark et al., who examine what the model attends to, Morris et al. Morris et al. (2023  ###reference_b19###) explore how information is preserved and can be retrieved from embeddings, offering insights into the reversibility and fidelity of the encoding process. Their method involves a multi-step process that iteratively corrects and re-embeds text, demonstrating the ability to recover most of the original text inputs exactly. Belrose et al. (2023) introduced a technique called causal basis extraction, which aims to identify influential features within neural networks Belrose et al. (2023  ###reference_b3###). This method stands out by focusing on the causality within network decisions.\nIn summary, while chain-of-thought prompting can generate errors during inference, requiring complex corrective approaches, in-context learning techniques also face challenges in prompt optimization and efficient retrieval. Furthermore, interpreting large language models remains problematic, exacerbated by models\u2019 tendency to exhibit overconfidence and provide unreliable or unverifiable explanations."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Training the ICE-T system consists of the following steps:\nGenerating questions: the process begins by generating a series of questions designed to prompt the Large Language Model (LLM);\nPrompting the LLM: Previously generated questions are used to prompt the LLM and collect the yes/no answers;\nVerbalizing the answers: for each instance within the training dataset, responses to prompts are collected and converted into numerical form, thus creating a low-dimensional feature vector for each instance;\nTraining a classifier: Previously obtained vectors, together with their respective labels, are then used to train a classifier\nThe Inference stage mirrors the training process: the LLM is presented with the same collection of questions. The responses obtained are numerically encoded in the same manner before being processed by the classifier that was trained during the Training stage. Training and inference process is illustrated in Figure 1  ###reference_###. Each step is explained below."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Generating questions",
            "text": "To train and use the system, we need to create multiple questions that more closely reflect the core principles behind the initial yes/no question. Those questions should be crafted in a way to uncover some additional details about the problem.\nConsider a use case where an expert is building a classifier to determine eligibility for medical trials based on patient data. In such a scenario, the classifier needs to assess various clinical inclusion criteria, which are typically derived from patient medical records. One of these criteria could be the patient\u2019s language proficiency, for instance, whether they speak English. A naive formulation of this question may be to present the question to the LLM in a prompt like the following:\nwhere the __RECORDS__ represents the appended textual medical records. Determining the answer to that question, which we call the \u201cprimary\u201d question, may not be easy given the medical records under consideration, requiring an understanding of somewhat subtle indicators that show if a patient actually speaks English. It is highly unlikely that the medical records will directly state the answer to that question.\nHowever, a series of \u201csecondary\u201d questions such as:\nmay allow the model to answer directly based on the information already contained in the documents presented to it, while also serving as strong indicators for the primary question. Secondary questions are also yes/no questions.\nCreating the secondary questions can be done in multiple ways, such as writing the questions manually using the expert knowledge or using the LLM to automatically generate a fixed size set of questions that might be useful in answering the original question. Starting from the primary question  we generate  additional questions, creating a set of all questions , where . This process is shown in Figure 1  ###reference_### with a red box, illustrating the creation of the questions and using them during the training and inference process. The same set  of questions is used for both training and inference.\nThe number  of secondary questions is decided based on factors such as: number of training samples, availability of the expert knowledge and the level of interpretability needed for a specific task. Our prior small-scale experiments have shown that secondary questions crafted by experts generally lead to improved performance compared to those generated by LLMs. However, in the experiments reported here, we chose a straightforward and reproducible approach where we exclusively use secondary questions created by an LLM. This choice was made to minimize human bias and showcase the method\u2019s effectiveness in scenarios where expert input is unavailable. The exact prompts used for creating secondary questions in our experiments are described in Section 5  ###reference_###.\n###figure_1###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Prompting LLM",
            "text": "The LLMs are prompted in two occasions. First, they are prompted to obtain the set of secondary questions , as described in Section 3.1  ###reference_###. Second, for each document, we prompt the LLM with the document and corresponding secondary questions. Then, for each question  the output  of the LLM is collected, creating a set of outputs for each document. The textual outputs are then assigned a numerical value and transformed into a feature vector , through the verbalization process explained in Section 3.3  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Verbalizing the answers",
            "text": "The output of the LLM in response to each prompt is limited to one of three possible values: Yes, No, or Unknown, depending on the answer to the question posed in the prompt. These responses are subsequently assigned numerical values for analysis, with \u201cYes\u201d translating to 1, \u201cNo\u201d to 0, and \u201cUnknown\u201d to 0.5."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Training a classifier",
            "text": "To train a classifier, we use a set  of low-dimensional numerical vectors, where  and corresponding labels , where each vector  has a corresponding binary label . Vectors  are obtained from the training textual data after prompting LLM to generate  outputs that are then assigned a numerical value. A classifier is then trained using a 5-fold cross-validation process and grid search for the best parameters. A choice of a specific classification algorithm will depend on the size of training data, values distribution and desired performance on a specific classification metric."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Data",
            "text": "This work utilizes data compiled from a range of sources, attempting to include a variety of domains and document lengths. The data used in the experiments described here spans the fields of medicine, law, climate science, and politics. It also includes documents of varying sizes, from brief tweets to extensive legal documents and detailed medical records."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Clinical trials",
            "text": "This dataset comes from Track 1 of the 2018 National NLP Clinical Challenges (n2c2) shared tasks333https://n2c2.dbmi.hms.harvard.edu/. It is designed to help in identifying patients within a corpus of longitudinal medical records who either meet or do not meet predefined selection criteria. These criteria are used for determining a patient\u2019s eligibility for inclusion in clinical trials. Stubbs et al. (2019  ###reference_b27###). The data consists of annotated American English clinical narratives for 288 patients according to whether they met a set of specific criteria. There are 13 criteria in total, and they include: DRUG-ABUSE: Drug abuse, current or past; ALCOHOL-ABUSE: Current alcohol use over weekly recommended limits; ENGLISH: Patient must speak English; MAKES-DECISIONS: Patient must make their own medical decisions; ABDOMINAL: History of intra-abdominal surgery, small or large intestine resection, or small bowel obstruction; MAJOR-DIABETES: Major diabetes-related complication; ADVANCED-CAD: Advanced cardiovascular disease (CAD); MI-6MOS: MI in the past 6 months; KETO-1YR: Diagnosis of ketoacidosis in the past year; DIETSUPP-2MOS: Taken a dietary supplement (excluding vitamin D) in the past 2 months; ASP-FOR-MI: Use of aspirin to prevent MI; HBA1C: Any hemoglobin A1c (HbA1c) value between 6.5% and 9.5%; and CREATININE: Serum creatinine  upper limit of normal. For every medical record, each criterion can have one of two potential values: \u201cmet\u201d or \u201cnot met.\u201d The value based on whether an individual has fulfilled a particular criterion. Data is split 70/30 on training and test sets respectively. Training test contains 202 medical record while the test set contains 86 records. Note that for some criteria, the ratio between positive and negative class is highly imbalanced. In our analysis we excluded KETO-1YR criterion as it contains no positive samples in the test set and only one positive sample in the training set.444Our methodology employs classifiers that are trained based on data distributions. As a result, we consistently achieve peak classification metrics, which is not a realistic performance, as the minority class is absent from the test dataset."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Catalonia Independence Corpus",
            "text": "This dataset contains a corpus in Spanish that consist of annotated Twitter messages for automatic stance detection Zotova et al. (2020  ###reference_b46###). It encompasses data collected over a 12-day span in February and March 2019, from tweets originating in Barcelona. Originally, each tweet is categorized into one of three classes: AGAINST, FAVOR, and NEUTRAL. These classes represent the user\u2019s stance towards the topic of Catalonia\u2019s independence. For the purpose of binary classification and to facilitate more effective comparisons with other datasets, we have omitted the NEUTRAL class, focusing exclusively on the AGAINST and FAVOR categories."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Climate Detection Corpus",
            "text": "This dataset contains climate-related paragraphs extracted from financial disclosures by companies. The text has been collected from corporate annual reports and sustainability reports. The paragraphs from those reports are hand-selected and then annotated as yes (climate-related) or no (not climate-related) Webersinke et al. (2021  ###reference_b32###)."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Medical health advice data",
            "text": "This dataset comprises a collection of sentences related to the medical domain, each accompanied by a label indicating whether the sentence offers medical advice. The labels can be one of three values: \u201cstrong advice\u201d, \u201cweak advice\u201d, or \u201cno advice\u201d. Yu et al. (2019  ###reference_b41###) For the purpose of binary classification task we combined \u201cstrong advice\u201d and \u201cweak advice\u201d into a single class: \u201cadvice\u201d. The dataset includes approximately 8,000 samples, which have been divided into training and test datasets following the 80/20 rule."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "The European Court of Human Rights (ECtHR) Data",
            "text": "The European Court of Human Rights (ECtHR) hears allegations that a state has breached human rights provisions of the European Convention of Human Rights (ECHR) Chalkidis et al. (2019  ###reference_b6###). The dataset for each case includes a series of facts in form of paragraphs extracted from the case description. Additionally, each case is associated with specific articles of the European Convention on Human Rights (ECHR) that may have been violated. In many cases, multiple articles are violated at the same time. To make this a binary categorization problem, we adopted a binary labeling system. Cases are marked with a \u201c1\u201d if any ECHR articles are violated, and a \u201c0\u201d if no violations are detected."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "UNFAIR-ToS Dataset",
            "text": "The UNFAIR-ToS dataset contains 50 relevant on-line consumer contracts, i.e. Terms of Service (ToS) from on-line platforms (e.g., YouTube, Ebay, Facebook, etc.). Each agreement has been annotated at the sentence level to identify various types of potentially unfair clauses, which could infringe upon user rights under European consumer law. This dataset categorizes unfair terms into eight distinct groups: Arbitration, Unilateral Change, Content Removal, Jurisdiction, Choice of Law, Limitation of Liability, Unilateral Termination, and Contract by Using Lippi et al. (2019  ###reference_b15###). To transform the analysis into a binary classification problem, we re-labelled each sentence as either \u201cunfair\u201d if it contains any type of the identified unfair terms, or \u201cnot unfair\u2019 if it does not fall into these categories."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We performed the experiments on a set of binary classification tasks on datasets from various domains, as described in the previous section. To generate the secondary questions, we employed a large language model. Prompting it only once, we obtained a set of secondary questions, which we accepted as provided, without any selection or modification. More specifically, we used the following prompt for creating all secondary questions: where is the number of additional questions we want to generate and primary_question is the primary question used to obtain the main information from the document. Note that in all our experiments. That means that for each document we use one primary and four secondary questions that are treated equally when prompting the LLM. Thus, for each document we collect five answers from the LLM that are then verbalized (assigned a numerical value) in the next step. To generate the secondary questions for all our experiments we use OpenAI\u2019s gpt-4-0125-preview model. To collect the answers in our experiments we use two generations of OpenAI\u2019s models: gpt-4-0125-preview Achiam et al. (2023 ###reference_b1###) and gpt-3.5-turbo-0125 Brown et al. (2020 ###reference_b5###). To choose the best classifier, we train several different classification algorithms. These include K-Nearest Neighbors, Decision Trees, Random Forest, Gaussian Naive Bayes, Multinomial Naive Bayes, AdaBoost, and XGBoost. We use a 5-fold cross-validation on our training data and also perform a grid search to fine-tune the parameters for each classifier. After training, we test them on a hold-out test set and choose the classifier that gives us the highest performance metric. Note that one can also adjust the training process to optimize for a specific performance metric if needed for a particular application. To perform these experiments, we used the scikit-learn library in Python. Additionally, we conducted a sensitivity analysis to enhance our understanding of the relationship between the number of features and model performance. This analysis helps determine the requisite number of secondary questions to attain a desired performance metric. For each dataset, we started by creating secondary questions and using the gpt-3.5-turbo-0125 model to generate responses for each sample. The outputs from the large language model were then transformed into 10-dimensional feature vectors. Subsequently, we constructed a series of simple Random Forest classifiers, starting with a single feature and incrementally adding more features up to ten. Given the random selection of features for classification, we repeated the experiment 100 times. We computed the performance metric for each iteration and dataset. The findings are detailed in Section 6 ###reference_### and illustrated in Figure 3 ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "The results of the classification experiments are summarized in Table 1. We can see that across all datasets, the ICE-T method consistently surpasses the zero-shot approach in performance for a given language models. Specifically, using the GPT-3.5 model, the average F1 score for the zero-shot approach is 0.683, but it increases to 0.845 with the ICE-T method. A similar trend is observed with the larger GPT-4 model, where the average F1 score improves from 0.7 using the zero-shot approach to 0.892 with the ICE-T technique. This improvement is not constant across the datasets, as we can see significant variations in performance and in improvements across different tasks.\n\nThe upper portion of Table 1 showcases the findings from the clinical trial dataset, as detailed in Section 4. The dataset\u2019s contents remain consistent across all sub-tasks within this clinical trial dataset, though each sub-task involves a distinct classification criterion based on 12 different criteria. In some sub-tasks, substantial improvements were observed over the zero-shot method. For instance, in the task CREATININE (involving serum creatinine levels exceeding the upper normal limit), the zero-shot method achieved an F1 score of 0.349. In contrast, the ICE-T technique utilizing the same large language model significantly improved this score to 0.721. Similarly, for the task ENGLISH (determining if a patient speaks English) using the larger GPT-4 model, the greatest increase noted exceeded 0.733 points, with the zero-shot approach at an F1 score of 0.233 and the ICE-T technique improving it to 0.966.\n\nAnalysis of tasks outside the clinical trial dataset revealed varied results, dependent on the specific domain. The task assessing \u201cCatalonia independence\u201d presented a notable challenge in the zero-shot setup for both models, barely achieving an F1 score above 0.5, with no significant improvements noted with the ICE-T technique. The task related to the European Court of Human Rights (ECtHR) already exhibited high baseline scores in the zero-shot setting, achieving 0.853 with GPT-3.5 and 0.861 with GPT-4. The application of the ICE-T technique yielded minimal improvement, with both models achieving an F1 score of 0.873. A similar scenario was observed with the Health advice dataset, where enhancements were negligible.\n\nHowever, the UNFAIR-ToS task demonstrated significant improvement using the ICE-T approach, particularly with the GPT-3.5 model. Here, the F1 score saw a dramatic increase from 0.335 to 0.887. Furthermore, our analysis reveals that the ICE-T technique, when applied to a smaller model, can surpass or match the performance of a larger model that uses the zero-shot approach. In our experiments, we assessed the F1 score of classification tasks executed by GPT-4 in a zero-shot setting against those performed by GPT-3.5 using the ICE-T technique across various datasets. In nearly all cases, except for two, the ICE-T-enhanced GPT-3.5 either outperformed or equaled the larger GPT-4 model on identical tasks. These findings are depicted in Figure 2.\n\nWe observed a minor variation in performance across different task groups. By categorizing clinical trial tasks into one group and other tasks into another, we observed a comparable average performance improvement when comparing the zero-shot to the ICE-T approach, as detailed in Table 4 in Appendix B. This consistency underscores the versatility of the ICE-T method across various domains and tasks."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Our study introduces the Interpretable Cross-Examination Technique (ICE-T), a novel prompting method that integrates LLM responses with traditional classification algorithms to improve the performance on binary classification tasks. This technique addresses key limitations in zero-shot and few-shot learning by employing a structured, multi-prompt approach that transforms qualitative data into quantifiable metrics, thus allowing a small, traditional classifier to effectively make decisions. Our results confirm that ICE-T consistently surpasses zero-shot baselines across multiple datasets and metrics, particularly in scenarios where model interpretability is crucial. This prompting strategy also demonstrates the potential for fully automated, high-performing AI systems accessible even to non-experts. The ICE-T method has demonstrated its capability to not only enhance performance over the zero-shot approach but also to do so with smaller models that might not perform as well in a zero-shot configuration. For example, the improvement in the CREATININE and ENGLISH tasks within clinical trials data underscores the method\u2019s ability to handle domain-specific challenges that require nuanced understanding, which zero-shot configurations typically struggle with."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Implications for Model Interpretability",
            "text": "A major advantage of the ICE-T approach is its interpretability. By generating a feature vector based on direct responses to structured prompts, experts can trace back the decision-making process, understanding which factors contributed most significantly to the model\u2019s classification. This is particularly valuable in fields like medicine and law, where decision rationale is as important as accuracy. The ability to dissect and validate each step of the model\u2019s reasoning aligns with the growing demand for transparency in AI applications, ensuring that decisions made by AI systems can be audited and trusted.\n\nMoreover, ICE-T is particularly valuable in situations where fine-tuning models is not viable. Fine-tuned models often suffer from a significant drawback: they lack transparency and become \u201cblack boxes,\u201d making their decision-making processes obscure. This lack of interpretability is particularly problematic in regulated sectors such as healthcare, law, and finance, where it\u2019s imperative to comprehend the basis of each decision. ICE-T overcomes these issues by employing a methodology that remains clear and interpretable, avoiding the opaqueness associated with fine-tuned systems."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Limitations and Future Work",
            "text": "Despite its strengths, the ICE-T method has some limitations. The quality of the output heavily relies on the initial set of questions generated for the model to answer. Poorly formulated questions or those that fail to capture the necessary subtleties of the task can limit the effectiveness of this technique. Moreover, the reliance on numerical scoring of textual answers might oversimplify complex answers. This can lead to a loss of nuance, especially when answers are confined to binary outputs.\n\nFuture research could explore more sophisticated methods for question generation, perhaps incorporating active learning where the system identifies and prioritizes questions that would most improve its understanding and performance. Additionally, exploring different methods of encoding responses into feature vectors could further enhance the model\u2019s accuracy and sensitivity to nuances in text.\n\nExpanding the scope of ICE-T to tackle problems beyond binary classification could also prove beneficial. Applying this method to multi-class classification tasks or even regression problems could test the adaptability and scalability of the approach, potentially making it more applicable across a wider array of domains. This expansion could lead to significant advancements in the field of machine learning where interpretability and accuracy are crucial.\n\nIn conclusion, the ICE-T method presents a promising avenue for enhancing the performance and interpretability of LLMs in binary classification tasks and beyond. By bridging the gap between traditional machine learning techniques and modern LLM capabilities, this approach offers a valuable tool for applications demanding high accuracy and clear reasoning in decision-making processes. Further refinements and adaptations of this technique could significantly impact the deployment of AI in critical sectors, enhancing both the reliability and accountability of automated systems."
        }
    ],
    "url": "http://arxiv.org/html/2405.06703v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "1.1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6",
            "5",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.4",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2405.06703v1",
        "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
        "research_background": "The paper \"Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance\" presents a new method aimed at enhancing the performance and interpretability of generative Large Language Models (LLMs) for binary classification tasks.\n\n### Motivation\nThe motivation behind this research is the challenge of achieving high accuracy and interpretability in LLM-based decision-making, particularly in sensitive fields like medicine where understanding the reasoning behind model decisions is critical. Traditional approaches using zero-shot or few-shot prompting methods offer some interpretability but tend to be unstructured and lack quantifiability. Meanwhile, highly specialized fine-tuned LLMs, though potentially more accurate, often struggle to explain their outputs unless explicitly trained for interpretability, which is a labor-intensive process. Additionally, developing effective prompting strategies poses significant challenges for non-expert users and can become impractical due to the complexity and number of potential rule combinations.\n\n### Research Problem\nThe core problem this paper addresses is the trade-off between achieving high model performance and maintaining a high level of interpretability in LLM-based classification tasks. Specifically, the paper aims to overcome the three critical issues: the difficulty for non-experts in developing effective prompting strategies, the challenge in designing an accurate rule set that covers all instances adequately, and the impracticality of hard-coding numerous potential rule combinations.\n\n### Related Work\nThe paper builds on several prior works in the field of LLM prompting strategies:\n- **Zero-shot prompting**: A method where the model is prompted with a specific text and question without additional training data.\n- **Chain-of-thought prompting**: Techniques that encourage models to generate detailed intermediate reasoning steps, enhancing interpretability and performance (Wei et al. (2022c), Wang et al. (2022a), Kojima et al. (2022)).\n- **Few-shot learning**: Methods involving a small number of example inputs to guide the model's task performance (Schick and Sch\u00fctze (2022), Gu et al. (2021)).\n- **Self-instruct prompting**: Techniques where the model generates instructional prompts itself to improve understanding (Wang et al. (2022b), Yang et al. (2024)).\n- **Iterative refinement**: An approach where the model's outputs are iteratively refined (Wu et al. (2022a), Trautmann (2023)).\n\n### Proposed Method\nThe Interpretable Cross-Examination Technique (ICE-T) seeks to improve LLM performance and interpretability by using multiple prompts to generate diverse responses, converting these responses into numerical values creating a feature vector, and subsequently using traditional classifiers to make the final decision. This process results in a low-dimensional feature vector composed of highly informative features, which can then be processed by relatively small classifiers, making the decision-making process more transparent and interpretable.\n\n### Evaluation and Conclusion\nThe authors demonstrate the efficacy of ICE-T through experimental evaluation on a binary classification task across multiple datasets split into 17 different tasks. The results show that ICE-T consistently outperforms a zero-shot baseline model in various classification metrics and can achieve comparable or superior results to larger models using the zero-shot approach. The proposed method also allows experts to clearly understand the rationale behind the decision-making process, thus addressing the initial motivation and research problem. The technique is mainly evaluated for binary classifications in this paper, but its potential applicability spans a broad range of scenarios, offering a promising avenue for future research and applications.",
        "methodology": "Certainly! The Interpretable Cross-Examination Technique (ICE-T) proposed in the paper is designed to enhance the performance of Large Language Models (LLMs) by utilizing highly informative features. Here's a detailed description of the process:\n\n### Methodology\n\nThe **training of the ICE-T system** includes the following crucial steps:\n\n1. **Generating Questions**:\n   - The process begins by creating a series of questions aimed at eliciting informative responses from the LLM.\n   \n2. **Prompting the LLM**:\n   - The previously generated questions are used to prompt the LLM, and the model's responses are collected, specifically looking for yes/no answers.\n\n3. **Verbalizing the Answers**:\n   - For each instance in the training dataset, the responses to the prompts are gathered.\n   - These responses are translated into numerical form, resulting in a low-dimensional feature vector for each instance.\n\n4. **Training a Classifier**:\n   - The feature vectors obtained, along with their corresponding labels, are used to train a separate classifier.\n\n### Inference Stage\n\n- During the inference stage, the process mirrors the steps taken during training:\n  - The same set of questions is presented to the LLM.\n  - The LLM's responses are collected and numerically encoded in the same manner as in the training phase.\n  - The numerically encoded responses are then processed by the classifier trained during the training stage.\n\n### Summary\n\nThe ICE-T system leverages the capability of LLMs to provide informative responses to specific prompts. These responses are then encoded into feature vectors and used to train a classifier, which ultimately enhances the classification performance. The methodology relies on both the LLM's ability to respond to structured prompts and the classifier's ability to learn from the numerically encoded responses.\n\n### Key Innovations\n\n- **Use of Highly Informative Features**: By carefully crafting questions that elicit yes/no answers from the LLM, the system extracts highly relevant features, ensuring that the responses are both interpretable and informative.\n- **Numerical Encoding of Responses**: Converting LLM responses into low-dimensional numerical vectors facilitates the training of a secondary classifier, optimizing the overall performance.\n- **Two-Stage Process**: The methodology\u2019s two-stage process of first prompting the LLM and then using the responses for classifier training provides a robust mechanism for utilizing pre-trained LLM capabilities in a structured framework.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Datasets:** The experiments are conducted using data from diverse sources to ensure a wide coverage of domains and document lengths. Specifically, the datasets span a variety of fields including medicine, law, climate science, and politics. The document sizes within these datasets range from brief tweets to extensive legal documents and detailed medical records.\n\n**Baselines:** The paper compares the performance of the proposed Interpretable Cross-Examination Technique (ICE-T) against several strong baseline models. These baselines likely include state-of-the-art large language models (LLMs) that are commonly used for tasks within these domains. However, specific names or types of baseline models are not mentioned in the provided text.\n\n**Evaluation Metrics:** The experiments are evaluated using metrics that are standard for the tasks at hand. In similar studies, common evaluation metrics often include accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC) for classification tasks. For document retrieval or ranking tasks, metrics such as Mean Reciprocal Rank (MRR) or Normalized Discounted Cumulative Gain (NDCG) might be used. Specific metrics used in this study are not outlined in the provided segment but are generally aligned with the type of data and tasks addressed.\n\n**Main Experimental Results:** The paper reports that the proposed ICE-T approach results in significantly better performance across the variety of domains tested. This indicates that utilizing highly informative features, as ICE-T does, can effectively enhance the performance of large language models in diverse and complex datasets.\n\nBy including datasets from multiple fields and document types and lengths, the study demonstrates the robustness and versatility of ICE-T, making it a promising technique for boosting LLM performance in real-world applications where data diversity is prevalent."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate and understand the performance of classifiers when trained with secondary questions generated by an LLM versus manually crafted by experts.",
            "experiment_process": "The experiment involves using a dataset for binary classification tasks. The primary question and secondary questions are both generated using OpenAI\u2019s gpt-4-0125-preview model. For each document, one primary question and four secondary questions are used, and answers from the LLM are transformed into numerical vectors. The classifiers tested include K-Nearest Neighbors, Decision Trees, Random Forest, Gaussian Naive Bayes, Multinomial Naive Bayes, AdaBoost, and XGBoost. A 5-fold cross-validation and grid search are conducted to fine-tune the classifiers. The classifier with the highest Micro F1 score on a hold-out test set is selected. The scikit-learn library in Python is used for these experiments.",
            "result_discussion": "The findings demonstrate that using secondary questions, even those generated by the LLM without any manual selection or modification, can enhance classification performance. Sensitivity analysis shows that increasing the number of features (secondary questions) generally improves Micro F1 scores, which is particularly beneficial in datasets with underrepresented classes.",
            "ablation_id": "2405.06703v1.No1"
        },
        {
            "research_objective": "To determine the optimal number of secondary questions necessary to maximise classifier performance in terms of Micro F1 score.",
            "experiment_process": "For each dataset, a series of secondary questions up to 10 are generated using the gpt-3.5-turbo-0125 model. Responses for each sample are converted into 10-dimensional feature vectors. Simple Random Forest classifiers are constructed starting with a single feature and incrementally adding up to ten features. The process is repeated 100 times with random selection of features for classification. The Micro F1 score is calculated for each iteration and dataset using the standard formula.",
            "result_discussion": "The sensitivity analysis reveals that adding more features consistently improves the Micro F1 score. This indicates that having a larger number of targeted secondary questions positively impacts the model's performance, helping to overcome the challenges posed by datasets with underrepresented classes.",
            "ablation_id": "2405.06703v1.No2"
        }
    ]
}