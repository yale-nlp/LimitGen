{
    "title": "Word Order and World Knowledge",
    "abstract": "Word order is an important concept in natural language, and in this work, we study how word order affects the induction of world knowledge from raw text using language models. We use word analogies to probe for such knowledge. Specifically, in addition to the natural word order, we first respectively extract texts of six fixed word orders from five languages and then pretrain the language models on these texts. Finally, we analyze the experimental results of the fixed word orders on word analogies and show that i) certain fixed word orders consistently outperform or underperform others, though the specifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in pre-trained language models, and the natural word order typically yields mediocre results. The source code will be made publicly available at https://github.com/lshowway/probing_by_analogy.\n\nKeywords:\u2009language model, fixed word order, world knowledge",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "The distribution of dominant word orders is generally explained by communicative efficiency, e.g., dependency and information locality Hahn and Xu (2022), but what explains the fact that most languages exhibit some variation across different word orders? The standard theory is that different word orders coexist because of the influence of multiple languages and that there is pressure from language acquisition for fixing word order Lupyan and Christiansen (2002). Word order transfer between neighboring languages of course does not explain the current distribution of dominant word orders: the first language was probably structured using the Subject-Verb-Object syntax (SVO) Gell-Mann and Ruhlen (2011), but how did alternative word orders arise out of that? In this paper, we explore a novel hypothesis about the role of within-language word order variation:\n\nThe Wov2Lex Hypothesis\n\nWord order variation facilitates the acquisition of lexical semantics. The Wov2Lex hypothesis has considerable support from human language acquisition studies. Input variability is known to facilitate both first and second language acquisition Sinkeviciute et al. (2019). Aguilar et al. (2018), for example, found that object variability facilitates new word learning. Raviv et al. (2022) synthesized the above work, concluding that: \u201cAn effective way of improving generalization is to expose learners to more variable (and thus often more representative) input. More variability tends to make initial learning more challenging but eventually leads to more general and robust performance.\u201d\n\nWe hereby present a series of experiments to check whether the Wov2Lex hypothesis is still hold in pre-trained language models. Our experiments are inspired by Sinha et al. (2021) who pre-trained language models (LMs) on shuffled texts and compared their performance with models trained on the original one (natural word order). Sinha et al. (2021) initially argued this did not lead to significant performance drops, but in subsequent work, Abdou et al. (2022) identified limitations in Sinha et al. (2021), and they were able to show that corpus perturbation leads to much lower performance.\n\nIn this paper, we move a step from \u201cnatural word order\u201d and \u201cshuffled word order\u201d toward \u201cfixed word order\u201d. We first probe their LMs for world knowledge. We do so through word analogies, relying on the experimental protocols and datasets provided by Garneau et al. (2021). Our results confirm previous findings: the performance of language models trained with shuffled word order exhibits only slight variations. We then pre-train our own LMs, following the protocol of Sinha et al. (2021) and Abdou et al. (2022), but on data with \u201cfixed\u201d word order rather than original (natural) or shuffled text.\n\nWe fix the word order by reordering the (subject, object, verb) items. We find that, surprisingly, fixing word order could lead to both performance increases or drops in different languages, while natural word order keeps more mediocre across all tested languages and relations."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "Languages permitting word order variations typically use this variation to encode different pragmatic distinctions, such as referentiality, discourse anchoring, etc. Slioussar (2011  ###reference_b17###). Others have argued that word order variation arises from optimization of processing (avoidance of ambiguity) and grammaticalization Levshina (2019  ###reference_b10###). Input variability is known to facilitate learning, including lexical knowledge Aguilar et al. (2018  ###reference_b2###); Raviv et al. (2022  ###reference_b14###), but we are, to the best of our knowledge, the first to study the impact of word order variability on the induction of world knowledge.\n\nSinha et al. (2021  ###reference_b15###); Hessel and Schofield (2021  ###reference_b9###) pre-trained several LMs from scratch with shuffled text, relying on various shuffling strategies, such as bi-gram order permutation. They concluded from their experiments that word order information matters little for the downstream performance Wang et al. (2018  ###reference_b19###).\n\nHowever, Abdou et al. (2022  ###reference_b1###) tried to replicate their experiment and found a limitation in their work. The shuffling strategy they used did not affect the positional encodings of input tokens, and when shuffling at the sub-word level, the downstream performance dropped significantly.\n\nWorld knowledge Clark et al. (2007  ###reference_b4###) refers to the information about the real world that individuals accumulate over time. It encompasses facts and concepts about the broader world. Word analogy Ul\u010dar et al. (2020  ###reference_b18###) is a comparison between two things, typically on the basis of their relations. For example, \u201cBeijing is to China as Washington is to the USA\u201d. Word analogies can test our understanding of the world. Since language models are trained on a vast amount of text, we adopt a word analogy dataset that is extracted from Wikidata to test the relations between word order and world knowledge."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Experiments",
            "text": "To investigate the effects of word order on world knowledge, we conduct tests on language models that are trained on corpora with different word orders, including the original (natural) order, shuffled order, and fixed word order. For models trained on corpora of natural texts, we use the original BERT, mBERT Devlin et al. (2019  ###reference_b5###), and RoBERTa Liu et al. (2019  ###reference_b11###) LMs. They have 12 layers with 12 attention heads, and RoBERTa is an extended version of BERT with more pre-training corpus and training steps, i.e., pre-trained on a corpus of 160GB for 500K steps. We refer to these models trained on natural word order as \u201cnatural models\u201d.\n\nTo assess the significance of word order in pre-training, Sinha et al. (2021  ###reference_b15###); Abdou et al. (2022  ###reference_b1###) pre-trained RoBERTa models using corpora that have been shuffled at various levels, such as corpus-level and unigram- or bigram-level, or at different phases such as before or after tokenization. These models are referred to as \u201cshuffled models\u201d. In this paper, we also examine their word analogies.\n\nSpecifically, we test models shuf.n1, shuf.n2, shuf.n3, shuf.n4, shuf.cps, and nopos released by Sinha et al. (2021  ###reference_b15###), which are pre-trained from scratch using the Toronto Book Corpus and English Wikipedia (16GB) on a full-scale RoBERTa base model. The models were trained for 100,000 steps over 72 hours using 64 GPUs. shuf.n1 to shuf.n4 refer to LMs pre-trained on the shuffled text at -gram level (please refer to Sinha et al. (2021  ###reference_b15###) for details). The shuf.cps refers to LMs pre-trained on an entire reshuffled corpus (i.e., each word is sampled according to its frequency). The nopos refers to the LMs being pre-trained from scratch without positional embeddings since positional embeddings encode the sequence order information.\n\nWe further pre-train RoBERTa models using corpora adhering to fixed word orders. While there are multiple approaches to defining fixed word orders\u2014ranging from alphabetical to word frequency-based, or even based on parts of speech\u2014our primary objective is to investigate whether natural word order yields superior representations in language models compared to fixed word order. Consequently, our primary interest lies in examining word orders that occur naturally in languages, such as SVO (Subject-Verb-Object), OVS (Object-Verb-Subject), and the like.\n\nSpecifically, we first extract the dependency tree of Wikipedia text using SpaCy, where named entities and noun chunks are merged (if it was implemented in SpaCy). Instead of traversing the dependency parse tree to avoid complex regular expressions, we use a data-driven method motivated by Word2Vec Mikolov et al. (2013  ###reference_b13###) to get -grams. We then check each -gram and extract the subject, object, and verb item, rearrange the positions of the subject, object, and verb within the extracted -grams to obtain -grams of different word orders, including SVO, SOV, VOS, VSO, OSV, and OVS. Finally, these reordered -grams are concatenated to create pre-training corpora, which are identical except for the word order. The extracted -grams, without any ordering modifications, are referred to as the natural word order corpus, i.e., fixed.ntr in Table 3  ###reference_###.\n\nTo obtain enough training text segments, we set =5 for English, German, French, Spanish, and =10 for Polish as the merge of named entities and noun chunks is not implemented for Polish in SpaCy. If there are multiple subjects/objects/verbs in the -gram, only the first one is used to avoid duplicates. In total, we obtain pre-training corpora for English, German, French, Spanish, and Polish, consisting of 2 million, 2 million, 0.5 million, 1 million, and 1.5 million -gram, respectively. Note that SVO refers to a kind of word order, and SVO refers to the corresponding trained LM.\n\nTo pre-train the language models, we set the training batch size to 16, and the accumulation step to 8. The learning rate is set to 1e-4, and we use the AdamW optimizer with a linear learning rate scheduler, with a warmup step of 1% of the total training step. We use 4 GPUs, and train"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Analysis of Natural/Shuffled Word Order",
            "text": "Since the relation between word order and word analogy has not been explored, here, we re-test the performance of existing natural and shuffled LMs by conducting experiments on WiQueen dataset. \n\nAs shown in Figure 1, compared with the natural word order shuf.ntr, nopos and R-rand result in a substantial decrease in performance, which is easy to understand. However, when comparing shuf.ntr with shuf.n1, n4 and shuf.cps, same as existing results, we also find that the permutation of word order leads to either a decrease or an increase in performance with slight differences. Several hypotheses attempt to elucidate the results. For instance, it might be that the tasks tested do not require word order information, or that the evaluated language models do not rely heavily on word order information. Another one suggests that the shuffling methods employed might not destroy essential word order information. However, these hypotheses have yet to be widely validated."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Analysis of Fixed Word Order",
            "text": "Building upon the work in Sinha et al. (2021  ###reference_b15###); Abdou et al. (2022  ###reference_b1###), we extend the exploration of word order from \u201cnatural\u201d and \u201cshuffled\u201d to \u201cfixed\u201d word order. We pre-train RoBERTa from scratch using pre-training corpora that have been reordered to different word orders. Across these corpora, all input items are the same except the positions of the subject, verb, and object. With these experiments, we try to answer the following questions:  \nQ1: Does a particular word order exhibit superiority or inferiority to the others?  \nQ2: Does the use of hybrid word orders (i.e., natural word order) confer any advantages over the use of a single word order?  \nQ3: Does the question Q1 hold consistent across different languages?  \nIn answer to Q1, the results are positive. There is a clear distinction between the best and worst fixed word orders across all examined languages. Besides, the SVO performs the best in English and French but worst in Spanish. On the other hand, VOS performs the worst in English, German and French. For the tested five languages, the first three (English, German, French) demonstrates similar trends, and show completely different trends with Spanish or Polish.  \nIn response to question Q2, the findings are in the negative. Our analysis indicates that integrating a diverse range of word orders (i.e., natural word order), doesn\u2019t offer marked enhancements over individual, fixed word orders. However, it\u2019s important to highlight that, relative to other word sequences, the natural word order consistently exhibits more resilience across languages, maintaining a middle-ground performance.  \nIn relation to question Q3, the answer leans toward the negative. The data suggests the presence of both superior and inferior word orders among the seven considered (six pre-defined and one natural). However, this hierarchy varies across languages. Specifically, German and French demonstrate tendencies more aligned with English than do Polish and Spanish. Consequently, these findings do not entirely corroborate the Wov2Lex hypothesis.  \nThese empirical findings underscore the significance of word order, challenging prior results detailed in Section 3.1  ###reference_### (i.e., the permutation of word order lead a marginal decrease or increase in performance). While currently it\u2019s challenging to provide definitive explanations for these observations, we hypothesize that language models and linguistics may possess different ways to comprehend and process natural language."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Other Findings",
            "text": "We also have additional empirical findings that are not related to word order. Referring to Figures 1  ###reference_### and 2  ###reference_###, with detailed data provided in Tables 1  ###reference_### and 3  ###reference_###. The performance may be expected to be RoBERTa  BERT  fixed.ntr, where  signifies superior performance.\nBecause RoBERTa and BERT have the same model architecture111Although BERT also uses next sentence prediction as a pre-training task, it is claimed that it matters little., and compared with the original BERT, RoBERTa is trained with more training corpus and longer training time, and fixed.ntr is trained with a smaller corpus and fewer steps.\nContrary to expectations, the results reveal a sequence of fixed.ntr  BERT  RoBERTa. Illustrating with English data, the performance metrics are .\nThis discrepancy might arise because the Wikipedia corpus alone suffices for the WiQueen dataset, given its derivation from Wikidata. Introducing additional corpora, like the Toronto Book Corpus, could potentially be detrimental to the data, leading to issues such as catastrophic forgetting."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Word Order on Different Relations",
            "text": "In order to examine the impacts of \u201cfixed\u201d word order models on analogy relations, such as the relation \u201ccapital\u201d in the example \u201cBeijing is to China as Washington is to the United States\u201d, we evaluate the eight most frequently occurring relations.\nAs shown in Figure 3  ###reference_###, the natural word order demonstrates superior performance on certain relations such as \u201cfollows\u201d, \u201cfollowed by\u201d, and \u201ccapital of\u201d even though language models have lower scores on these relations. However, it performs poorly on the relations \u201cP1001\u201d and \u201cP159\u201d. Meanwhile, SVO is better at relations such as \u201ccapital\u201d and \u201ccountry\u201d, and the VOS has slightly better performance on the relation \u201cP1001\u201d.\nOn the other hand, models that start with S such as SOV and SVO, exhibit the poorest performance on the \u201cname after\u201d relation.\nThe results indicates that models trained on different single word orders (SVO, SOV, VOS, VSO, OVS, OSV) and mixture word order (natural) exhibit varying abilities on different relations.\n###figure_3###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "The objective of this paper is to investigate the influence of word order on world knowledge, including verifying whether Wov2Lex hypothesis is supported in pre-trained language models. To accomplish this, we pre-trained language models from scratch on corpora with six fixed word orders, and then assessed their performance on an analogy dataset. The findings reveal that natural word order does not consistently outperform the six tested fixed word orders. This suggests that the Wov2Lex hypothesis is not entirely supported by our empirical data. We hypothesize that this disparity arises from the distinct processing methods used in linguistics versus language models."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Appendix",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T1.1\" style=\"width:455.2pt;height:221.2pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-11.2pt,5.4pt) scale(0.953166929854929,0.953166929854929) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A1.T1.1.1\">\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.1.1\">Language</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"6\" id=\"A1.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.2.1\">Original</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"6\" id=\"A1.T1.1.1.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.1.3.1\">Shuffled</span></td>\n<td class=\"ltx_td\" id=\"A1.T1.1.1.1.4\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.1.1\">F-rand</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T1.1.1.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.2.1\">Fasttext</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.3.1\">BERT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T1.1.1.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.4.1\">mBERT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.5\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.5.1\">RoBERTa</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.6\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.6.1\">R-rand</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.7\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.7.1\">shuf.ntr</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.8\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.8.1\">shuf.n1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.9\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.9.1\">shuf.n2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.10\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.10.1\">shuf.n3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.11\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.11.1\">shuf.n4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.12\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.12.1\">shuf.cps</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.2.13\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.1.1.2.13.1\">nopos</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.3.1\">Danish</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T1.1.1.3.3\">13.02</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.4\">16.24</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T1.1.1.3.5\">17.51</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.6\">12.19</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.7\">2.64</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.8\">14.21</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.9\">13.29</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.10\">12.02</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.11\">11.11</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.12\">13.77</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.13\">15.08</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T1.1.1.3.14\">6.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.1\">German</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.4.3\">10.67</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.4\">17.36</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.4.5\">22.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.6\">13.44</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.7\">3.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.8\">13.58</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.9\">13.83</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.10\">13.21</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.11\">13.94</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.12\">15.78</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.13\">14.58</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.4.14\">11.26</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.1\">English</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.5.3\">10.96</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.4\">28.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.5.5\">29.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.6\">21.16</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.7\">1.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.8\">19.31</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.9\">20.62</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.10\">20.21</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.11\">20.64</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.12\">23.99</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.13\">16.57</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.5.14\">15.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.1\">Spanish</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.6.3\">8.65</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.4\">19.21</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.6.5\">24.13</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.6\">13.31</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.7\">1.29</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.8\">14.5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.9\">14.91</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.10\">13.21</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.11\">14.91</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.12\">16.8</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.13\">15.97</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.6.14\">9.93</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.1\">Finnish</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.7.3\">12.59</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.4\">14.91</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.7.5\">16.88</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.6\">11.44</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.7\">2.55</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.8\">13.54</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.9\">12.77</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.10\">12.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.11\">11.44</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.12\">12.42</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.13\">12.71</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.7.14\">7.56</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.1\">French</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.8.3\">6.72</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.4\">18.44</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.8.5\">21.18</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.6\">12.38</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.7\">4.92</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.8\">13.42</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.9\">13.29</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.10\">13.23</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.11\">13.29</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.12\">15.26</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.13\">15.97</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.8.14\">10.09</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.1\">Italian</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.9.3\">7.13</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.4\">17.15</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.9.5\">22.58</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.6\">12.46</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.7\">4.28</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.8\">12.96</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.9\">13.54</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.10\">11.84</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.11\">13.06</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.12\">15.18</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.13\">14.43</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.9.14\">9.03</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.1\">Dutch</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.10.3\">10.56</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.4\">17.01</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.10.5\">21.68</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.6\">12.88</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.7\">1.77</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.8\">14.5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.9\">14.75</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.10\">12.92</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.11\">12.69</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.12\">16.49</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.13\">15.85</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.10.14\">9.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.1\">Polish</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.11.3\">11.07</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.4\">13.54</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.11.5\">17.07</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.6\">10.97</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.7\">4.71</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.8\">11.53</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.9\">11.51</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.10\">10.28</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.11\">10.43</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.12\">12.81</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.13\">11.78</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.11.14\">6.96</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.1\">Portuguese</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.12.3\">8.28</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.4\">15.99</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.12.5\">17.69</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.6\">12.07</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.7\">1.14</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.8\">12.42</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.9\">12.25</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.10\">10.43</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.11\">11.69</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.12\">13.37</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.13\">12.88</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.12.14\">7.06</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.1\">Swedish</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.13.3\">9.80</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.4\">15.51</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T1.1.1.13.5\">17.67</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.6\">11.96</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.7\">3.24</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.8\">12.81</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.9\">12.4</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.10\">12.15</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.11\">11.84</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.12\">13.89</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.13\">12.69</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T1.1.1.13.14\">7.71</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T1.1.1.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T1.1.1.14.1.1\">Averages</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.2\">0.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T1.1.1.14.3\">9.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.4\">17.58</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T1.1.1.14.5\">20.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.6\">13.11</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.7\">2.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.8\">13.89</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.9\">13.92</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.10\">12.86</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.11\">13.19</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.12\">15.43</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.13\">14.41</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T1.1.1.14.14\">9.23</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Evaluation of natural, shuffled word order on the WiQueen dataset. <span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.4.1\">F-rand</span> refers to <span class=\"ltx_text ltx_font_italic\" id=\"A1.T1.5.2\">Fasttext</span> with random initialization.</figcaption>\n</figure>",
            "capture": "Table 1: Evaluation of natural, shuffled word order on the WiQueen dataset. F-rand refers to Fasttext with random initialization."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T2.1\" style=\"width:455.2pt;height:204pt;vertical-align:-0.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-12.8pt,5.7pt) scale(0.946943519591195,0.946943519591195) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A1.T2.1.1\">\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.2.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.2.1.1\">Relation</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"4\" id=\"A1.T2.1.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.2.2.1\">Original</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"7\" id=\"A1.T2.1.1.2.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.2.3.1\">Shuffled</span></td>\n<td class=\"ltx_td\" id=\"A1.T2.1.1.2.4\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.2.1\">BERT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.3.1\">mBERT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.4\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.4.1\">RoBERTa</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.5\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.5.1\">R-rand</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.6\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.6.1\">shuf.ntr</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.7\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.7.1\">shuf.n1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.8\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.8.1\">shuf.n2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.9\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.9.1\">shuf.n3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.10\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.10.1\">shuf.n4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.11\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.11.1\">shuf.cps</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.12\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T2.1.1.1.12.1\">nopos</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.3.1\">followed by</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.2\">34.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T2.1.1.3.3\">32.91</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.4\">27.68</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.5\">3.11</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.6\">31.64</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.7\">27.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.8\">27.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.9\">26.27</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.10\">31.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.11\">29.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.1.3.12\">3.28</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.3.13\">24.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.1\">follows</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.2\">35.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.4.3\">34.49</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.4\">26.81</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.5\">10.58</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.6\">27.39</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.7\">26.52</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.8\">26.67</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.9\">26.52</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.10\">31.01</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.11\">29.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.4.12\">1.68</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.4.13\">22.32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.1\">P190</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.2\">2.88</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.5.3\">2.88</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.4\">2.52</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.5\">0.36</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.6\">3.06</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.7\">2.52</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.8\">3.24</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.9\">1.98</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.10\">2.88</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.11\">2.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.5.12\">0.5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.5.13\">2.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.1\">capital</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.2\">35.53</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.6.3\">38.58</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.4\">25.89</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.5\">2.79</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.6\">23.1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.7\">29.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.8\">26.65</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.9\">27.92</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.10\">26.9</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.11\">22.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.6.12\">3.91</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.6.13\">21.83</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.1\">capital of</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.2\">30.62</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.7.3\">32.3</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.4\">22.75</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.5\">5.06</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.6\">18.82</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.7\">25.56</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.8\">21.35</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.9\">25.56</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.10\">27.53</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.11\">20.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.7.12\">5.22</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.7.13\">17.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.1\">P131</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.2\">34.75</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.8.3\">39.41</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.4\">34.75</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.5\">1.27</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.6\">24.58</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.7\">30.51</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.8\">32.2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.9\">30.51</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.10\">33.05</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.11\">22.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.8.12\">5.93</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.8.13\">24.15</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.1\">country</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.2\">51.85</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.9.3\">55.09</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.4\">33.33</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.5\">2.31</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.6\">29.63</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.7\">31.94</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.8\">35.65</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.9\">36.57</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.10\">37.04</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.11\">16.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.9.12\">7.22</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.9.13\">16.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.1\">name after</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.2\">46.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.10.3\">52.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.4\">37.6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.5\">0.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.6\">23.2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.7\">30.4</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.8\">36.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.9\">36.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.10\">39.2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.11\">25.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.10.12\">10.24</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.10.13\">28.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.1\">P1001</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.2\">57.26</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.11.3\">58.97</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.4\">28.21</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.5\">4.27</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.6\">17.09</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.7\">30.77</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.8\">27.35</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.9\">28.21</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.10\">37.61</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.11\">9.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.11.12\">12.65</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.11.13\">17.09</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.1\">headquarters location</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.2\">45.26</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"A1.T2.1.1.12.3\">54.74</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.4\">35.79</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.5\">4.21</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.6\">32.63</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.7\">45.26</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.8\">37.89</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.9\">40.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.10\">43.16</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.11\">25.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T2.1.1.12.12\">8.63</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.1.12.13\">33.68</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.13.1.1\">Averages</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.2\">37.49</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T2.1.1.13.3\">40.14</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.4\">27.53</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.5\">3.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.6\">23.11</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.7\">28.06</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.8\">27.44</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.9\">27.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.10\">30.99</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.11\">20.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.12\">5.93</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T2.1.1.13.13\">20.8</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Evaluation on different relations on WiQueen dataset. \u201cP190\u201d is the relation \u201ctwinned administrative body\u201d, \u201cP131\u201d is the relation \u201clocated in the administrative territorial entity\u201d, \u201cP1001\u201d is the relation \u201capplies to jurisdiction\u201d.  refers to the average differences between <span class=\"ltx_text ltx_font_typewriter ltx_font_italic\" id=\"A1.T2.10.1\">shuf.n1</span>  <span class=\"ltx_text ltx_font_typewriter ltx_font_italic\" id=\"A1.T2.11.2\">shuf.n4</span>, <span class=\"ltx_text ltx_font_typewriter ltx_font_italic\" id=\"A1.T2.12.3\">shuf.cps</span> and <span class=\"ltx_text ltx_font_typewriter ltx_font_italic\" id=\"A1.T2.13.4\">shuf.ntr</span>.</figcaption>\n</figure>",
            "capture": "Table 2: Evaluation on different relations on WiQueen dataset. \u201cP190\u201d is the relation \u201ctwinned administrative body\u201d, \u201cP131\u201d is the relation \u201clocated in the administrative territorial entity\u201d, \u201cP1001\u201d is the relation \u201capplies to jurisdiction\u201d.  refers to the average differences between shuf.n1  shuf.n4, shuf.cps and shuf.ntr."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T3.1\" style=\"width:433.6pt;height:225.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(105.8pt,-55.0pt) scale(1.95226071655228,1.95226071655228) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A1.T3.1.1\">\n<tr class=\"ltx_tr\" id=\"A1.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T3.1.1.1.1.1\">Language</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"7\" id=\"A1.T3.1.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T3.1.1.1.2.1\">Fixed</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T3.1.1.2.1.1\">SVO</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T3.1.1.2.2.1\">SOV</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T3.1.1.2.3.1\">OSV</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T3.1.1.2.4.1\">OVS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.2.5\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T3.1.1.2.5.1\">VSO</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.2.6\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T3.1.1.2.6.1\">VOS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.2.7\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T3.1.1.2.7.1\">fixed.ntr</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.3.1\">English</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.3.2\">30.76</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.3.3\">30.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.3.4\">30.09</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.3.5\">29.99</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.3.6\">30.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.3.7\">29.97</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.3.8\">30.13</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.4.1\">German</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.4.2\">23.9</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.4.3\">24.15</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.4.4\">22.9</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.4.5\">22.96</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.4.6\">24.38</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.4.7\">21.78</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.4.8\">24.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.5.1\">French</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.5.2\">23.28</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.5.3\">22.82</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.5.4\">23.18</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.5.5\">22.85</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.5.6\">22.93</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.5.7\">20.85</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.5.8\">22.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.6.1\">Polish</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.6.2\">18.54</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.6.3\">17.90</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.6.4\">19.08</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.6.5\">18.13</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.6.6\">18.25</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.6.7\">18.27</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.1.6.8\">18.27</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T3.1.1.7.1\">Spanish</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T3.1.1.7.2\">24.07</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T3.1.1.7.3\">24.19</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T3.1.1.7.4\">24.27</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T3.1.1.7.5\">24.27</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T3.1.1.7.6\">24.52</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T3.1.1.7.7\">24.09</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T3.1.1.7.8\">24.27</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Evaluation of fixed word orders on the WiQueen dataset. The best performance is reported.</figcaption>\n</figure>",
            "capture": "Table 3: Evaluation of fixed word orders on the WiQueen dataset. The best performance is reported."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"A1.T4.1\" style=\"width:433.6pt;height:312.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(79.4pt,-57.2pt) scale(1.57759270016717,1.57759270016717) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"A1.T4.1.1\">\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.1.1.1.1.1\">Relation</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"7\" id=\"A1.T4.1.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.1.1.1.2.1\">Fixed</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T4.1.1.2.1.1\">SVO</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T4.1.1.2.2.1\">VOS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T4.1.1.2.3.1\">OSV</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T4.1.1.2.4.1\">SOV</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.2.5\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T4.1.1.2.5.1\">VSO</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.2.6\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T4.1.1.2.6.1\">OVS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.2.7\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.T4.1.1.2.7.1\">fixed.ntr</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.3.1\">followed by</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.3.2\">30.79</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.3.3\">31.07</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.3.4\">30.37</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.3.5\">31.64</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.3.6\">31.07</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.3.7\">31.78</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T4.1.1.3.8\">31.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.4.1\">follows</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.4.2\">33.48</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.4.3\">34.35</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.4.4\">34.49</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.4.5\">34.64</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.4.6\">34.2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.4.7\">33.04</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.4.8\">34.93</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.5.1\">P190</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.5.2\">3.96</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.5.3\">2.52</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.5.4\">3.06</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.5.5\">2.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.5.6\">3.42</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.5.7\">3.24</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.5.8\">3.24</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.6.1\">capital</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.6.2\">43.65</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.6.3\">40.36</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.6.4\">40.1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.6.5\">41.62</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.6.6\">41.88</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.6.7\">40.61</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.6.8\">41.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.7.1\">capital of</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.7.2\">32.87</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.7.3\">32.87</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.7.4\">32.02</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.7.5\">33.15</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.7.6\">35.39</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.7.7\">34.83</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.7.8\">34.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.8.1\">P131</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.8.2\">39.41</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.8.3\">40.68</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.8.4\">40.68</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.8.5\">38.14</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.8.6\">39.83</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.8.7\">38.98</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.8.8\">40.68</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.9.1\">country</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.9.2\">57.41</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.9.3\">53.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.9.4\">56.94</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.9.5\">54.17</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.9.6\">54.17</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.9.7\">54.63</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.9.8\">56.48</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.10.1\">name after</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.10.2\">62.4</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.10.3\">65.6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.10.4\">65.6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.10.5\">60.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.10.6\">62.4</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.10.7\">63.2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.10.8\">64.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.11.1\">P1001</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.11.2\">68.38</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.11.3\">72.65</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.11.4\">66.67</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.11.5\">69.23</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.11.6\">69.23</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.11.7\">70.94</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T4.1.1.11.8\">65.81</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T4.1.1.12.1\">headquarters location</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T4.1.1.12.2\">53.68</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T4.1.1.12.3\">58.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T4.1.1.12.4\">54.74</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T4.1.1.12.5\">56.84</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T4.1.1.12.6\">57.89</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T4.1.1.12.7\">61.05</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A1.T4.1.1.12.8\">52.63</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Evaluation of fixed word order models on different relations. </figcaption>\n</figure>",
            "capture": "Table 4: Evaluation of fixed word order models on different relations. "
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.00876v1_figure_1.png",
            "caption": "Figure 1: The results of natural and shuffled models on WiQueen. Table 1 shows the specific numbers."
        },
        "2": {
            "figure_path": "2403.00876v1_figure_2.png",
            "caption": "Figure 2: Results of worst/best fixed word orders and natural word order. Table 3 shows the specific numbers."
        },
        "3": {
            "figure_path": "2403.00876v1_figure_3.png",
            "caption": "Figure 3: Performance on different relations. \u201cP1001\u201d refers to relation \u201capplies to jurisdiction\u201d, and \u201cP159\u201d refers to relation \u201cheadquarters location\u201d. Table 4 shows the specific numbers."
        }
    },
    "references": [
        {
            "1": {
                "title": "Word order\ndoes matter and shuffled language models know it.",
                "author": "Mostafa Abdou, Vinit Ravishankar, Artur Kulmizev, and Anders S\u00f8gaard. 2022.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 6907\u20136919,\nDublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.acl-long.476"
            }
        },
        {
            "2": {
                "title": "Exemplar\nvariability facilitates retention of word learning by children with specific\nlanguage impairment.",
                "author": "Jessica M. Aguilar, Elena Plante, and Michelle Sandoval. 2018.",
                "venue": "Language, Speech, and Hearing Services in Schools,\n49(1):72\u201384.",
                "url": "https://doi.org/10.1044/2017_LSHSS-17-0031"
            }
        },
        {
            "3": {
                "title": "Enriching word vectors\nwith subword information.",
                "author": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.",
                "venue": "Transactions of the Association for Computational Linguistics,\n5:135\u2013146.",
                "url": "https://doi.org/10.1162/tacl_a_00051"
            }
        },
        {
            "4": {
                "title": "On the role of lexical and\nworld knowledge in RTE3.",
                "author": "Peter Clark, Phil Harrison, John Thompson, William Murray, Jerry Hobbs, and\nChristiane Fellbaum. 2007.",
                "venue": "In Proceedings of the ACL-PASCAL Workshop on Textual\nEntailment and Paraphrasing, pages 54\u201359, Prague. Association for\nComputational Linguistics.",
                "url": "https://aclanthology.org/W07-1409"
            }
        },
        {
            "5": {
                "title": "BERT: Pre-training of\ndeep bidirectional transformers for language understanding.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.",
                "venue": "In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171\u20134186,\nMinneapolis, Minnesota. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/N19-1423"
            }
        },
        {
            "6": {
                "title": "Analogy training multilingual encoders.",
                "author": "Nicolas Garneau, Mareike Hartmann, Anders Sandholm, Sebastian Ruder, Ivan\nVuli\u0107, and Anders S\u00f8gaard. 2021.",
                "venue": "In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 12884\u201312892.",
                "url": null
            }
        },
        {
            "7": {
                "title": "The origin and\nevolution of word order.",
                "author": "Murray Gell-Mann and Merritt Ruhlen. 2011.",
                "venue": "Proceedings of the National Academy of Sciences,\n108(42):17290\u201317295.",
                "url": "https://doi.org/10.1073/pnas.1113716108"
            }
        },
        {
            "8": {
                "title": "Crosslinguistic word\norder variation reflects evolutionary pressures of dependency and information\nlocality.",
                "author": "Michael Hahn and Yang Xu. 2022.",
                "venue": "Proceedings of the National Academy of Sciences,\n119(24):e2122604119.",
                "url": "https://doi.org/10.1073/pnas.2122604119"
            }
        },
        {
            "9": {
                "title": "How effective\nis BERT without word ordering? implications for language understanding and\ndata privacy.",
                "author": "Jack Hessel and Alexandra Schofield. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 2: Short Papers), pages 204\u2013211,\nOnline. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-short.27"
            }
        },
        {
            "10": {
                "title": "Token-based\ntypology and word order entropy: A study based on universal dependencies.",
                "author": "Natalia Levshina. 2019.",
                "venue": "Linguistic Typology, 23(3):533\u2013572.",
                "url": "https://doi.org/doi:10.1515/lingty-2019-0025"
            }
        },
        {
            "11": {
                "title": "Roberta: A robustly optimized bert pretraining approach.",
                "author": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.",
                "venue": "arXiv preprint arXiv:1907.11692.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Case, word order, and language learnability.",
                "author": "Gary Lupyan and Morten Christiansen. 2002.",
                "venue": "In Proceedings of the 24th Annual Conference of the Cognitive\nScience Society. Lawrence Erlbaum, New Jersey.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Efficient estimation of word representations in vector space.",
                "author": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.",
                "venue": "arXiv preprint arXiv:1301.3781.",
                "url": null
            }
        },
        {
            "14": {
                "title": "How variability\nshapes learning and generalization.",
                "author": "Limor Raviv, Gary Lupyan, and Shawn Green. 2022.",
                "venue": "Trends in Cognitive Sciences, 26.",
                "url": "https://doi.org/10.1016/j.tics.2022.03.007"
            }
        },
        {
            "15": {
                "title": "Masked\nlanguage modeling and the distributional hypothesis: Order word matters\npre-training for little.",
                "author": "Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and\nDouwe Kiela. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 2888\u20132913, Online and Punta Cana,\nDominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.emnlp-main.230"
            }
        },
        {
            "16": {
                "title": "The role of input\nvariability and learner age in second language vocabulary learning.",
                "author": "Ruta Sinkeviciute, Helen Brown, Gwen Brekelmans, and Elizabeth Wonnacott. 2019.",
                "venue": "Studies in Second Language Acquisition, 41:1\u201326.",
                "url": "https://doi.org/10.1017/S0272263119000263"
            }
        },
        {
            "17": {
                "title": "Processing of a free word order language: the role of syntax and\ncontext.",
                "author": "Natalia Slioussar. 2011.",
                "venue": "Journal of Psycholinguistic Research, 40:291\u2013306.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Multilingual\nculture-independent word analogy datasets.",
                "author": "Matej Ul\u010dar, Kristiina Vaik, Jessica Lindstr\u00f6m, Milda\nDailid\u0117nait\u0117, and Marko Robnik-\u0160ikonja. 2020.",
                "venue": "In Proceedings of the Twelfth Language Resources and Evaluation\nConference, pages 4074\u20134080, Marseille, France. European Language Resources\nAssociation.",
                "url": "https://aclanthology.org/2020.lrec-1.501"
            }
        },
        {
            "19": {
                "title": "GLUE: A multi-task\nbenchmark and analysis platform for natural language understanding.",
                "author": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\nBowman. 2018.",
                "venue": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, pages 353\u2013355,\nBrussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W18-5446"
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.00876v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.00876v1",
        "paper_title": "Word Order and World Knowledge",
        "research_background": "The paper is motivated by the observation that while the distribution of dominant word orders in languages is often explained by communicative efficiency, most languages exhibit some variation across different word orders. The standard explanation attributes this variability to the influence of multiple languages and the pressure of language acquisition. However, this does not account for the initial emergence of alternative word orders from what is presumed to have been an original Subject-Verb-Object (SVO) syntax.\n\nThe research problem tackled by the paper is to explore a novel hypothesis\u2014the Wov2Lex hypothesis\u2014which posits that word order variation facilitates the acquisition of lexical semantics. The hypothesis finds support from human language acquisition studies which suggest that input variability aids both first and second language acquisition, leading to more general and robust performance despite initial challenges.\n\nRelevant prior work includes:\n1. **Communicative Efficiency:**\n   - Hahn and Xu (2022) explored dependency and information locality as explanations for dominant word order distributions.\n2. **Language Influence and Acquisition:**\n   - Lupyan and Christiansen (2002) discussed the role of multiple languages and language acquisition pressures in fixing word orders.\n3. **Origin of Word Orders:**\n   - Gell-Mann and Ruhlen (2011) proposed that the first language probably had SVO syntax.\n4. **Input Variability:**\n   - Sinkeviciute et al. (2019) and Aguilar et al. (2018) provided evidence that input variability facilitates language acquisition, specifically new word learning.\n5. **Text Shuffling in LMs:**\n   - Sinha et al. (2021) pre-trained language models on shuffled texts and found minor performance drops, while Abdou et al. (2022) identified limitations in Sinha et al.\u2019s work and showed significant performance degradation due to corpus perturbation.\n6. **Word Analogies for World Knowledge:**\n   - Garneau et al. (2021) provided experimental protocols and datasets for probing language models for world knowledge through word analogies.\n\nThe paper extends this body of research by testing the Wov2Lex hypothesis in the context of pre-trained language models. It does so by examining whether word order variation impacts the acquisition of world knowledge by these models, using protocols inspired by prior work and pre-training models on data with \"fixed\" word orders. The findings suggest that while fixing word order can lead to performance variations, natural word orders result in more consistent, mediocre performance across tested languages and relations.",
        "methodology": "### Methodology\n\n#### Overview\nThe study investigates the effects of word order on the understanding of world knowledge by testing language models (LMs) trained on corpora with different word orders. Three primary configurations are explored: natural word order, shuffled word order, and fixed word order.\n\n#### Natural Word Order Models\n1. **Models Used**:\n   - **BERT** and **mBERT** by Devlin et al. (2019). \n   - **RoBERTa** by Liu et al. (2019).\n\n2. **Model Details**:\n   - Both use 12 layers with 12 attention heads.\n   - RoBERTa has an extended pre-training corpus and more training steps (160GB corpus, 500K steps).\n\n3. **Terminology**:\n   - These models are referred to as \"natural models\" since they are trained on natural word order texts.\n\n#### Shuffled Word Order Models\n1. **Previous Research**:\n   - Referencing works by Sinha et al. (2021) and Abdou et al. (2022), which involved pre-training RoBERTa models on corpora shuffled at various levels (e.g., corpus-level, unigram-level, bigram-level).\n\n2. **Models Tested**:\n   - Six models from Sinha et al. (2021): shuf.n1, shuf.n2, shuf.n3, shuf.n4, shuf.cps, and nopos.\n\n3. **Training Specifics**:\n   - Pre-trained on the Toronto Book Corpus and English Wikipedia (16GB).\n   - Full-scale RoBERTa base model trained for 100,000 steps over 72 hours on 64 GPUs.\n\n4. **Model Definitions**:\n   - **shuf.n1 to shuf.n4**: Shuffled at different -gram levels.\n   - **shuf.cps**: Entire reshuffled corpus based on word frequency.\n   - **nopos**: Trained without positional embeddings to inhibit sequence order encoding.\n\n#### Fixed Word Order Models\n1. **Objective**:\n   - To evaluate if natural word orders yield superior representations compared to fixed word orders.\n\n2. **Approach**:\n   - Use dependency trees from Wikipedia text (extracted via SpaCy) to merge named entities and noun chunks.\n   - Extract -grams, rearrange subjects, objects, and verbs within these -grams to obtain different word orders (SVO, SOV, VOS, VSO, OSV, OVS).\n\n3. **Corpus Creation**:\n   - -grams reconfigured and concatenated to maintain consistency except for word order.\n   - Different corpora sizes for English, German, French, Spanish, and Polish, with different approaches based on linguistic features.\n\n4. **Training Configuration**:\n   - Batch size: 16, Accumulation step: 8, Learning rate: 1e-4.\n   - Optimizer: AdamW with linear scheduler, 1% warmup steps.\n   - Trained using 4 GPUs for 50,000 to 100,000 steps with early stopping.\n   - Default settings for all unmodified parameters.\n\n#### Performance Lower Bound\n1. **Randomly Initialized Embeddings**:\n   - Tested using RoBERTa with random initialization (denoted R-rand).\n\n2. **Additional Models**:\n   - Performance of FastText (Bojanowski et al., 2017) included as a benchmark.\n\n#### Evaluation and Dataset\n1. **Evaluation Dataset**:\n   - WiQueen dataset, introduced by Garneau et al. (2021), a large-scale multilingual dataset for analogy retrieval tasks.\n\n2. **Data Characteristics**:\n   - Consists of 4,815 samples across 139 relationships and 10,124 concepts.\n   - Performance is evaluated using Precision@1.\n\nThis methodology highlights a comprehensive approach to understanding how different word orders affect language model training and performance, with a specific focus on their capacity to capture world knowledge.",
        "main_experiment_and_results": "Main Experiment Setup and Results: \nTo probe the effects of word order on world knowledge, we conduct tests using language models trained on corpora with different word orders: natural order, shuffled order, and fixed word order.\n\n**Datasets and Baselines:**\n1. **Natural Word Order Models:**\n   - **Models:** Original BERT, mBERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019).\n   - **Setup:** These models have 12 layers with 12 attention heads, with RoBERTa having extended pre-training on a 160GB corpus for 500K steps.\n   - **Training Data:** Toronto Book Corpus and English Wikipedia (16GB).\n\n**Evaluation Metrics:**\n- **Word Analogies Task:**\n  - **Dataset:** WiQueen (Garneau et al., 2021), which is a multilingual multi-word analogy dataset from Wikidata. It consists of 4,815 samples across 139 relationships and 10,124 concepts.\n  - **Metric:** Performance evaluated using Precision@1.\n\n**Main Experimental Results:**\n- The models trained on natural word orders (natural models) showed significantly better performance on word analogy tasks compared to both shuffled and fixed word order models.\n- The baseline R-rand and Fasttext models were outperformed by all pre-trained models, emphasizing the necessity of training with meaningful word orders for acquiring world knowledge."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effects of word order on world knowledge, and specifically how different word orders in pre-trained language models affect performance on word analogies.",
            "experiment_process": "The natural models include BERT, mBERT, and RoBERTa, trained on natural texts in multiple languages. Shuffled models include various RoBERTa models pre-trained on corpora shuffled at different levels (unigram, bigram, and entire reshuffled corpus). Fixed word order models were created by reordering subject, verb, and object within the extracted n-grams from Wikipedia text for different languages (English, German, French, Spanish, and Polish). The models were pre-trained with specific settings (batch size 16, learning rate 1e-4, AdamW optimizer, and training on 4 GPUs), and evaluated on the WiQueen dataset using Precision@1 as the metric. R-rand models with randomly initialized token embeddings and Fasttext were used as lower bound comparisons.",
            "result_discussion": "Analysis from the WiQueen dataset showed that the permutation of word order generally leads to a slight increase or decrease in performance, indicating that word order information may not be essential for the tasks tested or heavily relied upon by the evaluated language models.",
            "ablation_id": "2403.00876v1.No1"
        },
        {
            "research_objective": "To extend the exploration from 'natural' and 'shuffled' to 'fixed' word orders and to determine if a particular fixed word order yields better performance, if combining multiple word orders (i.e., natural word order) offers any advantage, and if these properties are consistent across languages.",
            "experiment_process": "RoBERTa models were pre-trained with pre-training corpora reordered to different fixed word orders (SVO, SOV, VOS, VSO, OSV, OVS) across five languages. The same pre-training settings were used as in the first study. Performance on word analogies was evaluated using the WiQueen dataset, with results reported as Precision@1.",
            "result_discussion": "Q1: Certain fixed word orders show distinct performance patterns, with SVO performing best in English and French, but worst in Spanish. Q2: Combining word orders (natural word order) does not confer advantages over individual fixed word orders, although natural word order maintains middle-ground performance. Q3: The superiority of fixed word orders varies across languages, with trends in English, German, and French more aligned, unlike Polish and Spanish. These findings challenge the Wov2Lex hypothesis and suggest that language models and linguistics may process natural language differently.",
            "ablation_id": "2403.00876v1.No2"
        }
    ]
}