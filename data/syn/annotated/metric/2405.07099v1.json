{
    "title": "Do Pretrained Contextual Language Models Distinguish between Hebrew Homograph Analyses?",
    "abstract": "Semitic morphologically-rich languages (MRLs) are characterized by extreme word ambiguity. Because most vowels are omitted in standard texts, many of the words are homographs with multiple possible analyses, each with a different pronunciation and different morphosyntactic properties. This ambiguity goes beyond word-sense disambiguation (WSD), and may include token segmentation into multiple word units. Previous research on MRLs claimed that standardly trained pre-trained language models (PLMs) based on word-pieces may not sufficiently capture the internal structure of such tokens in order to distinguish between these analyses.\n\nTaking Hebrew as a case study, we investigate the extent to which Hebrew homographs can be disambiguated and analyzed using PLMs. We evaluate all existing models for contextualized Hebrew embeddings on a novel Hebrew homograph challenge sets that we deliver. Our empirical results demonstrate that contemporary Hebrew contextualized embeddings outperform non-contextualized embeddings; and that they are most effective for disambiguating segmentation and morphosyntactic features, less so regarding pure word-sense disambiguation. We show that these embeddings are more effective when the number of word-piece splits is limited, and they are more effective for 2-way and 3-way ambiguities than for 4-way ambiguity. We show that the embeddings are equally effective for homographs of both balanced and skewed distributions, whether calculated as masked or unmasked tokens. Finally, we show that these embeddings are as effective for homograph disambiguation with extensive supervised training as with a few-shot setup.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Semitic morphologically-rich languages (MRLs) such as Arabic, Hebrew, and Aramaic are characterized by extreme ambiguity at the word level (Wintner, 2014; Tsarfaty et al., 2020). In a standard text, many words will be homographs with multiple possible analyses. This high ambiguity arises from several factors: prepositions, conjunctions, accusative pronouns, and possessive pronouns are often affixed to words; vowels are typically omitted in written texts; and proper nouns are not differentiated from common nouns.\n\nThe task of distinguishing between Hebrew homograph analyses is related to Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006; Navigli, 2009), yet it is more challenging. In standard WSD, a single orthographic form is associated with a single word analyzed in terms of different senses, often with the same morphosyntactic properties. In contrast, Semitic languages often require disambiguation beyond just sense. Hebrew word ambiguities can be categorized as: 1. Segmentation ambiguities, where an orthographic form may be segmented into multiple word units, each with its own role (POS tag). 2. Morphosyntactic ambiguities, where segmentation is clear but different morphosyntactic properties are present. 3. Sense ambiguities, where analyses differ in sense but not morphosyntactic properties. One orthographic form may exhibit multiple types of ambiguity simultaneously.\n\nPretrained contextualized language models, with standard word-piece tokenization, have shown excellence in WSD in English and other Indo-European languages (Yaghoobzadeh et al., 2019). However, for Hebrew and other Semitic languages, models might not effectively capture MRL structures to distinguish complex homograph analyses (Klein and Tsarfaty, 2020; Tsarfaty et al., 2020). This work takes Modern Hebrew as a case study to investigate whether homograph disambiguation is possible using contextualized embeddings across all ambiguity levels. Regarding Arabic, a sister language, Abderrahim and Abderrahim (2022) suggest potential in using pretrained contextualized embeddings, a topic for future work.\n\nHebrew presents challenges for homograph disambiguation due to limited corpus availability. Existing Hebrew treebanks are small, leaving most words underrepresented. Moreover, common Hebrew words in these corpora are problematic due to skewed distribution. For instance, the common Hebrew homograph \\<mhM> (mhm) is largely analyzed as a preposition or an interrogative, but its distribution is over 50:1, thus underrepresenting the secondary analysis in tagged corpora.\n\nPOS tagging systems might address these ambiguities. Habash and Rambow (2005) explore morphological tagging for WSD in Arabic, and several Hebrew POS tagging systems exist (Yona and Wintner, 2005; Adler and Elhadad, 2006; Shacham and Wintner, 2007). The current state-of-the-art for Hebrew POS tagging is the YAP morpho-syntactic parser (Tsarfaty et al., 2019). However, previous studies show that YAP's accuracy decreases with skewed homograph distributions (Shmidman et al., 2020).\n\nTo address analogous cases in other languages, researchers have proposed creating dedicated challenge sets with difficult-to-classify sentences (Gardner et al., 2020; Elkahky et al., 2018). In prior studies, 22 challenge sets for Hebrew homographs were produced, and a Bi-LSTM with non-contextualized embeddings achieved high accuracy, defining the state-of-the-art for Hebrew homograph disambiguation (Shmidman et al., 2020). This paper extends the investigation by evaluating whether contextualized embeddings from pretrained language models (PLMs) offer a better solution. We assess all existing contextualized Hebrew PLMs: multilingual BERT (\"mBERT\", Devlin et al., 2019), HeBERT (Chriqui and Yahav, 2021), and AlephBERT (Seker et al., 2021). Additionally, we use a new, larger dataset for Hebrew homograph disambiguation.\n\nOur experiments show that contextualized PLMs trained on large, unlabeled datasets and vocabulary effectively disambiguate word-internal homograph structures but face challenges in pure sense disambiguation. We demonstrate their efficacy in handling skewed homograph distributions and few-shot scenarios, establishing new state-of-the-art results for a morphologically-rich language printed without vowels, and provide a novel benchmark for assessing the morphological reach of future PLMs in Hebrew."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "The Data",
            "text": "The challenge sets for Hebrew homograph disambiguation from our previous study were limited in number (only 22 sets) and insufficiently representative regarding types of ambiguities; only one of the sets was a prefix-segmentation ambiguity. Further, they were limited to binary cases, where only two analyses exist.\nIn contrast, for this study we\nemployed field experts to choose the most critical homographs in the language. The experts chose 75 homographs\nfrom a list of the 3600 most frequent words in the language, balancing frequency of word occurrence with practical need for its disambiguation.\nAll of the homographs occur with a minimum frequency of 27 words per million in naturally occurring Hebrew text.\nOur challenge sets include homographs with 2-5 possible analyses. Our sets contain a wide representation of segmentation ambiguities (15 in number), as well as 5 cases of purely semantic ambiguities.\nFor each of the 75 homographs, we collect hundreds of naturally-occurring sentences attesting to each analysis. In almost all cases, we succeed in collecting 1000 sentences for the primary analysis, at least 500 sentences for the secondary analysis, and at least 250 for each additional analysis. The sentences were culled from newspapers, Wikipedia, literature, and social media. We employed a team of annotators who chose the relevant homograph analysis for each case.222The annotation process is detailed in Appendix LABEL:section:appendixCuration.\nAll in all, our 75 challenge sets contain 150K tagged sentences.\nThe full list of homographs and analyses is provided in Appendix A  ###reference_###.333The dataset is downloadable at: https://github.com/Dicta-Israel-Center-for-Text-Analysis/EACL_2023  ###reference_-Text-Analysis/EACL_2023###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experimental Setup",
            "text": "To evaluate the ability of pre-trained language models (PLMs) to disambiguate the in-context analyses of morphologically rich and highly ambiguous homographs in Hebrew, we adopt a \"word expert\" approach, producing dedicated classifiers for each individual homograph Zhao et al. (2020  ###reference_b27###).\n\nWe use two types of PLMs, contextualized and non-contextualized. For the non-contextualized case, we replicate our previous method detailed in Shmidman et al. (2020  ###reference_b20###). For each training example, we use a BiLSTM on top of the word2vec embeddings of all of the words in the sentence (other than the homograph itself) to produce an encoding for disambiguation.444We also tested fastText, but results were inferior. An MLP is trained to predict the correct homograph analysis based on this encoding.555For implementation details, see Appendix LABEL:section:appendixImplementation.\n\nFor the contextualized case, we run the sentence through a pretrained contextualized language model and retrieve the 768-dimension embedding representing the homograph in question. An MLP is trained to predict the correct analysis based on the homographs embeddings alone. In the standard \"unmasked\" scenario, the sentence is fed into the model as is, including the homograph in question. In the \"masked\" scenario, the homograph is replaced with a [MASK] token.\n\nWe evaluate the performance of each given method on each given challenge set using 10-fold cross-validation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": "Figure 2 demonstrates AlephBERT\u2019s performance on different ambiguity types. AlephBERT performs equally well on cases of segmentation ambiguity and morphosyntactic ambiguity. In contrast, when it comes to ambiguities that are purely semantic, the scores are noticeably lower. This is in line with the findings of Ettinger (2020), who shows that BERT is stronger with syntax than semantics; Goldberg (2019) also notes BERT\u2019s strong syntactic abilities.\n\nInterestingly, the same gap exists with the W2V-based method. Thus, both contextualized and non-contextualized embeddings struggle to differentiate between senses which are morphologically equivalent. Although such cases are only of minimal import when it comes to sentence parsing, they are critical for downstream tasks such as coreference resolution and relation extraction. It thus remains a desideratum to improve disambiguation of purely semantic Hebrew homographs.\n\nThe results in Figure 3 demonstrate that AlephBERT performs equally well on cases of binary homographs as on cases of three-way homograph classification. However, when faced with cases of 4-way or 5-way classification, accuracy declines. Previous studies have hypothesized that word-pieces are not adequate for capturing complex morphosyntactic structures due to arbitrary (non-linguistic) word-splits. To probe into this we investigate the question, do such splits affect performance. Our 75 homographs are all treated as single tokens in HeBERT and AlephBERT. However, many of the homographs are broken up into word pieces in mBERT, due to its meager Hebrew vocabulary. We thus compare mBERT\u2019s results on words treated as single tokens versus those that are broken up into two or three pieces, which are aggregated using first, sum, or average of the vectors. With regard to cases of split words, we train models using three separate methods: providing the MLP with only the embedding of the first word piece; with an average of the word piece embeddings; or with the sum of the embeddings. As shown in Figure 4, the splitting of a homograph into three word-pieces appears to have a negative impact on the ability of the resulting embedding to differentiate between homograph analyses, for all aggregation methods.\n\nWe consider whether AlephBERT embeddings are more effective if we replace the homograph word with [MASK] when running the challenge set sentences through AlephBERT. The motivation behind this experiment is that, as explained above, many of the homographs are skewed in their natural proportion. In such cases, we worry whether AlephBERT might be disproportionately influenced by the skewed distribution; replacing the word with [MASK] would prevent the model from being influenced as such. As shown in figure 5, AlephBERT achieves high scores both with balanced homographs as well as with homographs of highly skewed distribution. Using a [MASK] token instead of the actual word does not generally improve performance, whether or not the homographs are of skewed proportion.\n\nIn our experiments thus far, the 10-fold cross-validation allows the MLP to leverage 90% of the data in each fold (hundreds of sentences for each analysis) in order to learn the difference between the analyses. We now consider whether the AlephBERT embeddings can suffice on a few-shot basis, where the training stage has access to only 100, 50, 25, 10 or even 5 examples of each analysis. In these cases, we train an MLP based only on these few samples, and we use the rest of the sentences for evaluation. Astoundingly, as demonstrated in Figure 6, the AlephBERT embeddings provide a highly accurate solution even on this few-shot basis. Even when training with only 5 examples of each homograph analysis, AlephBERT reaches an accuracy that is not far below the accuracy achieved when performing full 10-fold CV across hundreds of sentences of each analysis.\n\nFinally, we probe the pretrained AlephBERT embeddings Yaghoobzadeh et al. (2019); Tenney et al. (2019); Klafka and Ettinger (2020); Belinkov (2021) to see whether in and of themselves they reflect clusters which correspond to different homograph analyses. We skip the MLP, and instead use the raw embeddings directly, classifying sentences based on their proximity to the centroid of the training samples for each homograph analysis. As shown in the orange bars in Figure 6, this method generally does not perform as well as the MLP-based method; however, the degradation is limited to only a few percentage points, indicating that the raw embeddings are generally clustered in groups which indeed reflect the distinctions between the analyses."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study we have utilized a wide-ranging collection of Hebrew homograph challenge sets in order to evaluate the extent to which raw contextualized embeddings can be leveraged to disambiguate Hebrew homographs. We found that contextualized embeddings can effectively disambiguate analyses of homographs, much more so than non-contextualized ones, regarding multiple types of ambiguity: segmentation, morphosyntactic and sense. Yet, efficacy on pure sense ambiguity is lower than on the other two types. Additionally, an increasing number of splits, or an increasing number of different possible analyses of a token, each lower efficacy.\n\nFinally, we found that contextualized embeddings can function effectively for this purpose on a few-shot basis, with as little as 5 examples of each analysis. This indicates that with relatively modest effort, highly ambiguous homographs may be effectively treated."
        }
    ],
    "url": "http://arxiv.org/html/2405.07099v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "4"
        ]
    },
    "research_context": {
        "paper_id": "2405.07099v1",
        "paper_title": "Do Pretrained Contextual Language Models Distinguish between Hebrew Homograph Analyses?",
        "research_background": "**Motivation:**\nThe main motivation for this paper stems from the need to effectively handle the inherent complexity and ambiguity of Hebrew, a Semitic morphologically-rich language (MRL). Hebrew, like other MRLs, presents substantial word-level ambiguity due to factors such as seamless affixing of prepositions and pronouns, omission of vowels in written text, and the non-differentiation between proper and common nouns. This ambiguity poses a significant challenge for computational language models to disambiguate homographs\u2014words that share the same orthographic form but have different meanings or grammatical functions. The motivation is to assess how well pretrained contextualized language models (PLMs) can tackle this homograph disambiguation.\n\n**Research Problem:**\nThe specific research problem addressed in this paper concerns whether pretrained contextualized language models can distinguish between different analyses of Hebrew homographs. This problem is complex because Hebrew homograph ambiguity can manifest in three main ways: segmentation ambiguities, morphosyntactic ambiguities, and sense ambiguities. Given the limited size and quality of existing Hebrew corpora, where many homographs are skewed in their distribution, the study aims to evaluate if PLMs can overcome these limitations and provide accurate disambiguation, especially focusing on morphologically-rich, vowel-less Hebrew text.\n\n**Relevant Prior Work:**\n1. **General Task of WSD:** The paper positions Hebrew homograph disambiguation as a more complex variant of the general Word Sense Disambiguation (WSD) problem, which has been extensively studied (Agirre and Edmonds, 2006; Navigli, 2009). WSD typically involves distinguishing senses of a word with identical morphosyntactic properties, whereas Hebrew homographs often require disambiguation beyond sense, including segmentation and morphosyntactic properties.\n\n2. **Challenges in Hebrew and MRLs:** Previous studies have noted the difficulties PLMs face with MRLs due to their complex morphosyntactic structure (Klein and Tsarfaty, 2020; Tsarfaty et al., 2020). This study builds on this understanding by examining Hebrew specifically.\n\n3. **Arabic WSD Methods:** Work on WSD in Arabic (Abderrahim and Abderrahim, 2022) highlights the potential for pretrained contextual embeddings, though empirical evaluations remain future research. Other Arabic WSD studies (Haffar and Zrigui, 2023; Merhbene et al., 2013; Shah et al., 2010) provide context but are not directly evaluated here.\n\n4. **Hebrew POS Tagging Systems:** The use of POS taggers for Hebrew, such as the YAP morpho-syntactic parser (Tsarfaty et al., 2019), has been explored but shows limited effectiveness for homographs due to skewed distribution (Shmidman et al., 2020).\n\n5. **Challenge Sets for Skewed Distribution:** Inspired by other languages' approaches to skewed data distribution (Gardner et al., 2020; Elkahky et al., 2018), the current research extends the scope from non-contextual Bi-LSTM models (Shmidman et al., 2020) to contextual models like mBERT, HeBERT, and AlephBERT.\n\n**Contribution:**\nThis paper extends the state-of-the-art in Hebrew homograph disambiguation by evaluating pretrained contextualized embeddings on a newly created, larger dataset. By doing so, the study aims to provide insights into the models' efficacy in handling skewed distributions, sense disambiguation challenges, and establishing robust benchmarks for future research.",
        "methodology": "To assess the proficiency of pre-trained language models (PLMs) in disambiguating in-context analyses of Hebrew homographs, which are both morphologically rich and highly ambiguous, we implement a 'word expert' strategy by creating dedicated classifiers for each homograph, drawing inspiration from the approach described by Zhao et al. (2020). \n\nPLMs Used:\n\n1. Contextualized Pre-trained Language Models\n2. Non-contextualized Pre-trained Language Models\n\nNon-Contextualized Case:\n\nFor this scenario, we replicate the methodology detailed in Shmidman et al. (2020). The steps are as follows:\n- Word Embedding: We employ word2vec embeddings for all the words in the sentence, excluding the homograph itself.\n- Encoding Generation: A BiLSTM (Bidirectional Long Short-Term Memory) network is applied on top of the word2vec embeddings to generate an encoding necessary for disambiguation.\n- Classifier Training: An MLP (Multi-Layer Perceptron) is trained to predict the correct analysis of the homograph using the above-mentioned encoding.\n\nContextualized Case:\n\nIn this scenario, the following steps are followed:\n- Sentence Processing: The entire sentence, inclusive of the homograph, is processed through a pretrained contextualized language model.\n- Embedding Extraction: The embedding representing the homograph, which is 768-dimensions, is retrieved.\n- Classifier Training: An MLP is employed to predict the correct analysis based on the embeddings of the homographs.\n\nScenario Variations:\n- Unmasked Scenario: The model is fed the sentence as it is, with the homograph included.\n- Masked Scenario: The homograph is replaced with a [MASK] token in the sentence before being fed into the model.\n\nPerformance Evaluation:\n\nEach method's performance is evaluated using 10-fold cross-validation on each challenge set. The evaluation metrics include:\n- F1 Score Calculation: For each homograph analysis, an F1 score is computed based on the precision and recall scores micro-averaged across all folds.\n- Macro-averaged F1 Score: The macro-average of the F1 scores for all possible analyses for a given homograph is computed, and this score is reported in the charts provided in the paper.",
        "main_experiment_and_results": "**Objective:** To evaluate the ability of pre-trained language models (PLMs) to disambiguate in-context analyses of morphologically rich and highly ambiguous homographs in Hebrew.\n\n**Approach:** \n- Adopt a \"word expert\" approach, creating dedicated classifiers for each individual homograph, following Zhao et al. (2020).\n- Use two types of PLMs: contextualized and non-contextualized.\n\n**Non-contextualized Model:**\n- Replicate the method detailed in Shmidman et al. (2020).\n- For each training example:\n  - Use BiLSTM on top of word2vec embeddings of all words in the sentence (excluding the homograph itself) to produce an encoding for disambiguation.\n  - An MLP is trained to predict the correct homograph analysis based on this encoding.\n\n**Contextualized Model:**\n- Run the sentence through a pretrained contextualized language model.\n- Retrieve the 768-dimensional embedding representing the homograph.\n- An MLP is trained to predict the correct analysis based on the homograph's embeddings alone.\n- Two scenarios tested:\n  - **Unmasked:** The sentence is fed into the model with the homograph included.\n  - **Masked:** The homograph is replaced with a [MASK] token.\n\n**Evaluation Metrics:**\n- Performance evaluated on each challenge set using 10-fold cross-validation.\n- Calculate F1 score for each homograph analysis based on precision and recall scores (micro-averaged across all folds).\n- Report macro-average of the F1 scores for all possible analyses for a given homograph."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the ability of pre-trained language models (PLMs) to disambiguate the in-context analyses of morphologically rich and highly ambiguous homographs in Hebrew.",
            "experiment_process": "We use two types of PLMs, contextualized and non-contextualized. For non-contextualized, we use a BiLSTM on top of word2vec embeddings, encoding all words in a sentence except for the homograph itself, and an MLP predicts the correct homograph analysis based on this encoding. For contextualized, a pretrained language model provides a 768-dimension embedding for the homograph, and an MLP predicts the analysis based on this embedding. We also compare models using masked ([MASK] token) versus unmasked scenarios. Performance is evaluated using 10-fold cross-validation, calculating F1 scores based on the precision and recall scores micro-averaged across folds.",
            "result_discussion": "Contextualized models HeBERT and AlephBERT outperform mBERT and the previous word2vec-based SOTA, effectively capturing Hebrew homograph distinctions even in an MRL. They perform well on segmentation and morphosyntactic ambiguities but struggle with purely semantic ambiguities similar to non-contextualized embeddings. The performance declines for 4-way or 5-way classification compared to binary or three-way classification. Splitting homographs into word-pieces negatively impacts performance. Replacing homographs with [MASK] did not generally improve performance, and AlephBERT embeddings achieved high accuracy even with few-shot training. Raw embeddings reflect distinctions between homograph analyses but less effectively than MLP-based methods.",
            "ablation_id": "2405.07099v1.No1"
        },
        {
            "research_objective": "To determine if pre-trained language models' performance in distinguishing homographs varies with word-piece splits and types of semantic ambiguity.",
            "experiment_process": "We compare mBERT results on homographs treated as single tokens versus those split into word pieces, using three aggregation methods: embedding of the first piece, average of pieces, and sum of pieces. Additionally, we run AlephBERT on masked versus unmasked homographs and evaluate using 10-fold cross-validation. Subsets are created based on segmentation, morphosyntactic, and purely semantic ambiguities.",
            "result_discussion": "Splitting homographs into word-pieces negatively impacts the ability of embeddings to differentiate analyses for all aggregation methods. Both contextualized and non-contextualized embeddings perform worse on purely semantic ambiguities, highlighting a need for improvement in this area. The type of ambiguity affects the performance, with AlephBERT struggling more with semantic ambiguities than segmentation or morphosyntactic ambiguities.",
            "ablation_id": "2405.07099v1.No2"
        }
    ]
}