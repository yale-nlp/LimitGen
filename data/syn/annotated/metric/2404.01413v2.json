{
    "title": "Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data",
    "abstract": "The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops proposed that such loops would lead to a phenomenon termed model collapse, under which performance progressively degrades with each model-data feedback iteration until fitted models become useless. However, those studies largely assumed that new data replace old data over time, where an arguably more realistic assumption is that data accumulate over time. In this paper, we ask: what effect does accumulating data have on model collapse?\n\nWe empirically study this question by pretraining sequences of language models on text corpora. We confirm that replacing the original real data by each generation\u2019s synthetic data does indeed tend towards model collapse, then demonstrate that accumulating the successive generations of synthetic data alongside the original real data avoids model collapse; these results hold across a range of model sizes, architectures, and hyperparameters. We obtain similar results for deep generative models on other types of real data: diffusion models for molecule conformation generation and variational autoencoders for image generation. \n\nOur work provides consistent empirical and theoretical evidence that data accumulation avoids model collapse.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The advent of large-scale generative models such as GPT-4 (Achiam et al., 2023), DALL-E (Ramesh et al., 2022), and Stable Diffusion (Rombach et al., 2022) has revolutionized the field of artificial intelligence. These models, trained on vast web-scale datasets, exhibit remarkable capabilities in generating text, images, and other media (Brown et al., 2020; Saharia et al., 2022). However, as these models become more widely used, an increasing amount of generated data populates the web. This raises a critical question: what are the consequences of training generative models on datasets containing their own outputs?\n\nRecent studies have investigated this question, revealing that training generative models on their own outputs can cause the performance of such models to progressively degrade with each model-fitting iteration, eventually rendering newer models useless (Hataya et al., 2023; Mart\u00ednez et al., 2023a; Shumailov et al., 2023; Alemohammad et al., 2023; Mart\u00ednez et al., 2023b; Bertrand et al., 2023; Briesch et al., 2023; Dohmatob et al., 2024a; b) (see Appendix A for review and discussion of prior work). This phenomenon was consequently labeled model collapse. Model collapse warns that democratizing access to generative models runs the risk of polluting the very data necessary to train future iterations of generative models.\n\nTo better understand this phenomenon, many prior works have considered a setup that assumes each model\u2019s generated data replaces previous data. In theory, this leads to very natural comparisons across generations as the total number of training points for each model remains fixed. In practice, subsequent generations of LLMs are often trained with increasing data over time \u2013 e.g., 1.4 trillion tokens for Llama 1 (Touvron et al., 2023a), 2 trillion for Llama 2 (Touvron et al., 2023b), 15 trillion for Llama 3 \u2013 in which presumably both human-generated and machine-generated data are accumulating in training sets collected from the internet. It was noted in some of those works (Hataya et al., 2023; Mart\u00ednez et al., 2023a; Alemohammad et al., 2023; Bertrand et al., 2023; Dohmatob et al., 2024b) that model collapse can be either slowed down or negated by mixing in clean data with the generated data.\n\nTo that end, in this work we study the effect of accumulating data on model collapse, rather than replacing data. Our data-accumulating setting is, in some sense, maximally pessimistic: it considers a hypothetical future where synthetic data are uncontrollably dumped on the internet to be vacuumed up for training the next iteration of generative models. Nevertheless, we find that model collapse is avoided when accumulating data.\n\nWe begin by studying model collapse experimentally with deep generative models trained on realistic data: transformers on causal language modeling (Sec. 2.1), diffusion models on molecular conformation (Sec. 2.2), and variational autoencoders on images (Sec. 2.3). \n\nTo understand why replacing data and accumulating data have different consequences for model collapse, we turn to an analytically tractable framework of a sequence of linear models, each trained on synthetic outputs generated from the previous-iteration\u2019s fitted linear model (Mobahi et al., 2020; Dohmatob et al., 2024a). Our work suggests that data accumulation may be robust to model collapse and emphasizes the importance of considering accumulating data and other real-world data dynamics in the analysis of model collapse in generative models trained on web-scale data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Accumulating Data Avoids Model Collapse in Deep\nGenerative Models",
            "text": "We first investigate model collapse experimentally in several classes of generative models. Here, and for the remainder of this manuscript, the term model collapse refers to notably worsening error over increasing iterations of the model-data loop, while avoiding model collapse refers instead to bounded error over such iterations. To test the effect of accumulating data on model collapse, we compare accumulating data against replacing data. We use three diverse experimental setups of causal transformers, diffusion models, and variational autoencoders trained on real text, molecular conformation, and image datasets, respectively. We find that replacing data yields model collapse for all models and all datasets, whereas accumulating data avoids model collapse.\n\nWe first train causal transformers on text data. Specifically, we pretrain 9M parameter GPT-2 and 12M, 42M and 125M parameter Llama2 language models for a single epoch on TinyStories, a 470M token GPT-3.5/4-generated dataset of short stories at a kindergarten reading level. For each model-fitting iteration, we sample a new dataset of the same size as TinyStories from the previous iteration\u2019s language model and then either replace or concatenate the previous dataset with the newly generated dataset. In each model-fitting iteration, we then pretrain a newly initialized model on the replaced or concatenated dataset from the previous iteration. We experiment with sampling the new datasets using temperatures or . We chose this combination of architectures, scales, dataset, and sampling because the setup necessitates pretraining multiple iterations of language models \u2013 a computationally costly endeavor \u2013 but we also wish to study realistic conditions where generative models are high-performing and generate diverse outputs. Because small language models (below 10M parameters) pretrained on TinyStories were shown to be able to generate coherent-albeit-simple English sentences, this choice of architectures, scales, dataset and temperature hopefully strikes a good balance between being representative, being diverse and being computationally feasible.\n\nWe ablate for several additional potential confounds beyond generation temperature. First, when accumulating data, subsequent model iterations are trained on larger datasets than when replacing data. To control for this, we also perform experiments in which data is replaced, but the size of the (fully synthetic) dataset is grown to match the training set size in the accumulation regime. Second, a possible concern could be that degrading performance when replacing data could be due to low model performance in iteration 1 (and thus the quality of the first synthetic dataset). We control for this by varying the amount of training performed in iteration 1 only and find that this has no significant impact. Lastly, we find that our results are also consistent across varying dataset sizes and training epochs.\n\nWe lastly train sequences of variational autoencoders (VAEs) on CelebA, a dataset of 200k images of human faces split between train and test sets, chosen as a balance between being a realistic dataset with many samples, color images and resolution, and computational feasibility of training multiple iterations of models on accumulating data. The loss is the standard VAE loss: reconstruction error plus the KL divergence between the encoder\u2019s output Gaussian and the isotropic Gaussian prior.\n\nInterestingly, unlike language modeling, the error of accumulating data does increase with the number of iterations (albeit much more slowly than with replacing data). We also note that Mart\u00ednez et al. found slightly contradictory evidence, specifically that a different architecture on a much smaller dataset exhibits fast performance deterioration even with accumulating data. Understanding under what conditions and why these discrepancies exist is an interesting direction we leave for future research."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Transformer-Based Causal Language Modeling",
            "text": "We first train causal transformers (Vaswani et al., 2017) on text data. Specifically, we pretrain 9M parameter GPT-2 (Radford et al., 2019) and 12M, 42M, and 125M parameter Llama2 (Touvron et al., 2023b) language models for a single epoch on TinyStories (Eldan & Li, 2023), a 470M token GPT-3.5/4-generated dataset of short stories at a kindergarten reading level. For each model-fitting iteration, we sample a new dataset of the same size as TinyStories from the previous iteration\u2019s language model and then either replace or concatenate the previous dataset with the newly generated dataset. In each model-fitting iteration, we then pretrain a newly initialized model on the replaced or concatenated dataset from the previous iteration. We experiment with sampling the new datasets using temperatures or . We chose this combination of architectures, scales, dataset, and sampling because the setup necessitates pretraining multiple iterations of language models \u2013 a computationally costly endeavor \u2013 but we also wish to study realistic conditions where generative models are high-performing and generate diverse outputs. Because small language models (below 10M parameters) pretrained on TinyStories were shown to be able to generate coherent-albeit-simple English sentences (Eldan & Li, 2023), this choice of architectures, scales, dataset, and temperature hopefully strikes a good balance between being representative, being diverse, and being computationally feasible.\n\nTable 1 shows samples of generated texts for GPT2 (9M) and Llama2 (125M) models at model-fitting iterations 3-5 when both accumulating and replacing data, as well as iterations 8-10 (replacing only).\n\nWe ablate for several additional potential confounds beyond generation temperature. First, when accumulating data, subsequent model iterations are trained on larger datasets than when replacing data. To control for this, we also perform experiments in which data is replaced, but the size of the (fully synthetic) dataset is grown to match the training set size in the accumulation regime. We find that model performance still degrades (albeit at a lower rate). This is shown in Appendix C, Table 2, right-most column.\n\nSecond, a possible concern could be that degrading performance when replacing data could be due to low model performance in iteration 1 (and thus the quality of the first synthetic dataset). We control for this by varying the amount of training performed in iteration 1 only and find that this has no significant impact.\n\nLastly, we find that our results are also consistent across varying dataset sizes and training epochs. These ablations are discussed in Appendix F."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Diffusion Models on Molecular Conformation Data",
            "text": "Experiments We next train sequences of diffusion models on molecular conformation data. Specifically, we train GeoDiff (Xu et al., 2022), a geometric diffusion model for molecular conformation generation, on the GEOM-Drugs (Axelrod & Gomez-Bombarelli, 2022) dataset. We down-sample the training split of GEOM-Drugs to molecular conformations, which we use as our initial training set, and perform diffusion steps for each prediction. For the loss, we use the standard loss used by GeoDiff: a weighted variational lower bound to the conditional likelihood; for more details, see Xu et al. (2022)."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Variational Autoencoders on Image Data",
            "text": "We lastly train sequences of variational autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) on CelebA (Liu et al., 2015), a dataset of 200k images of human faces, chosen as a balance between being a realistic dataset with many samples, color images and resolution, and computational feasibility of training multiple iterations of models on accumulating data. The loss is the standard VAE loss: reconstruction error plus the KL divergence between the encoder\u2019s output Gaussian and the isotropic Gaussian prior. See Appendix D for more experimental details.\n\nWe find that replacing data at each iteration again exhibits model collapse, and each iteration yields lower quality and less diverse generated faces until all model generations represent a single mode as shown in the left panel of Figure 6. In contrast, accumulating data at each iteration significantly slows model collapse. While the diversity of generations does go down as compared in the middle and right panel of Fig. 6, it still represents major axes of variation in the dataset, such as gender, but no longer seems to generate other details, along more minor axis of the data manifold, such as glasses and accessories. We discuss further analysis of VAE reconstructions in Appendix D.\n\nInterestingly, Mart\u00ednez et al. (2023a) found slightly contradictory evidence, specifically that a different architecture on a much smaller dataset exhibits fast performance deterioration even with accumulating data. Understanding under what conditions and why these discrepancies exist is an interesting direction we leave for future research."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Accumulating Data Avoids Model Collapse in Linear Models",
            "text": "To gain mathematical understanding and intuition, we employ an analytical framework introduced in prior work (Mobahi et al., 2020; Dohmatob et al., 2024a) to understand the difference between data accumulation and data replacement. The framework considers a sequence of linear models that are fit to the synthetic data sampled from the linear generative model based on the previously fit linear models.\n\nThe content in this section relies heavily on the framework and pioneering contributions of Dohmatob et al. (2024a). Our contribution is to study a different way to use synthetic data in training, namely accumulate, which seems to better align with certain real-world considerations. We use the same framework to analyze some other ways that synthetic data might have been used, such as replace.\n\nWe adapt notations from Dohmatob et al. (2024a). Define the distribution on given by:\n\nThe positive integer is the input-dimension, the matrix is the true covariance structure of the input, the vector is the true linear relationship used to generate the original data and the scalar is the level of label noise. We start at iteration with initial independent data points each following , that is, for each. We form the design matrix with as rows. We also form the vectors and with -th coordinate and respectively. In whatever follows, we will assume that has full column rank, i.e., is invertible and the model is underparameterized.\n\nWe generate synthetic data from the following sequence of distributions where is the number of iterations. The scheme is outlined as follows.\n\nFor :\nAccumulating Covariates/Features:\nAccumulating Targets: , where\nFit linear model:\nSample synthetic data for the next iteration: , where\n\nFor :\nAccumulating Covariates/Features:\nAccumulating Targets:\nFit linear model:\nSample synthetic data for the next iteration: , where\n\nHere, for a matrix with full column rank, is the Moore-Penrose pseudo-inverse of. The noise terms are independent of each other and of the covariates/features. Since has full column rank, so does for every.\n\nTo reiterate a comment made previously by Dohmatob et al. (2024a), although we present our results in the context of ordinary linear regression, our analysis can be readily extended to ridge regression and the kernel setting (Caponnetto & De Vito, 2007; Simon et al., 2021; Cui et al., 2021; Wei et al., 2022). We focus here on a simple useful model for studying model collapse."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Notation and Preliminaries",
            "text": "We adapt notations from Dohmatob et al. (2024a). Define the distribution  on  given by :\nThe positive integer  is the input-dimension, the matrix  is the true covariance structure of the input , the vector  is the true linear relationship used to generate the original data and the scalar  is the level of label noise. We start at iteration  with  initial independent data points  each following , that is,  for each . We form the design matrix  with  as rows. We also form the vectors  and  with -th coordinate  and  respectively. In whatever follows, we will assume that  has full column rank, i.e., ,  is invertible and the model is underparameterized.  \nWe generate synthetic data from the following sequence of distributions where  is the number of iterations. The scheme is outlined as follows.  \nFor :  \nAccumulating Covariates/Features:  \nAccumulating Targets: , where  \nFit linear model:  \nSample synthetic data for the next iteration: , where  \nFor :  \nAccumulating Covariates/Features:  \nAccumulating Targets:  \nFit linear model:  \nSample synthetic data for the next iteration: , where  \nHere, for a matrix  with full column rank,  is the Moore-Penrose pseudo-inverse of . The noise terms  are independent of each other and of the covariates/features. Since  has full column rank, so does  for every .  \nTo reiterate a comment made previously by Dohmatob et al. (2024a), although we present our results in the context of ordinary linear regression in , our analysis can be readily extended to ridge regression and the kernel setting (Caponnetto & De Vito, 2007; Simon et al., 2021; Cui et al., 2021; Wei et al., 2022). We focus here on a simple useful model for studying model collapse."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Precise Test Error Characterization Under Accumulating Data",
            "text": "Our goal is to establish an analytic formula for the parameters of the model in the data accumulation setting. We begin by characterizing the relationship between the fitted linear parameters and the true parameters. We remind the reader that we assume that the design matrix has full column rank, i.e., it is invertible. Proofs are deferred to App. B.\n\nIn the data accumulation setting, the fitted linear parameters can be expressed as:\n\\[ \n\\hat{\\beta}^{(t)} = \\beta + (X^T X)^{-1} X^T \\epsilon^{(t)}\n\\]\nwhere, recall, \\(\\beta\\) is the true parameter, \\(X\\) is the original design matrix, and \\(\\epsilon^{(t)}\\) is the extra noise added at the \u2019th iteration.\n\nFor an -fold synthetic data generation process with \\(m\\) samples per iteration and isotropic features \\(\\Sigma^x = \\sigma_x^2 I_d\\), the model can be analyzed under these conditions. \n\nThis difference can be intuitively explained by the differences in the way data are handled across iterations. In the data replacement setting, because previous data were discarded, the model is more strongly affected by the new noise that each iteration of generated data introduces, and adds that to the effects experienced in earlier iterations. But in the data accumulation setting, because iteration \\(t\\) contributes fraction \\(\\frac{1}{t}\\) to the training dataset, the additional noise from the \\(t\\)th iteration of synthetic data has its effect on the model MSE shrunken proportional to \\(1/t^2\\) (due to squared error). The summability of \\((1/t^2)\\) ensures stability in the model training process. This suggests that accumulating generated data with real data can indeed avoid model collapse."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Numerical Confirmation of Analytical Results",
            "text": "To confirm the analytical results, we numerically simulate the setup. The numerics almost perfectly matched the analytics (Fig. 7 ###reference_###): when data are replaced, the results grow with the number of iterations, with the prefactor set by the noise-to-signal ratio, but when data accumulate, the results rapidly plateau with the prefactor similarly set. For further details and higher model-fitting iterations, see Appendix Fig. 16 ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "This work explored the phenomenon of model collapse, an important concern as AI-generated content permeates the internet and finds its way into future training datasets. Prior work has shown that training on model outputs can lead to degraded performance (Mart\u00ednez et al., 2023a  ###reference_b18###; b  ###reference_b19###; Shumailov et al., 2023  ###reference_b27###; Alemohammad et al., 2023  ###reference_b2###; Hataya et al., 2023  ###reference_b12###; Bertrand et al., 2023  ###reference_b4###; Briesch et al., 2023  ###reference_b5###; Dohmatob et al., 2024a  ###reference_b9###; b  ###reference_b10###), implying that future model training faces a difficult challenge of ensuring strict training dataset hygiene.\n\nOur findings extend these prior works to show that if data accumulates and models train on a mixture of \u201creal\u201d and synthetic data, model collapse no longer occurs. We show this both experimentally on causal transformers for language modeling, diffusion models for molecule generation, and variational auto-encoders on image data as well as theoretically for linear regression. Together, these results strongly suggest that the \u201ccurse of recursion\u201d may not be as dire as had been portrayed \u2013 provided we accumulate synthetic data alongside real data, rather than replacing real data by synthetic data only.\n\nLooking to the future, many questions worth investigating remain. For instance, in future work we would like to explore different data generation and accumulation regimes, such as (1) additional \u201creal\u201d data being introduced in each model-fitting iteration and (2) different schedules of how much synthetic data is generated at each iteration and (3) human-filtering of generated data, e.g., as done in RLHF. Additionally, we note that in all our experiments, the synthetic dataset is generated by sampling from the previous model, i.e., with some stochasticity; in future work, we would like to explore also what happens if data is generated deterministically, e.g. with temperature 0 in a typical language model.\n\nLastly, it is worth noting that \u201cmodel collapse\u201d \u2013 as a term of art \u2013 has been used in various ways by various researchers; so care is required in comparing claims across articles. In reviewing the literature, we identified at least four related phenomena: (1) modal collapse \u2014 collapse to one (or a few) modes; (2) collapse to uniformity; and (3) amplification of artifacts introduced by models fit to previous synthetic data. Future work should map out what factors cause which to occur and what preventative strategies are effective at addressing each."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Acknowledgements",
            "text": "The content of this paper does not necessarily reflect the position or the policy of any of the funding agencies/entities. No endorsement should be inferred. M.G. acknowledges support through a grant from the Cooperative AI Foundation. R.S. acknowledges support from Stanford Data Science and from OpenAI\u2019s Superalignment Fast Grant Research Fellowship. A.G. acknowledges support from the NSF CAREER grant DMR-2045181, the Sloan Foundation, and by the Laboratory\nfor Physical Sciences through the Condensed Matter Theory Center.\nD.R. acknowledges support from the National Science Foundation under Cooperative Agreement PHY-2019786 (the NSF AI Institute for Artificial Intelligence and Fundamental Interactions  ###reference_aifi.org/###) and appreciates both the sanction and support of Sequoia Capital.\nS.K. is partially supported by NSF III 2046795, IIS 1909577, CCF 1934986, NIH 1R01MH116226-01A, NIFA award 2020-67021-32799, the Alfred P. Sloan Foundation, and Google Inc."
        }
    ],
    "url": "http://arxiv.org/html/2404.01413v2",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "2.1",
            "2.2",
            "2.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2",
            "2.1",
            "2.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.01413v2",
        "paper_title": "Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data",
        "research_background": "### Paper's Motivation\n\nThe motivation of the paper is driven by the potential risk associated with the increased use of large-scale generative models. These models, such as GPT-4, DALL-E, and Stable Diffusion, generate vast amounts of synthetic data that end up circulating on the web. A significant concern arises from the possibility of training future generative models on datasets containing their own generated outputs, potentially leading to a downside termed \"model collapse.\" This phenomenon presents a critical challenge in maintaining the robustness and accuracy of generative models as they are increasingly applied and developed.\n\n### Research Problem\n\nThe main research problem addressed in this paper is to understand and mitigate the phenomenon of model collapse that occurs when generative models are trained iteratively on datasets containing their own synthetic outputs. Traditional approaches have shown that replacing training data with synthetic data across iterations leads to progressive degradation in model performance. The paper seeks to investigate whether accumulating data \u2014 both real and synthetic \u2014 rather than replacing it, could prevent or mitigate model collapse.\n\n### Relevant Prior Work\n\nThe paper builds on a body of prior work that has observed and analyzed model collapse:\n1. **Initial Observations and Analysis**: Previous studies have identified and demonstrated the model collapse phenomenon, showing that models iteratively trained on their own outputs suffer degrading performance (Hataya et al., 2023; Dohmatob et al., 2024a; Mart\u00ednez et al., 2023a; Alemohammad et al., 2023; Bertrand et al., 2023).\n\n2. **Data Replacement**: A conventional setup in prior research assumes that each generation of models replaces previous training data with newly generated synthetic data. This setup has been criticized for not accurately reflecting real-world data dynamics, where the volume of data tends to increase over time (Touvron et al., 2023a; Touvron et al., 2023b).\n\n3. **Mixed Data Approaches**: Some prior works pointed out that model collapse could be slowed or even negated by integrating clean (real) data with generated (synthetic) data during the training process (Hataya et al., 2023; Mart\u00ednez et al., 2023a; Alemohammad et al., 2023).\n\n4. **Analytical Framework**: Theoretical analyses on synthetic data replacement (Mobahi et al., 2020; Dohmatob et al., 2024a) have shown that test error worsens linearly with iteration. This work extends these models to an accumulation scenario, showing that the test error remains finite and controlled when data are accumulated rather than replaced.\n\nBy addressing these gaps and building on these insights, the paper aims to show that accumulating both real and synthetic data, rather than replacing data with each generation, might provide a robust solution to avoid model collapse.",
        "methodology": "## Methodology\n\nThis section explores methods to mitigate model collapse when training generative models. Model collapse is defined as deteriorating performance over successive iterations in a model-data loop, while avoiding model collapse refers to maintaining performance within an acceptable error range. The study compares two strategies: **accumulating data** and **replacing data** to understand their effects on model collapse.\n\n**Experimental Setup**:\n- **Models and Data**: Three types of generative models are investigated\u2014causal transformers, diffusion models, and variational autoencoders (VAEs). These models are trained on different real-world datasets including text, molecular conformations, and images.\n- **Data Handling**: Each iteration either substitutes the new synthetic data for the old data (**replacing data**) or combines it with the old data (**accumulating data**).\n\n1. **Causal Transformers**:\n   - **Models**: GPT-2 and Llama2 with different parameter counts (9M, 12M, 42M, 125M).\n   - **Dataset**: TinyStories, a 470M token dataset of short stories at a kindergarten reading level.\n   - **Training**: Models are pretrained for a single epoch, iteratively sampling new datasets derived from the synthetic data generated in the previous iteration.\n   - **Sampling Temperature**: New datasets are generated using temperatures 0.3 and 1.0 to study the effects of sampling diversity.\n   - **Results**: Replacing data results in increasing test cross-entropy, indicating model performance degradation. Accumulating data results in either stable or decreasing test cross-entropy.\n\n2. **Diffusion Models**:\n   - **Experiments** similar to the causal transformers were carried out but specifics on the exact dataset and model configurations for these experiments are not provided here.\n\n3. **Variational Autoencoders (VAEs)**:\n   - **Model**: Standard VAE architecture.\n   - **Dataset**: CelebA dataset containing 200k images of human faces.\n   - **Training Loss**: Consists of reconstruction error and the Kullback-Leibler (KL) divergence between the encoder's output and an isotropic Gaussian prior.\n   - **Results**: Replacing data leads to rapid model collapse, with test error increasing and generated image diversity decreasing, converging to a single mode. Accumulating data results in slower test error increase and maintains some diversity (e.g., gender variation) although minor details like glasses and accessories are not consistently generated.\n\n**Noteworthy Findings**:\n- Accumulating data is more effective in maintaining model performance compared to replacing data, albeit with a slower but eventual increase in test error in VAEs compared to language models.\n- Indrawing the nuances, **Mart\u00ednez et al. (2023a)** highlighted scenario-specific discrepancies in VAE performance decline, motivating future exploration into these inconsistencies.\n\nOverall, accumulating synthetic data while training generative models presents a viable strategy to mitigate model collapse across text, image, and molecular data tasks, though further research is needed for a complete understanding under different conditions.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nIn this paper, we conducted a main experiment where we trained causal transformers, specifically the GPT-2 model (Radford et al., 2019) and several versions of the Llama2 model (12M, 42M, and 125M parameters) (Touvron et al., 2023b). These models were pretrained for a single epoch on the TinyStories dataset (Eldan & Li, 2023), which consists of 470M tokens of GPT-3.5/4-generated short stories aimed at a kindergarten reading level.\n\nDuring the training process, for each model-fitting iteration, we generated a new dataset of the same size as TinyStories from the language model trained in the previous iteration. We then either replaced the old dataset with the newly generated one or concatenated the new dataset with the old one. We experimented with sampling new datasets using temperatures of 0.3 and 1.0.\n\n### Evaluation Metrics\n\nThe primary evaluation metric used was the test cross entropy, which measures how well the model predicts a sample.\n\n### Results\n\n1. **Replacing Data**:\n   - We observed that for all architectures, parameter counts, and sampling temperatures, replacing data led to an increase in test cross entropy as the number of model-fitting iterations increased. This suggests that the model's performance degraded over successive iterations when the dataset was replaced.\n\n2. **Accumulating Data**:\n   - In contrast, accumulating data over iterations resulted in either equal or lower test cross entropy, indicating better or stable model performance with each iteration.\n\n3. **Effect of Temperature**:\n   - Lower sampling temperature (0.3) induced a faster increase in test error compared to higher temperature (1.0), although the degradation trend was evident for both temperatures.\n\nOverall, the main experiment demonstrated that accumulating data across model iterations helps in maintaining or improving model performance, whereas replacing data consistently leads to a decline in model performance, highlighting the challenges and considerations when updating training datasets in successive iterations."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To understand how accumulating data affects model collapse in generative models and compare this with the effect of replacing data.",
            "experiment_process": "Experiments were conducted using causal transformers, diffusion models, and variational autoencoders trained on real text, molecular conformation, and image datasets. For causal transformers, the study pretrained GPT-2 and Llama2 models on the TinyStories dataset, sampling new datasets from the previous iteration's model and either replacing or concatenating the previous dataset with the new one. Variational autoencoders were trained on the CelebA dataset, with the VAE loss calculated as the reconstruction error plus the KL divergence. Multiple ablation studies were conducted to control for confounding factors like dataset size and sampling temperature.",
            "result_discussion": "Replacing data led to model collapse in all cases, characterized by an increase in test cross-entropy or test error with each iteration. Accumulating data, however, resulted in equal or lower test cross-entropy for causal transformers and a slower increase in test error for VAEs. Notably, the study found that accumulating data slowed the degradation process compared to replacing data, but issues like lower diversity in generated faces were still observed. These results held across different architectures, parameter counts, and sampling temperatures.",
            "ablation_id": "2404.01413v2.No1"
        },
        {
            "research_objective": "To investigate the effect of accumulating versus replacing data on causal transformer-based language models in terms of model collapse.",
            "experiment_process": "Causal transformers, including 9M parameter GPT-2 and Llama2 models with various parameter counts (12M, 42M, 125M), were pretrained on the 470M token TinyStories dataset. In each model-fitting iteration, either the previous dataset was replaced or concatenated with a newly generated dataset of the same size. The effect of sampling temperature was also evaluated. Multiple iterations of model pretraining were carried out, and additional ablation studies were performed to address potential confounds such as dataset size and initial model performance.",
            "result_discussion": "Replacing data consistently led to an increase in test cross-entropy as the number of iterations increased. In contrast, accumulating data resulted in equal or lower test cross-entropy, regardless of the architecture, parameter count, or sampling temperature. Lower temperatures led to a faster increase in test error, but the overall trend held consistent. The study also controlled for confounding factors and confirmed that results were consistent across varying dataset sizes and training epochs.",
            "ablation_id": "2404.01413v2.No2"
        },
        {
            "research_objective": "To assess the impact of data accumulation versus data replacement on the performance of variational autoencoders trained on image data.",
            "experiment_process": "Sequences of VAEs were trained on the CelebA dataset comprising 200k images of human faces, using the standard VAE loss (reconstruction error plus KL divergence). The experiments compared replacing the dataset with each iteration versus accumulating data over iterations. Control experiments were referenced for further details, including potential confounding factors.",
            "result_discussion": "When data were replaced at each iteration, the test error rose significantly with each additional iteration, and generated images showed lower quality and less diversity. Accumulating data slowed this degradation significantly, although the test error still increased but at a much slower pace. While the diversity of generated images decreased, major axes of variation in the dataset (like gender) were still represented. However, this approach did not prevent all forms of degradation in output quality, and future work is needed to understand the conditions under which accumulating data can completely mitigate model collapse.",
            "ablation_id": "2404.01413v2.No3"
        }
    ]
}