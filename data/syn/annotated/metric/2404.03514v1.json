{
    "title": "Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach",
    "abstract": "Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks. Despite their great success, the knowledge provided by the retrieval process is not always useful for improving the model prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question without retrieval. Aiming to save the cost of retrieval, previous work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs\u2019 pretraining data. However, these data-aware methods pose privacy risks and memory limitations, especially when requiring access to sensitive or extensive pre-training data. Moreover, these methods offer limited adaptability under fine-tuning or continual learning settings.\n\nWe hypothesize that token embeddings are able to capture the model\u2019s intrinsic knowledge, which offers a safer and more straightforward way to judge the need for retrieval without the privacy risks associated with accessing pre-training data. Moreover, it alleviates the need to retain all the data utilized during model pre-training, necessitating only the upkeep of the token embeddings.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Retrieval-augmented large language models (RALMs) excel in various NLP tasks. However, the knowledge provided by the retrieval process is not always useful for improving the LLMs\u2019 prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question correctly without retrieval. For example, when posed with commonsense questions or queries within the knowledge scope of their pre-training data, LLMs might respond without necessitating retrieval. Moreover, the retrieval process can incur additional computational costs and latency, which could be avoided when the model\u2019s intrinsic knowledge has already been adequate.\n\nPrevious work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs\u2019 pretraining data. Such a data-aware approach is developed based on the heuristic that it is easier for LLMs to capture knowledge on entities that are frequently mentioned during pre-training. This adaptive approach can save context length, thereby reducing latency and cost during LLM inference, while also mitigating performance degradation caused by redundant retrievals in LLMs.\n\nHowever, the pre-training data might not always be available due to privacy and commercial constraints, especially when dealing with proprietary or sensitive datasets. This makes it infeasible to utilize the data-aware approaches in real business scenarios. In addition, the pre-training data are not necessarily aligned with the knowledge learned by LLMs. For example, the pre-training datasets may contain conflicting descriptions regarding the same entity. In such a case, it is uncertain whether the model is knowledgeable about the entity, even if it has been frequently mentioned in the pre-training data.\n\nIn this paper, we propose a novel model-aware approach to make the judgment about when to do/skip the retrieval. Instead of requiring access to the pre-training data, we leverage the pre-trained token embeddings that are believed to explicitly reflect the model\u2019s knowledge. In achieving this, we develop a simple yet effective representation-informed classifier that is capable of recognizing samples that are (not) in need of retrieval. This approach circumvents the risks associated with maintaining pre-training data via only requiring access to the pre-trained token embeddings, offering a safer and more straightforward way to judge the need for retrieval augmentation.\n\nIn summary, the main contributions of this work are as follows: We identify the privacy constraints inherent in Retrieval-augmented LLMs, and unveil the limitations of the existing data-aware approach. We introduce a novel model-aware approach that decides when to do/skip the retrieval process, by leveraging the token embeddings intrinsic to the model. This approach alleviates the dependency on the accessibility of pretraining data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Model-Aware Adaptive Retrieval Augmentation",
            "text": "In the context of open-domain entity-centric Question Answering (QA), the primary objective of the RALM method is to ascertain whether a given entity requires retrieval augmentation when the QA system is posed with a specific entity-centric question (e.g., \u2018Louisiana\u2019 is the entity of the question \u2018What is the capital of Louisiana?\u2019). The core of this task is to determine whether language models already possess knowledge of the entity, deciding if there is a need to retrieve external knowledge bases to enhance the model prediction. This adaptive retrieval approach can effectively save context length, thereby reducing latency during LLM inference and mitigating performance degradation caused by redundant retrievals in LLMs.\n\nOur rationale for utilizing entity embeddings as an indicator of an LLM\u2019s knowledge about an entity is grounded in extensive prior research. This research has established a significant correlation between entity embedding distribution and entity frequency in pre-training data across various models, from BERT to the GPT series. Entity embeddings have shown effectiveness for retrieval augmentation decisions. We developed an NN-classifier-based method, aiming to parallel the DM method. This classifier aids in determining when an entity requires retrieval augmentation based on its embedding characteristics.\n\nTo ensure clarity, we define the set of entities within the dataset; a specific entity, with denoting its index in the set; the tokenized representation of the entity using the GPT/Llama2 tokenizer; the first-layer token embedding of the tokenized entity; a neural network classifier; the binary outcome (indicating the need for retrieval augmentation).\n\nGiven an entity from the set, we tokenize it using the LLM\u2019s tokenizer (e.g., GPT-Neo/Llama2 tokenizer) to obtain its tokenized form. Subsequently, we extract the first-layer token embedding, which we hypothesize encapsulates information related to the entity\u2019s frequency.\n\nIn alignment with previous work, we curate a subset by randomly sampling the entity-centric data from every sub-relation dataset. Each entity is converted to its respective embedding and associated retrieval label (retrieve or not). These serve as training data for the neural network classifier.\n\nAfter the training of the classifier, we employ it to predict the binary outcome when presented with a new entity. This prediction assists in determining whether the entity requires retrieval augmentation for open-domain entity-centric QA tasks.\n\nOur novel model-aware retrieval augmentation method offers an efficient way to determine the need for retrieval augmentation in open-domain entity-centric QA scenarios. In contrast to the data-aware method requiring the availability of the pre-training data, our method focuses on the analysis of entity token embeddings, ensuring applicability and scalability in real-world QA systems."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Following the experiment setting as Mallen et al. (2023), we choose POPQA, an entity-centric open-domain QA dataset. We have the following research questions (RQs) to guide the experiments:  \nRQ2: Regarding the adaptability of our method, when an LLM is fine-tuned, with modified memorization capacity of entities, can our model determine the instances of entity necessitating retrieval?  \n\nIn this section, we will perform an extensive experimental analysis of our model-aware framework."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Model-aware vs. Data-aware QA Strategies Across Diverse Model Capacities (RQ1)",
            "text": "Given that the data-aware method Mallen et al. (2023) requires access to pre-training data, it cannot be compared in the privacy-preserving setting. To maintain consistency with the data-aware method, we evaluate our framework across models of varying capacities: GPT-Neo (1.3 billion), GPT-Neo (2.7 billion), Llama 2 (7 billion), and Llama 2 (13 billion). In this evaluation, we do not perform any fine-tuning on models.\n\nWe utilize the POPQA dataset Mallen et al. (2023), which comprises 14k questions designed for capturing factual details possibly overlooked in more mainstream QA datasets. More details refer to Appendix B.\n\nThe data-aware method essentially requires calculating word frequency for all entities within each sub-dataset (e.g., genre dataset) in the pre-training data. In contrast, our model-aware technique trains an NN classifier directly using the token embeddings of entities. This eliminates the need to access/interact with any pre-training data.\n\nTo delve deeper into the discussion of which scenario our model-aware or data-aware method performs better, we have chosen the most representative sub-datasets: \u2018mother\u2019 and \u2018capital of\u2019, and visualized them. The vertical axis, labeled \u2018Normalized Entity Token logits\u2019, represents the normalized neural network output for each entity. The horizontal axis labeled \u2018True\u2019 represents the logits distribution of the Llama 2 for the correct QA samples, while \u2018False\u2019 represents the logits distribution for the incorrect samples of the QA. From the violin plot, it can be observed that the logits distribution for the \u2018mother\u2019 dataset is relatively concentrated, whereas the \u2018capital of\u2019 logits distribution may exhibit a bimodal nature, indicating two primary prediction categories. Meanwhile, we can observe that the \u2018capital of\u2019 dataset exhibits a distinct peak in entity frequency. This suggests that LLMs are more likely to encounter and learn from these high-frequency entities during the pre-training phase. In contrast, the \u2018mother\u2019 dataset might encompass more ambiguous or unspecified entities, making methods based on model-aware embedding challenging to distinguish on such datasets.\n\nBuilding on data-aware model\u2019s configurations Mallen et al. (2023), we also incorporate two prominent retrieval systems into our research: BM25 Robertson and Zaragoza (2009) and Contriver Izacard et al. (2022). While BM25 is a static, term-based retriever that functions without prior training, Contriever undergoes pre-training on vast unlabeled corpora and is subsequently refined using the MS MARCO dataset Nguyen et al. (2016). We also delve into the GenRead Yu et al. (2023) parametric augmentation technique, prompting language models to produce contextual documents rather than exclusively depending on retrieval for responses. \n\n\u2018RA Llama 2\u2019 connotes the integration of retrieval-based augmentation for all QA queries. \u2018DM\u2019 Mallen et al. (2023) stands for the adaptive data-aware method, while \u2018MM\u2019 signifies our model-aware method which does not need to access pretraining data. It is worth highlighting that our model-aware approach yields competitive results across various retrieval-augmentation configurations."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Adaptability of the Model-aware Method in Fine-tuned LLMs (RQ2)",
            "text": "To evaluate our model-aware method\u2019s adaptation performance, we fine-tune a GPT-Neo 1.3B model. Specifically, from the POPQA dataset, we extract 70% of the questions related to entities from each sub-dataset for the purpose of fine-tuning the GPT-Neo model, aiming to enhance the model\u2019s retention of this specific knowledge. The remaining 30% are utilized as a test set to compare the effectiveness of DM and MM. For DM, entity frequencies were calculated using their original Wiki frequency.\n\nThis is because methods requiring fine-tuning, like the DM method, would cause a dramatic change in the entity frequency since there typically exists a notable difference between the entity frequencies of the pre-training data and the fine-tuning data. Therefore, DM would exhibit less stability compared to MM which does not directly rely on the frequency information, especially in the situations where multiple rounds of fine-tuning are required.\n\nGiven the prevailing trend of extensively fine-tuning large language models for specific downstream tasks, this stability and adaptability highlight a distinct advantage of our method."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce a novel model-aware approach to tackle challenges in retrieval-augmented LLMs. Leveraging token embeddings that capture the model\u2019s knowledge, we offer an efficient and privacy-conscious solution. Unlike methods dependent on inaccessible or sensitive pretraining data, our approach provides a flexible, scalable, and secure means to assess retrieval requirements. This innovation has broad implications for real-world applications, harmonizing efficiency and privacy while upholding model output quality."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "This work focuses on an entity-centric adaptive retrieval-augmentation technique. It might not work on document-centric QA tasks. We acknowledge the need for future research to explore the extension of our method to a wider range of QA tasks. Besides, how to particularly improve the performance of the retrieval model is beyond the scope of our paper, and has yet to be explored."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Ethics Statement",
            "text": "It is important to note that LLMs can still generate incorrect (hallucination) or biased outputs, even when they are retrieval-augmented. Therefore, it is always important to verify the outputs of language models with other sources of information."
        }
    ],
    "url": "http://arxiv.org/html/2404.03514v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.03514v1",
        "paper_title": "Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach",
        "research_background": "### Paper's Motivation\nThe paper is motivated by the need to optimize the performance of Retrieval-augmented Large Language Models (RALMs) in various NLP tasks while addressing privacy and computational concerns. Although RALMs can significantly enhance the prediction capabilities of LLMs through retrieval processes, this enhancement is not always necessary. For example, when dealing with commonsense questions or queries well within the knowledge scope of their pre-training data, LLMs might accurately respond without retrieval. Moreover, the retrieval process incurs extra computational costs and latency, which can be avoided if the model's intrinsic knowledge is adequate.\n\n### Research Problem\nThe main research problem tackled in this paper is the challenge of determining when a retrieval process is necessary for enhancing the performance of RALMs, especially in situations where access to pre-training data is restricted due to privacy and commercial constraints. This challenge is compounded by the fact that pre-training data might not always be aligned with the knowledge learned by the LLMs and can contain conflicting information about the same entity.\n\n### Relevant Prior Work\n1. **Data-Aware Approaches**: Previous works such as Mallen et al. (2023) have experimented with data-aware methods to decide when to perform retrieval by analyzing the LLMs' pre-training data. These approaches are based on the heuristic that models are more likely to have captured knowledge on entities that are frequently mentioned in the pre-training data.\n   \n2. **Challenges with Pre-training Data**: Prior studies, including those by Shao et al. (2023) and Huang et al. (2022), have highlighted the privacy and commercial constraints which make it infeasible to utilize pre-training data in real-world scenarios. Moreover, Gu et al. (2023) have discussed the issue of conflicting descriptions within pre-training datasets, which raise uncertainties about the model's actual knowledge.\n\n### Proposed Approach\nThe paper proposes a novel model-aware method, which leverages pre-trained token embeddings to determine when retrieval is needed, instead of relying on pre-training data. This method circumvents the issues related to privacy and data alignment, offering a safer and more straightforward way to judge the necessity of retrieval augmentation. The introduction of a representation-informed classifier capable of recognizing when samples need retrieval stands as the core innovation of this paper.\n\n### Contributions\n1. **Identification of Limitations**: The paper identifies the privacy constraints and limitations inherent in existing data-aware approaches.\n2. **Model-Aware Approach**: It introduces a model-aware method to decide when to skip or perform retrieval, reducing dependency on pre-training data by leveraging intrinsic token embeddings.\n3. **Empirical Validation**: Through extensive experiments and analyses, the paper demonstrates the superiority of the model-aware approach over the data-aware baseline.",
        "methodology": "### Methodology\n\nThe RALM (Retrieval Augmented Language Model) method is tailored for open-domain entity-centric Question Answering (QA). The primary goal is to determine whether a specific entity requires additional information retrieval to answer a posed question accurately (e.g., determining if extra information is needed to answer \"What is the capital of Louisiana?\" when 'Louisiana' is the entity in question).\n\nThe approach hinges on understanding whether language models (LLMs) already contain knowledge about the entity in question. If an LLM is found to possess sufficient knowledge of the entity, external information retrieval can be bypassed, which optimizes context length and reduces latency during inference. This also mitigates any potential performance degradation due to unnecessary retrievals, as noted by Mallen et al. (2023).\n\n#### Key Components and Innovations:\n\n1. **Entity Embeddings as Knowledge Indicators**:\n    - Previous research (Gao et al., 2019; Li et al., 2020; Cai et al., 2021; Mallen et al., 2023) has shown a significant correlation between entity embedding distributions and their frequency in pre-training data. \n    - Utilizing entity embeddings, RALM can effectively gauge whether to proceed with retrieval augmentation based on how frequently entities appeared in pre-training data.\n\n2. **NN-Classifier-Based Method**:\n    - A neural network (NN) classifier is introduced to parallel the decision-making (DM) method from Mallen et al. (2023).\n    - This classifier helps determine if an entity needs retrieval augmentation by analyzing its embedding characteristics.\n\n3. **Tokenization and Layer Extraction**:\n    - For a given entity \\( e_i \\) from the dataset \\( E \\), the entity is tokenized using the LLM\u2019s tokenizer (e.g., GPT-Neo/Llama2 tokenizer) to produce \\( t_i \\).\n    - The first-layer token embedding \\( v_i \\) of the tokenized entity \\( t_i \\) is then extracted, which hypothetically encapsulates the entity's frequency information.\n\n4. **Training Data Preparation**:\n    - A subset \\( S \\) is curated by randomly sampling entity-centric data from various sub-relation datasets.\n    - Each entity \\( e_i \\) within \\( S \\) is transformed into its respective embedding \\( v_i \\) and paired with a retrieval label \\( y_i \\) (retrieve or not), creating training data for the NN classifier \\( C \\).\n\n5. **Prediction and Decision Making**:\n    - After training the classifier \\( C \\), it can predict the binary outcome \\( y' \\) for any new entity \\( e' \\).\n    - This prediction determines whether the entity \\( e' \\) necessitates retrieval augmentation for effective open-domain entity-centric QA tasks.\n\n#### Novelty and Efficiency:\n- The model-aware retrieval augmentation method offers an innovative way to decide on the necessity of retrieval augmentation.\n- Unlike data-aware methods, which depend on the availability of pre-training data, this method analyzes entity token embeddings.\n- This approach promises accurate decision-making applicable and scalable in real-world QA systems.\n\nIn conclusion, the RALM method leverages entity embeddings and a neural network classifier to intelligently decide when retrieval augmentation is necessary, thus enhancing efficiency and performance in open-domain entity-centric QA systems.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Experiment Setup:**\nFollowing the experiment setting by Mallen et al. (2023), the main experiment utilizes POPQA, an entity-centric open-domain QA dataset. The primary objectives are aligned with two main research questions:\n\n- **RQ1**: Assessing the accuracy of the proposed model under privacy-related constraints compared to the state-of-the-art methods. Notably, while state-of-the-art (SOTA) methods require access to the pre-training data, for a fair comparison, we assume these data-aware methods have access to the frequency of the pre-training data.\n- **RQ2**: Evaluating the adaptability of the proposed method, specifically when a language model (LLM) is fine-tuned, which alters the memorization capacity of entities. The focus is on determining if the model can accurately identify instances that require entity retrieval.\n\n**Baselines:**\nThe experiment compares the proposed model with other state-of-the-art methods that also handle privacy-centric constraints in language models. The specific baselines from the study of Mallen et al. (2023) would include their methodology and other current leading approaches in entity-centric open-domain QA.\n\n**Evaluation Metrics:**\nAccuracy is the primary evaluation metric. A prediction is considered correct if any substring of the prediction exactly matches any of the gold answers. This strict criterion ensures that the evaluation is precise in terms of entity retrieval accuracy.\n\n**Main Experimental Results:**\nWhile the specific results are not provided in the given excerpt, we can infer that the results section would offer an extensive analysis of the accuracy performance:\n- Comparisons between the proposed model and the state-of-the-art methods under privacy constraints.\n- Assessment of the model's adaptability when the memorization capacity of entities is modified through LLM fine-tuning.\n- Detailed accuracy measurements, considering exact substring matches between predictions and gold answers. \n\nThis part of the study aims to validate the effectiveness of the proposed privacy-centric, adaptive model-aware approach in an entity-centric open-domain QA setting."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the model-aware method against the data-aware method across models with varying capacities to establish the effectiveness of the former in a privacy-preserving setting.",
            "experiment_process": "We evaluated our model-aware approach using models GPT-Neo (1.3 billion), GPT-Neo (2.7 billion), Llama 2 (7 billion), and Llama 2 (13 billion) without fine-tuning. The POPQA dataset was used as it comprises 14k questions that capture factual details. Evaluation involved comparing our method with the data-aware method that calculates word frequency for entities within sub-datasets. The data-aware method was employed with two retrieval systems: BM25 and Contriver; and the GenRead parametric augmentation technique. The performance of these methods on Llama 2-7B was compared.",
            "result_discussion": "The model-aware method consistently matched or outperformed the data-aware method, suggesting it is a viable alternative. The \u2018mother\u2019 dataset showed a concentrated logits distribution, making embedding methods less effective. However, the \u2018capital of\u2019 dataset exhibited a bimodal distribution, suggesting high-frequency entities that LLMs learn from. The model-aware approach was more stable and adaptable, yielding competitive results across various configurations.",
            "ablation_id": "2404.03514v1.No1"
        },
        {
            "research_objective": "Examine the adaptability of the model-aware method in fine-tuned large language models compared to the data-aware method.",
            "experiment_process": "We fine-tuned a GPT-Neo 1.3B model using 70% of the questions related to entities from each sub-dataset in the POPQA dataset and reserved 30% as a test set. The goal was to compare the effectiveness of the data-aware method (which calculated entity frequencies using their original Wiki frequency) and the model-aware method post fine-tuning.",
            "result_discussion": "The model-aware method exhibited slightly better accuracy than the data-aware method in the fine-tuned model, indicating its effectiveness and robustness. This underscores that the model-aware method remains stable and adaptable after fine-tuning, whereas the data-aware method became unstable due to changes in entity frequency resulting from fine-tuning.",
            "ablation_id": "2404.03514v1.No2"
        }
    ]
}