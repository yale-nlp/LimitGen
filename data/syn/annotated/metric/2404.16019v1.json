{
    "title": "The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models",
    "abstract": "Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who), and objectives (to what end) of human feedback collection. To navigate these questions, we introduce Prism, a new dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries to their contextual preferences and fine-grained feedback in live conversations with 21 LLMs. Prism contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualized feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalization and attribution of sample artifacts. We focus on collecting conversations that center subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of Prism via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design. https://github.com/HannahKirk/prism-alignment",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "",
            "text": "Human feedback has always been central to the training and evaluation of AI systems, and now serves a direct role for the alignment of large language models (LLMs), defined as the steering of AI systems towards more \u2018human-preferred\u2019 states. This increased emphasis on human feedback raises unresolved questions: how we collect human feedback when designing methodologies that rely on ordinal or cardinal scales, broad or fine-grained desiderata, and explicit or implicit preference signals; where we focus human labour when selecting the domains, topics, or tasks to collect feedback over; who we ask for feedback when recruiting participants to voice their idiosyncratic preferences, values, or beliefs; and to what end when specifying our objective to pursue personalised alignment or to aggregate individual preferences into a collective outcome favorable for societies at large.\n\nWhile human feedback learning has dramatically impacted the modern LLM ecosystem, answering these fundamental questions is constrained by gaps in existing datasets. Common traits include: (i) lacking clearly explained annotation paradigms for who decided which values to encode; (ii) collecting binary preference comparisons, instead of fine-grained ratings or explanations; (iii) relying on small, narrow, or unrepresentative samples, created mostly by few power contributors recruited from specific crowdwork or tech communities; and (iv) not providing information on these contributors (such as annotator IDs or sociodemographics) thus precluding investigation of sample artifacts and biases.\n\nMost datasets rely on contextual or revealed preferences which have limitations as the harbinger of alignment, and much attention is devoted to technical or statistical issues in learning from human feedback, rather than data-centric human factors.\n\nIn this paper, we introduce Prism, a new resource for navigating empirical questions surrounding human feedback. At a high-level, Prism maps detailed survey responses of humans from around the world onto their live conversations with LLMs. We rely on both the ask and observe paradigm of social scientific inquiry, motivated by the need to understand human preferences beyond atomistic observed behavior, and to respect active participation from diverse voices in their own words. Our setup permits empirical alignment approaches that require either binary contextual preference comparisons like classic RLHF, or stated preferences and principles, like constitutional AI. In addition to this high-level design feature of pairing preference ratings with diverse participant profiles, our eponymous features each form a contribution of the dataset.\n\nParticipatory: To diversify the voices contributing to alignment norms, Prism seeks sample breadth (geocultural variation across global regions) and depth (demographic variation in national samples). With informed consent and fair pay, we acknowledge \u2018participation as work\u2019 from 1,500 English-speaking participants recruited via a crowdwork platform.\n\nRepresentative: The aggregation of individual preferences requires some meaningful unit of a collective. For two national samples in Prism (UK and US), we recruit census-representative samples, but do not claim full statistical representativeness due to small sample sizes and a restricted pool of digital workers. We also seek a \u2018representative\u2019 model landscape by including 21 LLMs from various commercial providers and open-access channels, spanning various pre-existing alignment norms.\n\nIndividualised: Learning human preferences from an amorphous blob of \u2018generic\u2019 human data results in behaviors which are unexplainable, because they emerge from the \u201cindiscriminate aggregation\u201d of data; reductionist because human values are developed relationally and non-separable from the person, operating context, or community; and non-generalisable because hidden annotator context is subsumed as universalities. Prism links each preference rating to a unique pseudonymous ID and a detailed participant profile. Therefore, annotator biases and idiosyncratic variance can be studied.\n\nSubjective: Diverse perspectives on LLM behaviors are not needed equally everywhere. The prompt what is the capital of France? may not require a multiplicity of diverse collected perspectives but is abortion morally right or wrong? has no singularly correct answer. Prism contains contexts along the objective-subjective spectrum because participants split their efforts three ways between an unguided baseline of task-oriented or neutral dialogues, values-guided dialogues, and controversy-guided dialogues. To our knowledge, Prism is the first human feedback dataset to explicitly target cross-cultural controversies and value-laden prompts, where interpersonal disagreement is rife.\n\nMulticultural: Training data biases can have real effects on whose opinions LLMs represent, resulting in cultural homogenisation or algorithmic monoculture. To avoid value-monism, especially given we operate in the subjective paradigm, Prism places an extra emphasis on sourcing global participation, with English-speakers born in 75 different countries, covering all major ethnic and religious groups.\n\nAfter introducing Prism\u2019s features, we demonstrate the dataset\u2019s usefulness via three case studies. First, in Dialogue Diversity, we ask do different people initiate different discussions with LLMs? We find identity has some predictive power on topic, but tightly-clustered semantic spaces are shared by authors of many intersectional demographics. Second, in Preference Diversity, we"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "",
            "text": "There is a long history of analogue and digital technologies failing to meet the needs of a diverse userbase due to narrowly-conceived of, or narrowly-consulted, populations during design and development. Organised participation from affected stakeholder communities rose to prominence in the 20th century, in international development and city planning projects, and has been adopted as a \u2018quick-fix\u2019 solution to the design harms of AI and ML technology. LLMs, like any pre-trained AI system, are de facto participatory, relying on self-supervised learning over datasets authored by billions of internet users, but this involvement is passive and involuntary. Pre-training datasets is where a disparity in representation begins: not all cultures are represented equally, with Western values and rituals prioritised over others; and historically-minoritised communities are depicted in stereotypical lenses. AI technologies also rely on many layers of active labour for labelling gold data, evaluating model outputs or voting on more preferred text generations. This procedural participation is instrumentally valuable\u2014to steer a model towards being more performant, preferred, or commercially-profitable. While seemingly objective in its aims, there are still many value-laden decisions at play, for example, what counts as high-quality language, or whose definition of \u201cmore preferred\u201d outputs are encoded in annotation guidelines. As Kelty argues, participation then is still formatted, \u201cwith specific rules and scripts\u201d by the \u201cruling elite\u201d (in our case, technology providers and model trainers) who decide what valid and valuable participation looks like. In this sense, participation risks becoming cooptative or extractive. In contrast, participation can be intrinsically valuable\u2014as an act of justice or democratic right, especially when it is active and conscious. Considering the trend towards increasingly simulated \u201chuman\u201d participation, GPT-surrogates may proxy instrumental gains from human input but not intrinsic or experiential benefits that the human themselves gains from being part of a wider collective. As well as these motivations for participation\u2014from addressing design harms to fulfilling democratic ideals\u2014\u201cThe Participant\u201d has a central role in robust scientific research, which we turn to next. \n\nAs Henrich et al.\u2019s foundational article points out, while the majority of research subjects are recruited from Western, Educated, Industrialised, Rich and Democratic nations, most people in the world are not WEIRD. A tendency to overestimate universality is evidenced in the increasing trend in psychology studies towards not providing sample information, where conclusions about \u201cthe standard subject\u201d drawn from one population are generalised to the human species at large. The field of natural language processing (NLP) has faced analogous criticisms, where annotator-specific data is rarely released in favour of collapsed majority votes\u2014a practice that Prabhakaran et al. argues can cause real harm by obfuscating sociocultural disagreement and washing out minority group perspectives. Instead, there is a signal in \u2018noisy\u2019 disagreements among multiple human annotators, what Aroyo and Welty call \u201cCrowd Truth\u201d, for example to reveal challenging or ambiguous prediction items; or expose interpersonal subjectivities in societally-contentious issues like hate speech or safety, where annotator background can substantially affect ratings. Better documentation of dataset provenance and curation decisions helps to \u201cpush back against the prevalent notion that crowdworkers are interchangeable.\u201d Moving past the \u201cgeneric human\u201d is needed to acknowledge power structures that underpin supposedly \u2018neutral\u2019 technology. Talat et al. draws on Haraway\u2019s seminal work to levy a criticism on the ML community that questing for universal truth, \u201cthe God trick of objectivity\u201d, ends up disproportionately harming marginalised communities by relegating variation in subjective experience. Philosophers have long noted that measurement is representation: even scientific measurements \u201ctrade in selective distortion and systematic non-resemblance\u201d, rendering knowledge relational and cultural. To avoid disguising the particularities as universalities, we release pseudonymous identifiers and sociodemographic information, so that Prism can spotlight sample diversity while acknowledging specificity.\n\nWhile human input already featured in NLP systems as gold demonstrations or labels, a key paradigm shift arrived with RLHF, where human feedback directly conditions the loss function, thus overcoming the difficulties of specifying an explicit reward function. Combining human feedback, reinforcement learning, and natural language generation has a longer history than often gets credit, especially in machine translation and dialogue. Most human feedback pipelines collect human preference data, often in the format of comparisons, which is used to train a reward model, commonly specified by a Bradley-Terry model of preference probabilities. Some also rely on more abstract explication of preferences via constitutions or rules; fine-grained feedback; or implicit feedback data from natural language. RLHF pipelines are used to encode desirable but hard-to-define behavioural dimensions into LLMs, for example, helpfulness, honesty and harmlessness or informativeness. There are a number of optimisation approaches, including updating the LLM policy with PPO or REINFORCE, as well as methods that skip the separate reward model training like"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "",
            "text": "Prism maps the characteristics and preferences of diverse humans onto their real-time interactions with LLMs (Fig. 1  ###reference_###). There are two sequential stages: first, participants complete a Survey (\u00a7 3.1  ###reference_###) where they answer questions about their demographics and stated preferences, then proceed to the Conversations with LLMs (\u00a7 3.2  ###reference_###), where they input prompts, rate responses and give fine-grained feedback in a series of multi-turn interactions. Our motivation for this two-stage setup is three-fold: (i) we move away from the \u201cgeneric human\u201d by matching ratings to detailed participant characteristics; (ii) we can track how contextual preferences (in local conversations) depart from stated preferences (in survey); and (iii) participants have autonomy to communicate in their own words what is important and why [116  ###reference_b116###, 23  ###reference_b23###]. Both stages received ethics board approval and informed consent is collected from every data subject (see Appendix D  ###reference_###). Participants were paid  9.00/hour and the whole task took 70 minutes on average. Data collection ran from 22nd November to 22nd December 2023.222The study was piloted in October 2023 with two iterative rounds of beta tester feedback. All ethics approvals, data collection, processing and analysis was conducted primarily by researchers from the University of Oxford, assisted by researchers from the University of Pennsylvania, Bocconi University and Sheffield University. We provide a data statement in Appendix B  ###reference_###, data clause in Appendix C  ###reference_###, and full codebooks in Appendix X  ###reference_###.\nWe ask about participants\u2019 familiarity with LLMs, and whether to their knowledge they have used them indirectly (in existing products or workflows such as LinkedIn post-writing assistance); or directly (via a specialised interface like ChatGPT). Individuals that have used LLMs directly or indirectly (84%) are branched to further questions on how frequently they use LLMs (7% every day, 21% every week, and 20% every month) and what task(s) they use LLMs for (the most popular usecases are research overviews selected by 49%, professional work by 37%, creative writing by 31% and programming help by 27%).333We do not ask which specific LLMs participants use due to the rapidly changing landscape. It is noteworthy that 10% of participants are not familiar at all with LLMs and 15% have not used them directly or indirectly (see Appendix I  ###reference_###).\nSystem strings can guide LLM behaviours as a high-level global instruction prompts prepended to all subsequent interactions [117  ###reference_b117###, 118  ###reference_b118###], and have been analogised as \u201cconsitutions\u201d or governing principles for AI [27  ###reference_b27###]. Factuality, professionalism, humanness and harm all emerged as key discussions (see \u00a7 M.1  ###reference_1###) from the following prompt:\n\n{mdframed}[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\nImagine you are instructing an AI language model how to behave. You can think of this like a set of core principles that the AI language model will always try to follow, no matter what task you ask it to perform. In your own words, describe what characteristics, personality traits or features you believe the AI should consistently exhibit. You can also instruct the model what behaviours or content you don\u2019t want to see. If you envision the AI behaving differently in various contexts (e.g., professional assistance vs. storytelling), please specify the general adaptations you\u2019d like to see.\nPlease write 2-5 sentences in your own words.\n###figure_11### In contrast to this fluid and open-ended preference communication, we collect structured ratings on different behavioural attributes of LLMs. Participants score the importance of each attribute on a visual analog scale [119  ###reference_b119###] (Fig. 2  ###reference_###). For example, a statement like \u201cIt is important that an AI language model produces factual and informative responses\u201d maps (0,100) where the ends of scale are (Strongly disagree, Strongly agree). Numeric scores are recorded, but not shown to participants to avoid anchoring and dependency biases. The same fine-grained attributes appear in the Conversations stage; so, we can track how stated and global preferences relate to contextual and local preferences.444In the survey, in addition to the seven attributes in Fig. 2  ###reference_###, there is (i) an Other option with free-text, used by 332 participants (see \u00a7 Q.3  ###reference_3###), and (ii) a personalisation attribute with the statement \u201cIt is important that an AI language model learns from our conversations and feels personalised to me.\u201d We do not include personalisation in the conversations stage because the models are inherently not personalised to each user. Overall, we find clusters emerge between more subjective attributes (values, creativity and diversity) versus more objective attributes (factuality, fluency and helpfulness; see \u00a7 Q.1  ###reference_1###). While the majority of participants agree that these more objective attributes are important (highly-skewed positive distribution, ), there is little agreement on the meta-importance of subjective attributes (see \u00a7 Q.2  ###reference_2###). In fact, responses for whether value alignment itself is important follow an almost normal distribution ().\nValues and preferences are subjective and personal; so, we ask participants to describe themselves in their own words to ascribe autonomy in communicating salient aspects of identity, beyond essentialising associations with structured demographics alone. Honesty, hard work and empathy emerged as common values (see \u00a7 M.2  ###reference_2###) from the following prompt:\n\n{mdframed}[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\nPlease briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values you\u2019d want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email. Please write 2-5 sentences in your own words.\nFinally, we ask participants a series of standard demographic questions: age; gender; employment status, martial status; educational attainment, ethnicity; religious affiliation; English proficiency; country of birth; and country of current residence. For every question there is a \u201cPrefer not to say\u201d option. For gender, we allow participants to self-describe in addition to pre-specified options of Male, Female and Non-Binary. Due to the global nature of our study, ethnicity and religion are collected as self-describe strings because no pre-set groups exhaust how individuals may identify themselves across cultures and countries. We provide a manual annotation of these strings into aggregated categorisations for statistical analysis purposes (Appendix F  ###reference_###). Because of our sampling aims (described in \u00a7 3.3  ###reference_###), participants cover diverse demographics (Appendix G  ###reference_###) and geographies (Appendix H  ###reference_###), with representation from people born in 75 countries. However, the sample still skews White, Western and educated, and only contains English-language speakers (see Limitations  ###reference_tations###).\nTo encourage topical diversity along the objective-subjective spectrum (i.e., avoiding the case where every conversation starts with a greeting), we prime participants by asking them select a conversation type before inputting their opening prompt:555We ask for two of each conversation type ( in total). In practice, some deviated from this quota due to technical difficulties, instruction misunderstanding or simply losing count (we provide a counter on our interface, but not per type breakdowns). To control for this variance, we release a balanced subset of the data (see Appendix K  ###reference_###).\n[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\n Unguided. Ask, request or talk to the model about anything. It is up to you! \n Values guided. Ask, request or talk to the model about something important to you or that represents your values. This could be related to work, religion, family and relationship, politics or culture. \n Controversy guided. Ask, request or talk to the model about something controversial or where people would disagree in your community, culture or country.\nWe also wanted to encourage a variety of prompt formats (i.e., avoiding the case where all conversations open with information-seeking questions, at the expense of task-assistance); but, a careful balance must be struck between nudging participants without biasing them to the examples provided, thus losing information on their own free choices. We opt to offer some soft inspiration:\n\n{mdframed}[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\nNeed some inspiration? You can request help with a task (like writing a recipe, organising an activity or event, completing an assignment)\u2026 You can chitchat, have casual conversation or seek personal advice. You can ask questions about the world, current events or your viewpoints.\n###figure_12### ###figure_13### Participants construct a free-text prompt of their choosing and receive up to four responses from different LLMs.666We do not stream model responses because streaming was functionally unavailable in some APIs so would introduce identifiable model-wise differences. If a model fails or a response takes longer than 30 seconds, we drop this model from the response set and the participant may see  responses (see Appendix S  ###reference_###). The participants then rate each response on visual analog scale (VAS) [122  ###reference_b122###, 119  ###reference_b119###] from \u201cTerrible\u201d to \u201cPerfect\u201d. We record the position of the slider as a score from 1-100 but participants are not shown the number to avoid anchoring or conditional dependence of scores across different conversations.777We analyse central tendencies and spread of score distributions in Appendix R  ###reference_###. We opt for this form of cardinal feedback for three reasons: (i) it permits and encourages subjectivity [12  ###reference_b12###]; (ii) ratings can be converted to rankings but not vice versa; thus rankings permit studying the relative merit of cardinality versus ordinality for human reward modelling; (iii) it allows greater expressivity of preference intensity compared to prior paradigms of chosen:rejected pairs [15  ###reference_b15###].888For example, all responses could be very poor and similar (negative skew, small spread); all very good and similar (positive skew, small spread); or highly-distinguishable (no skew, wide spread). However, we acknowledge that the cardinal scales may introduce some intrapersonal measurement noise from a more cognitively demanding task and carries less interpersonal comparability than ordinal preferences (see Limitations  ###reference_tations###).\nThe highest-scoring LLM from the opening turn is locked into the conversation for subsequent turns, with random tie-breaks in the case of identical scores. We enforce that the participant must continue the conversation for at least one more turn, but ask them to vary their conversations between 2 and 10 turns to avoid introducing a dataset artefact. Despite successfully encouraging varying conversation lengths (), there is a large drop off after the enforced limit of  (see Appendix R  ###reference_###). Participants now rate two responses (on the same VAS scales), both sampled from the selected model (with a non-deterministic temperature). These within-model responses (when ) are much more similar in style and content than the across-model responses (when ), and score deviations are much narrower (see Appendix R  ###reference_###).\nAfter ending the conversation, participants give fine-grained feedback. First, participants rate statements about the performance of their highest-rated model like \u201cThe response was well-written and coherent\u201d on a VAS from Performed very poorly to Performed very well, or can select \u201cN/A\u201d if the statement is irrelevant in the conversational context. Second, we ask participants to consider why they chose this model, and rate statements like \u201cI chose this response because it was well-written and coherent\u201d on a VAS from Very unimportant to Very important (or select N/A). The attributes are shared with the Survey (see Fig. 2  ###reference_###). We find strong correlations between performance-choice pairs per attribute (except safety) but weak correlations of these pairs to their stated preference equivalent. We suggest this could be due to conversational, model or task-design confounders (see \u00a7 Q.1  ###reference_1###). In general, the distribution of scores over performance and choice attributes is narrower and positively skewed (bunched to 100) than their equivalent stated preference attributes (see \u00a7 Q.2  ###reference_2###). Finally, we collect open-ended natural language feedback on the whole conversation. Participants contributed valuable content-based and stylistic feedback ( words, , see \u00a7 M.3  ###reference_3###) in response to the following prompt:\n\n{mdframed}[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\nGive the model some feedback on the conversation as whole. Hypothetically, what would an ideal interaction for you look like here? What was good and what was bad? What (if anything) was missing? What would you change to make the conversation better?\nSamples matched to simplified census data (age, ethnicity, gender) were only available for the UK and US. The minimum pool size for a statistical guarantee of representativeness was 300 people which set a lower bound for the study spots. After finishing data collection, we observed some skew in our \u2018representative\u2019 samples between observed and expected distributions in recent census data, which we partially correct for (see Appendix L  ###reference_###). These samples permit the demographic depth needed for investigations on a more representative population, which can be replicated across two different countries; however their inclusion comes at the cost of biasing Prism as a whole towards two Western nations already over-represented in AI research.\nThe distribution of Prolific workers outside the US and the UK skews strongly towards Europe and Northern America, and some countries dominate continental counts (see Appendix J  ###reference_###). To avoid biasing the sample towards countries with more active workers, we set up 33 country-specific studies for every country that has at least 1 eligible worker, and balance the number of available spaces across studies to ensure each global region has approximately the same number of total spaces.999Participants still appear in our sample who were born or reside in countries that didn\u2019t have a dedicated country-wise study e.g., if their Prolific details were outdated or incorrect. We do not drop them. We balance each national sample by gender where possible (see LABEL:tab:studies).\nThe landscape of LLMs changes rapidly, so we aspired for our task to be representative of model properties so that it is model-agnostic or at least not model-stale. Since finishing data collection, there have already been notable releases, like Gemini [123  ###reference_b123###], Mixtral [124  ###reference_b124###], Claude-3 [125  ###reference_b125###], Command-R [126  ###reference_b126###] and Llama-3 [127  ###reference_b127###], with significant disruption to model leaderboards. Anticipating these developments, we included  models (9 open-access,  commercial-API) from various model families, providers and parameter sizes to diversify the underlying training data, capabilities and degree of existing safeguards or alignment biases already incorporated in post-training. To avoid text length confounding preference signal [128  ###reference_b128###] and reduce task-fatigue of participants, we include system prompts that instruct models to keep their response to 50 words or less. The full list of models, decoding parameters and generation details are discussed in Appendix S  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "",
            "text": "Prior to starting the survey, we ensure that participants are over 18 years old, obtain their informed consent, offer a brief primer on LLMs, and dissuade LLM-written responses. The survey constructs a participant profile with five core features.\nWe ask about participants\u2019 familiarity with LLMs, and whether to their knowledge they have used them indirectly (in existing products or workflows such as LinkedIn post-writing assistance); or directly (via a specialised interface like ChatGPT). Individuals that have used LLMs directly or indirectly (84%) are branched to further questions on how frequently they use LLMs (7% every day, 21% every week, and 20% every month) and what task(s) they use LLMs for (the most popular usecases are research overviews selected by 49%, professional work by 37%, creative writing by 31% and programming help by 27%).333We do not ask which specific LLMs participants use due to the rapidly changing landscape. It is noteworthy that 10% of participants are not familiar at all with LLMs and 15% have not used them directly or indirectly (see Appendix I  ###reference_###  ###reference_###).\nSystem strings can guide LLM behaviours as a high-level global instruction prompts prepended to all subsequent interactions [117  ###reference_b117###  ###reference_b117###, 118  ###reference_b118###  ###reference_b118###], and have been analogised as \u201cconsitutions\u201d or governing principles for AI [27  ###reference_b27###  ###reference_b27###]. Factuality, professionalism, humanness and harm all emerged as key discussions (see \u00a7 M.1  ###reference_1###  ###reference_1###) from the following prompt:\n\n{mdframed}[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\nImagine you are instructing an AI language model how to behave. You can think of this like a set of core principles that the AI language model will always try to follow, no matter what task you ask it to perform. In your own words, describe what characteristics, personality traits or features you believe the AI should consistently exhibit. You can also instruct the model what behaviours or content you don\u2019t want to see. If you envision the AI behaving differently in various contexts (e.g., professional assistance vs. storytelling), please specify the general adaptations you\u2019d like to see.\nPlease write 2-5 sentences in your own words.\n###figure_14### In contrast to this fluid and open-ended preference communication, we collect structured ratings on different behavioural attributes of LLMs. Participants score the importance of each attribute on a visual analog scale [119  ###reference_b119###  ###reference_b119###] (Fig. 2  ###reference_###  ###reference_###). For example, a statement like \u201cIt is important that an AI language model produces factual and informative responses\u201d maps (0,100) where the ends of scale are (Strongly disagree, Strongly agree). Numeric scores are recorded, but not shown to participants to avoid anchoring and dependency biases. The same fine-grained attributes appear in the Conversations stage; so, we can track how stated and global preferences relate to contextual and local preferences.444In the survey, in addition to the seven attributes in Fig. 2  ###reference_###  ###reference_###, there is (i) an Other option with free-text, used by 332 participants (see \u00a7 Q.3  ###reference_3###  ###reference_3###), and (ii) a personalisation attribute with the statement \u201cIt is important that an AI language model learns from our conversations and feels personalised to me.\u201d We do not include personalisation in the conversations stage because the models are inherently not personalised to each user. Overall, we find clusters emerge between more subjective attributes (values, creativity and diversity) versus more objective attributes (factuality, fluency and helpfulness; see \u00a7 Q.1  ###reference_1###  ###reference_1###). While the majority of participants agree that these more objective attributes are important (highly-skewed positive distribution, ), there is little agreement on the meta-importance of subjective attributes (see \u00a7 Q.2  ###reference_2###  ###reference_2###). In fact, responses for whether value alignment itself is important follow an almost normal distribution ().\nValues and preferences are subjective and personal; so, we ask participants to describe themselves in their own words to ascribe autonomy in communicating salient aspects of identity, beyond essentialising associations with structured demographics alone. Honesty, hard work and empathy emerged as common values (see \u00a7 M.2  ###reference_2###  ###reference_2###) from the following prompt:\n\n{mdframed}[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\nPlease briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values you\u2019d want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email. Please write 2-5 sentences in your own words.\nFinally, we ask participants a series of standard demographic questions: age; gender; employment status, martial status; educational attainment, ethnicity; religious affiliation; English proficiency; country of birth; and country of current residence. For every question there is a \u201cPrefer not to say\u201d option. For gender, we allow participants to self-describe in addition to pre-specified options of Male, Female and Non-Binary. Due to the global nature of our study, ethnicity and religion are collected as self-describe strings because no pre-set groups exhaust how individuals may identify themselves across cultures and countries. We provide a manual annotation of these strings into aggregated categorisations for statistical analysis purposes (Appendix F  ###reference_###  ###reference_###). Because of our sampling aims (described in \u00a7 3.3  ###reference_###  ###reference_###), participants cover diverse demographics (Appendix G  ###reference_###  ###reference_###) and geographies (Appendix H  ###reference_###  ###reference_###), with representation from people born in 75 countries. However, the sample still skews White, Western and educated, and only contains English-language speakers (see Limitations  ###reference_tations###  ###reference_tations###)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "",
            "text": "After completing the survey, participant move to the second stage, consisting of real-time conversations with LLMs. The conversations are mediated via a custom-built interface on Dynabench\u2014a platform designed to host dynamic dataset collection with humans and models-in-the-loop [120  ###reference_b120###, 121  ###reference_b121###].\nTo encourage topical diversity along the objective-subjective spectrum (i.e., avoiding the case where every conversation starts with a greeting), we prime participants by asking them select a conversation type before inputting their opening prompt:555We ask for two of each conversation type ( in total). In practice, some deviated from this quota due to technical difficulties, instruction misunderstanding or simply losing count (we provide a counter on our interface, but not per type breakdowns). To control for this variance, we release a balanced subset of the data (see Appendix K  ###reference_###  ###reference_###).\n[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\n Unguided. Ask, request or talk to the model about anything. It is up to you! \n Values guided. Ask, request or talk to the model about something important to you or that represents your values. This could be related to work, religion, family and relationship, politics or culture. \n Controversy guided. Ask, request or talk to the model about something controversial or where people would disagree in your community, culture or country.\nWe also wanted to encourage a variety of prompt formats (i.e., avoiding the case where all conversations open with information-seeking questions, at the expense of task-assistance); but, a careful balance must be struck between nudging participants without biasing them to the examples provided, thus losing information on their own free choices. We opt to offer some soft inspiration:\n\n{mdframed}[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\nNeed some inspiration? You can request help with a task (like writing a recipe, organising an activity or event, completing an assignment)\u2026 You can chitchat, have casual conversation or seek personal advice. You can ask questions about the world, current events or your viewpoints.\n###figure_15### ###figure_16### Participants construct a free-text prompt of their choosing and receive up to four responses from different LLMs.666We do not stream model responses because streaming was functionally unavailable in some APIs so would introduce identifiable model-wise differences. If a model fails or a response takes longer than 30 seconds, we drop this model from the response set and the participant may see  responses (see Appendix S  ###reference_###  ###reference_###). The participants then rate each response on visual analog scale (VAS) [122  ###reference_b122###  ###reference_b122###, 119  ###reference_b119###  ###reference_b119###] from \u201cTerrible\u201d to \u201cPerfect\u201d. We record the position of the slider as a score from 1-100 but participants are not shown the number to avoid anchoring or conditional dependence of scores across different conversations.777We analyse central tendencies and spread of score distributions in Appendix R  ###reference_###  ###reference_###. We opt for this form of cardinal feedback for three reasons: (i) it permits and encourages subjectivity [12  ###reference_b12###  ###reference_b12###]; (ii) ratings can be converted to rankings but not vice versa; thus rankings permit studying the relative merit of cardinality versus ordinality for human reward modelling; (iii) it allows greater expressivity of preference intensity compared to prior paradigms of chosen:rejected pairs [15  ###reference_b15###  ###reference_b15###].888For example, all responses could be very poor and similar (negative skew, small spread); all very good and similar (positive skew, small spread); or highly-distinguishable (no skew, wide spread). However, we acknowledge that the cardinal scales may introduce some intrapersonal measurement noise from a more cognitively demanding task and carries less interpersonal comparability than ordinal preferences (see Limitations  ###reference_tations###  ###reference_tations###).\nThe highest-scoring LLM from the opening turn is locked into the conversation for subsequent turns, with random tie-breaks in the case of identical scores. We enforce that the participant must continue the conversation for at least one more turn, but ask them to vary their conversations between 2 and 10 turns to avoid introducing a dataset artefact. Despite successfully encouraging varying conversation lengths (), there is a large drop off after the enforced limit of  (see Appendix R  ###reference_###  ###reference_###). Participants now rate two responses (on the same VAS scales), both sampled from the selected model (with a non-deterministic temperature). These within-model responses (when ) are much more similar in style and content than the across-model responses (when ), and score deviations are much narrower (see Appendix R  ###reference_###  ###reference_###).\nAfter ending the conversation, participants give fine-grained feedback. First, participants rate statements about the performance of their highest-rated model like \u201cThe response was well-written and coherent\u201d on a VAS from Performed very poorly to Performed very well, or can select \u201cN/A\u201d if the statement is irrelevant in the conversational context. Second, we ask participants to consider why they chose this model, and rate statements like \u201cI chose this response because it was well-written and coherent\u201d on a VAS from Very unimportant to Very important (or select N/A). The attributes are shared with the Survey (see Fig. 2  ###reference_###  ###reference_###). We find strong correlations between performance-choice pairs per attribute (except safety) but weak correlations of these pairs to their stated preference equivalent. We suggest this could be due to conversational, model or task-design confounders (see \u00a7 Q.1  ###reference_1###  ###reference_1###). In general, the distribution of scores over performance and choice attributes is narrower and positively skewed (bunched to 100) than their equivalent stated preference attributes (see \u00a7 Q.2  ###reference_2###  ###reference_2###). Finally, we collect open-ended natural language feedback on the whole conversation. Participants contributed valuable content-based and stylistic feedback ( words, , see \u00a7 M.3  ###reference_3###  ###reference_3###) in response to the following prompt:\n\n{mdframed}[\nleftmargin=0.1cm,\nrightmargin=0.1cm,\ninnerleftmargin=4pt,\ninnerrightmargin=4pt,\ninnertopmargin=4pt,\ninnerbottommargin=4pt,\nlinewidth=0.5pt,\nbackgroundcolor=lightoat,\n]\n\nGive the model some feedback on the conversation as whole. Hypothetically, what would an ideal interaction for you look like here? What was good and what was bad? What (if anything) was missing? What would you change to make the conversation better?"
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "",
            "text": "We had two sampling aims for the Prism dataset: (i) depth in the demographics represented within countries (see LABEL:tab:full_demographics) and (ii) breadth with geographic representation from each global region (see LABEL:tab:full_geographics). We recruit English-speaking participants from Prolific in two distinct paths:\nSamples matched to simplified census data (age, ethnicity, gender) were only available for the UK and US. The minimum pool size for a statistical guarantee of representativeness was 300 people which set a lower bound for the study spots. After finishing data collection, we observed some skew in our \u2018representative\u2019 samples between observed and expected distributions in recent census data, which we partially correct for (see Appendix L  ###reference_###  ###reference_###). These samples permit the demographic depth needed for investigations on a more representative population, which can be replicated across two different countries; however their inclusion comes at the cost of biasing Prism as a whole towards two Western nations already over-represented in AI research.\nThe distribution of Prolific workers outside the US and the UK skews strongly towards Europe and Northern America, and some countries dominate continental counts (see Appendix J  ###reference_###  ###reference_###). To avoid biasing the sample towards countries with more active workers, we set up 33 country-specific studies for every country that has at least 1 eligible worker, and balance the number of available spaces across studies to ensure each global region has approximately the same number of total spaces.999Participants still appear in our sample who were born or reside in countries that didn\u2019t have a dedicated country-wise study e.g., if their Prolific details were outdated or incorrect. We do not drop them. We balance each national sample by gender where possible (see LABEL:tab:studies).\nThe landscape of LLMs changes rapidly, so we aspired for our task to be representative of model properties so that it is model-agnostic or at least not model-stale. Since finishing data collection, there have already been notable releases, like Gemini [123  ###reference_b123###  ###reference_b123###], Mixtral [124  ###reference_b124###  ###reference_b124###], Claude-3 [125  ###reference_b125###  ###reference_b125###], Command-R [126  ###reference_b126###  ###reference_b126###] and Llama-3 [127  ###reference_b127###  ###reference_b127###], with significant disruption to model leaderboards. Anticipating these developments, we included  models (9 open-access,  commercial-API) from various model families, providers and parameter sizes to diversify the underlying training data, capabilities and degree of existing safeguards or alignment biases already incorporated in post-training. To avoid text length confounding preference signal [128  ###reference_b128###  ###reference_b128###] and reduce task-fatigue of participants, we include system prompts that instruct models to keep their response to 50 words or less. The full list of models, decoding parameters and generation details are discussed in Appendix S  ###reference_###  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "",
            "text": "First, we use all-mpnet-base-v2, a state-of-the-art pre-trained sentence transformer, to produce a 768-dimensional embedding for each opening prompt. Second, we reduce dimensionality with UMAP to reduce complexity prior to clustering. Third, we cluster the prompts using HDBScan, a density-based clustering algorithm, which does not force cluster assignment: 70% of prompts are assigned to 22 clusters and 30% remain as outliers. We use a minimum cluster size of 80 (of 8,011 prompts) and minimum UMAP distance of 0. Other hyperparameters are default. To interpret the identified clusters, we use TF-IDF to extract the top 10 most salient uni- and bigrams from each cluster\u2019s prompts and locate five prompts closest and furthest to the cluster centroids. Finally, we use gpt-4-turbo to assign a short descriptive name to each cluster based on the top n-grams and closest prompts.\n\nEach group within a demographic attribute appears at a variable base rate in our overall sample. If group members chose topics at random, then any topic in expectation will appear at this base rate. Intuitively, if a majority of our sample is from a certain demographic, it is unsurprising if topics are majority-represented by that demographic. So, for non-random group differences in topic prevalence, we consider if a group pulls more than its weight.\n\nFor the partial contribution of each demographic attribute, ceteris paribus, we estimate the following regression for each topic. The vectors gender, age, region, ethnicity, religion, and conversation type represent different sets of binary variables. For each set of variables, we remove the following base categories: Male, 18-24 years old, United States, White, Not religious, and Unguided. The coefficients of interest are contained in the vectors. The component of a vector can be interpreted as the increase in probability of a participant choosing a topic if they are in the group indexed by compared to the base group. We estimate the equation with Ordinary Least Squares and cluster standard errors at the individual level.\n\nTo understand dialogue spaces more granularly than topic, we examine local neighborhoods within the embedding space of opening prompts. We create local neighborhoods via a single-link hierarchical clustering algorithm that iteratively merges neighborhoods within a cosine distance threshold, so that the neighborhood size can vary but the semantic similarity of its members is tightly constrained. We opt to use this method because it is transparent and interpretable. We remove any singleton neighborhoods, and ego non-singleton neighborhoods containing only prompts authored by the same participant. For each remaining local neighborhood, we capture the demographic characteristics of prompt authors. We repeat this analysis examining properties of the neighborhoods for different thresholds. Cosine distances can lack robustness in high dimensions but this favors underestimating semantic similarity: if cosine distance is high, this doesn\u2019t mean things are not similar, but if cosine distance is low, then items are certainly very similar (more strict). If an author appears twice, we double count their characteristics to avoid overestimating diversity; but most prompts are from non-duplicated authors. Most duplicates come in the \u201cgreetings\u201d topic e.g., \u201cHello\u201d.\n\nWe require a summary metric of between-participant diversity to understand the composition of local neighborhoods. Let represent the set of demographic attributes, e.g., gender, age, and ethnicity. For each, there are possible groups. For a neighborhood size, the prevalence of each group is calculated, and the per demographic Shannon entropy is determined.\n\nSeveral adjustments are required. First, different attributes have varying numbers of possible groups. Second, not every group appears equally within a demographic. Finally, the expected diversity of a neighborhood grows with size. To account for these factors, we simulate the expected entropy based on randomly sampling a neighborhood size at population-wide probabilities. After making this adjustment per attribute, total entropy of the neighborhood is additive.\n\nOur priming instructions had a significant effect on conditioning participants\u2019 conversations. The topics significantly correlated with controversy conversations touch on divisive current debates, including issues of Gender and LGBTQ+ Identity like gender reassignment, pay gaps, and trans participation in sports; perspectives on the Israel\u2013Palestine Conflict; and Discussions on Abortion addressing its morality and legality in different global regions. We discuss the ethics of crowdsourcing perspectives to controversial issues. For topics significantly correlated to values guidance, the most prevalent topic overall addresses Managing Relationships (both professional and familial); other significant topics are Job Search (including unemployment and financial hardship); and Religion and Spirituality (across various global religions). In contrast, the \u201cunguided\u201d portion of the dataset acts as a control with more task-orientated or information-seeking dialogues, such as Popular Culture; Recipes and Cooking; and Travel Recommendations. Only one topic (Climate Change) is not significantly correlated to conversation type, so it could act as a future testbed for comparing neutral and more"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "",
            "text": "Our first experiment asks: do different people initiate different discussions with LLMs? Specifically, we operationalise different discussions as topic clusters within the opening conversational turn and different people as the variance in topic prevalence explained by demographic affiliations versus remaining idiosyncratic or unobserved factors. We focus only on human-authored opening prompts because they are not confounded by model response. This risks over-estimating the homogeneity of the discussions because opening prompts don\u2019t necessarily reflect full conversational trees, where starting with a greeting (e.g., \u201cHi, how are you?\u201d) can proceed in many different ways; and differently held personal beliefs are often not reflected in the opener (questions like \u201cwhat do you think of abortion?\u201d are more common than statements like \u201cI think abortion is right/wrong\u201d). As a byproduct of this investigation, we also provide an overview of the types of conversations contained in Prism.\n\nFirst, we use all-mpnet-base-v2, a state-of-the-art pre-trained sentence transformer, to produce a 768-dimensional embedding for each opening prompt. Second, we reduce dimensionality with UMAP, to reduce complexity prior to clustering. Third, we cluster the prompts using HDBScan, a density-based clustering algorithm, which does not force cluster assignment: 70% of prompts are assigned to 22 clusters and 30% remain as outliers. We use a minimum cluster size of 80, and minimum UMAP distance of 0. Other hyperparameters are default. To interpret the identified clusters, we use TF-IDF to extract the top 10 most salient uni- and bigrams from each cluster\u2019s prompts, and locate five prompts closest and furthest to the cluster centroids. Finally, we use gpt-4-turbo to assign a short descriptive name to each cluster based off the top n-grams and closest prompts.\n\nEach group within a demographic attribute appears at a variable base rate in our overall sample, e.g., {Females: 48%, Males: 50%, Non-binary people: 2%}. If group members chose topics at random, then any topic in expectation will appear at . Intuitively, if 64.6% of our sample is White, it is unsurprising if topics are majority-White. So, for non-random group differences in topic prevalence, we consider if a group pulls more than its weight:\n\nFor the partial contribution of each demographic attribute, ceteris paribus, we estimate the following regression for each topic for :\n\nwhere if the prompt of participant in conversation is categorised into topic. The vectors gender, age, region, ethnicity, religion and conversation type represent different sets of binary variables. For each set of variables, we remove the following base categories: Male, 18-24 years old, United States, White, Not religious and Unguided. The coefficients of interest are contained in the vectors. Component of vector can be interpreted as the increase in probability of a participant choosing topic if they are in the group indexed by (e.g., Female) compared to the base group (e.g., Male). We estimate equation with an Ordinary Least Squares and cluster standard errors at the individual level.\n\nTo understand dialogue spaces more granularly than topic, we examine local neighbourhoods within the embedding space of opening prompts. We create local neighbourhood via a single-link hierarchical clustering algorithm, that iteratively merges neighbourhoods within a cosine distance threshold, so that the neighbourhood size can vary but the semantic similarity of its members is tightly constrained. We opt to use this method because it is transparent and interpretable. Algorithm is in Appendix O. We remove any singleton neighbourhoods, and ego non-singleton neighbourhoods containing only prompts authored by same participant. For each remaining local neighbourhood, we capture the demographic characteristics of prompt authors. We repeat this analysis examining properties of the neighbourhoods. Cosine distances can lack robustness in high-dimensions but this favours underestimating semantic similarity: if cosine distance is high, this doesn\u2019t mean things are not similar, but if cosine distance is low, then items are certainly very similar (more strict). If an author appears twice, we double count their characteristics to avoid overestimating diversity (more strict); But most prompts are from non-duplicated authors. Most duplicates come in the \u201cgreetings\u201d topic e.g., \u201cHello\u201d.\n\nWe require a summary metric of between-participant diversity to understand the composition of local neighbourhoods. Let represent the set of demographic attributes, e.g., gender, age and ethnicity. For each, there are possible groups {,\u2026,} (e.g., Male, Female, Non-binary). For a neighbourhood size of, the prevalence of each group is, and the per demographic Shannon entropy is:\n\nSeveral adjustments are required. First, different attributes have varying: there are more possible geographic regions than genders. Second, not every group appears equally within a demographic: men are more common in"
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "",
            "text": "First, we use all-mpnet-base-v2, a state-of-the-art pre-trained sentence transformer [129], to produce a 768-dimensional embedding for each opening prompt. Second, we reduce dimensionality with UMAP [130], to reduce complexity prior to clustering. Third, we cluster the prompts using HDBScan [131], a density-based clustering algorithm, which does not force cluster assignment: 70% of prompts are assigned to 22 clusters and 30% remain as outliers. We use a minimum cluster size of 80, (of 8,011 prompts) and minimum UMAP distance of 0. Other hyperparameters are default. To interpret the identified clusters, we use TF-IDF to extract the top 10 most salient uni- and bigrams from each cluster\u2019s prompts, and locate five prompts closest and furthest to the cluster centroids (see Appendix N). Finally, we use gpt-4-turbo to assign a short descriptive name to each cluster based on the top n-grams and closest prompts.\n\nEach group within a demographic attribute appears at a variable base rate in our overall sample, e.g., {Females: 48%, Males: 50%, Non-binary people: 2%}. If group members chose topics at random, then any topic in expectation will appear at. Intuitively, if 64.6% of our sample is White, it is unsurprising if topics are majority-White. So, for non-random group differences in topic prevalence, we consider if a group pulls more than its weight:\n\nFor the partial contribution of each demographic attribute, ceteris paribus, we estimate the following regression for each topic for:\n\nwhere if the prompt of participant in conversation is categorised into topic. The vectors gender, age, region, ethnicity, religion and conversation type represent different sets of binary variables. For each set of variables, we remove the following base categories: Male, 18-24 years old, United States, White, Not religious and Unguided. The coefficients of interest are contained in the vectors:. Component of vector can be interpreted as the increase in probability of a participant choosing topic if they are in the group indexed by (e.g., Female) compared to the base group (e.g., Male). We estimate equation Eq. 2 with an Ordinary Least Squares and cluster standard errors at the individual level.\n\nTo understand dialogue spaces more granularly than topic, we examine local neighbourhoods within the embedding space of opening prompts. We create local neighbourhoods via a single-link hierarchical clustering algorithm [132], that iteratively merges neighbourhoods within a cosine distance threshold (), so that the neighbourhood size () can vary but the semantic similarity of its members is tightly constrained. We opt to use this method because it is transparent and interpretable. Algorithm is in Appendix O. We remove any singleton neighbourhoods (), and ego non-singleton neighbourhoods containing only prompts authored by the same participant. For each remaining local neighbourhood, we capture the demographic characteristics of prompt authors. We repeat this analysis examining properties of the neighbourhoods for. Cosine distances can lack robustness in high-dimensions but this favours underestimating semantic similarity: if cosine distance is high, this doesn\u2019t mean things are not similar, but if cosine distance is low, then items are certainly very similar (more strict). If an author appears twice, we double count their characteristics to avoid overestimating diversity (more strict); but most prompts are from non-duplicated authors (averaged across neighbourhoods). Most duplicates come in the \u201cgreetings\u201d topic e.g., \u201cHello\u201d.\n\nWe require a summary metric of between-participant diversity to understand the composition of local neighbourhoods. Let represent the set of demographic attributes, e.g., gender, age and ethnicity. For each, there are possible groups {,\u2026,} (e.g., Male, Female, Non-binary). For a neighbourhood size of, the prevalence of each group is, and the per demographic Shannon entropy is:\n\nSeveral adjustments are required. First, different attributes have varying: there are more possible geographic regions than genders. Second, not every group appears equally within a demographic: men are more common in the data than non-binary people. Finally, the expected diversity of a neighbourhood grows with. To account for these factors, we simulate the expected entropy based on randomly sampling a -sized neighbourhood at population-wide probabilities as:\n\nAfter making this adjustment per attribute, the total entropy of the neighbourhood is additive:"
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "",
            "text": "Our priming instructions had a significant effect on conditioning participants\u2019 conversations (see Fig. 4). The topics significantly correlated with controversy conversations touch on divisive current debates, including issues of Gender and LGBTQ+ Identity like gender reassignment, pay gaps and trans participation in sport; perspectives on the Israel\u2013Palestine Conflict; and Discussions on Abortion addressing its morality and legality in different global regions. We discuss the ethics of crowdsourcing perspectives to controversial issues in \u00a7 5. For topics significantly correlated to values guidance, the most prevalent topic overall addresses Managing Relationships (both professional and familial); other significant topics are Job Search (including unemployment and financial hardship); and Religion and Spirituality (across various global religions). In contrast, the \u201cunguided\u201d portion of the dataset acts as a control with more task-orientated or information-seeking dialogues, such as Popular Culture; Recipes and Cooking; and Travel Recommendations. Only one topic (Climate Change) is not significantly correlated to conversation type, so could act as a future testbed for comparing neutral and more heated discussions controlled by topic. Topics emerge related to when the data was collected\u2014both cyclical events like the Christmas holidays, but also very current affairs and news on Israel\u2013Palestine or various global elections, which could be used to evaluate how participants react to LLMs\u2019 ability to respond to recent events.\n\nOnce we control for conversation type, there are 565 non-significant relationships contained and 73 significant relationships (with conservative ). The significant relationships include that women and non-binary participants are more likely than men to talk about gender and LGBTQ+ identity, and prompts from non-binary authors occupy this topic at 3 times their proportion in the sample as a whole; older people (55+) are more likely to talk about elections and seek travel recommendations than younger people (18-24 years), and less likely to discuss managing relationships or job search; Black participants converse less about climate change than White participants; and almost all regions question LLMs about abortion less often than US participants (see \u00a7 N.2). There are issues of multicollinearity explaining some observed patterns: 94% of participants from the Middle East region are from Israel; 57% identify religiously as Jewish; and 40% have self-described ethnicities falling into \u201cOther\u201d. Accordingly, the strong significant effect on Middle Eastern participants discussing the Israel-Palestine conflict could have been routed through national, ethnic or religious affiliations.\n\nFrom 8,011 prompts, there are only 273 unique local neighbourhoods (3.4%) when , implying that Prism contains a high degree of semantically-diverse prompts and that much of the variation in dialogue may be idiosyncratic. This threshold is recommended by Hale [133]. We present similar findings for other in Appendix O. However, the semantically-constrained neighbourhoods that do emerge contain prompts of diverse authors, especially as  increases: only 12% of prompts appear in neighbourhoods with authors from a single geographic region, only 18% from single religion, and only 8% from single age. Once we combine intersections across five attributes (gender, age, ethnicity, religion, and region), less than 1% of prompts appear in neighbourhoods with no intersectional diversity, while 58% have representation from at least two subgroups for all attributes. 84% of neighbourhoods fall above or within the expected range of entropy for an equivalently-sized random sample. While tightly-clustered dialogue spaces tend to be heterogeneous, we anecdotally observe some homogeneous neighbourhoods\u2014the largest of which contain discussions of gun laws by predominantly White participants only in the US; and of Scottish independence, Brexit and UK elections from White participants in the UK. Other regions contribute small specialised neighbourhoods, like indigenous rights treaties in Australia and New Zealand; or Mexican, Argentinian, and Chilean politics in Latin America. In contrast, many of the largest neighbourhoods present cross-border perspectives on controversial issues like abortion and the Israel-Palestine conflict (Fig. 5).\n\nWith , we find 154 neighbourhoods (86% above or within 99% CI for expected entropy). The five largest of these contain 14\u201360 prompts, varying only in capitalization and punctuation. The first three are all greetings-based (\u201cHello\u201d, ; \u201cHello, how are you\u201d, ; \u201cHi\u201d, ) but the others provide multicultural perspectives on subjective issues. One neighbourhood (\u201cDoes God exist?\u201d, ) contains half religious participants, half non-religious, who are distributed across four ethnicities, balanced by age and gender, and with representation from every geographic region. The other (\u201cWhat do you think about abortion\u201d, ) is 60% male vs 40% female; 70% younger than 35 vs 30% older; 40% White vs 60% Non-White; 30% Christian vs 70% irreligious, and has"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "",
            "text": "In the second experiment, we ask do different people prefer differently-aligned models? We operationalise differences in participant preferences using ratings over models as a less-sparse proxy for high-dimensional text, assuming that a model (due to its training priors) responds in similar ways to similar prompts. Future work could instead design feature-engineered reward models, examining what participant, model or conversational characteristics predict a response-specific reward at the text-level. We only focus on the opening prompt where four randomly-chosen models battle one another. We examine both idiosyncratic variation (how bootstrapping samples of people drawn at random from the population affects the stability and spread of aggregated preferences); as well as groupwise variation (how including only certain groups affects aggregate preferences).\n\nThere are two additional confounders to consider. First, we cannot comment on between-group preferences for different models if (i) groups occupy distinct conversational topics (e.g., only men talk about aliens) and (ii) per-model performance is correlated to topic (say gpt-4 happens to perform poorly on alien-related knowledge). Assumptions of discursive heterogeneity are strengthened by \u00a7 4.1 and by our findings in \u00a7 U.1 that men and women still display different preferences even when fixing model-topic pairs. However, to further control for conversational context, we rely on the balanced subset of participants who have equal numbers of each conversation type (conversations from participants, see Appendix K). Second, not every participant rates every model (on average only 14 of 21), but this is assumed to be missing at random due to the design of our task.\n\nParticipants\u2019 raw scores are a number between 1-100 recorded on the interface. Consider two participants, A and B, who both rate model responses X and Y. Assume for both X and Y, but A rates higher than B, meaning there are substantial differences in score skew and spread. Imagine that this behaviour persists across all of A and B\u2019s conversations: A is consistently the optimist and B the pessimist. One explanation for this behaviour is that B just systematically uses scales differently, an issue of measurement invariance that is a known problem for subjective measures. If true, we should control for participant fixed effects by normalising score (with Z-values) across each participant\u2019s set of conversations, or normalise their cardinal comparisons into ranks. However, an alternative explanation is that A and B come from very different communities with divergent preferences, and it is the case that all the models are aligned in a way that makes them perform poorly to B\u2019s prompts. If we normalise B\u2019s scores, we flatten this signal. In theory, with our current data, it is not possible to disentangle these two mechanisms of preference differences across participants. While we encourage future work exploring how normalising preference ratings affect reward learning, in practice, we find very minor descriptive differences in scores across groups (Appendix R), and that model comparisons relying on raw and normalised scores are highly correlated.\n\nEven without identical numeric scores, participants may be indifferent between model responses, which we can reconstruct with a margin-of-victory, only counting if the score difference exceeds some tie threshold. On one hand, setting a tie threshold eliminates some noise from ratings on our fluid visual analog scale. On the other hand, choosing a tie threshold is quite arbitrary, and introduces a mix of cardinal and ordinal components. We examine sensitivity of model ranks to tie threshold in \u00a7 U.3. In addition, we calibrate expected indifference margins from our VAS on sparse cases where the same participant rates identical prompt-response pairs (see Appendix P), finding a median score difference of 1, and mean of 5.8. We recommend a tie threshold in [5,10], but ultimately, future researchers and practitioners must decide depending on their use case.\n\nFor each participant, we observe a partial profile of preference ratings over models. Partial because not every individual rates every model. Non-completeness rules out some social welfare functions over individual ranks. Different aggregation functions can be thought of as social choice functions, and choosing one over another depends on whether we trust the signal is cardinally versus ordinally measurable, and unit comparable or non-comparable. For example, selecting the most preferred model among our participants by highest mean score is a form of utilitarianism, but relies on the assumption cardinal scores can be meaningfully summed interpersonally. \n\nWe put two desiderata on a preference aggregation function in our setting. First, it must be frequency invariant, due to variability in model appearances because of failed external API calls (see Appendix S). Second, it must be intrinsically comparable across tournaments. For example, absolute Elo scores (i) cannot be compared across tournaments (or bootstrapped sampling frames); (ii) are sensitive to the order and outcomes of matches; and (iii) poorly handle intransitive preference cycles.\n\nGiven the degree of idiosyncratic"
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "",
            "text": "Participants' raw scores range from 1 to 100 and are recorded on the interface. Consider two participants, A and B, who both rate model responses X and Y. Assume for both X and Y, A rates a score higher than B, meaning there are substantial differences in score skew and spread. Imagine that this behavior persists across all of A and B\u2019s conversations: A is consistently the optimist and B the pessimist. One explanation for this behavior is that B systematically uses scales differently, an issue of measurement invariance that is a known problem for subjective measures. If true, we should control for participant fixed effects by normalizing scores (with Z-values) across each participant\u2019s set of conversations or normalize their cardinal comparisons into ranks. However, an alternative explanation is that A and B come from very different communities with divergent preferences, and it is the case that all the models are aligned in a way that make them perform poorly to B\u2019s prompts. If we normalize B\u2019s scores, we flatten this signal. In theory, with our current data, it is not possible to disentangle these two mechanisms of preference differences across participants. While we encourage future work exploring how normalizing preference ratings affect reward learning, in practice, we find very minor descriptive differences in scores across groups, and that model comparisons relying on raw and normalized scores are highly correlated.\n\nEven without identical numeric scores, participants may be indifferent between model responses, which we can reconstruct with a margin-of-victory, only counting a victory if the score difference exceeds some tie threshold. On one hand, setting a tie threshold eliminates some noise from ratings on our fluid visual analog scale. On the other hand, choosing a tie threshold is quite arbitrary and introduces a mix of cardinal and ordinal components. We examine the sensitivity of model ranks to the tie threshold. In addition, we calibrate expected indifference margins from our VAS on sparse cases where the same participant rates identical prompt-response pairs, finding a median score difference of 1 and a mean of 5.8. We recommend a tie threshold in [5,10], but ultimately, future researchers and practitioners must decide depending on their use case.\n\nFor each participant, we observe a partial profile of preference ratings over models. Partial because not every individual rates every model. Non-completeness rules out some social welfare functions over individual ranks. Different aggregation functions can be thought of as social choice functions, and choosing one over another depends on whether we trust the signal is cardinally versus ordinally measurable, and unit comparable or non-comparable. For example, selecting the most preferred model among our participants by the highest mean score is a form of utilitarianism, but relies on the assumption that cardinal scores can be meaningfully summed interpersonally. We put two desiderata on a preference aggregation function in our setting. First, it must be frequency invariant, due to variability in model appearances because of failed external API calls. Second, it must be intrinsically comparable across tournaments. For example, absolute Elo scores cannot be compared across tournaments (or bootstrapped sampling frames); are sensitive to the order and outcomes of matches; and poorly handle intransitive preference cycles. A lower-rated model defeating a higher-rated model results in a significant transfer of points, so it matters when this battle occurs in our sample. In our work, we are not constrained by functions that perform well in online settings (like Elo), and can instead analyze ranks by observing a full set of offline interactions. Different aggregation functions produce correlated ranks but introduce some movement among mid-leaderboard positions."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "",
            "text": "We examine three sources of variance in aggregating preferences, each representing a different thought experiment. First, what if we replaced the individuals in a small sample with different crowdworkers? We repeatedly sample individuals and filter to only their conversations (repeated over 1,000 iterations). We chose this as this is the order of magnitude of many early human feedback studies (see Tab. 6). Model comparisons, especially in the mid-ranks, are sensitive to which individuals are sampled, and there is substantial collective indifference demonstrated by a fairly evenly-distributed \u2018power\u2019 share among the top 10 models (see Fig. 6). This means small changes in sample inclusion can substantially change overall rankings.\n\nSecond, what if we had asked participants to converse on different or fewer topics? We consider how rankings change when we only include one domain of conversation. Overall, while command always performs well across the board and flan-t5 consistently poorly, other models like zephyr-7b lose substantial rank on one domain (unguided) but perform highly on another (controversy), or vice versa for claude-2. We encourage future work to understand interactions between preferred model behaviors and domain.\n\nFinally, what if we had only sampled people from some regions and not others? Given the degree of idiosyncratic variance, we only include regions (by birth country) with at least 20 members and exclude any \"Prefer not to say.\" Ranked outcomes are sensitive to the sampling geography, for example compared to its overall rank, palm-2 drops 4 places for only US participants and llama-7b drops 7 places in Asia, while mistral-7b gains 7 places if we only ask participants born in African countries. In sum, this exercise reveals substantial noise among model ranks, depending on which participants are included in the process.\n\nWe manually examine the battles between command and either gpt-4 or gpt-4-turbo to form some hypotheses about what drives score differences, which we then test on the dataset at large. Our hypotheses for positive correlates of score are (H1) text length, as well as presence of formatting factors: (H2) line breaks; (H3) enumeration i.e., \u201c1., 2., ..\u201d; and (H4) ending with a question directed at the participant. For negative correlates of score, we test presence of certain phrase groups: (H5) de-anthropomorphism i.e., \u201cAs an AI, I don\u2019t have personal opinions\u2026\u201d and (H6) refusal, i.e., \u201cSorry I cannot engage on controversial issues\u2026\u2019. We build these phrase banks based on a set of seed sentences and keywords in R\u00f6ttger et al., expanded via repeated random snowball sampling of keyword-matched model responses. Note for H5, we control for self-identification, i.e., \u201cas an AI built by Cohere\u201d matched on names of LLMs and their providers.\n\nWe find evidence that command does generate longer text than other models (even though we placed a 50-word limit in all system prompts), includes more formatting in answers (with line breaks and enumeration), and more frequently ends in a question e.g., \u201cWould you like to know more about any specific aspect of this issue?\u201d We run an OLS regression on score to find the partial effect of a continuous regressor for length, and dummies indicating the presence of formatting or phrase-based factors. For a base score, there are positive significant correlations from additional characters and ending in a question mark. Enumeration has a significant positive effect on score, but line breaks add a significant negative effect, suggesting line breaks without structuring may be poorly received. The presence of de-anthropomorphic phrases reduces score but not as substantially as refusals. The proportion of explained variance in score by these factors is low, so we encourage more sophisticated methods in future work for partialing out the effect of style versus content, or participant, model and conversation fixed-effects, as determinants of score."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "",
            "text": "The third experiment asks: how do the characteristics of the participants used to train LLMs impact the welfare of different sub-populations? We use the term welfare to capture the extent to which a chosen LLM aligns with the preferences of a population of users. We use two measures of welfare: average rating of a model, and the average likelihood that a model is chosen (highest rated in opening turn of conversation). The previous experiments establish that there is diversity in dialogue and preferences across participants. This suggests that the welfare of users of a LLM depend on the characteristics of the individuals who provide feedback to the LLM. While training LLMs on different sub-populations is beyond this paper\u2019s scope, we approximate the thought experiment by randomly generating sub-samples of individuals to select their favourite existing LLM (those in the seat of power), and measure the distribution of welfare imposed on different sub-populations (also called stakeholder populations [8  ###reference_b8###]). We conduct this experiment on the UK and US representative samples.\n\nLet  denote the population of participants,  denote a sub-population and  denote the power set of  (i.e. all subpopulations). To identify specific sub-populations, we define the choice function:  where  is a set geographical regions and  is a set demographic groups (rep denotes the whole population). Given  and , subpop returns the individuals in  that are in both  and . Our analysis uses the sub-populations given by: . We approximate the sub-population defined by a tuple  by selecting all the matching participants in our balanced sample that are in both  and .\n\nA sampling scheme is a tuple:  where  and . A sampling scheme randomly generates samples of  individuals from , the subpopulation of interest. We approximate a sampling scheme by using our approximation of sub-populations defined in the previous section and sampling  participants with replacement. Our main analysis uses the sampling schemes:\n.\n\nLet  denote the set of models. Our analysis requires a measure of welfare for an individual  if LLM  is chosen. We use two measures of individual welfare. i) . Given participant  and model ,  computes the mean rating  gives to LLM  in the first turn of a conversation. ii) .  computes the proportion of the 's conversations where LLM  is chosen, conditional on LLM  being shown. For both measures of individual welfare, if a participant is never shown a model, we set their individual welfare to NA.\n\nA sampling scheme , together with a preference aggregation method induce a distribution . The th component of  is the probability that a random sample drawn from the sampling scheme chooses the LLM indexed by . Our main analysis uses the preference aggregation method: . Given draw , we define\n\nwhere .  then returns a random element in maxRatingCandidates.\nIn words, maxRating computes the rating (as defined in the previous paragraph) given to each model by each participant in the draw of . It then computes the mean score of each model averaged across individual mean ratings and returns a model with the highest mean rating. We repeat the analysis for the method maxChoice which replaces rating with choice.\n\nFor simplicity, we summarise the welfare imposed on the population by a given model by a single number. For the main analysis, we use the measure  where\nand . We repeat that analysis for meanChoice which replaces rating with choice. Given a sampling scheme  and a subpopulation , the PMF of the distribution of welfare is described by the tuple: where  is a vector whose th component is given by .\n\nFor each , we compute the welfare distributions implied by each sampling scheme . We use maxRating to choose a LLM, and meanRating as our measure of welfare. We repeat the analysis using maxChoice to choose a LLM, and meanChoice as our measure of welfare. A concern is that our results are sensitive to randomness caused by different participants being shown different models. As a sensitivity check, we repeat the analysis with imputed scores for missing model ratings (similar to collaborative filtering), and repeat the whole exercise for the UK (see Appendix V  ###reference_###  ###reference_###).\n\nSampling exclusively from a specific group tends to reduce the welfare of individuals who are not in that group. For example sampling from  is FOSD by  for . In addition, the costs of sampling from a particular sub-population may exceed the benefits to the sub-population being sampled from. For example,  FOSD  for . Different sampling schemes can induce different welfare distributions via two mechanisms. First, the subpopulations sampled from may have different preferences conditional on conversation type. Second, the sub-populations sampled from may have different conversations, and in turn, choose models that are better at particular conversations. This experiment taken alone cannot disentangle these two mechanisms.\n\nSampling exclusively from a non-representative group does not necessarily imply low mean welfare\u2014this is likely to"
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "",
            "text": "Let  denote the population of participants,  denote a sub-population and  denote the power set of  (i.e. all subpopulations). To identify specific sub-populations, we define the choice function:  where  is a set geographical regions and  is a set demographic groups (rep denotes the whole population). Given  and , subpop returns the individuals in  that are in both  and . Our analysis uses the sub-populations given by: . We approximate the sub-population defined by a tuple  by selecting all the matching participants in our balanced sample that are in both  and .\n\nA sampling scheme is a tuple:  where  and . A sampling scheme randomly generates samples of  individuals from , the subpopulation of interest. We approximate a sampling scheme by using our approximation of sub-populations defined in the previous section and sampling  participants with replacement. Our main analysis uses the sampling schemes:\n.\n\nLet  denote the set of models. Our analysis requires a measure of welfare for an individual  if LLM  is chosen. We use two measures of individual welfare. i) . Given participant  and model ,  computes the mean rating  gives to LLM  in the first turn of a conversation. ii) .  computes the proportion of the \u2019s conversations where LLM  is chosen, conditional on LLM  being shown. For both measures of individual welfare, if a participant is never shown a model, we set their individual welfare to NA.\n\nFor simplicity, we summarise the welfare imposed on the population by a given model by a single number. For the main analysis, we use the measure  where\nand . We repeat that analysis for meanChoice which replaces rating with choice. Given a sampling scheme  and a subpopulation , the PMF of the distribution of welfare is described by the tuple: where  is a vector whose th component is given by .\n\nFor each , we compute the welfare distributions implied by each sampling scheme . We use maxRating to choose a LLM, and meanRating as our measure of welfare. We repeat the analysis using maxChoice to choose a LLM, and meanChoice as our measure of welfare. A concern is that our results are sensitive to randomness caused by different participants being shown different models. As a sensitivity check, we repeat the analysis with imputed scores for missing model ratings (similar to collaborative filtering), and repeat the whole exercise for the UK (see Appendix V  ###reference_###  ###reference_###  ###reference_###).\n###figure_34###"
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "",
            "text": "Almost all sampling schemes put positive weight on the LLM that maximizes mean welfare for the subpopulation. However, as the sample size of the sampling scheme falls, the probability of choosing an LLM with worse mean welfare rises. In our experiments, larger samples from the target sub-population appear to first order stochastically dominate smaller samples from the target sub-population.\n\nSampling exclusively from a specific group tends to reduce the welfare of individuals who are not in that group. For example, sampling from a particular sub-population may exceed the benefits to the sub-population being sampled from. Different sampling schemes can induce different welfare distributions via two mechanisms. First, the subpopulations sampled from may have different preferences conditional on conversation type. Second, the sub-populations sampled from may have different conversations and, in turn, choose models that are better at particular conversations. This experiment taken alone cannot disentangle these two mechanisms.\n\nSampling exclusively from a non-representative group does not necessarily imply low mean welfare\u2014this is likely to occur if the sub-population sampled from is a majority group. However, the group sampled may be worse off, illustrating the risk of relying on crude measures of welfare such as the average rating of the entire population.\n\nThe analysis for choice welfare measures highlights that regardless of the model chosen, the majority of participants will prefer a different model. The model that maximizes mean choice only achieves a score of a fraction of the participants preferring it. This implies that if a participant is shown the winning model, and three other models at random, the probability that the participant will choose the winning model is under a certain threshold. The probability they will pick the winning model over the other models in the experiment can only be lower. This suggests that given the current state of LLMs, we should not expect a single LLM to be the best for everyone in a given (sub-)population."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "",
            "text": "A central goal of Prism was to oversample conversational contexts where individuals and cultures most fervently disagree on what it means for a model to be aligned to their respective values, norms and opinions. Our priming was successful, and many of the conversations are controversial, value-laden, and, at times, cross the line into being offensive or harmful. In addition to ethical considerations surrounding data release (\u00a7 5.1  ###reference_###), we raise three discussion points on the boundaries of where we collect preferences, for whose benefit and with what lasting impact.\nIn economics, welfare is construed via choices, assumed in turn to reflect personal preferences. But aligning LLMs via revealed or contextual preferences (like in Prism)\u2014what Tasioulas [141  ###reference_b141###] calls a \u2018preference-based utilitarianism\u2019\u2014may not be synonymous with individual or societal well-being. First, my preferences may not be in my own best interest. Take, for example, the numerous Prism participants who asked LLMs \u201cwhat is your favourite color?\u201d or \u201cDo you believe in God?\u201d Despite the substantial body of work raising harms from anthropomorphised AI [142  ###reference_b142###, 143  ###reference_b143###, 144  ###reference_b144###, 145  ###reference_b145###, 146  ###reference_b146###], many desire a model with \u201cchatty\u201d responses that \u201csound human\u201d. This raises the (somewhat paternalistic) observation that learning from contextual preferences may result in outcomes that are not in the self-interest of the participant, due to myopia, irrationality or information asymmetries.262626We consider preference falsification or measurement errors separately in \u00a7 5.2  ###reference_### Second, my preferences may not be compatible with others\u2019 best interest. For example, there may be negative externalities imposed from Prism participants who want an LLM that takes a strong stance in a heated debate, inflicted on others who prefer that an LLM that abstains in favour of neutrality. On one hand, accepting any preference (even those carrying a risk of harm), adopts a lens of moral relativism or consensus ethics, where rights and wrongs are decided from the prevailing attitudes and beliefs within a particular community or culture. However, understanding how LLMs should behave via consensus ethics is then highly sensitive to whose voice is sampled. Take, for example, the Israel-Palestine conflict, a major topic in Prism. Participants from around the world discussed this topic (with a whole spectrum of views) and 47 participants from Israel also had opportunity to voice their opinions; but our sample contains very few participants from other Middle Eastern or Muslim countries. Prioritising participation of certain groups may be needed when participants\u2019 positionality affects their epistemic legitimacy to define harm [51  ###reference_b51###, 147  ###reference_b147###, 148  ###reference_b148###]. On the other hand, adopting more of a moral rationalist or deontological ethics frame, the question remains whether some things should just not be crowdsourced at all [2  ###reference_b2###], \u00e0 la Satz [149  ###reference_b149###]\u2019s moral limits of markets. Asking a representative sample how LLMs should respond to LGBTQ+ or racial issues in 19th century Victorian Britain would result in generations many UK citizens would find ethically impermissible today, despite majority consensus at the time. The middle ground between these extremes is far from unexplored\u2014moral relationalism maintains the role of continuous discursive interaction between individuals in their communities, the context in which an LLM is embedded, as responsible for bounding legitimate belief and action [31  ###reference_b31###]. From a pragmatic lens, model providers likely have the most power in bounding what are considered \u201clegitimate\u201d preferences to align a model to [2  ###reference_b2###]. In sum, when responding to the question of how LLMs should behave, relying on decontextualised observations of human behaviour always carries risk of reinforcing human bias from those in the seat of power. A careful balance must be struck to incorporate cultural values without introducing cultural biases [37  ###reference_b37###]. We hope Prism, with transparent attribution of idiosyncratic and sample bias, permits better acknowledgement of the power structures underlying alignment norms.\nOur findings reveal interpersonal variance in how people want LLMs to behave. Irreconcilable personal preferences and moral frameworks can co-exist in the abstract [12  ###reference_b12###], becoming a problem only insofar as they need be operationalised into a unit of alignment\u2014i.e., those that use or have stakes in the AI system. It matters whether we seek narrow alignment to individuals, groups organisations, or broad alignment to cultures, nations or, as sometimes implied, the whole species. How to proceed in areas of disagreement thus depends whether one\u2019s aims are, for example, to strive for consensus, prioritising the collective; or to satisfy idiosyncratic preferences, respecting an individual\u2019s liberties and avoiding the need to reconcile irreconcilable preferences across peoples. Practically, Prism permits multiple pathways for pluralistic AI [43  ###reference_b43###]: exploring personalised and steerable alignment using detailed profiles on the participant matched to their specific feedback and reward signals [2  ###reference_b2###, 3  ###reference_b3###]; and designing approaches for distributional or collective alignment, using multiple perspectives or principles across people to generate consensus [4  ###reference_b4###, 5  ###reference_b5###] or learn from a distribution of rewards [33  ###reference_b33###, 7  ###reference_b7###, 6  ###reference_b6###]. Participation often has this paradox of what Kelty [46  ###reference_b46###] calls contributory autonomy, that can tend towards \u201can alienated individuality, for instance, or identification with a particular collective\u201d [46  ###reference_b46###] (p.23). We do not see these pathways as antagonistic\u2014prioritising and emphasising individual subjectivity does not preclude finding common grounds on important development decisions for AI [77  ###reference_b77###, 150  ###reference_b150###, 151  ###reference_b151###]\u2014nor exhaustive, because alignment (as dependent on values and morals that emerge intersubjectively) can exist somewhere between individuals and sociological wholes [152  ###reference_b152###, 153  ###reference_b153###]. Prism permits modelling of both individual and collective preference, but asking individuals to deliberate their priorities in groups may yield different outcomes than gathering data from one person at a time [5  ###reference_b5###, 115  ###reference_b115###, 154  ###reference_b154###].\nIn our setting, we claim what Sloane et al. [48  ###reference_b48###] calls participation as work, that is offering fair remuneration and attribution of the consensual labour of workers contributing to our project. Notably, many participants (those familiar and unfamiliar with AI) contacted the researchers and reported enjoying or learning from the task, suggesting there was an \u201ceducation quotient\u201d or role of participation as experience [46  ###reference_b46###]. Our aims do evoke notions of participation as justice\u2014including more people at the table of LLM design and development\u2014but participation is in reality thin, because while we seek their view, we cannot grant participants the power to change behaviours of deployed LLMs [155  ###reference_b155###]. Even the etymological roots of participation centre on the notion of \u201csharing\u201d [46  ###reference_b46###] but there is no guarantee that the human workers upon whom the success of RLHF relies on, partake in any share of the profits from more usable or preferred LLM technologies. We release Prism in the hope it moves the needle towards more inclusive and diverse research on human-AI interactions, emphasising the central role of those who contribute their time and voice to generating human feedback data. Ultimately, how these contributions have impact depends on those in power (industry labs, academics, policymakers), because \u201cthe experience of participation must include the sense not only of having spoken, but of having been heard\u201d [p.18, 46  ###reference_b46###].\nThe conversations in Prism are highly personal, for example detailing views towards abortion, religion, immigration, workplace disputes or intimate relationships. We have pseudo-anonymised the data, checked for PII (Appendix E  ###reference_###), sought informed consent from every participant (Appendix D  ###reference_###), provided options for participants to withdraw their data, and clearly stipulated that attempts of deanonymisation violate our dataset\u2019s terms and conditions (Appendix C  ###reference_###). However, despite following these best practices, the risk for deanonymisation remains. We include a reporting mechanism on our website and GitHub for any participants and researchers to report issues.\nWe asked participants to engage the LLMs in controversial conversations. This comes with the benefit of expanding human preference data to discursive areas with the greatest expected degree of interpersonal disagreement, but at the risk of encouraging hateful, bigoted, biased or otherwise harmful content. Harmful content is a reported issue in other human feedback datasets, where some opt to moderate conversations prior to public release [16  ###reference_b16###] and others retain toxic content for the purpose of future research into conversational AI safety [17  ###reference_b17###, 18  ###reference_b18###]. Compared to these previous datasets, Prism has an exceptionally low level of flagged content as measured via the OpenAI moderation API (0.06% overall, and  for subcategories of sexually-explicit, violent, hateful, self-harm and harassment). However, it has been reported that the recall of this API may be low [18  ###reference_b18###]; so, this could be an underestimate. From examining prompts closest to topic centroids (\u00a7 N.2  ###reference_2###), it is clear there are some prompts with potential for harm. We provide metadata for every text instance in Prism, and opt to not filter any conversations. We believe it is a critical area of research to understand how state-of-the-art models respond when they are prompted to engage in such conversations, and how different people with diverse lived experiences react to safety interventions.\nOur findings suggest dialogue and model choice are driven somewhat by group affiliation and somewhat by idiosyncratic variance. However, Prism contains a rich array of information on each participant with both structured and unstructured components. There are endless ways we could have divided the data or understood participant identity, and despite our best efforts to assess sensitivity to design choices, each alternative may have resulted in very different outcomes [156  ###reference_b156###], and we are under-powered to test so many sparse combinations. Using less sparse groupings introduces biases\u2014for example, focusing on region risks lumping together participants from particular geographies as \u201ccultures\u201d [59  ###reference_b59###]. While we split out the UK and US to avoid these countries dominating their respective regions, there remain varying degrees of country-wise entropy in other regions\u2014as aforementioned, the Middle East has 94% individuals from Israel, and 100% of Non-US Northern Americans are Canadian (see Appendix H  ###reference_###). Similarly, we use more aggregated ethnicity and religion groupings for statistical power, but amorphous and heterogeneous categories like \u201cOther\u201d have limited or flawed real-world meaning as \u201cOther\u201d contains, for example, both those who identify as Indigenous or First Peoples and as Middle Eastern or Arab. It is an exciting direction for future work to explore free-form characterisations of identity (e.g., the free-text profile or system string) or ex-post groupings of people\u2019s preferences [8  ###reference_b8###], and examine how findings change when we break away from neatly-observed but essentialising demographic traits [157  ###reference_b157###].\nBeyond the complexities of intersectional identity and idiosyncratic variance of individuals within identity groups, there are other sources of variance in Prism which present a challenge for controlled experiments; particularly, the high-dimensionality of what exact topics each participant chooses to talk about, which models randomly get selected in-the-loop, and the stochasticity in their responses from a non-deterministic temperature. It is hard to pin down robust mechanisms of preference differences amongst individuals with so many sources of variation. We opted for choice of input prompt and conversation to be a free parameter in Prism as a more naturalistic setting of LLM use and because we wanted to understand dialogue diversity among participants. We do empirically find some regions of fixed prompt-response pairs from individuals who self-select into asking the same prompts as other participants (see Appendix P  ###reference_###). However, in a future specification of our task, we plan to sample multiple of our participants to rate the same fixed context, like in Dices [65  ###reference_b65###], which grants more statistical power to understand differences in preferences over fixed model outputs.\nRelatedly, our conclusions may be confounded by measurement invariance given our explicit focus on subjective, fluid and cardinal devices. This echos the economist\u2019s view, that it is foolish to rely too heavily on cardinal ratings over ordinal rankings to make interpersonal comparisons, or enforce preference construction, where intrinsic feelings are noisily-quantified on numeric scales.272727For example, our analysis using the meanRating welfare measure assumes that individuals use scores in the same way for ratings welfare measures. However, our analysis using meanChoice is not sensitive to use of ratings scale, and the results are qualitatively similar. There are also issues of preference falsification: while participants are financially incentivised to participate, they may not honestly report their preferences over models. We cannot rule out the possibility that participants select a \u2018bad\u2019 model to lock in for the subsequent turns of conversation if it is more interesting (thus preferable in our narrow task confines) to talk to a more offensive or controversial model, or to try to \u2018jailbreak it\u2019 [17  ###reference_b17###]. In hindsight, it may have been a smarter design choice to force participants to rank model responses, or to collect both ratings and rankings (notwithstanding decision fatigue), or make attempts to elicit more interpersonally comparable data via a willingness-to-pay monetary unit. Previous work also raises concerns over relying on human feedback as \u2018gold standard\u2019, for example whether participants can accurately rate factuality of an output, or are anchored on formatting and \u2018first impressions\u2019 (as we and Hosking et al. [158  ###reference_b158###] both find). Preferences, especially at a fine-grained level like in Prism, have high context-dependency [159  ###reference_b159###], so we caution against taking the ratings as revealing some objective truth, instead staying firmly rooted in the subjective paradigm [11  ###reference_b11###, 2  ###reference_b2###].\nMuch of AI, NLP and now RLHF is underpinned by crowdworker labour [160  ###reference_b160###]. Despite our aims to include more diverse voices in LLM development processes, we avoid overstating claims on diversity. Prism still only contains crowdworkers, who have significant sample biases [161  ###reference_b161###]; can only be so \u201crepresentative\u201d given the relatively small sample sizes; must be digital natives given the platformed nature of the work; and possess different incentives for engagement [162  ###reference_b162###].\nFor example, in our welfare analysis, despite having samples balanced by observed demographics for the UK and the US, the samples are too small to to expect them to be representative on features we do not observe. Furthermore, while Prism gains some dialectical diversity from different geographies of English, from varying speaker fluency, and from some contributions in other languages (1%, mainly Spanish), it is almost exclusively in English. Cultural diversity can only be measured so far without also accounting for linguistic diversity [37  ###reference_b37###]. Furthermore, while we try to sample from many regions, our sample is still dominated by White Western participants, especially when considering cultural phylogeny [59  ###reference_b59###], i.e., the non-independence of populations with shared history or migrations of peoples (for example, Australia vs UK vs Canada). We encourage future work prioritising human feedback collection in other languages to understand how models handle sociocultural and linguistic interactions [112  ###reference_b112###].\nWhen data collection began in mid-November, Prism contained the top ranking models on publicly available leaderboards but new models have since emerged. There is an incompatibility between the current pace of model releases and doing human participant research that requires lengthy processes of ethics approval, interface design, data processing and manual annotation. The expense and inconvenience of doing human research increases the attractiveness of simulating responses, usually with GPT-4 [57  ###reference_b57###]. So, while Prism does miss out on the newest players to enter the battle arena, we do provide carefully-sourced human data (including a survey which stands independently from the LLM conversations) combined with a wide distribution of model texts; so we hope the utility of the data persists in the coming years even as models change. We are still potentially limited when comparing open and closed-access models: while the former allows full transparency over system prompts, closed-access models can obscure additional instructions as hidden context. Including models from the same family allows comparisons by version or size, but introducing clones (models producing very similar outputs) can distort preference rankings [8  ###reference_b8###]. Prism is also limited by value-lock in [57  ###reference_b57###]\u2014the models are already tuned to cultural perspectives or alignment norms [34  ###reference_b34###, 35  ###reference_b35###], which precludes observing certain group preferences towards a wider set of behaviours [43  ###reference_b43###, 163  ###reference_b163###], and renders participants \u201cthin\u201d because they are \u201climited to existing designs with pre-existing purposes.\u201d [p.3, 23  ###reference_b23###]."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "",
            "text": "The conversations in Prism are highly personal, for example detailing views towards abortion, religion, immigration, workplace disputes or intimate relationships. We have pseudo-anonymised the data, checked for PII (Appendix E  ###reference_###  ###reference_###), sought informed consent from every participant (Appendix D  ###reference_###  ###reference_###), provided options for participants to withdraw their data, and clearly stipulated that attempts of deanonymisation violate our dataset\u2019s terms and conditions (Appendix C  ###reference_###  ###reference_###). However, despite following these best practices, the risk for deanonymisation remains. We include a reporting mechanism on our website and GitHub for any participants and researchers to report issues.\nWe asked participants to engage the LLMs in controversial conversations. This comes with the benefit of expanding human preference data to discursive areas with the greatest expected degree of interpersonal disagreement, but at the risk of encouraging hateful, bigoted, biased or otherwise harmful content. Harmful content is a reported issue in other human feedback datasets, where some opt to moderate conversations prior to public release [16  ###reference_b16###  ###reference_b16###] and others retain toxic content for the purpose of future research into conversational AI safety [17  ###reference_b17###  ###reference_b17###, 18  ###reference_b18###  ###reference_b18###]. Compared to these previous datasets, Prism has an exceptionally low level of flagged content as measured via the OpenAI moderation API (0.06% overall, and  for subcategories of sexually-explicit, violent, hateful, self-harm and harassment). However, it has been reported that the recall of this API may be low [18  ###reference_b18###  ###reference_b18###]; so, this could be an underestimate. From examining prompts closest to topic centroids (\u00a7 N.2  ###reference_2###  ###reference_2###), it is clear there are some prompts with potential for harm. We provide metadata for every text instance in Prism, and opt to not filter any conversations. We believe it is a critical area of research to understand how state-of-the-art models respond when they are prompted to engage in such conversations, and how different people with diverse lived experiences react to safety interventions."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "",
            "text": "Our findings suggest dialogue and model choice are driven somewhat by group affiliation and somewhat by idiosyncratic variance. However, Prism contains a rich array of information on each participant with both structured and unstructured components. There are endless ways we could have divided the data or understood participant identity, and despite our best efforts to assess sensitivity to design choices, each alternative may have resulted in very different outcomes [156  ###reference_b156###  ###reference_b156###], and we are under-powered to test so many sparse combinations. Using less sparse groupings introduces biases\u2014for example, focusing on region risks lumping together participants from particular geographies as \u201ccultures\u201d [59  ###reference_b59###  ###reference_b59###]. While we split out the UK and US to avoid these countries dominating their respective regions, there remain varying degrees of country-wise entropy in other regions\u2014as aforementioned, the Middle East has 94% individuals from Israel, and 100% of Non-US Northern Americans are Canadian (see Appendix H  ###reference_###  ###reference_###). Similarly, we use more aggregated ethnicity and religion groupings for statistical power, but amorphous and heterogeneous categories like \u201cOther\u201d have limited or flawed real-world meaning as \u201cOther\u201d contains, for example, both those who identify as Indigenous or First Peoples and as Middle Eastern or Arab. It is an exciting direction for future work to explore free-form characterisations of identity (e.g., the free-text profile or system string) or ex-post groupings of people\u2019s preferences [8  ###reference_b8###  ###reference_b8###], and examine how findings change when we break away from neatly-observed but essentialising demographic traits [157  ###reference_b157###  ###reference_b157###].\nBeyond the complexities of intersectional identity and idiosyncratic variance of individuals within identity groups, there are other sources of variance in Prism which present a challenge for controlled experiments; particularly, the high-dimensionality of what exact topics each participant chooses to talk about, which models randomly get selected in-the-loop, and the stochasticity in their responses from a non-deterministic temperature. It is hard to pin down robust mechanisms of preference differences amongst individuals with so many sources of variation. We opted for choice of input prompt and conversation to be a free parameter in Prism as a more naturalistic setting of LLM use and because we wanted to understand dialogue diversity among participants. We do empirically find some regions of fixed prompt-response pairs from individuals who self-select into asking the same prompts as other participants (see Appendix P  ###reference_###  ###reference_###). However, in a future specification of our task, we plan to sample multiple of our participants to rate the same fixed context, like in Dices [65  ###reference_b65###  ###reference_b65###], which grants more statistical power to understand differences in preferences over fixed model outputs.\nRelatedly, our conclusions may be confounded by measurement invariance given our explicit focus on subjective, fluid and cardinal devices. This echos the economist\u2019s view, that it is foolish to rely too heavily on cardinal ratings over ordinal rankings to make interpersonal comparisons, or enforce preference construction, where intrinsic feelings are noisily-quantified on numeric scales.272727For example, our analysis using the meanRating welfare measure assumes that individuals use scores in the same way for ratings welfare measures. However, our analysis using meanChoice is not sensitive to use of ratings scale, and the results are qualitatively similar. There are also issues of preference falsification: while participants are financially incentivised to participate, they may not honestly report their preferences over models. We cannot rule out the possibility that participants select a \u2018bad\u2019 model to lock in for the subsequent turns of conversation if it is more interesting (thus preferable in our narrow task confines) to talk to a more offensive or controversial model, or to try to \u2018jailbreak it\u2019 [17  ###reference_b17###  ###reference_b17###]. In hindsight, it may have been a smarter design choice to force participants to rank model responses, or to collect both ratings and rankings (notwithstanding decision fatigue), or make attempts to elicit more interpersonally comparable data via a willingness-to-pay monetary unit. Previous work also raises concerns over relying on human feedback as \u2018gold standard\u2019, for example whether participants can accurately rate factuality of an output, or are anchored on formatting and \u2018first impressions\u2019 (as we and Hosking et al. [158  ###reference_b158###  ###reference_b158###] both find). Preferences, especially at a fine-grained level like in Prism, have high context-dependency [159  ###reference_b159###  ###reference_b159###], so we caution against taking the ratings as revealing some objective truth, instead staying firmly rooted in the subjective paradigm [11  ###reference_b11###  ###reference_b11###, 2  ###reference_b2###  ###reference_b2###].\nMuch of AI, NLP and now RLHF is underpinned by crowdworker labour [160  ###reference_b160###  ###reference_b160###]. Despite our aims to include more diverse voices in LLM development processes, we avoid overstating claims on diversity. Prism still only contains crowdworkers, who have significant sample biases [161  ###reference_b161###  ###reference_b161###]; can only be so \u201crepresentative\u201d given the relatively small sample sizes; must be digital natives given the platformed nature of the work; and possess different incentives for engagement [162  ###reference_b162###  ###reference_b162###].\nFor example, in our welfare analysis, despite having samples balanced by observed demographics for the UK and the US, the samples are too small to to expect them to be representative on features we do not observe. Furthermore, while Prism gains some dialectical diversity from different geographies of English, from varying speaker fluency, and from some contributions in other languages (1%, mainly Spanish), it is almost exclusively in English. Cultural diversity can only be measured so far without also accounting for linguistic diversity [37  ###reference_b37###  ###reference_b37###]. Furthermore, while we try to sample from many regions, our sample is still dominated by White Western participants, especially when considering cultural phylogeny [59  ###reference_b59###  ###reference_b59###], i.e., the non-independence of populations with shared history or migrations of peoples (for example, Australia vs UK vs Canada). We encourage future work prioritising human feedback collection in other languages to understand how models handle sociocultural and linguistic interactions [112  ###reference_b112###  ###reference_b112###].\nWhen data collection began in mid-November, Prism contained the top ranking models on publicly available leaderboards but new models have since emerged. There is an incompatibility between the current pace of model releases and doing human participant research that requires lengthy processes of ethics approval, interface design, data processing and manual annotation. The expense and inconvenience of doing human research increases the attractiveness of simulating responses, usually with GPT-4 [57  ###reference_b57###  ###reference_b57###]. So, while Prism does miss out on the newest players to enter the battle arena, we do provide carefully-sourced human data (including a survey which stands independently from the LLM conversations) combined with a wide distribution of model texts; so we hope the utility of the data persists in the coming years even as models change. We are still potentially limited when comparing open and closed-access models: while the former allows full transparency over system prompts, closed-access models can obscure additional instructions as hidden context. Including models from the same family allows comparisons by version or size, but introducing clones (models producing very similar outputs) can distort preference rankings [8  ###reference_b8###  ###reference_b8###]. Prism is also limited by value-lock in [57  ###reference_b57###  ###reference_b57###]\u2014the models are already tuned to cultural perspectives or alignment norms [34  ###reference_b34###  ###reference_b34###, 35  ###reference_b35###  ###reference_b35###], which precludes observing certain group preferences towards a wider set of behaviours [43  ###reference_b43###  ###reference_b43###, 163  ###reference_b163###  ###reference_b163###], and renders participants \u201cthin\u201d because they are \u201climited to existing designs with pre-existing purposes.\u201d [p.3, 23  ###reference_b23###  ###reference_b23###]."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "",
            "text": "In one of the earlier seminal works on using human feedback to align AI systems, Bai et al.  ###reference_b15### discuss the need for alignment data as a public good. We echo this sentiment in our project culminating two years later, where we present Prism\u2014a new alignment dataset from 1,500 real humans, motivated by the need for inclusive and participatory processes that facilitate open scientific research on one of the most pressing normative and technical issues of our time: who decides how LLMs behave? We demonstrated the richness of Prism via three case studies, asking whether people talk about different things with LLMs, if they prefer differently aligned models and why it matters for societal welfare who we include in human feedback processes. These experiments only scratch the surface of research questions answerable with Prism\u2014we are excited to see how it can be used by social scientists and computer scientists alike to better understand how people from around the world perceive what it means for an LLM to be aligned.\nWe think it is important to be open and transparent about things we learnt during this process, and indicate what could be done differently a second-time around or with more resources. In general, we were bounded by the trade-off between sample breadth vs depth, where seeking both diversity and representativeness limits statistical power. Focusing on fewer models (say only command and gpt-4) would have permitted stronger conclusions about group differences in preferences between these models, but would ignore how other models are rated; Focusing all our annotation budget in the US would have enabled greater representativeness there but missed out on geographic and multicultural variation; Getting many participants to rate the same input\u2013output pair would control for conversational confounders in preferences but at the expense of missing out on what these people choose to talk to LLMs about; and relying on ordinal not cardinal preference profiles may have eliminated noise from measurement invariance but flattens the signal from groups for whom the technology does not work at all.\nAs the community devotes an ever-growing focus to \u201cscaling\u201d model capabilities, compute, data and parameters, we are concerned with how these systems scale across diverse human populations. Initial findings from Prism reveal human preferences vary substantially person-to-person, suggesting scale to participation in human feedback processes is a key consideration, especially when alignment norms are dependent on subjective and multicultural contexts. As LLMs scale across more diverse populations, we hope Prism allows investigation of emergent questions\u2014how we collect preferences, over what, from whom, for what end\u2014and promotes a new science of human feedback learning that is transparent, robust and inclusionary."
        }
    ],
    "url": "http://arxiv.org/html/2404.16019v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.16019v1",
        "paper_title": "The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models",
        "research_background": "### Motivation\n\nThe paper seeks to address the critical role of human feedback in the alignment of large language models (LLMs), which involves steering AI systems towards more human-preferred states. It emphasizes the need to explore how human feedback is collected, who provides this feedback, and the implications of such feedback for the personalized and collective alignment of AI systems. The motivation arises from current gaps and limitations in existing datasets, which often lack diverse and representative samples, nuanced feedback mechanisms, and transparency about the contributors providing the feedback.\n\n### Research Problem\n\nThe main research problem revolves around understanding and improving the methodologies for collecting human feedback for aligning LLMs. Specifically, the paper identifies several challenges:\n\n1. The need for clear annotation paradigms detailing who decides which values to encode in feedback.\n2. The necessity of collecting fine-grained ratings and explanations instead of binary preference comparisons.\n3. The importance of obtaining representative and diverse samples, rather than small, narrow, and often unrepresentative datasets.\n4. The requirement for transparency about the contributors, including their sociodemographic backgrounds, to investigate sample biases.\n\nThe paper proposes addressing these challenges through a new resource named Prism, which aims to map detailed survey responses from a diverse set of participants onto their interactions with LLMs.\n\n### Relevant Prior Work\n\n1. Various references underline the importance and challenges of collecting diverse and representative human feedback for AI alignment (e.g., [1], [2], [3], [4]).\n2. Existing datasets often exhibit common traits such as lacking clear annotation paradigms, relying on binary preference comparisons, and being created by narrow and unrepresentative samples ([11], [12], [13], [14], [15]).\n3. Contextual preferences typically captured by existing datasets have limitations and do not sufficiently address the alignment problem ([1], [19]).\n4. The technical and statistical issues in learning from human feedback have been widely studied ([20], [21], [22]), but there is less focus on data-centric human factors.\n5. The paper refers to a broad collection of studies concerning the effects of training data biases and the importance of sourcing diverse perspectives ([34], [35], [36], [37]).\n\n### Conclusion\n\nIn summary, the paper introduces Prism as a novel dataset designed to capture human feedback from a broad and diverse set of participants, aiming to address the limitations of current datasets in the LLM alignment ecosystem. By focusing on participatory, representative, individualised, subjective, and multicultural factors, Prism aims to provide a richer and more nuanced understanding of human preferences, which could lead to better-aligned AI systems for both individual and collective contexts.",
        "methodology": "The Prism Alignment Project leverages a two-stage methodology to capture diverse human preferences and interactions with large language models (LLMs). First, participants fill out a survey that gathers demographic information and their stated preferences regarding dynamics and use cases of LLMs. This is followed by multi-turn interactive conversations where participants input prompts, rate responses, and provide detailed feedback. This dual approach serves three objectives: it aligns LLM responses with individual participant characteristics, tracks divergences between contextual and stated preferences, and empowers participants to articulate their preferences autonomously.\n\nKey components of the study are:\n\n1. **Survey Stage**: Participants respond to demographic questions and set out their preferences about LLM behaviors and specific attributes they value. They answer questions about familiarity with LLMs (whether they've used them indirectly or directly) and the frequency and nature of their usage.\n\n2. **Conversations Stage**: Participants engage in various types of conversations with LLMs\u2014ranging from unguided to values-guided and controversy-guided\u2014to ensure diversity in interaction contexts. They receive up to four responses from different LLMs, rate these on a visual analog scale (VAS) from \"Terrible\" to \"Perfect\", and provide feedback on why they rated certain responses highly.\n\n3. **Contextual Adaptations**: System strings act as global instructions to guide LLM behavior throughout interactions. Participants are primed to select conversation types to encourage varied prompt formats and enhance the study\u2019s exploration of subjective-objective preference spectra.\n\n4. **Data Collection and Ethics**: Participants are compensated for their time, with ethics board approval and informed consent obtained for all stages. Data collection was methodically conducted with a global sample, although it skewed towards Western and educated participants who are English speakers.\n\n5. **Model Variability**: To ensure robustness and relevance despite the rapid evolution of LLMs, the study incorporated a diverse set of 15 models from different families and providers. Responses were limited to 50 words to mitigate text length confounding and user fatigue.\n\n6. **Detailed Feedback Mechanisms**: Beyond numerical ratings, participants also provide open-ended feedback, which helps fine-tune understanding of preferred LLM characteristics. The methodology avoids anchoring bias by hiding numeric scores and seeks deep qualitative insights into user interactions.\n\nThe innovations of Prism lie in its person-centered, diverse, and real-time feedback-based approach, ensuring that LLM alignments reflect varied human values. The use of VAS for rating and its comprehensive demographic and characteristic mapping avoid oversimplified user representation, setting a precedent for nuanced LLM training and alignment.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Main Experiment Setup:\n\n**Datasets and Preprocessing:**\n1. **Sentence Embeddings:** The all-mpnet-base-v2 sentence transformer model is used to generate 768-dimensional embeddings for opening prompts.\n2. **Dimensionality Reduction:** UMAP reduces the embedding dimensionality to simplify clustering.\n3. **Clustering:** HDBScan clusters the prompts, with specific hyperparameters like a minimum cluster size of 80 and no forced assignment, resulting in 70% prompts in 22 clusters and 30% as outliers.\n4. **Cluster Analysis:** TF-IDF extracts salient unigrams and bigrams from each cluster, and prompts closest and furthest from cluster centroids are identified.\n5. **Cluster Labeling:** gpt-4-turbo assigns descriptive names to each cluster.\n\n**Regression Analysis:** \n- Regression is performed to analyze how demographic attributes (gender, age, region, ethnicity, religion) influence the choice of conversation topics. The relationship is measured using Ordinary Least Squares and clustered standard errors at the individual level.\n\n**Local Neighbourhood Analysis:**\n- Single-link hierarchical clustering identifies local neighbourhoods within the embedding space at varying cosine distance thresholds.\n- Semantic similarity within neighbourhoods is tightly constrained.\n- Demographic characteristics of prompt authors in each neighbourhood are captured.\n\n**Evaluation Metrics:**\n1. **Shannon Entropy:** Used to measure the demographic diversity within local neighbourhoods.\n2. **Comparison of Model Responses:** Pairwise Rank Centrality and Convergence Voting methods were used for ranking models based on participant preferences.\n\n**Preference Ratings Normalization:** \n- Scores from participants are z-normalized to control for individual rating biases.\n- A tie threshold set between 5-10 helps eliminate rating noise.\n\n**Aggregation Function:** \n- Pairwise Rank Centrality is used to determine the most preferred models from multiple interactions.\n\n**Sampling Schemes:**\n- Various participant sub-populations are analyzed through different sampling schemes to see their effect on model preferences and overall welfare.\n\n\n#### **Main Experimental Results:**\n\n1. **Topic Influence by Demographics:**\n   - Topics sensitive to \"controversy\" prompts tend to touch on divisive issues like gender and LGBTQ+ identity, the Israel-Palestine conflict, and abortion.\n   - Values-guided conversations address managing relationships, job searches, and religion, compared to the more task-oriented unguided control group.\n\n2. **Significant Relationships in Topic Preferences:**\n   - Women and non-binary participants are more likely to discuss gender and LGBTQ+ topics.\n   - Older individuals (55+) tend to focus on elections and travel, while younger participants (18-24) discuss job searches and managing relationships.\n   - Black participants discuss climate change less frequently than White participants.\n   - Middle Eastern participants frequently discuss the Israel-Palestine conflict.\n\n3. **Local Neighbourhood Diversity:**\n   - Only 3.4% of prompts form unique local neighbourhoods, indicating high semantic diversity.\n   - Larger neighbourhoods are usually demographically diverse, with fewer than 1% of prompts in homogeneous neighbourhoods across all demographic attributes.\n\n4. **Preference Aggregation Sensitivity:**\n   - Rankings of models are sensitive to the sampled individuals and conversation domains.\n   - Model rankings and user preferences vary significantly depending on geographic and demographic contexts.\n\n5. **Model Performance Evaluation:**\n   - gpt-4 models perform worse in Prism than in ChatbotArena, possibly due to differences in participant community and conversation topics.\n   - Better scoring models often feature longer text, more formatted responses, and tend to avoid phrases indicating refusal or de-anthropomorphism.\n\n6. **Welfare Distribution Analysis:**\n   - Larger samples from a sub-population tend to yield better mean welfare.\n   - Exclusive sampling from a non-representative group can have adverse effects on those not sampled.\n   - The model that maximizes meanChoice is often not the most preferred by the majority, indicating no single model is best suited for all participants.\n\nIn summary, the experiment elucidates how demographic attributes influence topic choices within conversations and highlights the importance of aggregating preferences to account for participant diversity. Models\u2019 performances and participant preferences exhibit substantial variance across different demographic and conversational contexts."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Do different people initiate different discussions with LLMs? The goal is to understand if there are demographic-based variations in the topics people choose for their opening conversational turn with LLMs.",
            "experiment_process": "The experiment used human-authored opening prompts without model response to avoid confounding. First, the prompts were embedded using all-mpnet-base-v2 sentence transformer, then dimensionality was reduced using UMAP. The prompts were clustered using HDBScan, a density-based clustering algorithm. TF-IDF was used to extract salient unigrams and bigrams from each cluster, and gpt-4-turbo assigned descriptive names to clusters. Regression models were used to estimate demographic attributes' contributions to topic choices, and hierarchical clustering was used on the embedding space to understand local neighbourhoods.",
            "result_discussion": "The findings indicate that conversation topics significantly vary with demographics. For instance, women and non-binary participants are more likely to discuss gender and LGBTQ+ identity, older people discuss elections and travel more, and US participants frequently discuss abortion. There is substantial semantic diversity among prompts, but common discussion spaces are shared by people from different backgrounds, making them valuable for examining diverse perspectives on shared issues.",
            "ablation_id": "2404.16019v1.No1"
        },
        {
            "research_objective": "Do different people prefer differently-aligned models? This aims to explore if participant preferences vary significantly across different LLMs based on their demographic attributes.",
            "experiment_process": "Preferences were measured using ratings over four randomly chosen models in response to opening prompts. Both idiosyncratic and groupwise variations were examined. The balanced subset of participants with equal conversations per type was used to control for context. Scores were normalized for analysis, and preference aggregations were performed using Pairwise Rank Centrality derived from win-loss ratios between models.",
            "result_discussion": "The study found significant variability in preferences based on participant sampling, topic domains, and geographic regions. This suggests community-dependent model preferences and highlights models like GPT-4 performing poorly in Prism compared to ChatbotArena, whereas models like zephyr-7b did better in Prism. Factors like response length, formatting, and refusal phrases correlated with preference differences.",
            "ablation_id": "2404.16019v1.No2"
        },
        {
            "research_objective": "How do the characteristics of the participants used to train LLMs impact the welfare of different sub-populations? This seeks to understand how different demographic groups influence the alignment and perceived utility of LLMs based on their feedback.",
            "experiment_process": "The study used two welfare measures: average model rating and the likelihood of a model being chosen. Different sub-populations were simulated to choose their preferred LLM, and the welfare distribution of each group was measured. The analysis was conducted on UK and US representative samples with sampling schemes tested for different demographic groups using the method maxRating for mean ratings and maxChoice for model choice.",
            "result_discussion": "The results showed that larger samples result in better welfare outcomes, implying the importance of diverse feedback. Sampling from non-representative groups can reduce welfare for out-group individuals. The analysis also revealed that no single LLM is universally the best, highlighting the need for personalized AI models.",
            "ablation_id": "2404.16019v1.No3"
        }
    ]
}