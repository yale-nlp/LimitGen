{
    "title": "KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking",
    "abstract": "This paper introduces KnowHalu, a novel approach for detecting hallucinations in text generated by large language models (LLMs), utilizing step-wise reasoning, multi-formulation query, multi-form knowledge for factual checking, and fusion-based detection mechanism. As LLMs are increasingly applied across various domains, ensuring that their outputs are not hallucinated is critical. Recognizing the limitations of existing approaches that either rely on the self-consistency check of LLMs or perform post-hoc fact-checking without considering the complexity of queries or the form of knowledge, KnowHalu proposes a two-phase process for hallucination detection.\n\nIn the first phase, it identifies non-fabrication hallucinations\u2014responses that, while factually correct, are irrelevant or non-specific to the query. The second phase, multi-form based factual checking, contains five key steps: reasoning and query decomposition, knowledge retrieval, knowledge optimization, judgment generation, and judgment aggregation.\n\nOur extensive evaluations demonstrate that KnowHalu significantly outperforms SOTA baselines in detecting hallucinations across diverse tasks, e.g., improving by  in QA tasks and  in summarization tasks, highlighting its effectiveness and versatility in detecting hallucinations in LLM-generated content. The code is available at https://github.com/javyduck/KnowHalu.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Significant advancements have been achieved in the field of Natural Language Processing (NLP) with the advent of Large Language Models (LLMs). While these models excel in generating coherent and contextually relevant text, they are prone to \u2018hallucinations\u2019 \u2014 generating plausible but factually incorrect or unspecific information [1]. This poses a considerable challenge, especially in applications demanding high factual accuracy, such as medical records analysis [2], financial reports [3, 4], or drug design [5, 6].\n\nTo mitigate or detect hallucinations in LLMs, a series of approaches have been explored. For instance, self-consistency-based approaches detect hallucinations by identifying contradictions in responses that are stochastically sampled from the LLMs in response to the same query [7]. Other approaches detect hallucinations by probing LLMs\u2019 hidden states [8] or output probability distributions [9]. These methods do not incorporate external knowledge and are thus limited by LLMs\u2019 internal knowledge. Post-hoc fact-checking approaches have been recently shown to be effective even when LLMs\u2019 internal knowledge proves inadequate and achieved SOTA hallucination detection [10, 11]. However, due to the limitation of LLM reasoning capabilities, even when the extracted knowledge is correct, the models still have a hard time performing accurate factual checking, especially when the queries or logics are complicated (e.g., involving multiple factual assertions).\n\nRecognizing this gap, our work proposes a novel multi-phase hallucination detection mechanism, KnowHalu (pronounced as \u201cNo Halu\u201d), the overall framework of which is presented in Figure 1. In particular, we first perform the non-fabrication hallucination checking, where the answer indeed provides a fact but it is not the answer to the question, such as the answer \u201cChatGPT can do lots of things\u201d for the question \u201cWhat can ChatGPT do?\u201d This step is critical and largely missing in existing hallucination detection approaches, which fail to detect such non-relevant answers.\n\nWe then perform a step-wise decomposition of queries, which enables targeted retrieval of external knowledge pertinent to each logical step. For each decomposed logical step, we will perform the multi-form knowledge based factual checking, leveraging both the unstructured knowledge (e.g., normal semantic sentences) and structured knowledge (e.g., object-predicate-object triplets). This multi-form knowledge analysis captures a comprehensive spectrum of factual information, enhancing the reasoning capability of LLMs and ensuring a robust and thorough validation of each piece of retrieved knowledge. Finally, we perform the reasoning step by composing the step-wise factual checking results together and guide the LLMs to make the final judgment by providing related demonstrations. Our multi-step and multi-form knowledge based factual checking not only improves the accuracy of verification but also enhances the model\u2019s ability to handle intricate and layered queries.\n\nOur extensive experimental evaluations reveal that KnowHalu significantly outperforms state-of-the-art (SOTA) baselines in detecting hallucinations. The experiments, conducted across diverse datasets and tasks, demonstrate not only the high accuracy of our method in factual verification but also its versatility in handling various types of queries. In summary, we make the following key contributions:\n\nWe introduce KnowHalu, a novel approach with two main phases (non-fabrication hallucination detection and multi-step factual checking) for detecting hallucinations in text generated by LLMs, leveraging multi-form knowledge for factual checking. In particular, we define the categories of non-fabrication hallucinations in general for the first time.\n\nWe are the first work to explore the influence of both the formulations of the queries and the form of knowledge used for detecting hallucinations, highlighting our novel exploration into factors critical for improving hallucination detection accuracy. We further propose a verification mechanism where collections of facts are checked interdependently instead of in parallel, and an aggregation methodology based on the prediction results from different forms of knowledge to further reduce the hallucinations in judgment itself. Our experiments show that our method achieves a 15.65% improvement in hallucination detection on the question-answering task, and yields an additional 5.50% improvement in the text summarization task when compared to the SOTA baselines."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Hallucination of LLMs. Hallucination in the LLM literature generally refers to LLMs generating nonfactual, irrelevant, or unspecific outputs. Such phenomena have been observed in a variety of tasks [12], such as translation [13], dialogue [14], summarization [15], and question answering [16]. We summarize and describe the hallucination types for the QA task covered in this paper in Table 1. Benchmarks have been proposed to evaluate the extent to which LLMs hallucinate, such as Honovich et al. [17], Huang et al. [18], and Li et al. [19].\n\nHallucination detection and mitigation. Methods that attempt to detect and mitigate hallucination without fact-checking include methods based on chain-of-thought [20, 21, 22], methods based on self-consistency [7, 21], and methods that probe LLMs\u2019 hidden states [8] or output probability distributions [9]. Since these methods do not augment LLMs with external knowledge, they struggle when LLMs\u2019 internal knowledge is inadequate. On the other hand, fact-checking-based methods [23, 24, 25, 26, 27, 28, 19, 11] rely on retrieved knowledge to prevent hallucinations in LLMs. However, these methods are often limited by the ways they retrieve knowledge and utilize the retrieved knowledge. For instance, Li et al. [19] employs only a single query with the knowledge for detecting hallucinations in inputs that inherently necessitate multi-hop reasoning, which would benefit from a step-by-step query process. Semnani et al. [11] instead introduces a robust framework initially designed to deliver fact-checked responses. This innovative method employs a comprehensive process that entails generating queries to fetch information from Wikipedia, summarizing and filtering the retrieved content, and then crafting a response informed by this vetted knowledge. However, when adapting WikiChat for hallucination detection, although it effectively uses retrieved knowledge for fact-checking in parallel, it occasionally neglects the coherence of the facts being verified, potentially leading to inaccuracies. Besides, during the fact-checking phase, WikiChat directly retrieves the evidence with the claim, which may not be effective when the claim itself is the result of hallucination as shown in Appendix B. Our approach, in contrast, derives step-by-step queries with refined formulations and provides LLMs with either structured or unstructured knowledge for consecutive fact-checking, leading to better retrieval and higher knowledge utilization."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "KnowHalu",
            "text": "KnowHalu provides a systematic hallucination detection framework based on multi-step query and reasoning.\nIt starts with \u201cNon-Fabrication Hallucination Checking\u201d to pinpoint non-specific hallucinated answers, followed by \u201cFactual Checking\u201d, verifying the correctness of the answer through a multi-step process based on different forms of knowledge."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Non-Fabrication Hallucination Checking.",
            "text": "Current approaches in hallucination detection can mainly identify fabricated hallucinations, i.e., answers with mismatching facts [9  ###reference_b9###, 10  ###reference_b10###, 11  ###reference_b11###].\nYet, hallucinations also emerge in other types, as outlined in Table 1  ###reference_###, which extend beyond simple factual inaccuracies. A typical trait of these non-fabricated hallucinations is their factual correctness while a lack of direct relevance to the original query. For instance, given a question, \u201cWhat is the primary language in Barcelona?\", the hallucinated answer \u201cEuropean languages\" is factually correct but fails in providing specific answers, which is important for the quality of real-world LLMs.\nTo bridge this gap, in KnowHalu, we first introduce a \u201cNon-Fabrication Hallucination Checking\" phase, as depicted in the first row of Figure 1  ###reference_###. This step aims to identify non-fabrication hallucinations. A straightforward approach might involve prompting the language model to identify such hallucinations based on provided examples. However, this often results in high false positives, inaccurately flagging correct answers as hallucinations. To counter this, we address this challenge by solving an extraction task, which prompts the language model to extract specific entity or details requested by the original question from the answer. If the model fails to extract such specifics, it returns \u201cNONE\".\nThis extraction-based specificity check is designed to reduce false positives while effectively identifying non-fabrication hallucinations. Responses yielding \u201cNONE\" are directly labeled as hallucinations, and the remaining generations will be sent to the next phase for further factual checking.\nExamples of instructions in the extraction task for each type of non-fabrication hallucination are provided in Section A.1  ###reference_###."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Factual Checking",
            "text": "The Factual Checking phase consists of five key steps: (a) Step-wise Reasoning and Query breaks down the original query into sub-queries following the logical reasoning process and generates different forms of sub-queries; (b) Knowledge Retrieval retrieves knowledge for each sub-query based on existing knowledge database; (c) Knowledge Optimization summarizes and refines the retrieved knowledge, and maps them to different forms, such as unstructured knowledge (object-replicate-object triplet); (d) Judgment Based on Multi-form Knowledge assesses the answer for each sub-query based on multi-form knowledge; and (e) Aggregation combines insights of judgments based on different forms of knowledge and makes a further refined judgment.\na. Step-wise Reasoning and Query\nIn this step, we aim to break down the original query into local sub-queries following the reasoning logic, and we will retrieve knowledge for each sub-query sequentially (details in steps b and c), which is similar with ReAct [29  ###reference_b29###], to cumulatively perform factual checking along the reasoning process.\nOne main challenge here is \u201chow do we craft precise and effective sub-queries, which can accurately retrieve the relevant knowledge at each logical step?\u201d To address this challenge, we identify two key factors that significantly enhance the accuracy of knowledge retrieval for factual checking: (1) continuous and direct (one-hop) queries, and (2) the formulation of queries. We will analyze these two key factors, which lead to our design choice below.\nFirst, we observe that multi-hop queries (e.g., Example-1 in Figure 1  ###reference_###) often struggle to retrieve specific and related knowledge due to their inherently complex and ambiguous context. On the other hand, the one-hope queries are effective to retrieve the most relevant and useful knowledge.\nThus, we decompose the original query into sequence of simpler and direct one-hop sub-queries following the logical reasoning process, which significantly enhance the retrieval accuracy for factual checking.\nConcretely, this iterative querying process starts by interpreting the original query as a series of logical steps to form sub-queries accordingly, and then perform factual checking for each sub-query.\nFor instance, based on the example in Figure 1  ###reference_###, the initial query first confirms whether \u201cStar Wars\" is indeed the 1977 space-themed movie featuring Luke Skywalker. This step is crucial for connecting the movie to its composer. Subsequent queries delve deeper, examining the accuracy of other specific details provided in the answer, such as the composer\u2019s identity. The queries are intricately connected, each building upon the knowledge obtained from the previous one. This iterative process continuous to generate subsequent queries based on newly acquired knowledge, until the logical reasoning process is completed.\nSecond, we observe that the formulation of queries also plays a critical role for the final factual checking. In particular, queries with correct details will lead to high-quality knowledge retrieval; while queries with incorrect or unrelated entities may lead to poor and irrelevant knowledge retrieval.\nAs a result, we propose two query formulations: General Query and Specific Query.\nThe General Query avoids mentioning specific, potentially hallucinated details (e.g., \u201cWho composed the score for \u2019Star Wars\u2019?\");\nthe Specific Query is constructed based on the key entities mentioned in the answers (e.g., \u201cDid Joy Williams compose the score for \u2019Star Wars\u2019?\"), as shown in Figure 1  ###reference_###.\nMore concrete examples illustrating the impact of these two query formulations on the retrieval outcomes for both correct and hallucinated details can be found in Appendix B  ###reference_###.\nIn our experiments shown in Section 5.2  ###reference_###, we examine how different query formulations \u2014general and specific \u2014 affect knowledge retrieval and the accuracy of final hallucination detection, by leveraging only one or both query formulations.\nMore detailed prompts are provided in Section A.2  ###reference_###.\nb. Knowledge Retrieval\nWe perform knowledge retrieval for each sub-query generated from step a.\nIn particular, for QA tasks, we adopt the Retrieval-Augmented Generation (RAG) framework developed based on WikiPedia knowledge base [11  ###reference_b11###], and the retrieval is based on ColBERT v2 [30  ###reference_b30###] and PLAID [31  ###reference_b31###]. We retrieve Top-K relevant passages for each sub-query, each formatted as \u201cTitle: \u2026, Article: \u2026\".\nIn addition, when we perform knowledge retrieval for summarization tasks, we treat the source document itself as the knowledge base for retrieval. In particular, we first segment the original documents into distinct text chunks. We then embed the sub-queries and text chunks into dense vectors using a text encoder. Similarly, we will retrieve the Top-K text chunks that exhibit the highest cosine similarity with the input sub-queries.\nc. Knowledge Optimization\nThe knowledge retrieved for each sub-query is usually a long and verbose passage with distracting irrelevant details. Thus, this step aims to leverage another LLM to distill useful information and optimize clear and concise knowledge, which could be in different forms.\nIn particular, we propose two forms of knowledge, unstructured and structured knowledge.\nThe unstructured knowledge represents the texts retrieved from given knowledge bases in a concise way, such as \u201c\u2018Star Wars,\u2019 released in 1977, is the space-themed movie in which the character Luke Skywalker first appeared.\u201d\nSince the unstructured text may not be precise for logical reasoning, we also retrieve structured knowledge as object-predicate-object triplets, such as (\u201cStar Wars\", was, 1977 space-themed movie) and (Luke Skywalker, first appeared in, \u201cStar Wars\")\n(examples of our demonstrations are in Section A.3  ###reference_###).\nSuch multi-form knowledge will effectively assist LLMs to perform logical reasoning and final factual checking.\nIn addition, if a query retrieves no relevant knowledge, the LLM is instructed to respond with \u201cNo specific information is available\".\nd. Judgment Based on Multi-form Knowledge\nAfter obtaining the retrieved multi-form knowledge for sub-queries, we gather #Query# and #Knowledge# and present them to another LLM for hallucination judgment.\nThe #Judgment# assesses the sub-query and its corresponding knowledge sequentially to ascertain if there is any contradiction to verification each detail in the answer. If there is any conflict between the answer the knowledge from a sub-query, the judgment is INCORRECT. On the other hand, if all the details of the answer are verified by the knowledge of sub-queries, the judgment is CORRECT. For some scenarios, where the knowledge is inadequate for a conclusive judgment, the output will be INCONCLUSIVE.\nPrompts used to guide this judgment process are shown in Section A.4  ###reference_###.\ne. Aggregation \nThe judgment of hallucination above is based on each form of the retrieved knowledge (e.g., structured and unstructured). To mitigate the prediction uncertainty, here we aggregate these judgment based on the multi-form knowledge to make the final prediction.\nThe motivation for this aggregation mechanism is two-fold: 1) each knowledge form might uniquely identify cases that the other cannot, particularly when one yields an INCONCLUSIVE judgment and the other does not; 2) when the LLM makes a hallucinated judgment, it may lead to low confidence scores for the judgment of \u201cCORRECT\" or \u201cINCORRECT.\" Thus, we can rely on the alternative knowledge form if it demonstrates a higher confidence for the judgment.\nConcretely, we treat the judgment based on one form of the knowledge\u2014typically the form yielding better average accuracy\u2014as the base judgment and that of the other forms of knowledge as supplement judgment.\nIf the confidence score for the base judgment falls below the a specific threshold  and the supplement judgment maintains higher confidence above , we will take the supplement judgment as the final prediction. In all other cases, the base judgment will perform as the final prediction. The corresponding pseudo-code for this aggregation mechanism is provided in Section E.3  ###reference_###.\nBesides, note that the judgments may undergo different tokenizations. Specifically, \u2018INCORRECT\u2019 might be tokenized as \u2018INC\u2019, \u2018OR\u2019, \u2018RECT\u2019, whereas \u2018CORRECT\u2019 can be tokenized as \u2018COR\u2019, \u2018RECT\u2019 when using the GPT-3.5 model. In our experiments, to maintain consistency and mitigate the impact of varying tokenization patterns on judgment, we consistently rely on the confidence score associated with the first token of the judgment label as the representative probability for the entire judgment."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We have evaluated KnowHalu on the standard HaluEval dataset [19], comparing with SOTA hallucination detection baselines under different settings. We find that 1) KnowHalu consistently outperforms the baselines in terms of hallucination detection in different tasks, 2) different models benefit differently from knowledge forms; GPT-3.5 performs better with structured knowledge, while Starling-7b is more effective with unstructured knowledge; 3) the aggregation of predictions from different knowledge forms can further improve detection accuracy."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "Dataset. We conduct our experiments using the standard HaluEval dataset [19], and focus on hallucination detection for two primary tasks: multi-hop QA and text summarization. For the multi-hop QA task, the dataset comprises questions and correct answers from HotpotQA [32], with hallucinated answers generated by ChatGPT. In the text summarization task, the dataset includes documents and their non-hallucinated summaries from CNN/Daily Mail [33], along with hallucinated summaries generated by ChatGPT.\n\nIn our experiment, we randomly sample pairs from the QA task as the test set. Each test pair comprises both a correct answer and a hallucinated answer to the same question. Additionally, we sampled pairs from the summary task, with each pair containing both accurate and hallucinated counterparts for the same document. We use these balanced test sets for evaluation and comparison.\n\nBaselines. For the QA hallucination detection task, we consider five SOTA baselines: (1) HaluEval (Vanilla) [19], where the LLM is instructed to make a direct judgment without external knowledge; (2) HaluEval (Knowledge) [19], which leverages external knowledge for hallucination detection; (3) HaluEval (CoT) [19], which incorporates Chain-of-Thought reasoning [20] for hallucination detection; (4) GPT-4 (CoT) [34], which leverages GPT-4\u2019s intrinsic world knowledge and performs CoT reasoning; and (5) WikiChat [11], which generates responses by retrieving and summarizing knowledge from Wikipedia, based on which it refines responses to ensure accurate answer through fact-checking.\n\nFor the Summary hallucination detection task, we adopt three baselines: (1) HaluEval (Vanilla) [19], where the LLM is instructed to provide a direct judgment based on the source document and summary, (2) HaluEval (CoT) [19], which makes the judgment based on few-shot CoT reasoning; (3) GPT-4 (CoT) [34], which is instructed to provide a zero-shot judgment based on GPT-4\u2019s inherent reasoning abilities.\n\nThe prompts used to query GPT-4 (CoT) and WikiChat for hallucination detection are provided in Section A.5, and the version of GPT-4 used here is gpt-4-1106-preview.\n\nModels. For our experiments, we use two models: (1) Starling-7B [35, 36], an open-source model with high performance as shown in LMSYS Chatbot Arena Leaderboard [37, 38]; and (2) GPT-3.5 with version gpt-3.5-turbo-1106, a closed-source model from OpenAI.\n\nMetric. Our evaluation focuses on four key metrics: True Positive Rate (TPR), True Negative Rate (TNR), Abstain Rate for Positive cases (ARP), and Abstain Rate for Negative cases (ARN). TPR quantifies the ratio of correctly identified hallucinations, TNR measures the ratio of correctly identified non-hallucinations. ARP and ARN represent the model capability of identifying inconclusive cases.\n\nNote that it is not always possible to successfully retrieve the corresponding knowledge for verifying the answer.\n\nHowever, existing baselines based on external knowledge still require the model to provide a binary judgment (Yes/No); thus, the accuracy reported could be higher than their actual performance since some answers actually cannot be assessed with the available knowledge. On the contrary, KnowHalu allows the INCONCLUSIVE option to provide more informative judgments based on our capable framework."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Hallucination Detection on QA Task",
            "text": "Setup. To detect hallucinations in the QA Task, we test two distinct knowledge sources. The first, which we refer to as \u201coff-the-shelf knowledge,\" is the ground truth knowledge provided in the HaluEval dataset. This consists of specific passages that are directly related to the question-answer pairs within the dataset, which serves as a natural upper bound for the quality of retrieved knowledge. The second knowledge source, which we refer to as \u201cWiki retrieval knowledge,\" comes from the information retrieval system constructed over the Wikipedia Database as outlined in step (b) in Section 3.2. This system fetches the Top-K most relevant passages in response to a given query. We aim to evaluate the effectiveness of different hallucination detection approaches given these two knowledge sources. When utilizing the \u201cWiki retrieval knowledge\u201d, the number of fetched passages is consistently set to all methods. For our method, we report the results considering different query formulations. A detailed analysis of the influence of different query formulations and is presented in Section 5.2 and Section 5.3, respectively.\n\nResults. The main results are shown in Table 2. We observe that leveraging our sequential reasoning and query approach, coupled with a well-formulated query for knowledge retrieval and the aggregation of two distinct forms of knowledge, KnowHalu consistently outperforms baselines by approximately when using the same knowledge source with the same model. On the contrary, direct retrieval of knowledge without the reasoning sub-queries yields an accuracy of only and for Starling-7B and GPT-3.5 (i.e., HaluEval (Knowledge)). Furthermore, our results reveal several intriguing observations: (1) employing systematic, step-wise reasoning and querying enables a small 7B model within KnowHalu to achieve comparative performance with GPT-3.5; (2) using either Starling-7B or GPT-3.5 in KnowHalu demonstrates superior detection performance when compared to the powerful GPT-4, which has implicit reasoning capabilities and knowledge; (3) the form of knowledge matters for different models\u2014Starling-7B appears to perform better with unstructured knowledge, while GPT-3.5 seems to benefit more from structured knowledge (i.e., triplets), enhancing the need for aggregation mechanisms. A comprehensive analysis of the individual contributions of each component within KnowHalu is in Section 5."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Hallucination Detection on Summarization Task",
            "text": "Setup. In the task of text summarization, the original document serves as the primary source of knowledge. During our experiments, we segment the document into passages with fewer than  words each. Both the input query and retrieved passages are encoded using BGE large model [39] from FlagEmbedding [40, 41, 42]. For each query, the Top-K relevant passages are retrieved for knowledge optimization, with the number of passages set to 3. The impact of varying is further analyzed in Section 5.3.\nUnlike the QA task, to detect hallucinations in the summarization task, any detail in the summary that cannot be supported or identified in the original document will be considered as a hallucination, which means we have a complete knowledge source. As a result, the non-fabrication hallucination checking phase is not required for this task, allowing us to move directly to factual checking. The judgment now only includes CORRECT and INCORRECT, as cases that would be classified as INCONCLUSIVE in the QA task are inherently INCORRECT in text summarization task. In addition, given that some summaries are quite lengthy, we segment the original summary into small parts, with each segment comprising no more than  words. Each segment is independently evaluated for hallucination, and the entire summary is labeled as a hallucination if any part receives an INCORRECT judgment.\n\nResults. The main results are presented in Table 3. We observe that KnowHalu demonstrates a significant improvement over baselines, achieving a increase in performance compared to the baselines when using the same Starling-7B model, and a improvement over the baselines when utilizing GPT-3.5. Notably, we can see that nearly all variations of KnowHalu surpass the powerful GPT-4, demonstrating a superior performance. In particular, GPT-3.5 model demonstrates a great advantage when utilizing structured knowledge, whereas the Starling-7B model benefits more from unstructured knowledge.\n\nOn the other hand, we find the performance of GPT-3.5 used in KnowHalu could have high uncertainty. This is primarily due to two key observations during our experiments: first, GPT-3.5 occasionally rejects certain summaries, which are recognized with inappropriate or potentially invasive content, an issue not analyzed in baselines; second, GPT-3.5 tends to demonstrate \u2018lazy\u2019 behavior during the step-wise querying process. For instance, it may prematurely conclude that a summary does not necessitate further verification, or it might simply conclude the correctness of a summary even without generating any query. One such example is shown below: #Summary#: Shaving cream is the recommended treatment for jellyfish stings, but also helps remove any nematocysts that may be stuck to the skin. #Thought-1#: The summary does not contain any specific claims or details that need verification. Therefore, no queries are needed. Thus, a better base model would further improve KnowHalu. More detailed discussions on these interesting findings and examples are in Appendix D.\n\nIn addition, we also explore the influence of different versions of GPT-3.5 on hallucination detection, with detailed results presented in Appendix C. Consistent with the findings of Chen et al. [43] on HotpotQA [32], we observe a performance decrease (to ) when employing the prompting methods used in HaluEval, comparing version gpt-3.5-turbo-1106 to version gpt-3.5-turbo-0613. However, our method, even achieves a slightly higher performance from to , suggesting it can harness the intrinsic reasoning capabilities of the GPT-3.5 model more efficiently. Furthermore, we observe that the GPT-3.5 models, across both versions tested, show consistent improvement when utilizing structured knowledge forms, i.e., reasoning with triplets. This highlights its adeptness in handling structured information, which is one of the main findings in our KnowHalu framework."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Ablation Study",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Impact of Non-Fabrication Hallucination Checking",
            "text": "Prompts provided by HaluEval [19  ###reference_b19###] not only cover cases of fabrication hallucination but also non-fabrication hallucinations as shown in Table 1  ###reference_###.\nThus, our pipeline separates the process into two phases: (1) treating the detection of non-fabricated hallucinations as an independent task;\n(2) if a non-fabricated hallucination is detected, no further checking is required; otherwise, we proceed to a second-phase for factual checking. This approach raises two intriguing questions: (1) whether such decomposition improves hallucination detection performance, and (2) what is the performance when factual checking is conducted directly without any preliminary non-fabrication hallucination demonstration. As shown in Table 6  ###reference_###, incorporating non-fabrication checking consistently enhances the detection of hallucinated cases, with an approximately  improvement in TPR and  in false positive cases. Furthermore, even without non-fabrication checking, our standalone factual checking phase still surpasses the baseline using the same knowledge source, demonstrating the effectiveness of our multi-query and reasoning process."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Formulations of Queries",
            "text": "We investigate the impact of different query formulations used for knowledge retrieval on the accuracy of hallucination detection. All experiments are conducted using the Starling-7B model with  for Wiki retrieval knowledge. We evaluate the following three approaches: (1) using only specific queries, (2) using only general queries, and (3) combining the Top-K results from both query types. The results are detailed in Table 4  ###reference_###.\nAs we can see, the formulation of the query is crucial in knowledge retrieval. In particular, specific query formulation enhances the accuracy for non-hallucinated cases but reduces that for hallucinated ones due to polluted context. Conversely, using general queries yields an inverse effect. Combining both query types improves the overall detection accuracy by at least  and reduces the abstention rate by over , demonstrating that the combination of both query formulations indeed leads to more accurate and relevant knowledge retrieval. The results conducted with the off-the-shelf knowledge and the results for text summarization are presented in Section E.1  ###reference_###."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Number of Retrieval Knowledge",
            "text": "We explore how the number of retrieved Wiki passages, , impacts the performance of hallucination detection in this section. Throughout this analysis, we consistently employ a combination of both specific and general queries for knowledge retrieval, focusing on assessing the influence of varying . The results are presented in Table 5  ###reference_###. We can observe that increasing the number of retrieved passages enhances detection accuracy and reduces the abstain rate. In addition, the performance converges when  is greater than , and additional knowledge will only provide marginal improvement, highlighting the potential efficiency of KnowHalu. We also provide the results for similar experiments conducted on text summarization in Section E.2  ###reference_###."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Aggregation based on Multi-form Knowledge",
            "text": "We further explore mitigating hallucinations by implementing a confidence-based aggregation mechanism that utilizes various forms of knowledge, both structured and unstructured. The motivation of our approach is the observation that judgments susceptible to hallucinations typically have lower confidence levels compared to those that are accurate and free from hallucinations. Consequently, we adopt a strategy where if a base judgment, derived from one form of knowledge, displays low confidence (below ), and a supplementary judgment from a different form of knowledge shows significantly higher confidence (above ), the latter is prioritized based on its reliability.\nTo select thresholds  and , we employ a data-driven approach that utilizes the quantile of the confidence distribution associated with each form of knowledge. We achieve this by using a small validation set for both tasks, during which we collect confidence distributions for judgments obtained based on each knowledge type. This approach facilitates a more precise evaluation of  and  through an examination of various quantiles within these distributions. By adjusting  and  according to these quantiles on the validation set, we aim to identify the optimal configurations that yield the highest average accuracy for each task. In our experiments, we consistently utilize the judgments based on the form of knowledge that provides the best average accuracy as the base judgment. The specific values of  and , along with a detailed description of the process for selecting them, are provided in Section E.3  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we have introduced KnowHalu, a novel framework for detecting hallucinations in text generated by Large Language Models (LLMs). Our approach stands out by employing a two-phase process: Non-Fabrication Hallucination Checking and Factual Checking, which includes multi-form knowledge retrieval and optimization, along with an aggregation method for the final judgment. Through extensive experimentation on standard datasets, KnowHalu has demonstrated significant improvements over state-of-the-art baselines in detecting hallucinations in both QA and text summarization tasks. Future extensions include adapting the framework for dialogue systems and optimizing it for much longer responses."
        }
    ],
    "url": "http://arxiv.org/html/2404.02935v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.02935v1",
        "paper_title": "KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking",
        "research_background": "### Paper's Motivation\n\nThe paper is motivated by the widespread issue of hallucinations in text generated by large language models (LLMs). These hallucinations, where models produce factually incorrect or unrelated information, pose significant challenges in domains requiring high factual accuracy, such as medical records analysis, financial reporting, and drug design. Existing hallucination detection approaches, such as those that rely solely on internal model knowledge, often fall short in accuracy, especially when dealing with complex queries.\n\n### Research Problem\n\nThe central research problem addressed by the paper is the detection of hallucinations in LLM-generated text by leveraging external knowledge to improve factual accuracy. Specifically, the paper aims to develop a mechanism that not only identifies non-fabrication hallucinations (where the provided fact does not answer the question) but also ensures robust and comprehensive factual checking through multi-form knowledge analysis. The proposed method aims to outperform current state-of-the-art (SOTA) techniques by addressing gaps in existing approaches and enhancing the reasoning capabilities of LLMs.\n\n### Relevant Prior Work\n\n1. **Self-Consistency-Based Approaches**: These methods identify contradictions in various responses stochastically sampled from LLMs to detect hallucinations [7].\n\n2. **Hidden States and Output Probabilities**: Some approaches examine the hidden states [8] or output probability distributions [9] of LLMs, but these methods do not utilize external knowledge and are thus constrained by the model's internal information.\n\n3. **Post-Hoc Fact-Checking**: Recently, post-hoc fact-checking involving external knowledge has shown effectiveness in detecting hallucinations [10, 11]. However, these methods still struggle with accurate factual checking due to LLMs' limited reasoning capabilities, particularly with complex queries or logic.\n\n### Novel Contributions\n\n1. **Introduction of KnowHalu**: A novel approach with two main phases\u2014non-fabrication hallucination detection and multi-step factual checking\u2014leveraging multi-form knowledge including both unstructured and structured data.\n\n2. **Categorization of Non-Fabrication Hallucinations**: This work is the first to define categories of non-fabrication hallucinations in general.\n\n3. **Exploration of Query Formulation and Knowledge Form**: The research explores the impact of query formulations and knowledge forms on hallucination detection accuracy.\n\n4. **Interdependent Fact Verification Mechanism**: A novel mechanism where facts are checked interdependently rather than in parallel, coupled with an aggregation methodology based on predictions from different forms of knowledge to reduce hallucinations in judgments.\n\n### Experimental Findings\n\nThe paper's extensive experimental evaluations illustrate that KnowHalu significantly outperforms SOTA baselines in hallucination detection across diverse datasets and tasks, achieving notable improvements in both question-answering and text summarization tasks.",
        "methodology": "KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking\n\n**Methodology:**\nThe proposed method, KnowHalu, introduces a structured framework for detecting hallucinations by employing a multi-step process involving querying and reasoning. \n\n1. **Non-Fabrication Hallucination Checking**: This initial step aims to identify non-specific hallucinated answers. It serves as a preliminary filter to detect responses that may not be explicitly fabricated but still deviate from realistic or accurate information.\n\n2. **Factual Checking**: This subsequent step involves verifying the correctness of the answer through a multi-step process that leverages various forms of knowledge. This step is critical for ensuring the factual accuracy of the information provided. \n\nThe combination of these two steps enables a thorough and systematic approach to detecting hallucinations in responses, thereby improving the reliability and factuality of the information.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Dataset:**\n- **HaluEval dataset**: The primary dataset for evaluating the model's performance on hallucination detection.\n\n**Baselines:**\n- **SOTA hallucination detection baselines**: The main comparison is against the state-of-the-art hallucination detection models available at the time of the experiment.\n\n**Models:**\n- **KnowHalu**: The proposed model for hallucination detection.\n- **GPT-3.5**: A model included to examine performance with structured knowledge.\n- **Starling-7b**: Another model included to examine performance with unstructured knowledge.\n\n**Evaluation Metrics:**\n- **Hallucination detection accuracy**: This metric evaluates how well the models can identify hallucinations across different tasks.\n\n**Main Results:**\n1. **KnowHalu Performance**: KnowHalu consistently outperforms the baselines regarding hallucination detection across various tasks.\n2. **Benefit from Knowledge Forms**: Different models show varied improvements depending on the form of knowledge used:\n   - **GPT-3.5** benefits more from structured knowledge.\n   - **Starling-7b** is more effective with unstructured knowledge.\n3. **Prediction Aggregation**: Combining predictions from different knowledge forms enhances the overall detection accuracy."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To investigate the effect of separating the detection of non-fabrication hallucinations as an independent task and subsequently perform factual checking.",
            "experiment_process": "Prompts from HaluEval were used to cover both fabrication and non-fabrication hallucinations. The experiment tested two phases: (1) detecting non-fabrication hallucinations as an independent task, and (2) performing factual checking only if no non-fabrication hallucination is detected. Performance comparisons were made against a baseline that performs factual checking without prior non-fabrication detection. The True Positive Rate (TPR) and false positive cases were the evaluation metrics.",
            "result_discussion": "Incorporating non-fabrication checking consistently improved hallucination detection, enhancing TPR and reducing false positives. Even without non-fabrication checking, the standalone factual checking phase performed better than the baseline, indicating the effectiveness of the multi-query and reasoning process.",
            "ablation_id": "2404.02935v1.No1"
        },
        {
            "research_objective": "To evaluate the impact of different query formulations on the accuracy of hallucination detection.",
            "experiment_process": "Using the Starling-7B model for Wiki retrieval, three query formulations were tested: (1) specific queries only, (2) general queries only, and (3) combining Top-K results from both query types. Experiments assessed the performance in terms of accuracy of detecting hallucinated versus non-hallucinated cases.",
            "result_discussion": "Specific queries improved accuracy for non-hallucinated cases but reduced it for hallucinated ones due to polluted context, while general queries had the opposite effect. Combining both query types improved overall detection accuracy and reduced the abstention rate, indicating that a hybrid approach leads to more accurate and relevant knowledge retrieval.",
            "ablation_id": "2404.02935v1.No2"
        },
        {
            "research_objective": "To explore how the number of retrieved Wiki passages impacts hallucination detection performance.",
            "experiment_process": "Using a combination of specific and general queries for knowledge retrieval, the experiment varied the number of Wiki passages retrieved and assessed the effect on detection accuracy and abstain rate. Results were shown in Table 5.",
            "result_discussion": "Increasing the number of retrieved passages improved detection accuracy and reduced the abstain rate, with performance converging when the number of passages was greater than a certain threshold. Additional knowledge provided only marginal improvements, demonstrating the efficiency of KnowHalu.",
            "ablation_id": "2404.02935v1.No3"
        },
        {
            "research_objective": "To mitigate hallucinations using a confidence-based aggregation mechanism with various forms of knowledge.",
            "experiment_process": "A confidence-based aggregation method was used, where judgments with low confidence from one knowledge form could be overridden by higher confidence judgments from another knowledge form. Thresholds were selected using quantiles of confidence distributions from a validation set for each knowledge type. The experiment aimed to identify optimal thresholds that yield the highest average accuracy.",
            "result_discussion": "The approach of prioritizing higher confidence judgments significantly reduced hallucinations. The optimal thresholds resulted in improved average accuracy across tasks, validating the efficacy of the confidence-based aggregation mechanism.",
            "ablation_id": "2404.02935v1.No4"
        }
    ]
}