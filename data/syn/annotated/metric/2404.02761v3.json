{
    "title": "AQuA - Combining Experts\u2019 and Non-Experts\u2019 Views To Assess Deliberation Quality in Online Discussions Using LLMs",
    "abstract": "Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices. We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not been seen during training. The analysis of experts\u2019 vs. non-experts\u2019 annotations confirms theoretical findings in the social science literature.  \n\nKeywords:\u2009deliberative quality, adapter models, quality score",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "In the evolving landscape of democratic discourse, the concept of deliberation stands as a cornerstone, embodying the exchange of ideas, critical discussion, and consensus-building among citizens. Central to the efficacy of these deliberations is their quality, a multifaceted construct traditionally gauged by dimensions such as rationality, civility, reciprocity, and constructiveness. More recent research has explored various indicators of deliberative quality in online discussions. However, most of these approaches require manual annotation of discussion data from trained coders and serve to analyze the discussion in retrospect.\n\nAs the digital age drives an increasing volume of public conversations onto online platforms, the demand to assess their quality through the previously mentioned dimensions in an automated, scalable manner is growing.\n\nPrevious efforts have demonstrated the potential of using natural language processing (NLP) and machine learning algorithms to automatically identify features of deliberation such as argumentative structure, emotional tone, and engagement patterns. The interest in automating such assessments, with projects like the one implemented in the examination of argument and deliberative quality with adapter models, is growing.\n\nMotivated by this research, this study introduces AQuA, an index to measure the deliberative quality of individual comments in online discussions with a single score. While there is an ongoing debate on the usefulness of aggregating multiple indices of deliberation, we argue that for some tasks a single value, composed of several theoretically based criteria, is favorable. Our approach combines predictions on various dimensions of deliberation with insights gained from both expert and non-expert evaluations, resulting in a single deliberative quality score. We make use of data that has been annotated from both trained experts and crowd annotators, representing the non-experts\u2019 view.\n\nWe train 20 adapter models on aspects of deliberation to form the basis for a single deliberation score. To combine the automated predictions in a meaningful way, we define a single normalized score, creating an interpretable and explainable measure for deliberative quality. Finally, we show in experiments that our score can automatically assess the deliberative quality of discussion comments.\n\nOur method consists of two components: (1) the utilization of adapters trained on discrete facets of deliberation, and (2) the integration of annotations from experts and non-experts to establish a normalized score for deliberative quality. In developing this index, we extensively test and evaluate its effectiveness across diverse datasets, demonstrating its utility in real-world applications. By doing so, we aim to contribute to the burgeoning field of computational social science, offering scholars, policymakers, and practitioners a tool to monitor and analyze public dialogues. Our trained adapter weights and the code for calculating AQuA scores are available under https://github.com/mabehrendt/AQuA."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "Before explaining our approach in detail, we give an overview on the previous work to quantify aspects of deliberation in online discussions and the adapter approach to efficiently train language models for downstream tasks."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Deliberative Quality Indices",
            "text": "Various attempts have been made in the literature to conceptualize deliberation aspects to assess the quality of discourse. Here, we provide a summary of key indicators and metrics proposed in this domain.\n\nThe Deliberative Quality Index (DQI), introduced by Steenbergen et al. (2003 ###reference_b39###) and further refined by B\u00e4chtiger et al. (2022 ###reference_b7###), is a prominent and frequently applied metric for evaluating deliberative quality. The DQI comprises five dimensions: equality of participation, level of justification, content of justification, respect, and constructive politics. These dimensions are assessed for each contribution and averaged for a single speaker.\n\nScudder ###reference_b37###\u2019s (2022 ###reference_b37###) Listening Quality Index (LQI) emphasizes deliberative listening as a crucial factor in communication quality, organizing elements of existing measures into a hierarchical scale. This scale progresses from minimal listening to a stage where the speaker feels acknowledged, emphasizing the sequential fulfillment of criteria. The LQI differentiates between speakers and listeners, considering not just the contributions to the dialogue but also the participants\u2019 behavior and their feeling of being heard.\n\nThe Deliberative Reason Index (DRI) by Niemeyer et al. (2024 ###reference_b31###) seeks to capture deliberative quality at the group reasoning level rather than evaluating individual contributions. This approach, akin to the LQI, employs surveys conducted before and after discussions to gauge participants\u2019 views and preferences on debated topics, calculating agreement scores that are then aggregated to a group score.\n\nAlthough referred to as indices, the discussed methodologies do not necessarily provide a single index. They often yield multiple metrics rather than a singular measure, demanding a comprehensive evaluation to determine the overall quality of contributions or debates. Friess et al. (2021 ###reference_b19###) suggest aggregating the presence of deliberative qualities \u2014 rationality, respect, reciprocity, and civility \u2014 and computing their average to establish a quality ratio, treating each criterion with equal importance. We argue, however, that certain aspects may be more important than others to estimate the deliberative quality of a contribution (Chen, 2017 ###reference_b9###).\n\nWhile the indices presented are valuable for in-depth political debate analysis, their application requires extensive effort from trained coders for annotation and reliability assessments. To streamline the analysis of the deliberative quality of online discussions, several automation proposals have emerged. For instance, Wyss et al. (2015 ###reference_b43###) employ cognitive complexity to analyze Swiss parliamentary debates, using indicators derived from the Linguistic Inquiry and Word Count (LIWC) dictionary (Tausczik and Pennebaker, 2010 ###reference_b40###). Gold et al. (2015 ###reference_b22###) automate the measurement and annotation of features like participation and justification, subsequently employing a visual analytics system for data representation. Fournier-Tombs and Di Marzo Serugendo (2020 ###reference_b17###) introduced DelibAnalysis, a framework for predicting the DQI of online discussion contributions through machine learning, while Shin and Rask (2021 ###reference_b38###) proposed leveraging network and time-series analyses to assess deliberation criteria automatically.\n\nOur proposed method seeks to bridge the gap between NLP techniques and the theoretical aspects of deliberative quality assessment. We introduce the AQuA score to (i) combine the theoretical underpinnings of deliberation with the comment quality in online debates as perceived by non-experts, and thereby (ii) offering a tool to quantify deliberation aspects through advanced deep learning methods."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Adapters",
            "text": "Adapters, as introduced by Rebuffi et al. (2017), are an efficient approach to customize pre-trained language models like RoBERTa (Liu et al., 2019) for specific tasks. This method involves the integration of additional bottleneck layers into the model for each distinct task, which adds new weights while leaving the original pre-trained weights unaltered.\n\nThe concept of adapter layers was first applied to NLP by Houlsby et al. (2019), who adapted the Transformer architecture (Vaswani et al., 2017) to include these layers. The design of the adapter involves compressing the input\u2019s dimensionality to a significantly smaller size, applying a non-linear function, and incorporating a skip-connection to circumvent the bottleneck, with task-specific layer normalization parameters also being adjustable.\n\nThe strategic insertion of adapter layers has been a focus of research, with Houlsby et al. (2019) positioning them subsequent to both the multi-head attention and feed-forward layers within the Transformer architecture. Pfeiffer et al. (2021) found in an extensive search on architectural parameters, that placing only one adapter after the feed-forward layer in the Transformer works best throughout all their experiments. We also apply this architecture for our models.\n\nThe introduction of AdapterHub by Pfeiffer et al. (2020) and the adapters library by Poth et al. (2023) further facilitated the sharing and reuse of pre-trained adapters within the community.\n\nSubsequent studies, such as those by Mendonca et al. (2022), explored the training of individual adapters for dialogue quality estimation, and the use of AdapterFusion (Pfeiffer et al., 2021) to merge features from different adapters. Falk and Lapesa (2023a) trained 20 adapters on features for argument and deliberative quality to examine their dependencies. In our work, we follow a similar path to train adapters to evaluate specific aspects of deliberative quality and subsequently combine them to create a single deliberative quality metric."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   AQuA: An Additive Score for Deliberative Quality",
            "text": "With AQuA we propose a metric for assessing the quality of individual comments in online discussions. Our approach combines predictions on various dimensions of deliberation with insights gained from both experts\u2019 and non-experts\u2019 evaluations, resulting in a single deliberative quality score. Our methodology consists of the utilization of adapters trained on discrete facets of deliberation. We harness annotations of the same data, once labeled by trained experts for a variety of deliberative qualities, such as the degree of justification, and once labeled by non-experts on their personal assessment of the deliberativeness of a comment. \n\nThe idea of our approach is to aggregate individual scores calculated by adapters in a meaningful way to obtain a single score for each comment, in which some aspects contribute more to the perceived deliberativeness than others. For this reason we call our approach AQuA, an \u201cAdditive deliberative Quality score with Adapters\u201d."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Datasets",
            "text": "Our analysis is based on three datasets:\n\nThe KODIE dataset, comprising 13,587 comments that were collected and annotated as part of a scientific study that explored the impact of news organizations\u2019 interactive moderation on the deliberative quality of users\u2019 political discussions (Heinbach et al., 2022). The comments were posted on the Facebook pages of four German national and regional news outlets with high outreach and diverse audiences. These news outlets delivered data that included all published and deleted/hidden posts and comments on their Facebook pages for a period of 12 weeks per news outlet.\n\nThe #meinfernsehen2021 (German for my television) dataset (Gerlach and Eilders, 2022) is the result of a large scale citizen participation on the future of public television in Germany. Overall, 1,714 comments from the participation process have been manually coded as part of a quantitative content analysis to examine the discussion quality.\n\nThe CrowdAnno project Wilms et al. (2023) collected a non-expert representation of deliberative quality via crowd annotations for a subset of, i.a., both the KODIE and #meinfernsehen datasets.\n\nThe annotations from two different perspectives are explained in the following."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1.   KODIE & #meinfernsehen - the Experts\u2019 View",
            "text": "The KODIE annotation framework (Heinbach et al., 2022), assigns 23 score-based deliberative and further labels on other aspects to each comment. \n\nThese annotations were conducted by trained coders with a scientific background, focusing on deliberative criteria such as fact claims, relevance to the discussion topic, and respectful engagement with other users. The deliberative criteria can each be assigned to one of the three main dimensions of deliberation: Rationality, measured by indicators such as reasoning, solution proposals, and provision of additional knowledge. Reciprocity, measured as mutual references between users within a discussion. Civility, measured as the presence of a respectful interaction with others and the absence of insults, pejorative speech, and other markers of disrespect. The following coding scheme was used: all categories were coded on a four-point scale from \u201cclearly not present\u201d to \u201cclearly present\u201d. \n\nWe selected 19 out of the 23 deliberative quality criteria to train adapters, since some annotated aspects, e.g., threat of violence were not found in the data. In addition to the deliberative quality criteria, we included storytelling, which is considered a type II deliberation criterion, according to B\u00e4chtiger et al. (2009), since the description of personal experience when suggesting a solution contributes to the perceived quality of a comment (Falk and Lapesa, 2023b). The 20 deliberative aspects that we use are listed in Table 1. After filtering out data points with missing annotations and coding errors, we were left with a total of 13,069 comments to train our adapter models."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2.   CrowdAnno - the Non-Experts\u2019 View",
            "text": "In the CrowdAnno project, Wilms et al. (2023) gathered data on non-experts\u2019 perception of uncivil, deliberative, and fact-claiming communication within German online comments through crowd annotation. The dataset includes 13,677 comments from different news media comment sections and online citizen participation projects, annotated by 681 crowdworkers. For AQuA, we used a subset of 1,742 comments that are identical to the KODIE and #meinfernsehen data. Crowd workers were tasked with evaluating whether a comment is perceived as enriching and value-adding to the discussion or not, i.e., marking if it contains enriching communication, which could serve as a proxy for deliberative quality. The final score is aggregated from evaluations by 9 different crowd annotators via majority vote. To minimize annotator bias, the crowd workers were sampled to reflect various sociodemographic and educational backgrounds. We will write for the binary deliberativeness label of the -th comment."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Training the Adapters",
            "text": "To automatically predict the various deliberation criteria, we use pre-trained language models, such as BERT (Devlin et al., 2019). We follow the adapter approach: adapters are extra weights that are plugged into pre-trained language models and then learned for a specific task. The adapted language model for the -th deliberation criterion is written as , where is some text input. Note that while learning these extra weights, we do not alter the pre-trained model weights.\n\nMore precisely, we used the adapter architecture proposed by Pfeiffer et al. (2021), which is shown in Figure 2. We trained 20 individual adapters to predict scores for individual indicators for deliberative quality in user comments for the KODIE dataset. For training we perform a 65% (train), 15% (val), 20% (test) split on our dataset, resulting in 8,495 training data points, 1,960 for validation and 2,614 for testing.\n\nEach of the 20 adapters for AQuA is trained with a multi-label classification objective, minimizing the cross entropy loss. We train each adapter for 10 epochs and save the model with the best macro F1 score."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Calculating the Weights",
            "text": "Assigning an importance to the individual quality dimensions for the overall quality measurement is not a simple task. Our intuition for weighting the deliberative criteria is to include the perception of people who potentially read and write these comments. For that reason, we linked the scientific theory of deliberation to the view of non-scientists by combining the datasets described in detail in Section 3.1. More precisely, we obtain the weight for each deliberative criterion by capturing the relationship between the scientific label for each of the aspects of deliberation and the perception of crowd workers on the comments deliberativeness for all comments."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "3.4.   Building the AQuA Score",
            "text": "We build an overall quality score for each comment as the weighted sum of the weights and the predicted score for each of the quality adapters. The highest and lowest possible scores depend on the number of criteria and on the range of the predictions. Since the labels from KODIE are from the set, the predictions are also from this set. The highest possible score can be reached by setting all positively weighted criteria to their maximum value (i.e., 3) and all negatively weighted criteria to their minimum value (i.e., 0), where if and zero otherwise. Similarly, the smallest possible score is achieved by doing the reverse. To get a more intuitive range of values, we scale to an interval between 0 and 5, which is the definition of our proposed AQuA score. Figure 1 graphically illustrates how the AQuA score is calculated for a given input comment."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "3.5.   Applying the Score to English Comments",
            "text": "To apply our method to English datasets, we used the wmt19-en-de-model (Ng et al., 2019), to automatically translate all comments in the examined dataset from English to German. Another alternative would be to train adapter models on English data. Since the KODIE dataset consists of German Facebook comments on political issues, discussing German politicians as well, we decided not to translate these comments to train adapter models, but to translate English comments and use the pre-trained German models for evaluation."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Analysis and Experiments",
            "text": "After defining the AQuA score in the previous sections, we briefly discuss the choice of our base model and then analyze the weights that we calculated for the individual adapter predictions. Finally, we conduct several experiments to show that our model can successfully predict deliberative quality in user comments.\n\nAn interesting observation is that the lowest ranked comments in the dataset are much shorter than the high ranked ones. To study whether comment length alone is the most important factor that causes our model to predict a large score, we take a closer look at the distribution of scores depending on the length of the comment. Figure 3  ###reference_### displays the AQuA score (y-axis) in comparison to the comment length (x-axis, word count). While it is true that short comments get the lowest scores, which is probably due to the fact that they do not have much content, the visual analysis reveals also that medium length comments get the highest scores. This rules out that comment length is the most relevant factor for our score."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Choice of the Base Model",
            "text": "The predictions of each of the 20 trained adapters are a significant part that affects the composition of AQuA. The adapter weights can be trained with different base architectures. To determine which base model performs best, we examine the performance of different models, namely German BERT Base cased (Chan et al., 2020) and multilingual BERT (Devlin et al., 2019) in the cased and uncased variants, on the KODIE test split. The training procedure is the same as described in Section 3.2. The results are shown in Table 2. As the datasets are highly imbalanced, and some deliberative qualities do not occur often in the training data, we report the weighted averaged F1 score, i.e., a global weighted average F1 score for each class.\n\nThe trained adapter weights with the multilingual BERT model as base model outperform the German BERT model on 15 out of the 20 tasks. In direct comparison, the cased variant of Multilingual BERT performs slightly better than the uncased one. Based on these results we take the multilingual BERT Base cased model as our base model for calculating the AQuA score."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Insights from the Correlations",
            "text": "In the following, we discuss whether findings from previous deliberative research are consistent with our results. For an overview of the data distribution, Table 3 lists the absolute frequencies of each label for each deliberative quality criteria in the subset of the KODIE and #meinfernsehen datasets that have been annotated using the CrowdAnno framework. Note that these are not the frequencies in the dataset used for training the adapters. However, the small subset reflects the class imbalance that is present in the data, indicating that some categories such as vulgar language, insults, and even storytelling do not occur often.\n\nIt is striking that nearly all indicators for rationality are considered strongly positively by non-experts when evaluating deliberative quality of comments. Using well-reasoned arguments that are relevant to the topic has been found to be an important aspect in distinguishing between comments of high and low deliberative quality (Diakopoulos, 2015; Kolhatkar et al., 2020). Unfounded expressions of opinion, on the other hand, are perceived as non-constructive, i.e., negative, in user comments. Our results support that finding, as opinion is highly regarded negatively with perceived deliberative quality.\n\nOf all the indicators of reciprocity, referring to personal characteristics of others has the greatest positive impact on the overall score. This is surprising as deliberative literature primarily highlights engaging with others\u2019 positions, not their personal traits, as a quality indicator (e.g., Ziegele et al., 2020).\n\nWithin the civility criteria, sarcasm stands out negatively. Sarcasm, as well as doubting, criticism, and insults have been identified as one form of expressing disrespect towards other participants (Bender et al., 2011).\n\nStorytelling in the form of personal anecdotes can foster empathy and mutual understanding between participants and resolve differences (Black, 2008). Thus, it plays an important role in the weighting of AQuA."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Evaluating the Score",
            "text": "Having trained the AQuA score using the KODIE, #meinfernsehen and CrowdAnno datasets, we next show that the learned adapter weights transfer to other datasets as well and give scores that are qualitatively and also quantitatively convincing.\n\nAn interesting observation is that the lowest ranked comments in the dataset are much shorter than the high ranked ones. To study whether comment length alone is the most important factor that causes our model to predict a large score, we take a closer look at the distribution of scores depending on the length of the comment. Figure 3 displays the AQuA score (y-axis) in comparison to the comment length (x-axis, word count). While it is true that short comments get the lowest scores, which is probably due to the fact that they do not have much content, the visual analysis reveals also that medium length comments get the highest scores. This rules out that comment length is the most relevant factor for our score."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1.   SFU Opinion and Comments Corpus",
            "text": "We predict AQuA scores on comments of the SFU opinion and comment corpus (SOCC) (Kolhatkar et al., 2020). The dataset includes 1,121 comments on news articles that have been annotated for constructiveness (binary annotations) and toxicity (four-point scale from not toxic to very toxic). According to Kolhatkar et al. (2020), constructive comments are required \u201cto create a civil dialogue through remarks that are relevant to the article and not intended to merely provoke an emotional response.\u201d\n\nWe calculate AQuA scores and use them to predict the binary constructive label for each comment in the SOCC. Choosing a threshold of 2.3, i.e., inferring , if 2.3, we get an F1 score of 81.73. Note that the threshold is a hyperparameter and a value of 2.3 was chosen because it performed best on the data. As the dataset also comprises labels for toxic comments, we use the individual adapter predictions for screaming, vulgar, insults, sarcasm, and discrimination to predict the level of toxicity for each comment. Both the SOCC labels as well as our predictions are numbers from 0 to 3; therefore, we simply use the individual predictions of each adapter as an indicator for the toxicity level and calculate the weighted average F1 score.\n\nWith 829 comments labeled as not toxic at all (label 0), 172 with label 1, 35 with label 2, and only 7 comments that are marked as clearly toxic (label 3), the distribution is very similar to the one we see in the datasets we used for AQuA. Table 4 shows that we reach good F1 scores for adapters that align with toxicity."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2.   Europolis",
            "text": "For a qualitative analysis of the AQuA score, we apply it to the Europolis dataset (Gerber et al., 2018). Europolis includes transcribed speech contributions of a deliberative poll on migration and climate change, annotated for interactivity, respect, storytelling, justification, and common good. We calculate AQuA scores for each contribution in the dataset and report the top 3 highest and lowest ranked comments. For interpretability, we list both the predicted labels of the individual adapters and the original Europolis labels (in both cases only for values greater than 0). While both differ, the AQuA labels approximately match the original Europolis labels. The top 3 comments are all rated highly with positive deliberative aspects such as storytelling, justification, and additional knowledge, while the lowest comments exhibit negative deliberative aspects such as sarcasm and references to other participants. Overall, all of the lowest scored comments are questions to clarify certain aspects in the discussion, whereas the higher scored comments consist of sophisticated opinions.\n\nWhen comparing the AQuA predictions to the original Europolis labels, we find that the AQuA score seems consistent with the original labels, while enhancing the prediction since the AQuA score consists of 20 deliberative aspects instead of the 5. This demonstrates the value of AQuA as a unified score that can be applied to any dataset based on the chosen deliberative aspects. An interesting observation is that the lowest ranked comments in the dataset are much shorter than the high ranked ones. To study whether comment length alone is the most important factor that causes our model to predict a large score, we take a closer look at the distribution of scores depending on the length of the comment. While it is true that short comments get the lowest scores, which is probably due to the fact that they do not have much content, the visual analysis reveals also that medium length comments get the highest scores. This rules out that comment length is the most relevant factor for our score."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusion",
            "text": "In this work we introduce AQuA, an approach for an automated deliberative quality score based on large language models and adapters. The score combines annotations of experts and the view of non-experts on real online discussion comments. We show that the trained adapters are capable of predicting individual scores for different aspects of deliberative quality and that the overall score aggregates these predictions in a meaningful way.\n\nFurthermore, we evaluate our score (trained on KODIE and CrowdAnno) on two further datasets (SOCC and Europolis) to show that the predictions of the learned adapters transfer well to unseen datasets. First, we show that the adapter predictions that build the AQuA score are useful for classifying constructive and toxic comments on the SOCC dataset. Then we perform a qualitative analysis of the AQuA score by manually assessing the top 3 and bottom 3 scored comments in the Europolis dataset and show that comments with well-formed opinions receive large scores, while comments providing little value to the discussion receive lower scores.\n\nOverall, we show that AQuA can be used successfully to automatically assess deliberative quality while aligning with theoretical and empirical background in deliberation literature."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.02761v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.1",
            "4.3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.02761v3",
        "paper_title": "AQuA - Combining Experts\u2019 and Non-Experts\u2019 Views To Assess Deliberation Quality in Online Discussions Using LLMs",
        "research_background": "**Motivation:**\nThis paper is driven by the increasing need to assess the quality of public discussions taking place on online platforms in an automated and scalable manner. Traditional methods of evaluating deliberative quality, which include factors like rationality and civility, rely on manual annotations and retrospective analyses. With the rise of digital communications, there's an urgent need for more efficient methods that leverage natural language processing (NLP) and machine learning to automate this process.\n\n**Research Problem:**\nThe primary research problem addressed by this study is to develop an index that can automatically measure the deliberative quality of individual comments in online discussions through a single score. Existing approaches require manual annotations and retrospective analysis, making them less scalable for the large volumes of online discussions occurring today. Furthermore, there is an ongoing debate about the usefulness of aggregating multiple indices of deliberation into a single score, which this study aims to address by proposing a balanced and theoretically-grounded approach.\n\n**Relevant Prior Work:**\n1. **Core Concepts of Deliberation:**\n   - Deliberation encompasses the exchange of ideas, critical discussion, and consensus-building among citizens (Dryzek, 2002).\n   - Quality dimensions like rationality, civility, reciprocity, and constructiveness are crucial for evaluating deliberative discussions (Friess and Eilders, 2015).\n\n2. **Traditional and Manual Approaches:**\n   - Prior studies have largely relied on manual annotation to gauge deliberation quality (Steenbergen et al., 2003; Friess and Eilders, 2015; Scudder, 2022).\n\n3. **Automated Methods:**\n   - Initial efforts to automate the assessment of deliberative quality have used NLP and machine learning to identify features such as argumentative structure, emotional tone, and engagement patterns (Lawrence and Reed, 2020; Acheampong et al., 2020; Shin and Rask, 2021).\n   - Projects by Falk and Lapesa (2023a) in examining argument and deliberative quality with adapter models (Houlsby et al., 2019) have demonstrated significant interest and progress in this area.\n\n4. **Crowdsourced Annotations:**\n   - Previous research has explored combining expert and non-expert (crowd) annotations to create balanced and comprehensive evaluation metrics for deliberative quality.\n\nBy developing and testing the AQuA index, this study introduces a novel method for combining expert and non-expert evaluations to create an interpretable and explainable measure of deliberative quality. This research contributes to computational social science and provides a practical tool for scholars, policymakers, and practitioners for monitoring and analyzing public dialogues.",
        "methodology": "### AQuA - Combining Experts\u2019 and Non-Experts\u2019 Views To Assess Deliberation Quality in Online Discussions Using LLMs\n\n**Methodology:**\nIn AQuA, we propose a metric aimed at evaluating the quality of individual comments within online discussions. This metric synergizes predictive elements based on various deliberation dimensions with evaluations from both experts and non-experts, culminating in a unified deliberative quality score. The methodology is structured into two primary components:\n\n1. **Utilization of Adapters Trained on Deliberative Facets:**\n   - We employ specialized adapters trained to evaluate discrete facets of deliberation. These facets may include variables such as the degree of justification provided within the comment, among others.\n\n2. **Integration of Experts\u2019 and Non-Experts\u2019 Annotations:**\n   - Both experts and non-experts annotate the same data. Experts provide labels based on specific deliberative qualities, whereas non-experts rate the comments based on their personal perception of deliberativeness.\n   - We compute correlation coefficients between each deliberative criterion (provided by experts) and a binary indicator representing deliberativeness (provided by non-experts). These correlations help to normalize and combine the different criteria into a unified score.\n\n**Key Innovation:**\n- **Aggregative Scoring System:** The core innovation of AQuA lies in its method of aggregating the individual scores derived from the adapters. Instead of treating all deliberative facets equally, we assign varying weights to different aspects based on their contribution to the perceived deliberativeness. This weighted aggregation results in a single, additive deliberative quality score for each comment.\n\nBy integrating expert and non-expert insights and normalizing the deliberative criteria accordingly, our approach aims to provide a nuanced and comprehensive assessment of comment quality in online discussions. This makes AQuA a robust tool for evaluating deliberation, leveraging both specialized understanding and general perception.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\n\n1. **Model and Dataset**:\n    - The experiment centers around assessing deliberative quality in online discussions using a defined AQuA score.\n    - The base model used for predictions is briefly discussed, but specific details about the choice and architecture of the model are not explicitly provided.\n    - The dataset consists of user comments with varying lengths and presumably addresses the quality of deliberation within these comments.\n\n2. **Baselines**:\n    - The text no specific baselines are mentioned, focusing instead on the analysis of the model's predictions and characteristics of the dataset.\n\n3. **Evaluation Metrics**:\n    - The primary metric evaluated is the AQuA score itself, which aims to quantify the deliberative quality of user comments.\n\n#### Main Experimental Results\n\n- An interesting observation highlighted is the relationship between comment length and deliberative quality scores:\n    - **Short Comments**: These comments tend to receive the lowest AQuA scores, likely due to having less content that can be evaluated for deliberative quality.\n    - **Medium Length Comments**: Contrary to focusing on only short comments, the analysis reveals that medium-length comments receive the highest AQuA scores, suggesting a more nuanced relationship between comment length and quality.\n\nIn summary, the main experiment demonstrates that the AQuA model can effectively predict deliberation quality, with medium-length comments generally scoring higher than very short ones, indicating that deliberation quality involves more than just comment length."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the best base model for training adapter weights in predicting deliberative quality using the AQuA score.",
            "experiment_process": "The study evaluates different base models, including German BERT Base cased and multilingual BERT (cased and uncased variants), on the KODIE test split. The training procedure follows the same steps as described in Section 3.2. The performance is measured using the weighted averaged F1 score due to the highly imbalanced dataset.",
            "result_discussion": "The multilingual BERT model with cased variants outperforms the German BERT model in 15 out of the 20 tasks. As a result, the multilingual BERT Base cased model is chosen for calculating the AQuA score.",
            "ablation_id": "2404.02761v3.No1"
        },
        {
            "research_objective": "To verify the consistency and relevance of the calculated correlation coefficients used as weights in the AQuA score and compare them to findings in previous deliberative research.",
            "experiment_process": "The coefficients are calculated for each deliberative quality criterion using the subset of the KODIE and #meinfernsehen datasets annotated through the CrowdAnno framework. The study discusses the values and signs of the coefficients, listing absolute frequencies of each label to provide an overview of data distribution.",
            "result_discussion": "The results show that well-reasoned arguments and relevant content strongly positively correlate with higher deliberative quality, supporting earlier research. Conversely, unfounded opinions are negatively correlated. Surprising findings include that references to personal characteristics have a positive impact, while sarcasm has a high negative correlation. These insights help in understanding the weight given to various deliberative aspects in the AQuA score.",
            "ablation_id": "2404.02761v3.No2"
        },
        {
            "research_objective": "To evaluate the transferability and effectiveness of the AQuA score on different datasets, ensuring its qualitative and quantitative reliability.",
            "experiment_process": "The AQuA score is applied to the Europolis dataset, involving contributions annotated for interactivity, respect, storytelling, justification, and common good. The study calculates the AQuA scores and ranks contributions, comparing predicted labels from individual adapters and original Europolis labels.",
            "result_discussion": "The AQuA labels align well with original Europolis labels. Top-ranked comments exhibit positive deliberative aspects like storytelling and justification, while the lowest-ranked contain sarcasm and clarification questions. The AQuA score, representing 20 deliberative aspects, enhances predictions compared to the original 5 aspects, demonstrating its robustness and applicability across datasets.",
            "ablation_id": "2404.02761v3.No3"
        }
    ]
}