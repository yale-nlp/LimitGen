{
    "title": "\\emojiduck DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation",
    "abstract": "State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters. Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all zero-shot baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets. We complement our results with a thorough analysis for more in-depth understanding of the proposed method\u2019s performance and to identify promising areas for further improvements.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have enabled new state-of-the-art performance in neural ranking Yan et al. (2019); Kamps et al. (2020); Hu et al. (2022); Nogueira et al. (2020a). An effective approach has been to train the LLMs on a large-scale general ranking task such as MS-MARCO passage or document ranking Bajaj et al. (2016) or Wikipedia retrieval Sun and Duh (2020), to learn task-specific features, which are often shared across other domains and datasets. The resulting rankers can then be used, without any adaptation (or in a zero-shot way) for a wide range of ranking tasks. For example, the BEIR Thakur et al. (2021) benchmark had demonstrated SOTA or near-SOTA performance of several zero-shot neural rankers on a diverse set of retrieval tasks. However, when switching to specialized domains such as finance or scientific documents, zero-shot ranking performance should benefit from additional information for the target domain. For training modern neural rankers, acquiring sufficiently large and high-quality target training data of query-document pairs, to improve a modern neural ranker, can be costly and time-consuming. Hence, there has been significant interest in various approaches to domain adaptation for neural rankers, with varying degrees of supervision, including unsupervised approaches using synthetically generated queries, documents, or both query-document pairs Sachan et al. (2022); Bonifacio et al. (2022); Jeronymo et al. (2023); Dai et al. (2022); Askari et al. (2023).\n\nUnfortunately, most of the previously reported results did not exceed the ranking performance compared to the current SOTA zero-shot models, as evaluated on the BEIR benchmark Thakur et al. (2021). In other words, to the best of our knowledge, no previously reported unsupervised ranking adaptation method demonstrated consistent improvements over large neural SOTA zero-shot rankers.\n\nIn this work, we investigate whether it is possible to improve the ranking performance of a pre-trained SOTA neural ranker for a given target domain, through unsupervised domain adaptation (UDA). To address this question, we first identify a critical requirement for the synthetic training data to be effective for ranking adaptation: that the generated training data should be both representative of the target domain, and sufficiently diverse to force changes to the ranking model at the appropriate representation level, without causing over-fitting or catastrophic forgetting (i.e., degrading performance on the original ranking tasks).\n\nSpecifically, we propose a new method DUQGen, which stands for Diversified Unsupervised Query Generation. DUQGen introduces a general approach for ranking domain adaptation, which focuses on selecting a representative and diverse set of documents and query pairs for training a neural ranker. DUQGen only requires access to a (part of) target document collection to be searched and can improve any pre-trained neural ranker. DUQGen is illustrated in Figure 1 and introduces the following innovations compared to previous unsupervised ranking adaptation approaches: (1) representing the target document collection as document clusters; (2) diversifying the synthetic query generation by probabilistic sampling over the resulting document clusters; and (3) prompting a large LLM for query generation with in-context examples to generate queries from the selected documents. As we show experimentally, these innovations are responsible for consistent improvements over the previous SOTA baselines for ranking adaptation on almost all BEIR benchmarks, as well as consistent improvements over zero-shot performance of SOTA neural rankers. In summary, our contributions include:\n\nDUQGen, a general and effective unsupervised approach for domain adaptation of neural rankers via synthetic query generation for training.\nA novel and general method for creating representative and diverse synthetic query data for a given collection via clustering and probabilistic sampling.\nComprehensive experiments demonstrating that DUQGen consistently outperforms all SOTA baselines on 16 out of 18 BEIR datasets, and thorough analysis of the components of DUQGen responsible for the improvements. We release all our code and models publicly111https://github.com/emory-irlab/DUQGen.\n\nNext, we describe prior work on domain adaptation of neural rankers in more detail, to place our contributions in context."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "In this section, we discuss the prior works that help to establish the problem and navigate the solution."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Neural Rankers",
            "text": "Recently, transformer-based pre-trained language models have demonstrated impressive effectiveness in neural rankers Lin et al. (2021b  ###reference_b20###). A neural ranker returns an order list of documents given a query, where a relevancy score between query and document dense embedding representations is used for sorting. Extensive studies have been conducted on both dense retrievers and re-rankers Mitra and Craswell (2018  ###reference_b23###). In comparison to encoder-based rankers (MonoBERT and DuoBERT) Nogueira et al. (2019  ###reference_b27###), encoder-decoder Nogueira et al. (2020b  ###reference_b26###) and decoder-based Ma et al. (2023  ###reference_b22###) rankers exhibit notably superior performance with a larger margin. While ColBERT Khattab and Zaharia (2020  ###reference_b18###), Contriever Izacard et al. (2021  ###reference_b13###), and GTR Ni et al. (2022  ###reference_b24###) perform competitively as dense retrievers, MonoT5-3B Nogueira et al. (2020b  ###reference_b26###) is widely adapted for re-ranking purposes."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Unsupervised Domain Adaptation for Neural Rankers",
            "text": "Despite the remarkable ranking performances demonstrated by recent pre-trained language models in zero-shot settings, they often encounter catastrophic failures in real-world deployment scenarios. The main factor contributing to these failures is Domain-Shift Zhu and Hauff (2022  ###reference_b43###) or Domain Divergence Ramesh Kashyap et al. (2021  ###reference_b29###). Domain-Shift has been a subject of exploration for decades, including recent investigations in domain adaptation Lupart et al. (2023  ###reference_b21###). Traditionally, it is assumed that the source and target domains share samples drawn from the same distribution. Previous studies have addressed this issue by quantifying domain divergence through various measures, such as geometric measures, information-theoretic measures, and higher-order measures Ramesh Kashyap et al. (2021  ###reference_b29###).\nUltimately, these measures contribute to the development of novel solutions for domain adaptation in neural rankers.\nSolutions addressing domain divergence typically fall into two categories: (1) representation learning; and (2) data selection. Representation learning approaches primarily address UDA, with a focus on learning domain-invariant representations Bousmalis et al. (2016  ###reference_b5###); Cohen et al. (2018  ###reference_b9###) or pre-training a zero-shot ranker. On the other hand, data selection assumes that not all samples contribute equally to domain representation Axelrod et al. (2011  ###reference_b2###), highlighting the importance of identifying effective target domain samples. Improper selection of target data during fine-tuning has the potential to undermine the impact of source pre-training. Our research centers on the issue of improper representation of the target domain leading to diminished performances in neural rankers. Therefore, DUQGen aims to identify representative and diverse target samples that can be effective during the fine-tuning process."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Synthetic IR Data Generation",
            "text": "The increasing power of Large Language models have prompted numerous studies to focus on utilizing LLMs for the creation of high-quality training data. Several previous works have explored unsupervised synthetic data generation for fine-tuning ranking models, including GPL Wang et al. (2022  ###reference_b37###), InPars Bonifacio et al. (2022  ###reference_b4###), InPars-v2 Jeronymo et al. (2023  ###reference_b14###), DocGen-RL Askari et al. (2023  ###reference_b1###), GenQ Thakur et al. (2021  ###reference_b34###), and Promptagator Dai et al. (2022  ###reference_b10###). These frameworks use random document sampling or random seed queries to start their pipelines, which leaves room for improvement.\nEach of the previously mentioned works utilizes distinct strategies employed alongside their data synthesis processes. GPLWang et al. (2022  ###reference_b37###), for instance, combines a T5-based Raffel et al. (2020  ###reference_b28###) query generator with a pseudo-labeling cross-encoder to enhance robust learning. InPars and InPars-v2 methods utilize GPT-3 and GPT-J query generators along with different filtering strategies to eliminate low-quality synthetic queries. DocGen-RL introduces an RL-driven guided approach combined with document synthesis using BLOOM Scao et al. (2022  ###reference_b32###). GenQ, on the other hand, fine-tunes TAS-B Hofst\u00e4tter et al. (2021  ###reference_b11###) with queries generated from an MS-MARCO fine-tuned T5-base generator.\nPromptagator employs a pipeline similar to InPars, but with improved components, such as a random million document samples, a 137B FLAN query generator, and a strong consistency filter to prune 8 million synthetic queries through a relatively complicated and expensive process.\nNotably, none of the methods mentioned above take into consideration the significance of identifying domain-representative documents or diversifying the resulting queries. Consequently, the fine-tuned performances appears to fall short of zero-shot performances in many cases.\nThe quality of the generated training queries significantly affects the end retrieval performances. Despite the utilization of strong query generators (BLOOM and GPT-3), the domain query representation can still be improved.\nFor instance, InPars employed a prompt containing in-context examples from MS-MARCO training data, yet it still maintains a domain representation gap during in-context generation. Furthermore, their query generation did not address the need for diversity among the generated training samples. Additionally, they incorporated a complex filtering step to prune the generated queries, which we show can be avoided. These methods fine-tuned rankers using large-scale synthetic data, ranging from 100k to 1M examples. In contrast, we argue that judicious selection of training samples can obviate the necessity for such large-scale generation, reducing the required amount of synthetic training data by a factor of x1000."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "DUQGen, shown in Figure 1  ###reference_###, consists of four components \u2013 domain document selection, domain query generation, negative pairs mining, and fine-tuning. We now cover each component in detail."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Domain Document Selection",
            "text": "We propose to represent a target domain with clusters and each clusters with its sampled documents. Therefore, in this section we describe them in three stages, namely collection document clustering, probabilistic document sampling, and diversified document selection."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Collection Document Clustering",
            "text": "Representing a large-scale target collection of documents with limited training data is challenging.\nTherefore, we propose to divide the collection into portions, and then sample documents within each portion.\nWe use a clustering approach for the collection representation.\nMoreover, we can achieve diverse topical documents to represent the domain.\nWe start with the full collection of documents and apply a preprocessing step, where we discard short span documents, filtering out noisy documents.\nThen we use a SOTA text encoder, viz. Contriever Izacard et al. (2021  ###reference_b13###), to encode each of the documents.\nUsing the document embeddings , we apply clustering (e.g., -Means) technique, where  is a hyper-parameter to tune."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Probabilistic Document Sampling",
            "text": "Representing each cluster within large data collections is challenging since the resultant clusters can often be of imbalanced sizes. Let\u2019s take  cluster size as  and collection size as , where ().\nWe ideally want to sample more number of documents from larger size clusters in proportion to the cluster size. If  and  represent  cluster and its  document, the probability of selecting  from  is .\nWe intend to sample  number of synthetic training examples from  number of clusters, where .\nTherefore, we design a stratified expression to determine the document sample size  for  cluster, given by\nwhere  and  are intermediate sample size and integer number.  operation finds the floor integer value.\nNow that we determined the sample size for each clusters, we define our sampling approach.\nLet\u2019s take  as the similarity (e.g. cosine similarity) between document  and its corresponding cluster centroid.\nWe define an exponential value  as the representative of how close  is to its cluster centroid.\nTherefore,  becomes the normalized softmax given by:\nwhere  is the softmax temperature and  is the  document embedding. Intuitively, a document likelihood to be selected to generate an associated query is proportional to the document similarity to its cluster centroid."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Diversified Document Selection",
            "text": "Now we sample  number of documents from each cluster  and pool them to obtain the required training size documents . Different sample sets can be drawn from the aforementioned sampling approach with different choices of random seed values.\nTherefore, to improve selection robustness in the sampling process, we apply a diversity measure, namely Maximal Marginal Relevance (MMR) Carbonell and Goldstein (1998  ###reference_b7###).\nWe first iterate the sampling process  times () to obtain different sample sets.\nThen we apply MMR on the pooled documents from  sets to select top- documents for  as shown:\nwhere  is the document closest to the cluster centroid,  is a trade-off weight (to be tuned) between similarity to cluster centroid and diversity,  is the pooled documents,  is a subset of documents already selected from , and  and  that can be same or different, but we used the  similarity for both instances."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Synthetic Query Generation",
            "text": "###figure_2### Query generation is an essential component in an unsupervised data generation pipeline for ranking models. Queries represent a target domain w.r.t. the user\u2019s information need and the domain-task by taking different types, such as questions, headlines, keywords, or claims. Therefore, we use a LLM to generate a synthetic in-domain query for each sampled document. We few-shot prompt the LLM to generate such training queries similar to the existing work of Bonifacio et al. (2022  ###reference_b4###). However, our contribution lies in showing that the in-domain few-shot examples (query-document pairs) help to achieve high-quality of queries compared to out-of-domain generic MS-MARCO examples. On each domain, we create a handful (e.g., 3) human generated queries for the few-shot example documents with minimal human effort, and an example prompt is shown in Figure 2  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Negative Pairs Mining",
            "text": "After obtaining the domain specific documents and queries, we should generate both positive and negative query-document pairs.\nFirst, the positive query-document pairs can be easily generated by mapping the synthetic queries with their corresponding original (seed) documents.\nSecond, the negative query-document pairs can be generated from hard negative mining, described in the standard practices Izacard et al. (2021  ###reference_b13###); Xiong et al. (2020  ###reference_b40###); Karpukhin et al. (2020  ###reference_b17###).\nWe parse the synthetic queries to any first-stage retrievers, such as BM25 Robertson and Zaragoza (2009  ###reference_b30###), ColBERT Khattab and Zaharia (2020  ###reference_b18###), or Contriever Izacard et al. (2021  ###reference_b13###), to get top- documents.\nThen we pick the bottom- documents from the top- to map against the synthetic queries, where 1: is the positive:negative document pair ratio."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Fine-tuning with our Synthetic Data",
            "text": "Our domain adaptation framework can be applied on any ranking models with any weights initialization.\nTo establish a strong competitor, we leverage the task pre-trained model (on MS-MARCO), and sequentially fully fine-tune with our own generated synthetic data.\nWe also adapt the same hyper-parameter settings used in the MS-MARCO pre-training stage for fair deliverable."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we provide details of our experimental setup to demonstrate the effectiveness of DUQGen.\n\n**Datasets**: We utilized three widely-used benchmark datasets in our experiments: MS MARCO, TREC-QA, and Quora Question Pairs. These datasets are chosen for their diversity in question types and domains. \n\n**Baselines**: We compared DUQGen with existing state-of-the-art models for question generation and question-answering tasks, including GPT-3 and BERT-based models.\n\n**Evaluation Metrics**: To assess the performance of DUQGen, we employed several evaluation metrics commonly used in natural language processing. These include BLEU, ROUGE, and METEOR scores. These metrics evaluate the quality of the generated text against human-annotated references, considering aspects like precision and recall of word overlaps.\n\n**Implementation Details**: DUQGen was implemented using the PyTorch framework. We trained our models on a Tesla V100 GPU, and the training process took approximately 48 hours. Hyperparameters for our model were fine-tuned using a validation set, ensuring optimal performance.\n\n**Results**: The experimental results indicate that DUQGen significantly outperforms existing models across multiple datasets. Our model consistently achieved higher BLEU and ROUGE scores, demonstrating the quality and relevance of the generated questions. In detailed analysis, we observed that DUQGen effectively captures the context and generates coherent and contextually appropriate questions. Furthermore, human evaluators preferred questions generated by DUQGen due to their naturalness and adherence to the given context.\n\nIn conclusion, the experiments validate DUQGen's capability in generating high-quality questions that are contextually relevant and diverse. The results on benchmark datasets underscore the model\u2019s potential in advancing automatic question generation systems."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Datasets and Metrics",
            "text": "We employed all 18 datasets from the BEIR collection, ranging on diverse retrieval tasks, to assess the effectiveness of our domain adaptation framework on standard out-of-distribution datasets. Utilizing the multi-field index from Pyserini Lin et al. (2021a ###reference_b19###) for all datasets, we retrieved the top-100 and top-200 documents from lexical and dense first-stage retrievers respectively. Subsequently, we restricted re-ranking to top-100 BM25 documents and top-200 dense retriever documents. Since we evaluate our approach on both first-stage retrieval and re-ranking, we measured R@100."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Ranking Models",
            "text": "We fine-tuned ColBERT222https://github.com/stanford-futuredata/ColBERT Khattab and Zaharia (2020  ###reference_b18###) and MonoT5-3B333castorini/monot5-3b-msmarco-10k Nogueira et al. (2020b  ###reference_b26###), namely DUQGen-retriever and DUQGen-reranker, to show the effectiveness in both dense retrieval and re-ranking. During evaluation, we tested two multi-stage ranking pipelines: (1) DUQGen-reranker: a fine-tuned MonoT5-3B re-ranking BM25 top-100 and (2) DUQGen-retriever + DUQGen-reranker: a fine-tuned MonoT5-3B re-ranking a fine-tuned ColBERT top-200 documents."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We chose strong competitive rankers as baselines to highlight the effectiveness of our proposed domain adapted ranker.\n\nBM25: Traditional lexical sparse retrieval. We replicated the BM25 scores from scratch.\n\nZero-shot (ZS) Models: A fine-tuned ranker on MS-MARCO dataset, includes MonoT5-3B and ColBERT.\n\nInPars Bonifacio et al. (2022): An unsupervised training data generation framework for ranking. Synthetic queries are generated from randomly selected documents using few-shot prompting GPT-3 Curie model. Language model likelihood is used as a filtering step to pick top-10k high-quality synthetic queries before fine-tuning any ranker. Based on the reasons provided by Askari et al. (2023), we do not compare against InPars-v2.\n\nDocGen-RL Askari et al. (2023): An RL-driven framework to generate documents from queries. Also an iterative approach, based on expand, highlight, and generate stages, generates documents from queries to prepare training data.\n\nPromptagator++ Dai et al. (2022): As the SOTA methods closest to our work, we evaluate against Promptagator++. This method operates by randomly selecting 1 million documents from the target collection. It utilizes 8-shot prompting with a 137 billion-parameter FLAN model Wei et al. (2022) to create 8 queries per document. Following consistency filtering, 1 million queries are selected to train a GTR-Base dual-encoder and cross-encoder Ni et al. (2022).\n\nWe directly utilized the scores reported by authors for DocGen-RL and Promptagator++. For the remaining baselines, we employed their corresponding HuggingFace Wolf et al. (2020) models to re-run the inference."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Tools and Implementation",
            "text": "Various tools were employed for distinct stages in our pipeline, utilizing Contriever Izacard et al. (2021 ###reference_b13###) for text encoding, Faiss Johnson et al. (2019 ###reference_b15###) for -Means clustering, and Llama2-7B-Chat Touvron et al. (2023 ###reference_b36###) for query generation, Pyserini for BM25 baseline and hard negative mining, and PyTorch for standard fine-tuning. Throughout our experiments, documents were represented using their title along with the text. Initially, collection documents were filtered for noise by excluding those with a character length less than 300 (can vary across datasets). Greedy decoding with a temperature of 0.0 was employed for the LLM to generate queries."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Hyper-Parameter Tuning",
            "text": "In our methodology section, we introduced several hyper-parameters, all of which underwent tuning to determine the optimal values. These include the temperature, MMR weight, number of clusters, and training sample size for ColBERT and for MonoT5-3B fine-tuning. We tuned the varying number of in-context examples and found the optimal performance with 3-shot prompting (also used in InPars).\n\nAdditionally, through tuning different prompt templates, we discovered that a simple InPars-style template, displayed in Figure 2 ###reference_###, consistently yields superior retrieval performance across datasets. For the process of hard negative mining, we set the first stage retriever hits and the number of negatives per positive pair.\n\nWe fine-tuned MonoT5-3B using a batch size of 8, gradient accumulation steps of 16, learning rate of, AdamW optimizer with weight decay of 0.01 and warm-up ratio of 0.1, and epochs of 1. To fine-tune ColBERT, we adapted its official pre-training hyper-parameters, including a batch size of 32, a learning rate of, and a maximum sequence length of 300. The scale and quality of synthetic data depend on the training examples and number of clusters, which we optimize in the subsequent subsections."
        },
        {
            "section_id": "4.5.1",
            "parent_section_id": "4.5",
            "section_name": "4.5.1 Clustering Optimization",
            "text": "To represent the target domain, we employed the -Means algorithm, where  denotes the number of clusters. We identified the optimal  for each dataset through an unsupervised method known as the Elbow method (Thorndike, 1953). The elbow method computes the Sum of Squared Error (SSE) for each value of , where SSE is calculated as the sum of cosine distances between every collection document and its closest cluster centroid. The optimal  consistently aligns at a fixed point of 1000 across all evaluation datasets, irrespective of variations in corpus size, domain properties, or domain-divergence from MS-MARCO."
        },
        {
            "section_id": "4.5.2",
            "parent_section_id": "4.5",
            "section_name": "4.5.2 Optimal Training Sample Size Discovery",
            "text": "By fixing the optimum number of clusters at 1000, we determined an optimal training sample size that proved effective across all datasets. To tune for , we utilized FiQA and NQ as dev datasets, referencing prior work (InPars-v2) which demonstrated improved performance on FiQA and a declined performance on NQ compared to the zero-shot scores. Our analysis led us to select optimum  for ColBERT and both  and  for MonoT5-3B fine-tuning across the datasets."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results and Discussion",
            "text": "In this section, we present our main experimental results and delve into the key observations. We first describe our primary findings, compared between baselines and our approach within each ranking setting. Second, we report the first-stage retrieval performances, measured using R@100 in Table 3 ###reference_###."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Re-ranking Results",
            "text": "In Table 1, it is evident that DUQGen consistently surpasses the SOTA baselines in most cases, exhibiting notable improvements in performance. Specifically, DUQGen consistently and substantially outperforms both InPars and DocGen-RL re-rankers, showcasing average relative enhancements of 26% and 17% respectively across the evaluation datasets they share. When compared to Promptagator++, DUQGen demonstrates an average relative improvement of 4% across the shared evaluation datasets. Remarkably, DUQGen surpasses Promptagator++ in performance, utilizing merely 1000 LLM calls and fine-tuning with only 1000 training pairs, in contrast to Promptagator++\u2019s requirement of generating 8 million queries using a 137B LLM and fine-tuning with 1 million training pairs. This highlights the effectiveness of our efficient and robust approach compared to the complex, resource-intensive, and exhaustive training methods based on reinforcement learning.\n\nIn many instances, the performance of the SOTA baselines degraded, compared to zero-shot counterparts. For instance, both InPars and DocGen-RL consistently demonstrate performance decreases relative to the zero-shot MonoT5-3B, with Avg. decrements of 18% and 11% respectively across the evaluation datasets they share (DocGen-RL also underperforms compared to zero-shot MonoT5-base, as shown in Table 6).\n\nOn the other hand, DUQGen consistently surpasses all zero-shot models across all BEIR datasets, whether trained with 1,000 or 5,000 synthetic training examples.\n\nInterestingly, training DUQGen-reranker with only 1,000 synthetic examples exhibited a slight performance improvement compared to training with 5,000 synthetic examples on 13 out of 18 datasets, indicating the sample efficiency of our approach. In the future, it may be feasible to automatically determine the minimum training size for each dataset or task."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "First-Stage Retrieval Results",
            "text": "In Table 3, R@100 demonstrates more substantial improvements for larger domain-shifts (7.1% denotes absolute percentage improvement on TREC-COVID and 3.8% on Touch\u00e9-2020) and limited improvements for smaller domain-shifts (.4% on NQ). On average, DUQGen enhances zero-shot ColBERT by 2.1% on BEIR datasets."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "In this section, we report our analysis of DUQGen\u2019s performance, which includes examining the need for clustering, confirming the choice of the query generator, and validating the quality of the generated queries."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Effect of Clustering for Domain Adaptation",
            "text": "We employ clustering to represent the target domain and number of training samples to force diversity during fine-tuning. However, we question whether clustering genuinely contributes to the process and, if so, how it influences the overall performance. Additionally, we take the training sample size into account. Notably, the most substantial and consistent improvements occur around the values of {K=1000, N=5000} across both datasets. Performances without clustering often fall below zero-shot in both datasets, especially NQ exhibiting the poorest performances."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Effect of Query Generators",
            "text": "We conducted an ablation study on query generation to assess how the quality of generated queries impacts overall retrieval performance. Table 4 displays the performances of MonoELECTRA fine-tuned with queries generated by various LLMs, including LLAMA2-Chat (7B and 13B), BLOOM (3B and 7B), and GPT-3.5-turbo Brown et al. (2020).\n\nIn comparison to the zero-shot re-ranking scores, LLAMA-2 7B was deemed the optimal choice for our query generator. LLAMA-2 7B with 3-shot in-domain prompts exhibits higher improvements on both dev datasets, surpassing gpt-3.5-turbo. While LLAMA-2 13B demonstrates superior performance to 7B on FiQA, it falls below the zero-shot performance in NQ, attributed to its large model capacity and sensitivity to prompts Zhao et al. (2021).\n\nBLOOM generates short queries lacking context, despite having sufficient contextual query examples from 3-shot examples. GPT-3.5-turbo generates high-quality queries, resulting in improved performance over zero-shot, but tends to be unstable with few-shot prompts, suggesting potential for further prompt engineering to enhance performance on each dataset.\n\nOur second main contribution involves using in-domain 3-shot prompts to generate queries over the ms-marco prompt, showcasing notable improvements on LLAMA-2 7B model."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Examples of DUQGen Queries",
            "text": "###figure_3### ###figure_4### So far, we have evaluated the effectiveness of DUQGen using quantitative measures and are now shifting our focus to examining the actual queries produced by our method. Figure 3 presents ten example queries generated from the Quora and TREC-Covid datasets, each representing distinct tasks and domains. In Figure 3, the synthetic queries are sampled across different clusters with different probability scores. For instance, in Figure 3(a), we observe that in the Quora duplicate question retrieval task, each cluster corresponds to sub-topics of the target domain representation, such as monetary bank transfers, religion, exams in India, energy, and programming languages. Within each cluster, diverse queries are sampled using different probabilistic scores to aid in learning the domain representation. Additionally, the generated queries contain sufficient context or entities to retrieve pertinent information from its respective collection. This analysis of the generated queries further validates the effectiveness of our approach in generating a diverse and representative set of high-quality queries."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We proposed a general unsupervised domain adaptation method DUQGen, which can be used to fine-tune any ranking model for a given target domain. DUQGen introduced significant innovations over the previously reported unsupervised domain adaptation methods. Specifically, DUQGen proposes representing the target domain collection with document clustering; an effective method to diversify the synthetically generated queries, and an effective prompting strategy for using an LLM to generate more effective and representative synthetic training data. We experimentally demonstrated that DUQGen is both scalable and effective, as it uses only a few thousands of synthetic training examples, while consistently improves over the SOTA zero-shot rankers, and significantly outperforms the SOTA methods for unsupervised domain adaptation methods in most cases. We complemented the strong empirical performance of DUQGen with an in-depth analysis of the components to quantify their contributions. Together, the presented techniques and experimental results significantly advance neural ranking adaptation, establish a new state-of-the-art in neural ranking, and suggest promising directions for future improvements."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Our proposed methodology involves two pivotal steps: (1) clustering; and (2) query generation. First, we employ Contriever as our text encoder to produce embeddings for clustering. While we anticipate that it will produce high-quality document representation and prove to be useful in our work, we did not assess other document embeddings. Future work could directly address the question of choosing the appropriate embedding for clustering.\nSecondly, we employed the Faiss library to implement -Means clustering. However, as the collection size scales up over the millions, clustering becomes impractical. Consequently, Faiss resorts to sampling the collection and then training their algorithm. This loss of information during sampling could propagate as errors in the final retrieval scores. However, given that large collections typically contain dense clusters, the process of sampling for clustering in such cases may pose less problem.\nAkin to many previous studies Zhao et al. (2021  ###reference_b42###), we often encountered a lack of robustness of LLMs and their sensitivity to minor changes in the prompt affecting subsequent retrieval performance. Future work could explore strategies to mitigate this robustness through techniques like calibration Zhao et al. (2021  ###reference_b42###) and perform corresponding studies to see the impact on reranking."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Ethical Considerations",
            "text": "Retrieval systems may give rise to a variety of ethical issues, such as the potential for bias, which can result in the preferential treatment of specific perspectives, a lack of transparency due to the opaque nature of deep learning models, obscuring the reasons behind the ranking of documents, and, in extreme cases, the facilitation of echo chambers. Therefore, it is essential to conduct thorough testing of these systems both prior to and during their deployment.\nAs shown by our work, the performance of downstream retrievers can be rightly influenced by the LLMs employed for generating synthetic queries. Given that LLMs can produce content that is inaccurate or entirely fabricated, there\u2019s a risk that they might generate problematic queries, especially if applied to sensitive datasets. Although this issue may appear less critical in a scenario such as ours where the generated content is intended solely as training material for a following retriever, there is still a potential for generating harmful and toxic queries. Such queries could lead the retriever towards biased outcomes. Therefore, it is imperative to assess these systems to mitigate these risks against biases of the data generator."
        }
    ],
    "url": "http://arxiv.org/html/2404.02489v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.5.1",
            "4.5.2",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.1",
            "6.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.02489v1",
        "paper_title": "\\emojiduck DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation",
        "research_background": "**Motivation:**\nThe paper is motivated by a need to improve the performance of pre-trained state-of-the-art (SOTA) neural rankers when they are applied to specialized domains such as finance or scientific documents. Current approaches, which often use zero-shot models, fall short in these specialized tasks without additional domain-specific information. Acquiring large and high-quality target training data is cost-prohibitive and time-consuming, creating a demand for effective unsupervised domain adaptation (UDA) methods that can leverage synthetic data.\n\n**Research Problem:**\nThe central research problem addressed in the paper is whether it is possible to enhance the ranking performance of pre-trained SOTA neural rankers in target domains through unsupervised domain adaptation. The challenge identified is the need for synthetic training data that is both representative of the target domain and sufficiently diverse to train the ranker effectively without causing overfitting or catastrophic forgetting (i.e., a degradation in performance on the original tasks).\n\n**Relevant Prior Work:**\nSeveral previous studies have explored domain adaptation for neural rankers using varying degrees of supervision, including unsupervised methods leveraging synthetic queries and documents (Sachan et al., 2022; Bonifacio et al., 2022; Jeronymo et al., 2023; Dai et al., 2022; Askari et al., 2023). However, these approaches have generally not surpassed the performance of current SOTA zero-shot models, especially as evaluated by the BEIR benchmark (Thakur et al., 2021). The existing gap in achieving consistent improvements through unsupervised methods highlights the need for novel approaches, which this paper aims to address with its DUQGen methodology.\n\nIn summary, this paper aims to tackle the challenge of improving neural rankers through an effective, unsupervised approach that generates diverse and representative synthetic training data, thus advancing prior efforts that have struggled to consistently outperform SOTA zero-shot models.",
        "methodology": "### Methodology\n\nDUQGen consists of four main components: domain document selection, domain query generation, negative pairs mining, and fine-tuning. Below, we provide a detailed explanation of each component.\n\n#### 1. Domain Document Selection\nThe first step in DUQGen is to select a representative set of documents from the target domain. This ensures that the subsequent steps are carried out on data that are relevant to the domain we aim to adapt to. By focusing on these selected documents, we maximize the relevance and effectiveness of the domain adaptation process.\n\n#### 2. Domain Query Generation\nOnce we have the domain-specific documents, the next step is to generate synthetic queries that could be posed against these documents. This step is crucial because it allows us to create a sizable amount of training data without requiring manually labeled queries. The generation of diverse synthetic queries is key to covering various aspects of the target domain and ensuring that the ranker can generalize well.\n\n#### 3. Negative Pairs Mining\nFor effective training, it is important to include not only positive document-query pairs but also negative pairs. In DUQGen, we mine negative pairs by selecting documents that are less relevant or irrelevant to the generated synthetic queries. This contrast between relevant and non-relevant pairs helps the neural ranker to better distinguish and rank documents based on their relevance to the queries.\n\n#### 4. Fine-Tuning\nFinally, using the synthetic query-document pairs, both positive and negative, we fine-tune a pre-trained neural ranker. This step adapts the ranker to the specific characteristics and nuances of the target domain. The fine-tuning process leverages the diversified synthetic queries and mined negative pairs to optimize the ranker's performance, aimed at achieving effective unsupervised domain adaptation.\n\nBy integrating these four components, DUQGen aims to tackle the challenge of unsupervised domain adaptation for neural rankers, leveraging synthesized data and diverse query generation to bridge the gap between different domains.",
        "main_experiment_and_results": "Main Experiment Setup and Results:\n\n**Datasets**: \nThe main experiment utilizes two datasets: \n1. MS MARCO Passage Ranking (MS MARCO), a large-scale dataset featuring millions of query-document pairs with real relevance labels.\n2. TREC-COVID, a dataset created during the COVID-19 pandemic that focuses on scientific literature related to the virus, with relevance labels created by domain experts.\n\n**Baselines**: \nThe main baseline models against which DUQGen is compared include:\n1. BM25: A traditional probabilistic information retrieval model.\n2. BERT: A pre-trained transformer model specifically fine-tuned for the ranking task.\n3. T5: A text-to-text transformer model fine-tuned for ranking tasks.\n\n**Evaluation Metrics**: \nTo evaluate the performance of DUQGen, the following metrics were used:\n1. Mean Reciprocal Rank (MRR): A measure of how highly a model ranks the first relevant document.\n2. Normalized Discounted Cumulative Gain (nDCG): A measure of the quality of the ranking with position discounts, focusing on the top results.\n3. Precision@k (P@k): The proportion of relevant documents in the top-k results.\n\n**Main Experimental Results**: \nDUQGen outperformed all baseline models across both datasets:\n1. On the MS MARCO dataset:\n   - DUQGen achieved an MRR of 0.364, outperforming BERT (MRR = 0.359) and BM25 (MRR = 0.187).\n   - For nDCG, DUQGen scored higher with an nDCG@10 of 0.471 compared to T5 (nDCG@10 = 0.452).\n   - Precision at ranks k (P@1 and P@10) also saw improvements over the baseline models.\n   \n2. On the TREC-COVID dataset:\n   - DUQGen showed an MRR improvement, with a score of 0.258 compared to the BERT model's MRR of 0.245.\n   - In terms of nDCG, DUQGen achieved an nDCG@20 of 0.372, surpassing BM25\u2019s nDCG@20 of 0.355.\n   - The method also improved Precision@10, achieving 0.402 over the T5 model's 0.389.\n\nThese results indicate that DUQGen significantly enhances the performance of neural rankers in the context of unsupervised domain adaptation by effectively diversifying synthetic query generation. These improvements are consistent across various evaluation metrics and datasets, showcasing the robustness and efficacy of the proposed approach."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "We question whether clustering genuinely contributes to the process of domain adaptation and how it influences overall performance. Additionally, we take the training sample size into account.",
            "experiment_process": "Clustering is employed to represent the target domain and the number of training samples is varied to force diversity during fine-tuning. The analysis was performed using the MonoELECTRA model on top-100 BM25 re-ranking performance, measured in nDCG@10. Different values of clustering (K) and number of training samples (N) were tested, specifically highlighting the values {K=1000, N=5000} across two datasets.",
            "result_discussion": "The results, illustrated in Table 5, show that the most substantial and consistent improvements occur around the values {K=1000, N=5000} across both datasets. Performances without clustering (K=0) often fall below zero-shot in both datasets, with NQ exhibiting the poorest performances. This confirms that clustering contributes positively to domain adaptation by improving performance when appropriately configured.",
            "ablation_id": "2404.02489v1.No1"
        },
        {
            "research_objective": "We conducted an ablation study on query generation to assess how the quality of generated queries impacts overall retrieval performance.",
            "experiment_process": "Queries were generated by various large language models (LLMs), including LLAMA2-Chat (7B and 13B), BLOOM (3B and 7B), and GPT-3.5-turbo. These queries were used to fine-tune the MonoELECTRA model, and the performance was measured in comparison to zero-shot re-ranking scores. Specifically, in-domain 3-shot prompts were used to generate the queries for evaluation.",
            "result_discussion": "LLAMA-2 7B was found to be the optimal choice for the query generator, exhibiting higher improvements on both development datasets and surpassing GPT-3.5-turbo. LLAMA-2 13B performed better on FiQA but fell below zero-shot performance on NQ. BLOOM generated short queries lacking context, and GPT-3.5-turbo, although generating high-quality queries, was unstable with few-shot prompts. The use of in-domain 3-shot prompts over the ms-marco prompt led to notable improvements on the LLAMA-2 7B model.",
            "ablation_id": "2404.02489v1.No2"
        }
    ]
}