{
    "title": "Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models",
    "abstract": "Large language models (LLMs) often struggle with strict memory, latency, and power demands. To meet these demands, various forms of dynamic sparsity have been proposed that reduce compute on an input-by-input basis. These methods improve over static methods by exploiting the variance across individual inputs, which has steadily grown with the exponential increase in training data. Yet, the increasing depth within modern models, currently with hundreds of layers, has opened opportunities for dynamic layer sparsity, which skips the computation for entire layers.\n\nIn this work, we explore the practicality of layer sparsity and establish the relationship between model depth and layer sparsity. We propose Radial Networks, which perform token-level routing between layers guided by a trained router module. These networks can be used in a post-training distillation from sequential networks or trained from scratch to co-learn the router and layer weights. They enable scaling to larger model sizes by decoupling the number of layers from the dynamic depth of the network, and their design allows for layer reuse. By varying the compute token by token, they reduce the overall resources needed for generating entire sequences. Overall, this leads to larger capacity networks with significantly lower compute and serving costs.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language and vision models have recently achieved state-of-the-art performance across various tasks, yet due to their large computational requirements, they struggle with strict memory, latency, and power demands. As these transformers grow larger, they create opportunities for dynamic layer sparsity, which can skip individual layers on an input-by-input basis, as shown in Figure 2. \n\nThis type of sparsity was impractical at smaller scales and with previous neural architectures. At smaller scales, every layer contributes significantly to the computation for each input, and with previous architectures, e.g., convolutional neural networks (CNNs), models change their intermediate dimensions throughout their depth and skipping causes dimensional mismatches. \n\nThis work shows that the layer contributions vary among models and tasks, and often the earlier layers of the network contribute more than the later layers. This indicates that early-exit methods, which dynamically prune the later layers in the network, often focus on the wrong set of layers. This dynamic contribution can be exploited at the token-level if it can be predicted accurately and efficiently at runtime.\n\nThis work explores the opportunities for dynamic sparsity within the modern transformers by focusing on the OPT family of models for language and ViT models for vision. It profiles the residual blocks to quantify the importance of each intermediate layer to its output and then highlights trends across model size and block types. Then, it inserts oracles at every layer to calculate various accuracy proxies and simulate greedy decisions on which layers to dynamically skip per token."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Sparsity research with deep neural networks has a long history, and broadly can be categorized in terms of granularity, structure, and mode (static vs. dynamic). Figure 3 shows sparsity granularity, beginning with bits that construct parameter elements, elements that build blocks, and blocks that form layers. As the unit becomes larger, it becomes more difficult to arbitrarily prune without accuracy loss yet easier to accelerate with modern hardware. For instance, unstructured element sparsity in weights leads to high compression levels while maintaining model accuracy, yet it requires specializing sparse accelerators to translate compression into end-to-end speedup. In addition, the sparsity mode can either be static or dynamic. Static sparsity leads to more regular patterns that can be optimized by compilers and simpler architecture that do not need additional sparsity predictors, yet it must apply to all inputs together. In contrast, dynamic sparsity can take advantage of input-dependent characteristics to increase model accuracy at higher levels of compression. This work focuses on dynamic layer sparsity, which can take advantage of the recent explosion in model depth within language models."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dynamic Sparsity",
            "text": "Multiple prior works have proposed dynamic sparsity to accelerate DNNs across granularities. For example, Channel Gating introduced a method for dynamic channel sparsity that reduced the compute of CNN workloads without significant accuracy loss [5]. Precision Gating continued this line of research by applying dynamic sparsity at the bit level to reduce the required compute [10]. Later, DejaVu applied a similar approach within LLMs to induce dynamic sparsity on the channels within the FFN layer and across the heads of the attention layer [6]."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Early Exit",
            "text": "In addition to dynamic sparsity along the network width, multiple prior works have explored sparsity in the depth dimension. For instance, early-exit DNNs use dynamic sparsity along the depth dimension by allowing the computation to exit prematurely at fixed points within the network. This process must be trained end-to-end using a joint loss function that weights the contributions from each early-exit layer. However, this work shows that in many models, the earlier layers in the model often contribute more, and therefore early-exits are significantly more difficult to apply post-training."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Layer Sparsity",
            "text": "Transformer layers contain two residual blocks: attention (ATT) and feed-forward network (FFN). These blocks each contain the main residual branch, which comprises multiple individual layers, and the identity branch, which bypasses the residual branch and simply returns its input. They combine these branch outputs together, so that during training the main branch only has to learn the function residual. These blocks offer natural breakpoints within the model to profile and induce layer sparsity since they already provide skip-connections that have been trained along with the model.\n\nFigure 4 shows a lower-level view of these blocks within two transformer layers. It shows that the main branch and skip connections are combined at an addition node before they are passed to the next block. This structure enables easy profiling of the blocks by measuring the relative magnitudes into these additions. \n\nThis work focuses on the opportunities for dynamic layer sparsity and simulates layer skipping by allowing these oracles to have access to future information."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Profiling",
            "text": "The primary proxy used by these oracles is the analysis of the layer sparsity within OPT and ViT models with examples taken from WikiText-2 and COCO. The WikiText-2 examples are packed together to avoid the use of padding to simulate batch-size one inference. This batch-size one setting is very common in practice and avoids many complications with dynamic layer sparsity that arise when using batches of examples."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Residual Ratio",
            "text": "To profile these opportunities for dynamic sparsity, the analysis focuses on the contribution of the residual branch and explores efficient post-training proxies for expensive metrics, such as empirical layer sensitivities. By examining the average contribution of the main branch at the output, it becomes evident that skipping blocks with small contributions should have little overall effect on the network's output."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Model Size",
            "text": "###figure_5### ###figure_6### ###figure_7### Each data point represents a single token during the model generation phase. In addition, opportunities for layer sparsity seem to expand with model size. For instance, the median value for OPT-125M is only 20%, and it drops to 5.9% for OPT-66B. This distribution seems to track the number of model parameters, not just the number of layers. For example, OPT-2.7B and OPT-6.7B have the same number of layers, differing only in their hidden dimensions, yet the trend continues for OPT-6.7B. This likely continues for even larger models, making dynamic layer sparsity more practical within modern state-of-the-art models with greater than one trillion parameters."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Dynamic Depth",
            "text": "Dynamic layer sparsity leads to dynamic depth networks that adjust their depth based on their model inputs. \n\nEach data point represents an inference of a single token using a threshold, confirming a spread within the network depth, where most tokens only need between 40 and 70 blocks, instead of the full network at 80 blocks."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Routing Traces",
            "text": "For more detailed analysis, Figure 7 ###reference_### shows the routing for the OPT-13B model across a batch of WikiText-2 examples. This motivates the use of dynamic layer sparsity over early-exit models, since early exit can only skip later layers, which contribute the most to the network."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Vision",
            "text": "This analysis so far has focused on large language models, since they are currently larger than large vision models. Yet, recent vision transformers have been proposed with tens of billions of parameters [2]. These weights are not yet released, yet the trends between the smaller language and vision models can still be aligned at smaller scales to suggest the behavior of large vision models with billions or trillions of parameters. Figure 8 shows a comparison for the largest released ViT model, which contains 632M parameters across 24 layers."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Radial Networks",
            "text": "Given the high degree of dynamic layer sparsity, we propose a new neural architecture that natively supports arbitrary routing between layers. As shown in Fig 1, each token enters the network at step and then is routed dynamically to the next layer at later steps. This process allows for dynamic computation given the variable number of layers included in each token path. As with standard transformers, the network continues auto-regressively until an end-of-sequence token is produced."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Router",
            "text": "The router is the central component with the responsibility of directing each embedding between layers at each time step. Building on the success of mixture-of-expert models, we learn this router with a small multi-layer perceptron (MLP) model. Beginning from a layer, it maps from intermediate embeddings to output router logits for each layer. These logits are then passed into a softmax function to produce probabilities. The maximum probability layer is then chosen as the next layer in the forward pass. The iterations stop when the model chooses the output layer, or a set maximum number of layers are seen, which forces the output layer. This maximum number of layers is a hyper-parameter that limits the worst-case dynamic depth in the network."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Unified Cache",
            "text": "In standard sequential transformers, each layer activates for each token, and the attention mechanism references the key-value cache of previous tokens. Given the dynamic routing of radial networks, many of the layers are not activated for different tokens, resulting in a sparse key-value cache. To solve this, we use a shared global cache that stores all previous key-value pairs. Each embedding attends to the cached pairs of previous iterations and tokens. To distinguish between the cached values for the current and previous tokens, we use standard positional embeddings. These embeddings encode the relative position of tokens within the input sequence. A common approach is to use sinusoidal functions; for position and dimension, the positional embedding can be defined as follows, where is the dimensionality of the embeddings."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In the past, dynamic layer sparsity has not been practical due to small model sizes and incompatible neural architectures, which caused large contributions from each layer and varying internal dimensions. For these reasons, it has only been possible with techniques like early-exit, which requires expensive specialized training. Yet, as language models grow in size, each layer contributes less to output, creating opportunities for dynamic layer sparsity.\n\nTo take advantage of this dynamic sparsity, we propose Radial Networks, which perform token-level routing between layers guided by a trained router module. These networks can be used in a post-training distillation from sequential networks or trained from scratch to co-learn the router and layer weights. They enable scaling to larger model sizes by decoupling the number of layers from the dynamic depth of the network, and their design allows for layer reuse. By varying the compute token by token, they reduce the overall resources needed for generating entire sequences. Overall, this leads to larger capacity networks with significantly lower compute and serving costs."
        }
    ],
    "url": "http://arxiv.org/html/2404.04900v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "5",
            "5.1",
            "5.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.04900v1",
        "paper_title": "Radial Networks: Dynamic Layer Routing for High-Performance Large Language Models",
        "research_background": "### Motivation:\nThe emergence of large language and vision models has led to significant advancements in achieving state-of-the-art performance across various tasks. However, as these transformer models become larger, their substantial computational requirements pose challenges for memory, latency, and power efficiency. Addressing these challenges necessitates exploring strategies for dynamic layer sparsity, which involve selectively skipping individual layers based on the specific input, thereby aiming to optimize computational efficiency.\n\n### Research Problem:\nThe key research problem addressed in this paper is the optimization of computational resources in large transformer models through dynamic layer routing. The contribution of each layer to the model's final output can be minimal and varies depending on the token and task. Previous models and architectures were not suitable for dynamic sparsity due to significant contributions at smaller scales and the risk of dimensional mismatches when skipping layers. The paper's goal is to explore and quantify the dynamic contributions of layers in modern transformers and develop methods to accurately and efficiently predict and exploit token-level sparsity.\n\n### Relevant Prior Work:\n1. **Dynamic Layer Sparsity**: Previous attempts at dynamic layer sparsity were not feasible for smaller neural network architectures or for architectures like CNNs, as they changed dimensions through depth, leading to dimensional mismatches if layers were skipped.\n2. **Early-Exit Methods**: Existing early-exit methods that aim to prune later layers assume that earlier layers are more crucial, which the paper argues to be inaccurate. The paper posits that contribution varies dynamically among layers and tokens.\n3. **OPT Family of Models**: The paper specifically explores the OPT family of models [reference ###b9###] for language tasks and ViT models [reference ###b3###] for vision tasks. Understanding the intricacies of these models serves as the foundation for the proposed dynamic layer routing strategies.\n\nBy focusing on the micro-analysis of residual blocks and evaluating various models for different tasks, the study aims to quantify the intermediate layer importance and lay the groundwork for improved accuracy and efficiency through dynamic layer routing.",
        "methodology": "The proposed methodology introduces a novel neural architecture designed to natively support arbitrary routing between layers, accommodating the high degree of dynamic layer sparsity. Here are the key components and innovations of the methodology:\n\n1. **Dynamic Routing Between Layers**:\n    - Tokens are not constrained to follow a fixed sequence of layers; instead, they are dynamically routed through different layers based on certain criteria.\n    - This routing allows for more flexible and potentially more efficient computational paths for each token.\n\n2. **Variable Token Path Lengths**:\n    - Each token in the input can travel through a variable number of layers rather than a fixed, predetermined number.\n    - This dynamic computation means that different tokens might engage different numbers of layers based on their requirements, optimizing the use of computational resources.\n\n3. **Auto-Regressive Processing**:\n    - Similar to standard transformers, the network processes tokens in an auto-regressive manner. This means it predicts the next token based on the previously generated tokens until it determines the end-of-sequence token.\n\n4. **End-of-Sequence Token**:\n    - The process continues until the network produces an end-of-sequence token, marking the conclusion of the sequence generation.\n\nBy supporting this level of dynamic layer routing, the proposed Radial Networks architecture aims to enhance the performance and efficiency of large language models. This innovative approach leverages variable layer engagement, ensuring that different tokens may traverse different computational paths within the model as needed.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:**\n- **WikiText-2**: Used to analyze the dynamic layer routing within the OPT model.\n- **COCO**: Used alongside WikiText-2 examples to analyze the ViT model's layer sparsity.\n\n**Models:**\n- **OPT**: Evaluated using WikiText-2 to understand the behavior of dynamic layer routing.\n- **ViT (Vision Transformer)**: Evaluated using the COCO dataset in conjunction with WikiText-2 examples.\n\n**Evaluation Method:**\n- The main proxy utilized by the oracles to measure layer sparsity is the **residual ratio**, which is used as an indicator of the relative importance between the main computation branch and the skip branch within the network's architecture.\n\n**Experimental Conditions:**\n- **Batch-size one inference**: All WikiText-2 examples are packed together to mimic a batch-size one inference scenario. This setting is used to simulate a common practical scenario and to eliminate the complications associated with the dynamic layer sparsity that can occur with larger batch sizes.\n\n### Main Experimental Results\n\nThe experiments in this section predominantly focus on using the residual ratio to assess and analyze the dynamic layer sparsity within the models. The findings include observations on how dynamically routing layers based on the residual ratio affects model performance and efficiency:\n\n1. **Residual Ratio Analysis**: \n   - The residual ratio is used effectively to determine which layers can be dynamically skipped without significantly impacting model performance.\n   - For both OPT and ViT models, certain layers showed higher residual ratios, indicating a higher propensity for skipping during inference while maintaining performance.\n\n2. **Layer Sparsity Patterns**:\n   - Analysis of layer sparsity via the residual ratio indicated that both the OPT and ViT models could maintain high performance with a significant portion of layers being conditionally skipped based on the residual ratio.\n   - This dynamic routing resulted in more efficient model executions while preserving the high-level efficacy of the models.\n\nIn summary, the primary experiments demonstrate the effectiveness of using the residual ratio as a measure for dynamic layer routing, showing potential benefits in terms of computational efficiency without sacrificing model performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Explore the practical advantages and behavior of dynamic layer sparsity by profiling residual connections in large language models (LLMs)",
            "experiment_process": "The process involves defining the residual ratio as a proxy measure to capture the contribution of the residual branches. This ratio is utilized to profile the residual contribution across various models within the OPT family, ranging from OPT-125M to OPT-66B. Each data point represents an individual token during model generation, with experiments conducted using the Wikitext-2 dataset at a sequence length of 256.",
            "result_discussion": "Findings indicate that as model size increases, the distribution of residual ratios skews left, showing more opportunities for layer sparsity. For instance, the median residual ratio decreases from 20% in OPT-125M to 5.9% in OPT-66B. Insights reveal that residual ratio trends are linked to the overall number of parameters, not just the number of layers, substantiating that larger models will benefit from dynamic layer sparsity.",
            "ablation_id": "2404.04900v1.No1"
        },
        {
            "research_objective": "Demonstrate the dynamic depth behavior stemming from the application of dynamic layer sparsity based on input tokens.",
            "experiment_process": "By analyzing the residual ratio across all layers of an OPT-13B model, mean residual ratios are calculated across tokens fetched from Wikitext-2 data (sequence length 256). Variance in these ratios is visually displayed to highlight dynamic depth. Hypothetical oracles are employed to simulate dynamic depth, using a residual ratio threshold set at 5%, profiling how many layers each token actually requires during inference.",
            "result_discussion": "Results highlight that tokens typically utilize between 40 and 70 blocks rather than being processed across the full 80-layer network. Token-based variance suggests applying dynamic layer sparsity selectively. This reduces computational demand as most tokens necessitate fewer layers for processing, confirming efficient dynamic depth utilization.",
            "ablation_id": "2404.04900v1.No2"
        },
        {
            "research_objective": "Compare the residual ratios and potential for layer sparsity across different types of large models, specifically large language models and vision transformers.",
            "experiment_process": "The analysis includes large LLMs like OPT models and large vision models such as Vision Transformers (ViT). Residual ratios are compared across models like the ViT-632M (24 layers) and smaller ViT versions, checking if trends observed in LLMs like OPT-350M apply to vision transformers.",
            "result_discussion": "The study shows that the residual ratios of vision transformers are comparable to similarly sized OPT models. The pattern of decreasing residual ratios with increasing model size is observed, suggesting that larger vision models with billions or trillions of parameters will also benefit from dynamic layer sparsity like LLMs.",
            "ablation_id": "2404.04900v1.No3"
        }
    ]
}