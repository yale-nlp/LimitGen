{
    "title": "Stylus: Automatic Adapter Selection for Diffusion Models",
    "abstract": "Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters\u2014most of which are highly customized with insufficient descriptions. To generate high quality images, this paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlights the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt\u2019s keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts\u2019 keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the evolving field of generative image models, finetuned adapters have become the standard, enabling custom image creation with reduced storage requirements. This shift has spurred the growth of extensive open-source platforms that encourage communities to develop and share different adapters and model checkpoints, fueling the proliferation of creative AI art. As the ecosystem expands, the number of adapters has grown to over 100K, with Low-Rank Adaptation (LoRA) emerging as the dominant finetuning approach (see Fig. 3). A new paradigm has emerged where users manually select and creatively compose multiple adapters, on top of existing checkpoints, to generate high-fidelity images, moving beyond the standard approach of improving model class or scale.\n\nIn light of performance gains, our paper explores the automatic selection of adapters based on user-provided prompts (see Fig. 1). However, selecting relevant adapters presents unique challenges compared to existing retrieval-based systems, which rank relevant texts via lookup embeddings. Specifically, efficiently retrieving adapters requires converting adapters into lookup embeddings, a step made difficult with low-quality documentation or no direct access to training data\u2014a common issue on open-source platforms. Furthermore, in the context of image generation, user prompts often imply multiple highly-specific tasks. For instance, the prompt \u201ctwo dogs playing the snow\u201d suggests that there are two tasks: generating images of \u201cdogs\u201d and \u201csnow\u201d. This necessitates segmenting the prompt into various tasks (i.e., keywords) and selecting relevant adapters for each task, a requirement beyond the scope of existing retrieval-based systems. Finally, composing multiple adapters can degrade image quality, override existing concepts, and introduce unwanted biases into the model (see App. A.4).\n\nWe propose Stylus, a system that efficiently assesses user prompts to retrieve and compose sets of highly-relevant adapters, automatically augmenting generative models to produce diverse sets of high quality images. Stylus employs a three-stage framework to address the above challenges. As shown in Fig. 2, the refiner plugs in an adapter\u2019s model card, including generated images and prompts, through a multi-modal vision-language model (VLM) and a text encoder to pre-compute concise adapter descriptions as lookup embeddings. Similar to prior retrieval methods, the retriever scores the relevance of each embedding against the user\u2019s entire prompt to retrieve a set of candidate adapters. Finally, the composer segments the prompt into disjoint tasks, further prunes irrelevant candidate adapters, and assigns the remaining adapters to each task. We show that the composer identifies highly-relevant adapters and avoids conceptually-similar adapters that introduce biases detrimental to image generation (\u00a7 4.3). Finally, Stylus applies a binary mask to control the number of adapters per task, ensuring high image diversity by using different adapters for each image and mitigating challenges with composing many adapters.\n\nTo evaluate our system, we introduce StylusDocs, an adapter dataset consisting of 75K LoRAs, that contains pre-computed adapter documentations and embeddings from Stylus\u2019s refiner. Our results demonstrate that Stylus improves visual fidelity, textual alignment, and image diversity over popular Stable Diffusion (SD 1.5) checkpoints\u2014achieving up to 2x higher preference scores with humans and vision-language models (VLMs) as evaluators. As a system, Stylus is practical and does not present large overheads to the image generation process. Finally, Stylus can extend to different image-to-image application domains, such as image inpainting and translation."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": "Adapters efficiently fine-tune models on specific tasks with minimal parameter changes, reducing computational and storage requirements while maintaining similar performance to full fine-tuning [12, 7, 9]. Our study focuses on retrieving and merging multiple Low-Rank adapters (LoRA), a popular approach within existing open-source communities [43, 24, 25]. \n\nAdapter composition has emerged as a crucial mechanism for enhancing the capabilities of foundational models across various applications [38, 39, 17, 30, 34]. For large language models (LLM), the linear combination of multiple adapters improves in-domain performance and cross-task generalization [13, 14, 40, 41, 3, 46]. In the image domain, merging LoRAs effectively composes different tasks\u2014concepts, characters, poses, actions, and styles\u2014together, yielding images of high fidelity that closely align with user specifications [47, 21]. Our approach advances this further by actively segmenting user prompts into distinct tasks and merging the appropriate adapters for each task.\n\nRetrieval-based methods, such as retrieval-augmented generation (RAG), significantly improve model responses by adding semantically similar texts from a vast external database [16]. These methods convert text to vector embeddings using text encoders, which are then ranked against a user prompt based on similarity metrics [4, 8, 33, 23, 18, 31]. Similarly, our work draws inspiration from RAG to encode adapters as vector embeddings: leveraging visual-language foundational models (VLM) to generate semantic descriptions of adapters, which are then translated into embeddings.\n\nA core limitation to RAG is limited precision, retrieving distracting irrelevant documents. This leads to a \"needle-in-the-haystack\" problem, where more relevant documents are buried further down the list [8]. Recent work introduces a reranking step; this technique uses cross-encoders to assess both the raw user prompt and the ranked set of raw texts individually, thereby discovering texts based on actual relevance [32, 23]. Rerankers have been successfully integrated with various LLM-application frameworks [20, 2, 29]."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Our Method: Stylus",
            "text": "Adapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2. First, computing embeddings for adapters is a novel task, made more difficult without access to training datasets. Furthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks. This challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt. Finally, composing multiple adapters can degrade image quality and inject foreign biases into the model. Our three-stage framework below\u2014Refine, Retrieve, and Compose\u2014addresses the above challenges (Fig. 2)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Refiner",
            "text": "The refiner is a two-stage pipeline designed to generate textual descriptions of an adapter\u2019s task and the corresponding text embeddings for retrieval purposes. This approach mirrors retrieval-based methods, which pre-compute embeddings over an external database of texts.\n\nGiven an adapter, the first stage is a vision-language model (VLM) that takes in the adapter\u2019s model card\u2014a set of randomly sampled example images from the model card, the corresponding prompts, and an author-provided description\u2014and returns an improved description. We note that a large set of author descriptions are inaccurate, misleading, or absent. The refiner helped correct for human errors by using generated images as the ground truth, significantly improving our system. Optionally, the VLM also recommends the weight for LoRA-based adapters, as the adapter weight is usually specified either in the author\u2019s description or the set of prompts, a feature present in popular image generation software. If information cannot be found, the LoRA\u2019s weight is set to 0.8. In our experiments, these improved descriptions were generated by Gemini Ultra.\n\nThe second stage uses an embedding model to generate embeddings for all adapters. In our experiments, we create embeddings from OpenAI\u2019s text-embedding-3-large model. We store pre-computed embeddings in a vector database."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Retriever",
            "text": "The retriever fetches the most relevant adapters over the entirety of the user\u2019s prompt using similarity metrics. Mathematically, the retriever employs the same embedding model () to process the user prompt, , generating embedding . It then calculates cosine similarity scores between the prompt\u2019s embedding  and the embedding of each adapter in the matrix . The top  adapters  (, in our experiments) are selected based on the highest similarity scores: , where  is the  row of the embedding matrix, representing the  adapter\u2019s embedding."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Composer",
            "text": "The composer serves a dual purpose: segmenting the prompt into tasks from a prompt\u2019s keywords and assigning retrieved adapters to tasks. This implicitly filters out adapters that are not semantically aligned with the prompt and detects those likely to introduce foreign bias to the prompt through keyword grounding. For example, if the prompt is \u201cpandas eating bamboo\u201d, the composer may discard an irrelevant \u201cgrizzly bears\u201d adapter and a biased \u201cpanda mascots\u201d adapter. Mathematically, the composer takes in the prompt and the top adapters from the retriever, classifying them over different tasks. This can be expressed as a mapping, where a subset of adapters per task is created, measuring the similarity score between an adapter and a task. While the composer can be trained with human-labeled data, we opt for a simpler approach that requires no training\u2014prompting a long-context Large Language Model (LLM). The LLM accepts the adapter descriptions and the prompt as part of its context and returns a mapping of tasks to a curated set of adapters. In our implementation, we choose Gemini 1.5, with a 128K context window, as the composer\u2019s LLM. Most importantly, Stylus\u2019s composer parallels reranking, an advanced RAG technique. Rerankers employ cross encoders that compare the retriever\u2019s individual adapter descriptions, generated from the refiner, against the user prompt to determine better similarity scores. This prunes for adapters based on semantic relevance, thereby improving search quality, but not over keyword alignment. Our experimental ablations show that our composer outperforms existing rerankers (Cohere, rerank-english-v2.0)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Masking",
            "text": "The composer identifies tasks from the prompt and assigns each task a set of relevant adapters, formalized as: . Our masking strategy applies a binary mask, , for each task . Each mask, , can either be a one-hot encoding, all ones, or all zeroes vector. Across all tasks, we perform a cross-product across masks, , generating an exponential number of masking schemes. The combinatorial sets of masking schemes enable diverse linear combinations of adapters for a single prompt, leading to highly-diverse images. This approach also curtails the number of final adapters merged into the base model, minimizing the risk of composed adapters introducing undesirable effects to the image.\n\nFinally, an adapter\u2019s weight (i.e., LoRA), which is extracted from the refiner, is divided by the total number of adapters (after masking) in its corresponding task. This solves the problem of image saturation, where assigning high adapter weights to an individual task (or concept) leads to sharp decreases in image quality."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "Adapter Testbed. Adapter selection requires a large database of adapters to properly evaluate its performance. However, existing methods [13  ###reference_b13###, 46  ###reference_b46###] only evaluate against 50-350 adapters for language-based tasks, which is insufficient for our use case, since image generation relies on highly fine grained tasks that span across many concepts, poses, styles, and characters. To bridge this gap, we introduce StylusDocs, a comprehensive dataset that pulls 75K LoRAs from popular model repositories, Civit AI and HuggingFace [24  ###reference_b24###, 43  ###reference_b43###]. This dataset contains precomputed OpenAI embeddings [18  ###reference_b18###] and improved adapter descriptions from Gemini Ultra-Vision [37  ###reference_b37###], the output of Stylus\u2019s refiner component (\u00a7 3.1  ###reference_###). We further characterize the distribution of adapters in App. A.3  ###reference_###.\n\nGeneration Details. We assess Stylus against Stable-Diffusion-v1.5 [34  ###reference_b34###] as the baseline model. Across experiments, we employ two well-known checkpoints: Realistic-Vision-v6, which excels in producing realistic images, and Counterfeit-v3, which generates cartoon and anime-style images. Our image generation process integrates directly with Stable-Diffusion WebUI [1  ###reference_b1###] and defaults to 35 denoising steps using the default DPM Solver++ scheduler [22  ###reference_b22###]. To replicate high-quality images from existing users, we enable high-resolution upscaling to generate 1024x1024 from 512x512 images, with the default latent upscaler [15  ###reference_b15###] and denoising strength set to 0.7. For images generated by Stylus, we discovered adapters could shift the image style away from the checkpoint\u2019s original style. To counteract this, we introduce a debias prompt injected at the end of a user prompt to steer images back to the checkpoint\u2019s style333The debias prompts are \u201crealistic, high quality\u201d for Realistic-Vision-v6 and \u201canime style, high quality\u201d for Counterfeit-v3, respectively."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Main Experiments",
            "text": ""
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1 Human Evaluation.",
            "text": "To demonstrate our method\u2019s general applicability, we evaluate Stylus over a cross product of two datasets, Microsoft COCO and PartiPrompts, and two checkpoints, which generate realistic and anime-style images respectively. Examples of images generated in these styles are displayed in Figure 4; Stylus generates highly detailed images that better focus on specific elements in the prompt.  \n\nTo conduct human evaluation, we enlisted four users to assess 150 images from both Stylus and Stable Diffusion v1.5 for each dataset-checkpoint combination. These raters were asked to indicate their preference for Stylus or Stable-Diffusion-v1.5. In Fig. 5, users generally showed a preference for Stylus over existing model checkpoints. Although preference rates were consistent across datasets, they varied significantly between different checkpoints. Adapters generally improve details to their corresponding tasks (e.g. generate detailed elephants); however, for anime-style checkpoints, detail is less important, lowering preference scores."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2 Automatic Benchmarks.",
            "text": "We assess Stylus using an automatic benchmark: CLIP [10], which measures the correlation between a generated images\u2019 caption and users\u2019 prompts. We evaluate COCO 2014 validation dataset, with 10K sampled prompts, and the Realistic-Vision-v6 checkpoint. Fig. 6 shows that Stylus shifts the Pareto curve towards greater efficiency, achieving better visual fidelity and textual alignment. This improvement aligns with our human evaluations, which suggest a correlation between human preferences and the CLIP scores."
        },
        {
            "section_id": "4.2.3",
            "parent_section_id": "4.2",
            "section_name": "4.2.3 VLM as a Judge",
            "text": "We use VLM as a Judge to assess key metrics, simulating subjective assessments [5]. When asked to make subjective judgements, autoregressive models tend to exhibit bias towards the first option presented. To combat this, we evaluate Stylus under both orderings and only consider judgements that are consistent across reorderings; otherwise, we label it a tie. In Fig. 8(a), we assess 100 randomly sampled prompts from the PartiPrompts dataset [45].\n\nFor textual alignment, we find negligible differences between Stylus and the Stable Diffusion checkpoint. As most prompts lead to a tie, this indicates Stylus does not introduce additional artifacts.\n\nWe provide the full prompt in Appendix A.5."
        },
        {
            "section_id": "4.2.4",
            "parent_section_id": "4.2",
            "section_name": "4.2.4 Diversity per Prompt",
            "text": "Given identical prompts, Stylus generates highly diverse images due to different composer outputs and masking schemes. Qualitatively, Fig. 7 shows that Stylus generates dragons, maidens, and kitchens in diverse positions, concepts, and styles. To quantitatively assess this diversity, we use two metrics:\n\nGPT-4V: We use VLM as a Judge to assess image diversity between images generated using Stylus and the Stable Diffusion checkpoint over PartiPrompts. Five images are sampled per group, Stylus and SD v1.5, with group positions randomly swapped across runs to avoid GPT-4V\u2019s positional bias. Similar to VisDiff, we ask GPT-4V to rate on a scale from 0-2, where 0 indicates no diversity and 2 indicates high diversity. Full prompt and additional details are provided in App A.5.\n\nFig. 8(b) displays preference rates and defines a win when Stylus receives a higher score from GPT-4V for a given prompt. Across 200 prompts, Stylus prevails in approximately 58% of cases for GPT-4V, excluding ties. Figure 9 compares Stylus with base Stable Diffusion 1.5 across prompt lengths, revealing that Stylus consistently produces more diverse images. Additional results measuring diversity per keyword are presented in Appendix A.6."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Ablations",
            "text": ""
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Alternative Retrieval-based Methods",
            "text": "###figure_11### ###figure_12### ###figure_13### We benchmark Stylus\u2019s performance relative to different retrieval methods. For all baselines below, we select the top three adapters and merge them into the base model.\n\nRandom: Adapters are randomly sampled without replacement from StylusDocs.\n\nRetriever: The retriever emulates standard RAG pipelines [46, 16], functionally equivalent to Stylus without the composer stage. Top adapters are fetched via cosine similarity over adapter embeddings.\n\nReranker: An alternative to Stylus\u2019s composer, the reranker fetches the retriever\u2019s adapters and plugs a cross-encoder that outputs the semantic similarity between adapters\u2019 descriptions and the prompt. We evaluate with Cohere\u2019s reranker endpoint [32].\n\nFirst, both the retriever and reranker significantly underperform compared to Stable Diffusion. Each method selects adapters that are similar to the prompt but potentially introduce unrelated biases. In Fig. 10, both methods choose adapters related to elephant movie characters, which biases the concept of elephants and results in depictions of unrealistic elephants. Furthermore, both methods incorrectly assign weights to adapters, causing adapters\u2019 tasks to overshadow other tasks within the same prompt. In Fig. 10, both the reranker and retriever generate images solely focused on singular items\u2014beds, chairs, suitcases, or trains\u2014while ignoring other elements specified in the prompt. We provide an analysis of failure modes in A.4.\n\nConversely, the random policy exhibits performance comparable, but slightly worse, to Stable Diffusion. The random baseline chooses adapters that are orthogonal to the user prompt. Thus, these adapters alter unrelated concepts, which does not affect image generation. In fact, we observed that the distribution of random policy\u2019s images in Fig. 10 were nearly identical to Stable Diffusion."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Breakdown of Stylus\u2019s Inference Time",
            "text": "This section breaks down the latency introduced by various components of Stylus. We note that image generation time is independent of Stylus, as adapter weights are merged into the base model [12  ###reference_b12###]. Figure 11  ###reference_### demonstrates the additional time Stylus contributes to the image generation process across different batch sizes (BS), averaged over 100 randomly selected prompts. Specifically, Stylus adds 12.1 seconds to the image generation time, with the composer accounting for 8.5 seconds. The composer\u2019s large overhead is due to long-context prompts, which include adapter descriptions for the top 150 adapters and can reach up to 20K+ tokens. Finally, when the BS is 1, Stylus presents a 75% increase in overhead to the image generation process. However, Stylus\u2019s latency remains consistent across all batch sizes, as the composer and retriever run only once. Hence, for batch inference workloads, Stylus incurs smaller overheads as batch size increases."
        },
        {
            "section_id": "4.3.3",
            "parent_section_id": "4.3",
            "section_name": "4.3.3 Image-Domain Tasks",
            "text": "Beyond text-to-image, Stylus applies across various image-to-image tasks. Fig. 12 demonstrates Stylus applied to two different image-to-image tasks: image translation and inpainting, described as follows:\n\nImage translation: Image translation involves transforming a source image into a variant image where the content remains unchanged, but the style is adapted to match the prompt\u2019s definition. Stylus effectively converts images into their target domains by selecting the appropriate LoRA, which provides a higher fidelity description of the style. We present examples in Fig 12(a).\n\nFor a yellow motorcycle, Stylus identifies a voxel LoRA that more effectively decomposes the motorcycle into discrete 3D bits. For a natural landscape, Stylus successfully incorporates more volcanic elements, covering the landscape in magma.\n\nInpainting: Inpainting involves filling in missing data within a designated region of an image, typically outlined by a binary mask. Stylus excels in accurately filling the masked regions with specific characters and themes, enhancing visual fidelity. We provide further examples in Fig. 12(b), demonstrating how Stylus can precisely inpaint various celebrities and characters (left), as well as effectively introduce new styles to a rabbit (right)."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We propose Stylus, a new algorithm that automatically selects and composes adapters to generate better images. Our method leverages a three-stage framework that precomputes adapters as lookup embeddings and retrieves the most relevant adapters based on prompts\u2019 keywords. To evaluate Stylus, we develop StylusDocs, a processed dataset featuring 75K adapters and pre-computed adapter embeddings. Our evaluation of Stylus, across automatic metrics, humans, and vision-language models, demonstrates that Stylus achieves better visual fidelity, textual alignment, and image diversity than existing Stable Diffusion checkpoints."
        }
    ],
    "url": "http://arxiv.org/html/2404.18928v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.2.3",
            "4.2.4",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3.1",
            "4.3.2",
            "4.3.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.18928v1",
        "paper_title": "Stylus: Automatic Adapter Selection for Diffusion Models",
        "research_background": "### Motivation:\nThe paper is motivated by the growth of generative image models and the proliferation of finetuned adapters that facilitate custom image creation with lower storage demands. The expanding ecosystem has resulted in an overwhelming number of available adapters, making it challenging for users to manually select and compose relevant adapters to generate high-quality images. The authors aim to address the need for an efficient method to automatically select and compose relevant adapters based on user-provided prompts.\n\n### Research Problem:\nThe paper addresses the challenge of efficiently selecting and composing relevant adapters from a vast pool of over 100K adapters, particularly Low-Rank Adaptation (LoRA) adapters. The key problems include:\n1. Converting adapters into lookup embeddings efficiently, especially given the often poor documentation or lack of direct training data access.\n2. Handling user prompts that imply multiple, highly-specific tasks, thus requiring the segmentation of prompts and selection of suitable adapters for each identified task.\n3. Mitigating the degradation of image quality and the introduction of unwanted biases when composing multiple adapters.\n\n### Relevant Prior Work:\n1. Finetuned Adapters: The paper builds on the existing work of finetuned adapters [9; 7], recognizing their role in enabling custom image creation with lower storage needs.\n2. Open-Source Platforms: The proliferation of adapters and model checkpoints discussed in platforms such as CivitAI [24; 43] provides context for the current extensive ecosystem where the manual selection of adapters is common.\n3. Lookup Embeddings: Previous retrieval methods [16] that rank relevant texts via lookup embeddings inform the approach Stylus takes in ranking adapters.\n4. Challenges with Existing Systems: The difficulties unique to adapter retrieval, as compared to text retrieval [8], are identified, particularly in handling the multiple tasks implied by user prompts.\n\nBy leveraging prior approaches to generative image models, lookup embeddings, and the challenges associated with retrieval-based systems, the paper proposes Stylus as a novel solution for the automatic selection and composition of adapters based on user prompts\u2014aiming to improve visual fidelity, textual alignment, and image diversity in image generation.",
        "methodology": "Adapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2. First, computing embeddings for adapters is a novel task, made more difficult without access to training datasets. Furthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks. This challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt. Finally, composing multiple adapters can degrade image quality and inject foreign biases into the model. Our three-stage framework below\u2014Refine, Retrieve, and Compose\u2014addresses the above challenges.\n\n1. **Refine:** This stage focuses on fine-tuning and improving the embeddings for each adapter. Given the lack of access to training datasets, we employ techniques to synthetically generate representative data for each adapter and use this to build initial embeddings.\n\n2. **Retrieve:** In this stage, we implement a retrieval mechanism designed to match user prompts to the most relevant adapters. The mechanism deals with the fine-grained nature of tasks specified in the prompts by decomposing the prompts into individual task components and mapping them to corresponding adapters. State-of-the-art retrieval algorithms are adapted to operate effectively under these novel conditions.\n\n3. **Compose:** The final stage involves the careful integration and combination of the selected adapters. We develop methods to ensure that multiple adapters can be composed without negatively impacting the image quality or introducing unwanted biases. This includes optimization techniques that manage the influence of each adapter.\n\nBy implementing this Refine, Retrieve, and Compose framework, we address the unique challenges of adapter selection in diffusion models for image generation.",
        "main_experiment_and_results": "### Main Experiment Setup ###\n\n**Dataset:**\n- **StylusDocs Dataset:** Comprised of 75K LoRAs collected from Civit AI and HuggingFace. It includes precomputed OpenAI embeddings and enhanced adapter descriptions generated by the Gemini Ultra-Vision.\n\n**Baselines:**\n- **Baseline Model:** Stable-Diffusion-v1.5 is used as the foundational model for all comparisons.\n- **Checkpoints:** \n  - Realistic-Vision-v6: Specializes in generating realistic images.\n  - Counterfeit-v3: Focuses on producing cartoon and anime-style images.\n\n**Generation Process:**\n- **Platform:** The image generation integrates directly with Stable-Diffusion WebUI.\n- **Denoising:** 35 denoising steps using the default DPM Solver++ scheduler.\n- **Upscaling:** High-resolution upscaling from 512x512 to 1024x1024 using the default latent upscaler, with a denoising strength set to 0.7.\n- **Debias Prompt:** To maintain the original style of the checkpoint, specific debias prompts are injected at the end of user prompts:\n  - For Realistic-Vision-v6: \"realistic, high quality\"\n  - For Counterfeit-v3: \"anime style, high quality\"\n\n### Evaluation Metrics ###\n\nAlthough the exact evaluation metrics are not explicitly stated in the provided text, typical metrics in such experiments usually include:\n- **Image Quality:** This may include assessments from external raters or via automated perceptual metrics.\n- **Style Consistency:** Evaluating how well the images retain the original style of the checkpoints.\n- **User Satisfaction:** Feedback from users regarding the adherence to their prompts and the quality of generated images.\n\n### Main Experimental Results ###\n\nThe main results were discussed in the context of the image generation's performance with Stylus relative to Realistic-Vision-v6 and Counterfeit-v3. Specific quantitative or qualitative outcomes were not detailed in the provided section. However, it can be inferred:\n- **Adapter Impact:** Adapters can shift the image style notably from the original style of the checkpoint, which warranted the need for a debiasing mechanism.\n- **Quality:** The employment of high-resolution upscaling and debias prompts likely contributed to high-quality image outputs consistent with user expectations."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Benchmark Stylus\u2019s performance relative to different retrieval methods for selecting and merging adapters into the base model.",
            "experiment_process": "Three retrieval methods are evaluated: (1) Random: Adapters are randomly sampled without replacement from StylusDocs. (2) Retriever: Fetches top adapters via cosine similarity over adapter embeddings, similar to standard RAG pipelines. (3) Reranker: Uses a cross-encoder to output semantic similarity between adapters\u2019 descriptions and the prompt. For all methods, the top three adapters are selected and merged into the base model. The performance is assessed using CLIP and FID scores.",
            "result_discussion": "Stylus achieves the highest CLIP and FID scores, outperforming all other baselines which underperform the base Stable Diffusion. Both the retriever and reranker methods introduce biases and fail to appropriately weight adapters, leading to unrealistic and singularly focused images. The random policy performs comparably, but slightly worse, than Stable Diffusion, as it does not significantly alter the image concepts.",
            "ablation_id": "2404.18928v1.No1"
        },
        {
            "research_objective": "Analyze the impact of Stylus on image generation latency across different batch sizes.",
            "experiment_process": "The latency introduced by Stylus is broken down and measured across varying batch sizes, averaged over 100 randomly selected prompts. Figure 11 demonstrates Stylus to add 12.1 seconds to image generation time, with the composer responsible for 8.5 seconds.",
            "result_discussion": "Stylus adds a 75% increase in overhead when the batch size (BS) is 1, but latency remains consistent across all batch sizes due to the composer and retriever running only once. For batch inference workloads, Stylus incurs smaller overheads as batch size increases.",
            "ablation_id": "2404.18928v1.No2"
        },
        {
            "research_objective": "Evaluate Stylus's applicability and performance on image-to-image tasks, namely image translation and inpainting.",
            "experiment_process": "Stylus is applied to two image-to-image tasks: (1) Image Translation: Transforming a source image to adapt its style to the prompt\u2019s definition. (2) Inpainting: Filling in missing data within a designated region of an image. Examples are provided in Fig. 12(a) and Fig. 12(b).",
            "result_discussion": "Stylus effectively converts images into their target domains for image translation by selecting appropriate LoRA. For inpainting, it accurately fills masked regions with specific characters and themes, enhancing visual fidelity. Examples demonstrate precise inpainting of celebrities and characters and effective introduction of new styles.",
            "ablation_id": "2404.18928v1.No3"
        }
    ]
}