{
    "title": "Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness",
    "abstract": "Recent advancements in Natural Language Processing (NLP) have seen Large-scale Language Models (LLMs) excel at producing high-quality text for various purposes. Notably, in Text-To-Speech (TTS) systems, the integration of BERT for semantic token generation has underscored the importance of semantic content in producing coherent speech outputs. Despite this, the specific utility of LLMs in enhancing TTS synthesis remains considerably limited. This research introduces an innovative approach, Llama-VITS, which enhances TTS synthesis by enriching the semantic content of text using LLM. Llama-VITS integrates semantic embeddings from Llama2 with the VITS model, a leading end-to-end TTS framework. By leveraging Llama2 for the primary speech synthesis process, our experiments demonstrate that Llama-VITS matches the naturalness of the original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the LJSpeech dataset, a substantial collection of neutral, clear speech. Moreover, our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem dataset, a curated selection of emotionally consistent speech from the EmoV_DB dataset, highlighting its potential to generate emotive speech.\n\nKeywords:\u2009Text-To-Speech, Emotive Speech, Large-scale Language Model, Semantic Embedding",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Text-to-Speech (TTS) synthesis is a technology that transforms written text into its spoken equivalent, thereby enhancing content accessibility. This technology finds application in the production of audiobooks (Chen et al., 2022) and virtual assistants (Wu et al., 2023). However, traditional TTS models, which primarily focus on the acoustic features, often fall short in comprehending the semantic and emotional information embedded within the text.\n\nWith significant advancements in Natural Language Processing (NLP) technologies, particularly through Language Models (LMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018; Brown et al., 2020), which have demonstrated formidable capabilities in understanding and generating natural language, researchers have proposed various BERT-based TTS models (Mukherjee et al., 2022; Abbas et al., 2022; Li et al., 2023; Guo et al., 2022) to improve the expressiveness of synthesized speech. Nonetheless, the effectiveness and flexibility of BERT-based TTS models in diverse applications are limited due to the smaller parameter size of BERT models and the necessity for designing specific fine-tuning tasks to enhance their capabilities.\n\nOn the other hand, Large-scale Language Models (LLMs), such as Llama2 (Touvron et al., 2023), not only require decreasing computational resources and achieve higher levels of text generation but also possess excellent zero-shot learning capabilities. Moreover, they can achieve improvements comparable to fine-tuning by adjusting only a minimal number of parameters through prompt tuning (Liu et al., 2022; Tu et al., 2022). However, the potential of these LLMs for TTS tasks has not been fully explored.\n\nIn light of this context, we introduce Llama-VITS, a model that leverages semantic representations extracted from Llama2 on top of a state-of-the-art TTS model, VITS (Kim et al., 2021), enabling the generated speech to retain acoustic information while understanding and expressing semantics and emotions. Through comprehensive objective and subjective evaluations, Llama-VITS has been verified to surpass TTS baselines without semantic input or those integrated with BERT.\n\nThe main contributions encapsulate:\n- We propose Llama-VITS model that utilizes the semantic understanding and expression capabilities of Llama2, offering equal or superior acoustic performance compared to baseline models, along with a significantly enhanced ability to understand and express semantics and emotions.\n- Through empirical analysis, we demonstrate that global tokens in Llama-VITS provide more significant improvements than sequential tokens, contrasting with observations in BERT-based TTS models.\n- We quantitatively verified our findings using both subjective and objective metrics.\n\nOur code, models, audio demos, and the filtered single female speaker emotional dataset EmoV_DB_bea_sem are available at https://github.com/xincanfeng/vitsgpt.git."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": "TTS technology has significantly advanced in learning acoustic features through structural evolution. However, comprehending and conveying semantics remain challenging. Since BERT-like LMs have demonstrated profound capabilities in understanding semantics through extensive pre-training on vast text corpora, some studies have integrated BERT-like LMs with TTS technology to enhance synthesized speech. Nonetheless, research on incorporating GPT-like LMs within TTS technology is notably scarce."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Text-To-Speech Models",
            "text": "TTS task aims to generate natural, fluent, and easily comprehensible speech. Traditional TTS systems, e.g., a Statistical Parametric Speech Synthesis (SPSS) system, usually comprise multiple distinct components. These include a frontend module that converts text into linguistic features (such as duration and pitch), an acoustic model that maps these linguistic features to acoustic features, and a vocoder responsible for generating speech waveforms from the acoustic features. Over the past decades, the complexity of traditional models has been notable, attributed to their reliance on manually engineered features and the intricate communication between modules.\n\nTransitioning from Hidden Markov Models (HMM) based models, through Deep Neural Networks (DNN) models, to Generative Adversarial Networks (GAN) based models, there has been a notable enhancement in voice quality, yet the architectural complexity remains significant.\n\nThe advent of end-to-end TTS models marks a significant milestone, increasingly reducing the distinction between synthesized speech and human voice. End-to-end models are capable of transforming raw text directly into final speech output, which not only streamlines the structural complexity of TTS systems and facilitates easier deployment but also significantly reduces the dependency on manual feature engineering, simplifying the training process. Moreover, they notably enhance the naturalness and intelligibility of the speech, thereby becoming the predominant architecture in TTS models. For instance, Char2Wav introduces an attentive encoder-decoder framework for direct speech synthesis from text input. Tacotron undertakes training from the ground up and directly predicts linear spectrograms. Furthermore, the speech produced by Tacotron2 closely mirrors the natural human voice.\n\nIn the realm of end-to-end TTS models, many have adopted a non-autoregressive architecture. This architecture enables parallel data processing, where the model\u2019s output generation does not depend on the output of the previous time step, thereby enhancing processing speed. It also circumvents the error accumulation issue inherent in traditional autoregressive models, which significantly boosts TTS performance. FastSpeech and its variants exemplify this trend. FastSpeech employs a transformer-based architecture to generate mel-spectrograms in parallel. Building on FastSpeech, FastPitch predicts pitch contours during inference, enabling the production of more expressive and high-quality speech. FastSpeech2 further incorporates explicit duration prediction and introduces pitch and energy as conditional inputs.\n\nPrevious non-autoregressive approaches typically involve distinct training phases for acoustic models and vocoders. VITS introduces a more natural-sounding output compared to these two-stage systems through its one-stage parallel end-to-end architecture. Innovatively, VITS incorporates variational inference combined with normalizing flows and employs an adversarial training methodology. Due to VITS\u2019s exemplary performance across multiple benchmarks, we select it as the foundational TTS model for our system."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Fine-tuning BERT-like LMs for TTS",
            "text": "While TTS models have increasingly advanced in replicating acoustic features, insufficient training data can hinder the model\u2019s ability to learn the semantic nuances of the same input across different contexts, thus limiting its expressiveness. Consequently, researchers have turned to leveraging the transfer learning capabilities of BERT-like LMs. Ultimately, TTS systems that incorporate pre-trained and fine-tuned BERT-like LMs have achieved better understandings of semantics and enhanced generated speech, marking a significant advancement. Hayashi et al. (2019) utilized a pre-trained BERT model as an auxiliary input to enhance a Tacotron2-based TTS system, resulting in improved speech naturalness. Similarly, Yang et al. (2019) applied a pre-trained BERT model to achieve enhanced front-end accuracy. Kenter et al. (2020) demonstrated that integrating a BERT model, pre-trained on extensive unlabeled data and fine-tuned for speech, into an RNN-based TTS system enhances prosody. Kenter et al. (2020) specifically suggest updating the BERT\u2019s parameters during the training of their RNN-based speech synthesis model, emphasizing the critical role of fine-tuning the BERT component for optimal outcomes. As prompt tuning draws wide attention in guiding text or image generation, PromptTTS (Guo et al., 2022) takes a prompt representation with both style and content descriptions from a BERT model as input to generate speech with precise style control and high speech quality.\n\nIn particular, Mukherjee et al. (2022) utilized a pre-trained BERT model to develop a text emotion classification model, employing the final hidden states of the initial [CLS] token as a comprehensive representation of the text. Researchers such as Kenter et al. (2020); Li et al. (2021); Abbas et al. (2022) have applied word-level BERT to capture the semantic and syntactic structure of sentences, thereby aiding TTS synthesis. Li et al. (2023) introduced a phoneme-level BERT, designed with a preliminary task of predicting corresponding graphemes in addition to regular masked phoneme predictions, to enhance the naturalness of speech synthesized from out-of-distribution (OOD) texts.\n\nHowever, despite BERT\u2019s acknowledged capacity to provide detailed word importance, syntactic and semantic insights, and general knowledge (Hayashi et al., 2019; Kenter et al., 2020), its effectiveness is constrained by the particularities of fine-tuning approaches. Furthermore, BERT\u2019s inherent non-generative nature might limit its ability to account for information outside the immediate sentence context."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "2.3.   Integrating GPT-like LMs for TTS",
            "text": "Considering semantic understanding and expression capabilities, BERT is primarily utilized for comprehension tasks. In comparison, GPT excels not only in understanding text but also in generating natural and coherent text. Moreover, with the larger model parameters, GPT is particularly adept at zero-shot or few-shot learning, enabling its direct application to various tasks with little to no need for fine-tuning or structural modifications.\n\nHowever, research on leveraging GPT-like models to aid TTS systems is very limited. Stephenson et al. explore the potential of improving speech synthesis naturalness by text input lookahead with GPT prediction. Such an approach potentially restricts TTS applications, as altering the input is often undesirable. Furthermore, the findings were not verified by human subjective evaluation.\n\nSaito et al. suggest employing ChatGPT to aid in empathetic dialogue speech synthesis by extracting the context of conversations. They particularly instruct ChatGPT to produce three keywords that encapsulate the intention, emotion, and speaking style of speech observed in the dialogue history. These keywords are subsequently utilized to train a speech synthesis model. However, due to the inaccessibility of ChatGPT to the public, the researchers resort to processing ChatGPT\u2019s outputs with BERT to extract embeddings. This approach essentially positions ChatGPT as an alternative to manual annotation, yet it does not delve into investigating ChatGPT\u2019s internal representations and their potential impact on speech-related tasks.\n\nIn our study, we selected Llama2, a GPT-like LM, for integration into our TTS system, motivated by its technological advancements and potential for diverse applications. Llama2 stands out as one of the largest publicly accessible LMs, rivaling proprietary models such as GPT3.5 and PaLM (540B), and surpasses other open-source alternatives like MPT and Falcon in benchmark evaluations. Additionally, the novel architecture of Llama2 not only ensures enhanced security but also facilitates the extension of various downstream tasks.\n\nRelated research that employs Llama2 in speech and other multimodal tasks, coupled with the ongoing efforts to reduce computing costs associated with Llama2, underscores the model\u2019s significant research interest and its promising prospects in multimodal applications."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Methodology",
            "text": "We propose leveraging semantic embeddings derived from a GPT-like LM to improve TTS synthesis. In our work, Llama2 is employed as the GPT-like model, as elaborated in Section \u00a72.3, and VITS is utilized as the TTS model for generating audio from phoneme embeddings, as detailed in Section \u00a72.1. In essence, we extract semantic embeddings from the final hidden layer of Llama2 and integrate them with the original acoustic text embeddings of VITS, forming enhanced text embeddings for speech synthesis. Specifically, either a global token or a sequence of tokens is used to encapsulate the semantic attributes of an input sentence for varying objectives. The distinctions between these two token types are further explicated in Section \u00a73.1.\n\nWe explored five types of global tokens to represent the overarching semantic features of an input sentence, namely [AVE], [PCA], [LAST], [EIS_Word], and [EIS_Sentence], with each strategy employing a single token.\n\nIn the [AVE] strategy, the semantic token is derived by calculating the average of all tokens\u2019 output vectors for a sentence. Here, the semantic token obtained using the [AVE] strategy represents the output of the th token of sentence at the final hidden layer of Llama2, with comprising tokens.\n\nFor the [PCA] strategy, Principal Component Analysis is applied to the output vectors of a sentence to extract principal components and rescale the mean of the PCA results according to the original data\u2019s value range. This rescaling ensures that the PCA-processed data maintains a scale consistent with the original data, preserving the relative importance of semantic information numerically.\n\nIn the [LAST] strategy, the semantic token is obtained by selecting the last token from the output vector of a sentence, which represents the last token's representation after processing through all layers of Llama2 at the final layer.\n\nIn the [EIS_Word] and [EIS_Sentence] strategies, unlike the above approaches that utilize the sentence itself for representation, we derive the semantic representation of a sentence based on Llama2\u2019s comprehension. Adapted from Saito et al. (2023), we employ prompts to obtain Llama2\u2019s understanding of a sentence in terms of Emotion, Intention, and speaking Style. In the [EIS_Word] strategy, Llama2 is prompted to describe Emotion, Intention, and speaking Style with three separate words. In the [EIS_Sentence] strategy, Llama2 is guided to describe its understanding of the input sentence\u2019s Emotion, Intention, and speaking Style with an easy-to-understand sentence.\n\nIn the implementation of sequential tokens strategies, we focus on utilizing the complete set of tokens from the input sentence to represent its semantic information. Unlike the global token approaches, sequential tokens strategies encompass representations based on either text or phonemes, aiming to better align with the TTS model\u2019s potential emphasis on acoustic features.\n\nUnder the [TEX] strategy, we directly employ all tokens from the textual form of a sentence to represent its semantic information. If the output of a sentence at the final hidden layer of Llama2 consists of tokens, then the semantic token is represented as a sequence.\n\nIn the [PHO] strategy, the complete set of tokens from the phonemic form is considered. Here, denotes the phonemic representation of a sentence. If the output at the final hidden layer of Llama2 comprises tokens, then the semantic token is represented as a sequence.\n\nIn both strategies, the outputs respectively represent the th token of a sentence in its textual and phonemic forms at the final hidden layer of Llama2. This representation allows the TTS model to leverage the complete semantic information of a sentence, whether based on text or phonemes.\n\nTo obtain an embedding that integrates both semantic and acoustic information, for global tokens, we simply add the dimensionally unified global embedding to VITS\u2019s acoustic embedding.\n\nWe utilize the Scaled Dot-Product Attention mechanism to merge sequential embeddings with VITS\u2019s original acoustic embedding to gain enhanced embedding, which can be described by the following mathematical formulas:\n\nFirst, calculate the attention scores, where is the acoustic embedding in VITS with dimensions; and denotes the semantic embedding from Llama2, also with dimensions; is the batch size, is the sequence length, and is the embedding dimension; is the temperature for scaling. The transpose of transforms from for matrix multiplication.\n\nNext, apply the softmax function and dropout to the attention scores, obtaining the final attention weights. \n\nFinally, the output is calculated by weighting with the attention weights. The output, viewed as a text embedding fused with semantic information, has dimensions that match those of."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Semantic Embeddings Derived from Llama2",
            "text": "For each input sentence, we extract information from the final hidden layer before the output of Llama2. Different strategies are employed to create various tokens that serve as the semantic embedding for the sentence.\n\nWe explored five types of global tokens to represent the overarching semantic features of an input sentence, namely [AVE], [PCA], [LAST], [EIS_Word], and [EIS_Sentence], with each strategy employing a single token.\n\nIn the [AVE] strategy, the semantic token is derived by calculating the average of all tokens\u2019 output vectors for a sentence.\n\nFor the [PCA] strategy, we apply Principal Component Analysis to the output vectors of a sentence to extract principal components and rescale the mean of the PCA results according to the original data\u2019s value range, preserving the relative importance of semantic information numerically.\n\nIn the [LAST] strategy, the semantic token is obtained by selecting the last token from the output vector of a sentence.\n\nIn the [EIS_Word] and [EIS_Sentence] strategies, we derive the semantic representation of a sentence based on Llama2\u2019s comprehension. Adapted from existing practices, we employ prompts to obtain Llama2\u2019s understanding of a sentence in terms of Emotion, Intention, and speaking Style, and calculate the average of this understanding\u2019s representation to serve as the semantic embedding.\n\nIn the [EIS_Word] strategy, Llama2 is prompted to describe Emotion, Intention, and speaking Style with three separate words, resulting in a semantic token for the final representation.\n\nIn the [EIS_Sentence] strategy, Llama2 describes its understanding of the input sentence\u2019s Emotion, Intention, and speaking Style with an easy-to-understand sentence, which serves as the semantic token for the final representation.\n\nIn the implementation of sequential tokens strategies, we focus on utilizing the complete set of tokens from the input sentence to represent its semantic information. Unlike the global token approaches, sequential tokens strategies encompass representations based on either text or phonemes, aiming to better align with the TTS model\u2019s potential emphasis on acoustic features.\n\nUnder the [TEX] strategy, we directly employ all tokens from the textual form of a sentence to represent its semantic information.\n\nIn the [PHO] strategy, we consider the complete set of tokens from the phonemic form.\n\nIn both strategies, textual and phonemic representations allow the TTS model to leverage the complete semantic information of a sentence."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Fusing Semantic Embedding with Acoustic Embedding",
            "text": "To align the dimensions of semantic embedding extracted from Llama2, denoted as , with the acoustic embeddings from VITS, denoted as , we employ a linear projection. The original dimension of , , is projected to match the dimension of VITS acoustic embedding, , using a linear transformation matrix  of dimensions . The projected semantic embedding, , is calculated as follows:\n\nTo obtain an embedding  that integrates both semantic and acoustic information, for global tokens, we simply add the dimensionally unified global embedding to VITS\u2019s acoustic embedding, as shown in the equation:\n\nWe utilize the Scaled Dot-Product Attention mechanism to merge sequential embeddings with VITS\u2019s original acoustic embedding to gain enhanced embedding , which can be described by the following mathematical formulas:\n\nFirst, calculate the attention scores :\nwhere  is the acoustic embedding  in VITS with dimensions ;  and  denotes the semantic embedding  from Llama2, also with dimensions ;  is the batch size,  is the sequence length, and  is the embedding dimension;  is temperature for scaling.  denotes the transpose of , transforming  from  to  for matrix multiplication. The resulting  has dimensions .\n\nIf a source mask or target mask is present, a masking operation is applied, setting the attention scores at masked positions to a very low value (e.g., ) to nearly eliminate their weight contribution in the subsequent softmax step.\n\nNext, apply the softmax function and dropout to the attention scores, obtaining the final attention weights :\n\nFinally, the output  is calculated by weighting  with the attention weights:\nThe output , viewed as text embedding fused with semantic information, has dimensions  that match those of ."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Experiments",
            "text": "In our proposed Llama-VITS, global strategy [LAST] only uses the last token in the final hidden layer of Llama2 for each sentence. [AVE] uses the average of all tokens for each sentence. [PCA] uses the concatenation of all tokens whose dimension was reduced by Principal Component Analysis (PCA). [EIS_Word] and [EIS_Sentence] use the average of tokens for an answer, which is formed in three words or a sentence by prompts shown in Figure 2, to describe the Emotion, Intention, and speaking Style of the transcript.\n\nIn BERT-VITS baseline, global strategy [CLS] only uses the first token from the BERT-base-uncased model for each input sentence.\n\nIn our proposed Llama-VITS, sequential strategy [TEX] concatenates the sequence of tokens in a sentence generated by Llama2 using text input. [PHO] concatenates the sequence of tokens of a sentence generated by Llama2 using phonemic input.\n\nIn the baseline BERT-VITS, sequential strategy [BERT_TEX] concatenates all the tokens in a sentence extracted from BERT-base-uncased model. [BERT_PHO] concatenates all the tokens in a sentence extracted from BERT-x-phone-base model.\n\nWe utilized full LJSpeech, 1-hour LJSpeech, and EmoV_DB_bea_sem dataset for experimental verification. LJSpeech comprises 24 hours recorded of English speech by a single female speaker, where we evaluate how the embeddings extracted from Llama2 can help improve the speech naturalness. Besides full LJSpeech dataset, we also randomly filtered 1-hour LJSpeech which contains only 1-hour records as an ablation study to show how dataset size influences. EmoV_DB is a database of emotional speech that contains data for male and female actors in English and French. EmoV_DB covers 5 emotion classes, amused, angry, disgusted, neutral, and sleepy. To factor out the effect of different speakers, we filtered the original EmoV_DB dataset into the speech of a specific female English speaker, bea. Then we use Llama2 to predict the emotion label of the transcript chosen from the above 5 emotion classes, and select the audio samples which have the same predicted emotion. The filtered dataset contains 22.8-min records for training. We named the filtered dataset EmoV_DB_bea_sem and investigated how the semantic embeddings from Llama2 behave in naturalness and expressiveness on it. Please refer to Appendix A for more dataset statistics.\n\nOur Llama-VITS system was built on the VITS framework using its original implementation, augmented with semantic embeddings derived from Llama2 using its original implementation. For training LJSpeech, we use the public configs in the original implementation of VITS. For EmoV_DB_bea_sem, we use the same config as LJSpeech but changed batch size from 64 to 16 since this dataset is much smaller. Besides implementing our proposed Llama-VITS, we extracted corresponding semantic tokens [CLS], [BERT_TEX] from BERT uncased base model and [BERT_PHO] from BERT pre-trained on phoneme for comparison.\n\nIn comparing the experimental results, we choose 100k-step results on both full LJSpeech and 1-hour LJSpeech datasets since they are rather large. On EmoV_DB_bea_sem, we used the pre-trained checkpoint of LJSpeech on 100k-step and compared the fine-tuning results on EmoV_DB_bea_sem at 150k-step since it is rather small.\n\nBoth subjective and objective metrics are implemented for a comprehensive evaluation.\n\nIn subjective evaluation, we conduct Emotion Similarity Mean Opinion Score (ESMOS) experiments to evaluate emotion similarity for EmoV_DB_bea_sem. In the subjective evaluation, we compared [AVE], [TEX] and [PHO] strategies in our Llama-VITS with the corresponding token [CLS], [BERT_TEX] and [BERT_PHO] extracted from different BERT models and the baseline ORI-VITS who does not contain semantic tokens, with the ground truth samples GT.\n\nIn evaluating ESMOS, we randomly chose 5 samples from the total 51 test samples proportionally divided by us and received 100 test results from different speakers on Amazon Mechanical Turk. The result significance level is thus 500. Each participant is asked to give a score on emotion similarity compared with ground truth in a 5-scale: Excellent Match 5, Good Match 4, Fair Match 3, Poor Match 2, Bad Match 1. Note that in the ESMOS experiments, participants are asked to ignore the speakers\u2019 voice, style, and audio quality and only consider the emotiveness of the speech.\n\nIn objective evaluation, we utilize Mel-Cepstral Distortion (MCD), and speech recognition performance"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Experimental Settings",
            "text": "We propose Llama-VITS which uses semantic tokens derived from Llama2 to enhance acoustic embedding in VITS for better TTS performance. To show the effectiveness of our method, we experimented with two baseline models.\n\nIn the ORI-VITS baseline, we use the original VITS without external semantic information. In the BERT-VITS baseline, we extract various semantic tokens according to former research. Specifically, we use the [CLS] token of BERT as the global token. To form the baseline of the sequential token in BERT, we use all the tokens in the sentence trained by text or phoneme, named [BERT_TEX] and [BERT_PHO], respectively.\n\nIn our proposed Llama-VITS, we derive global token [AVE], [LAST], [PCA], [EIS_Word], and [EIS_Sentence], and sequential tokens [TEX] and [PHO] from Llama2, corresponding to those in BERT-VITS. We use Llama2 (13b) to generate semantic embeddings of dimension 5120. [CLS] and [BERT_TEX] tokens are extracted from BERT-base-uncased model which has a parameter size of 110M that generates token embedding of 768 dimensions. [BERT_PHO] token is extracted from BERT-x-phone-base model whose parameter size is 88M to generate token embedding of 768 dimensions.\n\nIn our proposed Llama-VITS, global strategy [LAST] only uses the last token in the final hidden layer of Llama2 for each sentence. [AVE] uses the average of all tokens for each sentence. [PCA] uses the concatenation of all tokens whose dimension was reduced by Principal Component Analysis (PCA). [EIS_Word] and [EIS_Sentence] use the average of tokens for an answer, which is formed in three words or a sentence by prompts to describe the Emotion, Intention, and speaking Style of the transcript.\n\nIn BERT-VITS baseline, global strategy [CLS] only uses the first token from the BERT-base-uncased model for each input sentence. In our proposed Llama-VITS, sequential strategy [TEX] concatenates the sequence of tokens in a sentence generated by Llama2 using text input. [PHO] concatenates the sequence of tokens of a sentence generated by Llama2 using phonemic input. In the baseline BERT-VITS, sequential strategy [BERT_TEX] concatenates all the tokens in a sentence extracted from BERT-base-uncased model. [BERT_PHO] concatenates all the tokens in a sentence extracted from BERT-x-phone-base model.\n\nWe utilized full LJSpeech, 1-hour LJSpeech, and EmoV_DB_bea_sem dataset for experimental verification. LJSpeech comprises 24 hours recorded of English speech by single female speaker, where we evaluate how the embeddings extracted from Llama2 can help improve the speech naturalness. Besides full LJSpeech dataset, we also randomly filtered 1-hour LJSpeech which contains only 1-hour records as an ablation study to show how dataset size influences. EmoV_DB is a database of emotional speech that contains data for male and female actors in English and French. EmoV_DB covers 5 emotion classes, amused, angry, disgusted, neutral, and sleepy. To factor out the effect of different speakers, we filtered the original EmoV_DB dataset into the speech of a specific female English speaker, bea. Then we use Llama2 to predict the emotion label of the transcript chosen from the above 5 emotion classes, and select the audio samples which has the same predicted emotion. The filtered dataset contains 22.8-min records for training. We named the filtered dataset EmoV_DB_bea_sem and investigated how the semantic embeddings from Llama2 behave in naturalness and expressiveness on it.\n\nOur Llama-VITS system was built on the VITS framework using its original implementation, augmented with semantic embeddings derived from Llama2 using its original implementation. For training LJSpeech, we use the public configs in the original implementation of VITS. For EmoV_DB_bea_sem, we use the same config as LJSpeech but changed batch size from 64 to 16 since this dataset is much smaller. Besides implementing our proposed Llama-VITS, we extracted corresponding semantic tokens [CLS], [BERT_TEX] from BERT uncased base model and [BERT_PHO] from BERT pre-trained on phoneme for comparison.\n\nIn comparing the experimental results, we choose 100k-step results on both full LJSpeech and 1-hour LJSpeech datasets since they are rather large. On EmoV_DB_bea_sem, we used the pre-trained checkpoint of LJSpeech on 100k-step and compare the fine-tuning results on EmoV_DB_bea"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experimental Results",
            "text": "We evaluated our proposed Llama-VITS along with baselines ORI-VITS and BERT-VITS models on three distinct datasets: the full LJSpeech, the 1-hour LJSpeech, and EmoV_DB_bea_sem. The experimental outcomes provide a comprehensive understanding of the model performance and the impact of semantic tokens selection. A summary of these results is articulated below and can be referenced in Table 1."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Results on full LJSpeech",
            "text": "Enhancements were observed with the BERT-VITS baseline. Specifically, BERT-VITS with [BERT_TEX] semantic tokens demonstrated superior performance in MCD (), indicating improved speech quality and reduced mel-cepstral distortion. Additionally, a reduced CER of  and WER of  were noted, highlighting enhanced automatic speech recognition accuracy. Our proposed Llama-VITS, integrating various global and sequential semantic tokens, displayed competitive performance. The [PCA] strategy stood out, achieving an MCD of , indicating optimal mel-cepstral distortion. The [EIS_Sentence], [AVE], and [LAST] tokens yielded a top-tier performance, underscoring their effectiveness in enhancing perceived speech quality."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Results on 1-hour LJSpeech",
            "text": "In the more challenging 1-hour LJSpeech dataset, all models experienced a slight performance decrease, an expected outcome given the reduced training data size. BERT-VITS baseline with [CLS] tokens exhibited notable MCD performance (), while the [BERT_PHO] excelled in (), reflecting enhanced speech naturalness and reduced mel-cepstral distortion. Llama-VITS with [AVE] tokens achieved the highest (), while [EIS_Sentence] tokens resulted in the most favorable MCD (), illustrating the model\u2019s versatility and efficacy in different token configurations."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Results on EmoV_DB_bea_sem",
            "text": "On this even more challenging dataset, a small improvement observed in BERT-VITS only exists in the [BERT_TEX] with a CER of . While our proposed Llama-VITS displayed notable enhancements. The [TEX] strategy achieves an ESMOS of , indicating much more emotiveness. The [LAST] yielded the best performance on CER of  and WER of , other strategies also perform better than or comparable to BERT-VITS, underscoring its effectiveness in enhancing perceived speech expressiveness."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   Analysis",
            "text": "Speaking of the strengths of different tokens, BERT-based tokens generally contribute to improving MCD and ASR scores, indicating the enriched semantic understanding translated to speech quality.\n\nTokens of Llama-VITS exhibited a balanced performance across all metrics, with specific token configurations excelling in particular aspects. For instance, [PCA] token emerged as a strong contender in reducing MCD, and [TEX] had superior performance to improve ESMOS score.\n\nIn individual comparisons, in the ESMOS metric for emotional expression, Llama-VITS\u2019s two sequential tokens generally surpassed BERT-VITS, particularly the [TEX] token. Therefore, we can infer that GPT-like LMs may have greater potential for TTS tasks than BERT-like models.\n\nFurther, our results reflect different patterns of gains from GPT-like and BERT-like models in TTS tasks. In the ESMOS emotion metric, Llama-VITS\u2019s sequential token [TEX] significantly outperformed other tokens, while for BERT-VITS, global tokens performed better.\n\nOverall, Llama-VITS showed a different pattern compared to BERT-VITS, and superior performance in ESMOS. These results highlight the potential for further exploration of semantic token types and fusion methods to achieve more significant enhancements in speech synthesis, particularly in scenarios constrained by limited and complex training data."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Discussions",
            "text": "In this section, we discuss factors influencing current outcomes. Based on this discussion, we also point out the directions for future work in Appendix 13  ###reference_###."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "6.1.   GPT-like vs BERT-like",
            "text": "Initial observations from our experiments indicate that, even without any fine-tuning of Llama2, Llama-VITS significantly outperforms both BERT-VITS and ORI-VITS in terms of emotional expressiveness. This finding opens up avenues for future research into emotive TTS tasks.\nFurthermore, a comparison between BERT-VITS and Llama-VITS highlights their distinct performance traits. BERT-VITS, leveraging deep contextual embeddings, provides profound semantic insights yet encounters challenges in customization and adaptability across a range of TTS tasks. Conversely, Llama-VITS can provide a more versatile and adaptable approach, with its array of token types demonstrating particular advantages across various evaluation metrics."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "6.2.   Semantic Token Strategy",
            "text": "The varying effectiveness of distinct semantic tokens underscores the importance of careful selection and integration tailored to the particular goals of TTS systems. Optimizing the type of token and method of fusion can be instrumental in enhancing aspects such as speech naturalness, emotional expressiveness, Mel Cepstral Distortion (MCD), or Automatic Speech Recognition (ASR) performance."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Conclusion",
            "text": "In summary, this study exemplifies a significant stride towards optimized TTS synthesis by integrating semantic tokens, leveraging the strengths of Llama-VITS. Our findings, validated by comprehensive experiments on the LJSpeech and EmoV_DB_bea_sem datasets, underscore the pivotal role of semantic embeddings in enhancing speech quality, naturalness, and emotiveness. The adaptability and efficacy of Llama-VITS, especially, open new vistas for customized and context-sensitive TTS applications."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Limitations",
            "text": "Compared with our baseline which uses different BERT models, we only tested our method using Llama2. As Kenter et al. (2020  ###reference_b23###) indicate for their BERT-based TTS model, small BERT models work better than big ones, but the parameter size of our proposed GPT-based TTS influence is yet studied by our research. Although BERT-based TTS models are normally finetuned on speech tasks to provide more explicit acoustic information for TTS, we didn\u2019t try designing prompts to generate acoustic features and only studied how general semantic information can help. Our experiments were conducted only on clean datasets with limited size, and the effect on more complex datasets is to be further explored. The integration of Llama2\u2019s embeddings introduces additional computational costs, potentially limiting real-time applications."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "9.   Acknowledgements",
            "text": "This research was conducted with the support of team members who contributed to varying extents. Particular gratitude is extended to Koichi Miyazaki for his sharing regarding foundational knowledge, his assistance in implementing the subjective evaluation and precious advice. We are also deeply appreciative of Masato Murata, Katsuhiko Yamamoto, and Li Li for their insightful suggestions to enrich the presentation of our paper and code."
        }
    ],
    "url": "http://arxiv.org/html/2404.06714v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.06714v3",
        "paper_title": "Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness",
        "research_background": "**Motivation:**\nThe paper is motivated by the limitations observed in traditional Text-to-Speech (TTS) models that focus primarily on acoustic features and therefore often fail to comprehend and express the semantic and emotional nuances embedded in the text. With the emergence of advanced Natural Language Processing (NLP) technologies, particularly language models like BERT and GPT, researchers have attempted to improve TTS expressiveness. However, these models face limitations in parameter size and need specific fine-tuning tasks. Large-scale Language Models (LLMs), such as Llama2, offer high text generation capabilities and zero-shot learning with minimal parameter adjustment but haven't been fully explored for TTS tasks. The paper seeks to leverage these advancements to develop a more semantically aware TTS model.\n\n**Research Problem:**\nThe central research problem addressed by the paper is how to enhance TTS synthesis by integrating semantic and emotional understanding from advanced language models, specifically using the capabilities of Llama2, to overcome the limitations of traditional and BERT-based TTS models. The paper aims to create a TTS model that not only retains acoustic quality but also effectively incorporates semantic and emotional information from the text.\n\n**Relevant Prior Work:**\n1. **Traditional TTS Models**: These models, mentioned as falling short in semantic and emotional comprehension, serve as a baseline understanding of current limitations in TTS technology.\n2. **BERT-based TTS Models**: Previous work (Mukherjee et al., 2022; Abbas et al., 2022; Li et al., 2023; Guo et al., 2022) explored BERT-based approaches to improve synthesis expressiveness but faced challenges in parameter size and the need for specialized fine-tuning tasks.\n3. **Advancements in NLP with LLMs**: The paper references the substantial progress achieved with LLMs like Llama2 (Touvron et al., 2023) in text generation and their potential for zero-shot learning and minimal parameter adjustment through prompt tuning (Liu et al., 2022; Tu et al., 2022).\n\nBy addressing the outlined research problem through the development of the Llama-VITS model, the paper aims to contribute a solution that leverages the semantic understanding and expressive capabilities of Llama2 while ensuring superior acoustic performance.",
        "methodology": "### Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness\n\n#### Methodology\n\n**Overview:**\nThe proposed method, termed Llama-VITS, aims to enhance Text-to-Speech (TTS) synthesis by integrating semantic embeddings derived from a GPT-like language model (LM) to enrich the acoustic text embeddings used by the TTS model. Specifically, Llama2, a GPT-like LM, and VITS, a TTS model, are employed.\n\n**Key Components:**\n1. **Semantic Embeddings:**\n   - **Llama2**: Utilized to extract semantic embeddings from its final hidden layer.\n   - **VITS**: Generates audio from phoneme embeddings.\n\n2. **Enhanced Text Embeddings:**\n   - The semantic embeddings from Llama2 are combined with VITS\u2019s acoustic text embeddings to form enhanced text embeddings for improved speech synthesis.\n\n**Strategies for Extracting Global Semantic Tokens:**\n1. **[AVE] Strategy**:\n   - Calculates the average of all token output vectors for a sentence.\n   - Formula: \\(\\boldsymbol{s}_{\\text{AVE}} = \\frac{1}{L} \\sum_{i=1}^{L} o_{i}^{H}\\)\n\n2. **[PCA] Strategy**:\n   - Applies Principal Component Analysis to extract principal components, then rescales them to preserve semantic information.\n   - Formula: \\(\\boldsymbol{s}_{\\text{PCA}} = \\text{PCA}(\\boldsymbol{o}_{1}^{H}, \\boldsymbol{o}_{2}^{H}, \\ldots, \\boldsymbol{o}_{L}^{H})\\)\n\n3. **[LAST] Strategy**:\n   - Selects the output vector of the last token of the sentence.\n   - Formula: \\(\\boldsymbol{s}_{\\text{LAST}} = o_{L}^{H}\\)\n\n4. **[EIS_Word] Strategy**:\n   - Uses Llama2's understanding of the sentence\u2019s Emotion, Intention, and speaking Style, described with three separate words.\n   - Formula: \\(\\boldsymbol{s}_{\\text{EIS\\_Word}} = \\frac{1}{L_{\\text{EI}}} \\sum_{i=1}^{L_{\\text{EI}}} (o_{\\text{E}}^{H}, o_{\\text{I}}^{H}, o_{\\text{S}}^{H})\\)\n\n5. **[EIS_Sentence] Strategy**:\n   - Uses Llama2\u2019s understanding described as an easy-to-understand sentence.\n   - Formula: \\(\\boldsymbol{s}_{\\text{EIS\\_Sentence}} = \\frac{1}{L_{\\text{EIS}}} \\sum_{i=1}^{L_{\\text{EIS}}} o_{\\text{EIS}}^{H}\\)\n\n**Sequential Token Strategies:**\n1. **[TEX] Strategy**:\n   - Employs all tokens from the textual form of the sentence.\n   - Formula: \\(\\boldsymbol{s}_{\\text{TEX}} = (o_{1}^{H}, o_{2}^{H}, \\ldots, o_{L}^{H})\\)\n\n2. **[PHO] Strategy**:\n   - Uses the complete set of tokens from the phonemic form of the sentence.\n   - Formula: \\(\\boldsymbol{s}_{\\text{PHO}} = (o_{\\text{ph}_1}^{H}, o_{\\text{ph}_2}^{H}, \\ldots, o_{\\text{ph}_L}^{H})\\)\n\n**Integration with VITS:**\n- **Global Tokens**:\n  - The global embedding is dimensionally unified and added to VITS\u2019s acoustic embedding.\n  - Formula: \\(\\boldsymbol{e}_{\\text{enhanced}} = \\boldsymbol{e}_{\\text{acoustic}} + \\boldsymbol{s}_{\\text{global}}\\)\n\n- **Sequential Tokens**:\n  - Utilizes Scaled Dot-Product Attention to merge sequential embeddings with VITS\u2019s original acoustic embedding.\n  - Attention score computation: \\( \\text{Attention} = \\frac{QK^T}{\\sqrt{d_k}} \\), where \\( Q = \\boldsymbol{e}_{\\text{acoustic}}, K = V = \\boldsymbol{s}_{\\text{sequential}} \\)\n  - Attention weights: \\( \\text{weights} = \\text{softmax}(\\text{Attention}) \\)\n  - Enhanced embedding output: \\( \\boldsymbol{e}_{\\text{enhanced}} = \\text{weights} \\cdot V \\)\n\nBy combining semantic and acoustic information, Llama-VITS aims to generate more natural and semantically aware speech synthesis.",
        "main_experiment_and_results": "#### Experiment Setup:\n1. **Models and Strategies**:\n   - **Llama-VITS** (Proposed Model): Utilizes various token strategies from Llama2.\n     - **Global Strategies**:\n       - **[LAST]**: Uses the last token from the final hidden layer of Llama2 for each sentence.\n       - **[AVE]**: Uses the average of all tokens for each sentence.\n       - **[PCA]**: Uses concatenation of all tokens with dimension reduction by Principal Component Analysis (PCA).\n       - **[EIS_Word]** & **[EIS_Sentence]**: Use the average of tokens to describe the Emotion, Intention, and speaking Style of the transcript.\n     - **Sequential Strategies**:\n       - **[TEX]**: Concatenates the sequence of tokens in a sentence generated by Llama2 using text input.\n       - **[PHO]**: Concatenates the tokens of a sentence generated by Llama2 using phonemic input.\n   - **BERT-VITS** (Baseline Model): Uses token strategies from BERT.\n     - **Global Strategy**:\n       - **[CLS]**: Uses the first token from the BERT-base-uncased model for each sentence.\n     - **Sequential Strategies**:\n       - **[BERT_TEX]**: Concatenates all tokens in a sentence from BERT-base-uncased model.\n       - **[BERT_PHO]**: Concatenates all tokens in a sentence from BERT-x-phone-base model.\n\n2. **Datasets**:\n   - **EmoV_DB_bea_sem**: Filtered subset of EmoV_DB, containing 22.8 minutes of emotional speech from a specific female English speaker.\n\n3. **Training Configuration**:\n   - **LJSpeech**: Uses public configs from the original VITS implementation.\n   - **EmoV_DB_bea_sem**: Uses the same config as LJSpeech but with a reduced batch size (from 64 to 16).\n\n4. **Evaluation Metrics**:\n   - **Subjective Metrics**:\n     - **Emotion Similarity Mean Opinion Score (ESMOS)**: Evaluates emotion similarity using Amazon Mechanical Turk ratings on a 5-point scale.\n   - **Objective Metrics**:\n     - **UTokyo-SaruLab Mean Opinion Score (UTMOS)**: Predictive MOS using speech samples.\n     - **Mel-Cepstral Distortion (MCD)**: Measures spectral distance.\n     - **Speech Recognition Performance**:\n       - **Character Error Rate (CER)**\n       - **Word Error Rate (WER)**\n\n#### Main Experimental Results:\n1. **Subjective Evaluation (ESMOS)**:\n   - **Llama-VITS** performed better in emotion similarity compared to the baseline BERT-VITS and ORI-VITS (without semantic tokens).\n   - Greater emotiveness was perceived especially using average ([AVE]), text ([TEX]), and phonemic ([PHO]) strategies.\n\n2. **Objective Evaluation**:\n   - **UTMOS**: Demonstrated the effective enhancement of speech naturalness with Llama-VITS.\n   - **MCD**: Showed a significant decrease indicating improved spectral quality.\n   - **CER and WER**: Improved intelligibility and recognition performance with Llama-VITS compared to BERT-VITS and ORI-VITS.\n\nSummarizing, the proposed Llama-VITS with various token strategies, especially average ([AVE]), text ([TEX]), and phonemic ([PHO]), outperformed the baseline methods in both subjective and objective measures of speech naturalness, intelligibility, and emotion similarity."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to evaluate the effectiveness of Llama-VITS in enhancing TTS performance by comparing it with baseline models ORI-VITS and BERT-VITS on various datasets.",
            "experiment_process": "In this experiment, Llama-VITS utilizes semantic embeddings from Llama2, while ORI-VITS uses no external semantic information and BERT-VITS incorporates BERT-based semantic tokens. Several strategies for deriving semantic tokens were evaluated: global strategies included [LAST], [AVE], [PCA], [EIS_Word], and [EIS_Sentence], and sequential strategies [TEX] and [PHO]. The datasets used were full LJSpeech (24 hours of speech from a single female speaker), 1-hour LJSpeech, and EmoV_DB_bea_sem (22.8 minutes of filtered speech data for a specific female English speaker). Multiple metrics were used for evaluation including UTMOS, MCD, CER, WER for objective assessment, and ESMOS for subjective evaluation.",
            "result_discussion": "The Llama-VITS model showed competitive performance compared to the baselines. On full LJSpeech, the [PCA] strategy was notably effective in reducing MCD, while [EIS_Sentence], [AVE], and [LAST] tokens achieved high UTMOS scores. In the 1-hour LJSpeech dataset, [AVE] and [EIS_Sentence] tokens performed best in UTMOS and MCD respectively. For the EmoV_DB_bea_sem dataset, Llama-VITS with the [TEX] and [LAST] strategies showed considerable improvements in ESMOS, CER, and WER, indicating enhanced emotional expression and speech recognition performance.",
            "ablation_id": "2404.06714v3.No1"
        },
        {
            "research_objective": "To investigate the impact of different semantic token strategies on the performance of the Llama-VITS model in TTS tasks.",
            "experiment_process": "Various global and sequential token strategies derived from Llama2 were explored. Global strategies used include [LAST], [AVE], [PCA], [EIS_Word], and [EIS_Sentence], while sequential strategies investigated were [TEX] and [PHO]. These were compared against BERT-VITS using [CLS], [BERT_TEX], and [BERT_PHO] tokens. The models were trained and evaluated on three datasets: full LJSpeech, 1-hour LJSpeech, and EmoV_DB_bea_sem. Evaluations were made using UTMOS, MCD, CER, and WER for objective metrics, and ESMOS for subjective measures of emotion similarity.",
            "result_discussion": "The experiments revealed that tokens from Llama-VITS generally provided balanced performance across various metrics. Specifically, the [PCA] token demonstrated significant improvements in MCD, while the [AVE] strategy enhanced UTMOS scores. On assessing Emov_DB_bea_sem, [TEX] strategy achieved higher ESMOS scores, highlighting improved emotional expressiveness. BERT-based tokens were observed to be effective in improving MCD and ASR scores, but Llama-VITS tokens outperformed them on UTMOS and ESMOS, indicating enhanced naturalness and emotive capabilities of the speech.",
            "ablation_id": "2404.06714v3.No2"
        },
        {
            "research_objective": "Evaluate the robustness and scalability of Llama-VITS when trained on a significantly reduced dataset.",
            "experiment_process": "The comparison involved training Llama-VITS with the 1-hour version of LJSpeech, alongside ORI-VITS and BERT-VITS baselines. Various token strategies such as [AVE], [TEX], and [PHO] were implemented with Llama-VITS, while [CLS], [BERT_TEX], and [BERT_PHO] were employed with BERT-VITS. Objective evaluations were conducted using UTMOS and MCD scores to assess the performance under constrained data conditions.",
            "result_discussion": "The reduced dataset resulted in decreased performance across all models, as expected. However, Llama-VITS with [AVE] tokens achieved the highest UTMOS score, and [EIS_Sentence] tokens resulted in the best MCD score, demonstrating the model's robustness and versatility even with limited training data.",
            "ablation_id": "2404.06714v3.No3"
        }
    ]
}