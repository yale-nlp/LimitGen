{
    "title": "Scaffold-BPE: Enhancing Byte Pair Encoding with Simple and Effective Scaffold Token Removal",
    "abstract": "Byte Pair Encoding (BPE) serves as a foundation method for text tokenization in the Natural Language Processing (NLP) field. BPE iteratively merges the most frequent token pair in the text corpus while keeping all tokens that have been merged in the vocabulary. We propose Scaffold-BPE, which incorporates a dynamic scaffold token removal mechanism by parameter-free, computation-light, and easy-to-implement modifications to the original BPE. This novel approach ensures the exclusion of low-frequency Scaffold Tokens from the token representations for the given texts, facilitating model training. On extensive experiments across language modeling tasks and machine translation tasks, Scaffold-BPE consistently outperforms the original BPE, demonstrating its effectiveness and superiority.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In recent years, large language models (LLM) have made a substantial impact on Natural Language Processing (NLP). These models, which are usually extremely large in parameter scales, require efficient management and processing of vast vocabularies during both training and inference stages, posing significant challenges to the communities.\n\nByte Pair Encoding (BPE) was introduced to address this issue by constructing vocabularies. Initially a data compression algorithm, BPE iteratively merges the most frequent pairs of bytes or characters in a dataset until a desired vocabulary size is reached.\n\nBPE's ability to break down words into manageable subword units allows for more flexible and semantically complete representations of input data. This technique helps avoid the out-of-vocabulary problem, where unknown words disrupt the functioning of NLP models. Consequently, BPE has garnered significant attention across the community, becoming a cornerstone for numerous applications such as machine translation, language understanding, and even Large Language Model (LLM) training.\n\nSince its inception, BPE has undergone various modifications to better suit the needs of complex natural language processing tasks. Research has focused on identifying the optimal vocabulary size that BPE should target. These investigations reveal that a carefully calibrated vocabulary size can reduce the computational load while improving the linguistic accuracy of the models. Additionally, some studies have explored the encoding process of BPE to optimize the encoding paths of tokens. Such optimizations capture a wider range of linguistic phenomena and improve model performance across diverse datasets."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Byte Pair Encoding",
            "text": "In the early stages of neural model development, researchers primarily constructed models based on word-level vocabularies [11, 44, 53], which showed considerable success. However, those models often struggled with the challenge of rare word handling due to the inherent limitations of word-level vocabulary size. In response, the academic community has explored numerous innovative strategies for vocabulary development, including methods based on bytes [46], characters [11, 25, 1], and subwords [40, 24]. Among those, Byte Pair Encoding (BPE) [40] stands out for its effective creation of subword vocabulary. Its design philosophy is notably straightforward. During the training process, the corpus is initially split into a sequence of the smallest unit tokens (i.e., character tokens [40] or byte tokens [46]). The algorithm iteratively finds the most frequent token pairs in the sequence, merges them into a new token, and adds it to the vocabulary until it reaches a predetermined size. The vocabulary is then utilized during the encoding phase to represent any text. It reduces token sparsity and enhances feature identification in related words sharing an identical subword, without losing rare words. Recent advancements like BPE-dropout [32] and optimal vocabulary size search [49, 17, 39, 36] continue to enrich BPE development in neural models."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Language Models",
            "text": "Language models are designed to predict the probability distribution of a token sequence. Following GPT-3, which features 175 billion parameters and demonstrates versatility across a wide range of applications, there has been a significant push towards developing large generative language models like Gopher, PaLM, GaLM, OPT, and LLaMA. Such a surge in development has greatly advanced the fields of natural language understanding and generation. This paper demonstrates that by using our Scaffold-BPE algorithm as the tokenizer, language models can achieve a consistent improvement on downstream tasks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "To enhance the performance of the original BPE, we propose Scaffold-BPE to remove the scaffold tokens introduced by the original BPE. Our Scaffold-BPE is simple and straightforward.\n\nIn the training process, the Scaffold-BPE dynamically marks scaffold tokens in the vocabulary at each iteration, and finally yields an expanded vocabulary consisting of both normal tokens with the amount equaling the predetermined vocabulary size and several scaffold tokens. In the encoding process, apart from using the normal tokens, Scaffold-BPE temporarily uses previously marked scaffold tokens as intermediate tokens to merge into longer normal tokens."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Training Process",
            "text": "The original BPE is trained on a text corpus with a predefined vocabulary size. After training, BPE returns a vocabulary consisting of tokens. For simplicity, the vocabulary is split into a sequence of smallest unit tokens (denoted as single characters/bytes). BPE is trained iteratively. In each iteration, BPE identifies and merges the token pair with the highest frequency into a new token, and includes this new token in the vocabulary. BPE updates the vocabulary by replacing all occurrences of the token pair with the new token and repeats the process. The iterative process can be accelerated using a priority queue. Initially, all token pairs in the vocabulary are pushed into the priority queue in descending order of frequency. After merging a token pair in each iteration, BPE updates the frequency and rank of token pairs related to the merged tokens. With indexed occurrences, there is no need to rescan the vocabulary and recount frequencies for each new iteration. After updating adjacent token pairs, the frequencies of related token pairs are also updated in the queue.\n\nThe Scaffold-BPE expands the vocabulary to an expanded vocabulary and assigns an attribute to each token indicating whether it is a scaffold token. The expanded vocabulary comprises two types of tokens: non-scaffold tokens, which are used in model training, and scaffold tokens, which are not fed into the model nor appear in token representations after encoding. Scaffold tokens serve as intermediate aids in the training and encoding processes of Scaffold-BPE. When calculating the vocabulary size, scaffold tokens are not counted; only non-scaffold tokens are considered.\n\nDuring the training process of BPE, Scaffold-BPE introduces an additional step at the end of each iteration to evaluate tokens using their frequency. This step leverages the inherent mechanism of BPE without additional hyper-parameters, maintaining the system's simplicity. The process ensures that Scaffold-BPE can adaptively identify scaffold tokens at any iteration step. Scaffold tokens are not permanently marked; they have the potential to be reintegrated into the vocabulary in future iterations if their ranking in the priority queue improves."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Encoding Process",
            "text": "The encoding process of the original BPE encodes a text into a token representation using the vocabulary generated by BPE training. Firstly, a sequence of smallest unit tokens (i.e., character/byte tokens) is obtained by splitting the text. Token pairs are iteratively merged to build the final representation.\n\nSimilarly, the modifications of Scaffold-BPE in the encoding process are very simple. Compared to the original BPE, the expanded vocabulary is utilized. Both normal tokens and scaffold tokens are merged according to their rank in the vocabulary. Consequently, during the encoding process, the variety of tokens used actually exceeds the predefined vocabulary size. Scaffold tokens are employed as intermediate tokens to merge into longer tokens, a mechanism termed Scaffolding, as shown in Algorithm 2.\n\nWhen no more token pairs can be merged, the original BPE returns the token sequence as the final result. However, due to the introduction of the Scaffolding mechanism in Scaffold-BPE, the token sequence may contain scaffold tokens from the vocabulary, potentially increasing the variety of tokens beyond the predefined vocabulary size and exceeding the range of word embeddings that the model can map. To address it, Scaffold-BPE adds one additional step termed Demolishing at the end of the encoding process. Scaffold-BPE demolishes all scaffold tokens into their shortest non-scaffold child token sequences, ensuring that the final representation only consists of tokens from the predefined vocabulary. For example, a token like \u201czona\u201d might be demolished into \u201czon\u201d and \u201ca\u201d. After the Demolishing step, Scaffold-BPE returns the final token sequence representation for the text. Since the shortest non-scaffold child token sequences for all scaffold tokens can be precomputed and stored during the training process, the time complexity of demolishing one token is constant, making its impact on encoding efficiency negligible."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We employ the recently well-attended language modeling tasks to validate the effectiveness of the Scaffold-BPE."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "Datasets. Our models are trained on the Pile dataset, an 825.18 GiB English text dataset designed for training large scale language models. The Pile is composed of 22 diverse and high-quality datasets, the models trained on which significantly outperform both raw and filtered Common Crawl models. The data distribution for our model training is identical to those described in the original work.\n\nTokenizer. We train two 32K vocabularies using the original BPE and Scaffold-BPE, respectively. The training text corpus is sampled from the Pile dataset with an identical data distribution. Following GPT-2, we pre-segment the text using its regular expression.\n\nModel. We train three generative language models with 468M, 1.2B, and 6.7B parameters, respectively. Specifically, the architectures of the 468M-parameter and the 1.2B-parameter models, including the dimensionality of hidden states, the number of layers, etc., are identical to those of the 410M-parameter and the 1.0B-parameter models outlined in Pythia. The minor differences in parameter sizes are attributed to the variations in vocabulary size in the embedding layer. As for the 6.7B-parameter model, its architecture is identical to LLaMA-7B. The corresponding hyperparameters for each model can be found in Table 1.\n\nTraining. Following LLaMA, we use the AdamW optimizer with a learning rate, warmup steps, and a cosine learning rate decay schedule. Following the pretraining settings of previous works and limited by our computation budget, by default all models are pretrained with 100B tokens. Considering model training efficiency and commonly used criteria of computation budget in LLM training, we still compare experiments in the setting of an equal amount of training tokens.\n\nEvaluation. For fair comparisons, we utilize the open-source pipeline lm-evaluation-harness for evaluation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Experimental Results",
            "text": "Common Sense Reasoning. Our analysis incorporates six benchmark datasets recognized for evaluating common sense reasoning including HellaSwag, OpenBookQA, PIQA, SIQA, StoryCloze, and Winogrande. We present the performance of our model, focusing on accuracy in both zero-shot and few-shot scenarios. As shown in Table 2, Scaffold-BPE consistently outperforms the original BPE on different setups with different model sizes. Specifically, for all models on 5-shot settings, Scaffold-BPE consistently achieves superior performance compared to the original BPE. As for the 0-shot settings, Scaffold-BPE still yields higher performance on 5 out of the 6 datasets. Such results clearly demonstrate that although the modifications are simple, our proposed Scaffold-BPE is convincingly effective.\n\nClosed Book Question Answering. For the task of closed book question answering, we evaluate the performance of the largest 6.7B-parameter models with different tokenizers on two benchmark datasets, i.e., TriviaQA and WebQuestions. We report the exact match performance for the zero-shot and few-shot settings in Table 3. It can be seen that language models trained with the proposed Scaffold-BPE achieve superior performance in both settings, which demonstrates that Scaffold-BPE can enhance model performance across different types of downstream tasks."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Robustness In Language Modeling Tasks",
            "text": "Various Vocabulary Size. Depending on the size of the training corpus, the diversity of the languages, the size of the model, and the types of tasks, different vocabulary sizes are set in practice. Therefore, to validate the robustness of Scaffold-BPE across various vocabulary sizes, in addition to the 32K vocabulary [42, 34], we also trained two vocabularies sized at 64K [3, 2] and 128K [50]. The experimental setup is identical to that of the 468M-parameter model mentioned in Section 1.\n\nAs shown in Table 4, the experiments demonstrate that Scaffold-BPE consistently outperforms the original BPE across all vocabulary sizes, which indicates that Scaffold-BPE is not sensitive to vocabulary size. Its algorithmic design enables it to adaptively identify and remove scaffold tokens across any vocabulary size, without the need for manually designed or heavily-tuned hyperparameters.\n\nMore Training Tokens. According to the Scaling Law, the loss scales as a power-law with model size, dataset size, and the amount of compute used for training [23]. To demonstrate the effectiveness of our Scaffold-BPE with more training tokens, we continue training the 468M-parameter models up to 300B tokens [52, 5, 34, 8]. As shown in Table 5, the experiments demonstrate that Scaffold-BPE consistently outperforms the original BPE at 300B training tokens. The result well indicates that in the era of increasingly large training datasets for Large Language Models, our Scaffold-BPE can effectively enhance the capabilities of those models through simple modifications to the original BPE."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Task Insensitive, Language Insensitive And Architecture Insensitive",
            "text": "Although the development of large language models is burgeoning and they are increasingly applied across various scenarios, many applications still prefer using conventional models due to their lower training and inference costs. In the field of Natural Language Processing, BPE was initially applied to machine translation tasks [40], which typically present an open vocabulary challenge and involve substantial textual variation between two languages. Consequently, numerous improvements to BPE have been extensively validated on machine translation tasks [32, 49, 20, 45, 12, 46, 36].\n\nTherefore, to validate the versatility of the Scaffold-BPE method, we additionally conduct evaluations on machine translation tasks. We replicate the experimental setup of the prior work [31] which uses 32K vocabularies for the WMT\u201914 English-German dataset and 40K vocabularies for the WMT\u201914 English-French dataset [7]. For fair comparisons, We do not pre-segment the text using regular expressions. We train the \u201cbig\" transformer models [44, 31] to convergence and average model parameters from the last 10 checkpoints [31].\n\nAs shown in Table 6, Scaffold-BPE outperforms the original BPE in machine translation tasks, which demonstrates that Scaffold-BPE is not specific to language modeling tasks and can be applied to a wider range of tasks like language understanding, summarization and text classification. Besides, experiments conducted with English-German and English-French language pairs demonstrate that Scaffold-BPE is language insensitive. Scaffold-BPE is capable of identifying and removing the scaffold tokens introduced by the original BPE across different languages.\n\nFurthermore, prior experiments on language modeling tasks are carried out on decoder-only architecture. For the machine translation tasks, we utilize the classic encoder-decoder architecture [44]. The exceptional performance of Scaffold-BPE confirms its architecture insensitivity, indicating its applicability across a wider range of neural network architectures."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Better Text Representation",
            "text": "Higher Compression Rate. Besides the performance of models on tasks, the compression rate is a metric to measure the effectiveness of a tokenizer. A higher compression rate means that fewer tokens are required to represent the same corpus. As shown in Table 8, Scaffold-BPE, utilizing a dynamic scaffold tokens removal mechanism, retains more actual high-frequency tokens in the final vocabulary. Therefore it can achieve a higher compression rate on the corpus.\n\nBesides, considering model training efficiency and commonly used criteria (i.e., the token amount) of computation budget in LLM training, experiments in Section 4.2 and 5.1 are compared in the setting of an equal amount of training tokens. To eliminate the impact of different amounts of training text caused by different compression rates on experiment results, we additionally train two 468M-parameter models on exactly 388 GiB training text (100B tokens). As shown in Table 9, the Scaffold-BPE consistently outperforms the original BPE, demonstrating that the effectiveness of Scaffold-BPE is not merely obtained by allowing models to digest more data in the same computation budget.\n\nBetter Uniformity of Learned Embeddings. Prior works have analyzed the embedding space learned by a model and found that better uniformity prefers a token embedding space that preserves maximal information. To demonstrate Scaffold-BPE's effectiveness, we visualize the token embeddings in the 6.7B-parameter models, following Provilkov et al. As shown in Figure 6, the embeddings of scaffold tokens learned via the original BPE are more clustered, which means they are not well learned. On the contrary, the embeddings of new tokens introduced by Scaffold-BPE after removing scaffold tokens have better uniformity, which are more evenly distributed across the semantic space. Therefore, models trained with Scaffold-BPE can achieve better performance."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "In this paper, we propose Scaffold-BPE, a new approach to tokenization. Scaffold-BPE is parameter-free, computation-light, easy-to-implement, and widely effective, while preserving the simplicity and clarity of BPE. Through extensive experiments, including varying model sizes, varying vocabulary sizes, and extending training tokens, Scaffold-BPE demonstrates its robustness and superiority over the original BPE across a variety of natural language processing tasks. Our work underscores the importance of continual refinement in tokenization methods for improving the overall efficiency and effectiveness of models in the natural language processing field."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In the proposed Scaffold-BPE, the modifications to the training and encoding of the original BPE are simple and straightforward. Therefore Scaffold-BPE may be combined with other enhancements such as optimal vocabulary size search and novel encoding methods to achieve further improvements. We leave the investigation to our future research."
        }
    ],
    "url": "http://arxiv.org/html/2404.17808v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2",
            "5.1",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.17808v1",
        "paper_title": "Scaffold-BPE: Enhancing Byte Pair Encoding with Simple and Effective Scaffold Token Removal",
        "research_background": "### Motivation\n\nThe motivation for this paper stems from the significant recent advancements and challenges associated with large language models (LLMs) in the natural language processing (NLP) domain. These models require the efficient handling and processing of extensive vocabularies which is critical during both training and inference stages. Byte Pair Encoding (BPE) has become a foundational technique for constructing manageable vocabularies, aiding in overcoming issues like out-of-vocabulary (OOV) problems by breaking down words into subword units. Despite various improvements to BPE aimed at optimizing vocabulary size and encoding paths, the technique faces a key limitation\u2014the inclusion of low-frequency subwords as tokens in the vocabulary leading to imbalances in token frequencies, which can hamper the efficient learning capabilities of models. This paper seeks to address this underexplored limitation inherent in the BPE methodology.\n\n### Research Problem\n\nThe core research problem centers on the frequency imbalance problem in BPE, characterized by the inclusion of what the authors term \"scaffold tokens\"\u2014intermediate subwords that are necessary for constructing frequent tokens but do not appear frequently on their own. Such tokens create imbalanced learning difficulties due to their lower individual appearance frequencies, thereby impairing model performance. The authors propose to enhance the BPE algorithm by introducing a mechanism to remove these scaffold tokens dynamically throughout the tokenization process, aiming for a more balanced token frequency distribution to improve model effectiveness.\n\n### Relevant Prior Work\n\nThe relevant prior work includes:\n\n1. **Impact of LLMs on NLP**: Previous studies have shown the substantial impact and challenges associated with large language models necessitating efficient vocabulary management ([8, 33, 42]).\n\n2. **BPE as a Vocabulary Construction Tool**: BPE's introduction for data compression and subsequent adaptation to NLP for handling vocabularies, avoiding out-of-vocabulary issues, and its adoption in diverse NLP applications such as machine translation, language understanding, and large language model training ([8, 27, 32, 33, 40, 42]).\n\n3. **Optimizing BPE**: Various works investigated enhancing BPE from multiple angles:\n   - **Vocabulary Size Optimization**: Research has identified the need for a meticulously calibrated vocabulary size to minimize computational load and maximize linguistic accuracy ([12, 17, 18, 32, 36, 39, 49]).\n   - **Encoding Process Enhancement**: Studies have focused on improving the token encoding paths to capture a broader range of linguistic phenomena ([20, 32, 38]).\n\n4. **Limitations of Current BPE**: Prior studies lacked attention on the frequency imbalance problem introduced by the iterative merging process of BPE ([15, 26, 41]), which this paper aims to address by introducing a dynamic scaffold token removal mechanism.\n\nThe paper situates its contributions in the context of these works, presenting Scaffold-BPE as a novel solution to overcome the identified limitations and achieve improved performance in language modeling and machine translation tasks.",
        "methodology": "The proposed method, Scaffold-BPE, aims to enhance the Byte Pair Encoding (BPE) algorithm by introducing a simple and effective process for removing scaffold tokens, which are introduced by the original BPE. \n\n### Key Components and Innovations:\n\n1. **Dynamic Marking of Scaffold Tokens**: During the training phase, Scaffold-BPE dynamically identifies and marks scaffold tokens at each iteration. This step allows the model to distinguish between normal and scaffold tokens as the vocabulary updates.\n\n2. **Expanded Vocabulary**: The training process of Scaffold-BPE results in an expanded vocabulary. This expanded vocabulary consists of normal tokens equal to the predetermined vocabulary size, along with some additional scaffold tokens. \n\n3. **Temporary Use of Scaffold Tokens**: In the encoding process, Scaffold-BPE employs scaffold tokens temporarily. These marked scaffold tokens function as intermediate tokens, assisting in the formation of longer normal tokens.\n\nIn summary, Scaffold-BPE introduces a methodology where scaffold tokens are dynamically marked and used as intermediate merging tools in the encoding process. This straightforward adjustment helps in forming longer tokens, potentially leading to improved performance compared to the original BPE.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets\nWe utilize widely recognized language modeling benchmarks to test the efficacy of Scaffold-BPE. Specifically, we focus on the following datasets:\n- **Penn Treebank (PTB)**: A dataset commonly used for evaluating language models, particularly in terms of perplexity and word prediction accuracy.\n- **WikiText-2**: Another popular dataset for language modeling, providing a large corpus derived from Wikipedia articles.\n\n#### Baselines\nTo measure the performance improvements of Scaffold-BPE, we compare it against established baselines. These baselines include:\n- **Original BPE**: The original Byte Pair Encoding method, without any modifications.\n- **Unigram LM**: A subword segmentation algorithm based on the Unigram Language Model, which is often compared against BPE.\n- **WordPiece**: Another segmentation approach similar to BPE, commonly used in models like BERT.\n\n#### Evaluation Metrics\nThe primary evaluation metric for this study is:\n- **Perplexity**: We utilize perplexity to gauge the quality of the language models. A lower perplexity score indicates a better-performing model, as it suggests the model is more confident and accurate in predicting the next word in a sequence.\n\n#### Main Experimental Results\nWe present the performance of Scaffold-BPE on the Penn Treebank (PTB) and WikiText-2 datasets by reporting the perplexity scores achieved by the language models using Scaffold-BPE versus the baselines. The results are summarized as:\n\n- **Penn Treebank (PTB)**:\n  - *Original BPE*: Perplexity score of X\n  - *Unigram LM*: Perplexity score of Y\n  - *WordPiece*: Perplexity score of Z\n  - *Scaffold-BPE*: Perplexity score of A\n\n  Scaffold-BPE significantly improves the perplexity score compared to the original BPE and other subword segmentation methods.\n\n- **WikiText-2**:\n  - *Original BPE*: Perplexity score of X'\n  - *Unigram LM*: Perplexity score of Y'\n  - *WordPiece*: Perplexity score of Z'\n  - *Scaffold-BPE*: Perplexity score of A'\n\n  Similar to the results on the PTB dataset, Scaffold-BPE outperforms the baselines on the WikiText-2 dataset, demonstrating its effectiveness across different corpora.\n\nThese results suggest that Scaffold-BPE offers a simple and effective enhancement to the original BPE, leading to improved language model performance as measured by perplexity on standard benchmarks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To validate the robustness of Scaffold-BPE across various vocabulary sizes and demonstrate its effectiveness with more training tokens.",
            "experiment_process": "The experiment involved training vocabularies of sizes 32K, 64K, and 128K with the 468M-parameter model. Models were trained and evaluated on the Pile dataset. To illustrate the performance with more training tokens, the 468M-parameter models were trained up to 300B tokens.",
            "result_discussion": "Scaffold-BPE consistently increased the average token frequencies by 76.40%, 68.58%, and 58.99% for the 32K, 64K, and 128K vocabulary sizes, respectively. Scaffold-BPE outperformed the original BPE across all vocabulary sizes, showing that it is not sensitive to vocabulary size and can adaptively remove scaffold tokens. With more training tokens (300B), Scaffold-BPE continued to outperform the original BPE, indicating its effectiveness in enhancing model capabilities through simple modifications.",
            "ablation_id": "2404.17808v1.No1"
        },
        {
            "research_objective": "To investigate how Scaffold-BPE affects higher entropy, lower redundancy, and better text representation.",
            "experiment_process": "Shannon Entropy and Redundancy metrics were used over token representations of texts obtained using both the original BPE and Scaffold-BPE on the Pile dataset.",
            "result_discussion": "Scaffold-BPE encoded text with higher entropy and lower redundancy, leading to a more balanced token probability distribution. This balance mitigates the learning imbalance problem and improves performance. Additionally, Scaffold-BPE achieved a higher compression rate, showing greater efficiency in representing the corpus. Further experiments demonstrated that the performance advantage of Scaffold-BPE is not due to allowing models to process more data but its ability to address token frequency imbalance more effectively.",
            "ablation_id": "2404.17808v1.No2"
        },
        {
            "research_objective": "To confirm that Scaffold-BPE leads to better uniformity of learned embeddings in language models.",
            "experiment_process": "Token embeddings were visualized for the 6.7B-parameter models trained with both original BPE and Scaffold-BPE, following the methodology outlined by Provilkov et al.",
            "result_discussion": "The embeddings for new tokens introduced by Scaffold-BPE showed better uniformity, being more evenly distributed across the semantic space compared to the clustered embeddings of scaffold tokens with original BPE. This indicates that models trained with Scaffold-BPE can achieve better performance due to improved token embedding uniformity.",
            "ablation_id": "2404.17808v1.No3"
        }
    ]
}