{
    "title": "Language Model Cascades: Token-level uncertainty and beyond",
    "abstract": "Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. Cascading offers a simple strategy to achieve more favorable cost-quality tradeoffs: here, a small model is invoked for most \u201ceasy\u201d instances, while a few \u201chard\u201d instances are deferred to the large model. While the principles underpinning cascading are well-studied for classification tasks\u2014with deferral based on predicted class uncertainty favored theoretically and practically\u2014a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across examples. To mitigate this issue, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that na\u00efve predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "",
            "text": "Recent advances in generative language modeling have yielded a series of Transformer-based models with remarkably improved quality on complex NLP tasks. Unfortunately, such models also involve significantly increased inference costs, which has motivated a series of efforts at reducing the same. These span careful infrastructure optimization, rethinking the autoregressive decoding that underpin Transformers, modifications of the underlying model architecture, and model compression strategies. \n\nCascading is one simple strategy to achieve more favorable cost-quality tradeoffs via adaptive inference. In a two-model cascade, a small model is invoked for most \u201ceasy\u201d instances, while a few \u201chard\u201d instances are deferred to a large model. Cascades have been widely explored in the vision domain and have seen increasing adoption within NLP. Importantly, cascades can be implemented in a black-box fashion over existing models, and do not necessitate any additional training.\n\nThe key challenge in cascading is to design a deferral rule which decides whether to defer an input to the larger model. The principles underpinning optimal deferral rules are well known for the classification setting, where the standard recipe is to employ the small model\u2019s prediction confidence, as canonically measured by its softmax probability output (Chow\u2019s rule). This simple deferral rule is remarkably hard to surpass in most natural settings. However, the narrative is more complex for generative LMs. \n\nWhile one can na\u00efvely translate Chow\u2019s rule for such models based on the softmax probability of the output sequence, this suffers from a length bias issue: one tends to defer longer predictions, regardless of quality. Further, simply normalizing the probability by sequence length tends to over-correct this bias, and defer shorter predictions. Intuitively, such na\u00efve translations of Chow\u2019s rule ignore a key distinction between the classification and generative LM setting. This variability complicates summarizing the sequence of uncertainty (or confidence) values into a single deferral score. \n\nTo mitigate the length bias and capture fine-grained information from the uncertainty vector over tokens, we propose to use quantiles over the vector. Via experiments on a range of NLP benchmarks and FLAN-T5 models, we show that these quantiles can capture rich and complementary sources of uncertainty information from the uncertainty sequence vector and do better than the simple aggregation schemes like sum and average. However, we observe that there is no fixed quantile value which works across all datasets. This motivates us to learn a deferral rule based on these quantile values as features, which can combine the strengths of these different quantile scores. We show that our trained deferral rule is the most consistently performant method compared to all the natural baseline aggregation strategies. We further show that using embeddings from the smaller model and intermediate embeddings from the larger model can give further performance improvement.\n\nTo summarize, our contributions are:\nWe show that simple sequence-level LM confidence measures for deferral can yield strongly sub-optimal cost-quality tradeoffs, owing to a length bias issue.\nWe introduce token-level uncertainty in the form of distribution quantiles, and show that they can yield consistently more effective cost-quality tradeoffs, owing to their finer-grained measure of uncertainty. However, there is no fixed quantile which works well across all settings.\nWe propose a post-hoc deferral rule trained on quantile features, and show it can outperform all other strategies on a range of NLP benchmarks for FLAN-T5 models. We further demonstrate that using the large model\u2019s intermediate embeddings can significantly boost performance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "",
            "text": "In this section, we discuss the relevant background and set up the problem of LM cascades.\nLanguage models (LMs). Given a finite vocabulary \n(e.g., tokens derived from SentencePiece (Kudo & Richardson, 2018  ###reference_b40###)),\na language model (LM)\ndefines a distribution  over all possible tokens given any context .\nThis in turn defines a distribution over sequences  for any , with\n via the chain rule of probability.\nLMs based on Transformers (Vaswani et al., 2017  ###reference_b78###) have proven increasingly popular in recent years.\nSuch models are typically pre-trained on large corpora based on self-supervised objectives (Radford et al., 2018  ###reference_b56###; Devlin et al., 2019  ###reference_b17###; Raffel et al., 2020  ###reference_b57###; Brown et al., 2020  ###reference_b7###; Tay et al., 2022  ###reference_b71###; Anil et al., 2023  ###reference_b3###).\nThese objectives\ninvolve different (input, output) pair constructions\n\n(e.g., masking out the next token),\nupon which one minimizes the cross-entropy or log-loss, i.e., .\nAt inference time, given a trained LM and any input context ,\nit is common to perform either classification or generation.\nIn the former, given a set of predefined choices  (e.g., { yes, no }),\none scores each  and returns the highest scoring choice.\nIn the latter,\none performs sampling from  to produce a suitable output string response, e.g., by\ntemperature (Ficler & Goldberg, 2017  ###reference_b21###),\ntop- (Fan et al., 2018  ###reference_b20###),\nor nucleus sampling (Holtzman et al., 2020  ###reference_b29###).\nModel cascades. Cascades are a simple, generic strategy to improve the inference cost-quality tradeoff (Wang et al., 2022  ###reference_b83###).\nGiven a collection of models of varying inference cost,\nthe key idea is to perform\nadaptive inference:\n\u201ceasy\u201d samples are afforded less computation compared to \u201chard\u201d samples.\nConcretely, for any test input,\none first executes the lowest cost model,\nand uses a deferral rule to determine whether\nto terminate with its prediction, or\nto invoke the next cheapest model.\nCascades can reduce the average inference cost if only a small fraction of inputs are deferred.\nCascades\nhave a long history of usage in vision (Viola & Jones, 2001  ###reference_b79###; Huang et al., 2018  ###reference_b31###; Wang et al., 2018  ###reference_b84###),\nwhere they are\noften applied for classification problems.\nGiven an instance space  and label space , the classification problem seeks a classifier\n\nwith good average quality under some distribution ,\nas measured by\n\nfor\nsome .\nIn the simplest case,  measures the classifier accuracy.\nNow suppose we have two classifiers ,\nwith inference costs (e.g., latencies)\n.\nOperationally, a cascade first invokes the \u201csmall\u201d model , and then applies a deferral rule to decide whether to either defer prediction to the \u201clarge\u201d model , or terminate with \u2019s prediction.\nMore precisely,\nlet\n denote the deferral rule,\nwhere\n denotes that we wish to defer to the large model.\nThen, the cascaded classifier is (Kag et al., 2023  ###reference_b37###; Jitkrittum et al., 2023  ###reference_b34###):\nGiven an input ,\nthe corresponding cascade quality and cost are:\nIdeally, one seeks to maximize quality\ngiven a budget  on average inference cost:\nWe note that the average cost  is related to the deferral rate , via .\nIn practice, one may set  for suitable  and threshold .\nOne may choose  to satisfy the inference cost constraint.\nNow, we discuss cascades for generative LMs.\nThis largely follows the setup described above, except that we now consider probabilistic models over sequences.\nConcretely, suppose we have two language models\n,\nwith inference costs .\nSimilarly,\nsuppose \nis a measure of the quality of a given distribution over responses for a given prompt.\nA cascade\n of these models\nis parameterized by\na\ndeferral rule ,\nand is given by:\nGiven an input sequence  and target sequence ,\nan LM cascade results in quality and cost\nWith these, we may construct a similar constrained objective as in Equation 1  ###reference_###.\nSimilarly to the classification case, we may parameterize the deferral rule as  for a suitable deferral score function  (which may depend on the output of ). We will investigate and analyze different types of deferral score functions on different NLP tasks.\nRecently, Chen et al. (2023b  ###reference_b9###) introduced the FrugalGPT system to achieve efficient inference via multiple strategies, including LM cascades.\nThey also learn a deferral score to determine whether or not to terminate prediction;\nhowever, this\ndepends on the input prompt and the generated output text,\nand does not consider the model\u2019s token-level uncertainty as we shall explore subsequently. A few works have proposed to learn a router which can decide which model to use amongst a set of models depending upon the input prompt  (Shnitzer et al., 2023  ###reference_b66###; Hari & Thomson, 2023  ###reference_b26###). However, their settings do not necessarily consider models of increasing capacity and hence, their routers depend only on the input prompt not on the model confidence."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "",
            "text": "A key question in the design of cascades is the choice of deferral rule. In this work, we seek to understand the behaviors of different types of deferral functions on NLP tasks. We start by discussing a few natural extensions of commonly used deferral rules for classification."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "",
            "text": "Chow-Sum. We start with the multi-class classification setting where the output space is defined. In the simplest case, one may defer if the confidence in the prediction of the small model is sufficiently low. There are several means of quantifying confidence in classification (Shafer & Vovk, 2008; Guo et al., 2017; Kendall & Gal, 2017; Jiang et al., 2018), but arguably the simplest is the predicted class probability (Huang et al., 2018; Wang et al., 2022; Jitkrittum et al., 2023), which aligns with Chow\u2019s rule from the closely related problem (see Mozannar & Sontag (2020); Narasimhan et al. (2022)) of learning to reject (Chow, 1970): where  denotes the predictive distribution over possible labels of the small model, and  denotes the predicted label. To design a deferral rule for LM cascading, a natural starting point is to mimic the predicted class probability: we may compute the (log) probability of the model-generated sequence. We term this approach Chow-Sum, as it involves the sum of per-token log probabilities. Analogous to the prediction rule for classification, we may set it up accordingly, denoting by  the set of all sequences. This requires searching over an exponentially large set; however, efficient approximations via greedy or beam search are feasible.\n\nChow-Average. Chow-Sum computes the aggregate sequence-level log-probability. A natural variant is the average of the per-token log-probabilities. This is equivalently the length normalized log-probability, or the log-perplexity (Chen et al., 1998): Note that this may be computed as above, without incorporating any length-normalization."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "",
            "text": "Given that Chow-Sum tracks closely with the well-established Equation 2, it is tempting to conclude that this emphatically solves the LM cascade problem. However, LMs can be susceptible to the length bias problem (Murray & Chiang, 2018; Adiwardana et al., 2020): shorter, lower quality responses may receive a higher probability than longer, higher quality responses. This may be seen as a consequence of the fact that each provides an imperfect estimate of a \u201ctrue\u201d probability, and that errors in these estimates compound with sequence length. The length-bias issue naturally suggests using an average instead of sum of log-probabilities. However, Chow-Average can over-correct for this length bias, and preferentially defer shorter predictions. We will see concrete examples of this behavior in \u00a74.\n\nMore fundamentally, both approaches are inherently limited in the way they aggregate token-level uncertainty. In particular, computing the sum or average of per-token probabilities may mask settings where individual tokens are highly uncertain, even if the entire sequence has reasonably high probability. Such token-level uncertainty may be highly important in certain settings such as fact-answering: here, an LM may be (correctly) highly confident on articles and other grammatical primitives, but these are of less interest than confidence on tokens corresponding to entities (say). This observation has been previously noted and exploited to allow certain \u201ceasy\u201d tokens to be quickly decoded (Schuster et al., 2022). This observation has also been exploited in knowledge distillation by using different teaching modes for \u201ceasy\u201d versus \u201chard\u201d tokens (Zhong et al., 2024).\n\nFigure 2 presents an example of this phenomenon on the WMT FR  EN dataset (details in \u00a73.5): there can be cases where most tokens are highly predictable, but a few tokens are less predictable. In such cases, Chow-Sum can yield overly optimistic uncertainty estimates. This motivates us to consider richer representations of uncertainty which can capture token-level uncertainty, instead of simply computing the sum or average over the sequence."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "",
            "text": "The discussion in \u00a73.2 suggests there is value in considering the following generalization of the maximal sequence probability: where  is the most probable output sequence (or a suitable approximation thereof) under . Here,  computes the -quantile of the set of per-token log probabilities. For instance,  would correspond to taking the minimum per-token log probability as the deferral score. One may regard quantiles as another way of converting the token-level uncertainty distribution into a single score, which are capable of capturing richer information from the token-level uncertainty distribution. For example, employing the maximal token uncertainty (i.e., the minimum of the per-token probabilities (Stengel-Eskin & Van Durme, 2022)) can be useful in scenarios where most tokens are predictable, but a few important tokens are not (per Figure 2). Next, we evaluate all the aforementioned approaches on multiple NLP tasks. For that, we describe the experimental setup used for the evaluation."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "",
            "text": "Models.  \nWe employ FLAN-T5 (Chung et al., 2022) models, which are T5 models (Raffel et al., 2020) that have undergone instruction tuning (Wei et al., 2022). This family offers a range of models of different sizes, spanning Small (M parameters) to XXL (B parameters), and have demonstrated strong few-shot performance on a range of NLP benchmarks. In the body, we primarily focus on a two-model cascade of FLAN-T5 Base and FLAN-T5 Large. Results for other models are included in the Appendix. We employ these models with few-shot prompting and greedy decoding.\n\nEvaluation.  \nWe summarize performance using the deferral curve. Consider a candidate deferral rule produced by thresholding. Let denote the associated cascaded LM. For a fixed threshold, we may compute the associated deferral rate, and the associated cascade quality. The deferral curve is produced by plotting the trade-off between deferral rate and cascade quality as is varied.\n\nDatasets.  \nIn the body, we show deferral curves for three different NLP tasks: MNLI (Williams et al., 2018), a multi-class classification problem; TriviaQA (Joshi et al., 2017), a closed-book question answering problem; and WMT DE FR, a translation problem. We report numbers for an expanded dataset pool. These span Classification (IMDb (Maas et al., 2011), SuperGLUE (Wang et al., 2019a), MNLI (Williams et al., 2018), ANLI (Nie et al., 2020)); Question answering (TriviaQA (Joshi et al., 2017), NaturalQA (Kwiatkowski et al., 2019), TyDiQA { ID, SW, FI } (Clark et al., 2020)); Reading comprehension (Lambada (Paperno et al., 2016), SQuAD (Rajpurkar et al., 2016)); Translation (WMT 14: EN FR (Bojar et al., 2014), WMT 19: DE FR (Foundation), and WMT 14: FR EN (Bojar et al., 2014)); and Common-sense reasoning (Winogrande (Sakaguchi et al., 2021)). Note that we treat all problems as finding a text to text mapping. So for classification tasks, we encode the classes as strings. For evaluation, we take the model\u2019s output text and perform a string comparison to the label. See Table 2 (Appendix) for more details."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "",
            "text": "We now empirically validate the critiques in \u00a73.2, demonstrating that using the standard sequence probability (Chow-Sum) to defer can result in overly penalizing longer sequences. Moreover, Chow-Average flips this bias and overly defers shorter sequences. We then verify that the Chow-Quantile generalization proposed above can capture richer token-level uncertainty.\n\nFigure 3 plots the deferral curves for three datasets. We see that (for a particular choice of quantile), Chow-Quantile consistently outperforms standard Chow-Sum and Chow-Average. While a particular choice of quantile is able to do well, there is no single consistent choice that performs well across tasks.\n\nNext, we discuss insights into the results by using WMT FREN as an example dataset. Why can Chow-Sum and Chow-Average be sub-optimal? To better understand the reason for the sub-optimality of Chow-Sum, Figure 4 studies the relation between the deferral rule and output length. Specifically, for each test prompt, let denote the result of decoding via the small model in the cascade. For each deferral rule, we compute the corresponding score and the length. For ease of comparison across different rules, we convert each score into the corresponding quantile. Figure 4 reveals that Chow-Sum tends to defer prompts with larger output lengths: the prompts with lowest scores have notably higher output length than those with higher scores. \n\nWe observe that for the Base model on the WMT datasets, even the BLEURT scores tend to have a non-zero negative correlation with output lengths. A closer look at the model predictions shows that longer predictions tend to have repetitions, as shown in Figure 5 (Top) and hence, are good candidates for deferral. (The shown predictions are truncated for clarity.) This shows that there is some signal in output length as a deferral score. However, Chow-Sum is overly biased towards deferring longer predictions and hence, can be sub-optimal. Interestingly, Chow-Average over-corrects this bias: it tends to overly defer prompts with lower output length.\n\nWhy does Chow-Quantile help? As discussed above, Chow-Quantile is able to capture rich information from the token-level uncertainty vector. We discuss below why Chow-Quantile-0 and Chow-Quantile-0.8 work well with respect to the WMT FREN dataset.\n\nChow-Quantile-0: The main insight is that the minimum token probability is able to capture repetitions and \u201c??\u201d (unknown tokens), as they generally tend to have lower probability values and are more uncertain. This confirms our understanding that quantiles can capture richer token-level uncertainty. We show two examples with the minimum Chow-Quantile-0 value for the WMT FREN dataset and FLAN-T5 Base in Figure 5 (Middle).\n\nChow-Quantile-0.8: Interestingly, Chow-Quantile-0.8 tends to defer shorter predictions. We show two examples with the minimum Chow-Quantile-0.8 value in Figure 5 (Bottom). To understand this, Figure 4(c) shows the average token probability as a function of the token index, for the WMT EN  FR dataset and FLAN-T5 Base model. As the token index increases, the average probability increases; i.e., the model tends to become more confident. Hence, the Chow-Quantile-0.8 is able to focus more on the shorter, uncertain outputs.\n\nIn summary, we have seen that Chow-Quantile-0 is able to focus more on identifying the presence of repetitions and unknown tokens \u201c??\u201d whereas Chow-Quantile-0.8 is able to capture the uncertainty in shorter predictions better. Thus, we conclude that different quantiles are able to capture richer and complementary measures of uncertainty. Moreover, we have already seen that there is no one quantile which works well across all datasets. Given this, a natural option is to learn how to combine various quantiles for a given dataset, which we consider next."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "",
            "text": "We show that training post-hoc deferral rules based on probability quantiles, and (optionally) suitable embeddings from the small and large model, can significantly improve the cost-quality tradeoff."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "",
            "text": "The idea of learning when to defer in a cascade follows a recent line of work on classification (Narasimhan et al., 2022; Kag et al., 2023; Jitkrittum et al., 2023). In a nutshell, for suitable feature mapping, we seek to learn a deferral score via a standard model class (e.g., a feedforward network). We then defer using the deferral score. To construct the input features, we set the feature vector to be a fixed length comprising the per-token probability quantiles from the small model. Additionally, we add the aggregate scores from Chow-Sum and Chow-Average (see Appendix C). \n\nTo fit the deferral scorer on a training set of input prompts, we minimize an empirical loss against a set of target labels. For tasks based on accuracy, we set the target label iff the large model is correct, and the small model is incorrect on the given example; i.e., it would benefit to defer to the larger model. We then fit the scorer with the binary logistic loss. For translation tasks, the target is the difference of BLEURT scores of the two models; we train with the square loss. We call this method Post-Hoc-Quantile (see Appendix A for details)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "",
            "text": "The above target labels exploit information from the large model during training. Importantly, we cannot directly use such information during inference, as it would require querying the large model (and thus defeat the point of cascading). Note, however, that in some settings it may be feasible to use intermediate information from the large model, e.g., token embeddings from an intermediate layer. Prior work has noted that such intermediate embeddings can often contain valuable information by themselves (Schuster et al., 2022). Inspired by this, we study the viability of using such intermediate embeddings for training post-hoc deferral rules.\n\nFor encoder-decoder models such as T5, such embeddings can be from either the encoder, decoder, or both. We study two methods - one which uses the final decoder embeddings of the smaller model averaged over all tokens. We call this method Post-Hoc-Embed-1. In the second method, we add the first token embedding from the first decoder layer of the large model as another input to the post-hoc rule. We call this method Post-Hoc-Embed-1+2.\n\nWe remark that previous work (Ren et al., 2023) has shown the value of final layer embeddings for selective generation, and the related problem of out-of-distribution detection. We caution also that while not as expensive as querying the entire large model, even extracting intermediate decoder embeddings can involve a non-trivial cost. Nonetheless, in settings where some increase in cost is acceptable, it is of interest whether these embeddings offer significantly valuable information."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "",
            "text": "For the same experimental setup as the previous section, Chow-Quantile-Best chooses the best method out of Chow-Quantile-* based on the validation data. Post-hoc routing is generally on-par with the Chow-* family of (non-learned) deferral rules. Post-Hoc-Embed-1 is able to improve upon Post-Hoc-Quantile and Chow-Quantile-* methods slightly but is slightly inferior compared to the Post-Hoc-Embed-1+2 method. This intuitively makes sense as this has more information compared to the Post-Hoc-Quantile method but still does not include any information about model 2. Strikingly, further using the larger model\u2019s intermediate embeddings can lead to significant improvements across all deferral rates, particularly for classification tasks. Intuitively, the first token\u2019s intermediate embedding could have a lot of information for classification tasks, where the answers typically comprise of a single word and only a couple of tokens. However, for generation and translation tasks with longer outputs, the main token containing the answer could be present later in the sequence and thus, there may be limited use of using the first token embedding. One may wonder if we really need quantiles to train the post-hoc deferral rule. In Appendix B.1, we show that na\u00efvely passing the probability vector with padded zeros performs poorly in many cases."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "",
            "text": "We have seen that there is value in going beyond simple sequence level uncertainty and considering finer measures of token level uncertainty as deferral rules for cascades. Moreover, we have seen that\nintermediate embeddings from the larger model can further boost performance.\nThis work raises a number of interesting directions for future work.\nFirst, we have used a -layer MLP to train post-hoc deferral rules using the quantiles of the probability distribution. It would be interesting to see how well a -layer Transformer works for this task, to potentially better exploit the sequential nature of the token probability vector.\nWe have focused on FLAN-T5 instruction tuned Encoder-Decoder models in this work. We believe that the insights and methods should generalize to Decoder-only architectures. It would be interesting to evaluate the proposed approaches for such architectures. Moreover, it has been observed that models with RLHF finetuning become uncalibrated (Figure 8 in OpenAI (2023  ###reference_b53###)). It would be interesting to see how various finetuning steps affect the findings in this work and what consequences they have for designing efficient cascades.\nMultiple works have considered alternative notions of uncertainty using the generative abilities of LMs, for example, reprompting the model to ask how confident it is about the answer or output an additional confidence score as part of the outputs (Kadavath et al., 2022  ###reference_b36###). It would be interesting to evaluate how well these measures work for cascades which we discuss in the next section."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "",
            "text": "There has been a large body of work on uncertainty quantification for LMs. We discuss some of the approaches below. They can be broadly divided into the following categories.\nConsensus-based.\nOne limitation of Equation 3  ###reference_###\nis that it considers a single output sequence, e.g., the most likely one.\nHowever, as there are many sequences that have similar meaning, it is intuitively more reliable to consider drawing multiple sequences from .\nOne may then assess the consensus in resulting predictions to measure confidence (Wang et al., 2023  ###reference_b85###; Xiao et al., 2021  ###reference_b88###; Chen et al., 2023c  ###reference_b11###);\nthis can help distinguish between models that are locally diffused versus peaked around a candidate sequence. Recently, Yue et al. (2023  ###reference_b89###) explored the use of answer consistency to construct effective cascades.\nDeep ensembles and dropout.\nOne approach to measure confidence is to create an ensemble of different models, and suitably aggregate their predictions (e.g., based on disagreement) (Van Landeghem et al., 2022  ###reference_b76###; Wang et al., 2019b  ###reference_b82###; Gleave & Irving, 2022  ###reference_b24###). However, these uncertainty estimation procedures involve additional computation (e.g., multiple inferences with a single model in dropout-based approaches and single inference with multiple models in ensemble-based approaches) compared to simply using softmax probability outputs from a single network. Such approaches are less appealing for use in cascades, where the primary goal is to improve efficiency.\nPost-hoc calibration/Answer- and length-bias calibration.\nFor tasks involving question-answering with multiple choices (e.g., (A), (B), (C)),\nseveral works have demonstrated that LMs can have prior biases to certain answers, which can be identified and corrected (Zhao et al., 2021  ###reference_b91###; Holtzman et al., 2021  ###reference_b30###; Kumar, 2022  ###reference_b43###; Murray & Chiang, 2018  ###reference_b50###; Mielke et al., 2022  ###reference_b48###; Jiang et al., 2021  ###reference_b33###).\nSemantic entropy.\nAnother key challenge in measuring the uncertainty for natural language outputs is that there are a lot of semantic equivalent sentences and hence, the probability can be divided among multiple outputs which mean the exact same thing. Kuhn et al. (2023a  ###reference_b41###) proposes to mitigate this problem by sampling multiple outputs and then clustering semantically equivalent outputs together and combining their probability together. It would be interesting to understand how well this method can work for our setting.\nGenerative uncertainty.\nThe above has largely focussed on generalizing the standard maximum predictive probability (Equation 2  ###reference_###) from classification to the LM setting.\nWhile this by itself leads to a rich array of possible confidence measures,\nLMs intriguingly offer a wholly new possible means of assessing confidence:\none may directly probe the model to obtain how confident it is on the proposed answer  (Kadavath et al., 2022  ###reference_b36###). Kadavath et al. (2022  ###reference_b36###) discuss various ways of the input prompt format for this confidence probe. They also discuss the training of an additional head of the model to predict the model confidence but again, it is not clear how this compares with the standard probability output by the model without any additional finetuning. However, (Shrivastava et al., 2023  ###reference_b67###) found that the confidence measures generated linguistically give worse estimates of uncertainty compared to the classical softmax-based measures even when these softmax-based probabilities come from a different and weaker model. Moreover, they observed that two sources of uncertainty are complementary and it can be beneficial to combine them.\nOther work.\nZhao et al. (2023  ###reference_b90###) proposed sequence-level calibration as a means to improve the generative ability of LMs; such calibration could also be useful in improving methods such as Chow-Sum.\nKuhn et al. (2023b  ###reference_b42###) proposed to ask the model to detect ambiguous questions which the model is likely to get wrong and answer clarifying questions if the question is indeed ambiguous. Hendy et al. (2023  ###reference_b27###) proposed to use an exogeneous quality estimation model to decide how to route between two models.\n\u0160akota et al. (2023  ###reference_b80###) similarly proposed to train a meta-model to pick an appropriate model from a family. Fadeeva et al. (2023  ###reference_b19###) did a comprehensive experimental analysis of various uncertainty methods."
        }
    ],
    "url": "http://arxiv.org/html/2404.10136v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "3.4",
            "3.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.10136v1",
        "paper_title": "Language Model Cascades: Token-level uncertainty and beyond",
        "research_background": "## Paper's Motivation and Research Problem\nThe primary motivation of the paper is to address the significant inference costs associated with advanced generative language models, as recent improvements in the quality of such models have simultaneously increased the computational resources required for their inference. The research problem focuses on developing cost-efficient strategies for deploying these models, specifically by leveraging a cascading mechanism where a small model handles easier instances while deferring harder ones to a larger model.\n\nThe key challenge within this cascading framework is designing an effective deferral rule, particularly for generative language models, where na\u00efve adaptations of existing deferral mechanisms (originating from classification settings) fail due to biases introduced by the sequential nature of language modeling. This research aims to create more nuanced deferral rules that consider token-level uncertainties within generated sequences, seeking to improve the cost-quality tradeoff without substantial increases in complexity or training requirements.\n\n## Relevant Prior Work\nThe paper builds on significant prior work in several interconnected areas:\n1. **Generative Language Models:**\n    - **Transformer-based models**: Advances in models such as those discussed by Radford et al. (2018), Raffel et al. (2020), Brown et al. (2020), and more recent contributions (Black et al., 2022; Hoffmann et al., 2022; Chowdhery et al., 2022; Wei et al., 2022; Chung et al., 2022; Tay et al., 2023; Anil et al., 2023; Touvron et al., 2023; Team et al., 2023) demonstrate improvements in complex NLP tasks but also highlight increased inference costs.\n2. **Infrastructure Optimization:**\n    - Efforts to optimize the infrastructure to mitigate inference costs, such as those by Chowdhery et al. (2022), Pope et al. (2022), and Sheng et al. (2023).\n3. **Decoding Strategies:**\n    - Rethinking autoregressive decoding, exemplified in works by Stern et al. (2018), Leviathan et al. (2023), Chen et al. (2023a), and Sun et al. (2023).\n4. **Model Architecture and Compression:**\n    - Innovations in the underlying model architecture (Dao et al., 2022) and model compression strategies (Frantar & Alistarh, 2023; Agarwal et al., 2023).\n5. **Cascading Mechanisms:**\n    - The concept of cascading for efficiency, with roots in vision tasks (Viola & Jones, 2001; Trapeznikov & Saligrama, 2013; Bolukbasi et al., 2017; Huang et al., 2018; Rawat et al., 2021; Kag et al., 2023; Jitkrittum et al., 2023), and its growing adoption in NLP (Mamou et al., 2022; Varshney & Baral, 2022; Khalili et al., 2022; Dohan et al., 2022; Chen et al., 2023a, 2023b).\n6. **Deferral Rules for Cascading:**\n    - Traditional deferral rules like Chow\u2019s rule (Chow, 1970) that have shown effective yet imperfect adaptation for generative models, indicating the need for more sophisticated methods to handle the sequence-based nature of language models efficiently.\n\nThis paper proposes innovative methods to overcome limitations in these prior approaches, notably by introducing and learning deferral rules based on token-level uncertainty quantiles, and leveraging embeddings from both smaller and larger models to enhance overall performance.",
        "methodology": "In this work, we investigate various deferral functions and their behaviors in the context of NLP tasks, focusing on the design of language model cascades. The key question addressed is the choice of the deferral rule that decides when to pass the decision from one model to a subsequent one in the cascade. Here, we explore natural extensions of commonly employed deferral rules in classification settings.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Models:**\nWe utilized FLAN-T5 models, which are variants of T5 models enhanced through instruction tuning. The family includes models of varying sizes from Small to XXL. The main focus in the body of the paper is on a two-model cascade comprising FLAN-T5 Base and FLAN-T5 Large. These models were used in a few-shot prompting setup with greedy decoding.\n\n**Evaluation:**\nPerformance was summarized using the deferral curve, which captures the trade-off between deferral rate and cascade quality by varying a threshold. The deferral rate () and cascade quality () were computed for each threshold. As a scalar summary, the area under the deferral curve (AUC-DF) was reported, with higher AUC-DF values indicating better deferral curves. Note that the range of AUC-DF values varies across datasets.\n\n**Datasets:**\nThe main results were shown for three NLP tasks:\n1. MNLI (a multi-class classification problem),\n2. TriviaQA (a closed-book question answering problem), and \n3. WMT (DE to FR translation).\n\nAdditionally, AUC-DF numbers were reported for an expanded dataset pool across various tasks:\n- **Classification:** IMDb, SuperGLUE, MNLI, ANLI\n- **Question Answering:** TriviaQA, NaturalQA, TyDiQA (ID, SW, FI)\n- **Reading Comprehension:** Lambada, SQuAD\n- **Translation:** WMT 14 (EN to FR), WMT 19 (DE to FR), WMT 14 (FR to EN)\n- **Common-sense Reasoning:** Winogrande\n\nAll problems were treated as text-to-text mappings, encoding classes as strings for classification tasks. The model outputs were evaluated via string comparison to the labels. Details on the datasets are available in the Appendix."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To examine the effectiveness of sequence-level log-probability based deferral rules (Chow-Sum and Chow-Average) in improving LM cascades and to address the limitations arising from length bias in token-level uncertainty.",
            "experiment_process": "The study employs FLAN-T5 models with few-shot prompting and greedy decoding. Experimental setups include generating deferral curves to assess the trade-off between deferral rate and cascade quality across different thresholds. Evaluated datasets encompass tasks like MNLI, TriviaQA, and WMT DE-FR, among others. The primary measurements are the deferral rate and cascade quality, summarized by the area under the deferral curve (AUC-DF).",
            "result_discussion": "The results indicate that Chow-Sum tends to defer longer outputs excessively while Chow-Average over-corrects by deferring shorter outputs. The Chow-Quantile method, which generalizes the sequence probability using quantiles, emerges as superior in capturing richer token-level uncertainties. However, no single quantile consistently excels across all tasks.",
            "ablation_id": "2404.10136v1.No1"
        },
        {
            "research_objective": "To empirically validate the critiques on Chow-Sum and Chow-Average deferral rules and to evaluate the effectiveness of Chow-Quantile in capturing token-level uncertainties.",
            "experiment_process": "Deferral rules were evaluated using deferral curves and AUC-DF values on multiple datasets including MNLI, TriviaQA, and WMT DE-FR. The study specifically examined the relation between deferral scores and output lengths, analyzing BLEURT scores and model predictions for longer outputs.",
            "result_discussion": "Chow-Sum is found to defer longer outputs due to length bias, and Chow-Average tends to favor shorter outputs. Comparatively, Chow-Quantile better captures token-level uncertainties, demonstrating improved performance by focusing on uncertain and important tokens. Different quantiles are shown to be effective in specific contexts, emphasizing the need for a combined approach tailored to different datasets.",
            "ablation_id": "2404.10136v1.No2"
        },
        {
            "research_objective": "To investigate the effectiveness of Post-Hoc-Quantile and embedding-based deferral rules in learning optimal deferral strategies in LM cascades.",
            "experiment_process": "Post-Hoc-Quantile utilizes per-token probability quantiles and aggregate scores from Chow-Sum and Chow-Average as features, training a deferral scorer via logistic or square loss. Post-Hoc-Embed methods incorporate intermediate embeddings from both smaller and larger models. Models were evaluated by AUC-DF on datasets like MNLI, TriviaQA, and WMT DE-FR over five random runs.",
            "result_discussion": "Post-Hoc-Quantile consistently outperforms non-learned methods. Embedding-based methods, especially using intermediate embeddings from the large model, provide significant improvements, particularly for classification tasks. However, they offer limited benefits for generation/translation tasks where the critical information may occur later in the sequence. The study highlights the advantage of incorporating richer contextual embeddings to enhance deferral accuracy.",
            "ablation_id": "2404.10136v1.No3"
        }
    ]
}