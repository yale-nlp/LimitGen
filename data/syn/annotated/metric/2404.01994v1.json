{
    "title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning",
    "abstract": "Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environments by following natural language instructions. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation, and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision.\n\nTo address this problem, we propose a Dual-levEL AligNment (DELAN) framework. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment.\n\nOur approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR, and CVDN.\n\nKeywords: Vision-and-Language Navigation, Cross-modal Alignment.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Vision-and-Language navigation (VLN) has attracted increasing interest from natural language processing, computer vision and robotics communities due to its potential of applications such as embodied robots (Gu et al., 2022). The VLN task requires the agent to navigate to a predetermined destination, guided by natural language instruction and real-time visual observations. As depicted in Figure 1, a comprehensive navigation instruction typically comprises a sequence of action, direction and landmark clues. The agent needs to proficiently model its historical experiences and accurately relate the landmark clues to the environmental observations in order to successfully navigate.\n\nSimilar to traditional Vision-Language tasks, the general model architecture of the VLN agent adopts a multi-modal late-fusion paradigm (Du et al., 2023). Under this paradigm, individual modalities are encoded by their respective encoders and then passed through a fusion module, as illustrated in Figure 2. In Vision-Language tasks, aligning unimodal representations before fusion has been extensively validated as it contributes to cross-modal modeling ability in the fusion stage and semantic comprehension ability of unimodal encoders (Li et al., 2021, 2022b). However, pre-fusion alignment has not yet been explored in the context of VLN tasks, resulting in non-negligible disparities among correlated modalities before fusion.\n\nTherefore, we propose to align corresponding unimodal representations and bridge the modality gaps prior to the cross-modal fusion stage in the VLN task. This idea is challenging for two reasons. Firstly, there are limited training signals available for aligning unimodal representations. Training signals about navigation, such as navigation progress and relative spatial position, are used for supervising post-fusion representations in previous works (Ma et al., 2019a; Chen et al., 2021). Secondly, VLN is a sequential decision process distinct from traditional Vision-Language tasks (Hong et al., 2021). The navigation history condenses visited panorama and performed actions to retain past information, while observation at each time step contains detailed landmark semantics corresponding to the instruction. Consequently, how to effectively leverage the different characteristics of navigation history and real-time observations to enhance the cross-modal alignment remains an open question."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Vision-and-Language Navigation",
            "text": "In recent years, training an instruction-following navigator has attracted growing research interest.\nEarlier studies mainly employ a sequence-to-sequence LSTM framework in VLN tasks Fried et al. (2018  ###reference_b9###); Tan et al. (2019  ###reference_b33###).\nOwing to the success of transformer Vaswani et al. (2017b  ###reference_b37###) in Vison-Language tasks (Tan and Bansal, 2019  ###reference_b34###; Lu et al., 2019  ###reference_b25###; Li et al., 2020  ###reference_b20###), transformer-based models are widespread in the VLN field.\nVLNBERT Hong et al. (2021  ###reference_b14###) devises a recurrent unit in the cross-modal transformer to model the navigation history.\nHAMT Chen et al. (2021  ###reference_b3###) proposes to encode all past observations and actions with hierarchical transformer.\nDUET (Chen et al., 2022  ###reference_b4###) utilizes a graph transformer to encode global history for efficient planning.\nBesides the improvement of model architecture, various other methods are proposed to enhance the navigation performance, including training strategies Wang et al. (2019  ###reference_b39###); Zhang et al. (2021  ###reference_b45###); Liang et al. (2022  ###reference_b22###) and auxiliary tasks Ma et al. (2019b  ###reference_b27###); Zhu et al. (2020  ###reference_b47###).\nDespite the progress above, aligning navigation modalities remains a formidable challenge.\nOAAM Qi et al. (2020  ###reference_b30###) and RelGraph Hong et al. (2020a  ###reference_b12###) utilize separate soft-attention modules to implicitly align visual representations with different attribute words.\nFGR2R Hong et al. (2020b  ###reference_b13###) grounds the observation to a certain sub-instruction using human annotations.\nDDL (Cheng et al., 2022  ###reference_b5###) provides token-level supervision by labeling landmark and action words relating to observation at each time step.\nHowever, these approaches only focus on the cross-modal fusion module, neglecting the modality gaps among unimodal representations that are commonly appeared in vision-language tasks Li et al. (2021  ###reference_b19###).\nIn contrast, our work presents a self-supervised framework to align these modalities before fusion stage by contrastive learning to promote cross-modal reasoning.\n###figure_3###"
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2.   Cross-modal Contrastive Learning",
            "text": "Cross-modal contrastive learning has proven its effectiveness in modeling vision-language relationships and has gained prominence as a popular strategy in VL tasks. CLIP Radford et al. (2021  ###reference_b31###) firstly employs cross-modal contrastive learning on large-scale image-text pairs. FILIP Yao et al. (2021  ###reference_b44###) utilizes token-wise maximum similarity to achieve the fine-grained alignment between visual and textual tokens. TACo Yang et al. (2021  ###reference_b42###) proposes a token-aware contrastive loss and a cascade sampling method for aligning video and text. X-CLIP Ma et al. (2022  ###reference_b28###) proposes attention method to realize multi-grained contrast between text and video.\nCITL Liang et al. (2022  ###reference_b22###) first introduces the contrastive learning into VLN domain but only considers the unimodal contrast.\nInspired by these works, we propose to apply contrastive learning to bridge the modality gaps. Furthermore, to handle the sparse training signals for pre-fusion alignment, we apply self-supervised learning strategies to realize our dual-level alignment framework."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Preliminaries",
            "text": "Given a similarity vector/matrix of two instances from distinct modalities, we need to reduce it to a scalar value. Attention mechanism is introduced in the aggregation process, allowing each element to attend to multiple contrasting elements and automatically balance their influences (Ma et al., 2022). Concretely, we first perform attention operations on row and column to reduce values in each dimension.\n\nFinally, we average two scale values to acquire the similarity score of two instances and define the above process as a reduce function."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Problem Formulation",
            "text": "Given the instruction, the agent receives a panorama observation of its surrounding environment at each step. The panorama observation consists of single view images with corresponding relative angles, where is the visual feature of the -th view and is the relative angle with respect to the current view. At each viewpoint, the agent has navigable candidate viewpoints. All observations and performed actions before time step form the history, where action denotes the turned angles at step. The navigation is finished when the agent makes a STOP decision. The task is considered accomplished when the agent stops near the target location (3m)."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Base Agent Model",
            "text": "Current VLN agents mainly adopt a multi-modal late-fusion learning paradigm. To encode each modality, they usually employ standard BERT for textual instructions and a vision transformer (ViT) for visual observations. As for the history modality, they encode historical observations and performed actions during navigation, enabling agents to remember the trajectory. Specifically, graph-based methods maintain a topological map to record historical experiences.\n\nThese unimodal embeddings are further fed to a cross-modal transformer to predict the action probability over the candidate viewpoints. The candidate viewpoint with the largest probability will be chosen as the next move location."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "3.3.   Contrastive Learning Base",
            "text": "Given a similarity vector/matrix of two instances from distinct modalities, we need to reduce it to a scalar value. The attention mechanism is introduced in the aggregation process, allowing each element to attend to multiple contrasting elements and automatically balance their influences (Ma et al., 2022). Concretely, we first perform attention operations on row and column to reduce values in each dimension.\n\nSimilarly, we perform attention operations on column and row to get a scalar value, respectively. Finally, we average two scalar values to acquire the similarity score of two instances and define the above process as a reduce function. \n\nThis setup allows the attention mechanism to effectively aggregate information and capture the relationship between instances."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Methods",
            "text": "In this section, we elaborate each component of our dual-level alignment (DELAN) framework. We explain the dual-level instruction construction in section 4.1 ###reference_###. Then we introduce the instruction-history level alignment in section 4.2 ###reference_###, and the landmark-observation level alignment in section 4.3 ###reference_###. Finally, we present the complete training strategy in section 4.4 ###reference_###. The pipeline of our framework is illustrated as Figure 3 ###reference_### and Figure 4 ###reference_###.\n###figure_4### helps global alignment. Given global instruction representation and global trajectory representation , the similarity between instruction and trajectory can be computed by matrix multiplication as:\nwhere represents for the instruction-trajectory similarity score.\nassists in emphasizing the most important words and downplaying words with low navigation information, like prepositions and conjunctions. For given local word representation and global trajectory representation , we also use matrix multiplication to get the similarity between word and trajectory:\nwhere represents for the word-trajectory similarity score vector.\nfacilitates generalizing viewpoints which closely follow the instruction. We can compute the similarity between instruction and viewpoint by global instruction representation and local viewpoint representation (equal to history representation) , which can be formulated as follows:\nwhere represents for the instruction-viewpoint similarity score vector.\ncontributes to monitoring the navigation progress, as each viewpoint embedding can implicitly align with its corresponding sub-instruction word embeddings. The similarity between word and viewpoint can also be obtained from local word representation and local viewpoint representation as:\nwhere represents for word-viewpoint similarity score matrix.\nWe employ the reduce function in equation 4 ###reference_### to derive scalar values from similarity score vectors or matrices, based on attention mechanisms. Given instruction-history pairs where represents in-batch -th instruction instance and represents in-batch -th history instance, the score measures the semantic similarity between two instances and can be averaged from various similarity scores:\nhelps align landmark tokens with specific view images of observation at each time step. The similarity matrix can be obtained from local landmark representation and local representation observation at time step as follows:\nwhere is the landmark-observation similarity score matrix at step .\nWe also employ the reduce function in equation 4 ###reference_### to dynamically score the importance of each element in similarity matrix and aggregate them.\nAnalogously, given landmark-observation pairs where represents in-batch -th landmark instance and represents in-batch -th observation instance at time step , the similarity score measures the semantic similarity between the landmark and observation :\nNote that is the trajectory length."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Dual-level Instruction Construction",
            "text": "A navigation instruction (e.g. \"Walk to the right past the TV.\") comprises a series of action, orientation, and landmark words. Since the navigation history has strong semantic correlation with the entire instruction, while the real-time observations are closely related to the landmark words in instructions, it is significant to explicitly represent such dual-level semantic correspondences. Therefore, an additional landmark part is introduced for alignment with observation while the original instruction part is reserved for alignment with history.\n\nSpecifically, we concatenate the original instruction with the extracted landmark words to form a dual-level instruction. We employ a standard pre-trained BERT model fine-tuned on a part-of-speech task to extract all nouns in the instruction as landmark words. The instruction level text guides the overall navigation, and the landmark level text helps focus on key details in observation. After this, we can drive the instruction part to align to the history modality and the landmark part to align to the observation modality respectively.\n\nFormally, we extract landmark words from original instructions. These landmark words are appended to the original instructions, subsequently fed into the text encoder to get textual embeddings, i.e., where is the instruction word matrix, and is the landmark word matrix. The representation of navigation history and real-time observations are computed based on an equation, resulting in the history embeddings matrix and observation embeddings matrix at time step . Note that is the total number of navigation steps."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Instruction-History Alignment",
            "text": "In the VLN task, it\u2019s crucial to model historical experiences which help the agent make informed decisions based on past information. The history modality exhibits a strong semantic correlation with instruction. The instruction comprehensively describes the navigation trajectory while the history records the experience along this trajectory. The instruction token sequence and history token sequence both adhere to chronological order and are synchronized in time. Current unimodal representation of history is solely derived from past observation and action (Chen et al., 2021; Cheng et al., 2022) but lacks adequate supervision from textual contents before the fusion stage.\n\nAdditionally, we introduce the global trajectory representation obtained through mean pooling of all history embeddings during navigation and choose the textual [CLS] embedding as the global instruction representation. \n\nAssists in emphasizing the most important words and downplaying words with low navigation information, like prepositions and conjunctions.\n\nFacilitates generalizing viewpoints which closely follow the instruction. \n\nContributes to monitoring the navigation progress, as each viewpoint embedding can implicitly align with its corresponding sub-instruction word embeddings.\n\nWe employ the reduce function to derive scalar values from similarity score vectors or matrices, based on attention mechanisms. \n\nGiven instruction-history pairs where represents in-batch -th instruction instance and represents in-batch -th history instance, the score measures the semantic similarity between two instances and can be averaged from various similarity scores."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Landmark-Observation Alignment",
            "text": "The panoramic observation at each time step consists of view images together with relative directions, providing a detailed spatial layout. Such observation exhibits strong semantic correlation with landmarks in instructions, as the navigation can be typically succeeded by following landmark entities in the environment based on landmark references in instructions. The pre-fusion alignment between landmark and observation enhances the interaction between related representations. Consequently, we align the observation modality with the landmark part of dual-level instructions. The observation and landmarks exhibit an element-wise correspondence, as landmarks only present in some of the view images and real-time observation contains only a subset of the landmarks mentioned in instruction."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Training Strategy",
            "text": "Two typical training strategies in VLN task are Imitation Learning (IL) and Reinforcement Learning (RL). In IL phase, the agent learns by following teacher action:\n\n\\[ L_{\\text{IL}} = - \\sum_t \\log p(a_t|s_t, i) \\]\n\nwhere \\( p(a_t|s_t, i) \\) is predicted action probability. In RL phase, the agent samples the action to explore the environment and learns from the reward:\n\n\\[ L_{\\text{RL}} = - \\sum_t \\log p(a_t|s_t, i) A_t \\]\n\nwhere \\( a_t \\) is the sampled action according to predicted action probability \\( p(a_t|s_t, i) \\) and \\( A_t \\) is the advantage in A2C algorithm.\n\nOverall, the total loss is calculated as:\n\n\\[ L = \\lambda_1 L_{\\text{IL}} + \\lambda_2 L_{\\text{RL}} \\]\n\nwhere \\( \\lambda_1 \\) and \\( \\lambda_2 \\) are weighting parameters that balance different losses."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Experiments",
            "text": "We evaluated our method on three representative VLN tasks (four datasets): VLN with fine-grained instructions R2R (Anderson et al., 2018b) and RxR (Ku et al., 2020); long-horizon VLN R4R (Jain et al., 2019); and vision-and-dialogue navigation CVDN (Thomason et al., 2020). \n\nR2R is the basic VLN dataset which contains step-by-step navigation instructions. It includes 90 scans with 22k human-annotated instruction-trajectory pairs. The dataset is divided into train, val seen, val unseen, and test unseen splits, consisting of 61, 56, 11, and 18 scans respectively. Scans in the unseen splits are excluded from the training set. \n\nRxR is a multilingual (English, Hindi, and Telugu) VLN dataset. Comparing to R2R, RxR is 10x larger with longer and more varied paths. \n\nR4R extends the R2R dataset by concatenating two adjacent trajectories and their instructions. It encourages the agent to navigate as humans plan instead of going directly to the destination. \n\nCVDN requires an agent to navigate based on multi-turn question-answering dialogues. The lengths of instructions and trajectories are much longer than in R2R. \n\nFor R2R, we use standard evaluation metrics following previous work: Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), Success rate weighted by Path Length (SPL). Metrics that measure the path fidelity between the predicted path and target path are introduced for R4R and RxR, including normalized Dynamic Time Warping (nDTW), Success weighted by normalized Dynamic Time Warping (SDTW), and Coverage weighted by Length Score (CLS).\n\nFor CVDN, we use Goal Progress (GP) in meters as the primary metric. GP measures the difference between completed distance and left distance to the goal, so the higher the better. Note that bold metrics in tables indicate the best results. \n\nThe experiments are conducted on a NVIDIA 3090 GPU. \n\nOur baseline models consist of HAMT and DUET. Different from the original implementation, we utilize CLIP-ViT-B-16 as the vision encoder for DUET. The batch size is set to 4 for DUET and 8 for HAMT. The optimizer is AdamW. The remaining training schedules remain consistent with the baselines. Inference is performed using a greedy search approach, following a single-run setting. Note that models marked with an asterisk in the table denote our re-implementation.\n\nTable 1 compares the performances of different agents on the R2R benchmark. Our method outperforms baselines over all dataset splits, achieving 62.69% SPL (+1.7%) on the test split.\n\nTable 2 shows results compared with state-of-the-art methods on the RxR dataset. Our method gets significant improvements (+1.1% on SPL and +1.0% on SR) on the test split compared with the backbones. All of the above demonstrate the effectiveness and generalization of our framework.\n\nTable 3 shows navigation results on the R4R dataset. DELAN performs consistently better than the baseline HAMT (+0.9% on SR), especially on these path fidelity-related metrics (+3.3% on CLS, +3.7% on nDTW, and +2.6% on SDTW). This demonstrates its strong instruction-following ability in long-horizon navigation scenarios. DELAN can model the relation between instruction and history to monitor progress and closely follow the abundant landmarks in long instructions to navigate.\n\nThe results in Table 4 demonstrate that our method enhances the goal progress of previous models, increasing HAMT\u2019s performance by 0.27 meters and DUET\u2019s by 0.23 meters in test environments. This highlights the effectiveness of our proposed framework in a dialog setting.\n\nWe first evaluate the necessity of employing alignment with dual levels rather than a single level, as shown in Table 5. Line 1 represents our DELAN framework with dual-level alignment and corresponding dual-level instruction. Line 2 depicts the use of single-level alignment, which aligns only the original instruction with concatenated visual-related components (history and observation). The results indicate that explicit alignment at dual levels significantly enhances navigation performance compared to directly aligning vision and language modalities.\n\nIn the DELAN framework, we adopt a method of separating landmark words from instructions to facilitate explicit modality alignment. Table 6 presents performance under three approaches with increasing degrees of separation between instructions and landmarks. Line 1 illustrates the strategy employed in DELAN, allowing instructions and landmarks to attend to each other using the shared text encoder. Line 2 represents a strategy where instructions and landmarks are encoded separately but with the shared text encoder. Line 3 represents a strategy utilizing two independent encoders to"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1.   Experimental Setup",
            "text": "We evaluated our method on three representative VLN tasks (four datasets): VLN with fine-grained instructions R2R (Anderson et al., 2018b), RxR (Ku et al., 2020); long-horizon VLN R4R (Jain et al., 2019); and vision-and-dialogue navigation CVDN (Thomason et al., 2020).\n\nR2R is the basic VLN dataset which contains step-by-step navigation instructions. It includes 90 scans with 22k human-annotated instruction-trajectory pairs. The dataset is divided into train, val seen, val unseen, and test unseen splits, consisting of 61, 56, 11, and 18 scans respectively. Scans in the unseen splits are excluded from the training set. RxR is a multilingual (English, Hindi, and Telugu) VLN dataset. Compared to R2R, RxR is 10x larger with longer and more various paths. R4R extends the R2R dataset by concatenating two adjacent trajectories and their instructions. It encourages the agent to navigate as a human plan instead of going to the destination directly. CVDN requires an agent to navigate based on multi-turn question-answering dialogues. The lengths of instructions and trajectories are much longer than R2R.\n\nFor R2R, we use standard evaluation metrics following previous work Anderson et al. (2018a, b): Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), Success rate weighted by Path Length (SPL). Metrics that measure the path fidelity between the predicted path and target path are introduced for R4R and RxR, including normalized Dynamic Time Warping (nDTW), Success weighted by normalized Dynamic Time Warping (SDTW) (Ilharco et al., 2019) and Coverage weighted by Length Score (CLS) (Jain et al., 2019). For CVDN, we use Goal Progress (GP) in meters (Thomason et al., 2020) as the primary metric. GP measures the difference between completed distance and left distance to the goal, so the higher the better. Note that bold metrics in tables indicate the best results. The experiments are conducted on a NVIDIA 3090 GPU.\n\nOur baseline models consist of HAMT (Chen et al., 2021) and DUET (Chen et al., 2022). Different from the original implementation (Chen et al., 2022), we utilize CLIP-ViT-B-16 (Li et al., 2022a) as the vision encoder for DUET. The batch size is set to 4 for DUET and 8 for HAMT. The optimizer is AdamW (Loshchilov and Hutter, 2017). The remaining training schedules remain consistent with the baselines. Inference is performed using a greedy search approach, following a single-run setting (Wang et al., 2019).\n\nNote that models marked with \u2217 in the table denote our re-implementation."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "5.2.   Main Results",
            "text": "Table 1 compares the performances of different agents on the R2R benchmark. Our method outperforms baselines over all dataset splits, achieving 62.69% SPL (+1.7%) on the test split. Table 2 shows results compared with state-of-the-art methods on the RxR dataset. Our method gets significant improvements (+1.1% on SPL and +1.0% on SR) on the test split compared with the backbones. All the above demonstrate the effectiveness and generalization of our framework. Table 3 shows navigation results on the R4R dataset. DELAN performs consistently better than the baseline HAMT (+0.9% on SR), especially on these path fidelity-related metrics (+3.3% on CLS, +3.7% on nDTW, and +2.6% on sDTW). This demonstrates its strong instruction-following ability in a long-horizon navigation scenario. DELAN can model the relation between instruction and history to monitor progress and closely follow the abundant landmarks in long instruction to navigate. The graph-based baseline DUET naturally generates more sub-trajectories for back-and-forth exploration in the predicted path, resulting in reduced path fidelity metrics (CLS, nDTW, and sDTW). However, our method can still improve almost all of its navigation metrics (especially +1.1% on SR), which explains the universality of our framework. The results in Table 4 demonstrate that our method enhances the goal progress of previous models, increasing HAMT\u2019s performance by 0.27 meters and DUET\u2019s by 0.23 meters in test environments. This highlights the effectiveness of our proposed framework in a dialog setting."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "5.3.   Ablation Study",
            "text": "We first evaluate the necessity of employing alignment with dual levels rather than single level, as shown in Table 5. Line 1 represents our DELAN framework with dual-level alignment and corresponding dual-level instruction. Line 2 depicts the use of single-level alignment, which aligns only the original instruction with concatenated visual-related components (history and observation). The results indicate that explicit alignment at dual levels significantly enhance navigation performance compared to directly aligning vision and language modalities.\n\nIn the DELAN framework, we adopt a method of separating landmark words from instructions to facilitate explicit modality alignment. Table 6 presents performance under three approaches with increasing degrees of separation between instructions and landmarks. Line 1 illustrates the strategy employed in DELAN, allowing instructions and landmarks attending to each other using the shared text encoder. Line 2 represents a strategy where instructions and landmarks are encoded separately but with the shared text encoder. Line 3 represents a strategy utilizing two independent encoders to encode instructions and landmarks respectively, ensuring complete instruction and landmark separation before the fusion stage. The results presented in Table 6 highlight the superiority of the separation approach in DELAN. Since individual landmark words still hold contextual relevance, allowing them to attend to instruction words makes practical sense.\n\nWe study the effectiveness of each component of DELAN over the R2R val unseen split. As shown in Table 7, both instruction-history (Instr-His) and landmark-observation (Land-Ob) alignment contributes to the excellent performance of DELAN, especially on metric NE and SR. Each component has a positive effect on the performance. These results demonstrate that DELAN is not only effective but also reasonable."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "5.4.   Further Analysis",
            "text": "We further analyze the agent\u2019s behavior in four specific navigation skills described in Yang et al. (2023). These skills are Stop, Turn, Object and Room, indicating the agent\u2019s ability in stopping, changing direction, object seeking, and room finding, respectively. A higher score means a stronger ability. As shown in Table 8, our DELAN framework significantly enhances the performance across all skills (average +1.5). We attribute the notable promotion in object seeking (+3.6) to our proposed landmark-observation level alignment, which enforces the agent to recognize landmarks appeared in panoramic observations. Meanwhile, the ability of agents to stop and change directions has also been improved, indicating that under the DELAN framework, the agent has a better judgment of instruction execution and navigation progress."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "5.5.   Qualitative Results",
            "text": "We visualize the trajectories of DUET (Chen et al., 2022) and DUET+DELAN in Figure 5. Due to the lack of the pre-fusion alignment, DUET has the unsatisfactory instruction-following capacity. For example, it fails to follow the instruction \"turn slight right\" or misses important landmark \"painting placed on the wall\", leading to either incorrect endpoints (upper) or unnecessary exploration trajectories (lower). Our proposed DELAN framework promotes instruction comprehension and helps the agent make correct decisions, resulting in a more precise and efficient navigation."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Conclusion",
            "text": "This paper highlights the significance of pre-fusion alignment in the VLN task and introduces a dual-level alignment (DELAN) framework, focusing on the instruction-history and landmark-observation levels. For adaptation to the dual-level alignment, we reconstruct a dual-level instruction through concatenating original instruction and landmarks. Experiments across various VLN benchmarks demonstrate the effectiveness, generalization and universality of our approach. We hope this work can inspire further explorations on modality alignment in VLN and related fields."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Acknowledgement",
            "text": "This work is supported by National Natural Science Foundation of China (No.71991471, No.6217020551) and Science and Technology Commission of Shanghai Municipality Grant (No.21DZ1201402)."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.01994v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.01994v1",
        "paper_title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning",
        "research_background": "### Motivation\n\nThe motivation behind this paper is driven by the significant challenges and opportunities in the field of Vision-and-Language Navigation (VLN). VLN has garnered substantial interest due to its applicability in various domains like embodied robotics, where agents navigate environments based on natural language instructions and real-time visual data. Despite advancements in this area, a crucial challenge remains in aligning unimodal representations before fusion to better utilize cross-modal modeling and unimodal comprehension abilities. The authors identify this pre-fusion alignment as unexplored territory within VLN, where current models suffer from disparities among correlated modalities prior to their fusion stage.\n\n### Research Problem\n\nThe main research problem addressed in this paper is how to effectively align unimodal representations in VLN tasks before the cross-modal fusion stage. This is challenging due to two primary reasons: \n1. Limited training signals are available for aligning unimodal representations in VLN, which are conventionally used for supervising post-fusion representations.\n2. The sequential decision-making nature of the VLN task adds complexity as it requires integrating navigation history and real-time observations, each having distinct characteristics.\n\nThe aim is to develop a framework that can align these unimodal representations pre-fusion, thereby improving the cross-modal interaction and resulting in better navigation performance.\n\n### Relevant Prior Work\n\n1. **Multi-modal Late-fusion Paradigm in VLN**: Existing VLN agents typically use a multi-modal late-fusion approach where individual modality encoders process inputs separately before combining them through a fusion module (Du et al., 2023).\n\n2. **Pre-fusion Alignment in Vision-Language Tasks**: Aligning unimodal representations prior to fusion has been extensively studied in the broader Vision-Language tasks, demonstrating significant improvements in cross-modal modeling and semantic understanding (Li et al., 2021, 2022b). However, such pre-fusion alignments have not been explored specifically within the VLN context.\n\n3. **Navigation Supervision Signals**: Prior works incorporate training signals for navigation progress and spatial position to supervise post-fusion representations. However, these works do not address pre-fusion alignment issues (Ma et al., 2019a; Chen et al., 2021).\n\n4. **Sequential Nature of VLN Tasks**: The nature of VLN as a sequential decision-making process sets it apart from other traditional Vision-Language tasks, requiring a nuanced approach to handle both navigation history and real-time observations effectively (Hong et al., 2021).\n\nBy positioning their work against this backdrop, the authors highlight the novelty and the need for their proposed Dual-levEL AligNment (DELAN) framework, which leverages cross-modal contrastive learning to address the identified gaps and challenges in pre-fusion alignment within VLN tasks.",
        "methodology": "The proposed method or model, termed **DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning**, is described in the methodology section as follows:\n\n### Overview\nThe methodology aims to effectively align instances from distinct modalities using a dual-level approach with cross-modal contrastive learning. The procedure involves:\n\n1. **Similarity Calculation**: A similarity vector or matrix is computed between two instances from different modalities.\n2. **Reduction to Scalar Value**: The similarity vector/matrix is reduced to a scalar value for later contrastive loss calculation using an attention mechanism.\n\n### Key Components\n1. **Attention Mechanism**:\n    - **Purpose**: To allow each element to attend to multiple contrasting elements and automatically balance their influences.\n    - **Process**:\n        - **First Attention Operation**:\n            - **Row-wise and Column-wise**: Attention operations are performed on rows and columns to reduce values in each dimension.\n            - **Mathematical Representation**: For a dimension \\( d \\), the elements are aggregated using attention weights:\n            \\[ sim'_{d} = generateAttentionRow(att_{d}) \\otimes sim_{d}, \\quad \\text{and} \\quad att_{d} = softmax(sim_{d}) \\]\n        - **Second Attention Operation**:\n            - **Column-wise and Row-wise**: Attention operations are applied again to obtain a scalar value from the reduced dimensional data:\n            \\[ sim'_{d} = generateAttentionCol(att_{d}) \\otimes sim_{d} \\]\n            - **Result**: Two scalar values are thus obtained.\n    - **Averaging**: The two scalar values are averaged to acquire a final similarity score between the instances.\n\n2. **Cross-Modal Contrastive Learning**:\n    - **General Formulation**: Based on Radford et al. (2021), it involves considering \\( P \\) pairs \\( (x_M, x_T) \\) from modalities \\( M \\) and \\( T \\) within each training batch.\n    - **Similarity Scores**: With similarity scores \\( s_{ij} \\) among instances \\( i \\) and \\( j \\), both \\( M \\)-to-\\( T \\) and \\( T \\)-to-\\( M \\) contrastive losses are calculated.\n    - **Contrastive Loss Function**:\n        - **Definition**: The total contrastive loss is a summation of the individual modality contrastive losses and is represented as:\n        \\[ \\mathcal{L} = \\mathcal{L}_{M \\rightarrow T} + \\mathcal{L}_{T \\rightarrow M} \\]\n        - **Temperature Parameter**: The loss calculation includes a temperature parameter \\( \\tau \\).\n\n### Innovations\n- **Dual-Level Attention**: By introducing a two-tier attention mechanism applied to both rows and columns, the methodology ensures robust feature aggregation from both dimensions.\n- **Cross-Modal Aggregation**: The method improves the alignment between different modalities by leveraging a cross-modal contrastive loss that enforces learning representations that are closer for matched pairs and further apart for non-matched pairs.\n- **Similarity Score Calculation**: The averaging of two scalar values obtained via attention operations fosters a balanced similarity score that is less sensitive to individual peaks in the similarity vectors or matrices.\n\nThe combination of advanced attention-based reduction techniques and cross-modal contrastive learning methodologies enables DELAN to effectively align visual and language data, contributing to improved performance in vision-and-language navigation tasks.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### VLN Tasks and Datasets:\nWe evaluated our method on three representative Vision-and-Language Navigation (VLN) tasks across four datasets:\n1. **R2R (Room-to-Room)**: A basic VLN dataset with step-by-step navigation instructions. It contains 90 scans with 22k human-annotated instruction-trajectory pairs and is divided into train, val seen, val unseen, and test unseen splits with 61, 56, 11, and 18 scans respectively.\n2. **RxR (Room-across-Room)**: A multilingual VLN dataset in English, Hindi, and Telugu, 10x larger than R2R with longer and more varied paths.\n3. **R4R (Room-for-Room)**: Extends R2R by concatenating two adjacent trajectories and their instructions to simulate a more human-like navigation plan.\n4. **CVDN (Continuous Vision-and-Dialog Navigation)**: Involves navigating based on multi-turn question-answering dialogues, with longer instructions and trajectories compared to R2R.\n\n#### Evaluation Metrics:\n- **R2R Metrics**: Trajectory Length (TL), Navigation Error (NE), Success Rate (SR), Success Rate weighted by Path Length (SPL).\n- **R4R and RxR Metrics**: normalized Dynamic Time Warping (nDTW), Success weighted by normalized Dynamic Time Warping (SDTW), Coverage weighted by Length Score (CLS).\n- **CVDN Metric**: Goal Progress (GP) in meters, measuring the distance completed towards the goal.\n\n#### Baseline Models:\n- **HAMT (Chen et al., 2021)**\n- **DUET (Chen et al., 2022)**: For this method, we used CLIP-ViT-B-16 as the vision encoder. The training schedule and inference were performed consistently with the baseline methods, using AdamW as the optimizer, with batch sizes of 4 for DUET and 8 for HAMT.\n\n#### Results:\n- **R2R**: DELAN outperformed all baselines, achieving a 62.69% SPL (+1.7%) on the test split.\n- **RxR**: DELAN showed significant improvements on the test split (+1.1% SPL and +1.0% SR) compared to the base models.\n- **R4R**: DELAN surpassed HAMT with a 0.9% increase in SR, and higher path fidelity metrics: +3.3% on CLS, +3.7% on nDTW, and +2.6% on sDTW. This indicates strong instruction-following ability in long-horizon navigation scenarios. It outperformed DUET by generating fewer sub-trajectories, showing improvements in almost all navigation metrics, particularly SR (+1.1%).\n- **CVDN**: DELAN improved HAMT\u2019s goal progress by 0.27 meters and DUET\u2019s by 0.23 meters, showcasing the framework's effectiveness in a dialog setting.\n\nOverall, the results demonstrate the effectiveness and generalization capabilities of the DELAN framework across various VLN tasks and datasets."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the necessity and effectiveness of employing dual-level alignment rather than single-level alignment in the DELAN framework for enhanced navigation performance.",
            "experiment_process": "The study compared two scenarios of alignment: Line 1 used dual-level alignment with corresponding dual-level instruction, and Line 2 used single-level alignment, which aligns only the original instruction with concatenated visual-related components (history and observation). Additionally, three approaches with varying degrees of separation between instructions and landmarks were evaluated: Line 1 used a strategy allowing instructions and landmarks to attend to each other using a shared text encoder, Line 2 encoded instructions and landmarks separately with a shared text encoder, and Line 3 used two independent encoders to completely separate instructions and landmarks before the fusion stage. Performance was assessed on multiple VLN benchmarks including R2R.",
            "result_discussion": "The results indicated that explicit alignment at dual levels significantly enhanced navigation performance. The strategy used in DELAN, where landmark words still hold contextual relevance and attend to instruction words, proved superior. This demonstrates that individual components of dual-level alignment substantially contribute to improved navigation performance.",
            "ablation_id": "2404.01994v1.No1"
        },
        {
            "research_objective": "To study the effectiveness of each component of DELAN over the R2R validation unseen split, particularly focusing on how different elements of the instruction-history level contrastive learning affect performance.",
            "experiment_process": "The influence of four components of the instruction-history level contrastive learning was assessed: instruction-trajectory (IT), word-trajectory (WT), instruction-viewpoint (IV), and word-viewpoint (WV). Performance was analyzed by removing each component one at a time and observing its effect on key metrics such as NE (Navigation Error) and SR (Success Rate).",
            "result_discussion": "Eliminating word-trajectory alignment significantly increased the trajectory length, indicating a decrease in the overall understanding of navigation progress. Removing instruction-viewpoint alignment most severely degraded the success rate. This underscores the importance of aligning history viewpoints with instructions. Each component positively affected performance, demonstrating that the comprehensive inclusion of these elements is integral to the DELAN framework's success.",
            "ablation_id": "2404.01994v1.No2"
        }
    ]
}