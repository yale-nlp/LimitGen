{
    "title": "V-FLUTE: Visual Figurative Language Understanding with Textual Explanations",
    "abstract": "Large Vision-Language models (VLMs) have demonstrated strong reasoning capabilities in tasks requiring a fine-grained understanding of literal images and text, such as visual question-answering or visual entailment. However, there has been little exploration of these models\u2019 capabilities when presented with images and captions containing figurative phenomena such as metaphors or humor, the meaning of which is often implicit. To close this gap, we propose a new task and a high-quality dataset: Visual Figurative Language Understanding with Textual Explanations (V-FLUTE). We frame the visual figurative language understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a claim (hypothesis) and justify the predicted label with a textual explanation. Using a human-AI collaboration framework, we build a high-quality dataset, V-FLUTE, that contains 6,027 image, claim, label, explanation instances spanning five diverse multimodal figurative phenomena: metaphors, similes, idioms, sarcasm, and humor. The figurative phenomena can be present either in the image, the caption, or both. We further conduct both automatic and human evaluations to assess current VLMs\u2019 capabilities in understanding figurative phenomena.111Code and data will be available at github.com/asaakyan/V-FLUTE",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "###figure_1### Figurative language is integral to human communication, enabling a variety of communicative goals Roberts and Kreuz (1994  ###reference_b40###), including affective communication Fussell and Moss (2014  ###reference_b14###). Figurative language presents a significant challenge to computational approaches as it requires understanding of implicit meaning behind an expression Stowe et al. (2022  ###reference_b46###); Shutova (2011  ###reference_b44###); Veale et al. (2016  ###reference_b48###); Zhou et al. (2021  ###reference_b56###). Recently, Chakrabarty et al. (2022  ###reference_b8###) proposed a task and dataset for Figurative Language Understanding through Textual Explanations (FLUTE) that frames the problem as an explainable textual entailment covering a variety of figurative language phenomena in text: metaphors, similies, idioms, and sarcasm. This dataset has been used successfully to advance and benchmark the capabilities of LLMs for understanding figurative language in text Saakyan et al. (2022  ###reference_b41###); Ziems et al. (2024  ###reference_b57###); Sravanthi et al. (2024  ###reference_b45###); Dey et al. (2024  ###reference_b12###). However, figurative meaning is also prevalent in visual phenomena, such as visual metaphors Akula et al. (2023  ###reference_b2###); Chakrabarty et al. (2023  ###reference_b9###), multimodal sarcasm Desai et al. (2022  ###reference_b11###), and humor Hessel et al. (2023  ###reference_b18###); Hwang and Shwartz (2023  ###reference_b20###). Yet so far most of the work on vision and language models (VLMs) has focused on understanding literal meaning in images and captions (e.g., ScienceQA Lu et al. (2022  ###reference_b30###), MMMU Yue et al. (2024  ###reference_b54###)) including work on explainable visual entailment Kayser et al. (2021  ###reference_b22###). Building on the idea of FLUTE Chakrabarty et al. (2022  ###reference_b8###) for text, we present a new dataset for visual figurative language understanding with textual explanations (V-FLUTE). Our dataset contains 6,027 image, claim, label, explanation instances spanning diverse figurative phenomena. Each instance contains an image (premise) and a textual claim (hypothesis) that is either entailed or contradicted by the image. Deciding the entailment relation requires the vision-language model to understand the implicit meaning in both the visual and textual modalities. Our dataset contains figurative phenomena present in the image, in the caption, or in both. In addition, to mitigate the dependence on spurious correlations, to more rigorously investigate reasoning capabilities, and to promote explainability, our task requires the model to generate a plausible explanation for the output label. See Figure 1  ###reference_### for two examples from our dataset. We make the following contribution towards assessing VLMs ability to understand multimodal figurative phenomena: V-FLUTE, a high-quality dataset of 6,027 image, claim, label, explanation instances built using a human-LLMs collaboration framework covering several phenomena: metaphors, similies, idioms, sarcasm, and humor (Section 3  ###reference_###). We will make the dataset available. A suite of evaluations to assess current VLMs\u2019 capabilities on this new task of explainable visual figurative entailment (Section 4.2  ###reference_### and 4.3  ###reference_###). A detailed human evaluation with error analysis yielding insights into types of errors for different classes of models (Section 5  ###reference_###)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Textual entailment MacCartney and Manning (2008); Bowman et al. (2015) and visual entailment Xie et al. (2019) tasks have been proposed to measure language and multimodal understanding. However, models trained to simply improve label accuracy on these data can be brittle and suffer from spurious correlations Poliak et al. (2018); Gururangan et al. (2018); McCoy et al. (2019); Gardner et al. (2021). Datasets such as e-SNLI Camburu et al. (2018) and e-SNLI-VE Kayser et al. (2021) augment existing entailment datasets with natural language explanations and train models to not only predict the label, but also generate a textual explanation for the reason behind the prediction. Such approach has been further adopted for a variety of tasks, such as commonsense reasoning Rajani et al. (2019); Aggarwal et al. (2021) and social norm understanding CH-Wang et al. (2023) among others Wiegreffe and Marasovic (2021). This approach has been extended to assess LLMs\u2019 capabilities on understanding figurative language through the FLUTE dataset Chakrabarty et al. (2022). FLUTE frames figurative language understanding as an explainable textual entailment task. Recent progress in multimodal models Li et al. (2022); Alayrac et al. (2022); OpenAI (2023); Team et al. (2023); Liu et al. (2023b); Anthropic (2024) prompts us to asses similar capabilities when extended to multimodal setting, testing the understanding of non-literal meaning contained in both images and text. We present an equivalent of the FLUTE dataset for the visual modality: V-FLUTE. A number of previous works has focused on modeling figurative phenomena beyond text. Chakrabarty et al. (2023) use a human-AI collaboration framework to generate visual metaphors from linguistic metaphors (HAIVMet dataset) and propose a visual entailment task as an extrinsic evaluation of dataset quality. The dataset contains images, claims, and labels, but no textual explanations. Yosef et al. (2023) proposed a benchmark (IRFL) where given an idiom, metaphor, or simile the model has to distinguish which of the four associated images implies the figurative meaning of the expression. This dataset focuses on the figurative meaning in the textual modality and does not contain textual explanations. There has also been work on understanding multimodal sarcasm with explanations Desai et al. (2022), mostly containing noisy user-generated text and crowdworker-written explanations. Other line of work has focused on understanding humor with multimodal models. MemeCap Hwang and Shwartz (2023) is a dataset for understanding memes. Hessel et al. (2023) release a corpus of annotated New Yorker Caption Contest entries, where the goal is to come up with a humorous captions for an image, with high-quality explanations for why the caption is humorous. The dataset is relatively limited in size containing only 520 unique instances in its training set. We leverage all these benchmarks to build V-FLUTE."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "V-FLUTE Task and Dataset",
            "text": "Following prior work on figurative language understanding in text defined as explainable textual entailment Chakrabarty et al. (2022), we define the visual figurative understanding as an explainable visual entailment task: given an image (premise) and a claim (hypothesis), output a textual explanation justifying whether the premise entails or contradicts the hypothesis and assign a label. We focus on the binary classification task since, for neutral labels, the explanations would be trivial (simply describing the image).\n\nTo build V-FLUTE, we start with existing multimodal figurative datasets and use human-AI collaboration frameworks with expert annotators Chakrabarty et al. (2022); Wiegreffe et al. (2022); Liu et al. (2022) to transform them into a high-quality, explainable visual entailment benchmark. These datasets cover particular phenomena such as metaphors, similes, idioms, sarcasm or humor. Each instance includes an image and a caption, and the figurative phenomenon can be either in the image, the caption or in both.\n\nWe transform each data into a unified image, claim, label, explanation format for explainable visual entailment. An overview of the dataset and our contributions can be found in Table 1. See examples from each dataset in Table 2. Below, we describe the construction of V-FLUTE for each figurative language type (metaphors & similes, idioms, sarcasm, and humor)."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Metaphors and Similes",
            "text": "Metaphors and similes are powerful rhetorical devices that can be expressed either in text or visually in an image. Visual metaphors are used as persuasive devices in various fields such as advertising Forceville (2002  ###reference_b13###); Scott (1994  ###reference_b42###). To create visual entailment instances containing metaphors and similes in V-FLUTE, we rely on two existing resources: HAIVMet Chakrabarty et al. (2023  ###reference_b9###) and IRFL Yosef et al. (2023  ###reference_b53###). Instances taken from HAIVMet contain the metaphor/simile as a part of the premise (image), while those taken from IRFL have the metaphor/simile as a part of the hypothesis (text)."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 HAIVMet as Data Source",
            "text": "The HAIVMet Chakrabarty et al. (2023) dataset consists of 1,193 images of visual metaphors spanning over 958 distinct linguistic metaphors. Each image is associated with a claim that can be contradicting or entailing the image. In addition, each image is associated with a visual elaboration that presents a textual description of the image (See Figure 2). This visual elaboration was used in the original paper to generate the visual metaphors (images).\n\nGenerating Textual Explanations. We augment the dataset with candidate textual explanations. We prompt ChatGPT (gpt-3.5-0914) to generate an explanation for every tuple of visual elaboration, claim, and label (See Figure 2; and prompt in Appendix D.1.1).\n\nExpert Verification. Each claim is paired with multiple images. However, since these images were automatically generated with DALLE-2 using the visual elaborations, not all are completely faithful. Moreover, some claims and labels were inconsistent. Finally, automatically generated LLM candidate explanations are not always correct and require refining. To tackle these issues, we employ an expert verification process involving three expert annotators with significant experience in figurative language and visual metaphor understanding. Since each claim can be paired with more than one visual metaphor, we ask annotators to select the visual metaphor most faithful to the linguistic metaphor and visual elaboration (see Image Selection in Figure 2) or select none in the rare case when none of the visual metaphors are of good quality. As a part of the same annotation round, we also ask them to verify and edit the explanation if necessary to ensure correctness and high quality. Post strict quality control, we have 857 image, claim, label, explanation instances."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 IRFL as Data Source",
            "text": "The IRFL dataset contains 1,440 figurative expressions, each associated with distinct images. One of those images represents the figurative expression, and the other 3 act as distractors.\n\nImage Selection. We automatically select images using CLIP. We select one of the distractor images that have the highest CLIPScore with the corresponding entailing image to create a challenging, contradictory instance.\n\nGenerating Textual Explanations. We prompt GPT-4 with the ground truth label, claim, and the image to explain the relationship between the image and the claim.\n\nExpert Verification. We recruit the same three expert annotators from HAIVMET annotations and ask them to verify the explanation is adequate and edit it when necessary. We also ask the annotator to discard rare noisy instances where the claim, image, and label do not fit. Post strict quality control, we are left with 1149 image, claim, label, explanation instances."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Idioms",
            "text": "The IRFL dataset contains idioms in addition to metaphors and similies. An identical procedure to the one described in Section 3.1.2 ###reference_.SSS2### was used for generating V-FLUTE instances for idioms (370 image, claim, label, explanation examples)."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Sarcasm",
            "text": "To create visual entailment instances containing sarcasm, we rely on the MuSE data Desai et al. (2022  ###reference_b11###). Similarly to IRFL, instances from MuSE data contain sarcasm in the hypothesis (text)."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 MuSE as Data Source",
            "text": "The MuSE dataset consists of 3510 distinct images, the respective sarcastic claims that act as contradiction instances, and crowd worker written explanations justifying the contradiction.\n\nGenerating Entailment Claims. Since the dataset only contains sarcastic instances, there are no claims with an entailment relationship. We generate the entailing claims by prompting GPT-4 to generate a non-sarcastic version of the claim while maintaining the user-generated informal style of the text.\n\nGenerating Textual Explanations. While the dataset already contains crowdworker-written explanations, upon inspection, they were often deemed poor quality, lacking enough details, and formulaic. To improve their quality, we use the dataset\u2019s existing crowdworker explanations and prompt GPT-4 to rewrite and generate high-quality candidate textual explanations given the claim and the label.\n\nExpert Verification. Each image is now paired with a GPT-4-generated entailing claim, an original contradicting claim, and their respective labels and explanations. The same three expert annotators checked if the generated explanations are adequate (i.e., complete, correct, and concise) and if not, edit them. Experts were also instructed to discard noisy examples, e.g. when the image does not contradict the sarcastic claim. Through strict quality control, we obtain 1,042 image, claim, label, explanation instances."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Humor",
            "text": "For multimodal humor, we rely on two datasets: MemeCap Hwang and Shwartz (2023 ###reference_b20###) and New Yorker cartoons Hessel et al. (2023 ###reference_b18###)."
        },
        {
            "section_id": "3.4.1",
            "parent_section_id": "3.4",
            "section_name": "3.4.1 MemeCap as Data Source",
            "text": "This dataset consists of memes along with their captions that describe the meme poster\u2019s intent (see example in Figure 5). Memes frequently contain implicit, non-literal meaning and rely on visual metaphors, posing a challenge to VLMs.\n\nClaim Generation. Since meme captions are not suited for an entailment task, we prompt GPT-4 with the caption to generate a claim from it (see example in Figure 5). We filter this set of samples further with GPT-4 by asking whether the image entails the claim and only selecting positive instances. In addition to generating claims that entail the meme, we generate counterclaims using GPT-4.\n\nGenerating Textual Explanations. We prompted GPT-4 with the ground truth label in the prompt to explain the relationship between the image and the claim. See prompts in Appendix D.4.\n\nExpert Verification. We hire the same three expert annotators to ensure the correctness of the data. Each annotator is tasked with verifying that 1) the generated claim fits the image and 2) the explanation is correct and complete, and if not, make the necessary changes. We also ask to discard samples with inappropriate content.\n\nAfter careful quality control, we have 1958 image, claim, label, explanation instances."
        },
        {
            "section_id": "3.4.2",
            "parent_section_id": "3.4",
            "section_name": "3.4.2 NYCartoons as Data Source",
            "text": "The NYCartoons dataset Hessel et al. (2023 ###reference_b18###) contains 651 high-quality instances from the New Yorker Cartoon Caption Contest. Each instance consists of a humorous image paired with a caption and a natural language explanation justifying the implicit humor between the caption and the image. We simply use the existing data where the caption is treated as a claim entailing the humorous image paired with an explanation."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Dataset Statistics",
            "text": "We split our data into 4,578 training, 726 validation, and 723 testing instances. Detailed counts per phenomenon and dataset, as well as other statistics, are in Appendix A."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We empirically study how several baseline models perform on the task of explainable visual entailment. We investigate both off-the-shelf and fine-tuned model performance.\n\nOur strongest fine-tuned model (LLaVA-7B-eViL+VF) outperforms the best off-the-shelf model (GPT-4-5shot) in terms of certain metrics, and performs competitively when incorporating the explanations quality with GPT-4 leading slightly, which is expected as GPT-4 is the teacher model with which the majority of the explanation candidates were generated. Adding the e-ViL dataset improves the performance slightly compared to only fine-tuning on V-FLUTE. Fine-tuning merely on e-ViL improves over a random baseline; however, the explanations are of poor quality.\n\nWe also utilize a hypothesis-only baseline Poliak et al. (2018) by including a model fine-tuned on the V-FLUTE dataset, but without the relevant image (with a white square as an input instead, denoted as Image). Fine-tuning on the full V-FLUTE dataset shows an improvement, suggesting VLMs benefit from visual information when dealing with figurative phenomena and do not just rely on the input text to make their prediction.\n\nLLaVA-7B and 34B lag behind Claude 3 and GPT-4 in zero-shot settings. However, scene graph prompting improves the zero-shot performance of the LLaVA-based models, allowing them to catch up to zero-shot API model performance (Gemini and Claude 3). The explanations generated by these models tend to overly focus on the contents of the scene graph rather than the underlying figurative phenomena, possibly causing a decrease in explanation score. The few-shot API models outperform zero-shot API models, and they are better than all configurations of open models, indicating the effectiveness of few-shot prompting (not available for LLaVA-based models as of now).\n\nWe plot the relative percentage decrease between performance metrics for LLaVA-eViL-VF, LLaVA-34B-SG, and GPT-4-5shot. Higher relative drop indicates higher difficulty of generating the correct explanation. For all models, we see a substantial decrease in performance, especially on challenging phenomena such as Humor (NYCartoons). For Metaphor (IRFL), Humor (MemeCap), and Idiom (IRFL) subsets, GPT-4 exhibits the lowest relative performance drop, while for Metaphor (HAIVMet), Humor (NYCartoons), and Sarcasm (MuSE), the fine-tuned model has the lowest drop.\n\nWe can see that the percentage drop is substantially higher for all models for the HAIVMet subset rather than the IRFL dataset, which contains metaphors in the image rather than in the text. This suggests it is harder for models to generate correct explanations when the figurative meaning is contained in the image rather than in the text, indicating the need to expand current datasets to include images with figurative meaning."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Models",
            "text": "We select a variety of models for our study (see taxonomy in Appendix, Figure 10). For off-the-shelf models, we explore both open and API-based models. For open models, we select the (current) state-of-the-art LLaVA-1.6 models Liu et al. (2024). LLaVA is one of the simplest, yet one of the most high-performing VLM architectures currently available. It utilizes a pretrained large language model (e.g., Mistral-7B Jiang et al. (2023)) and a vision-language cross-modal connector (e.g., an MLP layer) to align the vision encoder (e.g., CLIP Radford et al. (2021)) outputs to the language models. We select LLaVA-1.6 models in their 7B and 34B configurations (LLaVA-v1.6-7B and LLaVA-v1.6-34B respectively) and refer to them as LLaVA-ZS-7B and LLaVA-ZS-34B. Both models have been instruction-tuned on less than 1M visual instruction tuning samples to act as general language and vision assistants. It should, however, be noted that these models do not currently support few-shot multimodal prompting.\n\nIn addition to zero-shot testing, we also test these models using Compositional Chain-of-Thought Prompting proposed by Mitra et al. (2023). The method first prompts the model to generate a scene graph and then utilizes that scene graph in another prompt to answer the relevant question. The method works zero-shot without requiring fine-tuning. We refer to these models as LLaVA-ZS-7B-SG and LLaVA-ZS-34B-SG for the 7B and 34B LLaVA configurations described above.\n\nFor API-based models, we select three widely available state-of-the-art VLMs: Claude-3 Opus (claude-3-opus-20240229)Anthropic (2024), GPT-4 (gpt-4-1106-vision-preview) OpenAI (2023) and GeminiPro (gemini-pro-vision)Team et al. (2023). We refer to GPT-4 as the \u201cteacher\u201d model as most candidate explanations were generated with it.\n\nFor fine-tuned models, we focus on fine-tuning LLaVA-1.5-7B model Liu et al. (2023a) (the fine-tuning code for 1.6 model is not available during the time the paper was written). To minimize bias for a single instruction, we fine-tune and evaluate the models on a set of 21 instruction paraphrases (see Appendix Table 8). Three model configurations are tested:\n\nLLaVA-eViL is a checkpoint of LLaVA-v1.5-7B further fine-tuned on the eViL (e-SNLI-VE) dataset for explainable visual entailment Kayser et al. (2021) converted to the instruction format. We removed neutral label instances, which resulted in 275,815 training instances and 10,897 validation instances.\n\nLLaVA-VF is the same checkpoint fine-tuned on the training set of V-FLUTE. We also fine-tune the model with a white square instead of the V-FLUTE image (denoted by Image).\n\nLLaVA-eViL+VF is the same checkpoint fine-tuned on both eViL and V-FLUTE.\n\nAll hyperparameters are in Appendix C."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Automatic Metrics",
            "text": "The ExplanationScore computes the average between BERTScore based on the microsoft-deberta-xlarge-mnli model and BLEURT based on the BLEURT-20. Since our goal is to ensure models provide an answer for the right reasons, ideally, we would only count predictions as correct when the explanation is also correct. Thresholds are selected based on human evaluation of explanation quality in Section 5.3."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Automatic Evaluation Results",
            "text": "Table 3 shows the results based on the automatic evaluation. We also include results per phenomenon in Appendix F and the drop in performance when accounting for explanations score in Figure 6. Our results inform the following insights:\n\nOur strongest fine-tuned model (LLaVA-7B-eViL+VF) outperforms the best off-the-shelf model (GPT-4-5shot) in terms of the F1@0 score, and performs competitively when incorporating the explanations quality with GPT-4 leading slightly, which is expected as GPT-4 is the teacher model with which the majority of the explanation candidates were generated. Adding the e-ViL dataset improves the performance slightly compared to only fine-tuning on V-FLUTE. Fine-tuning merely on e-ViL improves over a random baseline; however, the explanations are of poor quality.\n\nWe also utilize a hypothesis-only baseline by including a model fine-tuned on the V-FLUTE dataset, but without the relevant image (with a white square as an input instead, denoted as Image). Fine-tuning on the full V-FLUTE dataset shows an improvement of over 8 points in F1@0, suggesting VLMs benefit from visual information when dealing with figurative phenomena and do not just rely on the input text to make their prediction.\n\nLLaVA-7B and 34B lag behind Claude 3 and GPT-4 in zero-shot settings. However, scene graph prompting improves the zero-shot performance of the LLaVA-based models, allowing them to catch up to zero-shot API model performance (Gemini and Claude 3). The explanations generated by these models tend to overly focus on the contents of the scene graph rather than the underlying figurative phenomena, possibly causing a decrease in explanation score. The few-shot API models outperform zero-shot API models, indicating the effectiveness of few-shot prompting (not available for LLaVA-based models as of now).\n\nWe plot the relative percentage decrease between F1@0 and F1@60 for LLaVA-eViL-VF, LLaVA-34B-SG, and GPT-4-5shot in Figure 6. Higher relative drop indicates higher difficulty of generating the correct explanation. For all models, we see a substantial decrease in performance, especially on challenging phenomena such as Humor (NYCartoons). For Metaphor (IRFL), Humor (MemeCap), and Idiom (IRFL) subsets GPT-4 exhibits the lowest relative performance drop, while for Metaphor (HAIVMet), Humor (NYCartoons), and Sarcasm (MuSE) the fine-tuned model has the lowest drop.\n\nWe can see that the percentage drop is substantially higher for all models for the HAIVMet subset rather than the IRFL dataset, which contains metaphors in the image rather than in the text. This suggests it is harder for models to generate correct explanations when the figurative meaning is contained in the image rather than in the text, indicating the need to expand current datasets to include images with figurative meaning."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Human Baseline",
            "text": "To find out how humans perform on the task, we hire two expert annotators with formal education in linguistics. We present them with 10 example instances and then ask them to complete 99 randomly sampled test set instances. We also evaluate our best model (see Table 3) on the same set. Results are shown in Table 4. Human performance is quite strong, almost reaching 90 F1@0 score overall. Human performance is better than our strongest fine-tuned model (LLaVA-7B-eVil+VF) performance with Annotator 1 and Annotator 2. Humans excel at interpreting memes, with both annotators reaching a 100% accuracy. Humans also perform noticeably better on the NYCartoons dataset and on the idiom subset of the task. The model has a slight edge in performance on the sarcasm and visual metaphor subsets of the task, perhaps due to difficulty of these subsets and any potential spurious correlations during fine-tuning."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Human Evaluation and Error Analysis",
            "text": "We conduct human evaluation of generated explanation to more reliably assess their quality and identify key errors in multimodal figurative language understanding. We recruit two expert annotators with background in linguistics for the task and sample 95 random instances from the test set. For each instance, we first provide the annotators with the image, claim and reference explanation and ask the annotators to choose the right label. If the annotator succeeds, they can view the rest of the task, which consists of 3 explanations from our top models by F1@0 in each category: LLaVA-eViL-VF, LLaVA-34B-SG, GPT-4-5shot. The explanations are taken for both correct and incorrect model predictions. For each explanation, we ask whether the explanation is adequate (accurate, correct, complete and concise).\nIf not, we ask them to identify one of the three main types of errors based on the following taxonomy:\nHallucination: explanation is not faithful to the image, indicating difficulties with basic visual comprehension (see prediction of a blunt tip when the pencil tip is actually sharp in row 1 of Table 5  ###reference_###).\nUnsound reasoning: sentences do not adhere to natural logic or violate common sense (e.g., concluding than an upwards arrow and lots of money imply an economic crisis, see row 3).\nIncomplete reasoning: while overall the explanation makes sense, it does not address the key property reasons why the image entails or contradicts the claim (for example, does not address the figurative part in the image, see row 2).\nToo Verbose: the explanation is too verbose to the point it would interfere rather than help one decide the correct label.\nError Type\n\n\n\nImage\n\n\n\nClaim\n\n\n\nLabel and Explanation\n\n\n\n\nHallucination (describes sharp pencil as having a blunt tip)\n\n\n\n\n\n\n\nThe tip is as sharp as a knife.\n\n\n\nPredicted Label: Contradiction\nPredicted Explanation: The image depicts a pencil with a blunt tip. [\u2026]\n\n\n\n\nIncomplete (does not address metaphorical meaning of iceberg imagery)\n\n\n\n\n\n\n\nThey looked at each other in an almost hostile way.\n\n\n\nPredicted Label: Entailment \nPredicted Explanation: The image depicts two characters looking at each other with a tense and confrontational expression, which is consistent with the claim [\u2026]\n\n\n\n\nUnsound\n(concludes that large amount of money + upward trending chart implies rough waters)\n\n\n\n\n\n\n\nThe world recession of 2001 was one of the worst in history.\n\n\n\nPredicted Label: Entailment\nPredicted Explanation: The image depicts a man in a boat, holding a large amount of money and a chart with an arrow pointing upwards. This suggests that the man is navigating through rough waters [\u2026]\n###figure_13### ###figure_14### ###figure_15###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "How Do Models Perform According to Humans?",
            "text": "###table_5### In Table 6  ###reference_###, we show adequacy and preference rates for explanations from the 3 systems, where an explanation is deemed adequate if both annotators agreed it is, and inadequate if both agreed it is not. The preference percentage is also taken among instances where the annotators agreed that the model\u2019s explanation is preferred among all the adequate explanations. The average IAA using Cohen\u2019s  is 0.47, indicating moderate agreement Cohen (1960  ###reference_b10###). We observe that the teacher model is leading in terms of the adequacy of the explanations and preference rate, as expected from a larger system equipped for higher quality reasoning and generation capabilities. Yet still only half of its explanations are considered adequate. This further confirms that despite impressive performance on the F1@0 scores, the models are not yet capable of producing adequate textual explanations in many instances."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "What Errors Do Models Make?",
            "text": "We also analyze to understand what type of errors do each model make when they are considered not adequate in the above evaluation. In Figure 7  ###reference_###, we illustrate the normalized frequency of error types when both annotators agree that the explanation is not adequate (i.e., out of all errors for this model, what percentage is each type of error?). In general, annotators did not consider verbosity to be a major issue of the systems. For GPT-4, the leading error type is hallucination, indicating the need to improve faithful image recognition even in the most advanced models.\nFor the fine-tuned model and LLaVA-34B-SG, the main error type is unsound reasoning, indicating that it is challenging for the models to reason about multimodal figurative inputs consistently.\n###figure_16###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "How Well Does the Explanation Score Predict Human Judgment on Adequacy?",
            "text": "We explore whether the proposed explanation score can capture human judgement of explanation adequacy. We collect all instances where both annotators agreed on the adequacy judgement for the explanation.\nWe evaluate if the explanation score described in Section 4.2  ###reference_### can act as a good predictor for the human adequacy judgment. We find that the area under the Precision-Recall curve is 0.79, and the maximum F1 score is 0.77, obtainable at the explanation score threshold of 0.53. Hence, we use this threshold to report the results in Table 3  ###reference_###. We also use the threshold of 0.6 since it maximizes F1 such that both precision and recall are above 0.75."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "We introduce a high-quality dataset for understanding figurative phenomena in multimodal input, V-FLUTE, framed as an explainable visual entailment. Our dataset consists of 6,027 image, claim, label, explanation instances spanning a variety of figurative phenomena such as metaphor, idiom, simile, sarcasm, and humor. We use this dataset to benchmark the performance of state-of-the-art vision-language models using both automatic and human evaluation and to identify critical areas of improvement for VLMs for this task."
        }
    ],
    "url": "http://arxiv.org/html/2405.01474v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.3",
            "3.3.1",
            "3.4",
            "3.4.1",
            "3.4.2",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1.1",
            "3.1.2",
            "3.3.1",
            "3.4.1",
            "4.1.1"
        ]
    },
    "research_context": {
        "paper_id": "2405.01474v1",
        "paper_title": "V-FLUTE: Visual Figurative Language Understanding with Textual Explanations",
        "research_background": "The paper titled 'V-FLUTE: Visual Figurative Language Understanding with Textual Explanations' addresses a significant challenge in computational understanding of language, extending it from textual to visual phenomena. Below is a summary of the paper\u2019s motivation, research problem, and relevant prior work:\n\n### Motivation\nLanguage is essential for human communication, allowing for a wide range of communicative goals such as affective communication. Despite its significance, language presents a challenge for computational models because it necessitates grasping implicit meanings behind various expressions. Recently, there has been progress in understanding language in text, but visual language remains underexplored. The motivation behind this paper is to address this gap and enhance the understanding of visual language using computational approaches.\n\n### Research Problem\nThe core research problem tackled by this paper is the understanding and interpretation of language in visual contexts. While recent efforts like FLUTE have made strides in textual language understanding, there is a lack of equivalent resources and methods for visual phenomena. This paper introduces a new dataset, V-FLUTE, which aims to assess visual-language models (VLMs) in their ability to understand and explain language present in images, captions, or both. The goal is to create a robust framework that can accurately determine entailment relations and provide plausible explanations, bridging the gap between visual and textual language understanding.\n\n### Relevant Prior Work\n1. **Textual Language Understanding**:\n   - **FLUTE Dataset**: Proposed by Chakrabarty et al. (2022), this dataset and task frame textual language understanding as an explainable textual entailment problem, covering metaphors, similes, idioms, and sarcasm.\n   - **LLMs Performance**: This dataset has helped advance and benchmark the capabilities of large language models (LLMs) in interpreting language in text (Saakyan et al. 2022; Ziems et al. 2024; Sravanthi et al. 2024; Dey et al. 2024).\n\n2. **Visual Language Understanding**:\n   - **Visual Metaphors**: Akula et al. (2023) and Chakrabarty et al. (2023) have explored visual metaphors.\n   - **Multimodal Sarcasm and Humor**: Desai et al. (2022) looked into multimodal sarcasm while Hessel et al. (2023) and Hwang and Shwartz (2023) focused on humor in visual contexts.\n   - **Existing VLM Work**: Most VLM work has traditionally focused on literal understanding in images and captions, as seen in tasks like ScienceQA (Lu et al. 2022) and MMMU (Yue et al. 2024). Kayser et al. (2021) contributed to explainable visual entailment but did not specifically address phenomena.\n\nBy introducing the V-FLUTE dataset and new evaluations, the paper builds on prior work to address the identified gap in visual language understanding and seeks to advance the capabilities of VLMs in this nuanced area.",
        "methodology": "The proposed method, V-FLUTE, focuses on the task of visual language understanding using textual explanations. Here\u2019s an overview of the key components and innovations of the method:\n\n1. **Task Definition**:\n   - Modeled after explainable textual entailment, visual understanding is framed as an *explainable visual entailment* task. Given an image (premise) and a claim (hypothesis), the model is required to output a textual explanation justifying whether the premise entails or contradicts the hypothesis, followed by assigning a label.\n\n2. **Focus on Binary Classification**:\n   - The focus is on binary classification (entailment vs. contradiction), as explanations for neutral labels are deemed trivial, merely describing the image without deeper analysis.\n\n3. **Dataset Construction**:\n   - **Data Sources**: Utilizes existing multimodal datasets, which incorporate various phenomena including metaphors, similes, idioms, sarcasm, and humor.\n   - **Human-AI Collaboration**: Employed frameworks involving human-AI collaboration and expert annotators for high-quality data transformation.\n   - **Unified Data Format**: Each dataset is transformed into a unified format of image, claim, label, and explanation to standardize the explainable visual entailment task.\n\n4. **Annotation and Quality Assurance**:\n   - Expert annotators play a crucial role in ensuring that the dataset is of high quality and covers a wide range of expressions.\n   - Each instance includes detailed annotations, ensuring both the image and caption are scrutinized for potential phenomena.\n\n5. **Innovations**:\n   - **Explainable Visual Entailment**: This is a novel setting that extends textual entailment tasks into the visual domain, requiring deeper multi-modal understanding.\n   - **Human-AI Collaboration**: Integrating expert annotations with AI systems to enhance the dataset's quality and ensure nuanced language phenomena are captured accurately.\n\nTo sum up, V-FLUTE introduces an innovative approach to combining visual and textual data to address the challenges of understanding language with explainable entailment, providing both labels and detailed explanations that reflect human-like comprehension of complex constructs.",
        "main_experiment_and_results": "***Main Experiment Setup and Results:***\n\n**Setup:**\n\n1. **Task:** Explainable Visual Entailment - evaluating the model's ability to understand and explain language in visuals.\n2. **Models Investigated:**\n   - **Off-the-shelf models:** GPT-4-5shot, Claude 3, Gemini (zero-shot and few-shot).\n   - **Fine-tuned models:** LLaVA-7B-eViL+VF, LLaVA-7B, LLaVA-34B.\n3. **Baselines:**\n   - **Hypothesis-only baseline:** Model fine-tuned on V-FLUTE dataset with a white square as an input instead of the relevant image.\n4. **Fine-tuning Datasets:**\n   - **V-FLUTE:** Primary dataset.\n   - **e-ViL:** Supplementary dataset incorporated to boost performance.\n5. **Metrics Used:**\n   - **F1@0:** Measures the accuracy of entailment without considering the quality of explanations.\n   - **F1@60:** Joint metric that incorporates explanation quality, with a threshold of 60 for explanations.\n\n**Key Findings:**\n\n1. **Comparison of Models:**\n   - **Performance:** The fine-tuned model LLaVA-7B-eViL+VF achieved the highest F1@0 score overall, outperforming the best off-the-shelf model GPT-4-5shot.\n   - **Explanations Quality:** F1@60 showed that GPT-4-5shot performed slightly better (49.81) compared to the best fine-tuned LLaVA-7B-eViL+VF (48.80), which aligns with the fact that GPT-4 was used to generate the majority of explanation candidates.\n\n2. **Zero-shot Performance:**\n   - **LLaVA-7B and 34B:** Lag behind Claude 3 and GPT-4 in zero-shot settings but catch up when scene graph prompting is used, despite a potential drop in explanation score due to over-focusing on scene graph content.\n\n3. **Few-shot Performance:**\n   - **Few-shot API Models:** Outperform zero-shot API and all configurations of open models, proving the efficacy of few-shot prompting.\n\n**Conclusion:**\n\n- Fine-tuned models like LLaVA-7B-eViL+VF show significant improvements over off-the-shelf and hypothesis-only models, underscoring the importance of visual context.\n- The efficacy of scene graph prompting helps bridge the performance gap in zero-shot setups.\n- Few-shot prompting exhibits superior performance, pointing towards its potential in this domain.\n- A substantial drop in joint metric scores for explanation quality indicates the need for more robust datasets with imagery to improve model performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To assess the quality and reliability of textual explanations generated for visual metaphors in the HAIVMet dataset.",
            "experiment_process": "1. Generate candidate explanations using ChatGPT (gpt-3.5-0914) for each visual elaboration, claim, and label. 2. Employ three expert annotators specializing in figurative language and visual metaphor understanding to verify and refine these explanations and select the most faithful visual metaphor for each claim. 3. After quality control, finalize 857 instances with verified explanations.",
            "result_discussion": "Through expert verification and strict quality control, the ablation ensures that the generated explanations are accurate and high-quality for 857 instances, illustrating the necessity of expert intervention for maintaining data quality.",
            "ablation_id": "2405.01474v1.No1"
        },
        {
            "research_objective": "To enhance and verify the quality of explanations for figurative expressions in the IRFL dataset.",
            "experiment_process": "1. Automatically select challenging contradicting image instances using CLIP for figurative expressions. 2. Generate textual explanations using GPT-4 (gpt-4-vision-preview). 3. Have three expert annotators review and refine these explanations, ensuring fidelity and discarding noisy data. 4. Finalize 1149 instances with verified explanations after strict quality control.",
            "result_discussion": "Post expert verification, the study achieves high-quality, adequately explained instances. This process underscores the importance of expert curation in filtering and improving automatic explanations.",
            "ablation_id": "2405.01474v1.No2"
        },
        {
            "research_objective": "To improve the quality of textual explanations for sarcastic claims in the MuSE dataset.",
            "experiment_process": "1. Generate entailment claims using GPT-4 by converting sarcastic claims into their non-sarcastic form. 2. Re-write low-quality crowdworker explanations with high-quality candidates using GPT-4. 3. Validate the explanations using the same three expert annotators, who check for completeness, correctness, and conciseness, and discard noisy examples. 4. Produce a final set of 1,042 verified instances.",
            "result_discussion": "The refined explanations achieve higher quality through expert verification, leading to a reliable dataset free from noisy examples and ensuring informative and accurate explanations.",
            "ablation_id": "2405.01474v1.No3"
        },
        {
            "research_objective": "To generate and verify textual explanations for memes to facilitate their understanding in entailment tasks using the MemeCap dataset.",
            "experiment_process": "1. Generate claims from meme captions using GPT-4 and filter for entailment using additional GPT-4 prompts. 2. Create counterclaims to ensure a diverse set of entailment and contradiction examples. 3. Generate explanations for the images and claims using GPT-4. 4. Have experts verify the correctness and completeness of generated claims and explanations, discarding inappropriate samples. 5. Obtain 1958 thoroughly verified instances post quality control.",
            "result_discussion": "The resultant dataset includes high-quality, verified examples of memes with claims and explanations. This involves rigorous expert validation to ensure that data is both appropriate and accurately explained.",
            "ablation_id": "2405.01474v1.No4"
        }
    ]
}