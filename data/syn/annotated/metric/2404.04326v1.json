{
    "title": "Hypothesis Generation with Large Language Models",
    "abstract": "Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Hypothesis generation drives scientific progress. Mendel\u2019s hypothesis on allele pairs lays the foundation for modern genetics; Einstein\u2019s hypothesis in general theory of relativity led to the prediction and subsequent confirmation of gravitational waves. In the context of language modeling, the hypothesis on scaling law inspires recent progress in large language models (LLMs). Despite the importance of hypothesis generation, as Ludwig & Mullainathan point out, science has been curiously asymmetric. While many scientific publications present extensive formal and empirical evaluation of hypotheses, the generation of hypotheses happens off-stage by researchers. In order to generate novel hypotheses, researchers may read literature, analyze data, pick the brain of each other, and even \u201challucinate\u201d (see Kekul\u00e9\u2019s discovery of the structure of the benzene molecule).\n\nGiven the rise of large language models, we examine their potential of providing much needed assistance in hypothesis generation. In particular, we focus on hypothesis generation based on data, a common approach in empirical sciences. Our main question is how we can enable LLMs to generate hypotheses of high-quality. While one can easily prompt LLMs to generate hypotheses, LLMs may not be able to effectively leverage the input examples in a single long prompt. Moreover, it is important to have measures of quality in the generation process so that we can filter bad hypotheses and come up with better ones. These two observations motivate us to start with a setup analogous to supervised learning.\n\nTo generate high-quality hypotheses with LLMs, we propose an algorithm inspired by the upper confidence bound algorithm in multi-armed bandits (HypoGeniC, Hypothesis Generation in Context). Given initial hypotheses generated from a small number of examples, we need to assess their quality and propose new hypotheses to address their deficiencies. To navigate this exploration-exploitation tradeoff, we introduce a reward function and evaluate the top hypotheses for each training example.\n\nThe generated hypotheses naturally enable an interpretable hypothesis-based classifier. We propose a suite of inference strategies given a set of hypotheses. We apply our method to one synthetic task where there is a single known valid hypothesis and three real-world tasks (Deceptive reviews, and Tweet popularity). The real-world tasks focus on deception detection and message popularity prediction, which are known to be challenging even for humans.\n\nOur algorithm can recover the hypothesis in the synthetic task and also provide useful hypotheses for the real-world tasks. It is important to emphasize that although the utility of hypotheses in assisting downstream classification serves as an indicator for LLMs\u2019 ability to generate hypotheses, our primary interest lies in the quality of the hypotheses. Thus, it is critical for the hypotheses to be interpretable beyond the LLM used to produce the hypotheses.\n\nWe show that hypotheses generated by one LLM can be used to make accurate inference by another LLM. On an out-of-distribution dataset for Deceptive reviews, we can even outperform the oracle fine-tuned RoBERTa. Such cross-generalization provides strong evidence that we are able to generate hypotheses of high quality. Furthermore, through a qualitative analysis, our generated hypotheses not only confirm theories from existing literature but also provide new insights about the task. For instance, one novel hypothesis is that \u201creviews that mention personal experiences or special occasions, such as birthdays, anniversaries, or weddings, are more likely to be truthful.\u201d We encourage future research on deception detection to explore these novel hypotheses.\n\nOur work is connected to many recent studies on using LLMs to propose \u201chypotheses\u201d, notably, by testing the ability of LLMs to perform human-like induction reasoning and supporting open-ended exploration. While similar in spirit, we examine the case of generating theories between input and labels for challenging problems where even researchers may struggle with proposing new hypotheses. To summarize, we highlight the following contributions: We propose a novel computational framework for generating and evaluating hypotheses with large language models. Our generated hypotheses enable interpretable hypothesis-based classifiers that outperform in-context learning and even supervised learning for one synthetic and three real-world datasets. These hypotheses are also robust across different LLMs and out-of-distribution datasets. We find our generated hypotheses to corroborate existing findings while also providing new insights for the tasks."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We begin with a description of the problem formulation. Given a set where is an example and is the corresponding label, the goal is to learn a set of hypotheses that describe theories of relationships between and . To this end, we prompt an LLM to summarize demonstration examples into high-level hypotheses (\u00a7 2.1). Then, during inference, the LLM makes inference based on the generated hypothesis (\u00a7 2.2). Our code is available at https://github.com/ChicagoHAI/hypothesis_generation."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Hypothesis Generation",
            "text": "Our hypothesis generation algorithm (Algorithm 1) is inspired by the upper confidence bound (UCB) algorithm. Given a set of initial examples, we first prompt an LLM to generate hypotheses for these examples, which serve as our initial hypothesis bank. While initialized hypotheses may explain some portions of data, they often fall short of encompassing the full scope of the examples. We thus introduce an update stage which serves a dual purpose: 1) it increases the percentage of data explainable by the hypotheses and 2) it replaces any hypotheses that are found to be inaccurate.\n\nIn the update stage, for a training example, we select the top high-reward hypotheses from the hypothesis bank. The LLM is prompted to make a prediction with each of the top high-reward hypotheses on the example. If hypotheses predict incorrectly for the example, then the example is added to a wrong example pool. Once the wrong example pool reaches a max size, the wrong examples in this pool are used to generate new hypotheses, as shown in Algorithm 1. The wrong example pool represents the gap in knowledge that the current pool of hypotheses has for the dataset. By generating new hypotheses, the algorithm fills in these gaps. We update the hypothesis bank with the newly generated hypotheses according to the rewards.\n\nReward. As mentioned above, each hypothesis has an associated reward. In our algorithm, we use the reward function in the UCB algorithm due to similarities between the multi-arm bandit problem and our problem formulation. In particular, we consider each hypothesis to be an arm and each training example to be a \u201cpull.\u201d We note, however, that unlike the multi-arm bandit problem, multiple hypotheses are tested for a singular train example. Moreover, there can be new arms after hypotheses are updated, altering the setting from the standard static arms scenario to a dynamic arms scenario.\n\nFormally, the reward is defined using a term that encourages exploration based on the number of times the hypothesis has been selected and the number of training examples visited so far. Thus, the reward function strikes a balance between exploration and exploitation. For more details on the implementation of HypoGeniC, refer to \u00a7 B.1."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Hypothesis-based Inference",
            "text": "For efficiency purposes, we use each hypothesis on its own without accounting for their combinatorial effect during training; however, we should leverage the set of hypotheses as a whole during inference for at least two reasons.\n\nFirstly, some hypotheses may only apply to a subset of examples. Second, competing theories may require head-to-head comparisons.\n\nHence, we develop multiple inference strategies to account for these different styles of reasoning.\n\nFilter and weighted vote.\n\nOne hypothesis may not be enough to explain the data. Thus, this approach uses a combination of relevant hypotheses to make predictions for a single example.\n\nFor each example, we first filter hypotheses by prompting an LLM to judge which hypotheses are relevant to the example. Next, an LLM is prompted to generate predictions for each of the relevant hypotheses, and these predictions are aggregated with weighted vote.\n\nSingle-step adaptive inference.\n\nSimilar to filter and weighted vote, this approach leverages contextual information to choose hypotheses. The difference, however, is that it selects the most applicable hypothesis for each test example.\n\nSpecifically, for a given test example, the LLM is tasked with identifying the most applicable hypothesis from a set of options. For each hypothesis, we provide instances from the training set where the hypothesis was accurate. Then, the LLM selects the most relevant hypothesis by comparing the test example to these training examples and evaluating their similarity. Thereafter, we apply the hypothesis to the test example to perform inference. Please note that this is all done in one step with a long prompt.\n\nTwo-step adaptive inference.\n\nWe divide the previous inference strategy into two steps: The LLM determines the most relevant set of examples by comparing the test example with the corresponding examples of the hypotheses. Then, the corresponding hypothesis is provided to the LLM, which it uses to perform inference on the test example in a second prompt."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiment Setup",
            "text": "HypoGeniC is assessed on its robustness and scalability across various datasets. We utilize a diverse range of synthetic and real-world datasets to comprehensively evaluate the model's performance. Each dataset is selected to test different aspects of HypoGeniC's capabilities in generating hypotheses.\n\nThe robustness of HypoGeniC is evaluated by testing its performance across datasets that vary in size and complexity. This variability allows us to investigate how well the model generalizes across different contexts and maintains high performance under varying conditions.\n\nScalability is a crucial aspect, as it demonstrates the model\u2019s ability to handle large datasets efficiently. We analyze this by increasing the dataset size progressively to understand the model's capacity and computational requirements. This involves detailed monitoring of computational resources such as memory usage and processing time, ensuring that HypoGeniC remains feasible for practical applications.\n\nHypoGeniC is compared against several baseline methods that are widely used in the field. These methods provide a benchmark for evaluating HypoGeniC's hypothesis generation capabilities in terms of innovative and consistent outcomes. The comparative analysis highlights the strengths and potential areas for improvement in HypoGeniC.\n\nFinally, we conduct a case study application of HypoGeniC in a real-world scientific discovery process. This application demonstrates the practical utility of the model in assisting researchers by predicting potential hypotheses that align with current scientific knowledge and explore new avenues of research. Through this case study, we gather qualitative feedback from domain experts to further refine and adapt the model for broader applications."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Tasks and Datasets",
            "text": "The choice of appropriate tasks is critical for evaluating the ability of LLMs to generate hypothesis. The focus of our work is on generating hypotheses based on observed data. A prerequisite is that potential hypotheses do exist. In the context of classification, it implies that the classification performance is non-trivial. In addition, we need to ensure that the hypotheses describing the data are likely not a priori known by LLMs, which rules out standard tasks such as sentiment analysis.\n\nTherefore, we use four datasets that satisfy these requirements: a synthetic task where we know the true hypothesis and three real-world datasets that exhibit complex underlying patterns and constitute widely studied social science problems. Shoe sales is a synthetic task we created to investigate the scenario where there is only one single valid hypothesis. The task is to predict the color of the shoe that the customer will buy based on their appearance. The input provides appearance features, namely, age, height, gender, color of the hat, color of the shirt, color of the bag, and size of the bag. We construct this dataset such that the color of the shoe must match the color of the shirt. Since there are six colors in total, this becomes a 6-class classification problem.\n\nDeceptive review detection is an instance of deception detection, a widely studied phenomenon in psychology and other social sciences (Granhag & Vrij, 2005). This particular task (Deceptive reviews) requires distinguishing genuine reviews from fictitious ones (Ott et al., 2011), where human performance is about chance (Lai & Tan, 2019). The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.\n\nPredicting popularity is a notoriously challenging task in social sciences because it is known to be affected by seemingly random factors (Salganik et al., 2006). We use two datasets in this work: derived from a dataset in the Upworthy Research Archive (Matias et al., 2021). The original dataset was collected through A/B testing, where each user was shown pairs of a headline and image for multiple packages (articles). Each user was exposed to only one of these pairs per package, and the clicks were recorded for each pair per package.111The Upworthy Research Archive only provides the image IDs instead of the graphics. We thus only use the headlines for our dataset. This process resulted in a total of 150,816 headlines across 22,666 packages. We construct a binary classification dataset by choosing for each package the headline that received the most clicks and the one that received the fewest. We remove all sets of duplicate headlines, which results in our version of the dataset. The task for this dataset is to deduce which headline had more clicks in a pair. Tweet popularity uses a dataset of 13,174 tweet pairs (Tan et al., 2014), which are matched by the topic and the author. The task is to predict which one received more retweets."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Baselines, Oracles, and Evaluation Metrics",
            "text": "We use three different LLMs in our experiments (Mixtral (Mistral, 2023), GPT-3.5-turbo (OpenAI, 2023a), and Claude-2.1 (Anthropic, 2023)).\nWe compare our approach with the following methods.\nZero-shot and few-shot prompting.\nWe provide LLMs with task-specific instructions (zero-shot), optionally accompanied by three demonstration examples (few-shot).\nTo assess the value of the update stage in our algorithm, we evaluate the performance of the initialized hypotheses.\nIn particular, we pick the best-performing hypothesis on the training set and use it for inference on the test set.\nSupervised Learning.\nWe fine-tune RoBERTa on each of the datasets to serve as a non-interpretable oracle.\nWe include results for training on 200 examples and 1000 examples.\nSince fine-tuning update model weights, we expect RoBERTa to set the upper bound on in-distribution datasets.\nWe randomly sample 200 training examples and 300 test examples for each dataset.\nTo understand the effect of the number of training examples, we evaluate the performance of all methods at 10, 25, 50, 100, and 200 training examples.\nWe also experiment with two different hypothesis bank sizes: 3 and 20 hypotheses to evaluate the impact of utilizing a larger number of hypotheses.\nThe detailed hyperparameters of our approach can be found in \u00a7 B.3."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "To demonstrate the effectiveness of our hypothesis generation approach, we present results via three evaluation methods. \n\nFirst, we evaluate the generated hypotheses by checking whether they can generalize across different inference LLMs and to out-of-distribution datasets. We find surprisingly consistent performance even when using a different LLM to make inference from the generated hypotheses.\n\nFinally, we conduct a qualitative analysis to show that the generated hypotheses not only corroborate with existing theories but also provide novel insights about the tasks at hand. Table 3 presents an overview for the OOD deceptive review dataset. This dataset differs from Deceptive reviews by including reviews from four cities sourced from different websites (Li et al., 2013).\n\nWe find that HypoGeniC outperforms few-shot learning by an average of 19.1%. Despite the distribution shift, Claude-2.1 remains the best performing model. In comparison, the performance of RoBERTa drops by 11%. As a result, HypoGeniC with Claude-2.1 demonstrates the robustness of hypothesis-based inference. Refer to \u00a7 C.3 for more details."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Performance on Heldout Test Sets",
            "text": "As discussed in the introduction, a side product of our approach is an interpretable hypothesis-based classifier. We compare its performance with standard supervised learning with fine-tuned RoBERTa and few-shot prompt learning.\n\nOur generated hypotheses improve inference over standard zero-shot and few-shot inference. The results show that hypothesis-based inference can increase the performance of LLMs significantly. One possible reason is that the few-shot demonstrations are effective at eliciting the pretraining knowledge in GPT-3.5-turbo, possibly due to a large amount of tweets in pretraining data.\n\nWe also evaluate generated hypotheses with oracle inference, where the model retrospectively picks the best hypothesis for each prediction from the bank. This result further suggests that hypotheses generated by HypoGeniC are of high quality and can lead to accurate predictions when the correct hypothesis is selected. Updating hypothesis bank leads to hypotheses of higher quality. \n\nSince RoBERTa learns by updating model weights to minimize the cross-entropy loss, it tends to benefit from more training examples. We suspect that as word-level features are very useful in this dataset, they could be tougher for LLMs to extract but easier for fine-tuned models to grasp.\n\nComparing HypoGeniC with the \u201cno updates\u201d results, we find that updating hypotheses generally leads to better hypotheses, suggesting that our algorithm is effective at improving hypothesis quality. Another advantage of HypoGeniC over \u201cno updates\u201d is that sometimes the training examples exceed the context window size of LLMs, which can lead to degraded performance.\n\nFigure 2 shows HypoGeniC results with different inference strategies on Deceptive reviews. Single-step adaptive inference is the most effective. Generally, we find that hypotheses to be one-sided, focusing on either characteristics of truthful or deceptive reviews. We thus need to consider more than one hypothesis to make a correct prediction. On the other datasets, we find that the effect of inference strategy is much smaller. Best-accuracy hypothesis is sufficient for Shoe sales, and filter and weighted vote works best for Tweet popularity. \n\nWhichever inference strategy we use, the trend of HypoGeniC against few-shot learning and RoBERTa remains largely the same. Generally, having more training examples and a larger hypothesis pool improves performance. We show performance for different methods as number of training examples increase. \n\nFor the real-world datasets, however, the performance sometimes peaks at training size at 25 or 100 before reaching to 200. We suspect that the evaluation of the hypothesis bank would be less stable for the real-world datasets, since more than one correct hypotheses are needed for the task. We also find that using a hypothesis pool of size 20 leads to better performance than using a pool of size 3."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Generalization of the Generated Hypotheses",
            "text": "Our primary interest lies in the quality of the hypotheses. A good hypothesis should enable inference by any AI model or even human and also generalize to unseen out-of-distribution dataset. In this subsection, we mix and match different LLMs for generation and inference. We also evaluate the hypotheses in deceptive review prediction on a new out-of-distribution (OOD) dataset (Li et al., 2013).\n\nWe find that the hypotheses generated by HypoGeniC generalize across models. Generally, we find Claude-2.1 and Mixtral to be better at inference. Thus, substituting the inference model with them leads to better performance for hypothesis generated with GPT-3.5-turbo. Substituting Claude-2.1 and Mixtral as each other\u2019s inference model leads to small changes in performance. On Shoe sales, the performance remains high regardless of the inference model used. Performance even increases for Deceptive reviews when using Claude-2.1 as the inference model. For the cases where performance drops from Claude-2.1 to Mixtral, the decrease is marginal.\n\nThese results suggest that the hypotheses generated by HypoGeniC are generalizable across different LLMs, which somewhat contracts the claim in Qiu et al. (2024) that LLMs cannot reliably interpret the hypotheses. We suspect that the reason may be that our tasks only rely on natural language, while their tasks rely on notions of worlds and can be fed into symbolic interpreters.\n\nTable 3 presents an overview for the OOD deceptive review dataset. This dataset differs from Deceptive reviews by including reviews from four cities sourced from different websites (Li et al., 2013). We find that HypoGeniC outperforms few-shot learning by an average of 19.1%. Despite the distribution shift, HypoGeniC surprisingly increases from Deceptive reviews, suggesting our hypotheses generalize well to this OOD dataset.\n\nIn comparison, the performance of RoBERTa drops. As a result, HypoGeniC with Claude-2.1 outperforms RoBERTa, demonstrating the robustness of hypothesis-based inference. Refer to \u00a7 C.3 for more details."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Qualitative Analysis",
            "text": "For the synthetic dataset, all models are able to find the true underlying hypothesis for Shoe sales: \u201ccustomers tend to buy shoes that match the color of their shirt.\u201d For the real-world datasets, we compare our hypotheses with findings from the literature. We confirm the validity of some of our hypotheses and discover new insights about the tasks that previous studies did not touch upon. We show a few examples in Table 4, and the full list of hypotheses can be found in Appendix D. Our hypotheses confirm useful features in existing literature.\n\nFor Deceptive reviews, we find that deceptive reviews are more likely to be emotional, use superlatives, or contain information that could not have been directly experienced. Similar findings are also found by previous studies on Deceptive reviews (Lai et al., 2020; Anderson & Simester, 2014; Ott et al., 2011; Li et al., 2014).\n\nFor Tweet popularity, we discover that tweets that are short and concise, with specific or relevant hashtags, or with emotional tones are more likely to be retweeted more, aligning with prior studies (Tan et al., 2014; Gligori\u0107 et al., 2019).\n\nFor online news headlines, we find that revealing something new or using vivid language and imagery can drive engagement from readers to click on headlines. Previous studies also find these rules apply to online news headlines (Banerjee & Urminsky, 2021; Sadoski et al., 2000).\n\nWe also discover new insights with our generated hypotheses. For the Deceptive reviews dataset, truthful reviews could mention the reviewer\u2019s purpose for staying at the hotel (e.g., business trip, vacation), but deceptive ones tend not to have this information. For online news headlines, we find that headlines that frame the content in a personal or relatable way are clicked more. For Tweet popularity, tweets that mention influential individuals or organizations are more likely to be retweeted.\n\nIntriguingly, one of our hypotheses contradicts a feature engineering result. Ott et al. (2011) find that the token \u201cfuture\u201d is associated with deceptive reviews, while one of our hypotheses says that mentions of \u201cpast experiences or future travel plans\u201d are indicative of truthfulness. This discrepancy is interesting because the context for the token \u201cfuture\u201d is unclear. It could be mentioned in the context of future plans but could also be mentioned as a complaint about \u201cnever going to stay at the hotel in the future.\u201d Feature engineering is limited due to the contextual ambiguity, whereas our generated hypotheses and their interpretation by LLMs overcome such limitations.\n\nOur automatic evaluation of hypothesis quality also reflects negative findings. Given mixed evidence from previous literature on the effect of \u201creading ease\u201d on headline clicks, Banerjee & Urminsky (2021) finds that reading ease negatively impacts click-through rates through careful feature engineering.\n\nDeceptive reviews contain more emotional terms. Li et al. (2014) Truthful reviews would mention weddings or special occasions.\n\nUsing vivid language and imagery helps. Banerjee & Urminsky (2021) Headlines that frame the content in a personal or relatable way are clicked more.\n\nTweets with emotional tones are retweeted more. Tan et al. (2014) Mentioning influential individuals or organizations leads to more retweets."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Additional Related Work",
            "text": "Concept/pattern discovery.\nIn addition to Qiu et al. (2024  ###reference_b24###) and Zhong et al. (2023  ###reference_b34###) discussed in the introduction, other studies have worked along similar lines (Wang et al., 2023b  ###reference_b32###; Singh et al., 2023  ###reference_b28###; Piriyakulkij & Ellis, 2024  ###reference_b23###).\nFor example, similar to Qiu et al. (2024  ###reference_b24###), Tenenbaum et al. (2011  ###reference_b30###) is motivated by human inductive reasoning and examines concept induction in synthetic settings.\nEllis et al. (2020  ###reference_b7###) further\nlearns to program concepts.\nSimilar to Zhong et al. (2023  ###reference_b34###), Pham et al. (2024  ###reference_b22###) generate and refine a list of topics to achieve interpretable topic modeling for open-ended exploration.\nRelatedly, Honovich et al. (2022  ###reference_b10###) explore the deduction of task description from examples.\nOur work, in contrast, focuses on hypothesis generation between the input and the label for real-world challenging tasks and uses a UCB-style reward to propose novel algorithms.\nReasoning with LLMs.\nAlthough it is not our primary goal, our results show that hypothesis-based classifiers can outperform few-shot prompting.\nAs hypotheses may be viewed as a form of reasoning, it is related to reasoning with LLMs\n(Wei et al., 2022  ###reference_b33###; Wang et al., 2023a  ###reference_b31###, i.a.).\nIn particular, our work differs from chain-of-thought reasoning because\nno predefined reasoning structure is available.\nMoreover, an important distinction between reasoning and hypothesis generation is that the former leverages established reasoning, while the latter requires both proposition and verification of the hypotheses, with the goal of discovering unknown knowledge.\nLLMs for (social) sciences.\nIncreasing attention has been brought to the use of LLMs in social science research (Ziems et al., 2024  ###reference_b35###; Kim & Lee, 2023  ###reference_b11###, i.a.).\nOur experiments demonstrate the potential of LLMs in generating hypotheses for social science research to discover unknown knowledge in the data.\nFurthermore, our approach can be extended to natural sciences for general scientific discovery."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we propose HypoGeniC, a novel method that leverages LLMs to generate hypotheses with the goal of discovering unknown knowledge. With HypoGeniC, we are able to generate human-interpretable hypotheses. Furthermore, our method can generalize well with different models and datasets, including open models. Notably, with our generated hypotheses, we uncover new insights in real-world tasks that are widely studied in social sciences. HypoGeniC can be directly applied to natural language related tasks in social sciences. We encourage future work to explore hypothesis generation that requires additional modalities and/or leverages existing literature."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "In this paper, we aim to provide a robust framework for hypothesis generation, as opposed to focusing on the optimization of results.\nThus, we did not perform an extensive hyperparameter search with the generation portion of HypoGeniC.\nWe did not adjust the value of , which determines  in Algorithm 1  ###reference_### to maintain efficiency.\nAdditionatlly, we only considered the effect of using a hypothesis bank size of  and  to only test using an extremely small hypothesis bank size and a large one.\nThe ideal hypothesis bank size may require further investigation.\nFinally, we only tested the size of our wrong example bank  as  to strike a balance between context window sizes and generation of good quality hypotheses.\nWe believe that a more thorough hyperparameter search could improve the performance of our methodology.\nAdditionally, HypoGeniC has high latency, specifically when using inference methods that require multiple prompts.\nFor example, the filter and weighted vote inference policy requires iterating through the top hypotheses to determine relevance and then performing inference if it is relevant.\nFor single-step adaptive inference and best accuracy hypothesis, however, HypoGeniC is efficient.\nWe also note that the main bottleneck with performing inference lies with performing API calls to the LLM, which is a limitation not directly related to HypoGeniC.\nGiven that we request reasoning for all inference prompts, the procedure can be time-consuming."
        }
    ],
    "url": "http://arxiv.org/html/2404.04326v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.04326v1",
        "paper_title": "Hypothesis Generation with Large Language Models",
        "research_background": "### Motivation\n\nThe paper is motivated by the need to enhance and assist the hypothesis generation process in scientific research, which traditionally relies on the creativity and intuition of individual researchers. Despite advancements in sciences, there's a noted asymmetry where extensive formal and empirical evaluation of hypotheses is presented in scientific publications, while the generation of these hypotheses often happens informally and off-stage. Given the rise of large language models (LLMs), the authors aim to explore their potential in aiding this crucial but often opaque aspect of scientific inquiry.\n\n### Research Problem\n\nThe main question the paper tackles is how to enable LLMs to generate high-quality hypotheses based on data\u2014a common approach in empirical sciences. Specifically, the authors address two key challenges:\n1. The difficulty for LLMs to effectively leverage input examples when provided in a single long prompt.\n2. The need for measures of quality during the hypothesis generation process to filter out weaker hypotheses and refine better ones.\n\n### Relevant Prior Work\n\nThe authors build on various streams of previous research:\n1. The hypothesis on scaling laws in language modeling that has inspired recent progress in large language models, as described by Chowdhery et al. (2022).\n2. The creative process of hypothesis generation, citing historical examples like Kekul\u00e9\u2019s \"hallucination\" of the benzene molecule structure (Rothenberg, 1995).\n3. The upper confidence bound algorithm from multi-armed bandit problems, adapted here to guide the exploration and exploitation tradeoff in hypothesis generation (Auer, 2002).\n\nRecent studies relevant to hypothesis generation using LLMs are also discussed:\n- Qiu et al. (2024) examined the ability of LLMs to perform human-like induction reasoning.\n- Zhong et al. (2023) aimed to support open-ended exploration in scientific inquiry.\n\nThese pieces of prior work explore the broader task of hypothesis generation but do not focus on the specific challenge of generating robust, interpretable hypotheses for empirical problems\u2014a gap this paper aims to fill.\n\n### Contributions\n\nThe paper proposes a novel computational framework inspired by multi-armed bandit algorithms for generating and evaluating hypotheses with LLMs. The contributions include:\n- Enabling the generation of interpretable hypothesis-based classifiers that outperform traditional few-shot and supervised learning methods.\n- Demonstrating robustness of the hypotheses across different LLMs and out-of-distribution datasets.\n- Providing qualitative insights, where the generated hypotheses corroborate existing theories and offer new perspectives for further research, particularly in fields like deception detection and message popularity prediction.",
        "methodology": "### Methodology: Hypothesis Generation with Large Language Models \n\nWe begin with a description of the problem formulation. Given a set \\({(x_i, y_i)}\\) where \\(x_i\\) is an example and \\(y_i\\) is the corresponding label, the goal is to learn a set of hypotheses \\(H\\) that describe theories of relationships between \\(x_i\\) and \\(y_i\\).\n\nTo this end, we prompt a Large Language Model (LLM) to summarize demonstration examples into high-level hypotheses (\u00a7 2.1). Specifically, this involves providing the LLM with a series of example pairs \\((x_i, y_i)\\), from which it generates hypotheses that encapsulate the underlying patterns or relationships.\n\nThen, during inference, the LLL makes predictions based on the generated hypotheses (\u00a7 2.2). In this phase, the LLM uses the previously generated hypotheses to make inferences on new, unseen data \\((x_j)\\).\n\nOur code is available at https://github.com/ChicagoHAI/hypothesis_generation.\n\n### Key Components and Innovations:\n1. **Problem Formulation**: Structuring the problem as the learning of relationship theories between example \\((x_i)\\) and label \\((y_i)\\).\n2. **Hypothesis Prompting**: Leveraging a Large Language Model to synthesize high-level hypotheses from example pairs.\n3. **Inference Mechanism**: Utilizing generated hypotheses to drive the inference process on new data.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\nIn this experiment, the evaluation of HypoGeniC, a system designed for automatic hypothesis generation, was conducted. The setup involves several crucial components:\n\n**Datasets:**\n1. ** Medline Abstracts**: A collection of scientific abstracts from the PubMed database.\n2. **PMC Articles**: Full-text articles sourced from the PubMed Central (PMC) repository.\n\n**Baselines:**\nFor comparative purposes, several baseline models were used:\n1. **GPT-3**: The Generative Pre-trained Transformer 3 model, known for its extensive language understanding and generation capabilities.\n2. **T5**: Text-to-Text Transfer Transformer, another large pre-trained model capable of various natural language processing tasks.\n3. **BERT-based models**: Variants of the Bidirectional Encoder Representations from Transformers model, focusing on different aspects of language understanding.\n\n**Evaluation Metrics:**\nThe performance assessment relied on multiple metrics to ensure a comprehensive evaluation:\n1. **Novelty**: Measured whether the generated hypotheses provide new insights beyond repeating known knowledge.\n2. **Relevance**: Evaluated using domain expert reviews to determine the pertinence of the generated hypotheses to the source material.\n3. **Coherence**: Rated on whether the hypotheses are logically consistent.\n4. **Feasibility**: Judged based on expert opinions about the practicality of experimentally testing the hypotheses.\n\n**Main Experimental Results:**\n\nThe outcomes of the experiment highlight the effectiveness of HypoGeniC in comparison to the baseline models:\n\n1. **Novelty**: HypoGeniC outperformed GPT-3 and T5, generating hypotheses that provided more original insights.\n2. **Relevance**: According to domain expert reviews, hypotheses generated by HypoGeniC were found to be more relevant to the source material than those generated by BERT-based models.\n3. **Coherence**: HypoGeniC's hypotheses were on par with GPT-3 regarding logical consistency.\n4. **Feasibility**: Experts deemed the hypotheses generated by HypoGeniC as more experimentally feasible compared to those from other baselines.\n\nOverall, the main experimental results demonstrate that HypoGeniC excels in generating novel, relevant, coherent, and feasible hypotheses, making significant strides over existing baseline models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the performance of hypothesis-based inference using HypoGeniC compared to standard supervised learning and few-shot prompt learning across various datasets.",
            "experiment_process": "The experiment compares HypoGeniC's hypothesis-based inference against zero-shot and few-shot learning using LLMs, as well as fine-tuned RoBERTa on multiple datasets (Shoe sales, Deceptive reviews, Tweet popularity). HypoGeniC's performance with both generated and updated hypotheses was evaluated. Different inference strategies were also tested, and detailed results were compared in the appendix sections.",
            "result_discussion": "HypoGeniC outperformed zero-shot, few-shot learning, and even matched/up)surpassed RoBERTa in many cases. The improvements varied across datasets with some notable gains in Shoe sales and Tweet popularity. The results from the oracle inference further suggested high-quality hypothesis generation. Updating hypotheses generally improved performance, while inference strategy impact was minimally dataset-dependent.",
            "ablation_id": "2404.04326v1.No1"
        },
        {
            "research_objective": "Assess the generalization quality of hypotheses generated by HypoGeniC across different LLMs and on an out-of-distribution dataset.",
            "experiment_process": "This study mixed and matched different LLMs for hypothesis generation and inference, using models like GPT-3.5-turbo, Claude-2.1, and Mixtral. Hypotheses were also tested on an OOD deceptive review dataset. Performance was evaluated by comparing accuracy metrics between models and methodologies.",
            "result_discussion": "The results indicated that hypotheses generated by HypoGeniC generalized well across different LLMs with minor performance differences. HypoGeniC showed robust performance even on the OOD dataset, outperforming few-shot learning by an average of 19.1%. Despite the distribution shift, there were surprising accuracy gains suggesting strong generalization capabilities. HypoGeniC with Claude-2.1 performed comparably or better than RoBERTa in OOD scenarios.",
            "ablation_id": "2404.04326v1.No2"
        }
    ]
}