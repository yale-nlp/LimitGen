{
    "title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
    "abstract": "In this paper, we introduce a novel psychological benchmark, CPsyExam, constructed from questions sourced from Chinese language examinations. CPsyExam is designed to prioritize psychological knowledge and case analysis separately, recognizing the significance of applying psychological knowledge to real-world scenarios.\nFrom the pool of 22k questions, we utilize 4k to create the benchmark that offers balanced coverage of subjects and incorporates a diverse range of case analysis techniques.\nFurthermore, we evaluate a range of existing large language models (LLMs), spanning from open-sourced to API-based models. Our experiments and analysis demonstrate that CPsyExam serves as an effective benchmark for enhancing the understanding of psychology within LLMs and enables the comparison of LLMs across various granularities.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The evaluation of language models has been an important topic with sustained vitality in the natural language processing community. With the development of pretrained language models, such as GPT and BERT, their increasing abilities in executing a range of different linguistic tasks in different domains call for more challenging and inclusive settings with comprehensive human baselines. Recently, researchers have constructed a series of benchmarks, such as GLUE, SuperGLUE, and CLUE, in order to evaluate natural language understanding (NLU) tasks. As more and more models show better performance on these tasks than humans, benchmarks that have massive multi-task components based on real-world exams are constructed to comprehensively assess the abilities of LLMs.\n\nThere has been a new trend of constructing diverse benchmarks focusing on different abilities and knowledge across different domains. With the increasing adoption of LLMs in psychological counselling and mental health support, there\u2019s an urgent need for a psychological evaluation benchmark to measure to what extent current LLMs understand psychological knowledge. As far as we are concerned, the domain of psychology in Chinese is still overlooked by existing benchmarks. First and foremost, not all benchmarks for LLMs encompass knowledge of psychology, and those that do offer inadequate coverage. For example, CMMLU only has one subject related to psychology, and CEVAL does not even include psychology-related subjects.\n\nSecondly, although there have been concurrent works like PsyBench and PsyEval, the questions in these benchmarks are either automatically generated by LLMs or of limited size. For example, PsyEval constructs Mental Health QA of 726 questions from MedQA through keyword matching and manual screening. PsyBench is constructed from GPT-4 and focuses on the balance of knowledge but is limited by the size of the dataset.\n\nIn this paper, we construct a large-scale psychological evaluation benchmark, CPsyExam, from a series of Chinese examinations that contain psychology subjects. Due to the specificity of the psychology domain, in order to comprehensively address the ability of LLMs in understanding psychological cases, we then divide CPsyExam into two parts: (1) Knowledge (KG) contains factoid-oriented questions with a wider coverage of psychology concepts from real examinations for professional counsellors, and (2) Case Analysis (CA) includes case-oriented questions focusing on method, diagnosis, and treatment required during counselling.\n\nWe further compare the performance of recent general domain LLMs and psychological-specific LLMs on CPsyExam. Our experiments reveal that compared to the foundation models, these fine-tuned models exhibit marginal gains or no improvement in understanding psychological knowledge. In some cases, their ability to analyze cases may even be compromised. Evidently, LLMs still have room for improvement in terms of mastering psychological knowledge and applying it to psychological case analysis.\n\nCPsyExam serves as a valuable benchmark for advancing LLMs\u2019 understanding of psychology. Our work has the following contributions: We provide a comprehensive and balanced dataset of Chinese psychology examination questions. We propose a psychological assessment framework which includes the knowledge session and case-analysis session. We construct the benchmark and release the SFT data which contribute to the enhancement of psychological competence in the LLMs."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Datasets in Psychology Domain",
            "text": "In the psychological domain, language resources are abundant in mental health support in the format of conversations. For example, EmpathicReactions investigates peoples\u2019 reactions to news stories and distinguishes between multiple psychological properties of empathy. EmpathicConversations constructs dyadic (two person) text conversations of crowd workers about the news articles. The two datasets are further adopted for empathy prediction. EmpatheticDialogues contains conversations with each conversation having two people discussing a situation that happened to one of them, related to a given feeling (32 emotion labels). EDOS uses emotional dialogues from OpenSubtitles and adds annotations with 32 fine-grained emotions, eight empathetic response intents, and the Neutral category. To enhance understanding of how empathy is expressed in context, several works offer extra annotations from more psychological perspectives. MentalHealthSubreddits develops a framework for characterizing the communication of empathy in text-based conversations and annotates a subset of 10k interactions on empathy. PsyQA adopts different strategies for annotation to present in-depth analysis of both lexical features and strategy patterns in the counseling answers. ESConv proposes an ESC Framework (which is grounded on the Helping Skills Theory) and provides annotations for help-seekers\u2019 problems, emotions, feedback, and the support strategies. CHQ-SocioEmo contains a total of 1,500 questions and answers pairs and covers a range of social support categories, such as informational support, emotional support, esteem support, network support, and tangible support. There are also datasets constructed from rewriting existing conversations using LLMs to enhance their topic coverage and expressiveness. AugESC is an augmented dataset derived from the crowdsourced ESConv corpus, involving prompting a LLM to complete full dialogues. SMILE employs ChatGPT to extend public single-turn dialogues from PsyQA into multi-turn ones. Considering the privacy and security issues of psychology consultations, real counseling data between doctors and patients is still hard to access. CounselChat data origins from counselchat.com, a website where individuals can seek assistance from licensed therapists. \u201cReflections\u201d is a fundamental verbal skill employed by mental health counselors to convey understanding and recognition of the client\u2019s experiences and concerns. PAIR is a dataset consisting of reflections portraying different levels of reflective listening skills. However, these datasets are usually not released to the public. The distribution of datasets in the psychology domain is not well balanced as most of them are focusing on empathy. Although these resources can be used to evaluate the levels of empathy in LLMs, there is still an urgent need for the evaluation of their knowledge of psychology."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Benchmarks of Large Language Models",
            "text": "Benchmarks of large language models can be categorized based on the domains they are focusing on. There have been integrated evaluation benchmarks focusing on general domains as well as on specific domains. The evaluations usually adopt a diverse set of evaluation methods, such as zero-shot evaluation, few-shot evaluation, and chain-of-thought evaluation.\n\nIn the general domain, benchmarks typically comprehensively evaluate the scope and depth of a model\u2019s academic and professional knowledge, such as MMLU Hendrycks et al. (2021 ###reference_b14###), CMMLU Li et al. (2023 ###reference_b21###), CEVAL Huang et al. (2023 ###reference_b15###), BIG-Bench Srivastava et al. (2023 ###reference_b34###), HELM Liang et al. (2023 ###reference_b22###), M3KE Liu et al. (2023 ###reference_b24###), Xiezhi Gu et al. (2023 ###reference_b11###) and MMCU Zeng (2023 ###reference_b43###).\n\nIn specific domains, there are also benchmarks available focusing on the evaluation of expertise for LLMs. In the medical domain, various benchmarks have been proposed, such as webMedQA He et al. (2019 ###reference_b13###), NLPEC Li et al. (2020 ###reference_b20###), IMCS21 Chen et al. (2022 ###reference_b8###), and CMB Wang et al. (2023 ###reference_b39###). In the financial domain, there are FinanceBench Islam et al. (2023 ###reference_b16###), PiXiu Xie et al. (2023 ###reference_b41###) and FinEval Zhang et al. (2023b ###reference_b45###). In the legal domain, there are LexGLUE Chalkidis et al. (2022 ###reference_b6###) and LegalBench Guha et al. (2023 ###reference_b12###). There are also benchmarks integrating multiple high-quality open-source datasets from different domains, such as OpenCompass Contributors (2023 ###reference_b9###) and AGIEval Zhong et al. (2023 ###reference_b47###).\n\nHowever, in the psychological domain, despite the ongoing development of benchmarks like PsyBench Zhang et al. (2023a ###reference_b44###) and PsyEval Jin et al. (2023 ###reference_b18###), we still think there\u2019s a need to fill the gap using genuine questions crafted by experts and problems derived from psychological case analyses."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "We gather psychological data from publicly available resources and adopt taxonomy criteria specific to different subjects within the domain to further categorize the questions. In the realm of psychology, the significance of case studies is on par with that of psychological knowledge. Analyzing cases is often a testament to a practitioner\u2019s ability to apply their skills within the field of psychology. Consequently, our dataset encompasses two key components: one designed to evaluate the LLM\u2019s grasp of psychological knowledge (KG), and the other aimed at assessing the LLM\u2019s proficiency in case analysis (CA).\n\nThe benchmark needs to be designed to accurately reflect LLM\u2019s competence in the field of psychology within the Chinese context. Therefore, we anticipate that it could cover the majority of topics tested on the Chinese Psychology Examinations but with a balanced distribution of questions across various subjects. We identify a wide range of examinations that include psychology subjects under the Chinese examination system.\n\nDue to the advancement of psychology and initiatives by global organizations, there has been a growing awareness of mental health in developing countries, including China. Educationally, China has established an examination system for psychology that assesses the foundational psychological knowledge of practitioners from diverse occupations and provides professional certifications for individuals aspiring to enter the field of psychological counseling.\n\nBelow is a list of examinations for target groups:\n\nPCE (Psychological Counselor Examination) The professional qualification examinations for first, second, and third-tier psychological counselors are designed to assess the competence and knowledge of individuals working in the field of psychology. These exams are typically structured as a series of multiple-choice or essay questions that cover various aspects of psychology.\n\nTQE (Teachers\u2019 Qualification Examination) The Teachers\u2019 Qualification Examination is a standardized assessment for individuals who wish to teach psychology in primary, middle, and high schools, as well as in vocational schools and higher education institutions. This examination is designed to ensure that educators possess the necessary knowledge and skills to effectively teach psychology courses.\n\nGEE (Graduate Entrance Examinations) It contains a wide range of subjects like general psychology, social psychology, experimental psychology, contemporary educational psychology, psychology and educational measurement, developmental psychology, modern psychology, and educational statistics.\n\nSSE (Self-study Examination) The Self-study Examination is an examination that assesses an individual\u2019s knowledge and understanding of psychology, specifically in the areas of educational psychology, medical psychology, advertising psychology, and journalism psychology. This examination is typically taken by individuals who are self-studying or not enrolled in a formal educational program.\n\nwhich reflects the comprehensive examination system in China\u2019s psychology education. Based on the categorization of examinations in psychology, we crawl public available resources online to construct a database of questions. The websites for the data crawling include ExamCoo 111https://examcoo.com, StudyEZ 222http://www.studyez.com/psychology/, Hxter 333www.hxter.com, MXQE 444http://tk.mxqe.com and book corpus on GEE.\n\nWe collect our data from both websites and books. For the data scraped from websites, we use a parsing program to extract the questions, while for the data from books, we extract it manually, resulting in structured data in a uniform format. Afterward, we preprocess all the data to eliminate duplicates and questions with incorrect formatting. We also remove questions that contain image links and standardize the question format by removing question numbers and option letters. Finally, we manually validate the dataset to ensure that there are no apparent grammatical errors in the questions.\n\nCA questions can be distinguished from KG questions using some empirical methods like keywords matching. We align the taxonomy of CPsyExam-KG questions with the Chinese examination system for psychology. Then we select all the psychological subjects in each examination as a subcategory, the detailed directory list of which can be found in Appendix A.\n\nBased on psychological counselling conventions, we divided the CA questions into three categories: Method, Diagnosis, and Treatment. The Method category assesses the LLM\u2019s ability to use the corresponding methodology in a specific case. The Diagnosis category pays attention to the LLM\u2019s ability to diagnose the visitor\u2019s illness. The Treatment category evaluates LLM\u2019s ability to treat patients.\n\nTo facilitate supervised fine-tuning as well as few-shot learning, each dataset for the task will be further partitioned into train, dev, test, and reserved. The test split will be used for the evaluation of LLMs. The reserved split will not be released and act as a control set for further evaluation. We sample psychology subjects uniformly under each exam, ensuring that the number of questions is consistent across all four exams. This approach is also used to create the test and reserve split. The remaining questions are all allocated to the train split.\n\nStatistics of the dataset is listed in Table 1. We show three examples from both KG and CA in Figure 2. We hold that CPsyExam-KG and CPsyExam-CA are essential and complementary in assessing psychological competency. Together, they serve as a comprehensive evaluation not only of practitioners\u2019 competency in"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Design Principles",
            "text": "In the realm of psychology, the significance of case studies is on par with that of psychological knowledge. Analyzing cases is often a testament to a practitioner\u2019s ability to apply their skills within the field of psychology. Consequently, our dataset encompasses two key components: one designed to evaluate the LLM\u2019s grasp of psychological knowledge (KG), and the other aimed at assessing the LLM\u2019s proficiency in case analysis (CA). The benchmark needs to be designed to accurately reflect LLM\u2019s competence in the field of psychology within the Chinese context. Therefore, we anticipate that it could cover the majority of topics tested on the Chinese Psychology Examinations but with a balanced distribution of questions across various subjects. We identify a wide range of examinations that include psychology subjects under the Chinese examination system."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Data Collection",
            "text": "Due to the advancement of psychology and initiatives by global organizations, there has been a growing awareness of mental health in developing countries, including China. Educationally, China has established an examination system for psychology that assesses the foundational psychological knowledge of practitioners from diverse occupations and provides professional certifications for individuals aspiring to enter the field of psychological counseling. Below is a list of examinations for target groups:\n\nPCE (Psychological Counselor Examination) The professional qualification examinations for first, second, and third-tier psychological counselors are designed to assess the competence and knowledge of individuals working in the field of psychology. These exams are typically structured as a series of essay questions that cover various aspects of psychology.\n\nTQE (Teachers\u2019 Qualification Examination) The Teachers\u2019 Qualification Examination is a standardized assessment for individuals who wish to teach psychology in primary, middle, and high schools, as well as in vocational schools and higher education institutions. This examination is designed to ensure that educators possess the necessary knowledge and skills to effectively teach psychology courses.\n\nGEE (Graduate Entrance Examinations) It contains a wide range of subjects like general psychology, social psychology, experimental psychology, contemporary educational psychology, psychology and educational measurement, developmental psychology, modern psychology, and educational statistics.\n\nSSE (Self-study Examination) The Self-study Examination is an examination that assesses an individual\u2019s knowledge and understanding of psychology, specifically in the areas of educational psychology, medical psychology, advertising psychology, and journalism psychology. This examination is typically taken by individuals who are self-studying or not enrolled in a formal educational program, which reflects the comprehensive examination system in China\u2019s psychology education.\n\nBased on the categorization of examinations in psychology, we crawl publicly available resources online to construct a database of questions. The websites for the data crawling include ExamCoo, StudyEZ, Hxter, MXQE, and book corpus on GEE. We collect our data from both websites and books. For the data scraped from websites, we use a parsing program to extract the questions, while for the data from books, we extract it manually, resulting in structured data in a uniform format. Afterward, we preprocess all the data to eliminate duplicates and questions with incorrect formatting. We also remove questions that contain image links and standardize the question format by removing question numbers and option letters. Finally, we manually validate the dataset to ensure that there are no apparent grammatical errors in the questions."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "CPsyExam",
            "text": "CPsyExam is a multi-task dataset of psychology designed to assess both knowledge and reasoning ability of LLMs in Chinese language. We use the collected 20k exam questions about psychology and further arrange them into tasks based on subjects and question types. We align the taxonomy of CPsyExam-KG questions with the Chinese examination system for psychology. Then we select all the psychological subjects in each examination as a subcategory, the detailed directory list of which can be found in Appendix A. Based on psychological counselling conventions, we divided the CA questions into three categories: Method, Diagnosis and Treatment. The Method category assesses the LLM\u2019s ability to use the corresponding methodology in a specific case. The Diagnosis category pays attention to the LLM\u2019s ability to diagnose the visitor\u2019s illness. The Treatment category evaluates LLM\u2019s ability to treat patients. To facilitate supervised fine-tuning as well as few-shot learning, each dataset for the task will be further partitioned into train, dev, test and reserved. The test split will be used for the evaluation of LLMs. The reserved split will not be released and act as a control set for further evaluation. We sample psychology subjects uniformly under each exam, ensuring that the number of questions is consistent across all four exams. This approach is also used to create the test and reserve split. The remaining questions are all allocated to the train split. Statistics of the dataset is listed in Table 1. We show three examples from both KG and CA in Figure 2. We hold that CPsyExam-KG and CPsyExam-CA are essential and complementary in assessing psychological competency. Together, they serve as a comprehensive evaluation not only of practitioners\u2019 competency in psychology but also of the LLM\u2019s expertise in the field."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Comparison with Existing Psychology Benchmarks",
            "text": "In the psychological domain, we have the ongoing development of benchmarks like PsyBench and PsyEval.\n\nPsyBench uses the method of inputting knowledge points into the GPT-4 to generate questions because of the pursuit of balanced knowledge. Although they utilize the method of error correction by experts to address the questions, the questions in our dataset were meticulously crafted from scratch by subject matter experts, resulting in a more professional and authoritative approach to assessing knowledge and setting options compared to the questions generated by GPT-4.\n\nIn the case of PsyEval, they focus on the field of mental health and want to measure LLM\u2019s ability to be relevant in the field of mental health, whereas PsyExam focuses on all subjects related to psychology, and is superior to PsyEval in terms of the comprehensiveness of coverage of topics in the field of psychology."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We conduct both zero-shot and few-shot evaluations for each model discussed above. Given the focus of CPsyExam is on how models can perform over Knowledge and Case Analysis questions, we report them separately. We also design a psychology-oriented prompt for the evaluation, which can be found in Figure 5 in the Appendix. Evaluation results are listed in Table 2.\n\nThere are three sections in the table:\n\n(1) Open-sourced Models. Our findings indicate that: (a) increased model size does not necessarily ensure improved performance on the CPsyExam, and (b) models that excel in other domains, such as YI-34B on the medical domain, may not necessarily perform optimally on the CPsyExam.\n\n(2) Psychology-oriented Models. Compared to the foundation models, these fine-tuned models show marginal gains or no improvement in understanding psychological knowledge, and their case analysis abilities may even be compromised in some instances.\n\n(3) Api-based Models. GPT-4 continues to outperform all other API-based models by a significant margin in the knowledge setting. Conversely, ChatGLM-turbo performs exceptionally well in the Case Analysis setting. Besides MRQA, CPsyExam includes an extra QA test set to evaluate generation-based questions. We adopt GPT-4 to judge API-based LLMs used in this work. The prompt used for the scoring is listed in Figure 6 in the Appendix.\n\nThe results suggest that ChatGLM-turbo has a better understanding of psychological knowledge and can be effectively prompted for psychological purposes. When the model size is relatively small, in most cases, the performance improvement from few-shot learning is not very significant, and may even lead to negative effects. However, as the model size increases, the benefits of few-shot learning become much more pronounced. For example, the ChatGLM-turbo, which already performs well in zero-shot settings, saw its performance double after few-shot training on the CA task. This is likely because larger models have greater learning capacity and expressive power. Larger models are able to capture more complex patterns and latent semantic relationships in the data, enabling them to learn and generalize more quickly from smaller amounts of training data.\n\nBased on the experimental findings, the model that underwent a fine-tuning process to enhance its psychological capabilities did not outperform the base model in the experiments and even showed a decline in performance. For example, MeChat is fine-tuned from the ChatGLM2-6B model, but its overall performance on the CPsyExam was weaker than that of the ChatGLM2-6B. We infer that this is because although MeChat\u2019s fine-tuning enhanced its conversational abilities, this may have come at the cost of reducing its performance in knowledge reasoning and text comprehension tasks. The model may have over-adapted to the fine-tuning data, while neglecting the knowledge it had learned during the pre-training stage.\n\nRegarding CPsyExam-KG, we perform analysis at the examination level, as depicted in Figure 3a. For CPsyExam-CA, we delve into various aspects of case analysis, presented in Figure 3b. By examining both figures, we determine that GPT-4 exhibits a stronger grasp of psychological knowledge across all examinations, yet it continues to face challenges with case analysis questions. The major gap for GPT-4 comes from Diagnosis and Treatment. For each subject included in CPsyExam, there are at least 32 questions, which exceeds the number of questions typically found in quizzes designed for human participants.\n\nWe have selected the top two models based on their performance over the CPsyExam benchmark to visualize their performance across each subject. Some subjects can be grouped together due to their shared background and domain similarity, which we have chosen to merge initially. The results for ChatGLM-Turbo and GPT-4 are presented in Figure 4. Despite being the two best-performing models on our CPsyExam benchmark, ChatGLM-Turbo shows insufficient robustness against certain subjects and is still outperformed by GPT-4 across a variety of subjects. We also observe that specific subjects, such as \"Psychology in Advertising,\" are more challenging than others."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiment Setup",
            "text": "In this section, we benchmark a series of publicly accessible LLMs using CPsyExam. We choose both open-sourced LLMs with model sizes ranging from 6B to 34B, such as ChatGLM2-6B, YI-6B, QWEN-14B, and YI-34B, and API-based LLM services like ERNIE-Bot-Turbo, ChatGPT, ChatGLM-Turbo, and GPT4. Additionally, a series of psychology-oriented models available online are also considered for comparison, including MeChat, MindChat, and SoulChat. Specifically, MeChat is fine-tuned from ChatGLM2-6B. MindChat has released two versions, MindChat-Qwen-7B-v2 and MindChat-Qwen-1_8B. SoulChat has a version based on ChatGLM-6B but is not compatible with most of the frameworks for evaluation, which is therefore not considered for comparison. As CPsyExam includes a training set for supervision purposes, we construct an instruction set for Supervised Fine-Tuning (SFT). In this work, we conduct SFT over ChatGLM3-6B, applying specific parameters to optimize the training process. Specifically, the SFT is carried out over 4 epochs with a batch size of 128. The learning rate is set to balance the learning efficiency and the risk of over-fitting. These parameters were chosen based on preliminary experiments that aimed to maximize the model\u2019s performance on validation sets."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Benchmarking Result",
            "text": "We conduct both zero-shot and few-shot evaluations for each model discussed above. Given the focus of CPsyExam is on how models can perform over Knowledge and Case Analysis questions, we report them separately. We design a psychology-oriented prompt for the evaluation, which can be found in Figure 5 in the Appendix. Evaluation results are listed in Table 2. There are three sections in the table:\n\n(1) Open-sourced Models. Our findings indicate that: (a) increased model size does not necessarily ensure improved performance on the CPsyExam, and (b) models that excel in other domains, such as YI-34B on the medical domain, may not necessarily perform optimally on the CPsyExam.\n\n(2) Psychology-oriented Models. Compared to the foundation models, these fine-tuned models show marginal gains or no improvement in understanding psychological knowledge, and their case analysis abilities may even be compromised in some instances.\n\n(3) Api-based Models. GPT-4 continues to outperform all other API-based models by a significant margin in the knowledge setting. Conversely, ChatGLM-turbo performs exceptionally well in the Case Analysis setting.\n\nBesides the MCQA and MRQA, CPsyExam includes an extra QA test set to evaluate generation-based questions. We adopt GPT-4 to judge API-based LLMs used in this work. The prompt used for the scoring is listed in Figure 6 in the Appendix.\n###figure_6### ###figure_7### ###figure_8###\n\nThe results suggest that ChatGLM-turbo has a better understanding of psychological knowledge and can be effectively prompted for psychological purposes."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Analysis from Multiple Perspectives",
            "text": "When the model size is relatively small, in most cases, the performance improvement from few-shot learning is not very significant, and may even lead to negative effects. However, as the model size increases, the benefits of few-shot learning become much more pronounced. For example, the ChatGLM-turbo, which already performs well in zero-shot settings, saw its performance double after few-shot training on the CA task. This is likely because larger models have greater learning capacity and expressive power. Larger models are able to capture more complex patterns and latent semantic relationships in the data, enabling them to learn and generalize more quickly from smaller amounts of training data.\n\nBased on the experimental findings, the model that underwent a fine-tuning process to enhance its psychological capabilities did not outperform the base model in the experiments, and even showed a decline in performance. For example, MeChat is fine-tuned from the ChatGLM2-6B model, but its overall performance on the CPsyExam was weaker than that of the ChatGLM2-6B. We infer that this is because although MeChat\u2019s fine-tuning enhanced its conversational abilities, this may have come at the cost of reducing its performance in knowledge reasoning and text comprehension tasks. The model may have over-adapted to the fine-tuning data, while neglecting the knowledge it had learned during the pre-training stage.\n\nRegarding CPsyExam-KG, we perform analysis at the examination level, as depicted in Figure 3a. \n\nFor CPsyExam-CA, we delve into various aspects of case analysis, presented in Figure 3b. \n\nBy examining both figures, we determine that GPT-4 exhibits a stronger grasp of psychological knowledge across all examinations, yet it continues to face challenges with case analysis questions. The major gap for GPT-4 comes from Diagnosis and Treatment.\n\nFor each subject included in CPsyExam, there are at least 32 questions, which exceeds the number of questions typically found in quizzes designed for human participants. We have selected the top two models based on their performance over the CPsyExam benchmark to visualize their performance across each subject. Some subjects can be grouped together due to their shared background and domain similarity, which we have chosen to merge initially.\n\nThe results for ChatGLM-Turbo and GPT-4 are presented in Figure 4. Despite being the two best-performing models on our CPsyExam benchmark, ChatGLM-Turbo shows insufficient robustness against certain subjects and is still outperformed by GPT-4 across a variety of subjects. We also observe that specific subjects, such as \"Psychology in Advertising\", are more challenging than others."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Analysis on model aspect",
            "text": "When the model size is relatively small, in most cases, the performance improvement from few-shot learning is not very significant, and may even lead to negative effects. However, as the model size increases, the benefits of few-shot learning become much more pronounced. For example, the ChatGLM-turbo, which already performs well in zero-shot settings, saw its performance double after few-shot training on the CA task. This is likely because larger models have greater learning capacity and expressive power. Larger models are able to capture more complex patterns and latent semantic relationships in the data, enabling them to learn and generalize more quickly from smaller amounts of training data.\n\nBased on the experimental findings, the model that underwent a fine-tuning process to enhance its psychological capabilities did not outperform the base model in the experiments, and even showed a decline in performance. For example, MeChat is fine-tuned from the ChatGLM2-6B model, but its overall performance on the CPsyExam was weaker than that of the ChatGLM2-6B. We infer that this is because although MeChat\u2019s fine-tuning enhanced its conversational abilities, this may have come at the cost of reducing its performance in knowledge reasoning and text comprehension tasks. The model may have over-adapted to the fine-tuning data, while neglecting the knowledge it had learned during the pre-training stage."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Analysis on benchmark aspect",
            "text": "Regarding CPsyExam-KG, we perform analysis at the examination level, as depicted in Figure 3a.\n\nFor CPsyExam-CA, we delve into various aspects of case analysis, presented in Figure 3b.\n\nBy examining both figures, we determine that GPT-4 exhibits a stronger grasp of psychological knowledge across all examinations, yet it continues to face challenges with case analysis questions. The major gap for GPT-4 comes from Diagnosis and Treatment. \n\nFor each subject included in CPsyExam, there are at least 32 questions, which exceeds the number of questions typically found in quizzes designed for human participants. We have selected the top two models based on their performance over the CPsyExam benchmark to visualize their performance across each subject. Some subjects can be grouped together due to their shared background and domain similarity, which we have chosen to merge initially.\n\nThe results for ChatGLM-Turbo and GPT-4 are presented in Figure 4. Despite being the two best-performing models on our CPsyExam benchmark, ChatGLM-Turbo shows insufficient robustness against certain subjects and is still outperformed by GPT-4 across a variety of subjects. We also observe that specific subjects, such as \"Psychology in Advertising\", are more challenging than others."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussions",
            "text": "EPPP primarily assesses eight aspects of psychology knowledge. You can find corresponding subjects for almost every subject in the GEE and PCE part of CPsyExam-KG, as both exams aim to evaluate students\u2019 grasp of psychological knowledge. Furthermore, because the EPPP focuses on the evaluation of psychological counselors, while CPsyExam-KG aims to assess a broader range of LLM\u2019s psychological abilities, there are additional sections in CPsyExam that are not present in the EPPP, such as SSE and TQE. These sections include questions that involve psychological analysis of phenomena related to various industries.\nEPPP primarily assesses six main areas of content. In this aspect, there are significant differences between CPsyExam-CA and EPPP. CPsyExam-CA focuses on assessing whether the test taker can analyze complex case scenarios and provide correct answers, examining their ability to solve problems in psychological contexts. On the other hand, EPPP places more emphasis on assessing the key skills and professional competence of psychologists. This difference may stem from the different target test takers of the two exams. CPsyExam-CA aims to assess large language models (LLMs), while EPPP targets human test takers. Therefore, although both exams evaluate performance in real-life scenarios, the content of the test questions differs."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Data leakage problem",
            "text": "It is true that a significant number of questions in CPsyExam are derived from publicly accessible information on the web. However, the answers on selected websites are not directly correlated with the questions in a straightforward manner, making it challenging for large language models (LLMs) to recall the correct answers. To address this challenge, we carried out relatively complex data preprossessing in order to obtain the final question-answer pairs. Our experiments support this approach. Even psychology-focused models like MeChat and MindChat, which leveraged Chinese psychology materials, scored only around 50% correct. In contrast, general-domain models like GPT-4 and ChatGLM-turbo, which were less likely to have encountered the questions before, performed better, achieving 67.43% and 64.58% correctness respectively. Additionally, we reserved a portion of the dataset for future validation, which can help mitigate potential data leakage problem."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "CPsyExam compares to English psychology standards",
            "text": "EPPP exam is the examination for professional practice in psychology of North American. It and CPsyExam both divide the entire examination into two parts, namely the knowledge part and the skill part. Therefore, we will compare these two parts of the exams separately.\nEPPP primarily assesses eight aspects of psychology knowledge. You can find corresponding subjects for almost every subject in the GEE and PCE part of CPsyExam-KG, as both exams aim to evaluate students\u2019 grasp of psychological knowledge. Furthermore, because the EPPP focuses on the evaluation of psychological counselors, while CPsyExam-KG aims to assess a broader range of LLM\u2019s psychological abilities, there are additional sections in CPsyExam that are not present in the EPPP, such as SSE and TQE. These sections include questions that involve psychological analysis of phenomena related to various industries.\nEPPP primarily assesses six main areas of content. In this aspect, there are significant differences between CPsyExam-CA and EPPP. CPsyExam-CA focuses on assessing whether the test taker can analyze complex case scenarios and provide correct answers, examining their ability to solve problems in psychological contexts. On the other hand, EPPP places more emphasis on assessing the key skills and professional competence of psychologists. This difference may stem from the different target test takers of the two exams. CPsyExam-CA aims to assess large language models (LLMs), while EPPP targets human test takers. Therefore, although both exams evaluate performance in real-life scenarios, the content of the test questions differs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In conclusion, CPsyExam is a benchmark for psychology that is crafted from human-generated questions and covers a broad range of subjects within the Chinese examination system. It is designed to assess LLMs\u2019 proficiency in both psychological knowledge and case analysis. CPsyExam is not only suitable for benchmarking LLMs but also provides a valuable resource for comparing the differences in psychology education across different countries."
        }
    ],
    "url": "http://arxiv.org/html/2405.10212v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.1",
            "4.3.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.10212v2",
        "paper_title": "CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations",
        "research_background": "### Paper's Motivation:\nThe paper is motivated by the growing importance of evaluating language models, particularly large language models (LLMs), as they become more advanced and are deployed in various domains, including psychology. Given the increasing capabilities of pretrained models like GPT and BERT, researchers have constructed several benchmarks to evaluate these models comprehensively. However, there is a gap in benchmarks specifically designed for the domain of psychology in Chinese. The rising adoption of LLMs in psychological counseling and mental health support creates an urgent need for a domain-specific benchmark to evaluate the models' understanding of psychological knowledge.\n\n### Research Problem:\nThe primary research problem addressed in this paper is the lack of a comprehensive evaluation benchmark for assessing large language models in the domain of psychology, particularly in Chinese. Existing benchmarks either do not include psychology-related subjects or offer inadequate coverage. Current efforts in creating psychology-related benchmarks suffer from limitations like automatically generated questions of limited size or lack of depth. There is a need to construct a large-scale, well-balanced psychological evaluation benchmark.\n\n### Relevant Prior Work:\nThe paper builds on and extends a diverse body of relevant prior work:\n\n1. **Language Model Benchmarks**:\n   - **GLUE and SuperGLUE**: Popular benchmarks for natural language understanding tasks (Wang et al., 2019).\n   - **CLUE**: A benchmark for evaluating Chinese language understanding (Xu et al., 2020).\n   - **MMLU and CMMLU**: Multi-task benchmarks based on real-world exams for assessing the general abilities of LLMs (Hendrycks et al., 2021; Li et al., 2023).\n\n2. **Psychology in Language Models**:\n   - **PsyEval**: Constructs a Mental Health QA dataset from MedQA, but focuses on automatically generated questions that are manually screened (Jin et al., 2020).\n   - **PsyBench**: Derived from GPT-4, emphasizes the balance of psychological knowledge but is limited by dataset size (Zhang et al., 2023).\n\n3. **Applications in Psychology**:\n   - Researchers like Lai et al. (2023) and Qiu et al. (2023) highlight the increased use of LLMs in psychological counseling and mental health support.\n\n### Contributions:\n  - **CPsyExam Benchmark**: A large-scale, well-balanced dataset of Chinese psychology examination questions containing multiple-choice, multiple-response, and open-ended questions. \n  - **Framework for Psychological Assessment**: Divides the assessment into knowledge (factoid-oriented questions) and case analysis (case-oriented questions focusing on methods, diagnosis, and treatment).\n  - **Comparison of LLMs**: Analyzes the performance differences between general domain LLMs and psychology-specific LLMs, highlighting areas needing improvement.\n\nIn conclusion, the paper's motivation lies in addressing the deficit of domain-specific evaluation benchmarks in psychology for Chinese LLMs, presenting a comprehensive solution in the form of CPsyExam designed to benchmark and enhance LLMs' capabilities in understanding and applying psychological knowledge.",
        "methodology": "### Methodology\n\nThe methodology for creating the CPsyExam benchmark involves several key steps and innovations aimed at evaluating the Large Language Model's (LLM) proficiency in psychology through a structured and comprehensive dataset. Below are the main components of the proposed method detailed in the methodology section:\n\n1. **Data Collection:** \n   - **Sources:** Psychological data is gathered from publicly available resources such as ExamCoo, StudyEZ, Hxter, MXQE, and book corpus on Graduate Entrance Examinations (GEE).\n   - **Extraction:** Data from websites is extracted using a parsing program, while data from books is extracted manually. This results in structured data in a uniform format.\n\n2. **Data Preprocessing:**\n   - **Cleaning:** Elimination of duplicates and questions with incorrect formatting.\n   - **Standardization:** Removal of questions containing image links, question numbers, and option letters.\n   - **Validation:** Manual validation to ensure there are no apparent grammatical errors.\n\n3. **Question Types:**\n   - **Multiple-Choice Questions (MCQA):** Widely used in domain evaluations despite potential uneven difficulty and the possibility of random guesses. It serves as the primary question type due to abundant resources.\n   - **Multiple-Response Questions (MRQA):** Less common but challenging as they require selecting all appropriate answers.\n   - **Question Answering (QA):** Tests the generation abilities of LLMs with diverse topics and application of specific techniques.\n   - **Case Analysis (CA):** Divided into Method, Diagnosis, and Treatment categories to assess methodology usage, diagnostic ability, and treatment skills.\n\n4. **Dataset Components:**\n   - **Knowledge Grasp (KG):** Focuses on evaluating the LLM\u2019s grasp of psychological knowledge.\n   - **Case Analysis (CA):** Aims at assessing the LLM\u2019s proficiency in analyzing psychological cases.\n\n5. **Categorization and Taxonomy:**\n   - **Aligned with Chinese Examination System:** Taxonomy for CPsyExam-KG questions is aligned with the Chinese examination system for psychology.\n   - **Psychological Counseling Categories:** CA questions are categorized into Method, Diagnosis, and Treatment based on psychological counselling conventions.\n\n6. **Data Splitting and Sampling:**\n   - **Uniform Sampling:** Psychology subjects are uniformly sampled under each exam ensuring consistent question numbers across exams.\n   - **Splits:** Data is partitioned into train, dev, test, and reserved splits; the reserved split is used for further evaluation and not released publicly.\n\n7. **Target Examinations:**\n   - **Psychological Counselor Examination (PCE):** Contains multi-tier exams to assess competence.\n   - **Teachers\u2019 Qualification Examination (TQE):** Ensures educators possess necessary knowledge.\n   - **Graduate Entrance Examinations (GEE):** Includes a range of subjects like general, social, and experimental psychology, among others.\n   - **Self-study Examination (SSE):** Assesses knowledge in areas like educational and medical psychology.\n\n8. **Manual and Automated Processing:**\n   - A combination of automated parsing programs and manual extraction and validation ensures the reliability and quality of the questions.\n\n### Innovation and Significance\n\nThis methodology brings several innovations and specific implementations to ensure that CPsyExam accurately reflects the Chinese context and evaluates the LLM\u2019s psychological competence comprehensively. The use of both knowledge-based and case analysis questions ensures a holistic assessment, and the adherence to the structure of existing Chinese examinations ensures its relevance and applicability. The dataset's design supports different types of learning tasks like supervised fine-tuning and few-shot learning, increasing its utility for various applications in LLM training and evaluation.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Setup\n\n1. **Datasets**: The main dataset used in the experiment is CPsyExam, a comprehensive benchmark designed for evaluating psychological reasoning and understanding using multiple choice questions and other examination formats. CPsyExam includes a training set intended for supervised fine-tuning (SFT).\n\n2. **Models Evaluated**:\n   - **Open-Sourced LLMs**: Models ranging from 6B to 34B parameters, including:\n     - ChatGLM2-6B\n     - YI-6B\n     - QWEN-14B\n     - YI-34B\n   - **API-Based LLM Services**:\n     - ERNIE-Bot-Turbo\n     - ChatGPT\n     - ChatGLM-Turbo\n     - GPT4\n   - **Psychology-Oriented Models**:\n     - MeChat (finetuned from ChatGLM2-6B)\n     - MindChat (versions MindChat-Qwen-7B-v2 and MindChat-Qwen-1_8B)\n\n3. **Supervised Fine-Tuning (SFT)**: Conducted on the ChatGLM3-6B model. Key parameters for the SFT process:\n   - **Epochs**: 4\n   - **Batch Size**: 128\n   - **Learning Rate**: Optimized based on preliminary experiments to balance efficiency and the risk of overfitting.\n\n#### Evaluation Metrics\n- The primary metrics for evaluation were not explicitly mentioned in the provided excerpt. Typically, for such benchmarks, metrics would include accuracy, F1 score, precision, recall, and possibly others depending on the specific tasks within the CPsyExam dataset.\n\n#### Results\n- The main experimental results aren't detailed in the provided section. Typically, such results would include a comparative analysis of the performance of each LLM on the CPsyExam dataset, indicating how well each model performed the given psychological reasoning tasks.\n\nTo summarize, the main experiment aimed to evaluate a range of models, both generic and psychology-oriented, on the CPsyExam benchmark to assess their performance in psychological reasoning tasks. The experiment also included fine-tuning specific models like ChatGLM3-6B to improve their performance."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the effectiveness of few-shot learning on different sized models for psychological knowledge and case analysis questions.",
            "experiment_process": "We conduct both zero-shot and few-shot evaluations for a range of models. Models include open-sourced models with sizes ranging from 6B to 34B and API-based models. Specifically, experiments involved models such as ChatGLM2-6B, YI-6B, QWEN-14B, YI-34B, ERNIE-Bot-Turbo, ChatGPT, ChatGLM-Turbo, and GPT-4. Few-shot learning was applied to assess its impact on performance, especially for larger models. The results were evaluated using Knowledge and Case Analysis questions, separately differentiating MCQA and MRQA questions.",
            "result_discussion": "The results indicate that few-shot learning provides significant benefits for larger models like ChatGLM-turbo, which saw its performance double after few-shot training on the CA task. This suggests that larger models capture more complex patterns and generalize more quickly from small amounts of data. However, smaller models do not benefit as much, and few-shot learning may even have negative effects.",
            "ablation_id": "2405.10212v2.No1"
        },
        {
            "research_objective": "To investigate if fine-tuning improves or degrades the performance of models on psychological tasks.",
            "experiment_process": "Several psychology-oriented models including MeChat, MindChat, and SoulChat were evaluated. MeChat is fine-tuned from ChatGLM2-6B. The experimental evaluation included comparisons before and after fine-tuning, focusing on the performance of these models against unfine-tuned versions.",
            "result_discussion": "The model fine-tuned to enhance psychological capabilities (MeChat) underperformed compared to its base model (ChatGLM2-6B). This is inferred to be because fine-tuning for conversational abilities may have reduced performance in knowledge reasoning and comprehension tasks, possibly over-adapting to fine-tuning data and neglecting pre-trained knowledge.",
            "ablation_id": "2405.10212v2.No2"
        },
        {
            "research_objective": "To analyze the persistent low performance of models on MRQA questions and identify the best-performing models across subjects in CPsyExam.",
            "experiment_process": "Focused error analysis was performed only on MCQA questions due to persistent low performance on MRQA questions. An analysis was conducted at the examination level for CPsyExam-KG and case analysis for CPsyExam-CA. Two of the best-performing models, ChatGLM-Turbo and GPT-4, were further analyzed across different subjects. Results were depicted to show domain-specific challenges and general performance.",
            "result_discussion": "The analysis reveals that GPT-4 consistently has a stronger grasp of psychological knowledge but struggles with case analysis questions, especially in areas like Diagnosis and Treatment. ChatGLM-Turbo, though proficient, lacks robustness in specific subjects and is outperformed by GPT-4. Certain subjects, like 'Psychology in Advertising,' are found to be more challenging.",
            "ablation_id": "2405.10212v2.No3"
        }
    ]
}