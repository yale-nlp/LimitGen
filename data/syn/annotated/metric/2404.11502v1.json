{
    "title": "Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large Language Models",
    "abstract": "In real world, large language models (LLMs) can serve as the assistant to help users accomplish their jobs, and also support the development of advanced applications. For the wide application of LLMs, the inference efficiency is an essential concern, which has been widely studied in existing work, and numerous optimization algorithms and code libraries have been proposed to improve it. Nonetheless, users still find it challenging to compare the effectiveness of all the above methods and understand the underlying mechanisms. In this work, we perform a detailed coarse-to-fine analysis of the inference performance of various code libraries. To evaluate the overall effectiveness, we examine four usage scenarios within two practical applications. We further provide both theoretical and empirical fine-grained analyses of each module in the Transformer architecture. Our experiments yield comprehensive results that are invaluable for researchers to evaluate code libraries and improve inference strategies.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "With the advancement and widespread of large language models (LLMs), the enhancement of inference efficiency in LLMs has emerged as an important topic of contemporary research. To achieve superior inference speed without significant performance degradation, researchers have proposed diverse inference optimization algorithms and libraries.\n\nCurrently, several prominent libraries have been widely used in the market, such as vLLM, DeepSpeed-MII, and TensorRT-LLM, etc. These libraries have notably elevated inference efficiency through sophisticated methodologies such as optimization algorithms and parallel computing. Nevertheless, a notable deficiency exists in the absence of a standardized evaluation benchmark for comprehensively comparing the performance across existing libraries. To address it, this work meticulously devises a series of evaluation experiments with the goal of impartially and objectively assessing the inference efficiency of each library.\n\nConcretely, this paper clearly defines two types of evaluation experiments: coarse-grained and fine-grained. In the coarse-grained evaluation, four text generation datasets with diverse length distributions are designed to simulate various generation tasks. We then explore two practical applications: offline batch inference and network service provisioning. The former involves assessments conducted in batch mode offline while the latter pertains to real-time online service scenarios. We assess the efficiency of each library in offline inference and also evaluate their performance at different request frequencies.\n\nIn conclusion, this investigation endeavors to delve into the inference efficiency of large language models through comprehensive and objective evaluation experiments. First, we propose a comprehensive benchmark which covers different task scenarios, and use them to evaluate different libraries in different usage scenarios, filling the gap in the inference benchmark. Finally, we have open-sourced the above dataset, code, and evaluation scripts, which are available online.\n\nIt is anticipated that the findings of this study will not only offer valuable insights for enhancing existing inference libraries but also establish a robust groundwork for the advancement of future inference algorithms and libraries."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminary",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Background of Transformer",
            "text": "In contemporary LLMs, the prevailing architecture is the Transformer decoder (Vaswani et al., 2017). Utilizing the LLaMA (Touvron et al., 2023a; b) model as a paradigmatic illustration, its design encompasses two principal components: the multi-head attention block (MHA module) and the feed-forward network (FFN module). Both of these modules are followed by an RMS normalization (Zhang & Sennrich, 2019) and a residual network.\n\nThe MHA module transforms the input into three matrices through different linear transformations, calculate the multi-head attention, and aggregate the results from multiple heads using formulas where certain parameters denote learnable elements.\n\nThe FFN module uses the SwiGLU activation function (Shazeer, 2020) to expand the intermediate state dimension with gated linear units, and then obtains the output result of the module through a linear transformation, involving the Hadamard product and parameter denotations.\n\nAfter training, the inference of LLMs typically involves auto-regressive generation. Algorithm 1 represents an enhancement of auto-regressive generation, delineated into two distinct phases: the prefill phase and the decoding phase. During the prefill phase (lines 1-3), the model generates the initial token and stores the matrices corresponding to the input tokens, called KV cache (Pope et al., 2022). Subsequently, in the decoding phase (lines 4-9), the model iteratively generates the next token by reusing the KV cache and updates the cache for future matrices."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Arithmetic Intensity",
            "text": "In model inference, temporal overhead mainly stems from GPU computation and memory access. Computation volume is measured in floating-point operations (FLOPs), and memory access in bytes of reads and writes (MOPs) (Kim et al., 2023  ###reference_b7###).\n\nGiven this background, we are poised to undertake a comprehensive evaluation of existing inference libraries through both overall (Section 3  ###reference_###) and fine-grained analyses (Section 4  ###reference_###). This dual approach allows us to thoroughly assess the performance of LLMs decoding and identify its primary bottlenecks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Overall Evaluation and Analysis",
            "text": "In this section, we conduct an overall evaluation of the inference efficiency of LLMs. We introduce a series of evaluation datasets tailored for two distinct usage scenarios."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Evaluation Scenarios",
            "text": "We examine two real-world usage scenarios: batch inference and server-based inference, across four specially constructed datasets to encompass a range of task scenarios."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Task Scenarios",
            "text": "We develop four datasets focusing on the generation tasks in various real-world scenarios. \n\nShort-to-Short Dataset. This dataset encompasses scenarios such as question answering and daily assistance, characterized by brief inputs and outputs. We meticulously select 1,000 examples from the Alpaca dataset (Taori et al., 2023), ensuring that both the input and output lengths predominantly remain under 50 tokens.\n\nShort-to-Long Dataset. Tailored for tasks like math problem solving and code generation, this dataset comprises scenarios with short inputs and more lengthy outputs. From the Alpaca dataset, we curate 1,000 instances where the input length does not exceed 50 tokens, while the output length varies up to 1,000 tokens.\n\nShort-to-16k Dataset. Building on the concept of the short-to-long dataset, we delve into scenarios demanding exceptionally long-text generation, such as story generation. We select instances from the Vicuna dataset (Chiang et al., 2023), requiring the model to produce outputs of exactly 16,000 tokens.\n\nLong-to-Short Dataset. Aimed at reflecting text summarization or multi-turn dialogue scenarios, this dataset features lengthy inputs with concise outputs. Compiled from the ShareGPT dataset (ShareGPT, 2023), it includes examples where the input ranges from 1,100 to 1,500 tokens and the output is limited to 120 tokens or less."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2 Usage Scenarios",
            "text": "We mainly consider the following two usage scenarios:\n\nBatching Inference.\nIn evaluating the capabilities of LLMs, it is necessary to process extensive amounts of input data in bulk offline. This context does not require a specific order or delay to process each input, allowing for the flexible arrangement of generation sequences. We employ the four datasets to assess the time taken by different libraries to process the entire dataset, along with the token throughput.\n\nServing Inference.\nContrary to batch inference, which is mainly used in research scenarios, serving inference is predominantly utilized in the network deployment to facilitate applications akin to ChatGPT. The metrics for this scenario include sequence and token throughput, measuring the system\u2019s efficacy in managing data sequences and tokens, respectively. To account for initial stabilization and concluding operations within the system, our analysis omits the first and last 100 requests. The evaluation allows for an in-depth investigation into how various libraries fare under simulated network service conditions, elucidating their capacity to manage varying loads and respond within acceptable timeframes."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Evaluation Setup",
            "text": "Libraries.  \nThe libraries under evaluation encompass Transformers (TRF) (Wolf et al., 2020), vLLM (Kwon et al., 2023), Deepspeed-MII (MII) (Microsoft, 2023), TensorRT-LLM (TRT) (NVIDIA, 2023), and llama.cpp (L.CPP) (Gerganov, 2023).  \nFor batching inference, we manually set the batch size for TRF and employ built-in batching strategies for the other four libraries. For serving inference, we evaluate the performance of vLLM and MII.  \n\nLLMs.  \nWe utilize four models for evaluation: Llama-2-7b-chat-hf, Llama-2-13b-chat-hf (Touvron et al., 2023b), vicuna-7b-v1.5-16k, and vicuna-13b-v1.5-16k (Chiang et al., 2023). The LLaMA-2 models, which are widely used in chat applications, are chosen to assess their performance across three scenarios: short-to-short (S2S), short-to-long (S2L), and long-to-short (L2S). The Vicuna models, designed for handling long contexts, are employed to evaluate performance on the short-to-16k (S-16k) dataset.  \n\nHardwares.  \nTo assess the influence of various hardware platforms on influence efficiency, we conduct experiments using three NVIDIA GPUs: RTX-3090, RTX-4090, and A800. Table 5 presents key specifications of these GPUs, encompassing GPU memory capacity, memory bandwidth, and BF16 floating-point operations (FLOPs) per second.  "
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Evaluation Results",
            "text": "Firstly from the scenario of batching inference in Table 2 and Table 6, we find that GPU computational performance is pivotal for short input-output pairs, whereas memory bandwidth becomes critical as sequences elongate. We have observed that the 4090 significantly outperforms the 3090 in the S2S dataset. However, this advantage diminishes in S2L and L2S datasets. Conversely, the A800 consistently excels across all datasets.\n\nWe hypothesize that the observed performance discrepancies are attributable to the differing specifications of the GPUs. The 4090 boasts double the computational power of the 3090, yet their bandwidths are comparable. In contrast, the A800 doubles both the computational power and bandwidth relative to the 4090.\n\nSecondly, vLLM and MII demonstrate superior efficiency compared to other libraries in batching scenarios. This advantage is primarily attributable to their advanced optimization technologies, including KV cache and batching strategies. When analyzing the results from the 7B and 13B models, it is evident that the 13B model\u2019s processing time is nearly double that of the 7B model for both vLLM and MII. This phenomenon is not observed in other libraries. Given that the computational FLOPs for the 13B model are twice those of the 7B model, a corresponding increase in processing time is expected. This indicates that the other libraries have room for improvement in GPU memory management.\n\nThirdly, the Dynamic SplitFuse batching strategy of MII demonstrates enhanced efficiency in serving inference scenarios of long sequences, as evidenced by the results depicted in Figures 1, 4, and 5. It is observed that with an increasing evaluation rate of requests, the vLLM initially exhibits a surge, followed by a gradual decline after reaching its peak performance. \n\nIn contrast, the token throughput for MII consistently rises, although the rate of increase gradually diminishes. This phenomenon becomes more evident as the sequences lengthen (Figures 1 and 5), because the Dynamic SplitFuse strategy enables more fine-grained segmentation of longer sequences. Regarding token latency, as the rate of requests escalates, both vLLM and MII show a steady increase in latency. Besides, the latency of the Dynamic SplitFuse strategy is observed to be higher when the GPU memory is limited (i.e., 3090 and 4090)."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Fine-grained Modular Evaluation and Analysis",
            "text": "In this section, we conduct a both theoretical analysis and practical evaluation to quantify the time, floating point operations, and memory read/write volumes of each module in LLaMA. This granular investigation provides a thorough understanding of the model\u2019s computational characteristics. Comparing these modules of Transformers and vLLM, we can derive insights into the optimization paths of current inference libraries and yield crucial guidance for future improvement."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Theoretical Analysis",
            "text": "In this section, we methodically dissect each operation within the LLaMA decoder layer, deriving theoretical formulas for the number of floating-point operations and the volume of memory reads/writes. This analysis is strictly limited to a single decoder layer; to extrapolate to real-world applications, one must multiply these findings by the total number of decoder layers. The outcomes of our analysis are detailed in Tables 4 ###reference_### and 4 ###reference_###.\n\nIn the following analysis,  represents the batch size,  represents the input sequence length,  represents the hidden size,  represents the intermediate size of FFN module,  represents the number of attention \u201cheads\u201d, and  represents the size of each \u201chead\u201d ( and  satisfying ).\n\nFLOP Analysis.\nFirst, let\u2019s analyze the prefill phase:\nFor the MHA module, the three linear projections can be expressed as matrix multiplications (Equation 1 ###reference_###), requiring  FLOPs. The calculation of relative positional encoding (RoPE) involves 4 multiplications and 2 additions, requiring  FLOPs.\n\nRegarding the attention calculation (Equation 2 ###reference_###), the multiplication of matrix  and matrix  requires  FLOPs. Dividing by  and calculating the softmax requires  FLOPs. Finally, multiplying with matrix  requires  FLOPs. Therefore, the attention calculation requires a total of  FLOPs.\n\nThe final linear transformation (Equation 3 ###reference_###) in the MHA module also requires  FLOPs.\n\nFor the FFN module (Equation 4 ###reference_###), the initial two linear projections require  FLOPs. The calculation of the activation function involves both multiplication and the Swish function, requiring  FLOPs. The final linear projection requires  FLOPs.\n\nThe calculation of the RMS normalization (RMSNorm) and the residual networks requires  FLOPs.\n\nFor the decoding phase, apart from the attention calculation, the FLOPs required for other parts can be obtained by substituting  into the corresponding formulas from the prefill phase. The FLOPs required for the attention calculation become .\n\nMOPs Analysis.\nDue to the fact that matrix multiplication is calculated in blocks in practical operations, memory read and write volumes can only be expressed in the form of progressive complexity .\n\nFirst, let\u2019s analyze the prefill phase:\nFor the MHA module, the three linear projections can be expressed as matrix multiplications (Equation 1 ###reference_###), requiring  MOPs. The calculation of RoPE involves  MOPs.\n\nRegarding the attention calculation (Equation 2 ###reference_###), the multiplication of matrix  and matrix  requires  MOPs. Dividing by  and calculating the softmax requires  MOPs. Finally, multiplying with matrix  requires  MOPs. Therefore, the attention calculation requires a total of  MOPs.\n\nThe final linear transformation (Equation 3 ###reference_###) in the MHA module also requires  MOPs.\n\nFor the FFN module (Equation 4 ###reference_###), the initial two linear projections  MOPs. The calculation of the activation function involves both multiplication and the Swish function, requiring  MOPs. The final linear projection requires  MOPs.\n\nThe calculation of the RMSNorm and the residual networks requires  MOPs.\n\nFor the decoding phase, apart from the attention calculation, the MOPs required for other parts can be obtained by substituting  into the corresponding formulas from the prefill phase. The MOPs required for the attention calculation become .\n\nHence, optimizing the implementation of attention, RoPE, RMSNorm, and residual networks is crucial for reducing MOPs during the inference stage of LLMs, which leads to the development of FlashAttention (Dao et al., 2022 ###reference_b5###) and PagedAttention (Kwon et al., 2023 ###reference_b8###). Additionally, maximizing the batch size in the decoding stage is necessary to enhance the performance of linear transformations, which necessitates the advance of batching strategies (Kwon et al., 2023 ###reference_b8###; Microsoft, 2023 ###reference_b11###)."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluation Setup",
            "text": "To accurately measure the execution time and memory read/write volume (MOPs) of various modules during real-world execution, we employ two tools: NVIDIA Nsight Compute CLI (NCU) and torch.profile. NCU is adept at quantifying the execution time and MOPs for individual CUDA kernels, while torch.profile offers detailed call stacks of CUDA kernels, enabling precise identification of specific modules. In the following experiments, we utilize simulated data with input lengths ranging from 32 to 2048 using a fixed batch size. We also conduct experiments varying different batch sizes and experiments varying different hardware. For each experiment, we employ both Transformers and vLLM libraries to generate two tokens using LLaMA-2 (7B) each on A800 GPU. This allows for execution of the prefill stage and the decoding stage once within each library."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Results",
            "text": "Firstly, our practical time-consuming results are consistent with our theoretical analysis results in Tables 4 and 4. Thus, we can estimate the runtime for each module during the prefill and decoding phases. For compute-bound operations (e.g., the linear transformation of MHA in the prefill phase), the estimation primarily relies on the number of FLOPs, represented as. For memory-bound operations, runtime is primarily influenced by the volume of I/O operations. The corresponding estimation equations are detailed below: where are the coefficients of different items. We can determine them through linear regression based on our experimental data, as presented in Table 13.\n\nSecondly, the attention module is the bottleneck during the prefill and decoding stage from the results in Figure 2. Notably, during the prefill phase, the conventional attention mechanism emerges as the primary bottleneck, particularly as the input length escalates. To address this challenge, the integration of FlashAttention (Dao et al., 2022) presents an effective optimization strategy. Conversely, in the decoding phase, inadequate management of the KV cache can result in the update of the KV cache emerging as the principal bottleneck with increasing input lengths. vLLM employs block management techniques for KV cache and PagedAttention (Kwon et al., 2023) mechanisms to streamline KV cache updates and attention calculations, contributing to enhanced efficiency in decoding tasks. \n\nIn addition, CUDA kernel fusion also plays a significant role in improving decoding efficiency. The vLLM library features specially designed CUDA kernels tailored for operations such as RoPE, Swish, and RMSNorm. Unlike the Transformers library, vLLM refines the implementation of these operations to optimize memory access patterns and reduce execution time, as illustrated in Figure 2.\n\nFinally, it is evident that linear transformation operations (i.e., MHA projection and FFN projection) still occupy a substantial portion of time during both the prefill and decoding phases. As shown in Figure 2, after various vLLM optimizations, linear transformations comprise the most time-consuming elements when the sequence length is short, and they also account for over 50% of the total time as the sequence length increases. Although optimizing matrix multiplication presents inherent challenges, it offers a promising path for future inference enhancements."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "System Optimization.\nThere are many optimization algorithms for inference in large language models. To address the low efficiency issue of multi-head attention calculation, FlashAttention (Dao et al., 2022  ###reference_b5###; Dao, 2023  ###reference_b4###) leverages optimization strategies employed in GPU-based matrix multiplication. This involves partitioning the matrices , , and  and directly computing the resultant matrix . By reducing the frequency of memory accesses, this optimization technique increases the arithmetic intensity and improves the efficiency of the attention module.\nTo optimize the management of the KV cache memory in decoding phase, vLLM proposes PagedAttention (Kwon et al., 2023  ###reference_b8###). This mechanism effectively mitigates the frequent update requirement of the KV cache and reduces memory fragmentation, leading to improved overall efficiency.\nIn practical applications, it is often necessary to handle multiple requests concurrently. While the current model architecture supports batch inference, a straightforward implementation requires completing all requests within a batch before initiating the next batch.\nTo address this limitation, researchers have proposed batching strategy such as continuous batching (Kwon et al., 2023  ###reference_b8###), inflight batching (NVIDIA, 2023  ###reference_b14###) and Dynamic SplitFuse (Microsoft, 2023  ###reference_b11###).\nTheir strategy involves immediately substituting completed requests with new ones, eliminating the need for padding tokens. This streamlined processing pipeline enhances throughput and efficiency by ensuring continuous computation without idle periods.\nInference Libraries.\nThe Transformers (Wolf et al., 2020  ###reference_b23###) library is a commonly used library in the field of natural language processing, providing code and archive points for many common models, making it convenient for users to use.\nTGI (Contributors, 2023a  ###reference_b2###) is a library developed by HuggingFace for further optimization of inference based on the Transformers.\nvLLM (Kwon et al., 2023  ###reference_b8###) mainly adopts targeted optimization strategies in terms of decoding efficiency, significantly improving the utilization efficiency of KV Cache by paging storage and combining with PagedAttention technology.\nDeepSpeed-MII has introduced Dynamic-SplitFuse technology to fully tap into the computing potential of GPUs. This technology achieves an increase in batch data and decoding throughput by splitting input prompts into multiple sub blocks and fusing requests for full decoding and incremental decoding.\nTensorRT-LLM is developed by Nvidia, which has been further optimized based on the previous FasterTransformer (NVIDIA, 2021  ###reference_b13###) library, improving the efficiency of running large models on Nvidia GPUs.\nLlama.cpp is entirely based on C/C++ implementation, with good cross platform compatibility and the ability to run on various computing devices.\nOther code libraries such as LightLLM (ModelTC, 2023  ###reference_b12###), LMDeploy (Contributors, 2023b  ###reference_b3###), StreamLLM (Xiao et al., 2023  ###reference_b24###), and Inferflow (Shi et al., 2024  ###reference_b18###) have made different optimization implementations for inference in LLMs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this work, we introduced a comprehensive benchmark that can encompass diverse task scenarios for the evaluation of various libraries. We integrated various common experimental settings in our framework, to provide a useful testbed for evaluating inference efficiency related libraries. Based on it, we proposed a detailed formula for analyzing the complexity of each component from LLaMA, which involves metrics such as FLOPs and MOPs. It can delineate the decoding bottlenecks in the inadequacy of memory bandwidth, and has been validated in our experiments. Besides, widely-used strategies and toolkits such as FlashAttention, PagedAttention and CUDA kernel fusion demonstrated the mitigation of memory access constraints is helpful to enhance the inference efficiency. It is anticipated that our findings will offer valuable insights for the advancement of future inference algorithms and libraries."
        }
    ],
    "url": "http://arxiv.org/html/2404.11502v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5"
        ],
        "methodology_sections": [
            "2.1",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.11502v1",
        "paper_title": "Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large Language Models",
        "research_background": "### Motivation\nWith the advancement and wide dissemination of Large Language Models (LLMs), enhancing their inference efficiency has become a critical area of research. The growing demand for faster and more efficient LLMs has led to the development of various inference optimization algorithms and libraries. Despite notable strides in improving inference efficiency through these sophisticated methodologies, there remains a significant gap: the lack of a standardized evaluation benchmark for comprehensively comparing performance across existing libraries.\n\n### Research Problem\nThe core issue addressed by this paper is the absence of a standard and comprehensive evaluation benchmark for assessing and comparing the inference efficiency of various prominent libraries used for LLMs. Without such a benchmark, it becomes challenging to objectively and impartially measure the effectiveness of different optimization techniques and libraries, thereby hindering progress in improving inference efficiency for LLMs.\n\n### Relevant Prior Work\n1. **LLMs and Inference Efficiency**:\n   - Zhao et al., 2023 highlights the advancements and widespread adoption of LLMs.\n   - Kim et al., 2023 and Miao et al., 2023 focus on various inference optimization algorithms aimed at enhancing inference speed without degrading performance.\n\n2. **Existing Inference Libraries**:\n   - Notable libraries such as vLLM (Kwon et al., 2023), DeepSpeed-MII (Microsoft, 2023), and TensorRT-LLM (NVIDIA, 2023) are identified as having made significant improvements in inference efficiency through optimization algorithms and parallel computing techniques.\n\n3. **Current Deficiencies**:\n   - Despite the existence of these advanced libraries, there is a conspicuous deficiency in standardized benchmarks for comparing their performance (N/A, implicit understanding from the introduction).\n\nThis paper seeks to fill this gap by devising a series of coarse-grained and fine-grained evaluation experiments to impartially assess the inference efficiency of each library. This involves defining various task scenarios and conducting an in-depth analysis to understand efficiency bottlenecks and computational complexities, ultimately providing a robust foundation for enhancing current and future inference algorithms and libraries.",
        "methodology": "**Methodology:**\n\nIn contemporary Large Language Models (LLMs), the prevailing architecture is the Transformer decoder. Utilizing the LLaMA model as a paradigmatic illustration, its design encompasses two principal components: the multi-head attention block (MHA module) and the feed-forward network (FFN module). Both of these modules are followed by an RMS normalization and a residual network.\n\n**Components:**\n\n1. **Multi-Head Attention (MHA) Module:**\n    - **Transformation**: The MHA module transforms the input into three matrices through different linear transformations.\n    - **Computation**: It calculates the multi-head attention.\n    - **Aggregation**: Afterwards, it aggregates the results from multiple heads.\n\n2. **Feed-Forward Network (FFN) Module:**\n    - **Activation Function**: The FFN uses the SwiGLU activation function to expand the intermediate state dimension with gated linear units.\n    - **Linear Transformation**: It then obtains the output result of the module through a linear transformation.\n\nBoth the MHA and FFN modules are followed by an RMS normalization and a residual network.\n\n**Inference Method:**\n\nAfter training, the inference of LLMs typically involves auto-regressive generation. Algorithm 1 represents an enhancement of auto-regressive generation delineated into two distinct phases:\n\n1. **Prefill Phase:**\n    - **Initialization**: The model generates the initial token.\n    - **Storage**: It stores the corresponding \\( K \\) and \\( V \\) matrices (known as the KV cache) for the input tokens.\n\n2. **Decoding Phase:**\n    - **Token Generation**: The model iteratively generates the next token.\n    - **Cache Update**: It reuses the KV cache and updates the cache for future \\( K \\) and \\( V \\) matrices.\n\nThis methodology leverages the efficient architecture of the Transformer decoder, enhanced by the structured approach of the prefill and decoding phases, to improve inference efficiency in large language models.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Evaluation Datasets\nTo assess the inference efficiency of large language models (LLMs), we have curated a series of evaluation datasets that correspond to two distinct usage scenarios:\n1. **Scenario 1:** Daily language generation tasks, which include datasets like Wikitext-103 and the Penn Treebank (PTB).\n2. **Scenario 2:** Complex problem-solving tasks, for which we use datasets from the Arithmetic Reasoning (AR) and Logical Deduction (LD) domains.\n\n#### Baselines\nWe compare the performance of our proposed models against the following well-established LLMs:\n- GPT-3\n- BERT\n- T5\n\n#### Evaluation Metrics\nWe employ the following metrics to gauge inference efficiency comprehensively:\n- **Latency:** The time taken to generate responses.\n- **Throughput:** The number of tokens processed per second.\n- **Token Accuracy:** The correctness of the generated tokens compared to the ground truth.\n- **Computational Cost:** Measured in FLOPs (Floating Point Operations per second).\n\n#### Main Experimental Results\nThe results of our main experiment yielded the following insights:\n- **Latency and Throughput:** Our models demonstrated a significant reduction in latency and improvement in throughput compared to baselines, particularly for daily language generation tasks.\n- **Token Accuracy:** While maintaining or slightly improving token accuracy, our models achieved competitive performance across both usage scenarios.\n- **Computational Cost:** Our models required fewer computational resources while sustaining competitive performance levels, making them more cost-effective compared to traditional LLMs.\n\nThese findings collectively highlight the efficiency gains and practical advantages of our proposed models, particularly in diverse application scenarios."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To impartially and objectively assess the inference efficiency of various code libraries for large language models (LLMs).",
            "experiment_process": "We evaluate four text generation datasets with diverse length distributions to simulate different generation tasks. Datasets include: (1) Short-to-short for tasks like question answering, (2) Short-to-long for tasks like math problem solving, (3) Short-to-16k for story generation, and (4) Long-to-short for tasks like text summarization. The libraries tested are Transformers (TRF), vLLM, Deepspeed-MII (MII), TensorRT-LLM (TRT), and llama.cpp (L.CPP) using models LLaMA-2-7b-chat-hf, LLaMA-2-13b-chat-hf, vicuna-7b-v1.5-16k, and vicuna-13b-v1.5-16k on NVIDIA GPUs (RTX-3090, RTX-4090, A800). Experiments involve batch inference processing large data volumes offline and serving inference for real-time network deployment. Metrics tracked include performance time, token throughput, sequence throughput, and token latency.",
            "result_discussion": "Batching inference results show GPU power is crucial for short input-output pairs, shifting to memory bandwidth importance as sequences lengthen. The A800 GPU consistently excels across scenarios. vLLM and MII exhibit superior efficiency thanks to their advanced optimization technologies, especially Dynamic SplitFuse in long-sequence serving scenarios, while other libraries show room for improvement. Analysis indicates a need to enhance memory management and batching strategies to further optimize performance.",
            "ablation_id": "2404.11502v1.No1"
        },
        {
            "research_objective": "To perform fine-grained modular evaluation of computational characteristics within the Transformer architecture to understand optimization opportunities.",
            "experiment_process": "We analyze the LLaMA decoder layer for time, FLOPs, memory read/write volumes, and arithmetic intensity. We compute these metrics for each module operation during the prefill and decoding phases using theoretical formulas and practical tools like NVIDIA Nsight Compute CLI and torch.profile. Experiments use simulated data with input lengths ranging from 32 to 2048 running on LLaMA-2 (7B) model on an A800 GPU. Runtime estimations for modules during the prefill and decoding phases are formulated, and strategies like FlashAttention and KV cache optimization are explored.",
            "result_discussion": "The results show attention modules as bottlenecks in both prefill and decoding stages. Optimizing the implementation of attention and related operations via FlashAttention and effective KV cache management significantly improves performance. Batching strategies and CUDA kernel fusion also enhance arithmetic intensity and efficiency in decoding. The primary time-consuming operations remain MHA and FFN projections even with optimizations, suggesting future efforts focused on improving matrix multiplication performance.",
            "ablation_id": "2404.11502v1.No2"
        }
    ]
}