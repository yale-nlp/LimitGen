{
    "title": "PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination",
    "abstract": "The potential for zero-shot generalization in vision-language (V-L) models such as CLIP has spurred their widespread adoption in addressing numerous downstream tasks. Previous methods have employed test-time prompt tuning to adapt the model to unseen domains, but they overlooked the issue of imbalanced class distributions. In this study, we explicitly address this problem by employing class-aware prototype alignment weighted by mean class probabilities obtained for the test sample and filtered augmented views. Additionally, we ensure that the class probabilities are as accurate as possible by performing prototype discrimination using contrastive learning. The combination of alignment and discriminative loss serves as a geometric regularizer, preventing the prompt representation from collapsing onto a single class and effectively bridging the distribution gap between the source and test domains. Our method, named PromptSync, synchronizes the prompts for each test sample on both the text and vision branches of the V-L model. In empirical evaluations on the domain generalization benchmark, our method outperforms previous best methods by 1% in base-to-novel generalization, and by 2.84% in cross-dataset transfer tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Training Vision-Language Models (VLMs) with large-scale image-text pairs is known for imparting robust generalization capabilities across diverse downstream tasks. However, training these models from scratch for each downstream task is very time-consuming. Moreover, the essence of pre-training with a large-scale dataset is lost when the pre-trained model is not generalizable across downstream tasks. This is due to unexpected changes in data distribution, and the sensitivity to these shifts leads to a decline in performance. To tackle this, there exist three most commonly used techniques: fine-tuning, prompt tuning, adapter, and LoRA. Among these, prompt tuning is the simple, recent, and most widely used technique for foundation models. However, prompt learning/tuning approaches are used during the training phase to learn representative prompts based on the training data for the downstream task. This approach does not specifically address the distribution shift present in the dataset. Recent methods, TPT and PromptAlign, adjust the learnable prompt tokens dynamically during testing to enable test-time adaptation and align the context of the test sample as per the seen distribution by the model. Specifically, TPT updates the learnable prompt tokens (keeping the model parameters frozen) by minimizing the entropy of top-confidently predicted samples, acquired through diverse augmented views of the incoming test sample. Additionally, PromptAlign aligns token distribution of the test sample in the visual branch with the pre-computed statistics of the complete proxy source dataset irrespective of the fact that one class distribution may have different mean and variance than the other classes.\n\nIn this work, we demonstrate the multi-modal test-time adaptation of prompts. In contrast to PromptAlign, which aligns the distribution for the complete source dataset with the test sample, we propose class-aware prototype alignment to address the distributional shift on a class-wise basis. For instance, in an open world there are 360 different breeds of dogs compared to only 71 for cats, leading to one class having higher variance than the others. For each test sample, we obtain randomly augmented views (for both text and image) that are fed to the model for prompt tuning on both the textual and visual branches. We adapt the learnable prompt tokens by aligning the prototype for the test sample and confident augmented views with the pre-computed class prototypes (obtained from the proxy source dataset) weighted by the mean probability of each class obtained from confident augmented views. Before alignment, we update the prompt tokens on both the text and visual branches using prototype discrimination and then use updated prompts to align the test sample and augmented views with class prototypes using mean class probabilities. This is based on the idea that the prototype vector can capture the complete information of mean and variance for each class distribution and hence it mitigates the class collapse (during test time adaptation) due to high variance of particular classes.\n\nOur contributions can be summarized as follows:\n- We propose a class-aware prototype alignment technique for individual test samples to align the context of each test sample with the source distribution on a class-wise basis, thereby mitigating the effects of distributional shift between classes.\n- We propose class-aware prototype discrimination to discover the class distribution for efficient alignment. Additionally, we propose the offline computation of class prototypes from a proxy source dataset for foundation V-L models.\n- We propose multi-modal test-time prompt tuning for both text and visual branches. Empirical evaluation on base-to-novel generalization, domain generalization, and cross-dataset transfer shows the efficiency of our method over existing methods."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Vision-Language (V-L) foundation models like CLIP and ALIGN have emerged as robust zero-shot generalizable models. They integrate image and text modalities through pre-training on extensive image-text pairs. However, adapting these models to specific downstream tasks with limited data remains challenging. Recent methods explore prompt tuning in CLIP-like models, treating prompts as continuous learnable vectors and fine-tuning them while keeping the model parameters frozen. CoOp proposed fine-tuning CLIP by learning a set of prompts in the text encoder. CoCoOp, an improvement over CoOp, dynamically conditions the text prompts by the image embeddings. MaPLe is a deep prompting baseline that tunes prompts on both text and image branches, further conditioning image prompts on text prompts using a V-L coupling function. However, these approaches necessitate training data for prompt learning, limiting adaptation to novel datasets during test time. Recent approaches like TPT aim to learn prompts exclusively at test time but encounter challenges in handling distribution misalignment between CLIP\u2019s pre-training data and downstream test data. PromptAlign addresses this by introducing token distribution alignment in the image branch. However, it does not account for the potential variance in class distributions. In contrast, our method, inspired by a multi-modal prompting variant, actively aligns class prototypes by leveraging a proxy dataset as a substitute for unavailable CLIP pre-training data. To our knowledge, our approach is the first to explicitly address class-aware distribution misalignment in V-L foundational models during test time."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "Revisiting CLIP: Our approach is based on the pre-trained V-L model: Contrastive Language-Image Pre-Training (CLIP). It consists of a text and visual encoder (denoted by  and , respectively, and their pre-trained parameters are represented by , respectively), used for mapping the text and image to the vector representation, respectively. The input image is , which is divided into  patches, and the [CLS] token is prepended to these  patch tokens that are projected to produce , where  is the embedding for the corresponding patch token in . The image encoder produces latent visual feature representation  with transformer blocks from . The class label  is embedded within a text template, such as \u201ca photo of a <CLS>\u201d resulting in , where SOS and EOS are the start and end token embeddings and  and  are the token embeddings corresponding to the text template and the class label, respectively. Similarly, the text encoder  encodes  with transformer blocks to produce latent text feature representation . For zero-shot inference, each text feature for class labels  is paired with an image feature to compute the similarity score  where  denotes cosine similarity. The predicted probability on X for each  is given as , where  is the temperature of softmax.\n\nPrompt Tuning: CLIP integrates a considerable pool of knowledge derived from its training on millions of image-text pairs characterized by varying degrees of noise. Prompt tuning methods aim to extract the rich features learned by the CLIP model. Recent approaches [46  ###reference_b46###, 47  ###reference_b47###, 21  ###reference_b21###, 3  ###reference_b3###, 43  ###reference_b43###] append extra learnable prompts to the input of image and text encoders while keeping them frozen. Modified input prompts with frozen encoders generate undistorted and rich CLIP features, where prompt tuning tries to map the context to the source distribution, i.e., the CLIP pre-training dataset. In our work, we use a recent multi-modal prompting baseline [21  ###reference_b21###] where prompt tuning is performed on both the text and image encoders.\n\nSpecifically, the image and text encoders process the input  and  respectively. The learnable prompts  and  represent the  visual and  textual tokens, respectively. We will call prompts  and  as p only. Our approach is based on deep prompting, as in[21  ###reference_b21###], along with text and image prompts at subsequent transformer blocks. We suggest referring to [21  ###reference_b21###] for more details on baseline architecture.\n\nTest Time Adaptation: Test-time adaptation aims to boost generalisation in a zero-shot manner. Existing methods, Test time prompt tuning (TPT)[35  ###reference_b35###] and PromptAlign[33  ###reference_b33###], both are introduced to provide the model context that is customized for each individual test sample in order to extract rich knowledge from CLIP. For both methods, several augmented views  are generated from the given test sample . The average entropy for the filtered views (selected using a confidence threshold) is then used to update the prompts p using the following unsupervised objective:\n\nwhere  is the average of vector class probabilities (over the filtered augmented views) produced by the model. Additionally, PromptAlign uses distribution alignment loss, which aligns the mean and variance of filtered augmented views of the test sample with source statistics across layers of the model."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Proposed Method: PromptSync",
            "text": "The multi-modal test-time prompt tuning method, PromptAlign [33], updates text and visual prompts using entropy loss and distribution alignment loss with highly confident augmented views (obtained from a test sample). PromptAlign, despite considering the distribution, does not take into account the fact that the distribution of each class/domain can be entirely different from other classes/domains, and hence using the source statistics of mean and variance for distribution alignment can still be suboptimal. Inspired by prototype learning [38] and Extreme-Multi-PatchSSL (EMP-SSL) [39], which establish a prototype/benchmark for each class/sample, we propose class-wise prototype alignment between original and augmented views for both source and test samples. The architecture of PromptSync is shown in Figure 1. We use the parameter update from prototype discrimination to generate the class probabilities for the test sample and its augmented views. We accumulate the average of gradients from prototype alignment loss weighted by class probabilities for confident augmented views. The accumulated gradient over multiple iterations is then applied for prompt tuning during test-time adaptation."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Class-aware Prototype Generation",
            "text": "We generated prototypes for each class for both text and visual branches. The prototype for each class is computed using a proxy source dataset. For a test sample and its random views (generated using a set of augmentations on ), the prototype vector is generated. Let\u2019s denote the token features of a sample at the output of the text encoder and visual encoder as  and , respectively. The prototype for a sample from text and visual branches is given as:\n\nwhere  represents the total number of tokens (learnable and non-learnable for both text and visual), excluding EOS, SOS, and CLS.  represents textual branch and visual branch respectively. For the proxy source dataset, the class-aware prototype is obtained as:\n\nwhere  contains all samples for class . The prototypes for augmented views are calculated using the augmented samples for each class denoted as  and the corresponding prototypes are denoted as ,  and  respectively."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Prototype Discriminating Loss",
            "text": "The discriminating loss is responsible for training learnable prompts to distinguish the context of samples from one class compared to other classes. This goal is achieved by pushing the class prototype for both text and visual branches away from the prototype of class where . Likewise, we pull prototypes and for the same class and push away augmented ones for . In this regard, contrastive learning [7, 14, 8, 23] offers a solution to pull prototypes of positive pairs and push away negative pairs. We refer to [34] to propose our discriminating loss, formally expressed as:\n\nwhere  and . The prototypes and additionally contain when . The resulting prompt update (learnable prompt tokens) is obtained after applying gradients for the discriminating loss. Since the proxy dataset will remain the same for all the test instances, the updated prompt can be saved and restored each time for an incoming test sample. We presented the study on performance and latency with and without saving these updated prompts in Appendix 10. For the rest of the paper, we generalize our method without requiring to save these updated prompts."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Prototype Alignment Loss",
            "text": "Loss can effectively separate different classes, but it cannot tune the prompt for the test sample, which comes from a different distribution than the source distribution. To address this, we propose the prototype alignment of the test sample (and its augmented views) with the class prototype obtained from the source distribution. We weigh the prototype alignment by the probability of the test sample belonging to a particular class. The probability is defined as the mean of probabilities (for class c) produced with the updated prompt across filtered augmented views (preserved after the confidence selection filter) including the test sample.\n\nThe alignment of the sample with the class prototypes for both text and visual branches is calculated as follows:\n\nThere is an issue with MSE loss since it gives an equal penalty (e.g., 0.1) for an increase from 1.2 to 1.3 and 1.7 to 1.8. However, we want to penalize more for the increase from 1.2 to 1.3 because an increase in MSE in the smaller range should be penalized more to preserve the base class performance. To address this, we penalize with a logarithmic scale, i.e., we use a logarithm for the penalty. Similarly, the penalty should be applied for angle alignment. The updated amplitude and angle alignment loss are adjusted accordingly. We combined the amplitude and angle loss with equal importance; thus, the prototype alignment loss is defined based on these considerations."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Algorithm Details",
            "text": "In order to compute the prototype discriminating loss on the source dataset, we require the pre-training dataset of the CLIP model. However, it was trained on over 400 million image-text pairs, which are not publicly available. Nevertheless, in previous works[2, 33], CLIP has been heavily tuned on the ImageNet[11] dataset to achieve excellent zero-shot performance. Hence, we use ImageNet as the proxy for the source dataset to compute prototypes for each class. These prototypes are computed offline for both the sample and its augmented views, and they are used directly during test-time adaptation. During each iteration of test-time adaptation, the meta-train stage is entered first. The model starts training using the prototype discriminating objective, and gradients are calculated, resulting in the prompt update (iteration). Subsequently, the meta-test stage is executed. Here, the augmented views are first filtered using a confidence threshold over predicted probabilities using the updated prompts. The mean probabilities are computed and used as weights. The model is trained, and the gradient of prototype alignment loss is calculated. We average out the gradients over all samples. Finally, the prompts are updated using combined objective. For the process, we accumulate the averaged gradients before final prompt update."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We have evaluated PromptSync on different benchmark settings (Appendix 9 ###reference_###) with different datasets described below:  \nDatasets: For domain generalisation setting, we follow PromptAlign [33 ###reference_b33###] and evaluated our method on four out-of-distribution (OOD) variants of ImageNet [11 ###reference_b11###]: ImageNetV2 [32 ###reference_b32###], ImageNet-Sketch [40 ###reference_b40###], ImageNet-A [18 ###reference_b18###] and ImageNet-R [17 ###reference_b17###]. We also consider the evaluation on a recent and challenging benchmark, namely, Photorealistic Unreal Graphics (PUG) dataset [4 ###reference_b4###], comprised of different textures, sizes, orientations, and backgrounds. For cross-dataset transfer setting, we follow TPT [35 ###reference_b35###] and evaluate the performance on 10 diverse image classification datasets with varying complexities for visual recognition tasks. This includes Caltech 101 [12 ###reference_b12###] for generic objects. Five fine-grained datasets (spanning images of animals, flowers and transportation) are StanfordCars [24 ###reference_b24###], Food101 [5 ###reference_b5###], Flowers102 [27 ###reference_b27###], FGVC-Aircraft [25 ###reference_b25###], OxfordPets [29 ###reference_b29###]. Moreover, four datasets, namely, SUN397 [37 ###reference_b37###], DTD [10 ###reference_b10###], UCF101 [36 ###reference_b36###], and EUROSAT [15 ###reference_b15###], comprise scenes, textures, human actions, and satellite imagery, respectively. For base-to-novel generalisation, we follow [21 ###reference_b21###] and evaluate our method on ImageNet and the 10 image classification datasets.  \nBaselines: We compared PromptSync with existing few-shot prompt learning methods for CLIP adaptation; these are CoOp [47 ###reference_b47###], CoCoOp [46 ###reference_b46###], TPT [35 ###reference_b35###], and PromptAlign [33 ###reference_b33###]. MaPLe [21 ###reference_b21###] is a multi-modal prompt learning baseline that adapts CLIP by learning prompts on both text and visual branches. TPT [35 ###reference_b35###] and PromptAlign [33 ###reference_b33###] are the test-time prompt tuning methods that tune the prompt for each incoming test sample, achieving state-of-the-art performance in prompt learning.  \nImplementation Details: We ran all experiments on a single NVIDIA A100 40GB GPU. Following [21 ###reference_b21###], we trained on ImageNet with 16-shot training data selected at random for each class using 2 prompt tokens for a depth of 3 layers (on CLIP ViT-B/16 backbone architecture). We optimized the prompts on both the text and visual branches using a single test image. We augmented each test image with 127 different views using random resized crops, background substitution, horizontal flip augmentations, and visual corruption. For text augmentation, we used hyponyms, synonyms, and meronyms from WordNet[26 ###reference_b26###]. Moreover, we generated various text prompts from pre-trained LLMs [6 ###reference_b6###]. Additionally, we randomly masked one of the learnable tokens for 15% of augmented views. We computed the gradients of alignment loss for a batch size of 128 images, including the original image. During the meta-train stage, we updated the original parameters (using a single iteration) and then optimized the prompts in the meta-test stage by calculating the gradients of alignment loss w.r.t. the updated parameters accumulated for a single () iteration to facilitate the one-to-one comparison with baselines. We obtained the top 10% confident predictions of augmented views based on the lowest entropy. We used the AdamW optimizer and a learning rate of for the fine-grained datasets and for the rest of the datasets."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Domain Generalization",
            "text": "We demonstrate that all test-time adaptation methods exhibit better performance (Table 1  ###reference_###) compared to the pre-trained CLIP model, highlighting the advantage of tuning V-L models at test time. Furthermore, we evaluated the ImageNet-trained model on various out-of-distribution (OOD) datasets and observed consistent improvement in performance compared to existing state-of-the-art (SOTA) approaches. The detailed results for each domain dataset are presented in Tables 1  ###reference_### and 2  ###reference_###. This confirms that alignment and discriminative training with augmented views on both the text and visual branches enhance the generalization performance of V-L models like CLIP."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Base to Novel Generalization",
            "text": "Table 3 presents the detailed performance report of PromptSync on base and novel classes across 11 recognition datasets. On average, our strategy outperforms the model performance by 1.29% on base classes and nearly 1% on novel classes. We observe that PromptAlign, based on a distribution alignment strategy, outperforms for novel classes in most cases, with an average improvement of 0.79% compared to the best-performing model. However, the margin of improvement is very low. In contrast, with TPT, the performance drops in some instances, such as for OxfordPets, Eurosat, and UCF101. This demonstrates that: 1) test-source alignment is crucial for prompt tuning. 2) Prompt tuning alone in the text branch is not sufficient for zero-shot generalization. Since distribution alignment does not promote discriminative learning and the entropy loss on the test dataset is noisy, PromptSync outperforms with class-aware prototype discrimination and alignment across different augmented views. Averaging the gradients further motivates domain-agnostic prompt tuning on both the text and visual branches. This enhances the zero-shot generalization of the V-L model compared to other state-of-the-art approaches. Moreover, our strategy for prompt tuning does not lose information for base classes."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Cross-Dataset Transfer",
            "text": "In Table 4, we compared the transfer performance of PromptSync with existing state-of-the-art methods using prompt learning. We evaluated methods for transfer performance across diverse cross-datasets. PromptSync consistently outperforms the previous best method, i.e., PromptAlign [33], across all cross-datasets, providing an average improvement. Compared to PromptAlign, which outperforms the previous method MaPLe + TPT by a very small margin, our method shows a significant average improvement over MaPLe + TPT. Other methods, CoOp and CoCoOp, on average, perform worse than zero-shot CLIP + TPT (except ProDA [45]). This affirms that both text-visual alignment and domain-agnostic parameter updates result in better transfer generalization across cross-datasets in V-L models. As opposed to our method, the previous approaches were not consistent in performance across all datasets, which further affirms the advantage of a domain-agnostic training strategy."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Ablation",
            "text": "Class-Aware Prototype Alignment: Table 5  ###reference_### summarizes the comparison between two alignment strategies: distribution alignment of the test sample with the class-agnostic source distribution. All results are on the ImageNet-A dataset. PromptAlign adopted distribution alignment along with averaged cross-entropy for prompt tuning. However, we perform domain-agnostic parameter updates with class-aware prototype alignment for the test sample. As shown in Table 5  ###reference_###, PromptAlign\u2020 without entropy loss is as good as vanilla MaPLe. This is due to the fact that distributional alignment does not promote any discriminative learning in the absence of entropy loss. However, because entropy loss is noisy due to the poor performance of the vanilla zero-shot V-L model, we propose the stronger discriminative loss of class prototype alignment for prompt tuning with source and test samples with augmented views. PromptSync\u2020 without entropy loss outperforms the corresponding counterpart PromptAlign\u2020. This is because the class-aware prototype alignment has both alignment and discriminative properties, thus improving test-time adaptation on its own. With additional signals from predicted probabilities for each class, the class-aware prototype alignment acts as a geometric regularizer, mitigating class collapse in prompt representation.\nLoss variants: We conducted an ablation study on amplitude and angle loss for the class-aware prototype alignment objective. Table 6  ###reference_### compares three loss choices: 1) amplitude loss, 2) angle loss, and 3) amplitude + angle loss. Clearly, the combination of amplitude and angle performs better than other choices. The formulation for the combination of amplitude and angle loss is the same as in equation 11  ###reference_###. We further investigated other variants, i.e., combining two of them without taking the log: 1) subtraction between amplitude and angle (sub) 2) the summation of exponential of both losses (sum_exp). Clearly, the formulation in equation 11  ###reference_### () performs best among other variants. Ablation on the proxy dataset is given in Appendix 12  ###reference_###, and ablation on performance and latency with and without saving updated prompts is provided in Appendix 10  ###reference_###. We also compared the number of augmented views and prompt updates in Appendix 11  ###reference_###."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Performance and Latency",
            "text": "The experiments presented in the Table 7  ###reference_### (Appendix) involve a comparison of different methods, namely MaPLe + TPT, PromptAlign, PromptSync*, and PromptSync. In these experiments, we evaluated the top-1 average accuracy (%) and latency (in hours for a single prompt update) of each method. Specifically, we investigated PromptSync with and without saving the updated prompt obtained after prototype discrimination, with the variant denoted as PromptSync* indicating the adaptation of prompt tokens for test samples after restoring saved prompt tokens.\nThe results, as shown in Table 7  ###reference_###, include latency measurements represented in hours for a single prompt update, and all evaluations are conducted on the ImageNet-A dataset. Notably, the PromptSync* variant demonstrates a faster processing time compared to the full PromptSync method, with only a marginal drop in performance. This outcome underscores the achieved generalization through prototype alignment. Furthermore, in comparison to previous methods such as MaPLe + TPT and PromptAlign, the PromptSync* variant exhibits only a slight increase in latency (0.03 hours) while still improving overall performance."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Sensitivity Comparison",
            "text": "We further performed the sensitivity comparison of our method as compared to other state-of-the-art baselines. In Appendix, Figure 2  ###reference_###(a) shows the comparison of performance during test time adaptation as the number of views increases. All the\nresults are on ImageNet-A dataset. In comparison to PromptAlign and MaPLe + TPT, their performance almost plateaus around 64 views with insignificant improvement further, while PromptSync shows a consistent improvement with the increase in views and insignificant improvement beyond 128. This proves the generalizability achieved by our method since it optimises base CLIP over a larger number of possible shifts in the dataset, resulting in better performance. Figure 2  ###reference_###(b) shows the performance comparison as the number of prompt update steps increases. All the methods increase their performance with an increase in the number of steps; however, our method shows better adaptation to the test sample with more steps in comparison to PromptAlign and MaPLe + TPT. For apples-to-apples comparison we perform a single-step update (128 views) following TPT [35  ###reference_b35###]."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "LAION400M Proxy Dataset Analysis",
            "text": "Given CLIP\u2019s impressive zero-shot performance on ImageNet, we opted for ImageNet as a viable proxy source dataset, aligning with prior research [33  ###reference_b33###]. We worked with a subset of LAION400M, comprising 2.5 million images (2 times the size of ImageNet). Furthermore, we carried out an ablation study on the alignment strategy using LAION400M as the source dataset, a dataset known to mirror CLIP\u2019s training dataset [9  ###reference_b9###]. The results for this ablation study is shown in Table 8  ###reference_### (Appendix). Notably, the performance impact remains consistent when utilizing this subset of LAION400M alongside ImageNet. Source class prototypes are computed on the proxy source data to derive the distribution for alignment during test time. As this proxy dataset aligns with the model\u2019s training set, this offline computation remains unchanged despite environmental shifts and only necessitates computation once."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Benchmark Settings",
            "text": "###figure_2### ###figure_3### Base-to-Novel Generalisation: Following MaPLe [21  ###reference_b21###], we evaluate PromptSync on a zero-shot setting. We split the dataset into base and novel classes. The model is trained only on the base classes in a few-shot setting and evaluated on the base and novel classes.\nCross-dataset Transfer: We evaluate PromptSync on the ImageNet[11  ###reference_b11###] pre-trained model on other datasets to determine the transfer performance. Following CoCoOp[46  ###reference_b46###], our model is trained on all 1000 ImageNet classes in a few-shot manner.\nDomain Generalisation: We evaluate PromptSync on out-of-distribution (OOD) datasets for domain generalizability. Similar to cross-dataset, we evaluate our ImageNet-trained model directly on OOD datasets, which are described in Section 4  ###reference_###."
        }
    ],
    "url": "http://arxiv.org/html/2404.07520v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "6",
            "9"
        ]
    },
    "research_context": {
        "paper_id": "2404.07520v2",
        "paper_title": "PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination",
        "research_background": "### Motivation:\n\nThe motivation for this paper arises from the significant challenge in training Vision-Language Models (VLMs) with large-scale image-text pairs, primarily the time-consuming process of training these models from scratch for each downstream task. There is a recognized need for models that retain their generalization capabilities across various tasks post pre-training, which is compromised when these models encounter unexpected changes in data distribution, leading to a performance decline. The motivation also stems from the observation that current methods such as prompt tuning do not specifically address distribution shifts present in the datasets. This paper aims to enhance the robustness and adaptability of VLMs in the presence of such shifts, particularly during the test phase, without modifying the model parameters.\n\n### Research Problem:\n\nThe primary research problem tackled in this paper is the adaptation of Vision-Language Models to various downstream tasks at test time, focusing specifically on addressing distributional shifts on a class-wise basis. Existing methods like prompt tuning, TPT, and PromptAlign offer adaptations but do not sufficiently address class-related distribution variances, which is crucial for maintaining performance across diverse and unbalanced datasets. This paper proposes a novel approach to mitigate these issues by dynamically adjusting prompts during the test phase, considering class-wise distribution differences to enhance zero-shot generalization.\n\n### Relevant Prior Work:\n\n1. **Vision-Language Models (VLMs) Training and Robustness**: Studies [31], [20], [44], [41], [42], and [1] have highlighted the robustness and generalization capabilities of VLMs trained with large-scale image-text datasets.\n\n2. **Challenges in Downstream Task Generalization**: References [16], [32], and [30] discussed the challenges VLMs face with unexpected data distribution changes that lead to performance declines.\n\n3. **Common Techniques for Adaptation**:\n   - **Fine-Tuning** [28]\n   - **Prompt Tuning** [47], a recent and widely used technique for foundation models [47], [46], [21], [22], [43]\n   - **Adapters** [13]\n   - **LoRA** [19]\n\n4. **Test-Time Adaptation Methods**:\n   - **TPT** [35]: Updates learnable prompt tokens during the test phase by minimizing entropy of top confidently predicted samples from diverse augmented views.\n   - **PromptAlign** [33]: Aligns the token distribution of the test sample in the visual branch with the pre-computed statistics of the complete proxy source dataset without considering class-wise distribution variations.\n\nBy exploring the limitations of these existing methods, particularly in handling class-wise distribution shifts, the paper proposes a novel class-aware prototype alignment and discrimination approach to improve VLMs' adaptability and generalization at test time.",
        "methodology": "### Methodology\n\n#### Revisiting CLIP\n\nOur approach is founded on the pre-trained Vision-Language (V-L) model: Contrastive Language-Image Pre-Training (CLIP). CLIP employs a text encoder () and a visual encoder (), with pre-trained parameters denoted by and respectively, to map text and images into vector representations. The input image  is segmented into  patches, and a [CLS] token is prepended to these  patch tokens to produce , where  represents the embedding for each patch token in . The image encoder then processes this to generate a latent visual feature representation  using a series of transformer blocks within .\n\nThe class label  is embedded within a text template, such as \u201ca photo of a <CLS>\u201d, resulting in , where SOS and EOS represent the start and end tokens, and  and  are the token embeddings for the text template and class label, respectively. The text encoder  subsequently encodes  with its transformer blocks to produce a latent text feature representation . For zero-shot inference, text features for class labels  are paired with image features to compute the similarity score  via cosine similarity (). The predicted probability for each  on X is calculated as , with  being the temperature of the softmax function.\n\n#### Prompt Tuning\n\nCLIP integrates extensive knowledge derived from training on millions of image-text pairs of varied noise levels. Prompt tuning methods are designed to extract these rich features learned by CLIP. Recent methods [46\u00a0###reference_b46###, 47\u00a0###reference_b47###, 21\u00a0###reference_b21###, 3\u00a0###reference_b3###, 43\u00a0###reference_b43###] involve appending extra learnable prompts to the input of the image and text encoders, while keeping the encoders frozen. These adjusted input prompts aim to map the context back to the CLIP pre-training dataset, generating undistorted and rich CLIP features.\n\nIn our work, we employ a recent multi-modal prompting baseline [21\u00a0###reference_b21###] where both text and image encoders undergo prompt tuning. Specifically, our approach uses the learnable prompts  and , representing the visual and textual tokens respectively, which we will collectively refer to as p. This deep prompting involves integrating text and image prompts at succeeding transformer blocks. For a detailed understanding of the baseline architecture, refer to [21\u00a0###reference_b21###].\n\n#### Test Time Adaptation\n\nTest-time adaptation is employed to enhance generalization capabilities in a zero-shot setting. Existing methods such as Test Time Prompt Tuning (TPT) [35\u00a0###reference_b35###] and PromptAlign [33\u00a0###reference_b33###] customize the model\u2019s context dynamically for each test sample to extract rich CLIP knowledge. These methods generate several augmented views  from a given test sample . The average entropy of these filtered views (selected based on a confidence threshold) is then used to update the prompts p, guided by the following unsupervised objective:\n\n\\[ \\mathcal{L} = \\sum_{i=1}^{n} - \\mathbf{P}_{i} \\log(\\mathbf{P}_{i}) \\]\n\nwhere  is the average probability vector class over the filtered augmented views produced by the model. Additionally, PromptAlign uses a distribution alignment loss, which ensures that the mean and variance of filtered augmented views align with the source statistics across the layers of the model.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Datasets**:\n- **Domain Generalisation Setting**: Evaluated on four out-of-distribution (OOD) variants of ImageNet:\n  - ImageNetV2\n  - ImageNet-Sketch\n  - ImageNet-A\n  - ImageNet-R\n- **Cross-Dataset Transfer Setting**: Evaluated on 10 image classification datasets:\n  - Caltech 101 (generic objects)\n  - Five fine-grained datasets: StanfordCars, Food101, Flowers102, FGVC-Aircraft, OxfordPets\n  - Four other datasets: SUN397 (scenes), DTD (textures), UCF101 (human actions), EUROSAT (satellite imagery)\n- **Base-to-Novel Generalisation**: Evaluated on ImageNet and the 10 image classification datasets mentioned above.\n- **Additional Benchmark**: Evaluated on the Photorealistic Unreal Graphics (PUG) dataset which includes various textures, sizes, orientations, and backgrounds.\n\n**Baselines**:\n- Compared against few-shot prompt learning methods for CLIP adaptation:\n  - CoOp\n  - CoCoOp\n  - TPT\n  - PromptAlign\n- Multi-modal prompt learning baseline:\n  - MaPLe\n- Test-time prompt tuning methods:\n  - TPT\n  - PromptAlign\n\n**Implementation Details**:\n- Conducted on a single NVIDIA A100 40GB GPU.\n- Training:\n  - Used 16-shot training data selected at random for each class.\n  - Employed 2 prompt tokens for a depth of 3 layers on CLIP ViT-B/16 backbone architecture.\n  - Optimized prompts on both text and visual branches using a single test image.\n- Data Augmentation:\n  - Applied 127 different views to each test image: random resized crops, background substitution, horizontal flip augmentations, and visual corruption.\n  - Text augmentation using hyponyms, synonyms, and meronyms from WordNet.\n  - Generated various text prompts from pre-trained LLMs.\n  - Randomly masked one of the learnable tokens for 15% of augmented views.\n- Optimization:\n  - Computed gradients of alignment loss for a batch size of 128 images.\n  - Meta-train stage: Single iteration to update original parameters.\n  - Meta-test stage: Optimized prompts by calculating gradients of alignment loss with respect to updated parameters.\n  - Used AdamW optimizer with different learning rates for fine-grained datasets () and other datasets().\n\n**Main Experimental Results**:\n- **Domain Generalisation**: Achieved high performance on OOD variants of ImageNet, showing the robustness of PromptSync across different variations and perturbations.\n- **Cross-Dataset Transfer**: Demonstrated superior accuracy and generalization capability on diverse datasets compared to baseline methods.\n- **Base-to-Novel Generalisation**: Outperformed baseline methods on the ImageNet and 10 other image classification datasets, indicating effective generalization from base to novel classes.\n- **PUG Dataset**: Substantially better performance on the challenging PUG dataset, showcasing the method's capability in handling significant variations in data.\n\nThe results indicate that PromptSync\u2019s class-aware prototype alignment and discrimination mechanisms effectively reduce domain gaps, allowing for more robust and adaptable vision-language models."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The objective is to compare different alignment strategies for prompt tuning in V-L models, focusing on class-aware prototype alignment to address test-time adaptation and class collapse issues.",
            "experiment_process": "The experiment compares two alignment strategies: distribution alignment with the class-agnostic source distribution and class-aware prototype alignment. This is conducted on the ImageNet-A dataset. The baseline approach, PromptAlign, uses distribution alignment along with averaged cross-entropy loss. The proposed method, PromptSync, uses class-aware prototype alignment without entropy loss. This alignment is further refined with augmented views and predicted probabilities acting as a geometric regularizer.",
            "result_discussion": "PromptSync without entropy loss outperforms its counterpart PromptAlign without entropy loss because of its combined alignment and discriminative properties, improving test-time adaptation and preventing class collapse in prompt representation.",
            "ablation_id": "2404.07520v2.No1"
        },
        {
            "research_objective": "To evaluate the impact of different loss variants on the performance of class-aware prototype alignment in PromptSync.",
            "experiment_process": "The study tests three different loss choices: amplitude loss, angle loss, and a combination of amplitude and angle loss. Additionally, two other combinations are explored: the subtraction between amplitude and angle, and the summation of the exponential of both losses. The performance of these loss variants is measured and compared.",
            "result_discussion": "The combination of amplitude and angle loss performs the best among all tested variants. The specific formulation outlined in equation 11 outperforms other variations, verifying that the combined loss offers superior results.",
            "ablation_id": "2404.07520v2.No2"
        },
        {
            "research_objective": "To investigate the performance and latency of different methods, particularly PromptSync, PromptSync*, MaPLe + TPT, and PromptAlign.",
            "experiment_process": "The methods are evaluated on the ImageNet-A dataset in terms of top-1 average accuracy and latency, with latency measured in hours per single prompt update. PromptSync* adapts prompt tokens for test samples after restoring saved prompt tokens, whereas normal PromptSync does not.",
            "result_discussion": "PromptSync* shows faster processing time compared to full PromptSync with only a slight drop in accuracy. It also exhibits improved overall performance with only a marginal increase in latency compared to previous methods like MaPLe + TPT and PromptAlign.",
            "ablation_id": "2404.07520v2.No3"
        }
    ]
}