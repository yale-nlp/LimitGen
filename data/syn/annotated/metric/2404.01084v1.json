{
    "title": "AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles",
    "abstract": "In this paper, we outline our submission for the SemEval-2024 Task 9 competition: \u2019BRAINTEASER: A Novel Task Defying Common Sense\u2019. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In Natural Language Processing (NLP), reasoning serves as the cognitive backbone, enabling systems to transcend mere language comprehension and delve into sophisticated understanding. Despite the excellence of Large Language Models (LLMs) in several linguistic tasks, their reasoning capabilities are still questionable to a non-negligible extent Floridi and Chiriatti (2020); Bender et al. (2021); Kauf et al. (2022); Zhang et al. (2023); Shi et al. (2023); Tyen et al. (2024); Giadikiaroglou et al. (2024), often posing the fundamental concerns of whether they can indeed reason or memorize exhaustively Yuan et al. (2022).\n\nSuch limitations can be probed via well-crafted datasets and benchmarks, showcasing varying LLM deficiencies at a time. As the core of the current paper, BrainTeaser Jiang et al. (2023b, 2024b) incorporates problems that stress models to think \"out-of-the-box\"; to this end, the key novelty of BrainTeaser is that in order to answer correctly, models need to defy default senses of concepts and common associations.\n\nAssuming that large-scale training and prompting may not always serve as universally applicable solutions towards flexible reasoning, we move one step back and leverage transfer learning techniques starting from smaller models based on masked language modelling, such as BERT Devlin et al. (2019) and consequent BERT-based encoders. Then, we proceed with similar techniques on LLMs, aiming to showcase that significant performance advancements using a small set of in-domain data for parameter updating can be achieved in comparison to merely querying the model\u2019s prior knowledge via prompting. Therefore, our contributions are:\n\nWe perform lightweight tuning on smaller encoder models and LLMs. We transform the multiple-choice problem to a binary classification one, aiming to explore diverging reasoning paths for models. We ground final performance on the models\u2019 \"prior knowledge\" in related problems. We delve into models\u2019 frequent failures to obtain a deeper understanding of reasoning cues that make models struggle the most.\n\nOur code is available on GitHub 111https://github.com/GiannisPana/AILS-NTUA-at-SemEval-2024-Task-9-Brainteaser."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "has enjoyed several advancements due to the surge of pre-trained language models and especially LLMs Sun et al. (2023  ###reference_b31###). Reasoning challenges incorporate commonsense reasoning Richardson and Heck (2023  ###reference_b26###), involving inference regarding everyday situations, mathematical reasoning Lu et al. (2023  ###reference_b22###), referring to the ability of solving mathematical problems, logical reasoning Yang et al. (2023  ###reference_b41###), which includes the systematic deduction of conclusions based on established principles and formal rules, causal reasoning Gendron et al. (2024  ###reference_b5###), which studies cause-and-effect relationships explaining why an event leads to another, and several other sub-tasks Vashishtha et al. (2020  ###reference_b36###); Wei et al. (2023  ###reference_b40###); Petersen and van der Plas (2023  ###reference_b25###).\nIn terms of reasoning evaluation, BigBench Srivastava et al. (2023  ###reference_b30###) comprises 204 reasoning tasks, targeting to explore the related capabilities of recent LLMs.\nSeveral dedicated datasets have been developed to tackle different reasoning challenges, including commonsenseQA Talmor et al. (2019  ###reference_b32###), WinoGrande Sakaguchi et al. (2019  ###reference_b27###), RiddleSense Lin et al. (2021  ###reference_b20###) and others; most of these datasets are incorporated in Tasksource Sileo (2023  ###reference_b29###). Especially RiddleSense questions aspects of reasoning close to BrainTeaser Jiang et al. (2023b  ###reference_b16###, 2024b  ###reference_b15###)."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Task and Dataset Description",
            "text": "The BrainTeaser task at SemEval-2024 (Jiang et al., 2023b, 2024b) features lateral thinking puzzles presented as multiple-choice questions (QAs). Each question offers four options, with one being the correct answer and the others serving as distractors. Additionally, the final option is always \"None of above\". It consists of two sub-tasks, Task A: Sentence Puzzle and Task B: Word Puzzle. In addition to the original puzzles, the dataset includes adversarial subsets created by manually modifying the original brain teasers while preserving their reasoning paths. The original data were perturbed in two ways: First, there is semantic reconstruction of each original question without altering the answers or the distractors. Second, the original data underwent context reconstruction, wherein the original reasoning path remains intact, but the brain teaser describes a new situational context. Overall, the dataset used for training and evaluation consists of triplets of data: original, semantic, and context reconstruction. Table 1 provides an example of the triplets of data that constitute the dataset.\n\nIn this sub-task, the sentence pairs are crafted in a manner that makes it relatively easy for humans to discern the correct statement, yet challenging for systems, even those equipped with commonsense understanding. Table 2 contains examples of the Sentence Puzzle dataset (on the left). The training data consists of 169 distinct multiple-choice QA sets, each accompanied by its semantic and context reconstructions, resulting in a total of 507 multiple-choice questions. This involves word-type brain teasers, where the answer defies the default meaning of the word and focuses on the letter composition of the question. The training dataset comprises 132 multiple-choice QAs, each accompanied by its semantic and context reconstructions, resulting in a total of 396 multiple-choice QAs. These brain teaser categories include puns, homophones, ambiguous words, and various other linguistic puzzles, as showcased in the examples provided in Table 2 on the right-hand side.\n\nThe Word Puzzle sub-task poses challenges not only for systems but also for humans in discerning the correct answer. The BrainTeaser dataset comprises 3 data splits, namely train, development (used during the practice phase), and the hidden test set, which was used for evaluation. Statistics are provided in Table 3. Throughout the evaluation phase, the leaderboard was kept concealed."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We focus on tuning language models belonging into two categories.\nFirst, we fine-tune variations of encoder models, namely BERT Devlin et al. (2019  ###reference_b3###), RoBERTa-large Liu et al. (2019  ###reference_b21###) and DeBERTaV3-base He et al. (2023  ###reference_b10###), to assess the impact of transfer learning using various datasets requiring similar reasoning abilities, apart from BrainTeaser. We study the problem using the provided multi-choice setup, but we also transform it into a binary classification task. Secondly, the encoders\u2019 results are compared with those obtained from fine-tuned LLMs using the BrainTeaser dataset. To achieve this, we fine-tune Llama 2 Touvron et al. (2023b  ###reference_b34###), Phi-2 Gunasekar et al. (2023  ###reference_b9###) and Mistral-7b Jiang et al. (2024a  ###reference_b14###), which have already demonstrated enhanced reasoning abilities. In this regard, we examine the effect of the model size on our task, which has already been reported in the literature to significantly influence the reasoning abilities of the models Touvron et al. (2023b  ###reference_b34###); Wei et al. (2022  ###reference_b39###), along with other tuning hyperparameters. Model details are presented in App. A  ###reference_###.\nFirst, we evaluate the effects of the pre-training on our task. Thus, we select two variations of each encoder: the vanilla one (using the default pre-trained basis and fine-tuned on BrainTeaser data only) and one that has undergone additional pre-training using supplementary commonsense reasoning datasets before fine-tuned on BrainTeaser. In the second case, we use the following pre-trained models:  1 BERT-SE: a BERT-base-uncased version pre-trained on the multiple-choice dataset used in SemEval-2020 Task 4b Wang et al. (2020  ###reference_b38###) 2 RoBERTa-WNGRD: a RoBERTa-large version pre-trained on the WinoGrande dataset, and  3 DeBERTaV3-TS: a DeBERTaV3-base model, pre-trained on diverse commonsense reasoning datasets, and fine-tuned with multi-task learning on over 600 tasks from the Tasksource collection.\nThis strategy involves treating the problem as multi-class classification: all four provided options are combined with the given question, and consequently these concatenated inputs are fed into the model, which is fine-tuned to select one of the four options as part of a multi-class classification problem.\nEach sample originally consisting of multiple-choice QAs with four available options, underwent the following transformation: each candidate answer (excluding the \"None of above\" option) was paired with the question receiving the label 0 if the choice was incorrect, or the label 1 for the opposite. In case all the 3 pairings returned 0, it is directly implied that \"None of above\" is the correct answer."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Encoder models",
            "text": "First, we evaluate the effects of the pre-training on our task. Thus, we select two variations of each encoder: the vanilla one (using the default pre-trained basis and fine-tuned on BrainTeaser data only) and one that has undergone additional pre-training using supplementary commonsense reasoning datasets before fine-tuned on BrainTeaser. In the second case, we use the following pre-trained models:  1 BERT-SE: a BERT-base-uncased version pre-trained on the multiple-choice dataset used in SemEval-2020 Task 4b Wang et al. (2020  ###reference_b38###  ###reference_b38###) 2 RoBERTa-WNGRD: a RoBERTa-large version pre-trained on the WinoGrande dataset, and  3 DeBERTaV3-TS: a DeBERTaV3-base model, pre-trained on diverse commonsense reasoning datasets, and fine-tuned with multi-task learning on over 600 tasks from the Tasksource collection.\nThis strategy involves treating the problem as multi-class classification: all four provided options are combined with the given question, and consequently these concatenated inputs are fed into the model, which is fine-tuned to select one of the four options as part of a multi-class classification problem.\nEach sample originally consisting of multiple-choice QAs with four available options, underwent the following transformation: each candidate answer (excluding the \"None of above\" option) was paired with the question receiving the label 0 if the choice was incorrect, or the label 1 for the opposite. In case all the 3 pairings returned 0, it is directly implied that \"None of above\" is the correct answer."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "LLMs",
            "text": "We demonstrate an in-depth examination of fine-tuning SoTa LLMs (Llama 2, Phi-2, and Mistral-7b) in the context of multi-class classification. Note that during inference, the models prompted to provide an explanation along with the label. This experimental step, which we have observed to improve the performance of the model, also provides a qualitative identification of flaws in the models\u2019 reasoning process. In our experiments, we explore various combinations of LoRA Hu et al. (2021  ###reference_b12###)  and  hyperparameters, using values of 16, 32, 64, and 128. For the analysis ahead, LLMs are denoted as model_r_a, reflecting these hyperparameters.\nAdditional technical information, including prompting details and specifics about QLoRA hyperparameters, is available in App. B  ###reference_###, C  ###reference_###, D  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experimental Results",
            "text": "Interestingly, the performance of the binary classification problem is significantly lower than that of the multi-class classification task. Initially, this behavior seemed counterintuitive since it appeared easier to determine whether a question is correct or not than to select the correct answer from four different options. However, this assumption is not accurate. Consider the word riddle: \u2018What is the capital in France?\" At first glance, the option \u2018F\u2019 seems incorrect, but when considering the options \u2018F,\u2019 \u2018E\u2019, \u2018A\u2019, and \u2018None of the above\u2019, \u2018F\u2019 emerges as the only correct answer, as it becomes apparent that the question refers to the capital letter rather than the capital city. Therefore, the diverse options provide crucial context to the models, explaining the superior performance of multi-class models. This lack of context is why we refrain from further exploring this methodology across all models in our study.\n\nThere are notable discrepancies between original and semantic contexts when compared to context reconstruction, particularly evident in the case of smaller encoder models.\n\nRegarding encoders, it is evident that, especially vanilla RoBERTa-large lacks robust commonsense reasoning and struggles to systematically handle ambiguity; in contrast, RoBERTa-large pre-trained on WinoGrande presents competitive performance. This notable enhancement (over 40%) due to WinoGrande pre-training suggests that this particular dataset effectively equips the model with the ability to understand word puzzle-related reasoning complexities, making its performance competitive despite the higher baseline reasoning benchmarks. \n\nOther than that, pre-training on other commonsense reasoning datasets does not significantly improve the overall performance for encoders. Conclusively, apart from WinoGrande, the rest of the extra pre-training datasets do not hold reasoning cues close to BrainTeaser\u2019s word puzzles.\n\nRegarding LLMs, Mistral-7b notably outperformed all others by a significant margin, even surpassing the 8 times larger model tuned using the same hyperparameters (Mixtral-8x7b). Llama 2 exhibited the worst results regardless of size (7/13 billion) and LoRA hyperparameters (r and a). Conversely, Phi-2 demonstrated relatively better performance, particularly considering its smaller parameter count (2.7 billion) compared to the other LLMs. However, both models performed worse compared to most fine-tuned encoders. This observation strongly confirms that word puzzles possess a distribution that diverges from the analytical commonsense reasoning required for sentence puzzles, entailing a unique set of cognitive demands.\n\nMistral-7b exhibits a trend where higher quality explanations were generated with higher values of lora rank r. However, the top-performing model showcased a configuration with r=16 and a=64. The QLoRA method explains why our top model has a rank of 16 instead of 128, contrary to common expectations. Drawing from the widespread presence of low-rank structures, we leverage the intrinsic low-rank structure in our problem. It is well-established that many tasks, particularly involving heavily over-parametrized models, exhibit low-rank properties post-training.\n\nOverall, our systems demonstrate proficiency in understanding and detecting wordplay patterns, consistently addressing ambiguity irrespective of contextual and semantic variations in brain teasers. Upon reviewing the short explanations provided with each prediction, we note thorough justifications even for incorrect answers. Errors typically adhere to specific wordplay patterns across original, semantic, and context multiple-choice questions."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we systematically evaluate pre-trained and fine-tuned encoders, along with instruction-tuned Large Language Models (LLMs), against two multi-class classification sub-tasks within the \"BRAINTEASER: A Novel Task Defying Common Sense\". We gain insights regarding the influence of leveraging in-domain data, the variability model scale and architecture introduce, as well as the examination of diverging reasoning paths. As future work, we will delve into further reasoning patterns LLMs tend to follow with regard to lateral thinking challenges."
        }
    ],
    "url": "http://arxiv.org/html/2404.01084v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2404.01084v1",
        "paper_title": "AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles",
        "research_background": "### Paper's Motivation:\nThe motivation behind this paper stems from the observation that despite the impressive achievements of Large Language Models (LLMs) in various linguistic tasks, their actual reasoning capabilities remain in doubt. This is particularly concerning when it comes to tasks that require unconventional or lateral thinking. The study aims to address these limitations and push the boundaries of LLMs' reasoning abilities.\n\n### Research Problem:\nThe core research problem addressed in this paper is the challenge of improving the ability of NLP models to tackle lateral thinking puzzles, which require thinking beyond default senses and common associations. Despite advances, current State-of-the-Art (SoTa) LLMs still struggle with these types of problems, showing limited accuracy and reasoning skills. The authors aim to enhance the performance of these models through techniques such as lightweight tuning and transfer learning, rather than relying solely on extensive training and prompting.\n\n### Relevant Prior Work:\nThe paper builds on several key prior works and concepts:\n- **Reasoning Limits of LLMs:** Observations by Floridi and Chiriatti (2020), Bender et al. (2021), Kauf et al. (2022), Zhang et al. (2023), Shi et al. (2023), and Tyen et al. (2024) on the questionable reasoning capabilities of LLMs. Yuan et al. (2022) posed fundamental questions about the models' ability to truly reason versus merely memorizing information.\n- **BrainTeaser Dataset:** Jiang et al. (2023b, 2024b) introduced the BrainTeaser dataset, which is designed to push models to think out-of-the-box and challenge their reasoning abilities by defying common associations. SoTa LLMs like ChatGPT exhibit limited success with this dataset, highlighting the challenge.\n- **Transfer Learning and BERT-Based Models:** The study draws on techniques involving masked language modeling and transfer learning, referencing foundational models like BERT (Devlin et al., 2019) to implement their approach, aiming to fine-tune smaller models with in-domain data to achieve better reasoning capabilities.\n\nBy integrating these insights, the paper seeks to advance the field of NLP reasoning, specifically targeting the improvement of performance in lateral thinking tasks through strategic model tuning and analysis.",
        "methodology": "### Methodology\n\nWe focus on tuning language models belonging into two categories to tackle lateral thinking puzzles as outlined in SemEval-2024 Task 9.\n\n**1. Fine-Tuning Encoder Models:**\n\nWe fine-tune variations of popular encoder models, specifically BERT, RoBERTa-large, and DeBERTaV3-base. The objective is to evaluate the impact of transfer learning using datasets requiring similar reasoning abilities in addition to the BrainTeaser dataset. \n\n**Multi-Choice to Binary Classification Conversion:**\n\nThough primarily performing experiments in a multi-choice setup, the task is alternately treated as a binary classification problem:\n\n- Each candidate answer (excluding the \"None of above\" option) is paired with the given question.\n- Each pairing is assigned a label: 0 for an incorrect choice and 1 for a correct choice.\n- If all three pairings return 0, then \"None of above\" is deemed the correct answer.\n\n**Encoder Models:**\n\n1. **BERT-SE:** BERT-base-uncased pre-trained on the multiple-choice dataset from SemEval-2020 Task 4.\n2. **RoBERTa-WNGRD:** RoBERTa-large pre-trained on the WinoGrande dataset.\n3. **DeBERTaV3-TS:** DeBERTaV3-base pre-trained on various commonsense reasoning datasets and fine-tuned with multi-task learning using the Tasksource collection of over 600 tasks.\n\nThese pre-trained models are then fine-tuned with the BrainTeaser dataset as part of a multiclass classification task. All four provided options are concatenated with the question, and the model is trained to select one of the four as the correct answer.\n\n**2. Fine-Tuning Large Language Models (LLMs):**\n\nWe also fine-tune LLMs, specifically Llama 2, Phi-2, and Mistral-7b, using the BrainTeaser dataset to compare their performance with that of the encoder models. These models are known for their enhanced reasoning capabilities due to their larger size and more sophisticated pre-training.\n\nThe detailed specifications of the models are presented in Appendix A.\n\n### Key Innovations:\n\n- **Binary to Multi-Class Conversion:** Transforming a multi-choice problem into a binary classification task for more granular evaluation.\n- **Cross-Comparison:** Comparing fine-tuned encoder models to LLMs to understand the impact of model architecture and pre-training on lateral reasoning tasks.\n- **Pre-Training Datasets:** Utilizing commonsense reasoning datasets to enhance models before fine-tuning on specific task data.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup:\n- **Tasks**: The experiment involved two sub-tasks, Task A: Sentence Puzzle and Task B: Word Puzzle.\n- **Datasets**: \n  - **Task A**: Comprises 169 distinct multiple-choice QA sets, each accompanied by semantic and context reconstructions, resulting in a total of 507 multiple-choice questions. Example pairs are provided in Table 2 (left-hand side).\n  - **Task B**: Consists of 132 multiple-choice QAs, each with semantic and context reconstructions, totaling 396 multiple-choice QAs. This sub-task focuses on word-type brain teasers involving puns, homophones, and ambiguous words, as shown in Table 2 (right-hand side).\n  - **Data Splits**: The dataset is divided into train, development (used during the practice phase), and a hidden test set used for evaluation. Statistics are available in Table 3.\n- **Evaluation Metrics**:\n  - **Instance-based Accuracy**: Evaluates each question individually.\n  - **Group-based Accuracy**: Assess questions as cohesive groups, scoring 1 only if all questions in a group are correctly solved. This broader metric measures the holistic performance of the system.\n\n#### Main Experimental Results:\n- **Results for Sentence Puzzle (Task A)**:\n  - Performance metrics highlighted the challenge for systems in distinguishing the correct statement amidst distractors.\n- **Results for Word Puzzle (Task B)**:\n  - Systems faced considerable difficulty with the linguistic intricacies of the word puzzles, akin to the challenges posed to human participants.\n\nAcross both tasks, the combination of instance-based and group-based accuracy metrics revealed the overall proficiency of models in handling lateral thinking puzzles, with detailed insights into strengths and weaknesses in reasoning through various scenarios and contexts."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Assess the impact of pre-training and fine-tuning on pre-trained transformer-based encoder models and their performance on BrainTeaser datasets.",
            "experiment_process": "Two variations of each encoder were evaluated: the vanilla version (default pre-trained model fine-tuned only on BrainTeaser) and another version with additional pre-training on commonsense reasoning datasets followed by fine-tuning on BrainTeaser. The models used include BERT (pre-trained on SemEval-2020 Task 4b), RoBERTa (pre-trained on WinoGrande), and DeBERTaV3 (pre-trained on diverse commonsense reasoning datasets). The problem was treated as a multi-class classification task, where each question combined with all four options is fed into the model to select the correct one.",
            "result_discussion": "Pre-training encoders on commonsense reasoning datasets significantly improved performance, with models capturing domain-agnostic features beneficial for BrainTeaser tasks. Additionally, several fine-tuned commonsense pre-trained encoders outperformed Llama 2 and Phi-2 models. Only Mistral-7b from the LLMs was able to surpass the encoder-type networks.",
            "ablation_id": "2404.01084v1.No1"
        },
        {
            "research_objective": "Examine the performance of state-of-the-art LLMs like Llama 2, Phi-2, and Mistral-7b in BrainTeaser multi-class classification tasks and analyze the impact of hyperparameter variations.",
            "experiment_process": "An in-depth examination of fine-tuning Llama 2, Phi-2, and Mistral-7b models, where models provide both a label and an explanation during inference. Various LoRA hyperparameter combinations (16, 32, 64, 128) were explored, and models were denoted as model_r_a to reflect these hyperparameters. Additional technical details and hyperparameters specifics were included in the appendix sections.",
            "result_discussion": "Mistral-7b consistently outperformed other models, even an 8 times larger Mixtral-8x7b. Llama 2 yielded the worst results regardless of size and hyperparameters, while Phi-2 performed better than Llama 2 but worse than fine-tuned encoders. Word puzzles posed a greater challenge compared to sentence puzzles, requiring unique cognitive demands. Higher quality explanations correlated with higher LoRA ranks, with the top-performing model having r=16 and a=64, leveraging intrinsic low-rank structures in heavily over-parametrized models.",
            "ablation_id": "2404.01084v1.No2"
        }
    ]
}