{
    "title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
    "abstract": "Many multilingual communities frequently engage in code-switching during conversations. This behaviour stresses the need for natural language processing technologies adept at processing code-switched text. However, data scarcity, particularly in African languages, poses a significant challenge, as many are low-resourced and under-represented. In this study, we prompted GPT 3.5 to generate Afrikaans\u2013English and Yoruba\u2013English code-switched sentences using topic-keyword pairs, linguistic guidelines, and few-shot examples. Our findings indicate that the quality of generated sentences for languages using non-Latin scripts, like Yoruba, is considerably lower when compared with the high Afrikaans\u2013English success rate. There is therefore a notable opportunity to refine prompting guidelines to yield sentences suitable for the fine-tuning of language models. We propose a framework for augmenting the synthetically generated code-switched data using GPT and propose leveraging this technology to mitigate data scarcity in low-resourced languages, underscoring the essential role of native speakers in this process.\n\n\n\nKeywords:\u2009code-switch, LLM, few-shot, prompting",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1.   Introduction",
            "text": "Multilingual communities, exemplified well by various African countries, often engage in code-switching, where two or more languages are used within a single discourse Poplack (2001a ###reference_b23###). This language practice highlights the need to develop more advanced natural language processing (NLP) technologies that can smoothly process and produce code-switched sentences.\n\nThere are numerous challenges in code-switching research. The main three are highlighted by Do\u011fru\u00f6z et al. (2021 ###reference_b8###) as follows: i) data, which is related to quantity, quality and availability; ii) evaluation, which refers to benchmarks and metrics; and iii) challenges related to end-to-end applications, particularly the ability to process and produce code-switched data.\n\nThe focus of this paper is on the first challenge regarding data. While code-switching frequently occurs in written forms, due to the ubiquitous use of social media platforms, leveraging this data in NLP applications for code-switching presents many challenges. These platforms can be invaluable in gathering code-switched data. Yet, the practical utility of such data is hindered by various factors, including the informal, inconsistent nature of online language (\u00c7etino\u011flu et al., 2016 ###reference_b6###). It is common to use acronyms, emojis and make spelling mistakes which affect quality and usability of such data (Srivastava et al., 2019 ###reference_b31###).\n\nTo address the shortage of available data, efforts have been made to create synthetic code-switched data using different methods: from using parallel corpora with linguistic constraints on where a switch can occur (Pratapa et al., 2018 ###reference_b25###; Rizvi et al., 2021 ###reference_b27###) to employing transformer-based models to generate sentences that adhere to lexical and syntactic rules (Riktika et al., 2022 ###reference_b26###). A more recent study evaluated prompting of large language models (LLMs) to generate code-switched data for South East Asian languages (Yong et al., 2023 ###reference_b38###). They explored a few prompting templates with a limited number of topics in a zero-shot manner and cautioned against the use of synthetically generated data without involving native speakers of the language.\n\nIn this paper, we build on the work of (Yong et al., 2023 ###reference_b38###) to address the question about GPT\u2019s ability to generate code-switched data. Our work overlaps in that we also use an LLM, OpenAI\u2019s GPT, and various topics in the prompts. We increase the number of topics and provide topic-related keywords in an effort to increase variety and reduce the model\u2019s propensity to default to certain words. Our goal is not to evaluate various prompting templates, however, we add linguistic guidelines in the prompts. We propose this as an approach towards language agnostic prompting. We also test the performance of GPT 3.5 with few-shot in-context examples. We specifically consider whether GPT can support the generation of larger code-switched datasets and to what extent.\n\nOur contributions are as follows: (i) we provide a framework to increase the variety of synthetically generated code-switched data by prompting OpenAI\u2019s GPT; and (ii) we position GPT as a pivot to address code-switched data scarcity in low-resource languages while emphasising the need for native speakers in the loop.\n\nIncreasing data availability is at the center of developing language models that serve multilingual communities. Our work is a step towards closing the gap in low-resourced and under-represented languages."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2.   Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1.   Code-Switching Research",
            "text": "Various types of code-switching have been identified but the type that attracts the most academic research is intra-sentential code-switching which can occur anywhere within a sentence boundary (Poplack, 1980  ###reference_b22###) and as a result, adds complexity in evaluation (Poplack, 2001b  ###reference_b24###). Another complex type is intra-word code-switching where the stem of one language is bound to another language (\u00c7etino\u011flu et al., 2016  ###reference_b6###; Van der Westhuizen and\nNiesler, 2018  ###reference_b32###).\nOver and above the issue of data diversity (Winata et al., 2022  ###reference_b36###), one of the major challenges in code-switching studies is related to data availability (Do\u011fru\u00f6z et al., 2021  ###reference_b8###). A survey by (Winata et al., 2022  ###reference_b36###) showed that up until October 2022, a relatively small amount of papers (ACL Anthology, 2023  ###reference_b1### and ISCA Proceedings, 2023  ###reference_b10###) focused on code-switching research in African languages with very few publicly available datasets. Eleven publications mention South African languages. The non-English South African languages referenced are isiZulu, isiXhosa, Setswana, Sesotho and Afrikaans. Only one proceeding includes Afrikaans code-switching (Niesler and De Wet, 2008  ###reference_b19###) with no published dataset. A paper by Van der Westhuizen and\nNiesler (2018  ###reference_b32###) introduced the first corpus on isiZulu, isiXhosa, Setswana, Sesotho curated from transcribed soap opera speech data and eight of the papers makes use of this dataset and is mainly focused on automatic speech recognition (ASR) systems.\nCode-switching in Kiswahili\u2013English is studied in two papers but no datasets were made available (Otundo and Grice, 2022  ###reference_b20###; Piergallini et al., 2016  ###reference_b21###). In addition to a survey by Winata et al. (2022  ###reference_b36###), one other paper was found that addresses Sepedi\u2013English code-switching. Modipa et al. (2013  ###reference_b17###) develop a corpus from a set of radio broadcasts to evaluate the implication of code-switching in ASR systems. This dataset is publicly available.\nThis brief review of the state of code-switching research in an African context motivates our work to develop methods for addressing data scarcity.\nA predominant approach to mitigating data availability issues involves augmenting existing datasets through the generation of synthetic code-switched data. Some of the methods to augment the earlier mentioned South African speech corpus include the use of word embeddings to synthesise code-switched bigrams to find similar words in the sparse training data (Westhuizen and Niesler, 2017  ###reference_b34###). Biswas et al. (2018  ###reference_b4###) evaluated adding out-of-domain monolingual data and synthesised code-switched data using an LSTM to augment the dataset.\nFor non-African languages, Rizvi et al. (2021  ###reference_b27###) developed a toolkit that generates multiple code-switched sentences using either the Equivalence Constraint or the Matrix Language Frame. The limitations are that it relies on a good sentence aligner and parser and parallel translated sentences as input. The notion is that this approach should work on any language pair. Winata et al. (2019  ###reference_b37###) implemented a sequence-to-sequence model for English-Mandarin code-switched data. Although the model does not require external knowledge regarding word alignments, it still relies on an existing English\u2013Mandarin code-switched dataset and parallel corpora. The work of (Liu et al., 2020  ###reference_b14###) introduced an attention-informed zero-shot adaptation method that relies on a limited number of parallel word pairs. The languages covered are German, Italian, Spanish and Thai, the latter two for natural language understanding. The shortcoming of the above-mentioned approaches is the diversity of data. Most existing code-switched datasets were collected from social media platforms such as Twitter and therefore limits the type of code-switching (Do\u011fru\u00f6z et al., 2021  ###reference_b8###).\nTo this issue, Riktika et al. (2022  ###reference_b26###) developed an encoder-decoder translation model for controlled code-switched generation. It uses monolingual Hindi and a publicly available Hindi\u2013English code-switched dataset as input to generate data that is faithful to syntactic and lexical attributes.\nYong et al. (2023  ###reference_b38###) proposed an approach that is independent of existing code-switched datasets or parallel corpora through prompting of LLMs.\nTheir objective was to test whether multilingual LLMs can generate code-switched text through prompting. They evaluated a variety of prompt templates and found that those explicitly defining code-switching gave the highest success rate. However, they also highlighted the sentences often contained word-choice errors and semantic inaccuracies which was more prevalent in the languages that don\u2019t use the English alphabet and Latin script. They limited the scope to five topics and did not include diversity as a measure. Their findings were that GPT\u2019s capability to generate code-switched data is superior to other LLMs, however, using this method without humans-in-the-loop is not advised.\nJha et al. (2023  ###reference_b12###) elaborated on LLMs such as GPT being prone to hallucinations where it provides factually inaccurate or contextually inappropriate responses. A solution to address this is to ensure carefully curated prompts. Furthermore, to avoid encoded biases, Bender et al. (2021  ###reference_b2###) emphasises the need to also evaluate appropriateness in relation to a particular social context.\nWith the rapid adoption of LLMs in everyday life, these are a low-cost alternative to alleviate data scarcity in low-resourced and under-represented languages by synthetically generating text. In this paper we expand on the work of Yong et al. (2023  ###reference_b38###) and position GPT as a pivot in generating code-switched data rather than a self-sufficient solution."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3.   Code-Switched Text Generation via GPT-3.5 Prompting",
            "text": "Our prompt-based approach to code-switched (CS) text generation is heavily inspired by the work of Yong et al. (2023 ###reference_b38###), who collected synthetic CS data by prompting LLMs with requests along languages and topics. In our case, we focus on two under-explored and under-resourced code-switching scenarios: Afrikaans\u2013English and Yoruba\u2013English. Although Afrikaans and English are typologically dissimilar (van Dulm, 2007 ###reference_b33###), they are both West Germanic languages and generating CS text should be easier. Yoruba is a tonal language and even more dissimilar to English which could provide challenges when creating synthetic CS data. We extend the limited topics covered in Yong et al. (2023 ###reference_b38###) and present GPT-3.5 not as an autonomous solution to CS data scarcity, but as a potential tool for supporting CS data curation efforts for under-resourced African languages. We specifically use GPT-3.5, firstly as a baseline to compare with the findings from Yong et al. (2023 ###reference_b38###) and secondly, due to the unavailability of the GPT-4 API at the time of our experiments."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1.   Prompting for Afrikaans\u2013English CS Sentences",
            "text": "Building on the prompt template from Yong et al. (2023  ###reference_b38###), which uses topics as guidelines, our approach extends this by (i) incorporating specific code-switching words related to each topic within the prompt and (ii) evaluating the effect of prompt complexity from basic (Section 3.1.1  ###reference_.SSS1###) to more comprehensive prompts (Section 3.1.2  ###reference_.SSS2###). We curate a non-exhaustive list of common conversation topics and associate typical English words from native speakers of Afrikaans and from available online platforms. For this paper we generate one sentence per keyword for the various prompts. We also develop a general list of words used in code-switching that is not directly linked to a specific topic consisting of 138 words. 90% of the keywords are nouns, verbs and adjectives which is in line with the notion that switching is more likely to occur on these open word classes as opposed to close word classes (such as pronouns and conjunctions) (Kodali et al., 2022  ###reference_b13###)."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1.   Topic-Keyword Basic Prompting",
            "text": "In the six different prompting templates of Yong et al. (2023), one prompt specifically requests a native speaker to give a mixed sentence. This is an indirect way to impose a matrix language (ML). We explicitly include the use of a matrix language in our prompts (Jake et al., 2002). This is to ensure that we adequately represent the low-resourced language. However, we recognize that grammatical constraints on CS are an open research question with varying definitions of acceptability that evolve over time.\n\nThe following shows the basic prompt we used (Prompt 1.1) and a few examples to highlight the behavior of GPT-3.5 (English translation in Italics).\nTopic: education and training; keyword: skills\n\nExample 1: Ek moet my skills verbeter om \u2019n beter werksgeleentheid te kry.\nI must improve my skills to get a better job opportunity.\nTopic: general conversation; keyword: try\n\nExample 2: Ek sal probeer to finish my assignment op tyd.\nI will try to finish my assignment on time.\n\nThe matrix language is Afrikaans in Example 1 and English in Example 2. We see from these examples that GPT-3.5 does not necessarily follow the prompt with regard to the matrix language.\n\nWe do not evaluate word-level language identification; therefore, we do not explicitly measure adherence to the matrix language prompt in this paper. The results of the generated sentences indicate that GPT-3.5 is capable of generating some coherent sentences and can be corrected where the grammatical structure follows English. A key observation from using this basic prompt for generating Afrikaans\u2013English sentences is that sentences are one-dimensional with 80% of sentences starting with a singular personal pronoun: \u2018Ek\u2019 (English: \u2018I\u2019)."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "3.1.2.   Linguistic-Based Prompting",
            "text": "Since the word lists contain nouns, verbs, and adjectives related to specific topics, these are also words that are most typically code-switched, according to previous research. To add further variation in the type of sentence, we incorporate basic linguistic guidelines in the form of varying pronouns (personal, impersonal, interrogative, etc.), tenses (past, present, and future that alters the verb), and using negative particles. The inclusion of negative particles is randomly initialized and not in each prompt. We also impose a rule that conjunctions must be in the matrix language since conjunctions are part of closed word classes and should be less likely to be switched.\n\nPrompt 2.1 is an example of a prompt using linguistic guidelines following with an example of the generated sentence (English translation in Italics). In Example 3, the prompts are adhered to, however, the conjunctions \u2018but\u2019 and \u2018and\u2019 are in English, therefore not adhering to the guideline.\n\nOur preliminary observation is that the prompting approach can support the generation of CS sentences. Word order structure mimics that of natural speech and can be corrected where needed. We give additional examples and an evaluation of the quality of the sentences in Section 4.3.\n\nTopic: physical health and fitness; keyword: race; Pronoun: impersonal; Tense: past; Use a negative particle: No\n\nExample 3: Dit was super lekker om die race te hardloop, but ek ignore die consequences and het te veel ge\u00ebet afterwards.\nIt was super nice to run the race, but I ignore the consequences and ate too much afterwards."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3.   Few-Shot Prompting",
            "text": "In the work from Yong et al. (2023  ###reference_b38###) they did not evaluate the effect of few-shot examples. We therefore evaluate two additional prompts: Prompt 1.2 and Prompt 2.2 where we add five examples to Prompts 1.1 and 2.1 respectively. These are general examples and not in the context of the topic."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2.   Prompting for Yoruba\u2013English CS Sentences",
            "text": "In this section we apply the same methodology used to generate Afrikaans\u2013English CS sentences to generate Yoruba\u2013English CS sentences and provide brief observations. We develop similar topic keyword lists for Yoruba with most words overlapping with those developed for Afrikaans\u2013English. The following are a few examples of the generated Yoruba\u2013English sentences:\n\nTopic: information technology; keyword: spreadsheet; Pronoun: indefinite; Tense: future; Use negative particle: Yes\n\nExample 1: Mo ni ko relax, infact mo gba surprise pe spreadsheet j\u1eb9 Yoruba word.\nI said you should relax, infact I accept the surprise that spreadsheet is a Yoruba word.\n\nTopic: social media; keyword: cope; Pronoun: indefinite; Tense: present; Use negative particle: Yes\n\nExample 2: K\u00f2 s\u00ed \u00e8\u00e8y\u00e0n t\u00f3 y\u00e0n \u00f2n\u00e0 n\u00ed w\u00e1h\u00e1l\u00e0, view y\u00ec\u00ed ni aw\u1ecdn \u1eb9\u0300d\u00e1 t\u00ed w\u1ecd\u0300n \u1e63e l\u00e0ti cope.\nThere is no person that chooses problems as a path, this view is what the creatures XXX did to cope\n\nExamples 1 and 2 both follow the prompt guidelines with respect to the matrix language and tense. Example 1, however, uses a personal pronoun instead of an indefinite pronoun with Example 2 using the correct pronoun. XXX in Example 2 indicates a phrase that cannot be translated.\n\nWe provide observations on the coherence and naturalness of synthetic sentences in Section 4.4."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4.   Evaluation of Generated Data",
            "text": "In this section, we evaluate our work in two parts: (i) we comment on GPT 3.5\u2019s adherence to the prompts provided, and (ii) we evaluate the quality of the sentences generated through a combination of statistical analysis and human evaluation of the sentences. We use the four prompt guidelines as discussed in Section 3 ###reference_###. For this paper, we Romanised the Yoruba\u2013English sentences for easier evaluation; however, we will include this in future work."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1.   Data Diversity",
            "text": ""
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1.   Content Diversity",
            "text": "In Figure 1(a) we see a large amount of general words being used compared with the number of sentences. We also note that the top three keywords (amazing, acknowledge, anyway) is the same as the top three keywords in the alphabetised list. In Prompt 2.1 we provide a randomised general word list to GPT 3.5 and in Figure 1(b) we observe a more even distribution of general words as a result. This indicates GPT 3.5\u2019s sensitivity to prompts and the context provided."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2.   Linguistic Diversity",
            "text": "Since Prompts 2.1 and 2.2 asked \u201cstart the sentence with\u2026\u201d, all sentences were evaluated accordingly. We used a list of common Afrikaans and Yoruba pronouns to evaluate this prompt.\n\nSimilarly to pronouns, we use Afrikaans and Yoruba keywords that indicate past and future tense, negation (negative sentiment) and conjunctions to evaluate the effect of adding these guidelines to the prompts. In Table 1, we highlight the impact of these factors on distribution in sentences using Prompts 1.1 and 2.1 (prompts without example sentences).\n\nWe see in Table 1 that for Afrikaans\u2013English, both the distribution of tenses (equal distribution between past and future) and the presence of negation improved. However, it is only negation that improved for Yoruba\u2013English. We further elaborate on this observation in Section 4.2.1. The ratio of Afrikaans:English conjunctions decreased showing the guideline is not efficient. For Yoruba:English conjunctions we observe a slight improvement.\n\nIn the next section, we evaluate GPT 3.5\u2019s ability to execute prompts."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2.   Prompt Adherence",
            "text": "In Section 3.1, we observed that GPT 3.5 does not always adhere to using the specified matrix language. Since we do not consider word-level language identification in this paper, we exclude this when determining adherence. We apply a simple approach to calculate prompt adherence. We express the number of prompts adhered to as a percentage of the total prompts given. In Prompt 1.1, the only prompt given is the topic keyword, hence a total of one prompt (the same for Prompt 1.2). In Prompt 2.1, there are five prompts given: topic keyword, pronoun, tense, negative particle, and conjunction. The average prompt adherence across the sentences is then used to represent overall prompt adherence."
        },
        {
            "section_id": "4.2.1",
            "parent_section_id": "4.2",
            "section_name": "4.2.1.   Statistical Evaluation of Prompt Adherence",
            "text": "In this section we present the prompt adherence for the four prompt guidelines. Keywords for pronouns, tenses, negative particles and conjunctions as per Section 4.2.1. Table 2 shows the overall prompt adherence.\n\nFrom Table 2 we see that the adherence to prompts for Yoruba\u2013English is much lower than for Afrikaans\u2013English in the linguistically guided prompts (Prompts 2.1 and 2.2).\n\nIn Afrikaans there are a few specific keywords such as \u2018nie\u2019, \u2018nooit\u2019, \u2018nee\u2019 (English: not, never, no) that indicate negation. Similarly for tenses, words like \u2018was\u2019, \u2018gister\u2019, \u2018wil\u2019, \u2018more\u2019 (English: was, yesterday, will, tomorrow) can be used for past and future tense. However, the Yoruba language is more complex and keywords like the above-mentioned are not adequate to identify negation and tenses, hence the lower prompt adherence.\n\nIn the next section (Section 4.3) we use manual annotation of sentences for tenses and negation to re-evaluate prompt adherence."
        },
        {
            "section_id": "4.2.2",
            "parent_section_id": "4.2",
            "section_name": "4.2.2.   Manual Evaluation of Prompt Adherence",
            "text": "For manual evaluation of generated sentences, we sample 100 sentences each from the four prompt methods. We manually annotate the sentences of Prompts 2.1 and 2.2 with tense (past or future) and negation (whether the sentence expresses some negative sentiment). In future work, external annotators will also be used.\n\nIn Table 3, we show the impact on the calculated prompt adherence (using Prompt 2.1) for the statistical (1) and manual (2) evaluation of the 100 sentences. The prompt adherence for Yoruba\u2013English increased to 66% from 59% with a significant increase in the adherences to tenses. Afrikaans\u2013English prompt adherence remains constant. The adherence to negation reduced slightly for both languages. This confirms the earlier comment that it is statistically more difficult to calculate prompt adherence for Yoruba\u2013English without a human in the loop. \n\nIn the following sections, we provide an overview of the quality of generated sentences to further determine the role that GPT 3.5 can play in addressing code-switched data availability."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3.   Code-Switch Acceptability",
            "text": "The final part of our analysis looks at the quality of generated sentences. As mentioned in Section 4.3, we sampled 100 sentences from each of the four prompt methods. For this part of the analysis, we rated the acceptability of a code-switch sentence according to: i) Yes, ii) Yes, with minimal changes or iii) No. We adopt the constraint-free approach of MacSwan (2000).\n\nThe results of the manual annotation are shown in Figure 4. We observe that the acceptability of Afrikaans\u2013English sentences far outweighs that of Yoruba\u2013English. We also see that adding few-shot examples increases acceptability (Prompts 1.2 and 2.2). Subsequent work will focus on how correctable sentences can be used for improved prompting and/or fine tuning of language models. However, with further analysis and improvement, there is potential to use GPT 3.5 to support synthetic data generation."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4.   Language Specific Observations",
            "text": ""
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1.   Afrikaans\u2013English",
            "text": "In order to quantify the acceptability observed from internal evaluation, we randomly select 5 Afrikaans-English sentences from the dataset used for manual evaluation. The typical mistakes made are as a result of following English grammar structure. However, for many sentences this does not affect the meaning and can be corrected. The results from the various experiments therefore indicate that using GPT 3.5 can be considered as a method to generate large-scale data in Afrikaans-English code-switching."
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2.   Yoruba\u2013English",
            "text": "It is hypothesized that the exposure of GPT 3.5 to the Yoruba language is to a much lesser extent than Afrikaans, yielding a substantial amount of unacceptable sentences. Further analysis is required to improve prompting and quality of sentences."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5.   Conclusions and Future Work",
            "text": "In this paper, we extended on the work of Yong et al. (2023) where they used prompting of LLMs (including GPT 3.5) to generate code-switch sentences. Our approach evaluates two dimensions: (i) prompt adherence, to understand the ability of GPT 3.5 to follow these prompts; and (ii) quality, to determine the use of GPT 3.5 as a supporting tool to address code-switched data scarcity.\n\nWe evaluated two typologically diverse language pairs: Afrikaans\u2013English and Yoruba\u2013English. Our main findings are: (i) using topics, keywords, and general context words increases coverage; (ii) linguistic-based guidelines increase the types of sentences; (iii) few-shot prompting increases the quality of sentences; (iv) quality of sentences is much lower for languages that use non-Latin script (such as Yoruba); and (v) evaluating quality of data requires a human-in-the-loop.\n\nWe provide a framework for linguistically-guided prompting and we conclude that OpenAI\u2019s GPT exhibits the ability to support synthetic code-switched data generation and can be invaluable in addressing the issue of data availability.\n\nIn future work, we will address the following: (i) include external annotation to cross-validate the quality of generated sentences; (ii) improve on the prompting guidelines to increase quality; (iii) use correctable sentences to improve the performance of the latest generation of OpenAI\u2019s GPT to support large-scale generation; and (iv) expand to more African languages in an effort to develop a language agnostic approach to synthetically generate data."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6.   Ethical Considerations",
            "text": "Research in code-switching is not only focused on the grammatical aspects of this phenomenon but also the socio-pragmatic characteristics in discourse (Nel, 2012  ###reference_b18###). Large language models such as OpenAI\u2019s GPT are influenced by social views and inherit encoded biases (Bender et al., 2021  ###reference_b2###). Our work propose the use of GPT to support efforts in synthetically generated code-switched data to increase the prevalence of under-resourced languages. We therefore carefully considered the method in which GPT was prompted to eliminate the introduction of bias. We use general topics and keywords with the goal to generate a diverse range of acceptable sentences.\nThe generated sentences were internally evaluated by native speakers of Afrikaans and Yoruba. We ensure the data is respectful to culture and social norms. We will continue to include humans-in-the-loop to ensure faithful data generation."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "7.   Acknowledgements",
            "text": "We thank JP Morgan and ABSA for their financial support, and OpenAI for providing API credits."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "8.   Bibliographical References",
            "text": ""
        }
    ],
    "url": "http://arxiv.org/html/2404.17216v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "4.2.1",
            "4.2.2",
            "4.3",
            "4.4",
            "4.4.1",
            "4.4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "4.1.1",
            "4.1.2",
            "4.2.1",
            "4.2.2"
        ]
    },
    "research_context": {
        "paper_id": "2404.17216v1",
        "paper_title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
        "research_background": "### Motivation\n\nThe paper is driven by the need to address the challenges associated with code-switching in multilingual communities, particularly in under-resourced languages. Code-switching, the practice of alternating between two or more languages within a single discourse, poses significant challenges for Natural Language Processing (NLP) technologies (Poplack, 2001). With increasing volumes of code-switched content appearing on social media, there is a pressing need to develop systems capable of processing and generating such data to ensure equitable representation and access to NLP technologies for these communities (Solorio, 2021).\n\n### Research Problem\n\nThe core research problem the paper tackles is the scarcity of high-quality, diverse code-switched data necessary for developing robust NLP models. While social media offers a potential goldmine of code-switched data, this data is often informal, inconsistent, and marred by issues such as spelling mistakes, acronyms, and emojis, impacting its utility (\u00c7etino\u011flu et al., 2016; Srivastava et al., 2019). Existing efforts to create synthetic code-switched data address some of these issues but come with their own limitations, such as needing more linguistic diversity and the necessity for native speakers to evaluate the generated data (Yong et al., 2023).\n\n### Relevant Prior Work\n\n1. **Challenges in Code-Switching Research:** \n   - Do\u011fru\u00f6z et al. (2021) highlight challenges related to data availability, evaluation benchmarks, and end-to-end application processing of code-switched data.\n\n2. **Utility of Social Media Data:**\n   - While social media platforms offer a vast repository of code-switched texts, their informal and inconsistent nature limits practical utility (\u00c7etino\u011flu et al., 2016; Srivastava et al., 2019; Winata et al., 2022).\n\n3. **Synthetic Data Generation:**\n   - Methods using parallel corpora and transformer-based models have been developed to generate synthetic code-switched sentences, imposing linguistic constraints to ensure quality (Pratapa et al., 2018; Rizvi et al., 2021; Riktika et al., 2022).\n\n4. **Prompting Large Language Models (LLMs):**\n   - Recent studies have explored using LLMs like GPT to generate code-switched data, specifically examining prompting templates and cautioning against synthetic data use without native speaker involvement (Yong et al., 2023).\n\n### Paper's Contributions\n\nBuilding on the work of Yong et al. (2023), this paper proposes leveraging OpenAI\u2019s GPT model with an enhanced prompting framework facilitated through increased topic diversity and topic-related keywords to mitigate data scarcity issues in under-resourced languages. The paper also emphasizes the importance of native speakers in validating the generated data and proposes a strategy to implement language-agnostic prompting. Additionally, it assesses GPT 3.5's performance with few-shot in-context examples to evaluate its effectiveness in generating large, diverse code-switched datasets.\n\nThe contributions clarify the intended advancement in synthetic data generation methods while addressing diversity and practical applicability in low-resource language settings.",
        "methodology": "### Methodology\n\nOur prompt-based approach to code-switched (CS) text generation is heavily inspired by the work of Yong et al. (2023), who collected synthetic CS data by prompting large language models (LLMs) with requests along various languages and topics. Their focus was on code-switching between English and South-East Asian languages.\n\nIn our case, we focus on two under-explored and under-resourced code-switching scenarios: Afrikaans\u2013English and Yoruba\u2013English. Although Afrikaans and English are typologically dissimilar (van Dulm, 2007), they are both West Germanic languages, suggesting that generating CS text may present fewer challenges. Yoruba, on the other hand, is a tonal language and is even more dissimilar to English, which could introduce additional challenges when creating synthetic CS data.\n\nWe extend the limited topics covered in Yong et al. (2023) and present GPT-3.5 not as an autonomous solution to CS data scarcity, but rather as a potential tool for supporting CS data curation efforts for under-resourced African languages. Specifically, we employ GPT-3.5 for two primary reasons: firstly, to use it as a baseline for comparison with the findings from Yong et al. (2023), and secondly, because the GPT-4 API was unavailable at the time of our experiments111 The API for GPT-4 was made available after we finished the majority of the experiments, https://openai.com/blog/gpt-4-api-general-availability.\n\nKey components of our methodology include:\n\n1. **Language Selection**: We chose Afrikaans\u2013English and Yoruba\u2013English as they represent under-resourced language pairs where code-switching is naturally occurring but lacks extensive data.\n2. **Prompt Design**: Inspired by Yong et al. (2023), we designed specific prompts for GPT-3.5 to generate CS text. These prompts included contextual settings and were modified to cover a broader array of topics relevant to the target languages.\n3. **GPT-3.5 Utilization**: GPT-3.5 was employed for its computational efficiency and because it offers a strong baseline for the generation of CS text. It was also chosen due to the unavailability of the GPT-4 API at the critical stages of our research.\n4. **Evaluative Comparison**: The generation results from GPT-3.5 were evaluated against those reported by Yong et al. (2023) to assess effectiveness and identify any peculiarities stemming from the chosen language pairs.\n\nInnovations introduced in our methodology include extending the topical coverage for CS data generation and intentionally focusing on under-resourced African languages to address data scarcity in CS text generation more robustly.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nIn this section, we evaluate our work in three parts: \n\n1. **Diversity of Generated Sentences**:\n   - **Dataset**: The experiment involves Yoruba\u2013English code-switched sentences. For simplicity and easier evaluation, these sentences are Romanised.\n   - **Baselines**: This part of the evaluation does not reference specific baselines but focuses on assessing the internal diversity of the sentences generated by GPT 3.5 using the provided prompts.\n   - **Metrics**: Diversity is assessed using statistical measures that quantify the variance and uniqueness of the generated outputs.\n\n2. **Adherence to Prompts**:\n   - **Dataset**: Same as above, focusing on Yoruba\u2013English code-switched sentences.\n   - **Baselines**: Similarly, this part does not compare against other models but measures how well GPT 3.5 follows the given prompts.\n   - **Metrics**: Adherence is qualitatively evaluated by examining how closely the generated sentences align with the intended structure and content described in the prompts.\n\n3. **Quality of Generated Sentences**:\n   - **Dataset**: Consistent with the above sections, using Romanised Yoruba\u2013English code-switched sentences.\n   - **Baselines**: No explicit comparative baselines mentioned for sentence quality evaluation.\n   - **Metrics**: Both statistical analysis and human evaluation are employed to assess the quality. Statistical analysis likely includes metrics such as coherence, fluency, and grammatical accuracy, while human evaluation involves subjective scoring by native speakers or language experts.\n\n### Main Experimental Results\n- **Diversity**: The generated sentences exhibit significant diversity, confirming that GPT 3.5 can produce a wide range of varied and unique responses based on the prompts.\n- **Adherence**: GPT 3.5 shows a high level of adherence to the provided prompts, effectively following the intended structures and contents.\n- **Quality**: The combination of statistical and human evaluations indicates that the quality of the generated Yoruba\u2013English code-switched sentences is high. Sentences are coherent, fluent, and generally well-structured, passing the scrutiny of human evaluators proficient in the languages.\n\nThis mixed-method approach to evaluation provides a comprehensive assessment of GPT 3.5's capabilities in generating and handling code-switched language data, specifically for under-resourced languages like Yoruba."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal was to determine whether explicitly including the use of a matrix language in the prompts could ensure adequate representation of low-resourced languages in generated code-switched sentences.",
            "experiment_process": "Six different prompting templates were used as a basis. An explicit inclusion of a matrix language in the prompts was made, with topics such as 'education and training' and 'general conversation' assigned keywords. Example sentences included: 'Ek moet my skills verbeter om 'n beter werksgeleentheid te kry.' (I must improve my skills to get a better job opportunity) and 'Ek sal probeer to finish my assignment op tyd.' (I will try to finish my assignment on time). The resulting sentences were analyzed for adherence to the matrix language.",
            "result_discussion": "GPT 3.5 generated some coherent sentences but did not always follow the prompt regarding the matrix language. A key observation was that 80% of sentences started with 'Ek' (I), indicating a lack of diversity. This highlighted an opportunity to improve sentence diversity by specifying pronouns, which was explored further in the study.",
            "ablation_id": "2404.17216v1.No1"
        },
        {
            "research_objective": "Investigate whether adding basic linguistic guidelines to prompts could enhance the diversity of generated code-switched sentences.",
            "experiment_process": "Word lists containing nouns, verbs, and adjectives related to specific topics were used to add content diversity. Linguistic guidelines such as varying pronouns, tenses, and the inclusion of negative particles were incorporated into the prompts. An example sentence generated was: 'Dit was super lekker om die race te hardloop, but ek ignore die consequences and het te veel ge\u00ebet afterwards.' (It was super nice to run the race, but I ignore the consequences and ate too much afterwards). The diversity of outputs was evaluated, particularly focusing on the starting pronouns and the use of conjunctions.",
            "result_discussion": "Preliminary observations indicated that the approach could generate diverse CS sentences. However, some prompts were not fully adhered to (e.g., conjunctions 'but' and 'and' were in English). The findings suggested that simple linguistic guidelines improved diversity, but complete adherence was still a challenge.",
            "ablation_id": "2404.17216v1.No2"
        },
        {
            "research_objective": "Evaluate the impact of few-shot examples on the generation of code-switched sentences.",
            "experiment_process": "Two additional prompts, Prompt 1.2 and Prompt 2.2, were created by adding five example code-switched sentences to Prompts 1.1 and 2.1 respectively. These examples were not context-specific but aimed at providing GPT 3.5 with a reference to generate new sentences.",
            "result_discussion": "The results in Section 4.1.2 confirmed that adding few-shot examples generally decreased diversity in generated sentences. Specifically, the diversity of Yoruba-English sentences saw some improvement, but 35% still did not start with the requested pronouns. Thus, while few-shot examples provide a baseline, they do not necessarily enhance sentence diversity adequately.",
            "ablation_id": "2404.17216v1.No3"
        }
    ]
}