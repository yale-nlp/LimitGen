{
    "title": "\u201cHunt Takes Hare\u201d: Theming Games Through Game-Word Vector Translation",
    "abstract": "A game\u2019s theme is an important part of its design \u2013 it conveys narrative information, rhetorical messages, helps the player intuit strategies, aids in tutorialisation and more. Thematic elements of games are notoriously difficult for AI systems to understand and manipulate, however, and often rely on large amounts of hand-written interpretations and knowledge. In this paper we present a technique which connects game embeddings, a recent method for modelling game dynamics from log data, and word embeddings, which models semantic information about language. We explain two different approaches for using game embeddings in this way, and show evidence that game embeddings enhance the linguistic translations of game concepts from one theme to another, opening up exciting new possibilities for reasoning about the thematic elements of games in the future.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "1. Introduction",
            "text": "Almost a decade ago, Cook and Smith wrote that the \u2018mechanics-first view on games is unnecessarily limiting\u2019 and called for AI research into game design to consider experiential, aesthetic and rhetorical aspects of games in more depth (Cook and Smith, 2015  ###reference_b5###). Despite new work along these lines, building automated systems that can understand, change or create the theming and contextual meaning for a videogame is an underexplored area of game AI research. Understanding a game\u2019s theme requires the ability to relate game concepts to real-world ideas. This is useful for a wide variety of applications, including the contextualisation of generated content, game design assistance and guidance, and tutorialisation. However, an abstract game carries no indication of how its components should be themed, and a surface-level translation of an existing theme that does not take into account how a game is played will likely lose coherence and impact.\n\nPrevious attempts to overcome this problem have tended to rely on pre-made databases of meaning, and user-defined graphs of concepts that relate to one another (Summerville et al., 2018  ###reference_b15###)(Treanor et al., 2012  ###reference_b16###). We cover some of this work in section \u00a73  ###reference_###, Related Work. While this approach has many benefits, it is very time-consuming, does not scale or generalise, and cannot be extended to procedurally generated concepts for which no prior knowledge exists.\n\nIn (Rabii and Cook, 2021  ###reference_b13###), Rabii et al present a technique which uses word embeddings to train a vector-space representation of game log data. They show that with no prior knowledge about the game other than the logs that it is possible to extract complex, high-level knowledge about the game\u2019s structure, dynamics and strategy. This approach circumvents some of the issues mentioned above, in that it produces knowledge about a game simply through observations of it being played. However, this knowledge is completely abstract \u2013 it has no connection to the grounded \u2018meaning\u2019 of the game.\n\nIn this paper we repurpose Rabii\u2019s work, and present our technique for using a combination of game log embeddings and word embeddings to translate a game\u2019s theme across a semantic space. We describe our system setup and how we relate game embeddings to the real-world linguistic knowledge base represented by word embeddings. We also describe our approaches to translating thematic elements across the word embedding space to yield new themes for games. We share some preliminary results here, and note the challenges we have encountered so far, and our preliminary steps to improve upon the work. This represents a starting point for a new application of word embeddings to help relate game dynamics to real-world ideas.\n\nThe remainder of this paper is organised as follows: in section \u00a72  ###reference_### we cover how word and game embeddings work; in \u00a73  ###reference_### we discuss related work in computational creativity and automated game design; in \u00a74  ###reference_### we cover the methodology of our approach to thematic translation; in \u00a75  ###reference_### we present and evaluate initial results from the system; and in \u00a76  ###reference_### we conclude our work and look to the future."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "2. Background",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "2.1. Word Embeddings",
            "text": "Word embeddings are representations of sequential data (usually language) in the form of n-dimensional vectors, used in natural language processing for tasks such as topic modelling and semantic distance measurement. The process is trained on a dataset of token sequences, and the positions of tokens in the resulting vector space aims to mimic their distributions in the original data. For natural language, this means that words with a similar semantic role in the training data tend to appear close to each other along certain dimensions in the embedded space. Word embeddings are useful not only for measuring the distance between tokens, but also because mathematical operations can be performed on the vectors, such as addition, subtraction or averaging. This allows for calculations to be performed on words to compose or subtract meanings. Parrish gives the example of colours in (Parrish, 2018  ###reference_b11###): subtracting the vector for the word Blue from the vector for the word Sky and adding the result to the vector for the word Grass yields a vector close to the word Green. The best-known algorithm for creating word embeddings is Word2Vec, which has been widely studied both within computer science as well as beyond in digital arts and other creative fields (Mikolov et al., 2013  ###reference_b10###). Word2Vec\u2019s simplicity and broad scope has made it a useful and accessible tool for artists and creative coders as well."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "2.2. Game Embeddings",
            "text": "In (Rabii and Cook, 2021  ###reference_b13###) Rabii and Cook present an application of word embeddings to gameplay data. They take a set of formally-annotated chess match logs, and translate it to a domain-specific language that they designed for the purposes of training an embedding. They then apply the word2vec algorithm to this chess gameplay data, yielding a vector-space embedding for the game logs. In their paper they show that the resulting embedding reveals a range of interesting information about chess, from fundamental structural concepts through to subtle strategic insights understood by proficient players.\nIn their original proposal, the authors note several important traits of their system: the design of the domain-specific language, the size of the data, the source of data points (for example, expert players versus novices) and the exact phrasing of queries all have a significant impact on the types of conclusions that can be drawn from the data. Despite this, the work clearly shows that the embeddings capture important truths about the game simply by seeking semantic connections between logs of game events. This technique is potentially very scalable and widely-applicable as a result, since a game developer can easily gather event logs for their own game, and create a vector embedding from it.\nBoth game and word embeddings represent relationships between concepts through a multi-dimensional vector space, learned from datasets. As stated above, we can operate within these vector spaces to transform and translate concepts, through processes like addition or subtraction. However, vector spaces can also be related to one another, particularly when we can provide points which overlap or connect semantically. We can draw connections between these spaces through the use of regression techniques (RODRIGUEZ et al., 2023  ###reference_b14###), which allow us to use known connections between spaces to infer new ones. In this paper, we present our efforts to draw a connection between a game embedding space (trained on chess data, as per Rabii and Cook) and a word embedding space (trained on English language text) to show how linguistic and ludic concepts can be linked together, and then transformed to find new themes and meaning in word embedding space."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "3. Related Work",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "3.1. Metaphor and Analogy",
            "text": "In this paper we seek to re-theme a game. We have a set of game elements, such as the pieces on a chessboard and the states and rules of the game, and we wish to rename these elements to align with a new theme. This has some similarity to the notion of metaphor or analogy in natural language processing research. A metaphor has two elements: a tenor, the root concept that is being conveyed, and a vehicle, a second concept that is used to represent the tenor in a new, metaphorical context. Metaphors usually connect concepts together through a specific connection point and then encourage the audience to extrapolate other, looser connections from there.\nMetaphor and analogy are a widely studied topic in broader AI and natural language processing research, especially in computational creativity where researchers such as Veale have extensively studied how computers can extract metaphors from language (Veale et al., 2018  ###reference_b17###). Veale\u2019s approach identifies common structures in natural language and mining those associations to create a large knowledge base. This is supported by existing linguistic knowledge graphs such as WordNet (Fellbaum, 1998  ###reference_b6###). In (Gero and Chilton, 2019  ###reference_b7###), Gero et al use the open-source knowledge graph GloVe to evaluate potential metaphors and grades different qualities of the metaphor based on their semantic similarity in the knowledge graph. Our approach also uses GloVe as a basis for its linguistic knowledge."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "3.2. Theming Games",
            "text": "Theming or otherwise providing semantic meaning to game elements is primarily a topic covered by automated game design research, which Cook defines as \u2018the science and engineering of AI systems that model, participate in or support the game design process\u2019 (Cook, 2022  ###reference_b4###). Such systems engage with the topic of game theming for a variety of reasons, including wanting to impose a theme on an existing game, or interpreting a game to understand possible themes it may be expressing. Treanor\u2019s work on micro-rhetorics for the Game-o-Matic system is one such example \u2013 micro-rhetorics are user-defined components of meaning which can be combined with one another to compose higher-level rhetorical messages (Treanor et al., 2012  ###reference_b16###). In his work on the Game-o-Matic, Treanor combines micro-rhetorics with thematic graphs that users provide (for example, \u2018cop arrests protester\u2019) to synthesise games which convey a theme.\nGEMINI furthers work in this area, by providing bidirectional interpretation (Summerville et al., 2018  ###reference_b15###). This allows the GEMINI system to not only create games based on thematic structures, but also to intuit possible thematic structures by examining an unthemed game, which is closer to the task we approach in this paper. This is done using a pre-compiled database of structures and their rhetorical interpretations, which formulates the task of theming a game to a logic programming task using answer set solvers. Notably, GEMINI operates on rules, not behaviour \u2013 \u2018mechanics\u2019, rather than \u2018dynamics\u2019, in the parlance of Hunicke et al (Hunicke et al., 2004  ###reference_b8###) \u2013 and its rhetorical databases are also defined in this way, describing the rhetorical meaning of game elements directly. Our approach considers the dynamic behaviour of game elements instead, by focusing on Rabii\u2019s game embeddings technique, and leverages large existing knowledge databases (in our case, word embedding models) that are not specific to any particular game."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "4. Methodology",
            "text": "Our goal is to build a system that can associate words in English to Chess game concepts, by relating English word embeddings to game embeddings built from Chess play data. If  is a game token, we note  its associated game vector. If  is a word in our dataset\u2019s English vocabulary, we note  its associated word vector. If  are words in our dataset\u2019s English vocabulary, we note  the average word vector . The relationship vector between two words  and  is denoted . This is a vector which can be \u2018added\u2019 to the vector  to yield the vector representing ."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.1. Training Embeddings",
            "text": "Our source for word vectors are the public domain GloVe pre-trained embeddings, trained on the Wikipedia 2014 and the Gigaword 5 corpus (Pennington et al., 2014). Its vocabulary contains 400,000 words, each represented by a 50-dimensional vector.\n\nTo create our game embeddings, we reproduced Rabii and Cook\u2019s data processing pipeline. We first downloaded a month of ranked matches data from the online Chess playing platform Lichess, and converted each logged game in the custom description language described by the authors. Samples of the converted data with their natural language equivalent are provided in Table 1.\n\nThe description language contains a total of 34 different tokens that represent different concepts used to describe a game of Chess. Tokens can represent players (e.g. White, Black), Chess pieces (Queen, Pawn), playing moves (Capture, Castling), game states (Checkmate, WinBlack) and even the eight rows and columns of the board (e.g. R0, R1, R7, C0, C1, C7).\n\nWe then used our corpus of Chess data to train a Word2Vec model using the same settings as Rabii and Cook. In the end, each of our 34 Chess tokens has a 5-dimensional vector representation."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "4.2. Themes and Rethemings",
            "text": "We define a theming as a function that associates game tokens built from our Chess description language to word vectors in our dataset. We consider Chess to have an already established theming, which we denote as the function . Our definition of Chess\u2019 current theme can be found in Table 2. It does not contain every token in the original description language, only those that we could associate to a specific word vector. For example, since we associate the game token King to the word \u201dking\u201d, we have:\n\nHowever,  cannot take as input the tokens that represent the rows and columns of the board (R0,...,C7), as we couldn\u2019t map them to a specific word in English. Some tokens can be associated to a word, but that word might have other meanings than the one we want to use (e.g. Draw does not refer to the act of drawing a picture, but a situation where no player wins). In that case, we associate the token to at most three words, and use the average of their word vectors. The set of tokens that are valid inputs for  is called .\n\nOur goal is to retheme Chess, i.e., create other themings with alternative token-to-words associations. We define  as a function that, given a token and a relationship vector, returns a word vector:\n\nThe relationship vector guides the semantic translation of the input token into the final output word. For example, if we built a function to shift the masculine words in Chess\u2019 theming to their feminine equivalents, we might have the following equations:\n\nIn the following sections, we describe three different models for building the  function, one using only word vectors, and two using a combination of word and game vectors. In the following results section, we show and discuss results from all three approaches."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "4.3. Word Vectors Only",
            "text": "Our first retheming model is built using the well-known Word2vec\u2019s analogy formula, adapted to the general case:\nFor the above example of translating across the theme vector of masculine to feminine, we would therefore calculate the retheming as:\nThis function only uses word vectors. We chose it as a baseline to compare with techniques that leverage the information stored in game vectors."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "4.4. Using Both Game and Word Vectors",
            "text": "Game embeddings contain expert knowledge about the specific game they were extracted from, while word embeddings represent the meaning of words in the broad context of their dataset. Some expert players assign a numeric valuation to each chess piece as a way to quantify how important they are for winning a game. This suggests that game embedding data carries important information about the structure of Chess as a game which the words alone do not. Therefore, we hypothesize that a retheming using both word and game vectors can provide us with more relevant results than ones purely based on word vectors.\n\nWe want our model to be able to translate relationships between game tokens (such as the relative value ordering of chess pieces) into relationships between words (e.g. the medieval hierarchy of Chess piece names). We are interested in models of the form:\nwhere  is a linear function that associates a game vector to a word vector. The choice of linear functions is motivated by the fact that semantic relationships between words and game concepts are captured in relationship vectors. A linear transformation  preserves relationship vectors up to a scaling factor.\n\nRecall that  is the set of tokens which are valid inputs for the theming function. For each game token , we train a linear regression model  on a subset  of  game tokens randomly chosen in  and their corresponding value given by . The purpose of  is to convert the gameplay relationships embedded in the game vector  to semantic relationships, encoded in a word vector. Unlike the method presented in Section 4.3, we don\u2019t want the prediction to be influenced by the currently given value of  so we make sure that the token  is absent from .\n\nOnce the conversion from the game to word embeddings space is done, we add the relationship vector that guides the retheming:"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "4.5. Choosing the guiding vector",
            "text": "The function requires a relationship word vector of the form , that we call the guiding vector. It is best to think of it as representing a semantic transformation from the starting word  to the target word . In our experiments, this transformation is applied to the narrative context of Chess, to shift it from its current theme to a different target theme. We experimented with two types of guiding vector."
        },
        {
            "section_id": "4.5.1",
            "parent_section_id": "4.5",
            "section_name": "4.5.1. Guiding based on a specific example",
            "text": "If we have a token in our Chess description language that we want to retheme to a specific word (e.g., map the token King to the word lion instead of the word king), we can provide the vector representing that exact transformation. All the other rethemed word vectors will be translated by that same vector. We can think of this translation as being anchored around the given example, with the other translations being secondary and hopefully following the same overall trajectory."
        },
        {
            "section_id": "4.5.2",
            "parent_section_id": "4.5",
            "section_name": "4.5.2. Guiding based on a target semantic field",
            "text": "Instead of providing a specific retheming for a token (), we can choose a broader semantic field, represented by a list of words . Let  be the words used in the current theming of Chess in Table 2  ###reference_###. We can create a vector representing the shift from one lexical field to another by first computing the average vector of each list \u2013 respectively  and . The guiding vector  represents the transformation of shifting from the starting theme to the one specified by the list of words given in input."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "5. Results",
            "text": "The models described in Section \u00a74 were built with the goal of retheming the tokens in our Chess description language. We present our results under different modalities in Tables 4 and 5. Each model is tasked with associating all the game tokens in Table 2 to a word embedding. We explore two types of retheming, each with the goal of shifting the original medieval warfare theming of Chess to a theme about wildlife. Table 4 contains the models output when prompted with the specific transformation (see 4.5.1). Table 5 contains results obtained when prompting the models with the target semantic field represented by the following words: lion, elephant, zebra, eating, becoming, extinction, control, win, loss. Our explorations showed that ideally, this list should not only contain words that refer to animals, as words about the basic concepts of a strategy game (e.g., win and loss) are important to define the semantic field of a Chess game.\n\nFor each modality, we compare the results of three models: a baseline retheming using only word vectors, followed by two rethemings using both word and game vectors combined, with the sampling parameter set to 5 and 10, respectively. The maximum value for is the size of the original Chess theming in Table 2 minus one, i.e., 16. Our experiments using values of close to 16 yielded poor results, with little variation between each output. Choosing lower values of provides more diversity in the output word vectors but increases the probability for each linear regression model to over-fit on the training set, which is evidenced by their average of 1 in the lower case where. Over-fitting reduces the model\u2019s ability to generalize, leading to more words that are either too close the original theme (such as or ) or seem too far from the target theme (e.g., , , ). We observed empirically that good trade-off seem to lie in the middle, with . This trade-off may be different for other games and for differently-sized or -distributed token sets, which future work will investigate.\n\nComparing the baseline rethemings (Column 1) and the ones with (Column 2), we observe that models that don\u2019t use game vectors often provide words that are close, if not exactly the same as the starting theme (for King, for Pawn, for Bishop) while the ones that do use them reuse few words from the original Chess theme."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "5.1. Comparing Retheming of Chess Pieces",
            "text": "We compare our model and the baseline model on the task of generating a set of chess pieces themed around wildlife. We use the results generated in Table 5 ###reference_###, columns 1 and 2. For each token related to a chess piece, we follow this algorithm:\n\nCompute the three closest words to the model\u2019s output.\nIf the guiding vector\u2019s target word is in the list, discard it.\nAmong the remaining words, pick the first noun of the list.\n\nThe result of that process is presented in Table 6 ###reference_###. We note that the baseline model outputs words that are quite similar from the starting theme (Column 1), while the model using game vectors seems to correctly feature a \u201dwildlife\u201d theme (Column 2).\n\nIn Column 2, we observe that the medieval social hierarchy of Chess has been replaced by a food chain: weaker pieces are associated with prey animals while stronger pieces are associated with predators. Interpreting the mapping, we remark that the King is linked to a , an animal that is both weak and cherished, which bears some similarity to the King\u2019s role in chess.\n\nThe Rook is associated with hunt, while the Bishop is associated with foxes. The Queen\u2019s moveset allows it to move either like a Bishop or a Rook, and its associated word hunters could be considered a composition of both the notion of a group hunt, represented by the Rook, and foxes which are solitary hunting animals, represented by the Bishop. Semantic blends such as this are necessarily fuzzy, but we find these examples inspirational and engaging.\n\nWe consider that our model\u2019s ability to provide a retheming whose semantic relationships seem to match, at least in an interpretive manner, with gameplay properties is a strong sign of its potential. The baseline model doesn\u2019t exhibit the same properties, suggesting that exploiting the information captured in game embeddings is valuable to explore the link between gameplay and narrative theming."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "6. Discussion and Conclusions",
            "text": "In this paper we presented preliminary results on the use of both word and game embeddings to model and translate thematic elements of games. We motivated our work by building on prior research into game embeddings for chess, and word embeddings for natural language, and showed how the two can be combined together in different ways to alter the theme of a game such as chess. We provided evidence that our technique works better than simply using word embeddings alone to retheme words related to a game, and we propose that this is because the game embeddings capture essential qualities of various game design components and this knowledge supports a richer translation. \n\nWe believe this approach has many promising and interesting features. For one, it captures the dynamics of a game, rather than its static elements: game embeddings reveal, for example, the relative strength of chess pieces, even though this knowledge is not described anywhere in the game\u2019s rules. Most automated game design research focuses on static mechanical elements of games, and struggles to bridge to dynamic or emergent properties. This could be an exciting new way to approach the understanding, extraction and use of dynamic game elements. \n\nEmbeddings are also scalable and comparatively easy to both create and understand. Adding logging to a game to record important events and create datasets, such as the one we used from chess matches, is a simple exercise no different to the debugging or playtesting that game developers already do. We have already worked with one independent game developer to add logging of this type to their game, and the process was clean and straightforward. Word2Vec arithmetic is a widely-used and taught technique among digital art communities and creative coders. This can be furthered through better tools build specifically to make training and exploring embedding spaces more amenable. This makes it a promising technique for widespread adoption in the games industry. \n\nWe are planning to experiment further with this technique on more traditional digital games, to assess how it performs on different kinds of game scenario and different kinds of game logs. At the time of writing, we are working with an independent game developer to gather data from one of their games, from both expert and novice players. We also hope to experiment with more ambitious uses for the technique as well, such as the ability to create names for game elements that have been procedurally generated at runtime. We believe there are many exciting applications for this basic technique waiting to be discovered."
        }
    ],
    "url": "http://arxiv.org/html/2405.09893v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "3",
            "3.1",
            "3.2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.5.1",
            "4.5.2"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.5.1",
            "4.5.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.09893v1",
        "paper_title": "\u201cHunt Takes Hare\u201d: Theming Games Through Game-Word Vector Translation",
        "research_background": "### Motivation\nThe motivation behind this paper stems from a recognition that current AI research in game design has not sufficiently addressed the experiential, aesthetic, and rhetorical aspects of games. Cook and Smith (2015) argued that the prevailing \"mechanics-first\" view of games is limiting and called for more comprehensive AI research that includes these aspects. The authors aim to fill this gap by focusing on the underexplored task of understanding, changing, or creating the theming and contextual meaning for videogames using AI.\n\n### Research Problem\nThe primary research problem this paper seeks to address is how to translate the abstract components of a game into meaningful and coherent themes that resonate with real-world concepts. Traditional approaches have relied on pre-made databases and user-defined graphs, which are not scalable or generalizable and cannot handle procedurally generated concepts. The paper seeks to overcome these limitations by combining game log embeddings with word embeddings to translate a game's theme across a semantic space. \n\n### Relevant Prior Work\n1. **Cook and Smith (2015):** They argued that the mechanics-first viewpoint in game design is limiting and suggested that AI research should encompass more experiential, aesthetic, and rhetorical aspects.\n2. **Summerville et al. (2018) and Treanor et al. (2012):** These works employed pre-made databases of meaning and user-defined concept graphs to address the problem of theming in games, but these approaches are time-consuming and do not scale well.\n3. **Rabii and Cook (2021):** Presented a technique using word embeddings to create a vector-space representation of game log data, which extracts complex knowledge about game structure, dynamics, and strategy with no prior knowledge other than gameplay logs. However, this knowledge remains abstract and lacks grounded meaning.\n\nThe authors build on the techniques from Rabii (2021) to establish a connection between abstract game log data and real-world linguistic knowledge represented by word embeddings. This innovative approach aims to address the identified shortcomings of previous methods, offering a scalable and generalizable solution to translating game themes.",
        "methodology": "The proposed method, titled \"Hunt Takes Hare: Theming Games Through Game-Word Vector Translation,\" aims to create a system that can map English words to Chess game concepts. The methodology involves several key steps and components:\n\n1. **Association of Vectors:**\n   - *Game Embedding:* For a game token \\( t_g \\), an associated game vector \\( v_g \\) is defined.\n   - *Word Embedding:* For an English word \\( t_w \\) in the dataset's vocabulary, an associated word vector \\( v_w \\) is defined.\n\n2. **Average Word Vector:**\n   - For a set of words \\( t_{w_1}, t_{w_2}, \\ldots, t_{w_n} \\) in the dataset's vocabulary, the average word vector \\( v_{w_{\\text{avg}}} \\) is calculated as the mean of individual word vectors.\n\n3. **Relationship Vector:**\n   - The relationship vector between two words \\( t_{w_1} \\) and \\( t_{w_2} \\) is denoted by \\( v_{r_{12}} \\). \n   - This relationship vector \\( v_{r_{12}} \\) can be 'added' to the vector \\( v_{w_1} \\) to yield the vector representing \\( v_{w_2} \\), demonstrating a translation in the semantic space.\n\nThe key innovation here lies in translating the relationships between natural language words into meaningful associations with game concepts via vector embeddings. This allows for thematic representation of game tokens by leveraging the vector arithmetic capabilities of embeddings, thus creating a robust system for word and game concept associations.",
        "main_experiment_and_results": "### Main Experiment Setup:\n\n#### Models:\nThree different models were employed for retheming the tokens in the Chess description language to a wildlife theme:\n1. **Baseline retheming using only word vectors.**\n2. **Retheming using both word vectors and game vectors with sampling parameter (\\(k\\)) set to 5.**\n3. **Retheming using both word vectors and game vectors.**\n\n#### Datasets:\n- The primary dataset involves the game tokens listed in Table 2 (which includes chess-related tokens such as King, Queen, Pawn, etc.).\n- A target semantic field represented by words like \"lion,\" \"elephant,\" \"zebra,\" \"eating,\" \"becoming,\" \"extinction,\" \"control,\" \"win,\" and \"loss.\"\n\n#### Evaluation Metrics:\n- **Cosine distance**: Used to find the closest words in the English dictionary to the word embeddings of the tokens.\n- **Top three closest words**: For evaluation, the top three closest words in the embedding space are presented instead of the full 50-dimensional vectors.\n- Empirical observations are made to evaluate diversity and generalization ability of the models' outputs.\n\n#### Main Experiment Procedure:\n1. Building models as described in Section 4.\n2. Conducting two retheming experiments:\n   - Prompting with a specific transformation as described in 4.5.1.\n   - Prompting with the target semantic field words.\n\n### Main Experimental Results:\n- **Baseline Model**:\n  - The baseline retheming using only word vectors often produced words that were very close, if not identical, to the original Chess theming, such as \"King\" remaining as \"King,\" \"Pawn\" as \"Pawn,\" and \"Bishop\" as \"Bishop.\"\n  \n- **Combined Word and Game Vectors**:\n  - Using combined word and game vectors showed less reuse of the original Chess theme words, introducing more thematic variation towards the wildlife theme.\n\n- **Empirical Observations**:\n  - A trade-off seems to occur balancing diversity and generalization.\n  - The guiding vector\u2019s target words, such as \"lion\" or \"wild,\" frequently emerged as the closest words in the output models. This echoes observations from previous work on word analogies using Word2Vec.\n  - To prevent over-dominance of target words, the practice of using the second closest word instead was employed.\n\nIn conclusion, combining word and game vectors with appropriate sampling parameters yields better thematic transformations of game tokens from Chess to the wildlife theme, as opposed to the baseline model using only word vectors."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Our goal is to build a system that can associate words in English to Chess game concepts, by relating English word embeddings to game embeddings built from Chess play data.",
            "experiment_process": "We first downloaded a month of ranked matches data from the online Chess playing platform Lichess, and converted each logged game in the custom description language. Samples of the converted data with their natural language equivalent are provided. The description language contains a total of 34 different tokens representing different Chess concepts. We used this corpus of Chess data to train a Word2Vec model, providing each of the 34 Chess tokens with a 5-dimensional vector representation.",
            "result_discussion": "Each Chess token (34 in total) was successfully mapped to a 5-dimensional vector, providing a comprehensive representation of the game concepts through embeddings.",
            "ablation_id": "2405.09893v1.No1"
        },
        {
            "research_objective": "Create a thematic association function that remaps Chess game tokens to word vectors, and explore whether using game embeddings along with word vectors can enhance thematic translations.",
            "experiment_process": "Three different models for building the function were developed and evaluated: one using only word vectors and two using both word and game vectors. For the model using only word vectors, the Word2Vec analogy formula was adapted. For models using both vectors, a linear regression model was trained to convert gameplay relationships embedded in game vectors to semantic relationships encoded in word vectors.",
            "result_discussion": "Using purely word vectors serves as a baseline. Incorporating game vectors revealed that game embeddings actually carry critical information about the game's structure, which isn't captured by word vectors alone. This was evidenced by the expert valuation analogy, where game embeddings aligned well with the valuation but word vectors did not. Therefore, using game vectors along with word embeddings provided more relevant and meaningful thematic transformations.",
            "ablation_id": "2405.09893v1.No2"
        }
    ]
}