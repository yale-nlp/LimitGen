{
    "title": "Calibrated Self-Rewarding Vision Language Models",
    "abstract": "Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches are resource-intensive and may not effectively reflect the target LVLM\u2019s preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by enabling the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Additionally, the approach shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning. Our data and code are available at https://github.com/YiyangZhou/CSR.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Vision-Language Models (LVLMs) [1  ###reference_b1###, 2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###] have achieved significant success by incorporating pre-trained large language models (LLMs) and vision models through instruction tuning. However, these LVLMs suffer from the hallucination phenomenon [5  ###reference_b5###], which generates text responses that are linguistically plausible but contradict the visual information in the accompanying image. For instance, the description generated by LVLMs may include visual elements that are not depicted in the image. This issue can also occur when the LLM is highly factual and the visual backbone is capable of producing sufficiently high-quality representations. As indicated in Cui et al. [6  ###reference_b6###], Guan et al. [7  ###reference_b7###], the potential reason for this lies in the misalignment problem between image and text modalities in LVLMs, which causes the model to prioritize the text knowledge present in the training language data while ignoring the actual visual input information.\n\nSeveral works have been proposed to enhance modality alignment capability in LVLMs through preference fine-tuning techniques, such as reinforcement learning from human feedback (RLHF) [8  ###reference_b8###] and direct preference optimization (DPO) [9  ###reference_b9###, 10  ###reference_b10###]. However, these methods often either introduce additional models, such as GPT-4, or depend on human annotation to generate preference data. This data generation process is not only resource-intensive but, more critically, fails to capture the inherent preferences of the target LVLM. Consequently, the target LVLM may easily discern preferences from such curated data, making them less effective (detailed analysis provided in Appendix A.4  ###reference_###). Recently, self-rewarding approaches have emerged, utilizing a single LLM for both response generation and preference modeling, showing promising results in LLM alignment [11  ###reference_b11###, 12  ###reference_b12###]. Unlike LLMs, LVLMs face modality misalignment issues in both response generation and preference modeling stages, potentially resulting in self-generated preferences overlooking visual input information. Directly applying these self-rewarding approaches to LVLMs is not capable of addressing the modality alignment problem and redirecting LVLM\u2019s attention towards emphasizing input image information.\n\nTo tackle these challenges, our work introduces the Calibrated Self-Rewarding (CSR) approach, aimed at calibrating the self-rewarding paradigm by incorporating visual constraints into the preference modeling process. Specifically, we train the target LVLM using an iterative preference optimization framework that continuously generates preferences and optimizes the target LVLM over multiple iterations. Starting with a seed model, each iteration employs sentence-level beam search [13  ###reference_b13###, 14  ###reference_b14###] to produce fine-grained candidate responses for each image and text prompt. During the beam search, for each generated sentence, we first utilize the language decoder to establish an initial reward (i.e., sentence-level cumulative probabilities). \n\n###figure_1### The primary contribution of this paper is CSR, a novel calibrated self-rewarding paradigm for improving modality alignment in LVLMs. Theoretically, with mild assumptions, we show that introducing visual constraints in the self-rewarding paradigm can improve performance. Empirically, when compared with other competitive approaches (see Figure 1  ###reference_### for some representative methods), the results demonstrate that CSR is capable of improving performance on comprehensive LVLM evaluation benchmarks, VQA tasks, and reducing hallucination, achieving up to a 7.62% improvement on average. Additionally, we demonstrate CSR is capable of continuously improving performance over iterations, compatible with different large vision-language backbone models, and redirecting the attention of LVLMs toward the visual modality to achieve stronger modality alignment."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "In this section, we will provide a brief overview of LVLM and preference optimization.\n\nLarge Vision Language Models. LVLMs extend LLMs to multimodal scenario, which progressively predict the probability distribution of the next token for each input prompt. Given an <image, text> pair as input prompt, LVLM outputs a text response.\n\nPreference Optimization. Preference optimization has shown promise in fine-tuning language models and aligning their behavior with desired outcomes. Given an input prompt, a language model with policy can produce a conditional distribution with as the output text response. The preference data is defined as , where and denote the preferred and dispreferred responses for the input prompt. Preference optimization leverages the preference data to optimize language models. Taking DPO [###reference_b15###] as a representative example, it formulates the probability of obtaining each preference pair as , where is the sigmoid function. DPO optimizes the language models with the following classification loss:\nwhere represents the reference policy, i.e., language model after supervised fine-tuning."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Calibrated Self-Rewarding Vision Language Models",
            "text": "To address this challenge, we propose Calibrated Self-Rewarding (CSR), a novel approach aimed at improving modality alignment in LVLMs by integrating visual constraints into the self-rewarding paradigm. As illustrated in Figure 2, CSR trains the target LVLM by alternately performing two stages: candidate response generation and preference curation and fine-tuning. In the candidate response generation stage, we employ sentence-level beam search for each input prompt to produce fine-grained candidate responses. During this process, the language decoder determines the initial reward for each generated sentence, which is then enhanced by incorporating an image-response relevance score. This enhancement guides the generation of subsequent sentences and finally generates the entire response. Moving on to the preference curation and fine-tuning stage, we use the responses with the highest and lowest cumulative scores to construct the preferred and dispreferred responses and utilize the constructed preference pairs for fine-tuning. In the remaining of this section, we will provide detailed explanations of CSR."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Step-Level Reward Modeling and Calibration",
            "text": "Before delving into how to generate candidate response and construct preference data, in this section, we first discuss how to formulate the reward within CSR. The ideal reward in the LVLM fulfills two specific criteria:\n\nVision-Constrained Reward: This aspect aims to integrate image-relevance information into the reward definition of LVLMs. By doing so, we address the limitation of LVLM in overlooking image input data when generating preferences.\n\nStep-Wise Reward: Instead of assigning a single reward for the entire response, we opt for a step-wise approach. This involves assigning rewards at each step of response generation. Compared to a single reward, this finer-grained reward offers more detailed guidance and is more robust.\n\nSpecifically, the self-generated instruction-following score is calculated using the language decoder of the LVLM. It represents the sentence-level cumulative probability of generating a sentence. A higher self-generated instruction-following score indicates a stronger capability of the generated response to follow instructions.\n\nWhile the self-generated instruction-following score partially reflects the LVLM\u2019s preference, it still suffers from modality misalignment, potentially overlooking visual input information. To address this, we introduce an image-response relevance score. This score depicts the relevance between the generated sentence and input image. We leverage CLIP-score for this calculation, where the vision encoder in the CLIP model aligns with the vision encoder in the target LVLM. The image-response relevance score is defined as:\n\nwhere the terms represent the visual CLIP embedding and textual CLIP embedding, respectively. By combining both scores, we aim to redirect the attention of LVLM towards the input visual information, thus enhancing its modality alignment ability."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Iterative Fine-Tuning",
            "text": "After establishing the reward framework in CSR, we next discuss our iterative fine-tuning process. Within this framework, we iteratively perform two essential steps, namely candidate response generation and preference data curation and optimization. These steps are elaborated upon as follows:"
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Step-Level Candidate Response Generation",
            "text": "In candidate response generation, our objective is to generate responses to build preference data. To accomplish this, we employ a sentence-level beam search strategy. Initially, we concurrently sample multiple candidate sentences, utilizing the \"end of sub-sentence\" marker (e.g., \".\" in English) as the delimiter. We select the top and bottom sentences to proceed to the subsequent round of sentence-level beam search. This iterative process continues until reaching the \"end of response,\" conventionally represented as . Once all sentences for a response are generated, the detailed algorithm for candidate response generation is outlined in Algorithm 1."
        },
        {
            "section_id": "3.2.2",
            "parent_section_id": "3.2",
            "section_name": "3.2.2 Preference Curation and Optimization",
            "text": "After generating candidate responses with their reward scores, our next step is to curate the preference dataset. For each input prompt, we select the responses to construct the preference dataset for fine-tuning. For each iteration, we denote the constructed preference data as: . After obtaining the preference data, we fine-tune the target LVLM using DPO. At iteration , we use the last iteration fine-tuned model  as the reference model. The training process is detailed in Algorithm 1."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": "In this section, we empirically investigate CSR in addressing the modality misalignment problem of LVLMs, focusing on the following questions:\n(1) Can CSR help improve the performance of models on both comprehensive benchmarks and hallucination benchmarks?\n(2) Can CSR iteratively improve multimodal alignment progressively in LVLMs and lead to more factual LVLMs?\n(3) Is CSR compatible with different open-sourced LVLMs?\n(4) How does CSR change attention weights and preference pairs to align image and text modalities?"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setups",
            "text": "Implementation Details.  \nWe utilize LLaVA-1.5 7B and 13B [1] as the backbone models. During the preference learning process, we adapt LoRA fine-tuning [18]. The images and prompts used to construct the preference data are randomly sampled from the detailed description and complex reasoning subclasses of the LLaVA150k dataset, totaling approximately 13,000 samples [19]. It is worth noting that each iteration uses the same prompt and image as the previous round. Overall, the iterative training is conducted over three iterations, completed on one A100 80GB GPU. It takes roughly 3.5 and 5 hours for fine-tuning LLaVA-1.5 7B and LLaVA-1.5 13B, respectively. For more detailed information on training hyperparameters and training data, please refer to Appendix A.1.\n\nEvaluation Benchmarks. We conducted evaluations on three types of benchmarks: comprehensive benchmarks, general VQA and hallucination benchmarks. Specifically, this includes: (1) Comprehensive benchmarks (MME [20], SEEDbench [21], LLaVAW [19], MMbench [22], MM-Vet [23]); (2) General VQA (ScienceQA (SQA) [24], VisWiz [25], GQA [26]); (3) Hallucination benchmark (POPE [27], CHAIR [28]). More detailed description are discussed in Appendix A.1.\n\nBaselines. We will first compare with the self-rewarding approach described by Yuan et al. [29]. Here, we directly apply self-rewarding to LVLM, using the prompts and experimental settings outlined in Yuan et al. [29] (see detailed settings in Appendix A.1 and Table 3). We also compared with several data-driven preference learning methods, including Silkie (Vlfeedback) [9], LLaVA-RLHF (Human-preference) [8], POVID [10], and RLHF-V [30]. Furthermore, we compared the performance of the optimized LLaVA-1.5 via these methods with other state-of-the-art open-source LVLMs, including InstructBLIP [31], Qwen-VL-Chat [32], mPLUG-Owl2 [33], BLIP-2 [34], and IDEFICS [35], after the final rounds of training. Additionally, to evaluate the effectiveness of these methods on other LVLMs, we applied them to a recent LVLM called Vila [36]. For more information on these baselines, please refer to Appendix A.1."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "CSR Continuously Improves Model Performance over Iterations. In Figure 3, we report the average performance of LLaVA-1.5 7B and 13B models concerning the number of training iterations on comprehensive benchmarks, general VQA tasks, and hallucination benchmarks. In the experiment, the 7B model achieved an improvement of approximately 7.62% across all benchmarks through online iterative updates, while the 13B model saw an improvement of approximately 5.25%. According to the full results in Table 6 and Table 7 of Appendix A.5, the improvement is particularly significant on the LLaVAW and CHAIR benchmarks, with improvements of 8.9% and 49.50%, respectively. The results indicate that CSR is capable of incrementally improving model performance over iterations, demonstrating its effectiveness in self-improving the quality of generated preference data and leading to stronger modality alignment. The degree of improvement gradually becomes smaller, which is not surprising, indicating that the model is gradually converging.\n\nCSR Outperforms Competitive Preference Fine-Tuning Baselines. Compared to preference data curation approaches (e.g., POVID, RHLF-V) that generate preference data from either additional models or human annotations, the superiority of CSR indicates that adapting a self-rewarding paradigm better captures the inherent preferences of the target LVLMs, achieving stronger modality alignment. Furthermore, CSR outperforms existing self-rewarding methods, demonstrating its effectiveness in calibrating the reward model by incorporating image-response relevance scores. This mitigates the potential issue of overlooking visual input information when estimating self-generated preferences.\n\nIn addition, we compare the performance of LLaVA-1.5 after three rounds of online CSR with other state-of-the-art open-sourced VLLMs and report the results in Table 5 of Appendix A.5. Although different open-sourced VLLMs utilize various image and text encoders, CSR still outperforms other open-sourced VLLMs in 9 out of 10 benchmarks, further corroborating the effectiveness of CSR in improving modality alignment."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Analysis",
            "text": "Ablation Study.\nTo validate the effectiveness of using the image-response relevance score () to complement the self-generated instruction following score (), we specifically compare CSR with three variants: (1) without applying CSR on LLaVA 1.5 (Base); (2) using CSR with only the self-generated instruction following score (Only ); and (3) using CSR with only the image-response relevance score (Only ). The results are reported in Table 2. We first observe that CSR improves performance by jointly considering both the self-generated instruction following and image-response relevance scores. This verifies its effectiveness in enhancing modality alignment by calibrating the language-driven self-rewarding paradigm with visual constraints.\n\nCompatibility Analysis.\nTo validate CSR for its applicability to other LVLMs, we deployed CSR on Vila 7B and conducted three rounds of online iterations. We conducted experiments on all ten evaluation benchmarks and tasks, and the results are shown in Figure 4. Similar to the findings in Figure 3, Vila demonstrates a similar phenomenon during the online iterations of CSR, where it can self-correct preferences, leading to gradual improvements in all benchmarks. For Vila, the overall performance improved by 3.37% after three rounds of CSR iterations, with particularly notable increases of 8.48% on VisWiz and 14.0% on MM-Vet. The compatibility analysis further corroborates the generalizability and effectiveness of CSR in enhancing the performance of LVLMs.\n\nHow Does CSR Change the Image-Response Relevance Over Iterations?\nTo investigate how CSR gradually improve the performance over iterations, we analyzed the change of self-generated preference data with the LLaVA-1.5 7B model. In Figure 5, we illustrated the distribution of image-response relevance scores of three iterations over 500 examples from LLaVA-150k. We first observe that both the chosen (preferred) and rejected (dispreferred) responses achieve higher image-response relevance scores after the model undergoes CSR online iterations. This indicates that, following CSR, the responses generated by LVLMs are more closely aligned with the image information. Secondly, it can be observed that after multiple rounds of online iterations with CSR, the average image-response relevance scores for the rejected and chosen responses become closer to each other. This makes the self-generated preference data during CSR iterations more challenging to distinguish, while further strengthening the learning process.\n\nHow Does CSR Improve Modality Alignment?\nTo further understand how CSR affects modality alignment, in Figure 6, we present the changes in image and text attention maps for three models: the original LLaVA-1.5 7B model, the self-rewarding approach, and CSR. These attention maps illustrate the distribution of attention scores over image and text tokens. We observe that applying CSR strengthens the model\u2019s attention to certain visual tokens. Simultaneously, the change of attention values of the text tokens indicates that CSR is capable of alleviating the issue of over-reliance on context mentioned in Huang et al. Additionally, compared with the self-rewarding approach, CSR shows a more effective distribution of attention between image and text tokens. These findings indicate that with CSR, LVLMs can better align different modalities through a calibrated self-rewarding strategy, focusing more on the visual modality rather than over-relying on contextual text."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Theoretical Explanation",
            "text": "In this section, we present a theoretical framework to explain the empirical phenomenon that incorporating an image-response relevance score can calibrate the self-rewarding procedure, ultimately improving generation accuracy.\nAs we consider an LVLM, to facilitate the analysis, we decompose the input prompt into , representing the image and text prompts respectively. Although text data typically comprises discrete tokens, we follow the CLIP theory literature [38  ###reference_b38###, 39  ###reference_b39###, 40  ###reference_b40###] in modeling them as continuous-value random vectors in this section to elucidate the rationale behind our proposed method. More specifically, we assume the following data generative model for  and :\nwhere  and  are two orthonormal matrixces, representing decoders that transform the latent (low-dimensional) signals  to images and text respectively. We assume the covariance matrices of  are identity matrices.  and  are noise vectors, and we assume they follow sub-gaussian distributions with well-conditioned covariance matrices and sub-gaussian norms upper bounded by a universal constant. We consider the infinite data setting. This is a widely used simplification to avoid the\ninfluence of sample randomness [41  ###reference_b41###, 42  ###reference_b42###, 43  ###reference_b43###]. According to [38  ###reference_b38###], with an abundance of image-text pairs, the learned visual CLIP embedding  and textual CLIP embedding  converge to  and  respectively. To simplify our analysis without loss of generality, we consider a single score for each response  and define the image-response relevance score .\nWe assume the ground truth  with weights  and . In CSR, we assume the conditional distribution at iteration ,  with , follows a Gaussian distribution , where  and  are the weights matrices for the image and text inputs respectively, and  is the standard deviation. As the likelihood is monotonically decreasing with respect to , we consider the self-generated instruction-following score .\nThen the calibrated reward score becomes  for some . In theoretical analysis, we consider a simpler version of CSR, where we assume  (whose distribution is denoted by ), and  is the text output generated by . As  depends on , we denote the solution  by . In the special case where , this corresponds to the setting where we do not use the image-response relevance score at all.\nTo evaluate the quality of the text output , we consider a regression problem where there is an outcome  associated with the ground-truth text output :  with . We evaluate the quality of  by considering the loss function . We then have the following theorem.\nSuppose that  lies in the LLM space ,  and , then there exists , such that\nOur theoretical analysis implies that as long as , which happens when the model tends to prioritize textual information over visual input. By incorporating the image-response relevance score (corresponding to ), CSR is able to increase the attention on image signals in generating . As a result, the solution produced by CSR will be better than the method without using the image-response relevance score (corresponding to )."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Large Visual-Language Model Hallucination. Recently, the rapid development of visual-language alignment methods [19  ###reference_b19###, 44  ###reference_b44###, 45  ###reference_b45###, 46  ###reference_b46###] and LLMs [47  ###reference_b47###, 48  ###reference_b48###, 49  ###reference_b49###, 50  ###reference_b50###, 51  ###reference_b51###] has significantly accelerated the progress of LVLMs, which extend LLMs with visual modalities and demonstrate impressive visual understanding by unifying the encoding of visual and text tokens [52  ###reference_b52###, 53  ###reference_b53###, 54  ###reference_b54###, 55  ###reference_b55###, 56  ###reference_b56###, 57  ###reference_b57###]. However, LVLMs still face the problem of hallucination [58  ###reference_b58###, 59  ###reference_b59###, 60  ###reference_b60###], where generated text descriptions contradict the visual modality information. Various approaches have been proposed to address hallucination in LVLMs, including enhancing dataset quality for fine-tuning [61  ###reference_b61###, 8  ###reference_b8###, 62  ###reference_b62###, 9  ###reference_b9###], manipulating the decoding process [37  ###reference_b37###, 63  ###reference_b63###, 64  ###reference_b64###, 65  ###reference_b65###, 66  ###reference_b66###, 67  ###reference_b67###], and leveraging external closed-source models to facilitate post-hoc mitigation of hallucination [58  ###reference_b58###, 68  ###reference_b68###]. Though these approaches alleviate hallucination to some extent, they do not focus directly on improving modality alignment.\nPreference and Modality Alignment. In large models, alignment is necessary to ensure their behavior aligns with human preferences [69  ###reference_b69###, 15  ###reference_b15###, 70  ###reference_b70###]. In LVLMs, alignment manifests as modality misalignment, where the generated textual responses are supposed to follow the input visual information. Recently, preference optimization has been used to address the modality misalignment problem. These optimizations involve preference data curated by human annotators [8  ###reference_b8###, 61  ###reference_b61###, 30  ###reference_b30###] and additional models (e.g., GPT-4) [9  ###reference_b9###, 10  ###reference_b10###]. While these methods improve the ability of LVLMs to align modalities, their reliance on human annotation or additional models is resource-intensive and may introduce additional biases. Furthermore, these models cannot fully capture the inherent preferences of LVLMs, making the curated preference data less effective. Instead, CSR leverages a calibrated self-rewarding strategy, aiming to stimulate the LVLMs\u2019 self-correction and enhancement capabilities, thereby further improving modality alignment.\nSelf-Improvement in Large Language Models. Self-improvement emerges as a powerful paradigm for LLMs to enhance themselves without significant external intervention. For example, self-rewarding and online alignment [71  ###reference_b71###] propose a method that selects consistent answers generated by the model to fine-tune itself, thereby improving its reasoning ability. Similarly, [12  ###reference_b12###] utilizes self-play to enhance the model\u2019s performance by distinguishing its self-generated responses from those in human-annotated training data. Unlike prior methods that primarily target LLMs, CSR addresses the modality misalignment issue in LVLMs during the preference modeling process by introducing visual constraints, making it particularly well-suited for LVLMs."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we investigate the challenge of enhancing modality alignment in LVLMs by introducing a self-rewarding approach, which integrates visual constraints into the preference modeling process of the self-rewarding paradigm. Our approach enhances the alignment between image and text modalities, significantly reducing hallucination and improving performance on various LVLM evaluation benchmarks. These empirical results are further supported by rigorous theoretical findings. Additionally, the method is capable of continuously enhancing LVLM capabilities over iterations, leading to better utilization of visual information."
        }
    ],
    "url": "http://arxiv.org/html/2405.14622v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "6"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.2.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.14622v3",
        "paper_title": "Calibrated Self-Rewarding Vision Language Models",
        "research_background": "**Paper's Motivation:**\n\nThe paper is driven by the need to address the hallucination problem in large vision-language models (LVLMs). Despite the success of LVLMs in integrating pre-trained large language models (LLMs) and vision models through instruction tuning, they often generate text that, although plausible, contradicts the visual information in the accompanying image. This discrepancy, known as the hallucination phenomenon, arises due to the misalignment between image and text modalities. The motivation is to enhance the alignment of these modalities to improve the quality and accuracy of responses generated by LVLMs.\n\n**Research Problem:**\n\nThe key research problem tackled in this paper is the modality misalignment in LVLMs, which leads to erroneous text responses that conflict with the visual input. Existing methods to enhance modality alignment, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), are resource-intensive and often fail to capture the inherent preferences of the target LVLM. Consequently, there is a need for a more efficient and effective approach to calibrate and align response generation in LVLMs by incorporating visual constraints into the preference modeling process.\n\n**Relevant Prior Work:**\n\n1. **Large Vision-Language Models (LVLMs)** have achieved success through the integration of pre-trained large language models and vision models via instruction tuning (References [1] to [4]).\n2. **Hallucination Phenomenon**: This issue has been identified in LVLMs where generated text responses contradict the visual information (Reference [5]).\n3. **Misalignment Problem**: The cause of hallucination is traced back to the misalignment between image and text modalities, as discussed by Cui et al. [6] and Guan et al. [7].\n4. **Preference Fine-Tuning Techniques**: Methods such as RLHF (Reference [8]) and DPO (References [9] and [10]) have been proposed but involve additional models or human annotations, making them resource-intensive and less suited to capture target LVLM preferences.\n5. **Self-Rewarding Approaches**: Recent methods that employ a single LLM for both response generation and preference modeling have shown promise in LLM alignment (References [11] and [12]), but these approaches do not address modality misalignment in LVLMs.\n\nThe paper\u2019s novel contribution, the Calibrated Self-Rewarding (CSR) approach, aims to introduce visual constraints into the self-rewarding framework to tackle the response generation and preference modeling stages' modality alignment issues in LVLMs.",
        "methodology": "The \"Calibrated Self-Rewarding (CSR)\" method is structured to enhance modality alignment in Large Vision-Language Models (LVLMs) by embedding visual constraints into the self-rewarding framework. Here's an overview of the proposed method and its key components:\n\n1. **Alternating Training Stages**: CSR alternates between two primary stages:\n   - **Candidate Response Generation**: This involves generating potential responses for a given input prompt using sentence-level beam search.\n   - **Preference Curation and Fine-Tuning**: This involves selecting the best and worst responses based on calibrated rewards to create preference pairs for fine-tuning the model.\n\n2. **Candidate Response Generation**:\n   - **Sentence-Level Beam Search**: For each input prompt, the method employs beam search to generate detailed candidate responses.\n   - **Initial Reward Assignment**: The language decoder assigns an initial reward to each generated sentence.\n   - **Reward Calibration**: The initial reward is then adjusted by incorporating an image-response relevance score, which reflects how well the text aligns with the visual content. This calibrated reward informs the generation of subsequent sentences, culminating in a complete response.\n\n3. **Preference Curation and Fine-Tuning**:\n   - **Constructing Preferences**: Responses with the highest and lowest cumulative calibrated rewards are selected to create preferred and dispreferred response pairs.\n   - **Utilizing Preference Pairs**: These pairs are used to fine-tune the LVLM, helping it learn from both positive and negative examples based on the calibrated rewards.\n\nThe innovative aspect of this methodology lies in the integration of visual constraints to refine the self-rewarding paradigm, aiming to strengthen the coherence and relevance of the generated responses with the accompanying visual context. Detailed explanations of the CSR process follow in the remainder of the section.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\nIn the main experiment, we focus on empirically investigating the capability of Calibrated Self-Rewarding (CSR) to address the modality misalignment problem in Large Vision Language Models (LVLMs). We explore the following key questions:\n\n1. **Dataset and Benchmarks:**\n    - The experiments are conducted on both comprehensive benchmarks and hallucination benchmarks. Comprehensive benchmarks assess overall performance while hallucination benchmarks specifically evaluate the factual accuracy of the models.\n\n2. **Baselines:**\n    - We compare CSR-enhanced models against standard LVLMs without CSR to assess improvements in multimodal alignment and factual accuracy.\n\n3. **Evaluation Metrics:**\n    - For comprehensive benchmarks, metrics such as accuracy, F1 score, and precision-recall are used to evaluate performance.\n    - For hallucination benchmarks, metrics focus on the factual accuracy and the tendency of the model to hallucinate information, though specific metrics are not detailed in the provided section.\n\n### Main Experiment Results\n\n1. **Performance on Benchmarks:**\n    - The results illustrate that CSR significantly improves the performance of LVLMs on both comprehensive and hallucination benchmarks. This demonstrates that CSR is effective in enhancing the alignment and factuality between vision and language modalities.\n\n2. **Compatibility:**\n    - The experiment shows that CSR is compatible with various open-sourced LVLMs, confirming its broad applicability and potential for integration into a wide range of existing models.\n\nOverall, the main experiment confirms the efficacy of CSR in improving modality alignment and factual accuracy in LVLMs, reinforcing its value as a technique for enhancing multimodal systems."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The goal is to validate the effectiveness of incorporating image-response relevance scores into the self-generated instruction following scores in the Calibrated Self-Rewarding (CSR) approach for better modality alignment in large Vision-Language Models (LVLMs).",
            "experiment_process": "The ablation study involved comparing the full CSR approach with three variants: (1) without applying CSR on LLaVA 1.5 (Base), (2) using CSR with only the self-generated instruction following score (Only ), and (3) using CSR with only the image-response relevance score (Only ). The performance of these variants was reported in Table 2, with experiments conducted on ten benchmarks and tasks.",
            "result_discussion": "The results showed that the full CSR approach improved performance by considering both self-generated instruction following and image-response relevance scores. This indicates the effectiveness of CSR in enhancing modality alignment by calibrating the language-driven self-rewarding paradigm with visual constraints.",
            "ablation_id": "2405.14622v3.No1"
        },
        {
            "research_objective": "To validate the generalizability and applicability of CSR to different LVLMs like Vila 7B, and to observe how its performance changes over multiple iterations.",
            "experiment_process": "CSR was deployed on Vila 7B and three rounds of online iterations were conducted. Experiments were run on all ten evaluation benchmarks and tasks, with performance improvements tracked across each round, as shown in Figure 4.",
            "result_discussion": "The findings revealed a gradual improvement in benchmarks post-CSR iterations. Vila 7B showed an overall performance improvement of 3.37% after three CSR rounds, with specific improvements of 8.48% on VisWiz and 14.0% on MM-Vet. This demonstrates the compatibility and effectiveness of CSR in enhancing LVLMs across various benchmarks.",
            "ablation_id": "2405.14622v3.No2"
        },
        {
            "research_objective": "To investigate how the CSR method affects the image-response relevance scores over multiple iterations, thereby explaining the mechanism behind performance improvements.",
            "experiment_process": "The analysis involved tracking the distribution of image-response relevance scores over three iterations using the LLaVA-1.5 7B model on 500 examples from LLaVA-150k. Figure 5 illustrated the changes in relevance scores for chosen (preferred) and rejected (dispreferred) responses.",
            "result_discussion": "Both chosen and rejected responses achieved higher image-response relevance scores after CSR iterations, indicating more closely aligned responses with image information. Over multiple CSR rounds, the average relevance scores for rejected and chosen responses became closer, making self-generated preference data harder to distinguish, thereby reinforcing the learning process.",
            "ablation_id": "2405.14622v3.No3"
        },
        {
            "research_objective": "To understand the impact of CSR on modality alignment by analyzing changes in image and text attention maps.",
            "experiment_process": "The study compared the attention maps of the original LLaVA-1.5 7B model, the self-rewarding approach without calibration, and CSR. Attention maps were generated to show the distribution of attention scores on image and text tokens.",
            "result_discussion": "CSR was found to strengthen attention to certain visual tokens and alleviate over-reliance on contextual text tokens. Compared to the self-rewarding approach, CSR provided a more balanced distribution of attention between image and text tokens, highlighting improved modality alignment through a calibrated self-rewarding strategy.",
            "ablation_id": "2405.14622v3.No4"
        }
    ]
}