{
    "title": "MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation",
    "abstract": "Large Language Models (LLMs) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, leading to repetitive teaching on the knowledge the student models have already learned and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive, and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, rather than distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the student MT model on about 10% of examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve translation performance on unseen contexts and words.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLM) have shown their impressive capabilities across almost all natural language tasks (Brown et al., 2020; Zhao et al., 2023). However, their ability strongly correlates with the model size. In the field of machine translation, competitive results can only be evidenced on larger LLMs, while medium-sized LLMs like Alpaca (Taori et al., 2023) and ParroT (Jiao et al., 2023a) still lag behind supervised NMT systems by a large margin (Jiao et al., 2023a; Zhu et al., 2023). How to efficiently transfer knowledge from larger LLMs to existing MT models that are affordable to deploy, is an important research direction.\n\nThe most common method for knowledge transferring is knowledge distillation (KD) (Hinton et al., 2015; Kim and Rush, 2016), where given an unlabeled corpus, a student model is trained to mimic the output of a teacher model on the corpus. Although KD is a well-studied technique and has proven effective in many previous works (Kim and Rush, 2016; Wang et al., 2021; Liu et al., 2023), we argue that when transferring knowledge from giant LLMs to existing MT models, the traditional KD method does not take the capability of the student and teacher model into consideration, therefore leaving much room for improvement in terms of both efficiency and effectiveness.\n\nFirstly, in contrast to student models in previous works (Kim and Rush, 2016; Wang et al., 2021; Liu et al., 2023) that are randomly initialized, recent student MT models (Hsieh et al., 2023; Fu et al., 2023) already exhibit a reasonable level of language proficiency, i.e., they can already accurately translate most examples in the unlabeled corpus. This renders the fine-tuning of student models on all teacher outputs both redundant and inefficient.\n\nSecondly, the efficacy of KD is significantly constrained by the coverage of the monolingual corpus, which impedes their performance when translating words in novel contexts or words unseen in the monolingual corpus. However, modern LLMs grasp strong translation and language knowledge, as well as the ability to follow human instructions. This enables the development of more efficient and effective strategies for addressing these problems.\n\nIn this paper, we introduce MT-Patcher, a novel framework designed for the knowledge transfer from LLMs to existing MT models in a selective, comprehensive, and proactive manner. The design philosophy of MT-Patcher is inspired by effective teaching strategies observed in real-world scenarios. Rather than subjecting students to endless drills, an effective teacher would first assess the student\u2019s current abilities, then design practice to reinforce areas of weakness and extend learning to new situations (Lee Jr and Pruitt, 1979; Epstein and Voorhis, 2001). Leveraging the strong language capabilities of LLMs, our method seeks to emulate these pedagogical strategies. Specifically, we gather instructional data from GPT-4, which demonstrates how to identify and correct errors in student model translations, anticipate additional potential errors that the student models may commit, and synthesize diverse contexts for relevant translation knowledge that aids the student model in rectifying these errors. We subsequently fine-tune an existing proficient LLM on these data to transform it into an MT-Patcher model.\n\nWe conduct experiments on translating specific language phenomena (chemistry materials and Chinese idioms) and on general machine translation benchmarks (WMT22 Chinese-English, English-German and English-Japanese). Experimental results show that fine-tuning the student model on only 10% examples selected by MT-Patcher is equivalent to fine-tuning on all examples as in KD, and enlarging the fine-tuning corpus via the context synthesis and proactive error prediction technique further improves the translation performance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Numerous studies have attempted to leverage LLMs for machine translation. Initial efforts (Lin et al., 2022; Vilar et al., 2022; Agrawal et al., 2023; Zhu et al., 2023; Hendy et al., 2023; Jiao et al., 2023b) centered on in-context learning, which utilizes several translation examples to guide the translation behavior of LLMs. Subsequent research (Jiao et al., 2023a; Li et al., 2023) shifted the focus to fine-tuning LLMs on existing parallel corpora to more effectively harness their translation capabilities. However, the translation performance of LLMs has not been as remarkable as their performance in other NLP tasks. Only state-of-the-art LLMs such as GPT-3 and GPT-4, which boast more than 100 billion parameters, can rival the performance of commercial translation systems (Hendy et al., 2023; Jiao et al., 2023b). Meanwhile, other medium-sized LLMs significantly trail behind supervised MT models (Zhu et al., 2023; Li et al., 2023; Jiao et al., 2023a). Li et al. (2023) suggest that the primary barrier to enhancing LLMs\u2019 performance is the lack of translation knowledge. Given that larger LLMs inherently possess more knowledge due to the scaling law (Kaplan et al., 2020), our work concentrates on transferring knowledge from these models to existing MT models.\n\nKnowledge distillation (KD), which improves smaller student models by learning on larger teacher models\u2019 output, is widely used in machine translation. Two common KD methods are LogitKD (Hinton et al., 2015; Tan et al., 2018), which optimizes the student model to match the teacher model\u2019s predicted distribution, and Sequence KD (SeqKD) (Kim and Rush, 2016; Wang et al., 2021; Gu et al., 2018; Zhou et al., 2019), where the student learns from the teacher-generated pseudo target sequence. As LogitKD requires access to the teacher\u2019s logits, it is impractical for distilling from proprietary LLMs. Therefore, we base our method on SeqKD, where student refers to the smaller MT model we would like to improve, and teacher refers to larger LLMs which possess more translation knowledge than student. Selective KD has been proposed by Wang et al. (2021) and Liu et al. (2023), but they all rely on comparing student models\u2019 outputs to oracle references. Unlike these works, our method instructs the LLM to identify student translation errors directly.\n\nWith the growing generative capabilities of Large Language Models (LLMs), many works attempt to harness them for corpora generation. The generated corpora can serve as demonstrations for few-shot prompting (Sahu et al., 2022), fine-tuning corpora for existing models (Yoo et al., 2021), or seed corpora for human refinement (Yuan et al., 2021a). Studies such as Chung et al. (2023); Yu et al. (2023) also explore ways to balance diversity, accuracy, and bias reduction in LLM-based dataset synthesis. However, these approaches often generate datasets from scratch, ignoring the capabilities of the models being optimized, resulting in less efficiency compared to our method."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we present MT-Patcher , a framework that distills knowledge from LLMs to existing MT systems more efficiently and effectively. The process of MT-Patcher undergoes two stages:\nKnowledge Selection: In this stage, the LLM acts as the feedbacker, which provides natural language feedback to translations of student models. Based on the\nfeedback, we select source sentences with identified errors, which indicate knowledge deficiency of the student models, to the next stage.\nKnowledge Extension: In this stage, the LLM acts as the parallel data synthesizer and word analoger, which help the student model learn words it makes mistakes on by extending to more diverse contexts and similar words.\nFigure 1  ###reference_### illustrates how MT-Patcher works.\nThe goal of the parallel data synthesizer is to synthesize parallel sentences  that contain a specific pair of phrases  where the student model makes mistakes in the context , in order to generalize the current translation knowledge to more contexts. Ideally, the synthesized parallel sentences should be semantically diverse yet still similar to the original context in other aspects. However, in the preliminary experiments, we find that even for powerful LLMs like GPT-4, when conditioning them on the original context , the generated parallel data lacks diversity and mostly resembles .\nTo tackle this problem, we introduce another module called sentence analyzer, which first extracts the information of domain, topic and style of the original context. We then instruct the LLMs to synthesize parallel sentences with the same attributes as well as containing the phrase pair . This process can be seen as an information bottleneck where we squeeze the semantic information yet keep other attributes.\nWe further introduce the word analoger to proactively predict potential errors the student model may commit. For example, if the student MT model incorrectly translates the term methanol, an educated guess is that it may struggle with translating words within the domain of chemistry, such as benzene and ethanol. By anticipating these potential errors, we can enhance the student model\u2019s translation capability for words not present in the monolingual corpus.\nPractically, given a source sentence  and a word  that the student MT model mistranslates, the word analoger aims to associate more words from two perspectives: (1) category, i.e., words belonging to the same category as , and (2) semantic, i.e., words that frequently co-occur with . We also require that the generated words should be rare and challenging in the prompt, ensuring that the student model will struggle to translate them accurately."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Knowledge Selection via Feedbacker",
            "text": "When transferring knowledge from LLMs to existing MT models, traditional SeqKD would finetune the student model on all teacher\u2019s output, ignoring the fact that the student model can already translate most of the examples well. Furthermore, several recent studies have unveiled emergent abilities in LLMs, such as Self-Refinement (Madaan et al., 2023  ###reference_b21###) and Self-Debug (Chen et al., 2024  ###reference_b4###), suggesting that iterative refinement of an initial draft may be a more effective strategy to tap into the knowledge reserves of LLMs.\nTo improve the efficiency of SeqKD and better elicit LLMs\u2019 knowledge, we propose to finetune LLMs to be a feedbacker, which produces natural language feedback of the student models\u2019 translation instead of directly generating its own translations.\nFormally, given a source sentence  and its corresponding translation , the goal of the feedbacker is to generate a comprehensive assessment . This assessment comprises tuples of , where  describes whether  contains translation errors,  corresponds to the source span, explanation and correction of the -th identified error, respectively, and  is the final post-edited translation that incorporates all error corrections."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Knowledge Extension via Parallel Data Synthesizer and Word Analoger",
            "text": "Another limitation of SeqKD is that the knowledge it can transfer is strictly limited to the given monolingual corpus. This limitation can hinder its generalizability in two key ways. Firstly, the correct translation of mistranslated words or phrases can only be learned within the contexts present in the given monolingual corpus, potentially limiting its applicability to broader contexts. Secondly, SeqKD also lacks the capacity for knowledge extrapolation, which prevents it from transferring knowledge that does not occur in the monolingual corpus.\nInspired by the principle of knowledge extension when designing good practice in the educational process (Lee Jr and Pruitt, 1979  ###reference_b17###; Epstein and Voorhis, 2001  ###reference_b6###), we transform LLMs into two modules to mitigate above two problems, respectively: parallel data synthesizer and word analoger.\nThe goal of the parallel data synthesizer is to synthesize parallel sentences  that contain a specific pair of phrases  where the student model makes mistakes in the context , in order to generalize the current translation knowledge to more contexts. Ideally, the synthesized parallel sentences should be semantically diverse yet still similar to the original context in other aspects. However, in the preliminary experiments, we find that even for powerful LLMs like GPT-4, when conditioning them on the original context , the generated parallel data lacks diversity and mostly resembles .\nTo tackle this problem, we introduce another module called sentence analyzer, which first extracts the information of domain, topic and style of the original context. We then instruct the LLMs to synthesize parallel sentences with the same attributes as well as containing the phrase pair . This process can be seen as an information bottleneck where we squeeze the semantic information yet keep other attributes.\nWe further introduce the word analoger to proactively predict potential errors the student model may commit. For example, if the student MT model incorrectly translates the term methanol, an educated guess is that it may struggle with translating words within the domain of chemistry, such as benzene and ethanol. By anticipating these potential errors, we can enhance the student model\u2019s translation capability for words not present in the monolingual corpus.\nPractically, given a source sentence  and a word  that the student MT model mistranslates, the word analoger aims to associate more words from two perspectives: (1) category, i.e., words belonging to the same category as , and (2) semantic, i.e., words that frequently co-occur with . We also require that the generated words should be rare and challenging in the prompt, ensuring that the student model will struggle to translate them accurately."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Implementation of MT-Patcher",
            "text": "Theoretically, state-of-the-art LLMs like GPT-4 can already serve as an MT-Patcher to transfer its knowledge to MT models. However, in practice, because we do not have unlimited access to GPT-4, we instead collect the demonstration data from GPT-4. Specifically, given a student model, we first use it to generate its translation on 20,000 monolingual sentences randomly selected from the monolingual corpus. We then leverage GPT-4 to execute the pipeline of MT-Patcher including (1) giving feedback  given the source sentence and student\u2019s translation , (2) analyzing the domain, topic and style  of the source sentence  (3) making analogies  given the source sentence  and a word  in  (4) synthesizing parallel sentences containing error source words  and their corrections  with the same domain, topic and style attribute . Finally, we finetune the teacher LLM on these data to transform it to an MT-Patcher. All prompts we use for building MT-Patcher can be found in Appendix A  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate our method on Chinese-English and English-German translation. For student translation models, we consider NLLB-200 3.3B, a multilingual translation model pre-trained on 200 languages. Having been trained on massive parallel data, it can already translate reasonably well but falls short of language knowledge compared to LLMs, making it an ideal knowledge recipient for our experiment.\n\nDue to the increasing interest in adopting LLMs for MT, we also consider ParroT, an LLM-based MT model finetuned on WMT validation sets from LLaMA-7B. The backbone LLMs for building MT-Patcher in this paper are LLaMA2-13B and Baichuan-2-13B. LLaMA2-13B is an English LLM and used to build MT-Patcher for English-German translation models. Baichuan-2-13B is trained on a mix of both Chinese and English corpus and demonstrates much stronger abilities in Chinese compared to LLaMA2. Therefore, we adopt it for building MT-Patcher for Chinese-English translation models. For each language pair considered, we fully finetune the corresponding LLM on the collected data for 3 epochs.\n\nWe compare the translation performance of the following methods: \n- Student is the translation model to be patched. In this paper, it refers to NLLB 3.3B or ParroT.\n- Teacher is the model that is achieved by finetuning the larger LLM to perform translation directly. \n- SeqKD are models achieved by finetuning the Student model on the Teacher\u2019s translations.\n- MT-Patcher (PE) is the variant of MT-Patcher, finetuning the Student model on the post-editing results in feedback.\n- MT-Patcher (PE + PDS) is the variant of MT-Patcher which finetunes the Student model on the post-editing results as well as additional synthesized parallel sentences. \n\nFrom Table 1, we can first see that the performance of MT-Patcher (PE) is better than SeqKD-Equal, and can be comparable to SeqKD-Full. This indicates the proposed method can select more valuable examples and discard useless examples. We also find our method suffers less from catastrophic forgetting compared to SeqKD-Full. This makes MT-Patcher an appealing method for real-world applications, considering the cost for finetuning the Student model is growing nowadays.\n\nWe can also see that applying the parallel data synthesizer and word analoger to generate more patch data can further improve the translation performance of MT-Patcher, highlighting the benefits of extending coverage of context and knowledge during the process of knowledge transferring. \n\nIt is worth noting that in the English-German direction, the teacher based on LLaMA-2-13B performs substantially worse than the student (NLLB 3.3B), which is consistent with previous findings that it is not trivial to adopt existing LLMs to outperform supervised translation models. As a result, SeqKD from this teacher leads to poor performance. However, based on the same backbone LLM, MT-Patcher can still improve the performance of the Student model. This can be attributed to the hypothesis that revising an initial draft is a better way to elicit the knowledge of LLMs than direct generation, which we provide a further analysis.\n\nAlthough we mainly focus on settings where we have strong teachers (which is why we choose different teacher models for English-German and Chinese-English translation), we also experiment with medium resource translation: WMT22 English-Japanese, using LLaMA2 as the teacher and NLLB 3.3B as the student. We present the results in Table 3. We find MT-Patcher can still outperform SeqKD in this setting.\n\nFrom Table 2, we can see that despite that the Teacher model achieves significantly better performance than the Student model, the SeqKD-Full method can only narrow less than half of the gap. However, by synthesizing more contexts for each error, MT-Patcher (+PE + PDG) improves the relative performance from 59.2% to 80.5% for chemistry materials, and 57.5% to 69.8% for Chinese Idioms, indicating the importance of translation knowledge in multiple contexts in order to generalize to novel contexts better.\n\nWe can also observe that both SeqKD-Full and MT-Patcher (+PE + PDG) cannot behave well on the Unseen Word set, which can be attributed to their inability to extrapolate from the observed errors to unseen errors. By generating analogous words to anticipate more errors, the translation performances on Unseen Word are significantly improved, validating the effectiveness of the proposed error anticipation method."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Settings",
            "text": "For student translation models, we consider NLLB-200 3.3B (NLLB Team et al., 2022), a multilingual translation model pre-trained on 200 languages. Having been trained on massive parallel data, it can already translate reasonably well but falls short of language knowledge compared to LLMs, making it an ideal knowledge recipient for our experiment. Due to the increasing interest in adopting LLMs for MT, we also consider ParroT (Jiao et al., 2023a), an LLM-based MT model fine-tuned on WMT validation sets from LLaMA-7B (Touvron et al., 2023).\n\nThe backbone LLMs for building MT-Patcher in this paper are LLaMA2-13B (Touvron et al., 2023) and Baichuan-2-13B (Baichuan Inc, 2023). LLaMA2-13B is an English LLM and used to build MT-Patcher for English-German translation models. Baichuan-2-13B is trained on a mix of both Chinese and English corpus and demonstrates much stronger abilities in Chinese compared to LLaMA2. Therefore, we adopt it for building MT-Patcher for Chinese-English translation models. For each language pair considered, we fully fine-tune the corresponding LLM on the collected data for 3 epochs. See Appendix B for more implementation details.\n\nWe compare the translation performance of the following methods:\n\nStudent is the translation model to be patched. In this paper, it refers to NLLB 3.3B or ParroT.\n\nTeacher is the model that is achieved by fine-tuning the larger LLM to perform translation directly. For a fair comparison, we fine-tune the LLM on GPT-4\u2019s translation on the monolingual sentences.\n\nSeqKD are models achieved by fine-tuning the Student model on the Teacher\u2019s translations.\n\nMT-Patcher (PE) is the variant of MT-Patcher, fine-tuning the Student model on the post-editing results in feedback.\n\nMT-Patcher (PE + PDS) is the variant of MT-Patcher which fine-tunes the Student model on the post-editing results as well as additional synthesized parallel sentences generated by parallel data synthesizer containing (error, correction) pairs. Unless other stated, we set the number of pseudo-parallel sentences to be 4 in this paper.\n\nMT-Patcher (PE + PDS + WA) is the variant of MT-Patcher which fine-tunes the Student model on the post-editing results and parallel sentences generated by parallel data synthesizer containing (error, correction) pairs and additional word pairs from word analoger. We generate 2 analogous words for each category and 1 context for each word."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results on General Machine Translation",
            "text": "Table 1 presents experimental results on general machine translation benchmarks: WMT22 Chinese-English and English-German translation. We randomly select 1,000,000 sentences from RefinedWeb and WuDao 2.0, respectively, as English and Chinese monolingual corpus. Performance is evaluated in COMET and sacreBLEU. \n\nFrom Table 1, we can first see that the performance of MT-Patcher (PE) is better than SeqKD-Equal and can be comparable to SeqKD-Full. This indicates the proposed method can select more valuable examples and discard useless examples. We also find our method suffers less from catastrophic forgetting compared to SeqKD-Full (See Appendix C for more experimental results). This makes MT-Patcher an appealing method for real-world applications, considering the cost for finetuning the Student model is growing nowadays.\n\nWe can also see that applying the parallel data synthesizer and word analoger to generate more patch data can further improve the translation performance of MT-Patcher, highlighting the benefits of extending coverage of context and knowledge during the process of knowledge transferring.\n\nIt is worth noting that in the English-German direction, the teacher based on LLaMA-2-13B performs substantially worse than the student (NLLB 3.3B), which is consistent with previous findings that it is not trivial to adopt existing LLMs to outperform supervised translation models. As a result, SeqKD from this teacher leads to poor performance. However, based on the same backbone LLM, MT-Patcher can still improve the performance of the Student model. This can be attributed to the hypothesis that revising an initial draft is a better way to elicit the knowledge of LLMs than direct generation, which we provide a further analysis in Section 5.2.\n\nAlthough we mainly focus on settings where we have strong teachers (which is why we choose different teacher models for English-German and Chinese-English translation), we also experiment with medium resource translation: WMT22 English-Japanese, using LLaMA2 as the teacher and NLLB 3.3B as the student. We present the results in Table 3. We find MT-Patcher can still outperform SeqKD in this setting."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Results on Specific Language Phenomena",
            "text": "In order to understand how MT-Patcher can improve the effectiveness of knowledge transfer, we present experiments on the Chinese-to-English translation for two specific language phenomena: chemistry materials and Chinese idioms. We select them for two reasons: (1) Both belong to long-tailed knowledge that student MT models cannot grasp very well. (2) There are also distinctions between them: chemistry materials represent simple, context-free knowledge, while Chinese idioms represent more abstract and metaphorical knowledge.\n\nSpecifically, for each language phenomenon, we first collect a list of 6,000 of them and their corresponding translations from the web. We then split these word pairs into two categories: Seen and Unseen, and create a monolingual set as well as two test sets based on the split:\n\nMonolingual Set. For each word pair in the Seen set, we ask GPT-4 to synthesize one sentence that contains the source word. This set is for SeqKD and MT-Patcher to leverage.\n\nTest Set for Unseen Context. For each word pair in the Seen set, we also ask GPT-4 to synthesize one parallel sentence pair that contains the source and target word in the source and target sentence, respectively. This set is for testing models\u2019 generalization ability when source words are seen yet contexts are novel.\n\nTest Set for Unseen Word. We collect the test set for Unseen Word in a similar way as Unseen Context using the word pairs in the Unseen set. This set is for testing models\u2019 generalization ability to novel words.\n\nWe take the Baichuan-2-13B as the LLM and NLLB 3.3B as the student model, and present the experimental results in Table 2. The accuracy of translating chemistry materials represents the percentage of test examples where the correct translation of the source chemistry material is found in the translation. Regarding Chinese idioms, due to the difficulty of providing reference translations of them, we instead ask GPT-4 to assess the translation quality given the source sentence, target sentence, and dictionary definition. We report the average score, which ranges from 0 to 5. For ease of comparison, we also report how different models perform relative to the feedbackers, for which we directly take its correction as the translation.\n\nFrom Table 2, we can see that despite that the Teacher model achieves significantly better performance than the Student model, the SeqKD-Full method can only narrow less than half of the gap. However, by synthesizing more contexts for each error, MT-Patcher (+PE + PDG) improves the relative performance from 59.2% to 80.5% for chemistry materials, and 57.5% to 69.8% for Chinese Idioms, indicating the importance of translation knowledge in multiple contexts in order to generalize to novel contexts better.\n\nWe can also observe that both SeqKD-Full and MT-Patcher (+PE + PDG) cannot behave well on the Unseen Word set, which can be attributed to their inability to extrapolate from the observed errors to unseen errors. By generating analogous words to anticipate more errors, the translation performances on Unseen Word are significantly improved, validating the effectiveness of the proposed error anticipation method."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "We provide further analysis on how MT-Patcher works and its applicability to real-world scenarios. All experiments are conducted on the WMT22 Chinese-to-English translation datasets, and the student MT model is NLLB 3.3B.\n###figure_2###"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Impact of the number of synthesized contexts per word and analogous word",
            "text": "In Figure 2  ###reference_###, we plot how increasing the number of synthesized contexts per word and analogous words affects the translation performance of the student model. Note that we only synthesize one context for each analogous word. We can see increasing both numbers results in improved translation performance. For synthesized contexts, the gain plateau between 16 to 32 suggests this amount of different contexts is adequate for word or phrase learning. For analogous words, however, we observe the performance grows at a log-linear rate 333It is worth noting that this does not mean MT-Patcher can improve the translation performance endlessly, since it cannot generate an unlimited amount of valid analogous words. The performance will eventually plateau, although we have not scaled to the number due to the computational limitation..\n###figure_3###"
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Does asking for feedback better elicit LLMs\u2019 translation knowledge?",
            "text": "We conduct a head-to-head comparison between two ways to leverage the teacher LLM: ask the teacher to directly provide translation vs. ask MT-Patcher to give feedback on the student\u2019s translation. Specifically, we randomly select 1000 examples and compare the correction provided by MT-Patcher to the translation provided by the teacher. The comparison is made by both human and GPT-4.\nThe results are shown in Figure 3  ###reference_###. It can be seen that MT-Patcher\u2019s corrections are considered by both GPT-4 and human evaluators to be comparable or better than the teacher\u2019s translation on more than 80% examples, demonstrating the benefits of eliciting LLM\u2019s knowledge in the form of feedback."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "The Effectiveness of Iterative Feedback",
            "text": "###figure_4### In this section, we explore whether the application of iterative feedback on post-edited translations can enhance the final translation quality, thereby yielding a better Student model. While iterative feedback may incur additional computational costs, it allows us to compare feedback across multiple iterations and assess the reliability of error identification and correction from the feedbacker. Intuitively, if an error span identified and rectified in the -th epoch is still deemed problematic in the subsequent epoch, it suggests an inconsistency in the feedbacker\u2019s decision-making process. To prevent the introduction of incorrect knowledge during the knowledge transfer process, examples with such inconsistencies are discarded.\nWe randomly select 2000 instances of MT-Patcher\u2019s feedback on NLLB-3B\u2019s translation results and apply iterative feedback. We then ask GPT-4 to evaluate the feedback quality after each iterative feedback epoch. The results, depicted in Figure 4  ###reference_###, indicate that iterative feedback can enhance the accuracy of corrections in remaining examples, converging to 90.4% after 4 epochs at the expense of filtering out approximately 20% of examples. To understand the quality-quantity trade-off of demonstration data, we further fine-tune the Student NLLB model on post-editing data after each iterative feedback epoch and display the translation performance in Table 4  ###reference_###. Despite a decrease in the amount of fine-tuning data as the epoch increases, the translation performance of the fine-tuned model continues to improve, highlighting the significance of high-quality fine-tuning data."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Transferability of MT-Patcher",
            "text": "The construction of MT-Patcher is model-dependent; that is given an MT model, LLMs are finetuned on the data from GPT-4 which demonstrates how to execute the MT-Patcher pipeline on the translation of the corresponding MT model. Considering the cost of data collection and model training, one may question whether MT-Patcher is transferable, i.e., a patcher model for one MT model can improve the performance of another MT model. We present such results in Table 5  ###reference_###. Although the performance of applying MT-Patcher to its dedicated MT model is superior, the application of MT-Patcher trained on another model still significantly surpasses the baseline results, suggesting the potential for a robust MT-Patcher across various MT models."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We introduce MT-Patcher, a framework designed to leverage capabilities of LLMs to enhance the efficiency and effectiveness of translation knowledge transfer from LLMs to existing MT models. Our approach involves a pipeline that initially generates feedback on translations produced by MT models, followed by the synthesis of potential errors and diverse contexts to systematically rectify these translation errors. Through experimentation on both general and narrow domain MT benchmarks, we demonstrate that MT-Patcher effectively improves student MT models\u2019 performances compared to SeqKD baselines, and exhibits successful transferability across different models. In the future, we plan to refine our method from two angles. Firstly, previous works have identified translationese as a significant issue, and training on pseudo data generated by LLMs can exacerbate this problem. A promising solution could involve retrieving target sentences containing correction words and back-translating them to the source side. Secondly, the feedback\u2019s reason field contains a wealth of valuable information. We intend to explore more efficient strategies to harness this data."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix x1",
            "parent_section_id": null,
            "section_name": "Appendix",
            "text": ""
        },
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Prompts for MT-Patcher",
            "text": "Table 7  ###reference_###, 8  ###reference_###, 9  ###reference_###, 10  ###reference_### shows the prompt we used for the feedbacker, sentence analysis, parallel data synthesis and word analogy task, respectively."
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Implementation details",
            "text": "We fully finetune LLMs on the collected demonstration data from GPT-4 for 3 epochs. The learning rate is set to , and the batch size is 64. During training, we only compute the next token prediction loss on the response tokens."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C MT-Patcher\u00a0suffers less from catastrophic forgetting.",
            "text": "We test the GermanEnglish performance of competitors in the ChineseEnglish setting, including the original student model (ParroT-7B), SeqKD-Full, and MT-Patcher (PE). We found SeqKD-Full experiences a significant decrease in performance, while MT-Patcher\u2019s performance degradation is much less. This suggests that MT-Patcher is less prone to catastrophic forgetting, thereby demonstrating its potential for repeated application to a target MT system without detriment to its initial capabilities."
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Details of datasets used for chemistry materials and Chinese idioms",
            "text": "For chemistry materials, the data is extracted from Inventory of Existing Chemical Substances in China, released by Ministry of Ecology and Environment, China 444https://www.mee.gov.cn/gkml/hbb/bgg/201301/t20130131_245810.htm  ###reference_/t20130131_245810.htm###.\nFor Chinese idioms, we use the crawled data from the Github repo 555https://github.com/pwxcoo/chinese-xinhua  ###reference_###, and have manually checked the data quality (Of the randomly selected 50 examples, there are only 2 examples that have quality issues).\nWe split each word set to two subsets with 5500 and 500 words, respectively, and use GPT-4 to synthesize contexts for them. Figure 6  ###reference_### illustrates the process of constructing the monolingual set and two test sets.\n###figure_5###"
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E Prompts for Evaluation",
            "text": "Table 11  ###reference_### shows the prompt we used for evaluating the translation quality of Chinese idioms. Table 12  ###reference_### shows the prompt we used for translation comparison between direct generation and feedback.\nAssuming you are a highly proficient translator skilled at providing detailed and comprehensive assessments of machine translations. I will give you a <srclang> sentence X and its <tgtlang> translation Y, and I would like you to help assess the translation. \n1. You should first provide an overall assessment. \n2. Following that, \n- If there are no errors, just say \"No error.\" and do not provide an explanation. \n- If there are errors, please specify \n- the error type, \n- the corresponding segment in the <srclang> sentence X, \n- the corresponding segment in the translation Y, \n- the reason for the error, \n- and the correct translation for the segment \n- If there are errors, you should also provide a good translation at the end of the assessment. \n4. For multiple errors, you should address them separately. \n5. Try to pinpoint the smallest segments containing errors and explain them, avoiding cases where the error encompasses the entire sentence. \n6. Carefully read the original text and the translation to identify all translation errors. \n7. Your response should be in English. \n8. Be concise. \n\nNow, please assess the following translation: \n\n<srclang>: <srctext> \n<tgtlang>: <tgttext> \n\nAssessment:\nSuppose you are a language expert of <srclang> and <tgtlang>. Given a sentence X, please point out its topic, domain and style. \nInput: \nX: <srctext> \nOutput:\nSuppose you are a language expert of <srclang> and <tgtlang>. Given a topic, a domain and a style, as well as a bilingual word pair, please generate a pair of parallel sentences that adhere to the given topic, domain and style. They should also contain the given word pair. \nInput: \nDomain: <domain> \nTopic: <topic> \nStyle: <style> \nWord Pair: <wordpair> \n\nOutput:\nAssume you are a <srclang> and <tgtlang> language expert with a wealth of knowledge and associative ability in both languages. I will give you a word/phrase P from an <srclang> sentence X. Please associate from the following aspects and generate three words similar to X for each aspect, and provide the <tgtlang> translation of these words. \n\nAspects of association: \n- Category. What kind of category does this word belong to? \n- Semantics. What words often appear in the same context as the given word? \n\nNOTE, the associated words should be rare words, so that it is unlike for a machine translation system to translate it correctly. \n\nInput: \nX: <srctext> \nP: <errorword> \n\nOutput:\nAssume you are a language expert in English and Chinese. I will give you a Chinese idiom S, a sentence X that contains S, and a machine-generated English translation Y of the source sentence X.\nI will also give you the explanation/definition E of the idiom S. Your task is to first identify the translation of S in Y, and judge whether the translation of the idiom is correct. \n\nNote:\n1. The score range is 0/1/2/3/4/5, where \n- 0: Completely incorrect translation or no translation \n- 1: Literal translation of the original, without conveying any implied meaning, leaving non-Chinese background readers baffled \n- 2: Literal translation of the original, partially conveying the implied meaning, easy for non-Chinese background readers to understand \n- 3: Interpretative translation of the idiom, but only partially conveying the implied meaning \n- 4: Interpretative translation of the idiom, fully conveying the implied meaning \n- 5: The translation perfectly conveys the implied meaning of the idiom, is very easy for all readers to understand, and also maintains the aesthetic sense of the original \n\n2. You should generate the explanation of your decision concisely. \nNow, please process the following inputs:\nAssume you are a language expert in Chinese and English.\nI will give you a sentence X, the word P in that sentence, and two translations of the sentence X: A and B. Your task is to assess which translation contains the correct translation of the word P. \n\nRequirements: \n(1) Ignore other differences between the two translations. Only compare the translation of the word P. \n(2) Your answer should first state the reason for your comparison, and then give your comparison. \n(3) Your comparison should be A, B, C and D. \n- A: the first translation of the word P is better. \n- B: the second translation of the word P is better. \n- C: Both are fine. \n- D: Both are bad. \n\nNow, please process the following inputs:"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T1.2.2.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.3.1\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S4.T1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1\">Chinese</span>  <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.2\">English</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S4.T1.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.2.1\">English</span>  <span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.2.2.2.2\">German</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.5.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"4\" id=\"S4.T1.4.5.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.4.5.1.1.1\">Teacher Model: Baichuan2 13B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"4\" id=\"S4.T1.4.5.1.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.4.5.1.2.1\">Teacher Model: Llama2 13B</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.4\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.4.4.3\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.1\">||</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.4.4\">COMET</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.4.5\">BLEURT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.4.6\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.4.2\">||</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.4.7\">COMET</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.4.8\">BLEURT</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.4.9\">BLEU</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.6.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T1.4.6.2.1\">Teacher</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.6.2.2\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.6.2.3\">80.5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.6.2.4\">67.8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.6.2.5\">23.9</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.6.2.6\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.6.2.7\">81.4</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.6.2.8\">72.9</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.4.6.2.9\">26.0</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.7.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\" id=\"S4.T1.4.7.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.4.7.3.1.1\">Student Model: ParroT-7B</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.4.8.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.8.1.1\">Student</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.8.1.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.8.1.3\">75.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.8.1.4\">60.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.8.1.5\">18.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.8.1.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.8.1.7\">80.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.8.1.8\">69.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.8.1.9\">23.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.9.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.9.2.1\">SeqKD-Equal</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.9.2.2\">119k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.9.2.3\">76.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.9.2.4\">61.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.9.2.5\">21.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.9.2.6\">107k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.9.2.7\">80.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.9.2.8\">70.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.9.2.9\">24.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.10.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.10.3.1\">SeqKD-Full</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.10.3.2\">1M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.10.3.3\">76.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.10.3.4\">61.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.10.3.5\">22.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.10.3.6\">1M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.10.3.7\">80.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.10.3.8\">71.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.10.3.9\">24.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.11.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.11.4.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.4.11.4.1.1\">\\hdashline</span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T1.4.11.4.1.2\">MT-Patcher</span>\n</th>\n<td class=\"ltx_td\" id=\"S4.T1.4.11.4.2\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.11.4.3\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.11.4.4\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.11.4.5\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.11.4.6\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.11.4.7\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.11.4.8\"></td>\n<td class=\"ltx_td ltx_nopad_r\" id=\"S4.T1.4.11.4.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.12.5\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" id=\"S4.T1.4.12.5.1\">+ PE</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.12.5.2\">119k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.12.5.3\">76.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.12.5.4\">61.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.12.5.5\">22.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.12.5.6\">107k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.12.5.7\">80.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.12.5.8\">71.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.12.5.9\">24.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.13.6\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" id=\"S4.T1.4.13.6.1\">+ PE + PDS</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.13.6.2\">595k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.13.6.3\">77.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.13.6.4\">62.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.13.6.5\">23.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.13.6.6\">535k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.13.6.7\">81.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.13.6.8\">72.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.13.6.9\">25.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.14.7\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" id=\"S4.T1.4.14.7.1\">+ PE + PDS + WA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.14.7.2\">1.07M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.14.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.14.7.3.1\">78.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.14.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.14.7.4.1\">63.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.14.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.14.7.5.1\">23.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.14.7.6\">963k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.14.7.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.14.7.7.1\">81.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.14.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.14.7.8.1\">72.6</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.14.7.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.14.7.9.1\">26.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.15.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\" id=\"S4.T1.4.15.8.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.4.15.8.1.1\">Student Model: NLLB 3.3B</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.16.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.16.9.1\">Student</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.16.9.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.16.9.3\">76.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.16.9.4\">63.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.16.9.5\">20.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.16.9.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.16.9.7\">86.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.16.9.8\">76.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.16.9.9\">34.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.17.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.17.10.1\">SeqKD-Equal</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.17.10.2\">104k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.17.10.3\">79.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.17.10.4\">66.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.17.10.5\">25.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.17.10.6\">124k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.17.10.7\">85.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.17.10.8\">74.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.17.10.9\">32.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.18.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.18.11.1\">SeqKD-Full</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.18.11.2\">1M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.18.11.3\">79.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.18.11.4\">66.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.18.11.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.18.11.5.1\">25.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.18.11.6\">1M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.18.11.7\">84.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.18.11.8\">74.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.18.11.9\">31.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.19.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.19.12.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T1.4.19.12.1.1\">\\hdashline</span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T1.4.19.12.1.2\">MT-Patcher</span>\n</th>\n<td class=\"ltx_td\" id=\"S4.T1.4.19.12.2\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.19.12.3\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.19.12.4\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.19.12.5\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.19.12.6\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.19.12.7\"></td>\n<td class=\"ltx_td\" id=\"S4.T1.4.19.12.8\"></td>\n<td class=\"ltx_td ltx_nopad_r\" id=\"S4.T1.4.19.12.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.20.13\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" id=\"S4.T1.4.20.13.1\">+ PE</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.20.13.2\">104k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.20.13.3\">79.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.20.13.4\">67.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.20.13.5\">24.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.20.13.6\">87k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.20.13.7\">86.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.20.13.8\">76.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.20.13.9\">34.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.21.14\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" id=\"S4.T1.4.21.14.1\">+ PE + PDS</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.21.14.2\">520k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.21.14.3\">79.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.21.14.4\">67.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.21.14.5\">24.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.21.14.6\">435k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.21.14.7\">86.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.21.14.8\">77.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.4.21.14.9\">34.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.22.15\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T1.4.22.15.1\">+ PE + PDS + WA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.4.22.15.2\">936k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.4.22.15.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.22.15.3.1\">80.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.4.22.15.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.22.15.4.1\">68.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.4.22.15.5\">25.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.4.22.15.6\">783k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.4.22.15.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.22.15.7.1\">87.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.4.22.15.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.22.15.8.1\">77.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S4.T1.4.22.15.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.22.15.9.1\">35.6</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Translation performance of the proposed method and other baselines on the WMT22 ChineseEnglish and EnglishGerman test sets.  denotes the number of examples used to finetune the student model. SeqKD-Full refers to the student model finetunes on the full 1M pseudo parallel sentences, while SeqKD-Equal finetunes on random subsets of the teacher\u2019s translations with equal size to that of <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T1.12.1\">MT-Patcher</span>.</figcaption>\n</figure>",
            "capture": "Table 1: Translation performance of the proposed method and other baselines on the WMT22 ChineseEnglish and EnglishGerman test sets.  denotes the number of examples used to finetune the student model. SeqKD-Full refers to the student model finetunes on the full 1M pseudo parallel sentences, while SeqKD-Equal finetunes on random subsets of the teacher\u2019s translations with equal size to that of MT-Patcher."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T2.1.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" id=\"S4.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.2.1\">Chemistry Materials</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" id=\"S4.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.3.1\">Chinese Idioms</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.1.2.2.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.2.2.2\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.1.2.2.2.1\">Unseen Context</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.2.2.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.1.2.2.3.1\">Unseen Word</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.2.2.4\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.1.2.2.4.1\">Unseen Context</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S4.T2.1.2.2.5\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T2.1.2.2.5.1\">Unseen Word</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3.3\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S4.T2.1.3.3.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3.2\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3.3\">Rel. Perf.</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3.4\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3.5\">Rel. Perf.</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3.6\">Score</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3.7\">Rel. Perf.</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3.8\">Score</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.3.3.9\">Rel. Perf.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.1.4.4.1\">Student</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4.2\">6.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4.3\">22.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4.4\">6.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4.5\">23.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4.6\">1.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4.7\">39.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4.8\">1.16</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4.9\">37.4%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.1.5.5.1\">Teacher</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.5.2\">26.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.5.3\">97.4%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.5.4\">25.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.5.5\">97.4%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.5.6\">2.78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.5.7\">92.3%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.5.8\">2.82</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.5.5.9\">91.0%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.1.6.6.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T2.1.6.6.1.1\">\\hdashline</span>Feedbacker</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.6.2\">26.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.6.3\">100%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.6.4\">26.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.6.5\">100%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.6.6\">3.01</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.6.7\">100%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.6.8\">3.10</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.6.6.9\">100%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.1.7.7.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S4.T2.1.7.7.1.1\">\\hdashline</span>SeqKD-Full</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.7.2\">15.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.7.3\">58.1%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.7.4\">10.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.7.5\">40.0%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.7.6\">1.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.7.7\">54.8%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.7.8\">1.62</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.7.7.9\">52.3%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.1.8.8.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T2.1.8.8.1.1\">MT-Patcher</span></th>\n<td class=\"ltx_td\" id=\"S4.T2.1.8.8.2\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.1.8.8.3\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.1.8.8.4\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.1.8.8.5\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.1.8.8.6\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.1.8.8.7\"></td>\n<td class=\"ltx_td\" id=\"S4.T2.1.8.8.8\"></td>\n<td class=\"ltx_td ltx_nopad_r\" id=\"S4.T2.1.8.8.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.9.9\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" id=\"S4.T2.1.9.9.1\">+ PE</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.9.2\">15.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.9.3\">59.2%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.9.4\">11.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.9.5\">41.5%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.9.6\">1.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.9.7\">57.5%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.9.8\">1.78</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.9.9.9\">57.4%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.10.10\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" id=\"S4.T2.1.10.10.1\">+ PE + PDS</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.10.2\">21.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.10.3\">80.5%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.10.4\">11.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.10.5\">42.3%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.10.6\">2.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.10.7\">67.8%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.10.8\">1.81</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.1.10.10.9\">58.4%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.11.11\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T2.1.11.11.1\">+ PE + PDS + WA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.11.2\">21.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.11.3\">82.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.11.4\">16.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.11.5\">61.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.11.6\">2.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.11.7\">69.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.11.8\">2.02</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.11.9\">65.2%</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Performance of different models when translating chemistry materials (evaluated in accuracy) and Chinese Idioms (evaluated by scores given by GPT-4). Rel. Perf: the relative performances of models compared to feedbacker, which is the best extent we can elicit knowledge from LLMs in this table.</figcaption>\n</figure>",
            "capture": "Table 2: Performance of different models when translating chemistry materials (evaluated in accuracy) and Chinese Idioms (evaluated by scores given by GPT-4). Rel. Perf: the relative performances of models compared to feedbacker, which is the best extent we can elicit knowledge from LLMs in this table."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T3.3.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.3.1.1.2\">BLEU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.3.1.1.3\">COMET</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.3.1.1.4\">BLEURT</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.3.2.1.1\">Student</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.2.1.2\">15.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.2.1.3\">85.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.2.1.4\">58.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.3.3.2.1\">SeqKD</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.2.2\">16.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.2.3\">85.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.3.2.4\">61.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T3.3.4.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.3.4.3.1.1\">MT-Patcher</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.3.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.4.3.2.1\">16.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.3.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.4.3.3.1\">86.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.3.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.4.3.4.1\">62.2</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Effectiveness of <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.5.1\">MT-Patcher</span>\u00a0on WMT English  Japanese translation test sets. The student model is NLLB 3.3B.</figcaption>\n</figure>",
            "capture": "Table 3: Effectiveness of MT-Patcher\u00a0on WMT English  Japanese translation test sets. The student model is NLLB 3.3B."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.8.9.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T4.8.9.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.8.9.1.2\">COMET</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.8.9.1.3\">BLEURT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.8.9.1.4\">BLEU</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.2\">79.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.3\">67.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.4\">24.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.2.2.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.2\">79.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.3\">67.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.4\">24.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.3.3.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.3.3.2\">80.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.3.3.3.1\">67.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.3.3.4\">24.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.4.4.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.4.2.1\">80.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.4.3.1\">67.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.4.4.1\">25.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.5.5.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.5.2.1\">80.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.5.3\">67.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.5.4\">25.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.6.6.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.6.6.2\">80.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.6.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.6.6.3.1\">67.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.6.6.4\">24.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.7.7.1\"></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.7.7.2\">79.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.7.7.3\">67.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.7.7.4\">24.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T4.8.8.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.8.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.8.8.2.1\">80.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.8.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.8.8.3.1\">67.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.8.8.4\">24.9</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Translation performance of NLLB-3B model finetuned on post-editing data after  epochs of iterative feedback.</figcaption>\n</figure>",
            "capture": "Table 4: Translation performance of NLLB-3B model finetuned on post-editing data after  epochs of iterative feedback."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T5\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.6\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T5.6.7.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T5.6.7.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T5.6.7.1.2\">NLLB</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T5.6.7.1.3\">ParroT</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.4.4\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S5.T5.4.4.5\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T5.1.1.1.1\">ZHEN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.2.2.2\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T5.2.2.2.1\">ENDE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.3.3.3\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T5.3.3.3.1\">ZHEN</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.4\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T5.4.4.4.1\">ENDE</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.6.8.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T5.6.8.1.1\">Student</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.6.8.1.2\">76.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.6.8.1.3\">86.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.6.8.1.4\">75.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.6.8.1.5\">80.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.6.9.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T5.6.9.2.1\">SeqKD-Full</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.6.9.2.2\">79.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.6.9.2.3\">84.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.6.9.2.4\">76.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.6.9.2.5\">80.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T5.5.5.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T5.5.5.1.1\">\\hdashline</span>NLLB<sup class=\"ltx_sup\" id=\"S5.T5.5.5.1.2\">\u2020</sup>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.5.5.2\">80.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.5.5.3\">87.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.5.5.4\">77.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.5.5.5\">81.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T5.6.6.1\">ParroT<sup class=\"ltx_sup\" id=\"S5.T5.6.6.1.1\">\u2020</sup>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.6.6.2\">79.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.6.6.3\">86.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.6.6.4\">78.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T5.6.6.5\">81.8</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Translation performances when applying <span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T5.11.1\">MT-Patcher</span>\u00a0trained on one student model to another. Performances are evaluated by COMET score. Models with  are <span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T5.12.2\">MT-Patcher</span>\u00a0(+ PE + PDS + WA) trained for the corresponding MT model. For reference, we also list the performances of the original student model and SeqKD-Full baselines.</figcaption>\n</figure>",
            "capture": "Table 5: Translation performances when applying MT-Patcher\u00a0trained on one student model to another. Performances are evaluated by COMET score. Models with  are MT-Patcher\u00a0(+ PE + PDS + WA) trained for the corresponding MT model. For reference, we also list the performances of the original student model and SeqKD-Full baselines."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T6.5\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T6.5.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" id=\"A1.T6.5.1.1.1\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T6.5.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.5.1.1.2.1\">COMET</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T6.5.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.5.1.1.3.1\">BLEURT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T6.5.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T6.5.1.1.4.1\">BLEU</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T6.5.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T6.5.2.1.1\">Student</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T6.5.2.1.2\">82.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T6.5.2.1.3\">70.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T6.5.2.1.4\">26.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.5.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T6.5.3.2.1\">SeqKD-Full</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T6.5.3.2.2\">75.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T6.5.3.2.3\">62.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T6.5.3.2.4\">22.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T6.5.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A1.T6.5.4.3.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T6.5.4.3.1.1\">MT-Patcher</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T6.5.4.3.2\">81.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T6.5.4.3.3\">69.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T6.5.4.3.4\">26.3</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Translation performance on WMT22 German  English test set. SeqKD-Full and <span class=\"ltx_text ltx_font_smallcaps\" id=\"A1.T6.7.1\">MT-Patcher</span>\u00a0are finetuned student models on pseudo Chinese  English parallel sentences.</figcaption>\n</figure>",
            "capture": "Table 6: Translation performance on WMT22 German  English test set. SeqKD-Full and MT-Patcher\u00a0are finetuned student models on pseudo Chinese  English parallel sentences."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"A5.T7\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Prompt that we use for the feedbacker task.</figcaption>\n</figure>",
            "capture": "Table 7: Prompt that we use for the feedbacker task."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"A5.T8\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Prompt that we use for the sentence analysis task.</figcaption>\n</figure>",
            "capture": "Table 8: Prompt that we use for the sentence analysis task."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"A5.T9\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Prompt that we use for the parallel data synthesizer task.</figcaption>\n</figure>",
            "capture": "Table 9: Prompt that we use for the parallel data synthesizer task."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"A5.T10\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Prompt that we use for the word analogy task.</figcaption>\n</figure>",
            "capture": "Table 10: Prompt that we use for the word analogy task."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"A5.T11\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>Prompt that we use for evaluating the quality of translating Chinese idioms.</figcaption>\n</figure>",
            "capture": "Table 11: Prompt that we use for evaluating the quality of translating Chinese idioms."
        },
        "12": {
            "table_html": "<figure class=\"ltx_table ltx_figure_panel\" id=\"A5.T12\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>Prompt that we use for comparing translations from direction generation and feedback.</figcaption>\n</figure>",
            "capture": "Table 12: Prompt that we use for comparing translations from direction generation and feedback."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.09522v2_figure_1.png",
            "caption": "Figure 1: The illustration of MT-Patcher framework. The correct translation for the source sentence should be \u2018Methanol is a colorless transparent liquid.\u2019."
        },
        "2": {
            "figure_path": "2403.09522v2_figure_2.png",
            "caption": "Figure 2: Translation performance as the number of synthesized contexts per word and analogous word grows."
        },
        "3": {
            "figure_path": "2403.09522v2_figure_3.png",
            "caption": "Figure 3: Comparison of translation quality on error words between the Teacher\u2019s translation and the feedbacker\u2019s correction."
        },
        "4": {
            "figure_path": "2403.09522v2_figure_4.png",
            "caption": "Figure 4: Accuracy of corrections and percentage of remaining data after applying different epochs of iterative feedback."
        },
        "5": {
            "figure_path": "2403.09522v2_figure_5.png",
            "caption": "Figure 5: Illustration of variants of MT-Patcher. PDS denotes the parallel data synthesizer, and WA denotes the word analoger."
        },
        "6": {
            "figure_path": "2403.09522v2_figure_6.png",
            "caption": "Figure 6: Illustration of the process how the monolingual set and two test sets are splitted from initial collected word sets."
        }
    },
    "references": [
        {
            "1": {
                "title": "In-context examples selection for machine translation.",
                "author": "Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 8857\u20138873, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.564"
            }
        },
        {
            "2": {
                "title": "Baichuan 2: Open Large-scale Language Models.",
                "author": "Baichuan Inc. 2023.",
                "venue": null,
                "url": "https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf"
            }
        },
        {
            "3": {
                "title": "Language models are few-shot learners.",
                "author": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.",
                "venue": "CoRR, 2005.14165.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Teaching large language models to self-debug.",
                "author": "Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. 2024.",
                "venue": "In The Twelfth International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=KuPixIqPiq"
            }
        },
        {
            "5": {
                "title": "Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions.",
                "author": "John Chung, Ece Kamar, and Saleema Amershi. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 575\u2013593, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.34"
            }
        },
        {
            "6": {
                "title": "More than minutes: Teachers\u2019 roles in designing homework.",
                "author": "Joyce L. Epstein and Frances L. Van Voorhis. 2001.",
                "venue": "Educational Psychologist, 36(3):181\u2013193.",
                "url": "https://doi.org/10.1207/S15326985EP3603_4"
            }
        },
        {
            "7": {
                "title": "APE at scale and its implications on MT evaluation biases.",
                "author": "Markus Freitag, Isaac Caswell, and Scott Roy. 2019.",
                "venue": "In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 34\u201344, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W19-5204"
            }
        },
        {
            "8": {
                "title": "Specializing smaller language models towards multi-step reasoning.",
                "author": "Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023.",
                "venue": "In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10421\u201310430. PMLR.",
                "url": "https://proceedings.mlr.press/v202/fu23d.html"
            }
        },
        {
            "9": {
                "title": "Non-autoregressive neural machine translation.",
                "author": "Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. 2018.",
                "venue": "In International Conference on Learning Representations.",
                "url": "https://openreview.net/forum?id=B1l8BtlCb"
            }
        },
        {
            "10": {
                "title": "How good are GPT models at machine translation? a comprehensive evaluation.",
                "author": "Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023.",
                "venue": "CoRR, cs.CL/2302.09210v1.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Distilling the knowledge in a neural network.",
                "author": "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.",
                "venue": "ArXiv, abs/1503.02531.",
                "url": "https://api.semanticscholar.org/CorpusID:7200347"
            }
        },
        {
            "12": {
                "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.",
                "author": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 8003\u20138017, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-acl.507"
            }
        },
        {
            "13": {
                "title": "ParroT: Translating during chat using large language models tuned with human translation and feedback.",
                "author": "Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023a.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15009\u201315020, Singapore. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.findings-emnlp.1001"
            }
        },
        {
            "14": {
                "title": "Is ChatGPT a good translator? Yes with GPT-4 as the engine.",
                "author": "Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023b.",
                "venue": "CoRR, cs.CL/2301.08745v3.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Scaling laws for neural language models.",
                "author": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.",
                "venue": "CoRR, cs.LG/2001.08361v1.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Sequence-level knowledge distillation.",
                "author": "Yoon Kim and Alexander M. Rush. 2016.",
                "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327, Austin, Texas. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/D16-1139"
            }
        },
        {
            "17": {
                "title": "Homework assignments: Classroom games or teaching tools?",
                "author": "Jackson F Lee Jr and K Wayne Pruitt. 1979.",
                "venue": "The Clearing House, 53(1):31\u201335.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Eliciting the translation ability of large language models via multilingual finetuning with translation instructions.",
                "author": "Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen. 2023.",
                "venue": "CoRR, cs.CL/2305.15083.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Few-shot learning with multilingual generative language models.",
                "author": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019\u20139052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.emnlp-main.616"
            }
        },
        {
            "20": {
                "title": "Selective knowledge distillation for non-autoregressive neural machine translation.",
                "author": "Min Liu, Yu Bao, Chengqi Zhao, and Shujian Huang. 2023.",
                "venue": "In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI\u201923/IAAI\u201923/EAAI\u201923. AAAI Press.",
                "url": "https://doi.org/10.1609/aaai.v37i11.26555"
            }
        },
        {
            "21": {
                "title": "Self-refine: Iterative refinement with self-feedback.",
                "author": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023.",
                "venue": "In Thirty-seventh Conference on Neural Information Processing Systems.",
                "url": "https://openreview.net/forum?id=S37hOerQLB"
            }
        },
        {
            "22": {
                "title": "No language left behind: Scaling human-centered machine translation.",
                "author": "NLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.",
                "venue": null,
                "url": "http://arxiv.org/abs/2207.04672"
            }
        },
        {
            "23": {
                "title": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only.",
                "author": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.",
                "venue": "arXiv preprint arXiv:2306.01116.",
                "url": "http://arxiv.org/abs/2306.01116"
            }
        },
        {
            "24": {
                "title": "A call for clarity in reporting BLEU scores.",
                "author": "Matt Post. 2018.",
                "venue": "In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013191, Brussels, Belgium. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/W18-6319"
            }
        },
        {
            "25": {
                "title": "COMET: A neural framework for MT evaluation.",
                "author": "Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.emnlp-main.213"
            }
        },
        {
            "26": {
                "title": "Translationese as a language in \u201cmultilingual\u201d NMT.",
                "author": "Parker Riley, Isaac Caswell, Markus Freitag, and David Grangier. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7737\u20137746, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.691"
            }
        },
        {
            "27": {
                "title": "Data augmentation for intent classification with off-the-shelf large language models.",
                "author": "Gaurav Sahu, Pau Rodriguez, Issam Laradji, Parmida Atighehchian, David Vazquez, and Dzmitry Bahdanau. 2022.",
                "venue": "In Proceedings of the 4th Workshop on NLP for Conversational AI, pages 47\u201357, Dublin, Ireland. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2022.nlp4convai-1.5"
            }
        },
        {
            "28": {
                "title": "BLEURT: Learning robust metrics for text generation.",
                "author": "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2020.acl-main.704"
            }
        },
        {
            "29": {
                "title": "Multilingual neural machine translation with knowledge distillation.",
                "author": "Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu. 2018.",
                "venue": "In International Conference on Learning Representations.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Stanford alpaca: An instruction-following llama model.",
                "author": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.",
                "venue": "https://github.com/tatsu-lab/stanford_alpaca.",
                "url": "http://arxiv.org/abs/GitHub"
            }
        },
        {
            "31": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2307.09288"
            }
        },
        {
            "32": {
                "title": "Prompting palm for translation: Assessing strategies and performance.",
                "author": "David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022.",
                "venue": "CoRR, 2211.09102.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Selective knowledge distillation for neural machine translation.",
                "author": "Fusheng Wang, Jianhao Yan, Fandong Meng, and Jie Zhou. 2021.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6456\u20136466, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.acl-long.504"
            }
        },
        {
            "34": {
                "title": "GPT3Mix: Leveraging large-scale language models for text augmentation.",
                "author": "Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyoung Park. 2021.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2225\u20132239, Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.findings-emnlp.192"
            }
        },
        {
            "35": {
                "title": "Large language model as attributed training data generator: A tale of diversity and bias.",
                "author": "Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2306.15895"
            }
        },
        {
            "36": {
                "title": "Synthbio: A case study in faster curation of text datasets.",
                "author": "Ann Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris Callison-Burch, Andy Coenen, and Sebastian Gehrmann. 2021a.",
                "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).",
                "url": "https://openreview.net/forum?id=Fkpr2RYDvI1"
            }
        },
        {
            "37": {
                "title": "WuDaoCorpora: A super large-scale chinese corpora for pre-training language models.",
                "author": "Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021b.",
                "venue": "AI Open, 2.",
                "url": "https://doi.org/10.1016/j.aiopen.2021.06.001"
            }
        },
        {
            "38": {
                "title": "A survey of large language models.",
                "author": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.18223"
            }
        },
        {
            "39": {
                "title": "Understanding knowledge distillation in non-autoregressive machine translation.",
                "author": "Chunting Zhou, Jiatao Gu, and Graham Neubig. 2019.",
                "venue": "In International Conference on Learning Representations.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Multilingual machine translation with large language models: Empirical results and analysis.",
                "author": "Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023.",
                "venue": "CoRR, cs.CL/2304.04675v2.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.09522v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.09522v2",
        "paper_title": "MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation",
        "research_background": "### Motivation\nThe motivation behind this paper stems from the need to improve machine translation (MT) performance in medium-sized language models, which are typically more affordable to deploy but underperform compared to larger language models (LLMs). The authors highlight the disparity in translation capabilities between large and medium-sized LLMs, emphasizing the necessity of efficient knowledge transfer methods to bridge this gap. Traditional knowledge distillation (KD) techniques, although effective, do not account for the existing proficiencies of MT models, resulting in redundancy and inefficiencies. Moreover, their reliance on monolingual corpora limits their performance when dealing with novel contexts or unseen words. The motivation is thus to develop a more efficient, selective, and effective knowledge transfer framework that leverages the strengths of LLMs while addressing these inefficiencies.\n\n### Research Problem\nThe research problem addressed in this paper is how to efficiently transfer knowledge from larger, more capable LLMs to medium-sized, existing MT models in such a way that the process is both cost-effective and performance-enhancing. Specifically, it focuses on developing a method that selectively fine-tunes student MT models by identifying and correcting specific areas of weakness, predicting potential errors, and synthesizing diverse contexts to cover translation knowledge comprehensively. This involves moving beyond the traditional KD methods that do not take the initial proficiency of student models into account, which leads to unnecessary computational loads and suboptimal knowledge transfer.\n\n### Relevant Prior Work\nThe paper builds on a variety of prior research:\n\n1. **Large Language Models in MT**: Brown et al. (2020) and Zhao et al. (2023) demonstrated the power of large LLMs in natural language tasks. However, smaller models like Alpaca (Taori et al., 2023) and ParroT (Jiao et al., 2023a) do not perform as well in MT, lagging behind supervised NMT systems (Jiao et al., 2023a; Zhu et al., 2023).\n\n2. **Knowledge Distillation (KD)**: Traditional KD methods involve training a student model to mimic a teacher model\u2019s outputs on an unlabeled corpus. This approach was pioneered by Hinton et al. (2015), with subsequent improvements by Kim and Rush (2016), Wang et al. (2021), and Liu et al. (2023). However, these methods typically do not consider the existing proficiencies of student models, leading to inefficiencies.\n\n3. **Proficient Student Models**: Recent studies (Hsieh et al., 2023; Fu et al., 2023) have shown that student MT models often possess a reasonable level of language proficiency from the start, suggesting that blanket fine-tuning on all teacher outputs is redundant.\n\n4. **Efficacy and Coverage Limitations**: Traditional KD is constrained by the coverage limitation of monolingual corpora, affecting performance when dealing with novel contexts or unseen words. Modern LLMs, on the other hand, have strong translation capabilities, suggesting the potential for more effective knowledge transfer strategies.\n\nBy referencing these works, the paper underscores the inefficiencies and constraints of current KD approaches and sets the stage for their proposed method, MT-Patcher, which aims to address these gaps by providing a more targeted and comprehensive transfer of knowledge.",
        "methodology": "**MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation**\n\n**Methodology:** \n\nIn this section, we introduce MT-Patcher, a framework designed to distill knowledge from Large Language Models (LLMs) into existing Machine Translation (MT) systems more efficiently and effectively. This framework operates through two primary stages:\n\n### 1. Knowledge Selection\n\nIn the first stage, the LLM functions as a feedbacker, providing natural language feedback on the translations generated by the student models. Based on this feedback, source sentences with identified errors\u2014indicating the knowledge deficiencies of the student models\u2014are selected for progression to the next stage.\n\n### 2. Knowledge Extension\n\nIn the second stage, the LLM serves dual roles: as a parallel data synthesizer and a word analoger. This stage aims to help the student model learn words it frequently mistranslates by presenting these words in more diverse contexts and in relation to similar words.\n\n- **Parallel Data Synthesizer:** This component's objective is to generate parallel sentences containing specific word pairs where the student model demonstrates errors. These synthesized sentences aim to generalize the existing translation knowledge to broader contexts. Ideally, these parallel sentences should maintain semantic diversity while staying similar to the original context. Preliminary experiments, however, reveal that even LLMs like GPT-4 tend to generate parallel data that lack diversity and are overly representative of the original context.\n\n  To resolve this issue, we introduce the **Sentence Analyzer** module. The sentence analyzer extracts information pertaining to the domain, topic, and style of the original context. The LLMs are then instructed to generate parallel sentences that retain these attributes while incorporating the specific phrase pairs. This method functions as an information bottleneck, condensing the semantic content while preserving other attributes.\n\n- **Word Analoger:** The word analoger proactively predicts potential mistranslations by the student model. For instance, if \"methanol\" is mistranslated, it is likely that the student model may also struggle with other chemistry-related terms such as \"benzene\" and \"ethanol.\" By anticipating such errors, we can bolster the student model\u2019s translation accuracy for uncommon terms not found in standard monolingual corpora.\n\n  Practically, given a source sentence containing a word that the student MT model misinterprets, the word analoger identifies additional words from two perspectives:\n  \n  1. **Category:** Words belonging to the same category as the mistranslated word.\n  2. **Semantic:** Words that frequently co-occur with the mistranslated word.\n\nMoreover, the generated words are intentionally rare and challenging to ensure that the student model struggles to accurately translate them, thereby improving its robustness.\n\n### Model Innovations:\n\n- **Selective Feedback Loop:** The selective feedback loop ensures that only sentences with identified errors proceed to the next stage, focusing the learning process effectively.\n- **Information Bottleneck:** The sentence analyzer creates a bottleneck that retains domain-specific attributes while condensing semantic information, aiding in the generation of more diverse yet contextually relevant parallel sentences.\n- **Proactive Error Anticipation:** By identifying and incorporating potentially challenging terms through the word analoger, the model anticipates and remedies specific categories of translation errors, enhancing overall translation quality.",
        "main_experiment_and_results": "**Main Experiment Setup:**\n\n1. **Datasets:**\n   - Language pairs: Chinese-English and English-German translation tasks.\n   - Models fine-tuned using GPT-4's translations on monolingual sentences.\n\n2. **Student Translation Models:**\n   - NLLB-200 3.3B: A multilingual translation model pre-trained on 200 languages.\n   - ParroT: An LLM-based MT model finetuned on WMT validation sets from LLaMA-7B.\n\n3. **Teacher Models:**\n   - LLaMA2-13B: Used for building MT-Patcher for English-German translation.\n   - Baichuan-2-13B: Used for building MT-Patcher for Chinese-English translation, demonstrating stronger abilities in Chinese.\n\n4. **Evaluation Baselines and Methods:**\n   - **Student:** The original translation model (NLLB-200 3.3B or ParroT).\n   - **Teacher:** LLM fine-tuned directly for translation.\n   - **SeqKD:** Models trained using Sequence-level Knowledge Distillation.\n   - **MT-Patcher (PE):** Finetuning the student model on post-editing results.\n\n5. **Implementation Details:**\n   - LLMs are fully fine-tuned for 3 epochs for each language pair.\n\n**Evaluation Metrics:**\n- Main evaluation carried out through the translation performance comparison of the aforementioned methods.\n\n**Main Experimental Results:**\n\n1. **Overall Performance:**\n   - **MT-Patcher (PE):** Shows better performance than SeqKD-Equal and is comparable to SeqKD-Full, indicating the method's effectiveness in selecting valuable examples and discarding less useful ones.\n\n2. **English-German Translation:**\n   - Teacher model (LLaMA-2-13B) performs worse than the student (NLLB 3.3B), consistent with findings that existing LLMs struggle to outperform well-supervised translation models.\n   - Despite poor performance with SeqKD, MT-Patcher improves the student model's performance, suggesting that revising an initial draft is more effective for leveraging LLM knowledge.\n\n3. **Medium Resource Translation (WMT22 English-Japanese):**\n   - MT-Patcher outperforms SeqKD, indicating its robustness across different resource settings.\n\nIn summary, MT-Patcher presents substantial improvements over SeqKD in multiple settings by leveraging extended contexts and error anticipation mechanisms, proving to be an effective method for enhancing student translation models with selective and extendable knowledge distillation."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of MT-Patcher in improving the general machine translation capability of student models through selective knowledge transfer from LLMs.",
            "experiment_process": "The evaluation involved Chinese-English and English-German translations using student models: NLLB-200 3.3B and ParroT. The backbone LLMs used for MT-Patcher were LLaMA2-13B (for English-German) and Baichuan-2-13B (for Chinese-English). The methods compared included: the Student model, the Teacher model, SeqKD, MT-Patcher (PE), MT-Patcher (PE + PDS), and MT-Patcher (PE + PDS + WA). The LLMs were finetuned for 3 epochs on collected data. Performance was measured using COMET, BLEURT, and sacreBLEU metrics.",
            "result_discussion": "MT-Patcher (PE) outperformed SeqKD-Equal and was comparable to SeqKD-Full, indicating effective selection and discarding of examples. The application of synthesizers (PDS) and word analogers (WA) further improved translation performance by extending context and knowledge coverage. Revising drafts (MT-Patcher) was more effective than direct generation (SeqKD), even when the teacher model underperformed the student model.",
            "ablation_id": "2403.09522v2.No1"
        },
        {
            "research_objective": "To assess MT-Patcher's ability to improve the translation of specific language phenomena, particularly chemistry materials and Chinese idioms, emphasizing long-tailed knowledge and varying levels of abstraction and context dependency.",
            "experiment_process": "For Chinese-to-English translation, experiments focused on chemistry materials and Chinese idioms. Data included 6,000 word pairs split into Seen and Unseen categories. Three sets were created: Monolingual Set (source sentences synthesized by GPT-4), Test Set for Unseen Context (parallel sentences for seen words), and Test Set for Unseen Word (parallel sentences for unseen words). Baichuan-2-13B and NLLB 3.3B were used as teacher and student models, respectively. Performance metrics included translation accuracy for chemistry materials and quality scores (0-5) given by GPT-4 for Chinese idioms.",
            "result_discussion": "Teacher models achieved significantly better performance than student models, but SeqKD-Full only narrowed the performance gap by less than half. MT-Patcher (+PE + PDG) improved relative performance significantly for both chemistry materials and Chinese idioms by incorporating more contextual examples. Neither SeqKD-Full nor MT-Patcher (+PE + PDG) performed well on unseen words, yet generating analogous words notably enhanced performance on this test set, showcasing the effectiveness of error anticipation.",
            "ablation_id": "2403.09522v2.No2"
        }
    ]
}