{
    "title": "X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment",
    "abstract": "The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, large multimodal models (LMMs) have evolved to respond in alignment with human intent through visual instruction-following (VIF). In LLaVA1.0, a method was proposed to automatically construct a VIF dataset using GPT4, which demonstrated excellent performance in visual question answering (VQA). However, there are two main limitations to the data generated in LLaVA1.0: first, it was constructed using a text-only version of GPT4, which does not accept images as input; and second, it targeted only English.\n\nSubsequently, LLaVA1.5 incorporated the multilingual instruction dataset ShareGPT, demonstrating its potential in multilingual processing. However, ShareGPT uses an instruction following (IF) dataset for LLMs, still suffers from a lack of vision information. To address this issue, ShareGPT4V, a VIF dataset created using GPT4-V, which accepts image information as input, was released. ShareGPT4V is also limited because it consists only of English question-answering, posing a constraint in aligning multiple languages to acquire multilingual information.\n\nIn this context, we propose constructing a multilingual VIF dataset based on object relational information and a multilingual LMM that efficiently utilizes this dataset. The proposed multilingual VIF dataset was composed of 23,496 question-and-answer pairs centered around objects, locations, atmospheres, and conversations to ensure the diversity of expressions. The target languages were selected considering linguistic diversity by choosing English, Chinese, and Korean, which belong to different language families.\n\nWe also propose the development of a multilingual LMM, X-LLaVA, utilizing the proposed data. X-LLaVA is a model that enhances LLaVA1.5, by applying the following three enhancement methods: (1) vocabulary expansion for target language, (2) pretraining for connecting knowledge across multiple languages, and (3) multilingual VIF. First, bilingual-based vocabulary expansion involves adding words to a pretrained language model to strengthen the relatively limited vocabulary of Korean compared to English. Second, additional pretraining was conducted to link the English and Korean knowledge. Third, we conducted multilingual training using the proposed VIF dataset.\n\nThe contributions of this study can be summarized as follows: We propose a training framework of multilingual LMM for enriching a specific language availability. We have constructed a multilingual VIF dataset based on different task-oriented types. Through an in-depth analysis, we demonstrate the real-world effectiveness of the multilingual approach employed in our dataset. Finally, we emphasize that the 91K datasets and models constructed in this study can be implemented with relatively small resources, costing approximately $3,200 and utilizing an A6000 GPU."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Vision-Language Models",
            "text": "With the advancement of LLMs, proposals have been made to extend LLMs to include additional modalities Zhang et al. (2023  ###reference_b36###). The primary idea was to focus on aligning information between vision and language Alayrac et al. (2022  ###reference_b2###). A prime example of this is CLIP Radford et al. (2021  ###reference_b31###) and ALBEF Li et al. (2021  ###reference_b21###), which integrated representations of images and text using contrastive learning Chen et al. (2020  ###reference_b6###); Lee et al. (2022  ###reference_b17###) to unify distinct types of information. Subsequent enhancements, as observed in BLIP Li et al. (2022  ###reference_b20###) and BLIP-2 Li et al. (2023b  ###reference_b19###), utilized assorted data and Q-Former\u2019s trainable query vectors to strengthen this alignment. Most recently, MiniGPT4 Zhu et al. (2023  ###reference_b38###) proposed a fine-tuning method to generate responses that are more aligned with the user intent, demonstrating the potential for conversational image-text models. Concurrently, InstructionBLIP Dai et al. (2023  ###reference_b9###), LLaVA1.0 Liu et al. (2023b  ###reference_b25###), and LLaVA1.5 Liu et al. (2023a  ###reference_b24###) have advanced our understanding of complex prompts through more sophisticated visual instruction finetuning (VIT) Liu et al. (2023b  ###reference_b25###)."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Visual Instruction Following Datasets",
            "text": "In LLMs, IF is used to ensure that the language model generates responses that align with user objectives. Recently, there has been a proposal for research to create a VIF dataset that includes image data in the IF. The construction of a VIF dataset is costly and time-consuming because it requires the simultaneous consideration of images, queries, and answers. Therefore, automatic generation methods are commonly used, with two primary approaches: one using GPT for data generation and the other using a template-based method that transforms existing data using predefined templates.\nTable 1  ###reference_### presents a comparison of the representative VIF datasets. The initial versions of the VIF dataset were constructed using template-based models. Multi-Instruct Li et al. (2023a  ###reference_b18###) and InstructBLIP, which fall under this category, are fast and cost-effective as they involve rule-based transformation of existing data. However, they have the limitation of being oriented towards specific tasks such as image captioning or classification.\nIn contrast to template-based construction, LLaVA introduces a more flexible generative data construction method that utilizes the GPT. Using object location and caption information from COCO Lin et al. (2014  ###reference_b23###), LLaVA constructed 158K diverse VIF datasets with three different styles: detailed description, complex reasoning, and conversational. However, because these datasets do not use images in their generation, SharedGPT4V Chen et al. (2023b  ###reference_b5###), and LVIS-INSTRUCT4V Wang et al. (2023  ###reference_b33###), which include images in their construction, were proposed. However, these datasets are predominantly written in a single language. To address the need for multilingual capabilities, the M3IT dataset was released  Li et al. (2023c  ###reference_b22###). M3IT is an instruction-tuning dataset comprising 40 tasks translated into 80 languages that offers broad accessibility."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Data Generation",
            "text": "###figure_1### In this study, we were inspired by the VIF data generation method using the GPT of LLaVA and have built upon it. However, to minimize the loss of information from the images and include more detailed information, we directly input the image and object information into the GPT4-V model to construct our data.\nWe constructed four types of multilingual VIF datasets (mvif) for three languages (English, Korean, and Chinese): (1) Object-centric, (2) Location-centric, (3) Atmosphere-centric, and (4) Conversation."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "The Focus of Data Building",
            "text": "The mvif data proposed in this research concentrate on the relational factual information between objects. This focus diverges from the description and reasoning-centered question-answering proposed by LLaVA, leading to minimal information redundancy between the two datasets. Although LLaVA\u2019s data are commendable, we assessed whether data designed for reasoning purposes might incorporate subjective viewpoints, thereby potentially introducing bias toward certain objects. Therefore, our study aims to develop a functional-relationship-based multilingual VIF dataset that, deliberately avoids overlap with LLaVA.\nThe target languages selected were English, Chinese, and Korean, each belonging to a distinct language family. This choice was intended to evaluate how multilingual training affects the languages of different cultures and character systems."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Image Selection Criteria",
            "text": "To construct the mvif dataset, 23,496 images from the visual Genome Krishna et al. (2017  ###reference_b16###) were used. A challenge was encountered when generating data using GPT4: if an image contained fewer than three major objects, the constrained context could limit the diversity of question answers. However, answering questions generated using images with over ten objects often results in a focus on objects that are either exceedingly small or insignificant. Consequently, we speculate that images selected from the visual Genome, where the number of main objects corresponds to ."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Proposed VIF Dataset",
            "text": "Figure 1  ###reference_### shows an example of the method used to construct the proposed mvif dataset. As illustrated, an image and a prompt, which are metadata for question generation, were fed into GPT4-V. Subsequently, GPT4-V was designed to generate questions and answers in three languages. For conversation data, we designed a prompt to produce eight pairs of dialogues for each image in a multi-turn format. For the dataset construction, we provided two seed examples to GPT4-V to guide the construction of data suitable for the purpose through in-context learning.\nA total of $3,200 was used to generate 91K data points. Detailed prompts used in data construction; the four types of generated data samples and inspection procedure can be found in the Appendix G.\n(1) Object-centric image description.\nObject-centric data focuses on providing detailed description of objects in an image, comprising questions and answers that include the shape, condition, and characteristics of the objects. The aim of constructing these data was to facilitate the learning of the intimate details of images by focusing on the specific attributes of the objects as they appear. Additionally, as shown in the \u201cMain objects\u201d section of Figure 1  ###reference_###, a list of main objects was inputted into the GPT4-V prompt to prevent errors in object specification that might occur during question generation.\n(2) Location-centric image description. Location-centric data is a type of question-answering data that focuses on describing the relative positions of objects within an image. However, when the same object appears multiple times in an image, this perspective can alter the location information. To address this effectively, we enabled GPT4-V to autonomously generate a relationship graph that served as the basis for answering the question. Consequently, when GPT4-V receives an image and a list of objects, it first generates a scene graph and then produces locational questions and answers regarding the image.\n(3) Atmosphere-centric image description.\nAtmosphere-centric data include descriptions that focus more on the overall ambiance of an image than on individual objects. It encompasses a holistic depiction of the complex interplay among multiple objects.\n(4) Conversational question and answering  Conversational data is structured as an 8-turn Q&A dataset to incorporate more in-depth and extensive information regarding the images. Unlike other datasets, this dataset is designed to infer human emotions or include subjective information about the mood of the image."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Proposed Multilingual Model",
            "text": "In this section, we introduce the proposed X-LLaVA model, an effective approach for multilingual processing through multilingual VIT Liu et al. (2023b  ###reference_b25###). X-LLaVA applies the following three enhancement methods to the same model structure as LLaVA1.5: (1) vocabulary expansion for the target language, (2) pretraining for multilingual knowledge association, and (3) multilingual VIT. Figure 2  ###reference_### demonstrates the three proposed methods and the structure of LLaVA1.5.\n###figure_2###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Recap of LLaVA1.5",
            "text": "Figure 2  ###reference_### (a) shows the basic structure of the LLaVA1.5 model. LLaVA1.5 basically consists of a visual encoder and an LLM for natural language generation. The visual encoder utilizes a pretrained CLIP\u2019s Vision Transformer Yuan et al. (2021  ###reference_b35###) , and the LLM  utilized the pretrained LLaMA2-based models  Touvron et al. (2023  ###reference_b32###); Peng et al. (2023  ###reference_b29###). LLaVA uses image  and query  as inputs. In the case of image , the output representation from the visual encoder, , is converted into a vision-language representation  through a projection layer . For text , it passes through the embedding layer  of LLaMA to generate the text representation .  and , generate through these two processes are concatenated and then passed through the entire layer of the LLaMA2 to produce a response. In this context, the projection layer serves the function of transforms image representation  into a word embedding format that can be understood using the LLaMA2.\nTo achieve image-language alignment, we train the process to connect the two representations, which LLaVA does in two steps. The first is image-text alignment through image captioning, and the second is VIT. X-LLaVA is trained in the same manner, and the details of the two phases are described in Section 4.3  ###reference_###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Enriching the LLM Vocabulary",
            "text": "In the LLaVA model, when querying in Korean for the LLaMA2-13B language model, issues arise, such as responses in English or English-Korean code-switching. This stems from a problem with the tokenizer, where 89.7% is in Latin script, while Korean only constitutes 0.37%, leading to insufficient Korean expressiveness and biases in the pretraining data owing to lexical bias. To address these issues, we expanded the Korean vocabulary in the LLaMA2 and conducted additional pretraining for knowledge infusion. (Figure 2  ###reference_### (b), (c))\nVocabulary expansion involves adding 7,478 words from the KoBERT111https://github.com/SKTBrain/KoBERT vocabulary to the LLaMA2 tokenizer. And we randomly initialize embeddings for these newly added words. Ultimately, the proposed tokenizer possessed a dictionary of 39,478 entries. As a subsequent step, the model was further enhanced with knowledge information using English Wikipedia data  and Korean Wikipedia data . Through this process, our model learns representations for the newly added vocabulary. If the pretraining dataset (7.8GB) is defined as , then the loss function  is expressed as follows.\nHere,  is the size of ,  denotes the number of tokens in -th data sample .  represents -th token of sequence , and  represents the sequence of tokens before the -th token. In this context,  is the causal language modeling loss function, where  denotes the model parameters."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "X-LLaVA",
            "text": "In this section, we describe the method for training X-LLaVA using the LLaMA2 model, which has proceeded word expansion and bilingual dictionary pretraining, as previously introduced X-LLaVA, like LLaVA, is trained in two stages: image-language connection via captioning and multilingual VIT. However, unlike LLaVA1.5, to efficiently conduct multilingual training, we follow the cross-lingual language model pretraining method  Conneau and Lample (2019  ###reference_b7###), simultaneously utilizing a mix of English and Korean for training.\nIn the first stage, we train only the projection layer  using the image-caption datasets LLaVA-CC3M Liu et al. (2023b  ###reference_b25###)  and its machine-translated Korean counterpart, LLaVA-KoCC3M. This stage involves representation learning in which image representations are converted into word embeddings that are comprehensible to the LLaMA2. During this process, both Korean and English are learned concurrently while simultaneously aligning [image-English-Korean]. We define the dataset for Stage-1 as .\nIn the second stage, we conducted VIT on X-LLaVA to enhance its capabilities as a multilingual visual assistant. For VIT as described in  Liu et al. (2023b  ###reference_b25###), we use the LLaVA instruct dataset (158K, ), its machine-translated counterpart (158K, ), and the mvif dataset (91K, ) generated in Section 3  ###reference_###. In this stage, unlike the first stage, we train the projection layer and language model simultaneously. Define the dataset for Stage-2 training as . The formula for training the Stage-2 can be expressed as follows:\nWhere ,  represents the total number of conversation turns. In Stage 1,  because the dataset  is composed of a single turn. In Stage 2,  is also true in all case, except for multi-turn conversations.\nIn the dataset , which can be either  or  depending on the stage, , , and  denote the -th component of the image, the question (instruction) in turn , and the answer in turn , respectively."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Quantitative Evaluation",
            "text": "In this section, we describe the quantitative evaluation methods and criteria for the proposed X-LLaVA. Through these comparisons, we aim to address the three research questions proposed in Section 1: (1) What impact does vocabulary expansion, intended to enhance multilinguality, have on vision-language models? and (2) How does bilingual training affect the relationship between these two languages? and (3) Which aspects of the model were strengthened by utilizing our proposed mvif data?"
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Experiment Environments",
            "text": "To ensure a fair comparison of LMMs, we must define task selection for evaluation and specify the LMM model used for evaluation. Below are the benchmark datasets used for evaluation, with the following characteristics for each benchmark:\n\n(English) VQA2.0: A dataset containing open-ended questions about images Goyal et al. (2017), GQA: A VQA-format dataset considered Scene Graph Hudson and Manning (2019), LV (LLaVAw from Liu et al. (2023b)) and POPE Yifan Li and Wen (2023).\n\n(Korean) KoViz: A VQA-format dataset and KoLiv: A VQA-format dataset considered Korean culture and daily life Kim et al.\n\n(English-Korean) BVQA Kim et al. (2024): A VQA dataset considering Bilingual Out-side Knowledge.\n\nFor our experiments, we converted the VQA2.0 and BVQA Kim et al. (2024) datasets into the VIF format using the VQA-to-VIF data transformation method proposed in LLaVA1.5. Following this conversion, we proceeded with VIT over all the training sets from the proposed benchmark in only one epoch. The evaluation methodology and prompts were adopted directly as proposed in LLaVA1.5 (See Appendix C). Experimental environments and answers generated for each model were made publicly accessible222github.com/AnonymousMercy/NACCL_submit to ensure reproducibility and facilitate comparison of the models."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Intrinsic Evaluation of X-LLaVA",
            "text": "An intrinsic evaluation was conducted to explore the three research questions we proposed. To achieve this, we train the three models under different conditions. Table 2 lists the training environments and performances of the three models. X-LLaVA refers to the model that underwent both vocabulary expansion and knowledge enhancement as well as the VIT proposed in Section 4. X-LLaVA(-P) is a model created to compare the effects of pretraining methods on Koreans and English data proposed in Section 4.2. This model is a version of X-LLaVA that does not utilize Wiki for pretraining during its training phase. X-LLaVA(-V,-P) represents a model that neither underwent vocabulary expansion nor used Wiki for pretraining, essentially using pure LLaMA2. Finally, to assess the impact of the mvif data proposed in Section 3, we compared the results of each model with and without the addition of mvif.\n\nThe influence of Pretraining. A comparison between the X-LLaVA and X-LLaVA(-P) models showed that additional pretraining using Wikipedia uniformly enhanced the performance in both Korean and English, with a particularly notable improvement in Korean. Therefore, the effectiveness of pretraining in Korean and English using Wikipedia was evident.\n\nThe influence of VIT using mvif. When models were tuned with the proposed dataset (+O), a performance improvement ranging from 0.2 to 3 was observed across almost models for the target language. Although the extent of improvement is modest, it is noteworthy that despite the grammatical differences between Korean and English, where knowledge loss might be anticipated, there was an observable enhancement in the English performance. This indicates that multilingual VIF can be expected to improve performance in both less- and high-resource languages."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Extrinsic Evaluation of X-LLaVA",
            "text": "We conducted a comparative evaluation of the performance of our X-LLaVA model in Korean and English against other LMMs. The models compared were BLIP-2, InstructBLIP, LLaVA1.5, and KoLLaVA, and the distinctive features of each model are presented in Table 3.\n\nThe effect of multilingual training.\nTypically, when training languages with different character systems, the performance of a relatively highly resourced language may deteriorate Pires et al. (2019). However, with the multilingual training methods and data (mvif) we proposed, no decrease in performance was observed. During analysis, we observed that X-LLaVA generally performed better on GQA and BVQA, which asked about relationships and knowledge.\n\nComparison of X-LLaVA with KoLLaVA. KoLLaVA is the Korean version of LLaVA1.5, a model trained after automatically translating CC3M, VQA2.0, GQA, and Visual Genome data used in LLaVA1.5. Additionally, it was trained using the Korean version of the BVQA. However, as only the 7B model is currently publicly available, it may be challenging to evaluate the same levels.\n\nComparison X-LLaVA with LLaVA1.5(O or B).\nLLaVA1.5 was trained on about 1.5 times more data (665K VIFs) than X-LLaVA. Nevertheless, BVQA data has never been utilized for training, which may be disadvantageous for the BVQA evaluation. We trained LLaVA1.5 on Korean and English data for three epochs to tune the BVQA for a fair evaluation. The results show a significant improvement in Korean performance on the BVQA. On the other hand, this model, being biased towards VQA data, showed lower performance in the writing evaluation (LV). Conversely, LLaVA1.5(O) in Table 3, a model trained on the LLaVA1.5 with mvif data, exhibited the highest performance on LV."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Qualitative Evaluation",
            "text": "In this section, we describe the qualitative evaluation methods and the results for X-LLaVA. In contrast to quantitative evaluations, which are similar to classification assessments, qualitative evaluations, such as writing evaluations, differ significantly. Although human evaluation may be the fairest approach to qualitative assessments, it is practically challenging. Therefore, in LIMA Zhou et al. (2023  ###reference_b37###), a GPT preference evaluation method that closely resembles human evaluation results was proposed.\nIn our study, we directly employed the GPT preference evaluation method. The process is as follows: First, we input an image and a question into two models being compared to obtain answers A and B. Then, we provided GPT4 with the image, question, and both answers to receive feedback such as \u2018Answer A is better\u2019, \u2018Answer B is better\u2019, or \u2018Both answers are similar\u2019, and measured the proportions. To compare the standing and generation abilities of recent LMMs in vision language, we used the GPT evaluation dataset proposed by LLaVA444\u2018qa90_gpt4_answer\u2019 at github.com/haotian-liu/LLaVA. However, because this dataset is in English, we translated it into Korean, followed by a review from five annotators to ensure data quality. Afterward, we proceeded with the evaluations."
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Preference Evaluation using GPT4-V",
            "text": "###figure_3### ###figure_4### Comparing X-LLaVA with others in Korean.\nFigure 3  ###reference_### presents the results of the GPT preference evaluation for each model. The X-LLaVA model outperformed all other models, except for the GPT4-V model. Notably, it obtained a 19% higher preference rate than the KoLLaVA, indicating the exceptional effectiveness of the proposed methods and datasets in enhancing Korean writing skills.\nComparing X-LLaVA with Others in English. Figure 4  ###reference_### shows the results of English GPT preference evaluations. Interestingly, similar to Korean, the X-LLaVA received approximately 25% higher preference scores for English than LLaVA1.5. This indicates that pretraining of our proposed LLM and mvif datasets can also enhance English writing abilities.\n###figure_5### ###figure_6### X-LLaVA vs GPT4-V. Therefore, does evaluator GPT4-V generate better answers than X-LLaVA? We conducted the evaluations by comparing the GPT4-V and X-LLaVA models. Experimental results show that for both languages, GPT4-V\u2019s answers are preferred over those of X-LLaVA, with a significant performance difference. However, these results stem from GPT4-V generating answers that are more than 30% longer and more verbose compared to LLaVA-based models. This may also be because the GPT rates its own generated content more favorably as it becomes more familiar with it. To mitigate this, in experiments where the answers were limited to 30 words, the results changed significantly, with GPT scoring 42 compared to 17 for X-LLaVA. Detailed statistical analysis related to this can be found in Figure 5  ###reference_### and Appendix E."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Human-assisted Preference Evaluation",
            "text": "As previously described, the performance of GPT preference evaluation may vary according to the number of words. Consequently, a question arises: Can LIMA\u2019s assertion that GPT evaluations are akin to human assessments be extended to the vision-language model proposed in this study? We conducted a human preference evaluation using three human annotators. Figure 6  ###reference_### presents the results of the human evaluation for GPT4-V and X-LLaVA in the comparative assessment, with the response length restricted to 30 words. Although GPT maintained a slight advantage, the preference scores were nearly identical. However, we observed that GPT evaluations resulted in ties 2.9 times more frequently than human evaluations. This observation can be interpreted to suggest that GPT tends to avoid ambiguous decisions compared to humans, who possess relatively clear criteria. Thus, the vision-language model can be considered as augmenting rather than substituting human evaluations. Details supporting this, along with comprehensive human evaluation results and analyses for the entire model, are available in Appendix F."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we propose a framework for constructing data and training models for the efficient multilingual expansion of LMM. For data construction, we suggested a method to easily build multilingual VIF dataset based on the relational metadata between images and objects using GPT4-V. We also demonstrated a framework for efficient multilingual learning, which includes vocabulary enhancement, knowledge reinforcement based on pretraining, and a multilingual VIT framework. The experimental results confirmed that the proposed X-LLaVA model exhibited similar or superior performance compared to existing models that primarily focused on Korean and English as single languages. Finally, our proposed multilingual expansion framework can be trained in 7.5 days with a single A6000 GPU, and the 91K training data can be managed with relatively minimal resources, costing around $3,200."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A Data Generation Example",
            "text": "###figure_7###"
        },
        {
            "section_id": "Appendix 2",
            "parent_section_id": null,
            "section_name": "Appendix B Data Statistics",
            "text": "###figure_8### ###figure_9### ###figure_10### ###figure_11### In this section, we present a detailed analysis of the dataset. Figure 8  ###reference_### and Figure 9  ###reference_### visualize the frequency distribution of words contained in English questions and answers within the dataset. These graphs follow the order of words in sentences, starting from the center and progressing outward. In other words, the center represents the first word of the sentence, and each subsequent word is represented outwardly based on its position within the sentence.\nFigure 10  ###reference_### and Figure 11  ###reference_### depict the word lengths of English queries and responses in the context of mvif, providing an overview of the dataset\u2019s distribution."
        },
        {
            "section_id": "Appendix 3",
            "parent_section_id": null,
            "section_name": "Appendix C Training Details and Hyperparameters",
            "text": "Training details. Like LLaVA1.5, we applied Low-Rank Adaptation (LoRA) Hu et al. (2021  ###reference_b12###) for visual instruction-following. All the used hyperparameters are identical. Furthermore, we also utilized LoRA in the Korean-English pretraining phase of LLaMA2 to reduce GPU memory usage. The LoRA parameters applied during the pretraining phase were taken directly from the parameter settings suggested in Chinese Alpaca Cui et al. (2023  ###reference_b8###).\nTraining order for VIF data. We observed significant performance variations depending on the order of the data during the training with VIF data. Therefore, during the visual instruction tuning phase, all data were shuffled and trained together.\nEvaluation Metric for Korean and Chinese. In this study, the Korean (BVQA, KoLiv, KoViz) and Chinese (VQA-ch) data used differ from English VQA in that they only have one answer per data point. Consequently, answers like \u201cYes\u201d, \u201c\ub124(yes)\u201d, and \u201c\uc608(yes)\u201d all have the same meaning, but there is an issue where only \u201c\ub124\u201d is counted as the correct answer. Therefore, we conducted post-processing to treat all three responses as correct. We applied the same performance evaluation method across all models. The detailed evaluation script is in our repository: https://github.com/AnonymousMercy/NACCL_submit  ###reference_bmit###\nHyperparameters. We employed the same hyperparameter settings as LLaVA1.5."
        },
        {
            "section_id": "Appendix 4",
            "parent_section_id": null,
            "section_name": "Appendix D Generated Data Samples",
            "text": "###figure_12### ###figure_13###"
        },
        {
            "section_id": "Appendix 5",
            "parent_section_id": null,
            "section_name": "Appendix E Statistical Analysis of Responses in the Qualitative Evaluation Experiment",
            "text": "###figure_14### ###figure_15### Analysis of Qualitative Evaluation Figure 14  ###reference_### shows examples of responses from each model in the qualitative evaluation. When examining the responses of X-LLaVA, it is noticeable that there is a tendency to focus on the positions of objects. This indicates that X-LLaVA has been trained on the mvif dataset, which includes a variety of tasks involving Objects and Locations. Additionally, as observed in Figure 15  ###reference_###, X-LLaVA outperformed all other models except for the GPT4-V model in the evaluations. Particularly, as shown in Table 6  ###reference_###, X-LLaVA used a more diverse and extensive range of expressions compared to KoLLaVA, which likely had a significant impact on the comparisons in Figure 15  ###reference_###. This suggests that the LLM vocab expansion technique employed in training X-LLaVA contributed to its effectiveness. However, despite using a variety of expressions similar to GPT4-V, X-LLaVA was outperformed by GPT4-V by a margin of 33%, implying that GPT4-V likely used more implicit and advanced vocabulary within shorter sentences.\n###figure_16### ###figure_17### ###figure_18### ###figure_19### Figure 16  ###reference_###19  ###reference_### visualize the distribution of token lengths In this study, the model proposed, X-LLaVA, tends to produce relatively shorter responses. When contrasted with the results of the GPT4-V qualitative evaluation, Figure 18  ###reference_### shows that LLaVA1.5, while having a distribution of response lengths similar to other models, has a lower win rate than X-LLaVA, suggesting that the X-LLaVA model generally produces higher quality English responses than the LLaVA1.5 model. Additionally, Figure 19  ###reference_### shows that KoLLaVA, despite generally having longer responses than X-LLaVA, has a relatively lower win rate. This indicates a tendency of the X-LLaVA model to generate higher quality Korean responses relative to the same response length."
        },
        {
            "section_id": "Appendix 6",
            "parent_section_id": null,
            "section_name": "Appendix F Human Preferenece Evaluation Details",
            "text": "###figure_20### The Human Preference Evaluation shown in Figure 6  ###reference_### was carried out with three evaluators using the following criteria: For a result to be classified as \u2018XLLaVA Wins,\u2019 either all three evaluators needed to select it or at least two did. A \u2018Tie\u2019 was determined either when all evaluators agreed on it or when their selections were evenly split across \u2018XLLaVA Wins,\u2019 \u2018Tie,\u2019 and \u2018XLLaVA Loses.\u2019 Similarly, \u2018XLLaVA Loses\u2019 was classified when all three agreed on it or at least two of the three chose it.\nTable 7  ###reference_### presents the numerical results corresponding to those depicted in Figure 6. The evaluation results between Human and GPT4-V show an 80%(12/15) agreement rate for \u2018XLLaVA Wins\u2019 and approximately an 82%(32/38) agreement rate for \u2018XLLaVA Loses (GPT4-V Wins)\u2019. However, for the \u2018Tie\u2019 category, the GPT4-V Evaluation only shows about a 27% agreement rate with human evaluations, indicating a significant difference compared to the results of Human Preference Evaluation. Therefore, at this stage, it is challenging for GPT preference evaluations to serve as a complete substitute for human assessments. Nonetheless, the overarching trends observed in these preference evaluations bear some resemblance to those in human assessments, suggesting that they constitute a meaningful metric for consideration.\nWe also extended our evaluations to include models other than X-LLaVA, employing the same human evaluation protocol. Figure 20  ###reference_### displays the human evaluation results in Korean for all models examined in this study. Consistent with previous discussions, while the overall trend in GPT and human evaluations across different models was generally similar, GPT was more prone to result in ties in preference assessments.\nA notable aspect of this experiment is that, in contrast to the GPT evaluations, X-LLaVA achieved a complete win in all trials against BLIP2 and InstructBLIP2, models that lack proficiency in Korean. Conversely, the GPT evaluations depicted in Figure 5  ###reference_### resulted in a \u201cTie\u201d for 7 to 17% of the cases involving these two models, which also do not understand Korean. This pattern indicates that GPT adopts a highly conservative approach in its evaluations, potentially due to its methodology or criteria for determining outcomes, emphasizing caution and possibly erring towards neutrality when faced with ambiguous cases."
        },
        {
            "section_id": "Appendix 7",
            "parent_section_id": null,
            "section_name": "Appendix G Inspection Procedure Details",
            "text": "We have employed two annotators, one native English-Korean speaker and one native English-Chinese speaker, to inspect the generated data for 24,000 images. To facilitate efficient data inspection, we utilized a WebUI-based data inspection platform (LabelOn), where annotations can be verified through Figure 21  ###reference_### and  22  ###reference_###. Each annotator received parallel sets of English-Chinese or English-Korean datasets to review for Pass/Error statuses. Both annotators inspected the data over the course of one month. As a result, 504 data points were removed. The two main issues with the removed data were identified as (1) proper noun objects and (2) cultural differences. Below are examples:\n(1) For the issue of proper noun objects:\nQuestion: Describe the scene in the image\nAnswer: \u201c..north ridge of Mount Stuart..\u201d\nIn cases like the above, GPT4-V labeled the location with the proper noun \u201cMount Stuart\u201d based on its own knowledge despite it being difficult to specify the place from the input image. Such data were problematic and, therefore, deleted.\n(2) For the issue of cultural differences:\nWe found that GPT4-V is also biased towards English-speaking cultures. For example,\nQuestion: Describe the scene in the image\nAnswer: \u201c\u2026. creepy food \u2026.\u201d\n\u2018Creepy food\u2019 is usually associated with Halloween foods and was translated into Korean and Chinese as \u201c\uc18c\ub984\ub07c\uce58\ub294 \uc74c\uc2dd (scared food)\u201d and \u201c\u60ca\u609a\u98df\u7269 (thriller food)\u201d, respectively. This is not only a rarely used expression in Korea and China but also has the potential for mistranslation. In this paper, we removed the 504 training data with the issues mentioned above and shared both the original 24K dataset and the post-processed (final) dataset of 23.4K.\n###figure_21### ###figure_22### ###figure_23###"
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S1.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Summary of multi-modal instruction tuning datasets. \u2018Visible\u2019 refers to the including of images in the data generation process. The availability of a \u2018Parallel\u2019 pertains to whether the dataset can be used translation task.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S1.T1.12\" style=\"width:433.6pt;height:111.5pt;vertical-align:-0.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-135.0pt,34.5pt) scale(0.61625,0.61625) ;\">\n<p class=\"ltx_p\" id=\"S1.T1.12.12\"><span class=\"ltx_text\" id=\"S1.T1.12.12.12\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S1.T1.12.12.12.12\" style=\"width:703.6pt;height:181pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"S1.T1.12.12.12.12.12\"><span class=\"ltx_text\" id=\"S1.T1.12.12.12.12.12.12\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S1.T1.12.12.12.12.12.12.12\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S1.T1.12.12.12.12.12.12.12.13.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.1\">Dataset</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.2\">Domain</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.3\">Data Type</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.4\"># of Words</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.5\">Visible</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.6\">Captioned by</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.7\"># of Instances</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.8\">Multilingual</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.9\">Parallel</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S1.T1.12.12.12.12.12.12.12.13.1.10\">Open</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S1.T1.1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.2\">MiniGPT4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.3\">Daily life</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.4\">Description, Discourse</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.1\">80 </span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.5\">\u2717</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.6\">Template-based</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.7\">5K</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.8\">\u2717</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.9\">\u2717</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1.1.1.1.1.1.10\">\u2713</span></span>\n<span class=\"ltx_tr\" id=\"S1.T1.3.3.3.3.3.3.3.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.3.3.3.3.3.3.3.3.3\">MultiInstruct</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.3.3.3.3.3.3.3.3.4\">General</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.3.3.3.3.3.3.3.3.5\">Description, Reasoning</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.2.2.2.2.2.2.2.2.1\"> 100</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.3.3.3.3.3.3.3.3.6\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.3.3.3.3.3.3.3.3.7\">Template-based</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.3.3.3.3.3.3.3.3.2\"> 235K</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.3.3.3.3.3.3.3.3.8\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.3.3.3.3.3.3.3.3.9\">\u2717</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.3.3.3.3.3.3.3.3.10\">\u2717</span></span>\n<span class=\"ltx_tr\" id=\"S1.T1.5.5.5.5.5.5.5.5\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.5.5.5.5.5.5.5.5.3\">InstructBLIP</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.5.5.5.5.5.5.5.5.4\">Daily life</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.5.5.5.5.5.5.5.5.5\">Description, Reasoning, Discourse</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.4.4.4.4.4.4.4.4.1\"> 200</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.5.5.5.5.5.5.5.5.6\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.5.5.5.5.5.5.5.5.7\">Template-based</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.5.5.5.5.5.5.5.5.2\"> 1.6M</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.5.5.5.5.5.5.5.5.8\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.5.5.5.5.5.5.5.5.9\">\u2717</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.5.5.5.5.5.5.5.5.10\">\u2717</span></span>\n<span class=\"ltx_tr\" id=\"S1.T1.6.6.6.6.6.6.6.6\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.6.6.6.6.6.6.6.6.2\">LLaVA</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.3\">Daily life</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.4\">Description, Reasoning, Discourse</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.1\"> 200</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.5\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.6\">GPT-based</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.7\">1.15M</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.8\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.9\">\u2717</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.6.6.6.6.6.6.6.6.10\">\u2713</span></span>\n<span class=\"ltx_tr\" id=\"S1.T1.7.7.7.7.7.7.7.7\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.7.7.7.7.7.7.7.7.2\">MultiModalGPT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.3\">General</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.4\">Description, Discourse</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.1\"> 200</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.5\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.6\">GPT-based</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.7\">6K</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.8\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.9\">\u2717</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.7.7.7.7.7.7.7.7.10\">\u2717</span></span>\n<span class=\"ltx_tr\" id=\"S1.T1.8.8.8.8.8.8.8.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.8.8.8.8.8.8.8.8.2\">SharedGPT4V</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.3\">General</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.4\">Description, Reasoning, Discourse</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.1\"> 200</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.5\">\u2713</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.6\">GPT-based</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.7\">100K</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.8\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.9\">\u2717</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.8.8.8.8.8.8.8.8.10\">\u2713</span></span>\n<span class=\"ltx_tr\" id=\"S1.T1.9.9.9.9.9.9.9.9\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.9.9.9.9.9.9.9.9.2\">LVIS-INSTRUCT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.3\">Daily life</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.4\">Description</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.1\"> 100</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.5\">\u2713</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.6\">GPT-based</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.7\">220K</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.8\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.9\">\u2717</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.9.9.9.9.9.9.9.9.10\">\u2713</span></span>\n<span class=\"ltx_tr\" id=\"S1.T1.11.11.11.11.11.11.11.11\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T1.10.10.10.10.10.10.10.10.1\">M<sup class=\"ltx_sup\" id=\"S1.T1.10.10.10.10.10.10.10.10.1.1\">3</sup>IT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.3\">General</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.4\">Description, Reasoning</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.2\"> 200</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.5\">\u2717</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.6\">GPT-based</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.7\">2.4M</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.8\">\u2713</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.9\">\u2717</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.11.11.11.11.11.11.11.11.10\">\u2713</span></span>\n<span class=\"ltx_tr\" id=\"S1.T1.12.12.12.12.12.12.12.12\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.2\">Ours</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.3\">Daily life</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.4\">Description, Discourse</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.1\"> 200</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.5\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.6\">GPT-based</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.7\">91K</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.8\">\u2713</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.9\">\u2713</span>\n<span class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S1.T1.12.12.12.12.12.12.12.12.10\">\u2713</span></span>\n</span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n</figure>",
            "capture": "Table 1: Summary of multi-modal instruction tuning datasets. \u2018Visible\u2019 refers to the including of images in the data generation process. The availability of a \u2018Parallel\u2019 pertains to whether the dataset can be used translation task."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T2\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.2.2.3\">Model</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T2.2.2.4\">VIF</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1\">BVQA<sup class=\"ltx_sup\" id=\"S5.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T2.1.1.1.1.1\">k</span></sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.2.2.2\">BVQA<sup class=\"ltx_sup\" id=\"S5.T2.2.2.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T2.2.2.2.1.1\">e</span></sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.2.2.5\">GQA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.2.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.3.1.1\">XLLaVA(-V,-P)</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.1.2\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.3.1.3\">51.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.1.4\">33.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.3.1.5\">62.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.4.2\">\n<td class=\"ltx_td\" id=\"S5.T2.2.4.2.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T2.2.4.2.2\">+ O</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.4.2.3\">51.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.4.2.4.1\">36.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.4.2.5\">61.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.5.3.1\">XLLaVA(-P)</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S5.T2.2.5.3.2\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.5.3.3\">56.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.5.3.4\">32.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.5.3.5\">62.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.6.4\">\n<td class=\"ltx_td\" id=\"S5.T2.2.6.4.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T2.2.6.4.2\">+ O</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.6.4.3\">56.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.6.4.4\">32.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.2.6.4.5\">62.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.7.5.1\">XLLaVA</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S5.T2.2.7.5.2\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.7.5.3\">57.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.7.5.4\">33.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.2.7.5.5\">63.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.8.6\">\n<td class=\"ltx_td ltx_border_bb\" id=\"S5.T2.2.8.6.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T2.2.8.6.2\">+ O</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.2.8.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.8.6.3.1\">57.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.2.8.6.4\">34.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.2.8.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.2.8.6.5.1\">64.0</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Intrinsic evaluation. Where (-V) represents without vocabulary expansion, and (-P) denotes without multilingual pretraining step. Metric is Accuracy(%).</figcaption>\n</figure>",
            "capture": "Table 2: Intrinsic evaluation. Where (-V) represents without vocabulary expansion, and (-P) denotes without multilingual pretraining step. Metric is Accuracy(%)."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T3\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.1\">LMM</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.2\">LLM</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.3\">#PT</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T3.1.1.1.4\">#VIT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.5\">BVQA<sup class=\"ltx_sup\" id=\"S5.T3.1.1.1.5.1\">k</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.6\">KoViz</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T3.1.1.1.7\">KoLiv</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.8\">BVQA<sup class=\"ltx_sup\" id=\"S5.T3.1.1.1.8.1\">e</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.9\">VQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.10\">GQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.11\">LV</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1.12\">POPE</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.2.1.1\">BLIP-2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.2.1.2\">Vicuna13B</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.2.1.3\">129M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.1.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.1.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.1.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.1.7\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.1.8\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.1.9\">41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.1.10\">41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.1.11\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.1.12\">85.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.3.2.1\">InstructBLIP</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.3.2.2\">Vicuna7B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.3.2.3\">129M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.3.2.4\">1.2M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.3.2.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2.8\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2.10\">49.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2.11\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2.12\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.4.3.1\">InstructBLIP</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.4.3.2\">Vicuna13B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.4.3.3\">129M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.4.3.4\">1.2M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.4.3.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3.8\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3.10\">49.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3.11\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3.12\">78.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.5.4.1\">LLaVA1.5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.5.4.2\">Vicuna7B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.5.4.3\">558K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.5.4.4\">665K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4.5\">16.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4.6\">33.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.5.4.7\">44.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4.8\">25.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4.9\">78.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4.10\">62.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4.11\">64.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4.12\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.5.4.12.1\">85.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.6.5.1\">LLaVA1.5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.6.5.2\">Vicuna13B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.6.5.3\">558K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.6.5.4\">665K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.5.5\">27.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.5.6\">24.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.6.5.7\">33.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.5.8\">26.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.5.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.6.5.9.1\">80.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.5.10\">63.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.5.11\">65.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.5.12\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.6.5.12.1\">85.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.7.6.1\">LLaVA1.5(O)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.7.6.2\">Vicuna13B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.7.6.3\">558K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.7.6.4\">756K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.6.5\">32.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.6.6\">24.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.7.6.7\">23.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.6.8\">29.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.6.9\">78.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.6.10\">45.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.6.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.7.6.11.1\">70.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.7.6.12\">85.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.8.7.1\">LLaVA1.5(B)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.8.7.2\">Vicuna13B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.8.7.3\">558K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.8.7.4\">857K</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.7.5\">54.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.7.6\">50.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.8.7.7\">52.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.7.8\">33.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.7.9\">76.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.7.10\">63.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.7.11\">22.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.8.7.12\">85.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.9.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.9.8.1\">KoLLaVA</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.9.8.2\">Synatra7B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.9.8.3\">595K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.9.8.4\">612k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.9.8.5\">45.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.9.8.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.9.8.6.1\">55.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.9.8.7\">54.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.9.8.8\">5.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.9.8.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.9.8.10\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.9.8.11\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.9.8.12\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.10.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.1\">X-LLaVA</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.2\">Ours</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.3\">1.2M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T3.1.10.9.4\">407K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.10.9.5.1\">57.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.6\">51.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T3.1.10.9.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.10.9.7.1\">61.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.10.9.8.1\">34.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.9\">75.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.10.9.10.1\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.11\">57.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.1.10.9.12\">85.5</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Extrinsic evaluation results. Where (O), (B) represents training with <span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T3.5.1\">mvif</span> and BVQA dataset,<span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T3.6.2\">#PT</span> is the number of pretraining data, <span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T3.7.3\">#VIT</span> is the number of VIT data. POPE is a benchmark for evaluation of hallucination.</figcaption>\n</figure>",
            "capture": "Table 3: Extrinsic evaluation results. Where (O), (B) represents training with mvif and BVQA dataset,#PT is the number of pretraining data, #VIT is the number of VIT data. POPE is a benchmark for evaluation of hallucination."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_minipage ltx_align_middle\" id=\"A3.T4.2.2\" style=\"width:173.4pt;\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A3.T4.2.2.3.1.1\">component</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T4.2.2.3.1.2\">value</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A3.T4.2.2.4.2.1\">Dropout</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T4.2.2.4.2.2\">0.05</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.5.3.1\">Learning rate</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.5.3.2\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.6.4.1\">Optimizer</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.6.4.2\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.2.2\">\n, \n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.2.3\">0.9, 0.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.7.5.1\">Epoch for VQA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.7.5.2\">1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.8.6.1\">Batch size (VQA)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.8.6.2\">8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.9.7.1\">Low-rank size</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.9.7.2\">8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.10.8.1\">ora_alpha</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.10.8.2\">32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.11.9.1\">lora_trainable</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.11.9.2\">q,v,k,o,gate,down,up_proj</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T4.2.2.12.10.1\">LoRA layer,</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.12.10.2\">q, k, v</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.13.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"A3.T4.2.2.13.11.1\">Random Seed</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"A3.T4.2.2.13.11.2\">42</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Applied hyperparameters.</figcaption>\n</figure>",
            "capture": "Table 4: Applied hyperparameters."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"A3.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A3.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A3.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"A3.T5.1.1.1.1\">GPU</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T5.1.1.1.2\">Training Phase</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T5.1.1.1.3\">Duration</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T5.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A3.T5.1.2.1.1\">A6000x1</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T5.1.2.1.2\">CC3M</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T5.1.2.1.3\">96.6h</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T5.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T5.1.3.2.1\">A6000x1</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T5.1.3.2.2\">Wiki-Pretraining(ko)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T5.1.3.2.3\">28.4h</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T5.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A3.T5.1.4.3.1\">A6000x1</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T5.1.4.3.2\">VIF</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T5.1.4.3.3\">64.1h</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T5.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"A3.T5.1.5.4.1\">Total</th>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\" id=\"A3.T5.1.5.4.2\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A3.T5.1.5.4.3\">182.1h</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Duration of Each Training Phase. It took a total of approximately 7.5 days to train the proposed enhanced model using the A6000 GPU.</figcaption>\n</figure>",
            "capture": "Table 5: Duration of Each Training Phase. It took a total of approximately 7.5 days to train the proposed enhanced model using the A6000 GPU."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"A5.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A5.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A5.T6.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A5.T6.1.1.1.1\">POS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A5.T6.1.1.1.2\">X-LLaVA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A5.T6.1.1.1.3\">KoLLaVA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"A5.T6.1.1.1.4\">GPT4-V</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"A5.T6.1.2.2.1\">Duplicate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A5.T6.1.2.2.2\">\u2713</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A5.T6.1.2.2.3\">\u2717</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A5.T6.1.2.2.4\">\u2713</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A5.T6.1.2.2.5\">\u2717</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A5.T6.1.2.2.6\">\u2713</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"A5.T6.1.2.2.7\">\u2717</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A5.T6.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.3.1.1\">Noun</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T6.1.3.1.2\">899</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T6.1.3.1.3\">414</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T6.1.3.1.4\">266</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T6.1.3.1.5\">133</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T6.1.3.1.6\">879</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T6.1.3.1.7\">455</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.4.2.1\">Verb</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.4.2.2\">247</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.4.2.3\">59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.4.2.4\">39</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.4.2.5\">19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.4.2.6\">251</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.4.2.7\">74</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.5.3.1\">Modifier</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.5.3.2\">67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.5.3.3\">14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.5.3.4\">30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.5.3.5\">10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.5.3.6\">64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.5.3.7\">19</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.6.4.1\">Indep.</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.6.4.2\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.6.4.3\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.6.4.4\">10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.6.4.5\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.6.4.6\">4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.6.4.7\">2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.7.5.1\">Relational</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.7.5.2\">533</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.7.5.3\">23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.7.5.4\">89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.7.5.5\">15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.7.5.6\">498</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.7.5.7\">27</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.8.6.1\">Ending</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.8.6.2\">366</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.8.6.3\">21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.8.6.4\">51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.8.6.5\">16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.8.6.6\">370</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.8.6.7\">29</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.9.7.1\">Affix</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.9.7.2\">49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.9.7.3\">4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.9.7.4\">6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.9.7.5\">4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.9.7.6\">61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.9.7.7\">7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.10.8.1\">Symbols</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.10.8.2\">138</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.10.8.3\">3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.10.8.4\">106</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.10.8.5\">5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.10.8.6\">142</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.10.8.7\">2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A5.T6.1.11.9.1\">F.L.</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.11.9.2\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.11.9.3\">1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.11.9.4\">35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.11.9.5\">1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.11.9.6\">0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T6.1.11.9.7\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T6.1.12.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A5.T6.1.12.10.1\">Total</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T6.1.12.10.2\">2303</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T6.1.12.10.3\">541</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T6.1.12.10.4\">632</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T6.1.12.10.5\">205</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T6.1.12.10.6\">2269</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T6.1.12.10.7\">615</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>It is to compare the number of parts of speech based on the Korean answers to the qualitative evaluation that limited 30 words, and \u2018Duplicate\u2019 means whether or not words are duplicated. In the following table, \u2018Part Of Speech\u2019 is specified as \u2018POS\u2019, \u2018Independent\u2019 is specified as \u2018Indep.\u2019, and \u2018Foreign Language\u2019 is specified as \u2018F.L.\u2019</figcaption>\n</figure>",
            "capture": "Table 6: It is to compare the number of parts of speech based on the Korean answers to the qualitative evaluation that limited 30 words, and \u2018Duplicate\u2019 means whether or not words are duplicated. In the following table, \u2018Part Of Speech\u2019 is specified as \u2018POS\u2019, \u2018Independent\u2019 is specified as \u2018Indep.\u2019, and \u2018Foreign Language\u2019 is specified as \u2018F.L.\u2019"
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"A6.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A6.T7.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A6.T7.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"A6.T7.1.2.1.1\">Evaluator</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T7.1.2.1.2\">XLLaVA Wins</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T7.1.2.1.3\">Tie</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T7.1.2.1.4\">XLLaVA Loses</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A6.T7.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A6.T7.1.3.1.1\">GPT4-V(G)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T7.1.3.1.2\">15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T7.1.3.1.3\">37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T7.1.3.1.4\">38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T7.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A6.T7.1.4.2.1\">Human(H)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T7.1.4.2.2\">37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T7.1.4.2.3\">14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T7.1.4.2.4\">39</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T7.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"A6.T7.1.1.1\">G\u00a0\u00a0H</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A6.T7.1.1.2\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A6.T7.1.1.3\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A6.T7.1.1.4\">32</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>It displays the number of samples chosen by GPT4-V and Human Evaluators for \u2018XLLaVA Wins\u2019, \u2018Tie\u2019, and \u2018XLLaVA Loses\u2019, respectively in Figure\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11399v3#S6.F5\" title=\"Figure 5 \u2023 6.1 Preference Evaluation using GPT4-V \u2023 6 Qualitative Evaluation \u2023 X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.11399v3#S6.F6\" title=\"Figure 6 \u2023 6.1 Preference Evaluation using GPT4-V \u2023 6 Qualitative Evaluation \u2023 X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. \u2018G\u00a0\u00a0H\u2019 signifies instances where both evaluators (Human, GPT4-V) indicate the same outcome for each of the 90 samples.</figcaption>\n</figure>",
            "capture": "Table 7: It displays the number of samples chosen by GPT4-V and Human Evaluators for \u2018XLLaVA Wins\u2019, \u2018Tie\u2019, and \u2018XLLaVA Loses\u2019, respectively in Figure\u00a05 and\u00a06. \u2018G\u00a0\u00a0H\u2019 signifies instances where both evaluators (Human, GPT4-V) indicate the same outcome for each of the 90 samples."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.11399v3_figure_1.png",
            "caption": "Figure 1: An example of prompt and result using data construction."
        },
        "2": {
            "figure_path": "2403.11399v3_figure_2.png",
            "caption": "Figure 2: (a) Architecture of LLaVA1.5 & (b,c) The proposed language model pretraining"
        },
        "3": {
            "figure_path": "2403.11399v3_figure_3.png",
            "caption": "Figure 3: Korean Preference evaluation results by GPT4-V"
        },
        "4": {
            "figure_path": "2403.11399v3_figure_4.png",
            "caption": "Figure 4: English Preference evaluation results by GPT4-V"
        },
        "5": {
            "figure_path": "2403.11399v3_figure_5.png",
            "caption": "Figure 5: Korean Preference evaluation results by GPT4-V when limited to 30 Words."
        },
        "6": {
            "figure_path": "2403.11399v3_figure_6.png",
            "caption": "Figure 6: Preference evaluation results by human"
        },
        "7": {
            "figure_path": "2403.11399v3_figure_7.png",
            "caption": "Figure 7: \nExample for Query Generation. An input image, system message, and main objects are given as inputs, and as output, four different query-response samples are generated. EN : English, KO : Korean, CN : Chinese."
        },
        "8": {
            "figure_path": "2403.11399v3_figure_8.png",
            "caption": "Figure 8: \nThis chart displays the frequency of words found in the mvif questions, organized according to their syntactic order."
        },
        "9": {
            "figure_path": "2403.11399v3_figure_9.png",
            "caption": "Figure 9: \nThis chart displays the frequency of words found in the mvif answers, organized according to their syntactic order."
        },
        "10": {
            "figure_path": "2403.11399v3_figure_10.png",
            "caption": "Figure 10: \nThis graph represents the word length distribution of questions in the mvif."
        },
        "11": {
            "figure_path": "2403.11399v3_figure_11.png",
            "caption": "Figure 11: \nThis graph represents the word length distribution of answers in the mvif."
        },
        "12": {
            "figure_path": "2403.11399v3_figure_12.png",
            "caption": "Figure 12: Example of each model\u2019s answer to \u201cDescribe what is interesting about the image\u201d"
        },
        "13": {
            "figure_path": "2403.11399v3_figure_13.png",
            "caption": "Figure 13: The results of multi-turn conversations across various models."
        },
        "14": {
            "figure_path": "2403.11399v3_figure_14.png",
            "caption": "Figure 14: Examples of answers from \u2018X-LLaVA\u2019, \u2018LLaVA\u2019, \u2018KoLLaVA\u2019, and \u2018GPT4-V\u2019 models to English and Korean qualitative evaluations."
        },
        "15": {
            "figure_path": "2403.11399v3_figure_15.png",
            "caption": "Figure 15: Results of GPT preference evaluation for English when limited to 30 words."
        },
        "16": {
            "figure_path": "2403.11399v3_figure_16.png",
            "caption": "Figure 16: \nThis graph shows a histogram comparing the token lengths of English answers for \u2018X-LLaVA\u2019 and \u2018GPT4-V\u2019."
        },
        "17": {
            "figure_path": "2403.11399v3_figure_17.png",
            "caption": "Figure 17: \nThis graph shows a histogram comparing the token lengths of Korean answers for \u2018X-LLaVA\u2019 and \u2018GPT4-V\u2019."
        },
        "18": {
            "figure_path": "2403.11399v3_figure_18.png",
            "caption": "Figure 18: \nThis graph shows a histogram comparing the token lengths of English answers for \u2018X-LLaVA\u2019 and \u2018LLaVA1.5\u2019."
        },
        "19": {
            "figure_path": "2403.11399v3_figure_19.png",
            "caption": "Figure 19: \nThis graph shows a histogram comparing the token lengths of Korean answers for \u2018X-LLaVA\u2019 and \u2018KoLLaVA\u2019."
        },
        "20": {
            "figure_path": "2403.11399v3_figure_20.png",
            "caption": "Figure 20: Preference evaluation by human in comparison with other models"
        },
        "21": {
            "figure_path": "2403.11399v3_figure_21.png",
            "caption": "Figure 21: \nThis figure represents the worker status board on the LabelON data review platform. Information about the annotators is shown in purple, the work target samples in blue, Passed Data in green, and Error Data in red."
        },
        "22": {
            "figure_path": "2403.11399v3_figure_22.png",
            "caption": "(a) English-Chinese dataset inspection procedure"
        },
        "23": {
            "figure_path": "2403.11399v3_figure_23.png",
            "caption": "(b) English-Korean dataset inspection procedure"
        }
    },
    "references": [
        {
            "1": {
                "title": "Sharegpt.",
                "author": "",
                "venue": "https://sharegpt.com/%7D%7D,year={2023}.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Flamingo: a visual language model for few-shot learning.",
                "author": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko\u0142 aj Bi\u0144kowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\u00e9n Simonyan. 2022.",
                "venue": "In Advances in Neural Information Processing Systems, volume 35, pages 23716\u201323736. Curran Associates, Inc.",
                "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf"
            }
        },
        {
            "3": {
                "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
                "author": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023.",
                "venue": "arXiv preprint arXiv:2308.12966.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Visual instruction tuning with polite flamingo.",
                "author": "Delong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan Wang. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2307.01003"
            }
        },
        {
            "5": {
                "title": "Sharegpt4v: Improving large multi-modal models with better captions.",
                "author": "Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023b.",
                "venue": null,
                "url": "http://arxiv.org/abs/2311.12793"
            }
        },
        {
            "6": {
                "title": "A simple framework for contrastive learning of visual representations.",
                "author": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.",
                "venue": "In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1597\u20131607. PMLR.",
                "url": "https://proceedings.mlr.press/v119/chen20j.html"
            }
        },
        {
            "7": {
                "title": "Cross-lingual language model pretraining.",
                "author": "Alexis Conneau and Guillaume Lample. 2019.",
                "venue": "Advances in neural information processing systems, 32.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Efficient and effective text encoding for chinese llama and alpaca.",
                "author": "Yiming Cui, Ziqing Yang, and Xin Yao. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2304.08177"
            }
        },
        {
            "9": {
                "title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning.",
                "author": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.",
                "venue": "In Thirty-seventh Conference on Neural Information Processing Systems.",
                "url": "https://openreview.net/forum?id=vvoWPYqZJA"
            }
        },
        {
            "10": {
                "title": "MASSIVE: A 1M-example multilingual natural language understanding dataset with 51 typologically-diverse languages.",
                "author": "Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem Natarajan. 2023.",
                "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4277\u20134302, Toronto, Canada. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2023.acl-long.235"
            }
        },
        {
            "11": {
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering.",
                "author": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017.",
                "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Lora: Low-rank adaptation of large language models.",
                "author": "Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021.",
                "venue": "In International Conference on Learning Representations.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering.",
                "author": "Drew A Hudson and Christopher D Manning. 2019.",
                "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Korean localization of visual question answering for blind people.",
                "author": "Jin-Hwa Kim, Soohyun Lim, Jaesun Park, and Hansu Cho.",
                "venue": null,
                "url": null
            }
        },
        {
            "15": {
                "title": "Bok-vqa: Bilingual outside knowledge-based visual question answering via graph representation pretraining.",
                "author": "Minjun Kim, Seungwoo Song, Youhan Lee, Haneol Jang, and Kyungtae Lim. 2024.",
                "venue": "arXiv preprint arXiv:2401.06443.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations.",
                "author": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017.",
                "venue": "International journal of computer vision, 123:32\u201373.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Efficient multilingual multi-modal pre-training through triple contrastive loss.",
                "author": "Youhan Lee, KyungTae Lim, Woonhyuk Baek, Byungseok Roh, and Saehoon Kim. 2022.",
                "venue": "In Proceedings of the 29th International Conference on Computational Linguistics, pages 5730\u20135744, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.",
                "url": "https://aclanthology.org/2022.coling-1.504"
            }
        },
        {
            "18": {
                "title": "Otter: A multi-modal model with in-context instruction tuning.",
                "author": "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023a.",
                "venue": null,
                "url": "http://arxiv.org/abs/2305.03726"
            }
        },
        {
            "19": {
                "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
                "author": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023b.",
                "venue": "In International Conference on Machine Learning.",
                "url": "https://api.semanticscholar.org/CorpusID:256390509"
            }
        },
        {
            "20": {
                "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.",
                "author": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022.",
                "venue": "In ICML.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Align before fuse: Vision and language representation learning with momentum distillation.",
                "author": "Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. 2021.",
                "venue": "In NeurIPS.",
                "url": null
            }
        },
        {
            "22": {
                "title": "M3it: A large-scale dataset towards multi-modal multilingual instruction tuning.",
                "author": "Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. 2023c.",
                "venue": "arXiv preprint arXiv:2306.04387.",
                "url": null
            }
        },
        {
            "23": {
                "title": "Microsoft coco: Common objects in context.",
                "author": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014.",
                "venue": "In Computer Vision \u2013 ECCV 2014, pages 740\u2013755, Cham. Springer International Publishing.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Improved baselines with visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.",
                "venue": "arXiv preprint arXiv:2310.03744.",
                "url": null
            }
        },
        {
            "25": {
                "title": "Visual instruction tuning.",
                "author": "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b.",
                "venue": "In NeurIPS.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Ziya-visual: Bilingual large vision-language model via multi-task instruction tuning.",
                "author": "Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing Zhang, Yan Song, and Pingjian Zhang. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2310.08166"
            }
        },
        {
            "27": {
                "title": "Gpt-4 technical report.",
                "author": "OpenAI. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.08774"
            }
        },
        {
            "28": {
                "title": "Klue: Korean language understanding evaluation.",
                "author": "Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Jiyoon Han, Jangwon Park, Chisung Song, Junseong Kim, Yongsook Song, Taehwan Oh, et al. 2021.",
                "venue": "arXiv preprint arXiv:2105.09680.",
                "url": null
            }
        },
        {
            "29": {
                "title": "Instruction tuning with gpt-4.",
                "author": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.",
                "venue": "arXiv preprint arXiv:2304.03277.",
                "url": null
            }
        },
        {
            "30": {
                "title": "How multilingual is multilingual BERT?",
                "author": "Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996\u20135001, Florence, Italy. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/P19-1493"
            }
        },
        {
            "31": {
                "title": "Learning transferable visual models from natural language supervision.",
                "author": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.",
                "venue": "In International conference on machine learning, pages 8748\u20138763. PMLR.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Llama 2: Open foundation and fine-tuned chat models.",
                "author": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.",
                "venue": "arXiv preprint arXiv:2307.09288.",
                "url": null
            }
        },
        {
            "33": {
                "title": "To see is to believe: Prompting gpt-4v for better visual instruction tuning.",
                "author": "Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. 2023.",
                "venue": "arXiv preprint arXiv:2311.07574.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Evaluating object hallucination in large vision-language models.",
                "author": "Kun Zhou Jinpeng Wang Wayne Xin Zhao Yifan Li, Yifan Du and Ji-Rong Wen. 2023.",
                "venue": "In The 2023 Conference on Empirical Methods in Natural Language Processing.",
                "url": "https://openreview.net/forum?id=xozJw0kZXF"
            }
        },
        {
            "35": {
                "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet.",
                "author": "Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. 2021.",
                "venue": "In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 538\u2013547. IEEE.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Vision-language models for vision tasks: A survey.",
                "author": "Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2023.",
                "venue": "arXiv preprint arXiv:2304.00685.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Lima: Less is more for alignment.",
                "author": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023.",
                "venue": "arXiv preprint arXiv:2305.11206.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
                "author": "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.",
                "venue": "arXiv preprint arXiv:2304.10592.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.11399v3",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "6",
            "6.1",
            "6.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.2",
            "5.3",
            "6.1",
            "6.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.11399v3",
        "paper_title": "X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment",
        "research_background": "### Motivation\n\nThe paper is motivated by the evolving capabilities of large multimodal models (LMMs) that can align responses to human intent through visual instruction-following (VIF), as evidenced by recent advancements exemplified in models like LLaVA1.0 and LLaVA1.5. Despite these advancements, existing models face limitations, particularly in the domain of multilingual processing. LLaVA1.0's dataset was limited because it was constructed using a text-only version of GPT-4 and only targeted English. Although LLaVA1.5 incorporated the multilingual instruction dataset ShareGPT, it still lacked vision information. ShareGPT4V, which included vision input, remained restricted to English. Therefore, there is a need to create and optimize LMMs that can effectively handle multilingual VIF.\n\n### Research Problem\n\nThe research problem being addressed is the construction and optimization of a multilingual VIF dataset that can be used to enhance LMMs' performance across different languages. The goal is to overcome the limitations of existing datasets, which either do not incorporate visual information or are restricted to a single language. Specifically, the paper seeks to:\n1. Construct a diverse multilingual VIF dataset targeting multiple languages.\n2. Develop a multilingual LMM, X-LLaVA, which can leverage this dataset to enhance performance in visual question answering and other related tasks.\n\n### Relevant Prior Work\n\n1. **LLaVA1.0 (Liu et al. 2023b  ###reference_b25###):** Proposed a method to automatically create a VIF dataset using GPT-4, showing excellent performance in visual question answering but was limited to text-only inputs and English language.\n2. **LLaVA1.5 (Liu et al. 2023a  ###reference_b24###):** Expanded on LLaVA1.0 by incorporating the multilingual ShareGPT dataset but lacked vision information, only addressing the text-based multilingual instruction following.\n3. **ShareGPT (Chen et al. 2023a  ###reference_b4###):** An instruction-following dataset for LLMs used in LLaVA1.5, but still lacking vision information.\n4. **ShareGPT4V (Chen et al. 2023b  ###reference_b5###):** A VIF dataset created using GPT-4-V which accepts image information as input but was limited to English.\n5. **KoLLaVA:** A previously proposed model that set a baseline for the performance of Korean language LMMs, against which X-LLaVA is compared.\n6. **MiniGPT4, MultiInstruct, InstructBLIP, MultiModalGPT, LVIS-INSTRUCT, M3IT:** Previously proposed datasets focusing on various aspects like daily life, general descriptions, reasoning, and discourses, but with limitations in terms of multilingual and multimodal capabilities.\n\nBy addressing the gaps identified in these prior works, the proposed X-LLaVA aims to achieve a multilingual and visually informed dataset and model that substantially improves upon existing benchmarks in both multilingual and multimodal processing.",
        "methodology": "In the study titled \"X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment,\" the authors propose a method that builds upon the VIF data generation technique utilizing the GPT of LLaVA. To enhance the richness of information from the images and include more detailed data, the researchers have opted to directly input both the image and object information into the GPT4-V model for data construction. This approach aims to reduce information loss that might occur during data processing.\n\nThe key components and innovations of their methodology include:\n\n1. **Direct Input to GPT4-V Model**: Unlike previous methods which might have indirect processing steps, this study inputs both image and object information directly into the GPT4-V model. This step is crucial for preserving intricate details from the images.\n\n2. **Multilingual VIF Datasets (mvif)**: The researchers constructed four types of multilingual VIF datasets in three different languages (English, Korean, and Chinese). These datasets are categorized as follows:\n   - **Object-centric**: Focusing on the objects within the images.\n   - **Location-centric**: Emphasizing the locations depicted in the images.\n   - **Atmosphere-centric**: Concentrating on the atmosphere or environment within the images.\n   - **Conversation**: Including conversational contexts related to the images.\n\nBy integrating these four categories in multiple languages, the study aims to improve and enrich the training and functionality of vision-language models, enhancing their performance in bilingual or multilingual settings.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Datasets:\nThe main experiments for X-LLaVA involve evaluating the system on multiple multilingual datasets. Specific datasets are referred to for both the vision and language components, ensuring comprehensive coverage of the bilingual capabilities promoted by the model. However, the exact names of these datasets are not mentioned in the provided excerpt.\n\n#### Baselines:\nSeveral state-of-the-art vision-language models serve as baselines for their experiments. These baseline models likely include widely used vision-language systems that have been evaluated on multilingual tasks, enabling direct comparison with X-LLaVA\u2019s performance.\n\n#### Evaluation Metrics:\nFor assessing X-LLaVA, multiple quantitative evaluation metrics are employed:\n- **Recall@K**: This metric evaluates the model by checking if the correct item is within the top K retrieved results.\n- **Mean Average Precision (mAP)**: This metric assesses the ranking quality by averaging the precision values at different recall levels.\n- **BLEU and METEOR scores**: These metrics are used to measure the quality of translations or language generation tasks, focusing on precision and harmonic mean of precision and recall, respectively.\n\n#### Main Experimental Results:\nThe quantitative comparisons aim to address three specified research questions. The main results demonstrate the following:\n\n1. **Impact of Vocabulary Expansion**: By expanding the model\u2019s vocabulary to enhance multilinguality, the results show a marked improvement in the ability of the model to handle diverse languages, indicating robustness in bilingual and potentially multilingual contexts.\n   \n2. **Bilingual Training Effect**: Bilingual training allows the model to find a more balanced and integrative relationship between the two languages. This effect is evidenced by better performance metrics when compared to monolingual or non-bilingual adjusted models.\n\n3. **Strengthening through mvif Data**: The use of mvif data (multilingual visual input features) shows significant strengthening of the model in various aspects. The results highlight improvements in Recall@K, mAP, and language generation quality metrics (BLEU and METEOR scores), confirming that mvif data contributes to the model's enhanced multimodal understanding.\n\nThe findings suggest that X-LLaVA achieves superior or competitive results against recognized baselines in multilingual vision-language tasks, supporting the effectiveness of the proposed modifications and training strategies."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the intrinsic impact of vocabulary expansion, pretraining, and proposed dataset on the performance of the X-LLaVA model.",
            "experiment_process": "Three models were trained under distinct conditions: (1) X-LLaVA with vocabulary expansion, knowledge enhancement, and VIT from Sections 4.2 and 4.3; (2) X-LLaVA(-P) without using Wiki for pretraining; and (3) X-LLaVA(-V,-P) using pure LLaMA2 with no vocabulary expansion or Wiki pretraining. Intrinsic evaluations involved comparing the performance metrics of these models, both with and without the addition of mvif data.",
            "result_discussion": "Vocabulary expansion improved the Korean model by 6.1 points and the English model by 0.8 points. Pretraining using Wikipedia enhanced performance uniformly in both languages, with a notable improvement in Korean. Tuning with the proposed dataset (mvif+O) resulted in a performance boost (0.2 to 3 points), indicating that even languages with grammatical differences benefited from the multilingual VIF.",
            "ablation_id": "2403.11399v3.No1"
        },
        {
            "research_objective": "Conduct a comparative performance evaluation of X-LLaVA in Korean and English against other Leading Multimodal Models (LMMs).",
            "experiment_process": "Models including BLIP-2, InstructBLIP, LLaVA1.5, and KoLLaVA were compared. The evaluation metrics included scores on datasets like BVQAk, Koviz, KoLiv for Korean, and VQA, GQA, BVQAe, LV, POPE for English, presented in Table 3.",
            "result_discussion": "X-LLaVA outperformed other models significantly in Korean evaluations with an average of 57.0 points. For English, X-LLaVA had the highest performance in BVQAe and GQA. The multilingual training method did not degrade the performance for English, and X-LLaVA demonstrated superiority in Korean over KoLLaVA by 5.2 points and showed a balanced performance across languages compared to LLaVA1.5.",
            "ablation_id": "2403.11399v3.No2"
        },
        {
            "research_objective": "Compare X-LLaVA's preference rates in Korean and English to other leading models, evaluated using GPT4-V.",
            "experiment_process": "Preference evaluation was conducted using GPT4-V, which compared the performance of X-LLaVA to other models in Korean and English. The preference rates were gathered by comparing answers generated by the models.",
            "result_discussion": "X-LLaVA outperformed other models significantly in Korean (except GPT4-V) with a 19% higher preference rate than KoLLaVA. In English, X-LLaVA scored approximately 25% higher than LLaVA1.5. Although GPT4-V was preferred overall, X-LLaVA was competitively effective in enhancing writing skills. Limiting answer length to 30 words showed significant changes in preference scores.",
            "ablation_id": "2403.11399v3.No3"
        },
        {
            "research_objective": "Assess human preference of X-LLaVA compared to GPT4-V with a word constraint, addressing potential variances in word-length bias.",
            "experiment_process": "Three human annotators were employed to evaluate the answers generated by GPT4-V and X-LLaVA with the length restricted to 30 words. The preference scores were calculated and compared with GPT evaluations.",
            "result_discussion": "Human and GPT evaluations resulted in almost identical preference scores, with GPT evaluations showing more ties, suggesting human annotators have clearer criteria and avoid ambiguity less. This confirms that the vision-language model can enhance human evaluations, rather than substitute them.",
            "ablation_id": "2403.11399v3.No4"
        }
    ]
}