{
    "title": "EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM",
    "abstract": "In e-commerce, accurately extracting product attribute values from multimodal data is crucial for improving user experience and operational efficiency of retailers. However, previous approaches to multimodal attribute value extraction often struggle with implicit attribute values embedded in images or text, rely heavily on extensive labeled data, and can easily confuse similar attribute values. To address these issues, we introduce EIVEN, a data- and parameter-efficient generative framework that pioneers the use of multimodal LLM for implicit attribute value extraction. EIVEN leverages the rich inherent knowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled data. We also introduce a novel Learning-by-Comparison technique to reduce model confusion by enforcing attribute value comparison and difference identification. Additionally, we construct initial open-source datasets for multimodal implicit attribute value extraction. Our extensive experiments reveal that EIVEN significantly outperforms existing methods in extracting implicit attribute values while requiring less labeled data.\u2020\u2020\u2020Work done as an intern at Amazon.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Product attributes are crucial in e-commerce, aiding retailers in product representation, recommendation, and categorization, and assisting customers in product searching, comparison, and making informed purchasing decisions (Xu et al., 2019; Yan et al., 2021; Yang et al., 2023; Shinzato et al., 2023). Despite their importance, the accurate listing of these attributes remains a challenge. Sellers often fail to specify all relevant attribute values or list them incorrectly, leading to inefficiencies and potential customer dissatisfaction Lin et al. (2021); Khandelwal et al. (2023). To address these issues, the task of Attribute Value Extraction (AVE) has emerged as a key area of research in e-commerce. AVE seeks to automate the extraction of attribute values from product profiles such as product titles, descriptions, and images Zheng et al. (2018); Wang et al. (2020, 2022).\n\nExisting approaches for multimodal attribute value extraction can be broadly categorized into three categories: extractive, discriminative, and generative.\n\nMost extractive studies focus on extracting attribute values that are explicitly stated in product text data Zhu et al. (2020); Yang et al. (2022); Li et al. (2023); Xu et al. (2023). However, in real-world scenarios, an attribute value that needs to be obtained may not appear as a subsequence of the product text, but can be inferred from the product image, implied text context, or prior knowledge about this product type Zhang et al. (2023); Khandelwal et al. (2023); Blume et al. (2023). Take products in Figure 1 for example. The value \u201cround neck\u201d of the \u201cneckline\u201d attribute does not appear in product textual information, but can be easily identified from its product image. Similarly, the value \u201crain boot\u201d corresponding to the attribute \u201cboot style\" in the second product is not explicitly stated but is implicitly embedded in its textual context \u201ctransparent waterproof\u201d and visual information. In addition, previous discriminative and generative approaches for multimodal AVE are highly data-hungry, requiring large amounts of labeled data for training but still perform poorly in extracting implicit attribute values Zhang et al. (2023); Fu et al. (2022). Furthermore, similar implicit attribute values are easily confused by the recent generative AVE model Zhang et al. (2023).\n\nTo tackle these challenges, we introduce EIVEN, a data and parameter-efficient multimodal generative framework for multimodal implicit attribute value extraction. EIVEN utilizes the rich inherent knowledge of a pre-trained LLM and vision encoder to lessen reliance on extensive attribute-specific data. Additionally, to address the issue of model confusion caused by similar attribute values, we introduce a novel technique termed \"Learning-by-Comparison\". This approach feeds the model with pairs of instances that share the same attribute but potentially have different attribute values, forcing the model to compare and distinguish them.\n\nOur contributions are summarized as follows:\n\n- To the best of our knowledge, we are the first work to explore multimodal LLM for the emerging real-world problem of implicit attribute value extraction.\n- We propose a novel Learning-by-Comparison technique to reduce model confusion among similar attribute values.\n- We construct initial open-source datasets for multimodal implicit AVE. 111https://github.com/HenryPengZou/EIVEN\n\nExtensive experiments show that our framework greatly outperforms recent multimodal AVE works, even with less labeled data."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "EIVEN Framework",
            "text": "Problem Formulation. Given a product\u2019s image and text context and a specified attribute, our goal is to extract the value for the corresponding attribute. Specifically, in our task of extracting implicit attribute values, the ground truth attribute value does not appear as a subsequence of the text context, but can be inferred from the product image, text context, or prior knowledge. In this work, we formulate the task of extracting implicit attribute values as the problem of generating answers given a question and product information. For example, the question could be \"What is the Sleeve Style of this product?\" and the generated answer could be \"Short Sleeve\" by inferring from the product\u2019s image and text context. Figure 2  ###reference_### presents an overview of our efficient multimodal LLM, and Figure 3  ###reference_### illustrates our Learning-by-Comparison strategies. Next, we explain our key components in detail."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Image Embedding",
            "text": "We leverage projected multi-granularity visual features to serve as the visual token input to our LLM model. Specifically, we extract visual features from the token in every layer of the vision encoder and then concatenate them. A simple visual projection network is used to adapt and transform the visual features to the same dimension as the text embedding of the LLM. Here, the weight matrices of the downsampling and upsampling layer, the bias terms, and the SwiGLU activation function are utilized. In this way, we empower the LLM to understand visual features at multiple levels of granularity, such as edges, textures, patterns, parts, and objects, which enables more effective extraction of attribute values."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Efficient Multimodal LLM",
            "text": "Previous generative works in multimodal implicit attribute value extraction Zhang et al. (2023  ###reference_b35###); Khandelwal et al. (2023  ###reference_b10###) require large amounts of attribute-specific labeled data to achieve good performance. However, in the ever-evolving field of e-commerce, new products with unique attributes and values are constantly being introduced by different retailers and merchants. Gathering a large number of annotations for each new attribute is time-consuming and expensive Yang et al. (2023  ###reference_b33###); Lai et al. (2021  ###reference_b12###); Zou and Caragea (2023  ###reference_b38###); Zou et al. (2023  ###reference_b39###). To reduce reliance on labeled data, we pioneer the exploration of leveraging pre-trained LLMs for the multimodal implicit AVE task. Trained on vast and diverse datasets, LLMs have demonstrated remarkable understanding, generative capabilities, and few-shot transfer learning ability Touvron et al. (2023  ###reference_b26###); Liu et al. (2023  ###reference_b15###); Wang et al. (2023  ###reference_b29###); Tian et al. (2023  ###reference_b25###); Dong et al. (2023  ###reference_b4###); Lai et al. (2024  ###reference_b11###), making them a promising approach to be explored for implicit attribute value extraction.\n\nHowever, LLMs typically comprise billions of parameters, rendering their full-scale fine-tuning both resource-demanding and inefficient. To address this, we resort to parameter-efficient fine-tuning strategies, which has been proven to achieve performance comparable to full fine-tuning but with substantially fewer trainable parameters Hu et al. (2023  ###reference_b9###); Houlsby et al. (2019  ###reference_b8###); Luo et al. (2023b  ###reference_b18###); Tian et al. (2024  ###reference_b24###). Specifically, we insert a lightweight adapter before every attention layer in our LLM. The mechanism of adapters is defined as:\n\n\nwhere  is the input and output of the adapter,  denotes for the downsampling and upsampling layers,  is an optional activation function depending on the choice of adapters.\n\nDuring training, we freeze all parameters in our LLM (LLaMA-7B Touvron et al. (2023  ###reference_b26###)) and the large image encoder, and only fine-tune these inserted lightweight adapters and the visual projection network. Formally, given a product image embedding , text context , and an attribute-related question , the input of our multimodal LLM is denoted as . The overall training objective  of our multimodal LLM can be defined as:\n\n\nwhere  is the batch size,  represents the ground-truth answer,  is the -th token of ,  represents the tokens before ,  denotes all parameters of adapters in LLM, and  denotes all parameters in the visual projection network.\n\n\nIn our training scheme, although we use LLM, thanks to these lightweight adapters, the number of trainable parameters can be kept at a very small scale, e.g., 2~5M. This greatly reduces the memory requirement and allows efficient training of EIVEN on the same single 32G V100 GPU as the previous work Zhang et al. (2023  ###reference_b35###), while achieving significantly better performance even with much less labeled data."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Learning-by-Comparison",
            "text": "Many attributes have very similar attribute values, such as \u2018Crew Neck\u2019, \u2018Scoop Neck\u2019, and \u2018Cowl Neck\u2019, which can confuse models. To help models better distinguish these similar attribute values, we propose a new technique called Learning-by-Comparison (LBC) to assist model training.\n\nDuring training, in addition to the original product information, and the query attribute, we randomly sample another product with the same attribute and include its image and text context in the model input for comparison. We have designed three strategies: LBC_Judge_Last, LBC_Judge_First, and LBC_Better_Instance. We modify the attribute-related question and ground-truth answer accordingly. For example, in LBC_Judge_Last, we first ask the model to identify the value of the query attribute for both products, and then ask the model to compare and determine whether they have the same attribute value. The answer should be in the format of \"First: {attribute value of the first product}; Second: {attribute value of the second product}; {comparison result}\". Through this approach, the model is compelled to distinguish similar attribute values. Note that during the validation and testing phase, only the original product information and the attribute-related question are used."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Open-Source Multimodal Implicit AVE Dataset",
            "text": "Multimodal implicit AVE is an emerging problem, and there is currently a lack of truly open-sourced datasets for multimodal implicit AVE. 222The claimed released multimodal implicit AVE dataset from DEFLATE Zhang et al. (2023  ###reference_b35###) is encrypted, and our multiple attempts to request decrypted data have failed. Existing AVE datasets either do not contain product images or lack implicit attribute values. Thus, in this section, we introduce and make available several datasets to facilitate further research in this area.\nSpecifically, we present three multimodal implicit AVE datasets: Clothing, Footwear, and General. The statistics of these datasets are summarized in Table 6  ###reference_###. All of them are derived and sampled from two publicly available datasets, MAVE Yang et al. (2022  ###reference_b34###) and Amazon Reviews 2018 Ni et al. (2019  ###reference_b20###). There are a total of 68,423 samples that cover 12 diverse product attributes and 87 common attribute values. Specifically, for each product attribute, we randomly collect product instances including the product texts (titles and product categories) and attribute values from the MAVE dataset. We collect popular attribute values with more than 100 instances for effective evaluation and randomly sample up to 1000 instances per attribute value to limit the dataset size. Since the MAVE dataset does not provide product images and is derived from the multimodal Amazon Reviews 2018 dataset, we collect the corresponding product images from the Amazon Reviews 2018 dataset using their shared product identification number. Furthermore, the MAVE dataset contains only explicit attribute values. To evaluate performance on implicit attribute value extraction, we manually removed all explicit attribute value mentions from the product text for each product and its corresponding attribute. Therefore, attribute values in these data can only be inferred from product images, text context, or prior knowledge, i.e., implicit attribute values. Lastly, we split the train, test, and validation sets in a ratio of 0.75:0.15:0.15. We open-sourced these datasets."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiment",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setup",
            "text": "Baselines: We compare EIVEN with representative baselines in multimodal AVE: the latest generative work DEFLATE Zhang et al. (2023  ###reference_b35###), the representative discriminative work CMA-CLIP Fu et al. (2022  ###reference_b6###) and the extractive work M-JAVE Zhu et al. (2020  ###reference_b37###). Detailed descriptions of baselines are provided in Appendix C  ###reference_###. Implementation Details: We select the ViT-B/16 Dosovitskiy et al. (2021  ###reference_b5###) of the pre-trained CLIP Radford et al. (2021  ###reference_b21###) as our image encoder. The multi-granularity visual features contain 4  tokens extracted from every 3 layer of ViT-B/16. We use LLaMA-7B Touvron et al. (2023  ###reference_b26###) as our LLM. The default dimension of the two-layer visual projection network is set to 128, and the dimension of the adapter in LLM is set to 8. LBC_Judge_Last is used as our default Learning-by-Comparison strategy. RepAdapter Luo et al. (2023a  ###reference_b17###, b  ###reference_b18###) is adopted as our LLM adapter in default. We use AdamW Loshchilov and Hutter (2019  ###reference_b16###) as the optimizer and train the model for 15 epochs. During the generation stage, we use top-p sampling as our decoding strategy with the temperature of 0.1 and the top-p value of 0.75."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Performance Comparison with Baselines",
            "text": "As can be seen from these comparison results, EIVEN can deliver significantly better performance on average than the other baseline methods. For instance, EIVEN can surpass the recent generative approach DEFLATE by 18.09%p on the Clothing dataset and 17.20%p on the General dataset. Also, EIVEN is much more data-efficient compared to previous generative attribute value extraction approaches. Using only 100 labels per attribute value, EIVEN can outperform or perform on par with other baselines trained with all labels (i.e., 1000 labels per attribute value) on all three datasets. These results indicate the effectiveness of our efficient multimodal LLM framework with the Learning-by-Comparison technique."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Ablation Study and Analysis",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Effectiveness of Each Component",
            "text": "In order to quantify the impact of each component and modality in EIVEN, we measure and summarize the micro-F1 result of EIVEN after removing different components and modalities in Table 2  ###reference_###. First, we observe that the performance decreases after replacing multi-granularity visual features with the single-granularity feature\nor removing Learning-by-Comparison, suggesting that both of them contribute to the final performance of EIVEN. Notably, the performance of EIVEN-Base is still much better than DEFLATE, justifying the significant benefits of leveraging the LLM for implicit AVE. Besides, we can see that removing either the image or text context can significantly hurt model performance, which demonstrates the necessity of combining all these modalities in the implicit attribute value extraction task. Interestingly, the text modality plays the most important role, even when most of the ground truth attribute values cannot be explicitly identified from the product text. The possible reason is that implicit attribute values can still be inferred from the text context given the strong prior knowledge learned in LLM, as illustrated in the second product in Figure 1  ###reference_###. On the other hand, extracting some product attribute values from images requires fine-grained visual understanding and thus is more challenging, especially when labels are limited."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Learning-by-Comparison Strategies",
            "text": "We explore different Learning-by-Comparison (LBC) strategies as illustrated in Figure 3  ###reference_###. The results of these strategies are presented in Table 3  ###reference_###. It is evident that all three strategies help improve the model\u2019s performance. This validates our motivation that including two instances into the model\u2019s input and asking the model to compare their attribute values can help alleviate model confusion among similar attribute values and improve overall performance. While there is no significant difference in performance among the three strategies, we believe that more effective LBC strategies can be devised to further enhance the model\u2019s performance, and we leave them for future exploration."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Qualitative Examples",
            "text": "Figure 5  ###reference_### demonstrates diverse qualitative examples and responses from the most recent generative work in implicit attribute value extraction DEFLATE and our method EIVEN. Compared to DEFLATE, EIVEN achieves overall better generation results across diverse product categories and attributes. In the first example, EIVEN extracts the correct attribute values for the product\u2019s sleeve style from the product image. In contrast, DEFLATE is confused by the strap in the neckline and generates incorrect answers. In the sixth example, EIVEN demonstrates its ability to infer the correct value \"Rain Boots\" for the attribute \"Boot Style\" from the text context \"Transparent Clear Waterproof Martin\", prior knowledge, and product image. We also visualize some failure cases in the last two examples. We observe that EIVEN can make mistakes when multiple reasonable attribute values exist."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose EIVEN, an efficient generative framework using multimodal LLM for implicit attribute value extraction. EIVEN leverages the rich internal knowledge of pre-trained LLM to reduce reliance on attribute-specific labeled data and adopts lightweight adapters for parameter-efficient fine-tuning of LLM. Besides, to enhance the visual understanding ability of our model, we feed multi-granularity visual features into LLM and propose Learning-by-Comparison strategies to alleviate model confusion among attribute values. We also release the first open-source dataset. Through extensive experiments on three multimodal implicit attribute value extraction datasets, we found that EIVEN can significantly outperform previous works using fewer labels, making it an efficient solution for implicit attribute value extraction."
        }
    ],
    "url": "http://arxiv.org/html/2404.08886v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2404.08886v1",
        "paper_title": "EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM",
        "research_background": "**Motivation:**\n\nThe paper highlights the significant role of product attributes in e-commerce, enhancing tasks such as product representation, recommendation, categorization, searching, comparison, and aiding customers in making informed decisions. Despite their importance, accurate listing of these attributes is often challenging due to sellers not specifying all relevant attribute values or listing them incorrectly. Consequently, this can lead to inefficiencies and potential customer dissatisfaction. The need to address these issues motivates the research into Attribute Value Extraction (AVE), which aims to automate the extraction of attribute values from product profiles like titles, descriptions, and images.\n\n**Research Problem:**\n\nThe core research problem tackled in this paper is the extraction of implicit attribute values that are not explicitly stated in product text data. This problem is particularly challenging as these attribute values can only be inferred from product images, implied text contexts, or prior knowledge about the product type. Existing multimodal AVE approaches, which are highly data-hungry, also struggle with accurately extracting these implicit values and often confuse similar implicit attributes. Therefore, there is a need for a more efficient and effective solution for extracting these implicit attribute values.\n\n**Relevant Prior Work:**\n\n1. **Extractive Studies:**\n   - These focus on extracting attribute values that are explicitly stated in product text data (Zhu et al., 2020; Yang et al., 2022; Li et al., 2023; Xu et al., 2023).\n\n2. **Implicit Attribute Value Extraction:**\n   - Earlier studies indicate that implicit attribute values often need to be inferred from images or implied text context (Zhang et al., 2023; Khandelwal et al., 2023; Blume et al., 2023).\n\n3. **Discriminative and Generative Approaches:**\n   - Previous discriminative and generative approaches require large amounts of labeled data but still perform poorly in extracting implicit attribute values and can confuse similar attribute values (Zhang et al., 2023; Fu et al., 2022).\n\nThe paper proposes EIVEN, a data and parameter-efficient multimodal generative framework leveraging a pre-trained large language model (LLM) and vision encoder to efficiently extract implicit attribute values, and introduces a novel \"Learning-by-Comparison\" technique to mitigate model confusion.",
        "methodology": "### EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM\n\n**Methodology: Problem Formulation**\nGiven a product\u2019s image and text context and a specified attribute, our goal is to extract the value for the corresponding attribute. Specifically, in our task of extracting implicit attribute values, the ground truth attribute value does not appear as a subsequence of the text context, but can be inferred from the product image, text context, or prior knowledge. In this work, we formulate the task of extracting implicit attribute values as the problem of generating answers given a question and product information. For example, the question could be \"What is the Sleeve Style of this product?\" and the generated answer could be \"Short Sleeve\" by inferring from the product\u2019s image and text context.\n\nNext, we explain our key components in detail.\n\n---\n\n### Key Components and Innovations\n\n#### 1. **Efficient Multimodal LLM**\n- This component integrates information from product images and text contexts to generate accurate attribute values. The model is designed to process visual and textual data simultaneously, ensuring a comprehensive understanding of the product.\n- The multimodal nature allows the LLM (Language Model) to leverage both visual cues from the product image and textual information from the product description or other contexts to infer the attribute values effectively.\n\n#### 2. **Learning-by-Comparison Strategies**\n- **Contrastive Learning**: By comparing different product images and text contexts, the model learns to distinguish subtle differences and similarities. This helps in improving the accuracy of attribute value extraction by understanding what specific features correspond to certain attribute values.\n- **Pairwise Comparison**: The model is trained by providing pairs of products with known attribute values, allowing it to learn the distinguishing features that lead to different attribute values.\n- **Triplet Loss**: A specialized form of comparison where the model looks at an anchor product, a positive example (similar product with the same attribute value), and a negative example (different attribute value). This hones the model's ability to identify the correct attribute value by reinforcing correct inferences and discouraging incorrect ones.\n\n### Innovation and Efficiency:\n- **Multimodal Data Fusion**: The model's capability to integrate visual and textual data ensures that it can handle cases where the attribute value is not explicitly mentioned in the text but can be inferred from the image or a combination of both modalities.\n- **Inference from Prior Knowledge**: The model is equipped with mechanisms to utilize prior knowledge, enhancing its ability to infer attribute values even when direct information is not available within the provided text or image.\n\n---\n\nBy leveraging these key components and innovative strategies, EIVEN aims to efficiently and accurately extract implicit attribute values, setting a new standard for multimodal learning tasks.",
        "main_experiment_and_results": "## Main Experiment Setup\n\n### Baselines\nEIVEN is compared against several representative baselines in the multimodal Attribute Value Extraction (AVE) field:\n1. **DEFLATE Zhang et al. (2023)**\n2. **CMA-CLIP Fu et al. (2022)**\n3. **M-JAVE Zhu et al. (2020)**\n\nDetailed descriptions of these baselines are provided in Appendix C of the paper.\n\n### Evaluation Metrics\nThe main evaluation metric used is **micro-F1 (%)**, following the latest work by Zhang et al. (2023). The extraction results are determined to be correct based on an exact match criteria, requiring the entire sequence of words to be correct.\n\n### Implementation Details\n- **Image Encoder:** ViT-B/16 from the pre-trained CLIP by Dosovitskiy et al. (2021) is selected. Multi-granularity visual features are extracted, with 4 tokens taken from every 3 layers of ViT-B/16.\n- **Language Model (LLM):** LLaMA-7B by Touvron et al. (2023) is used as the language model.\n- **Visual Projection Network:** The default dimension is set to 128.\n- **Adapter Dimension in LLM:** Set to 8.\n- **Learning Strategy:** LBC_Judge_Last is the default Learning-by-Comparison strategy.\n- **LLM Adapter:** The RepAdapter by Luo et al. (2023a, b) is employed by default.\n- **Optimizer:** AdamW by Loshchilov and Hutter (2019) is used.\n- **Training:** The model is trained for 15 epochs.\n- **Generation Stage:** Top-p sampling is used for decoding with a temperature of 0.1 and a top-p value of 0.75.\n\n### Main Experimental Results\nThe reported results include micro-F1 scores from a single run for each model. Specific numerical results and further details are omitted in this section but are presumably reported in the main paper body."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To quantify the impact of each component and modality in EIVEN on the extraction of implicit attribute values.",
            "experiment_process": "The micro-F1 result of EIVEN is measured and summarized after removing different components and modalities. Specifically, the study replaces multi-granularity visual features with single-granularity features, removes the Learning-by-Comparison component, and evaluates the performance after removing either the image or text context.",
            "result_discussion": "Performance decreases when either the multi-granularity visual features are replaced or the Learning-by-Comparison component is removed, indicating their importance. Removing text or image context significantly hurts performance, with the text modality being crucial even when implicit values can't be explicitly identified from the text. This suggests that strong prior knowledge in the LLM helps infer implicit attribute values.",
            "ablation_id": "2404.08886v1.No1"
        },
        {
            "research_objective": "To explore and compare different Learning-by-Comparison (LBC) strategies for alleviating model confusion among similar attribute values.",
            "experiment_process": "Different LBC strategies are applied and their results are presented in Table 3. These strategies involve including two instances into the model\u2019s input and asking it to compare their attribute values.",
            "result_discussion": "All three LBC strategies help improve model performance, validating the importance of including comparative inputs to reduce confusion and improve performance. Although no significant difference is found among the strategies, further improvement in LBC strategies is suggested for future work.",
            "ablation_id": "2404.08886v1.No2"
        },
        {
            "research_objective": "To qualitatively compare the generative results of EIVEN with a recent generative work DEFLATE across diverse product categories and attributes.",
            "experiment_process": "Figure 5 demonstrates diverse qualitative examples and the responses from DEFLATE and EIVEN. Examples include both successful extractions of attribute values and observed failure cases.",
            "result_discussion": "EIVEN consistently outperforms DEFLATE in generating correct attribute values across diverse categories. For instance, EIVEN successfully extracts the correct sleeve style from an image while DEFLATE is misled by other elements. In another case, EIVEN infers the correct 'Boot Style' from text and image context, which DEFLATE fails to do. However, the study also acknowledges failure cases where multiple reasonable values exist.",
            "ablation_id": "2404.08886v1.No3"
        }
    ]
}