{
    "title": "LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs",
    "abstract": "Fine-tuning pre-trained large language models (LLMs) with limited hardware presents challenges. Various distributed fine-tuning methods have been proposed to alleviate these constraints. However, determining the most effective method for achieving rapid fine-tuning in a given environment remains unclear. To address this challenge, we introduce LLMem, a solution that identifies the optimal fine-tuning method across multiple GPUs. Experimental results demonstrate LLMem's ability to effectively select distributed fine-tuning methods for LLMs with more than a billion parameters.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Since the introduction of the Transformer model by Vaswani et al. (2017), researchers have proposed numerous language models based on it. As the model's performance has improved, its size has grown exponentially, necessitating a substantial dataset for training. However, training emerging large language models (LLMs) is infeasible without a dedicated infrastructure with high-performance hardware due to memory constraints. Instead, it is preferred to utilize a small dataset to fine-tune a pre-trained model for a specific application.\n\nTo efficiently handle small datasets and reduce training time, the conventional method of data parallelism places the entire model on each GPU, splits the dataset, and trains simultaneously. \n\nTensor parallelism divides each parameter tensor in the model into rows or columns and distributes them to each GPU, using only partitioned parameters on each GPU during computation. For example, Megatron-LM Shoeybi et al. (2019), a representative tensor parallelism method, splits a tensor along its rows or columns considering the position and connection of operators. \n\nAs we described above, various distributed fine-tuning methods have been proposed, and the fine-tuning time required for each is different. For instance, conventional data parallelism provides the shortest fine-tuning time. On the other hand, tensor parallelism has no benefit in saving fine-tuning time. Users may want to select an appropriate method that has a short fine-tuning time. \n\nDNNMem Gao et al. (2020) is the most recent work providing key equations for model training by analyzing the connections between operators and live tensors in the forward and backward passes. However, it has limitations for fine-tuning LLMs. \n\nTo address these challenges, we propose LLMem, a method to estimate the peak memory consumption when applying distributed fine-tuning methods. \n\nLLMem considers several factors for each method, including recombining parameters prior to computation when applying advanced data parallelism and the output driven by all-gather in the backward pass when using tensor parallelism. Additionally, LLMem analyzes the difference in allocation method between the transformer and the lm_head part and reflects it in the estimation.\n\nTo the best of our knowledge, this is the first work to estimate the peak memory consumption for LLM fine-tuning.\n\nIn summary, our contributions are:\n\nWe provide an algorithm to determine the most efficient distributed fine-tuning method. \n\nOur source code repository can be found at https://github.com/taehokim20/LLMem."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "GPU Memory Estimation",
            "text": "There have been several attempts to enhance performance and efficiency in training deep learning models. DNNMem Gao et al. (2020) sequentially traverses the computation graph of a DL model, focusing on efficient management and reuse of computation resources. Our LLMem is inspired by DNNMem, whose mechanism is described in more detail in Section 3. TSplit Nie et al. (2022) also looks at optimizing the workflow by calculating the operational needs for the visiting operator. However, TSplit lacks an explanation of the detailed estimation process and its accuracy. SchedTune Albahar et al. (2022) optimizes resources by considering DL model characteristics as well as different hardware types running the job."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Distributed Fine-Tuning with GPUs",
            "text": "Data parallelism can enhance fine-tuning speed in proportion to the number of GPUs. The ZeRO optimizer Rajbhandari et al. (2020), described in Section 1, is widely used as an alternative.\n\nThe ZeRO optimizer selectively gathers only the model parameters or gradients required during the computation process and utilizes reduce-scatter after the computation to maintain their partitioning on each GPU.\n\nAnother method, tensor parallelism, results in each GPU producing only partial results, necessitating that all GPUs receive the same input data. The widely adopted tensor parallelism method, Megatron-LM Shoeybi et al. (2019), splits each model parameter tensor by row or column. Other proposed methods Xu et al. (2021) Wang et al. (2021) Bian et al. (2021) achieve additional savings by sharding both input and model parameters.\n\nIf constraints cannot be met with any distributed fine-tuning method on GPUs alone, we can use heterogeneous fine-tuning utilizing CPU memory. ZeRO-offload Ren et al. (2021) manages gradients, optimizer states, and optimizer computation on the CPU while retaining parameters and forward and backward computation on the GPU."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Motivation",
            "text": "To select distributed fine-tuning methods, it is crucial to understand the impact of advanced data parallelism mechanisms. Existing approaches often overlook scenarios where techniques like ZeRO Stage 3 optimizer or tensor parallelism are applied across multiple GPUs. This oversight can result in significant estimation errors. \n\nIn this section, we implement the existing DNNMem Gao et al. (2020), validate the implementation results, and discuss factors affecting the accuracy of these methods during the fine-tuning of pre-trained transformer-based language models."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "DNNMem Implementation",
            "text": "DNNMem source codes are not publicly available and are mainly based on TensorFlow, so we implement DNNMem based on the description in the paper Gao et al. (2020). First, we extract the corresponding computation graph from a given pre-trained DL model to identify the output size in each operator based on parameters, batch size (), and sequence length (). The next step is to compute peak GPU memory usage at each operator while traversing the graph. Additionally, we reflect that PyTorch aligns with multiples of 512 bytes for internal tensor fragmentation."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Limitations of DNNMem for LLM Fine-Tuning Memory Estimation",
            "text": "DNNMem Gao et al. (2020  ###reference_b8###) does not handle mixed precision, which is commonly used in fine-tuning pre-trained language models. In addition, it does not consider how memory chunks are managed to ensure that forward pass parameters and backward pass gradients share the same space Fang et al. (2022  ###reference_b7###). Furthermore, DNNMem overlooks extra usage during the initial fine-tuning iteration due to optimizer states."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Single-GPU Memory Usage Estimation",
            "text": "This section outlines considerations for estimating computational performance of transformer-based language models on a single GPU. The symbols used in the explanation are organized in Table 1."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Workflow for Fine-Tuning Pre-Trained Models",
            "text": "Initialization phase. The initialization phase preceding fine-tuning involves allocating memory for the CUDA context, responsible for managing information to control GPU devices, and memory for applying chunk-based memory management Fang et al. (2022  ###reference_b7###). \n\nFine-tuning phase. During the fine-tuning phase, param fp16 goes through forward and backward passes, and param fp16 is converted to gradient fp16, as illustrated in Figure 2  ###reference_###. After the backward pass, the ADAM optimizer updates parameters using optimizer states, including param fp32, momentum fp32, and variance fp32 tensors. Momentum fp32 and variance fp32 tensors consume memory based on the actual tensor size. Subsequently, memory is retained until the fine-tuning process is complete."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Memory Consumption with Structure of Transformer-based Decoder Model",
            "text": "First, the Transformer-based decoder models Vaswani et al. (2017) are largely divided into a transformer model for fine-tuning and lm_head for output, as shown in Figure 3. The part that uses the chunk memory is the transformer model in which the parameters are updated because the input embedding has a large dictionary. Therefore, it is managed separately.\n\nThe system allocates GPU memory based on the actual size of each momentum fp32 and variance fp32, so GPU memory must be calculated for each tensor of each operator. Since the amount of GPU memory consumed by Bias or LayerNorm is very small, they can use space with other memory fragmentation. Therefore, we only calculate the GPU memory usage due to Embedding or Linear operator parameters.\n\nPyTorch provides gradient checkpointing as an option to save memory during fine-tuning. Therefore, we support estimating GPU memory usage due to each operator\u2019s input/output tensors considering gradient checkpointing. Since the output tensors of the current operator are the input tensors of the next operator, we focus on the output. It is challenging to accurately predict GPU memory consumption due to the outputs of operators within a model. We observed that the layer and embedding outputs of the transformer model are kept in GPU memory for efficient gradient checkpointing, which minimizes the increase in fine-tuning time. The estimation error rate is reduced using the equation, which accounts for our observation.\n\nLastly, the lm_head part, including the loss calculation part, converts the transformer outputs into logits. Then, the value obtained by shifting the sequence length of the logits by one space is stored in a separate temporary variable and used for the loss calculation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Multi-GPU Memory Usage Estimation",
            "text": "Tensor parallelism divides the parameter values of each operator by  and does not combine them again, as shown in Figure 5(b). It splits each model parameter tensor by row or column to apply tensor parallelism to multiple pre-trained language models. We call this one-dimension tensor parallelism (1D TP). Let us assume that we apply 1D TP to a linear operation on four GPUs. The linear operator\u2019s equation is , where  is output,  is input,  is params/gradients, and  is bias. The linear matrix multiplication process when each parameter tensor is split into columns is shown in Figure 6. We shard parameters by column because the output size after multiplication is the same as the size of the bias without sharding, so it is not affected by the use of bias. In the backward pass, the fine-tuning goes through an all-gather process.  is the total temporary buffer size for tensors imported from the other GPUs, calculated by multiplying the output size of each layer by the number of layers.\n\nIt is possible to achieve hybrid parallelism by fine-tuning through a combination of data and tensor parallelism."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Distributed Fine-Tuning Method Decision",
            "text": "Algorithm 1 describes the process for selecting the optimal method to fine-tune a pre-trained model based on the estimation results. Here, CDP represents one method, and the remaining estimations are connected to ADP, TP, and DP+TP, respectively. Of these methods, the optimal one is the method that requires the shortest time for fine-tuning.\n\nLLMem takes a pre-trained model , the total number of GPUs to fine-tune, and the maximum sequence length .  is a list that stores the performance evaluation score of each method. , , , and  correspond to CDP, ADP, TP, and DP+TP, respectively. LLMem increments the batch size  for each method to get the performance score. CDP uses a certain amount of data for fine-tuning in one iteration. In addition, since the ZeRO-3 optimizer increases the total communication volume of a baseline DP, the performance score of CDP is considered accordingly. In one iteration, ADP uses a specific volume, TP uses another, and DP+TP uses a different volume of data for fine-tuning.  is the number of GPUs used for DP. These values become the performance scores of each method. Finally, LLMem selects the method with the highest performance score (if the scores are tied, select CDP, ADP, TP, and DP+TP in that order). If the performance scores of all methods are zero, heterogeneous training using CPU memory is selected as an alternative. \n\nInput: Pre-trained model , , and \nOutput: Selected fine-tuning method and the optimal configuration."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we compare the performance of LLMem when applying various distributed fine-tuning methods. In addition, our DNNMem implementation is included in the evaluation."
        },
        {
            "section_id": "7.1",
            "parent_section_id": "7",
            "section_name": "Experimental Setup",
            "text": "In a multi-GPU environment, we employ a Tesla V100 with 4 GPUs in CloudLab CloudLab (2023  ###reference_b5###). We utilize the Colossal-AI framework Li et al. (2023  ###reference_b9###), which is popular for distributed fine-tuning, along with PyTorch 2.0.1 with CUDA 11.7. The models used in the experiment include OPT Zhang et al. (2022  ###reference_b23###), BLOOM Workshop et al. (2022  ###reference_b21###), CodeGen Nijkamp et al. (2022  ###reference_b12###), BioGPT Luo et al. (2022  ###reference_b10###), GPTBigCode Allal et al. (2023  ###reference_b2###), GPT Neo Black et al. (2021  ###reference_b4###), and LLaMA Touvron et al. (2023  ###reference_b17###). The dataset applied is alpaca data Taori et al. (2023  ###reference_b16###), which consists of 52K instruction-following entries."
        },
        {
            "section_id": "7.2",
            "parent_section_id": "7",
            "section_name": "Estimation of Single-GPU Memory Usage",
            "text": "###figure_9###"
        },
        {
            "section_id": "7.3",
            "parent_section_id": "7",
            "section_name": "Estimation of Multi-GPU Memory Usage",
            "text": "CDP. The experimental results are consistent as mentioned in Section 7.2.\n\nADP. We examined the behavior of ADP on multiple GPUs. The error rate in multi-GPU setups can be attributed to differences in execution across GPUs and the structure of larger models, which increase the complexity of allocations.\n\nTP and DP+TP. Our study showed how the 4TP and 2DP+2TP setups influence fine-tuning on four GPUs. In TP, temporary buffers are effectively used during operations, mitigating excess resource demands. Despite larger models and batch sizes, errors remained manageable. In combined DP and TP scenarios, allocations can vary more widely, impacting prediction accuracy."
        },
        {
            "section_id": "7.4",
            "parent_section_id": "7",
            "section_name": "Fine-Tuning Method Selection with LLMem",
            "text": "Table 3 assesses whether LLMem finds the optimal fine-tuning method to achieve the fastest fine-tuning for various models. When measuring the time taken for each method, we applied the maximum batch size. LLMem typically selects TP when DP causes issues. It is challenging for LLMem to choose DP+TP because only 4 GPUs were used in the experiment. DP+TP allows for more diverse combinations depending on the number of GPUs used and is more likely to be selected. LLMem also suggests CPU offloading when needed."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This paper introduces LLMem, a method for estimating resource consumption during fine-tuning of large language models (LLMs) on multi-GPU setups. We analyze factors affecting performance, considering different allocation methods for the transformer and output sections. Experimental results demonstrate that LLMem achieves accurate estimations with minimal error rates."
        }
    ],
    "url": "http://arxiv.org/html/2404.10933v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "3",
            "3.1",
            "3.2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "5",
            "6"
        ],
        "main_experiment_and_results_sections": [
            "7",
            "7.1",
            "7.2",
            "7.3",
            "7.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "3.1",
            "3.2",
            "7",
            "7.1",
            "7.2",
            "7.3",
            "7.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.10933v1",
        "paper_title": "LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs",
        "research_background": "### LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs\n\n#### Motivation:\nThe rapid advances and widespread adoption of the Transformer model, as introduced by Vaswani et al. (2017), have led to a proliferation of large language models (LLMs) that require substantial computational resources to train. However, training these large models from scratch is often infeasible due to hardware constraints, making fine-tuning pre-trained models on specific tasks a more practical approach. This process can result in GPU out-of-memory (OOM) errors because of the large model sizes, even when datasets are small.\n\nTwo prominent methods exist to distribute the training load across GPUs to mitigate memory usage issues:\n1. **Data Parallelism** - This method places the entire model on each GPU and splits the dataset. While reducing training time, it still results in high memory usage.\n2. **Tensor Parallelism** - This method divides each parameter tensor among GPUs, significantly reducing memory usage but not affecting fine-tuning time.\n\nDespite the existence of these methods, there's a notable gap in tools that can accurately estimate GPU memory usage for fine-tuning LLMs in a multi-GPU environment, making it difficult for researchers and practitioners to choose the appropriate method upfront. \n\n#### Research Problem:\nThe primary research problem addressed in this paper is the lack of precise tools and methods to estimate GPU memory consumption when fine-tuning large language models using distributed training methods (data parallelism and tensor parallelism) in multi-GPU setups. Accurate estimation is critical to prevent GPU OOM issues and optimize fine-tuning time. Existing methods, like DNNMem, are limited to single GPU environments and cannot be applied directly to LLMs fine-tuned across multiple GPUs due to differences in memory distribution and consumption patterns.\n\n#### Relevant Prior Work:\n1. **Transformer Model (Vaswani et al., 2017)** - The foundational architecture on which many LLMs are based.\n2. **ZeRO (Rajbhandari et al., 2020)** - Proposes advanced data parallelism by partitioning model components (parameters, gradients, optimizer states) across GPUs to save memory but encounters limitations due to the requirement of all-gather operations during computation phases.\n3. **Megatron-LM (Shoeybi et al., 2019)** - Demonstrates tensor parallelism by splitting tensors along rows or columns to reduce GPU memory usage more effectively than traditional data parallelism.\n4. **DNNMem (Gao et al., 2020)** - The most recent approach to estimating GPU memory usage on a single GPU, providing key equations for this purpose by analyzing the interconnections of operators and live tensors during forward and backward passes. However, it falls short in multi-GPU environments and the specific context of LLM fine-tuning.\n \n#### Proposed Solution (LLMem):\nThe paper proposes **LLMem**, a novel tool for estimating GPU memory consumption when fine-tuning pre-trained LLMs using distributed methods across multiple GPUs. LLMem accounts for various factors such as:\n- **Recombining Parameters** - Before computation in advanced data parallelism.\n- **All-Gather Operations** - During the backward pass in tensor parallelism.\n- **Structural Differences** - Between transformer components and the language modeling head (lm_head) in LLMs.\n\nLLMem provides insightful GPU memory usage predictions with high accuracy (within a 1.6% error rate for single GPU and an average of 3.0% for multi-GPU environments on models with over a billion parameters). \n\n#### Contributions:\n1. An innovative method for estimating GPU memory usage during LLM fine-tuning on single and multiple GPUs.\n2. An algorithm for selecting the most efficient distributed fine-tuning method based on these estimations.\n3. Experimental validation demonstrating LLMem\u2019s superior accuracy compared to existing tools like DNNMem.\n\nThe source code for LLMem is made available at the linked GitHub repository, encouraging further development and practical application of the proposed techniques.",
        "methodology": "The methodology section of the paper proposes a method to estimate the GPU memory usage associated with fine-tuning pre-trained transformer-based language models on a single GPU. The key components and innovations of the proposed method are as follows:\n\n1. **Transformer-Based Language Models**: The focus is on estimating memory usage specifically for transformer-based language models, which are prevalent in tasks related to natural language processing.\n\n2. **Single GPU Consideration**: The methodology is tailored to scenarios where a single GPU is employed for fine-tuning tasks, which is a common setup for many researchers and practitioners.\n\n3. **Symbols and Table**: The explanation uses a set of symbols organized in Table 1. Although Table 1 is referenced, the exact symbols and their meanings are not provided in the provided text.\n\nTo summarize the proposed model or method, it systematically estimates the GPU memory usage by taking into account the architecture and operations specific to transformer-based models, ensuring accuracy and practicality for single GPU environments. The inclusion of symbols likely aims to clarify and support the explanation provided in the methodology.",
        "main_experiment_and_results": "Main Experiment Setup:\n- Dataset: The specific datasets used for model fine-tuning were not detailed in the provided text.\n- Baselines: The comparison is made against ground truth data (actual GPU memory usage) and a previously implemented method called DNNMem for GPU memory usage estimation on a single GPU.\n- Methods: Various distributed fine-tuning methods are employed to assess memory usage.\n- Evaluation Metrics: The primary metric is the peak GPU memory usage estimate compared with the actual measured peak GPU memory usage (ground truth).\n\nMain Experimental Results:\nThe results focus on the comparison between LLMem's estimated GPU memory usage and the actual (ground truth) memory usage. Additionally, comparisons are made with the DNNMem implementation for single GPU scenarios. The goal is to demonstrate the accuracy and efficacy of LLMem's memory usage estimation in different fine-tuning setups and distributed environments, though specific numerical results or performance metrics are not provided in the excerpt."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To select distributed fine-tuning methods, it is crucial to estimate GPU memory usage accurately.",
            "experiment_process": "Implemented DNNMem based on the description in the paper. Extracted the corresponding computation graph from a given pre-trained model to identify the output size in each operator. Computed pre-allocated GPU memory, including CUDA context and weight tensors of the model, before operator execution. Next, computed peak GPU memory usage at each operator while traversing the graph. Accounted for internal tensor fragmentation and memory block management. Validated the implementation by comparing GPU memory estimation results for the BERT model on the GLUE benchmark using PyTorch 2.0.1 with CUDA 11.7 on NVIDIA RTX2060.",
            "result_discussion": "Our DNNMem implementation shows 34.38% and 20.48% error rates in comparison to the DNNMem paper's 31.42% and 19.12% error rates, respectively. The similar error rates suggest that our implementation for single-GPU comparisons is valid.",
            "ablation_id": "2404.10933v1.No1"
        },
        {
            "research_objective": "To demonstrate the limitations of DNNMem for LLM fine-tuning memory estimation and the improvements offered by LLMem.",
            "experiment_process": "Compared the GPU memory usage estimation results of LLMem and DNNMem on a single GPU using various pre-trained LLMs. The estimation was conducted considering factors like mixed precision, shared GPU memory space for forward and backward gradients, and extra GPU memory usage during the initial fine-tuning iteration due to optimizer states.",
            "result_discussion": "LLMem predicts peak GPU memory usage with minimal error rates compared to ground truth, outperforming DNNMem, especially as the total parameter size increases. DNNMem fails to predict GPU memory consumption accurately in distributed fine-tuning across multiple GPUs.",
            "ablation_id": "2404.10933v1.No2"
        },
        {
            "research_objective": "To evaluate the accuracy of LLMem's memory usage estimates and its ability to select appropriate fine-tuning methods while avoiding GPU OOM issues.",
            "experiment_process": "Utilized a Tesla V100 with 4 GPUs, Colossal-AI, and PyTorch 2.0.1 for the experiments. Fine-tuned models like OPT, BLOOM, CodeGen, BioGPT, GPTBigCode, GPT Neo, and LLaMA on alpaca data. Compared the peak GPU memory usage estimated by LLMem and DNNMem for single and multi-GPU setups, using various distributed fine-tuning methods like TP, DP+TP, ADP, and checked ground truth memory usage.",
            "result_discussion": "LLMem consistently predicted GPU OOM for all models and closely approximated the peak GPU memory usage for ground truth data. For multi-GPU setups, LLMem showed slight error increases due to memory usage gaps between GPUs and memory allocator characteristics. The method selection experiment indicated LLMem\u2019s effectiveness in choosing the optimal fine-tuning method while preventing GPU OOM, though it faced challenges with DP+TP due to limited GPU count.",
            "ablation_id": "2404.10933v1.No3"
        }
    ]
}