{
    "title": "Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning",
    "abstract": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues.\n\nOur process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as \u201cacceptable\u201d or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging \u201cAssessment and Plan\u201d section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than physician-authored notes (4.1/5).\n\nOur cost analysis for inference shows that our LLaMA-Clinic model achieves a 3.75-fold cost reduction compared to an external generic LLM service. Additionally, we highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice. We have made our newly created synthetic clinic dialogue-note dataset and the physician feedback dataset publicly available to foster future research.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recent advancements in LLMs have transformed the field of natural language processing (NLP). However, the application of LLMs in the medical domain is still in its early stages. Proprietary LLMs, such as GPT-4 and Med-PaLM, have demonstrated impressive capabilities in medical knowledge and clinical NLP tasks. However, most proprietary LLMs have limited flexibility for domain-specific fine-tuning, primarily due to restricted access to model weights. Additionally, proprietary LLMs raise several concerns pertinent to the healthcare sector, including HIPAA compliance, data security, cost, and transparency of training data.\n\nThe emergence of powerful open-source LLMs has opened up opportunities for domain-specific fine-tuning within the clinical field, yielding promising results. For example, Meditron, a LLaMA-2 model pretrained on a vast corpus of curated medical literature, outperformed GPT-3.5 in the MedQA benchmark. However, most research on open-source models has concentrated on medical knowledge injection rather than practical applications in real-world clinical workflow such as clinical note generation.\n\nIn this work, we address a practical question clinicians face in their everyday routine: How can we best adapt an open-source LLM for the specific use case of clinical note generation? Clinical note documentation represents a significant burden for healthcare practitioners and appears to be a natural application for LLMs, given their remarkable generative capabilities. Recent research on LLMs for clinical text summarization found that LLM-generated outputs are preferred over human summaries for their completeness and accuracy.\n\nThe 2023 ACL ClinicalNLP and CLEFImage workshops explored the generation of clinical notes from patient-doctor conversations using the newly released Ambient Clinical Intelligence Benchmark (ACI-BENCH) dataset. The most notable results were achieved using GPT-4 along with few-shot in-context learning. However, these initial exploratory studies for clinical note generation with LLMs revealed significant limitations:\n\n- Variation in ground-truth quality: There was considerable variation in the quality, format, and style of \u201creference notes\u201d, suggesting that a note similar to a reference note might not necessarily be of high quality for real-world clinical applications.\n- Limited fine-tuning datasets: Previous studies conducted only limited supervised fine-tuning (SFT) of open-source LLMs using small training datasets, thereby not fully exploring their potential for domain- and task-specific adaptation.\n- Lack of advanced training strategy: The potential of data augmentation and reinforcement learning remains unexplored.\n\nIn this study, we revisit the task of outpatient note generation, focusing on adapting an open-source LLM\u2014the LLaMA-2 13 billion parameter model. We thoroughly evaluated techniques for domain- and task-specific adaptation, ranging from continued pretraining and SFT to reinforcement learning informed from both AI and human feedback. This work makes several specific contributions:\n\n- Clinical LLM fine-tuning playbook: We provide a comprehensive guide for healthcare organizations to fine-tune local LLMs on their own data. Our relatively small, open-source LLM achieved expert-level performance, scoring higher (4.2/5) in real-world readiness compared to physician-authored notes (4.1/5) in the \"Assessment and Plan\" section, as evaluated by clinical experts.\n- New algorithm: We are among the first groups to explore the role of reinforcement learning in clinical note generation. We proposed DistillDirect, a strategy to ensure on-policy learning during direct preference optimization (DPO) for model distillation.\n- Cost analysis: Our analysis showcased the order of magnitude inference cost reduction of using an open-source LLM compared to larger proprietary models. With the clear cost reduction and privacy/security benefits, healthcare institutions are well-positioned to adopt open-source LLMs, given their access to proprietary training data and domain expertise.\n- Open datasets: We have made our newly created synthetic clinic dialogue-note dataset and physician feedback dataset publicly available. We also released documentation of our complete trial-and-error process during model development, including strategies for handling technical challenges like training instability and loss spikes.\n- Key considerations for future clinical note-generation tasks: We underscored the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice. We also recommend a workflow for physicians to work with AI-generated notes in real-world practice, ensuring physicians\u2019 full oversight and ultimate accountability for the AI-generated content."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Problem Formation",
            "text": "Given a recorded dialogue from a patient-doctor clinic encounter, we task LLMs to generate a high-quality outpatient note akin to one written by a clinician. This scenario is becoming increasingly prevalent due to the rising popularity of both in-person and virtual scribes, and automatic speech recognition devices in real-world settings. Our research focus is on generating the \u201cSubjective\u201d and \u201cAssessment and Plan\u201d sections of outpatient notes. This decision is based on feedback from our physician coauthors, which indicates that discussing all details of the \u201cObjective\u201d section, such as physical examination results, is impractical during actual clinical encounters.\n\nFurthermore, much of the objective data, including lab and imaging results, are directly integrated into Electronic Medical Records (EMR), making clinical notes generation for those sections easy and sometimes unnecessary. For each dialogue, two separate prompts are sent to the LLM to generate the \u201cSubjective\u201d and \u201cAssessment and Plan\u201d sections, respectively."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Experimental Design",
            "text": "Model Selection: We selected Meta\u2019s LLaMA-2 model, with 13 billion parameters (LLaMA-2-13B), as our foundation model for training [24 ###reference_b24###]. We conducted experiments using both the base and chat models of LLaMA-2. Gemini 1.0 Pro (hereafter referred to as Gemini Pro) by Google was selected as the proprietary model for generating reference notes and acting as the teacher model [25 ###reference_b25###].\n\nExperiments Overview:\nWe demonstrated the experiments pipeline in Figure 1 ###reference_###. Initially, we undertook domain-specific adaptation of LLaMA-2-13B through continued pretraining on discharge summaries from MIMIC-IV. This was followed by task-specific fine-tuning with SFT through instruction tuning. Next, we conducted reinforcement learning from AI feedback (RLAIF) using DistillDirect, our enhanced approach to performing DPO on a distilled dataset. Finally, a panel of physicians evaluated the notes generated by our LLaMA-Clinic model and the notes generated by actual physicians and the Gemini Pro model in a blinded evaluation. All training was performed using Low-Rank Adaptation (LoRA). Detailed experimental methodologies are provided in the Methods section. Additionally, we have documented our complete experimental process in Supplementary Method 1, offering readers a comprehensive view of this complex trial-and-error journey."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Dataset and Preprocessing",
            "text": "Dataset:\nWe used three datasets for our experiments.\n\nACI-BENCH: ACI-BENCH represents the largest clinic dialogue-note dataset publicly available to date, comprising 207 cases. The dataset\u2019s dialogues were crafted by a team with medical expertise, and its clinical notes were initially generated using an automatic note generation system, then reviewed and revised by domain experts. An observation from our study is the notable variation in the format, style, and quality of the \u201creference notes\u201d within ACI-BENCH, especially in the section of \u201cAssessment and Plan\u201d. While this diversity mirrors the reality of clinical practice, where different doctors may produce vastly different notes, it presents a challenge to use these notes as a \u201cgold standard\u201d for training an LLM to replicate. Consequently, we established a simple yet specific note format, recognized as \u201cbest practice\u201d by a panel of licensed internal medicine physicians, to standardize our training approach. For model training, we retained only the dialogue section from ACI-BENCH and employed Gemini Pro to generate notes based on the \u201cbest practice\u201d format, serving as our reference notes. We demonstrated two examples of clinical notes before and after the change in Figure 2. The prompts to Gemini Pro for note generation are provided in Supplementary Method 3. An example of patient-doctor dialogue is provided in Supplementary Method 8.\n\nDialogue-G: To enhance model training via data augmentation, we created a synthetic dataset of clinical dialogue-note pairs using Gemini Pro. This dataset, which we have named Dialogue-G, comprises 1,291 cases. More specifically, we compiled transcribed outpatient notes from the publicly available synthetic MTSamples dataset and utilized Gemini Pro to transform these notes into dialogues. Subsequently, we used these dialogues as inputs for Gemini Pro once again, this time to generate clinical notes based on the \u201cbest practice\u201d format described above. We make the Dialogue-G dataset publicly available. The prompts to Gemini Pro for Dialogue-G creation are provided in Supplementary Method 2.\n\nMIMIC-IV: MIMIC-IV encompasses 431,231 unique hospital admissions from 299,712 patients who were admitted to either the Intensive Care Unit or the Emergency Department of Beth Israel Deaconess Medical Center in Boston, Massachusetts. We utilized discharge summaries from MIMIC-IV for continued pretraining. Notably, the \u201cbrief hospital course\u201d section of the discharge summaries is structurally akin to the \u201cassessment and plan\u201d section in outpatient notes. We compiled a subset of discharge summaries with only the \u201cbrief hospital course\u201d using methods detailed in, referred to as \u201cDischarge-short\u201d. We denoted the complete discharge summaries dataset as \u201cDischarge-long\u201d.\n\nData Split:\nFor continued pretraining, we explored both the Discharge-long dataset (1.2 billion tokens) and the Discharge-short dataset (0.2 billion tokens).\n\nWe combined the training subsets from ACI-BENCH (dialogue n = 67) and Dialogue-G (dialogue n = 1291), then split this data equally for SFT and RLAIF, stratified by the data source. Note for each dialogue, there will be two separate reference notes for \u201cSubject\u201d and \u201cAssessment and Plan\u201d, respectively.\n\nFor RLHF, we utilized dialogues from the training, task2, and task3 subsets of ACI-BENCH (dialogue n = 147) as input prompts and sampled from our model\u2019s outputs for human preference labeling. We excluded data from Dialogue-G to ensure in-distribution training during this final stage of model development. This decision was based on the observed perplexity of 2.79 for Dialogue-G, in contrast to 5.62 for ACI-BENCH, as calculated using the LLaMA-2 chat model after continued pretraining.\n\nWe examined results on the validation subset of ACI-BENCH throughout experiments. The final physician reader study was performed using the ACI-BENCH test1 subset (dialogue n = 40)."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Analysis of Continued Pretraining",
            "text": "We presented the training loss curve in Supplementary Figure 2. Across all experiments, the training loss rapidly decreased after the initial few hundred steps, then leveled off, showing minimal improvement thereafter. The lowest training loss achieved with the Discharge-long dataset is approximately 0.9, whereas, with the Discharge-short dataset, it remained around 1.4. According to scaling laws, a lower training loss is generally expected to correlate with improved performance in downstream tasks [30, 31]. The trajectories of training loss were similar for both the chat and base models. When experimenting with various training strategies and hyperparameters, we frequently observed training loss spikes that were slow to recover, as shown in Supplementary Figure 1. This phenomenon is well-documented in LLM pretraining [32]. We proceeded with models that did not exhibit loss spikes for further studies (i.e., our SFT and RLAIF experiments)."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Analysis of SFT and RLAIF",
            "text": "As the primary objective of SFT and RLAIF is to align the output of LLaMA-2 with that of Gemini Pro, we evaluated the models against reference notes created by Gemini Pro. We reported the results post-SFT and RLAIF, alongside those from the unmodified LLaMA-2, in Table 1. As anticipated, the vanilla chat model outperformed the base model out-of-box. However, continued pretraining with out-of-distribution corpora (i.e., MIMIC-IV discharge summaries) compromised the chat model\u2019s capacity to follow instructions. SFT significantly enhanced performance, particularly for the chat model over the base model. The model\u2019s performance was further boosted by RLAIF using DistillDirect. Notably, our training with DistillDirect frequently encountered instability\u2014a well-known challenge in reinforcement learning. We have detailed our experiments, including ablation studies, to find a stable training setup in Supplementary Method 1.3.\n\nWhen comparing models pretrained with Discharge-long versus Discharge-short, the latter consistently performed better despite a higher training loss in the pretraining phase. The chat model pretrained with Discharge-short emerged as the top performer, and we selected this model checkpoint for the final RLHF stage. Interestingly, the vanilla LLaMA-13B models, directly subjected to SFT and RLAIF without prior continued pretraining, exhibited strong performances, including high scores for \u201cSubjective\u201d.\n\nQualitative analysis for a specific case is presented in Figure 3, aligning with the above quantitative findings. Continued pretraining effectively adopted the style and peculiarities from discharge summaries but at the expense of diminished instruction-following ability and increased hallucinations. The quality of outputs significantly improved post-SFT but remained overly verbose, while RLAIF effectively refined outputs to adhere to the format of reference notes, assisting in reducing hallucinations.\n\nUpon manually reviewing outputs from all model checkpoints post-RLAIF, our physician author noted that the \u201cSubjective\u201d sections were generally of high quality and nearly indistinguishable from notes authored by clinicians. However, the \u201cAssessment and Plan\u201d sections could be improved to more accurately and concisely reflect medical reasoning."
        },
        {
            "section_id": "2.6",
            "parent_section_id": "2",
            "section_name": "Analysis of RLHF",
            "text": "We gathered quantitative feedback from three physician reviewers, who noted that the most frequent issues in the initial round of notes were inaccurate information or hallucinations. Two out of three reviewers observed that the quality of notes significantly improved with fewer hallucinations after one round DPO, describing notes as \u201coften indistinguishable.\u201d Consequently, we conducted only two rounds of DPO, mindful of its time-intensive nature. The example in Figure 3 illustrates that RLHF resulted in more granular changes, building on the results of RLAIF while maintaining the same output structures. We named the model after our RLHF step as LLaMA-Clinic."
        },
        {
            "section_id": "2.7",
            "parent_section_id": "2",
            "section_name": "Analysis of Physician Reader Study",
            "text": "###figure_4### We presented the results from the physician reader study in Figure 4. Two internal medicine physicians and one family medicine physician, in a blinded review, evaluated notes authored by physicians, LLaMA-Clinic, and Gemini Pro based on three criteria: real-world readiness, completeness, and accuracy (Figure 4 A). The median word counts and interquartile ranges (IQR) for notes authored by physicians, LLaMA-Clinic, and Gemini Pro were 118 (IQR: 94-150), 128 (IQR: 108-145), and 128 (IQR: 100-164), respectively. No statistically significant differences in word counts were observed among the three groups (Kruskal-Wallis H test: p = 0.292). We assessed inter-rater reliability (IRR) utilizing Gwet\u2019s AC2 statistics. The AC2 scores for the three metrics ranged from 0.80 to 0.83, signifying a high degree of agreement among reviewers. While minor score variances likely are not practically significant, Gemini Pro achieved the highest scores across all three criteria (Figure 4 B). Remarkably, the majority (90.4%) of the individual evaluations rated the notes generated by LLaMA-Clinic as \u201cacceptable\u201d or higher across all three criteria (Figure 4 C). Furthermore, the overall distribution of scores was also similar among the three groups (Figure 4 C). This aligns with qualitative feedback from two out of three reviewers stating that the quality of notes was, for the \u201cmajority\u201d of the time, indistinguishable among the groups.  \nThe metric of real-world readiness yielded intriguing observations. Physicians were asked to evaluate the notes as though they would be utilized in everyday clinical practice, assuming the physicians would proofread and make necessary edits. This inquiry was designed to assess the readiness of AI-generated notes in a practical workflow that places clinicians at the center and in full control. Compared to notes authored by physicians, LLaMA-Clinic received higher overall readiness scores, especially in the more challenging \u201cAssessment and Plan\u201d section (4.1 vs. 4.0), although it scored slightly lower in accuracy (Figure 4 b). This finding may suggest that physicians might be more tolerant of minor factual inaccuracies in a real-world setting if the notes require fewer edits.  \nWe employed a blinded review process to assess the potential consequences of factual errors. One physician evaluated the likelihood and severity of harm associated with notes that received an accuracy or completeness score below 5 in his review (n = 22, 31 and 39 for Gemini Pro, LLaMA-Clinic, and Physician, respectively). All notes within the LLaMA-clinic and physician groups were assigned scores of \u201cNone\u201d for the extent of harm and \u201cLow\u201d for the likelihood of harm. In contrast, the Gemini Pro group contained a single case rated with \u201cMild to Moderate\u201d extent of harm and a \u201cMedium\u201d likelihood of harm."
        },
        {
            "section_id": "2.8",
            "parent_section_id": "2",
            "section_name": "Cost Analysis for Model Development and Inference",
            "text": "We provided our estimations of both GPU and human costs, measured in hours, for developing LLaMA-Clinic in Figure 5. The majority of compute hours were required during the continued pretraining stage. However, unsurprisingly, the most time-consuming step overall involved physician labeling during RLHF. Importantly, the costs displayed in Figure 5 only consider the training steps directly involved in developing LLaMA-Clinic, and do not include the trial-and-error expenses from all experiments, as detailed in Supplementary Method 1.\n\nLastly, we compared the inference costs associated with deploying an open-source LLM versus those of a proprietary LLM. A complex array of factors influences the total cost of deploying a model for production. These include hardware and software configurations, labor costs associated with constructing, validating, and refining the model, and the implementation of comprehensive security measures to mitigate misuse and enhance threat detection. To ensure a fair comparison, we focused on inference costs. We calculated the inference costs of deploying open-source models in a serverless cloud environment provided by a third-party vendor. We compared these with the costs of using the proprietary models\u2019 APIs. Overall, proprietary models are more expensive than the open-source options. For proprietary models, costs significantly increase with the more advanced models. Similarly, for open-source models, costs increase with larger model sizes, as measured by the number of parameters. LLaMA-Clinic demonstrates a clear price advantage compared to its teacher model, Gemini 1.0 Pro, with a 3.75-fold cost reduction. Assuming one million requests for clinical note generation, the estimated annual inference cost for LLaMA-Clinic is $800 USD, compared to $3,000 USD for Gemini 1.0 Pro. Moreover, the amount mentioned is for the inference cost for one type of note. The total cost of supporting all types of notes will be significantly higher, but the relevant cost difference should remain the same."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "Potential of Training LLMs for the Medical Domain:\nOur study highlights the potential and feasibility of training a smaller open-source LLM for the important task of generating clinical notes. With domain- and task-specific training steps, we have shown that even with a relatively small dataset and limited computing resources, it is possible to enhance an open-source LLM to perform comparably to physicians in this task. Distinguishing from prior research [19  ###reference_b19###, 20  ###reference_b20###, 5  ###reference_b5###], our work differs in several aspects including the choice of model, training dataset, training strategies, and a focused alignment with real-world physician preferences.\nOpen Source vs. Proprietary Models:\nAdapting an open-source LLM offers several benefits. Access to model weights enables flexible domain adaptation, including continued pretraining, SFT, and reinforcement learning as demonstrated in this study. These models can be hosted within healthcare institutions\u2019 firewalls, allowing full control over model weights and mitigating concerns around data leakage and HIPAA violations. Our simplified cost analysis demonstrates that open-source models generally offer a notable cost advantage for deployment compared to larger proprietary LLMs. Furthermore, models with smaller weights often achieve higher throughputs and reduced latency on the same computing infrastructure at inference time [35  ###reference_b35###].\nTackling the Note-Generation Task with LLMs:\nOur methodology to adapt LLM for clinical note generation revealed several key considerations. We initiated this process by establishing a \u201cbest practice\u201d note format informed by a consensus among our physician authors. This approach tackles the variability in the style, format, and quality of physician notes, which could otherwise compromise the training of LLMs. Instead of relying on the LLM to identify the most effective note structure, we advocate for healthcare providers to set these standards themselves. It is important to acknowledge that the \u201cbest practice\u201d note format employed in this study reflects the consensus of a group of internal medicine physicians within a single organization. This format may not be generalizable to other specialties or other organizations, such as orthopedic surgery. Future work that explores specialty-specific best practices for clinical note documentation could be highly beneficial. Furthermore, we recommend a workflow in which providers critically review and refine AI-generated notes prior to their filing. Under such a workflow, physician preferences may shift toward notes requiring fewer revisions and edits, with a higher tolerance for minor inaccuracies. This strategy emphasizes the critical role of physicians in meticulously evaluating all AI-generated content, thereby maintaining accountability for the resulting clinical records.\nExploration of Best Practices to Adapt Open-souce LLM:\nThe process of training a domain- and task-specific open-source LLM is inherently complex, involving numerous engineering decisions without a clear consensus on best practices. These uncertainties span a range of decisions, including, but not limited to, selecting from various open-source models (e.g., LLaMA-2 [24  ###reference_b24###], Mixtral [36  ###reference_b36###], Gemma [37  ###reference_b37###]), choosing between base and chat models, determining the most effective training corpora, selecting data for SFT and reinforcement learning, and exploring extensive combinations of hyperparameters. We have documented our complete experimental journey in Supplementary Method 1 to provide readers with insight into this trial-and-error process. Below, we discuss several key technical considerations encountered during model training.\nContinued Pretraining (CP) vs. Supervised Finetuning (SFT):\nContinued pretraining of an LLM using a domain-specific corpus is recognized for enhancing performance on downstream tasks [38  ###reference_b38###, 39  ###reference_b39###]. This phase is considered a knowledge injection process, given that LLMs acquire the vast majority of their knowledge during the pre-training phase [40  ###reference_b40###, 41  ###reference_b41###]. Several clinical LLMs that underwent continued pretraining with medical corpora, such as PubMed literature, combined with SFT have shown significant improvements in medical knowledge benchmarks [16  ###reference_b16###, 14  ###reference_b14###, 42  ###reference_b42###].\nHowever, a critical distinction exists between tasks focused on medical knowledge (e.g., answering USMLE questions) and those aimed at clinical note generation. Notably, GatorTronGPT [43  ###reference_b43###], the only LLM trained from scratch using EMR data from real patients to date, performed lower in both MedQA and PubMedQA compared to other clinical LLMs [16  ###reference_b16###, 14  ###reference_b14###, 39  ###reference_b39###]. This outcome indicates that EMR data alone may lack comprehensive medical knowledge. For the task of note generation, we hypothesized that continued pretraining with clinical notes could offer benefits by introducing greater lexical variance, unique semantic patterns, and diverse formatting similar to prior work [44  ###reference_b44###].\nOur experiments did not conclusively demonstrate the anticipated benefits of continued pretraining, as the out-of-the-box LLaMA-2 model without continued pretraining achieved the highest ROUGE-1 scores after SFT and RLAIF (Supplementary Table 3). We opted to proceed with the continued pretrained model for RLHF due to subtle peculiarities observed upon manual inspection, though these differences were minor (Supplementary Method 1.3). Given the significant time and computational resources required for continued pretraining, its utility, particularly with clinical notes, merits further exploration in future work.\nData Selection for Continued Pretraining:\nAnother potential factor in the less impressive improvement from continued pretraining may be attributed to the variance in data distribution between discharge summaries and outpatient notes. To address this, we performed experiments focused on a condensed version of discharge summaries, hypothesizing that the \u201cbrief hospital course\u201d section would contain data of higher quality than the complete discharge summary. Indeed, models trained on the \u201cbrief hospital course\u201d section outperformed those trained on the full summaries (Table 1  ###reference_###). Interestingly, we observe that during the pretraining stage, models trained on full summaries achieved lower training losses (see Supplementary Figure 2). However, this did not lead to better performance in the downstream task. We speculate that the structured nature of the full discharge summaries, which include sections such as laboratory results and medication lists, presents more straightforward learning targets for the model. This allows it to achieve lower training losses, which do not necessarily translate into improved task performance. This observation underscores the necessity for thorough analysis of the data used for pretraining.\nRLAIF and RLHF:\nThe most notable performance improvement was observed during the RLAIF and RLHF stages. The DPO step has considerably streamlined the reinforcement learning process by eliminating the need for training an explicit reward model. However, best practices for implementing DPO are yet to be established. Our approach, which stages reinforcement learning using DPO into RLAIF and RLHF, draws inspiration from recent advancements in distilled DPO, adversarial preference optimization, and online AI feedback [45  ###reference_b45###, 46  ###reference_b46###, 47  ###reference_b47###]. It offers several enhancements (Figure 6  ###reference_###). First, for each iteration of RLAIF with DistillDirect, we used the target policy\u2019s outputs as \u201creject samples\u201d to inform feedback, ensuring on-policy learning as opposed to the off-policy training in previous work [46  ###reference_b46###]. Second, during the RLHF phase, we gathered human preferences on responses generated by the target policy, promoting online and on-policy training. With carefully selected training hyperparameters, DistillDirect may achieve performance improvements without overfitting, despite the limited training data and repetitive use of the same prompts in each training cycle (see Supplementary Table 4). Prior work suggested that RLHF has robust generalization capabilities, even when facing distribution shifts between training and test data, albeit at the expense of output diversity [48  ###reference_b48###].\nLimitations:\nOur study serves as a proof of concept and encounters limitations, notably the scarcity of publicly available patient-doctor dialogues for model training, with the latest ACI-BENCH dataset comprising fewer than 300 cases. Additionally, the effectiveness of outpatient note generation is fundamentally linked to the content of patient-doctor dialogues. Although the ACI-BENCH data may have synthesized dialogues to encapsulate all necessary information for composing a comprehensive note, such ideal conditions may not always reflect real-world scenarios. For instance, time constraints may prevent physicians from discussing every detail of medical reasoning with patients, potentially degrading the quality of the generated notes due to the suboptimal input dialogue.\nOur research was also constrained by limited academic computing resources and the availability of physician evaluators, which restricted our ability to conduct extensive hyperparameter searches or additional rounds of RLHF. Our final evaluation was conducted with only three physicians due to the task\u2019s time-intensive nature. Ideally, future developments would involve experiments with the latest powerful open-source models, continued pretraining on in-domain and in-distribution data (e.g., outpatient notes rather than inpatient discharge summaries), SFT and RLAIF on a larger amount of real patient-doctor dialogues, RLHF until a performance plateau is observed, and evaluation by a diverse group of medical professionals.\nConclusions:\nOur research underscores the potential of training an open-source LLM for outpatient note generation with promising clinical applicability. The domain- and task-specific adaptation processes, such as continued pretraining, SFT, and reinforcement learning, are uniquely feasible with open-source models. Healthcare institutions are in a privileged position to undertake such endeavors, given their access to extensive EMR data and a wealth of domain expertise critical for implementing RLHF. Our work was based on fewer than 1,500 patient-doctor dialogues and limited physician preference data. When implementing a similar project in a healthcare institution, these training data could be scaled up by several orders of magnitude, likely boosting performance further. Furthermore, the prospect of conducting similar work for other clinical note-generation tasks, such as creating discharge summaries for hospitalized patients, is particularly exciting."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": ""
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Details of ACI-BENCH Subsets",
            "text": "ACI-BENCH comprises five data subsets: train, validation, test1, test2 and test3. Test1 and test2 correspond to the test sets from ACL ClinicalNLP MEDIQA-Chat 2023 TaskB and TaskC, respectively. Test3 corresponds to TaskC of CLEF MEDIQA-SUM 2023. Given the scarcity of publicly available clinical dialogue-note datasets, we used the train, test1, and test3 subsets for various stages of model development in our study. The blinded clinical reader study was performed on the subset of test1. We anticipate that beyond the proof-of-concept study presented here, healthcare institutions could feasibly develop models using several orders of magnitude more real patient records."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Low-Rank Adaptation (LoRA)",
            "text": "We used LoRA to train LLaMA-2-13B models for all phases of training. LoRA is a method that involves freezing the pre-trained model weights and only training a small percentage (<1%) of the model weights, i.e., by incorporating trainable rank decomposition matrices into each layer of the transformer architecture.\n\nAs a quick summary, let us assume that we have the original weight matrix. LoRA works by adding a low-rank matrix to the original weight matrix:  where  and . , so the matrices  are limited by a lower rank , reducing the need to train all the parameters. Training is only performed on this , and original model weights are kept the same. We then scale  by , where  is a constant in .\n\nTraining Details: In all training steps, LoRA parameters were configured with  set to 8, an  of 32, and a dropout rate of 0.05. All attention blocks were included in the Lora target modules."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Continued Pretraining",
            "text": "Continued pretraining extends the initial pretraining process of an LLM, using the same autoregressive objective to maximize the likelihood of predicting the next token. For LLaMA-2-13B, this objective is next token prediction, based on all previously predicted tokens. Research from LIMA indicated that the majority of knowledge embedded in LLMs is acquired during the pretraining stage [41  ###reference_b41###]. In the process of domain adaptation, it is a common practice to employ pretraining or continued pretraining using domain-specific corpora to improve performance in downstream tasks [16  ###reference_b16###, 39  ###reference_b39###]. In our work, we experimented with continued pretraining on the MIMIC-IV discharge summaries.\n\nTraining Details:\nWe followed the training scripts outlined in Meta\u2019s official LLaMA recipe repository [50  ###reference_b50###]. We employed mixed-precision training with a batching strategy of packing and a context length of 4096 tokens. We utilized Fully Sharded Data Parallel (FSDP) on either 4 Nvidia A6000 or 4 Nvidia A100 GPUs. We maintained a batch size of 4 during training with a gradient accumulation step of 1. Consistent with LLaMA-2, we set a peak learning rate of 3e-4 for the continued pre-training stage. The AdamW optimizer with a cosine learning rate scheduler was used, and the model was trained for one epoch. The exponential moving average of training loss as shown in Supplementary Figure 2 was calculated using the pracma package from R with a window size of 250 [51  ###reference_b51###]."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Supervised Finetuning (SFT)",
            "text": "We implemented SFT in the format of instruction tuning, utilizing the ACI-BENCH (with our newly created reference notes) and Dialogue-G datasets. Instruction tuning plays a crucial role in the task-specific adaptation of an LLM, facilitating the model\u2019s learning of specific tasks through guided instructions. In this approach, each sample consists of a prompt and an answer, with our study employing patient-doctor dialogues as prompts and clinical notes as answers. SFT employed the same autoregressive objective as continued pretraining, but losses on tokens from the prompt were zeroed out, consistent with the approach used in LLaMA-2.\n\nTraining Details: We used a similar experiment setup as continued pretraining, including following LLaMA-recipes to perform mixed precision training on 4 GPUs using FSDP. For SFT, we selected the batching strategy of padding and trained on 3 epochs. Consistent with LLaMA-2, we set a peak learning rate of 2e-5. We truncate prompt (including instruction and dialogue) at a max length of 3000 tokens, and truncate note to 1000 tokens. We set a value of -100 for labels on prompt tokens to zero out losses from prompts."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Reinforcement Learning with AI Feedback (RLAIF)",
            "text": "Here, we review related works on direct preference optimization and distilled direct preference optimization. Subsequently, we introduce our enhanced approach, termed DistillDirect, utilized in this study for RLAIF.\n\nDirect Preference Optimization (DPO): Reinforcement Learning with Human Feedback (RLHF) has been instrumental in the success of LLMs like InstructGPT. However, conventional RLHF strategies, such as proximal policy optimization (PPO), are marked by their technical complexity and the high demand for computational resources. The emergence of direct alignment techniques, notably DPO, has greatly streamlined this process by eliminating the need for training a reward model. DPO begins by collecting a preference dataset, where for each prompt, there is a preferred answer and a rejected answer. Following the notations, DPO optimizes the language model (target policy) using a specific loss function. This loss function increases the probability of the latest model generating preferred responses over the original model and decreases the probability of generating rejected responses over the original model.\n\nDistilled Direct Preference Optimization (distilled DPO): The original distilled DPO methodology, applied in training, involves generating a collection of responses for each prompt from various LLMs. These responses are then evaluated by a teacher model (e.g., GPT-4) to provide preference feedback. Applying RLAIF with distilled DPO has yielded positive outcomes, notably enhancing intent alignment. However, distilled DPO\u2019s reliance on a pre-collected preference dataset renders it suboptimal due to the off-policy and offline characteristics, which are elaborated more in the next section.\n\nOnline vs. Offline Training and On-policy vs. Off-policy Training: These terms describe the nuances in the creation of a preference dataset. For any given prompt, initially, two responses are generated from an LLM. These responses are then assessed for preference by humans or AI, being labeled as preferred and rejected. In this context, training is considered on-policy if the responses are sampled from the latest version of the LLM during RLAIF training. Off-policy training indicates otherwise. Learning is deemed online if the preference labeling and training is conducted in real-time, directly on the outputs from the currently trained policy. It is considered offline if preference labeling and training are performed in separate, discrete steps.\n\nGiven the significant time and financial costs associated with collecting preference data, utilizing pre-collected preference datasets, such as those employed in distilled DPO, is a common practice. Defined by the criteria mentioned above, this method falls under the category of offline, off-policy learning. A critical caveat of employing a pre-collected preference dataset for offline and off-policy training is the occurrence of distribution shifts. More specifically, distribution shifts arise because the preferred and rejected responses are sampled from a policy that differs from the current policy. Research has shown that online RLAIF systems, which are designed to mitigate these distribution shifts, significantly surpass the effectiveness of offline DPO methodologies.\n\nRLAIF using DistillDirect:\n\nIn our study, we introduce an improved approach based on distilled DPO, termed DistillDirect, specifically designed to ensure on-policy learning on a distilled dataset. In each training cycle, we begin by sampling a response from the current policy, ensuring that the learning process remains strictly on-policy. This sampled response is then designated as the rejected response, while a reference response from the teacher model is considered the preferred outcome. This approach implicitly assumes that the response from the current model is generally less favored than that from the teacher model\u2014an assumption that we validated through manual review in each round of training.\n\nTraining details:\nWe utilized the trl library from Huggingface to conduct DistillDirect. Due to computational limitations, experiments were conducted on a single Nvidia A100 GPU with 80GB of graphics memory. To optimize memory usage, pure BF16 training was utilized with a micro-batch size of 1 and gradient accumulation steps of 8. Following a limited learning rate search detailed in Supplementary Method 1.3 and Supplementary Table 4, a learning rate of 5e-6 was chosen. The optimizer used was paged_adamw_32bit. Within the DPOTrainer class, we set the beta hyperparameter to 0.1 and passed None to ref_model. Three rounds of DistillDirect were performed, with each round involving one epoch of training.\n\nText generation was implemented using the Transformers library. When generating \u201crejected\u201d samples from the latest model checkpoint, we consistently applied do_sample=True, top_p=1.0, temperature=1.0, top_k=50, repetition_penalty=1.2, and use_fast_kernels=False. The maximum number of newly generated tokens was set to 1000."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Reinforcement Learning from Human Feedback (RLHF)",
            "text": "In the concluding phase of our training process, we select the most effective model from the preceding stage for further online and on-policy learning using DPO (Figure 6 ###reference_###C).\n\nPhysician Preference Data Collection:\nIn each round of DPO, for a specific prompt, three responses are generated and evaluated by our physician reviewers. Three licensed internal medicine physicians are tasked with providing preference feedback by selecting the most and least preferred responses, with criteria focusing on clinical readiness, correctness, and adherence to the desired format. A notable adaptation in our approach is that reviewers are also instructed to make minor adjustments to improve the quality of the preferred responses, such as correcting factual inaccuracies. Detailed instructions for collecting preference data are available in Supplementary Method 4.\n\nTraining details:\nFor RLHF, we employed an experimental setup analogous to that described in RLAIF. We conducted two rounds of DPO on human preference data. Diverging from the approach taken in RLAIF, we executed three epochs of training in each DPO round due to the limited size of the dataset.\n\nBased on the findings from ablation studies detailed in Supplementary Method 1.3 and Supplementary Table 5, we opted for a lower temperature setting at this stage. In the initial round of DPO, we generated three responses using the same configuration, including a temperature setting of 0.6, for preference labeling. In the second round of DPO, we diversified the temperature settings, resulting in three responses with temperatures set at 0.6, 0.4, and 0.2, respectively, for preference labeling."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Physician Reader Study",
            "text": "The three internal medicine physicians engaged in preference data collection were tasked with writing clinical notes based on conversations from the ACI-BENCH test1 subset, adhering to the pre-defined \u201cbest practice\u201d format. AI-generated notes were produced by LLaMA-Clinic and Gemini Pro, employing identical generation-related hyperparameters (temperature of 0.2, top_p of 0.7 and top_k of 40). To ensure consistency in presentation across all notes, we implemented a basic post-processing step. This step standardized aspects like style, spacing, and capitalization to minimize any formatting discrepancies between human-authored and model-generated notes.\n\nThe physician-authored notes, alongside those generated by Gemini Pro and LLaMA-Clinic, were reviewed by three additional physicians, who were not involved in the preference labeling. These licensed physicians, who specialize in general internal medicine or family medicine, boast rich experience in outpatient practice. The notes were presented in a random order, anonymized to remove any identifying information, and labeled as note 1, note 2, and note 3 to mask the origin of each note from the evaluators. Before assessing the notes, evaluators were instructed to read the entire patient-provider conversation. They were then asked to rate the quality of each note across three criteria: \u201caccuracy,\u201d \u201ccompleteness,\u201d and \u201creal-world readiness.\u201d For each criterion, a scoring system from 1 to 5 was used, ranging from very poor to very good, with higher scores reflecting superior quality. Specifically for \u201creal-world readiness,\u201d evaluators were prompted to consider the scenario of integrating AI-generated clinical notes into their daily practice, including the necessity to proofread and potentially edit these notes before filing them into medical records. Detailed instructions and the scoring rubric are available in the Supplementary Method 5.\n\nFor the harm evaluation, we followed the same approach as in [5, 58] by asking one physician reviewer to assess all notes with an accuracy or completeness score of less than 5 and answer two questions related to the extent and likelihood of harm. The instruction for harm evaluation can be found in Supplementary Method 6."
        },
        {
            "section_id": "4.8",
            "parent_section_id": "4",
            "section_name": "Statistical Analysis",
            "text": "The non-parametric Kruskal-Wallis H test was selected to compare differences in word counts among the three-note groups, utilizing the scipy package in Python [59  ###reference_b59###]. We measured IRR using Gwet\u2019s AC2 statistics implemented through the irrCAC package in R [60  ###reference_b60###, 61  ###reference_b61###]. We reported results with quadratic weights for Gwet\u2019s AC2, as this approach is reliable for ordinal data against the grey zones [62  ###reference_b62###]. Due to our relatively small reviewer pool, we chose not to conduct statistical significance testing on the physician reader study, aligning with practices observed in the deep learning community [24  ###reference_b24###, 63  ###reference_b63###]."
        },
        {
            "section_id": "4.9",
            "parent_section_id": "4",
            "section_name": "Model Development Cost Estimation",
            "text": "In Figure 5  ###reference_###A, we provide cost estimations for the training steps directly involved in the development of LLaMA-Clinic. These costs should be viewed as minimal estimates and will likely fall short of the actual budget requirements since they do not include the trial-and-error expenses from various experiments detailed in Supplementary Method 1, such as trialing different models, conducting hyperparameter searches, and debugging. The hours for continued pretraining are based on training using the Discharge-short dataset. For GPU hours, we accounted for the total number of Nvidia A100 GPU hours utilized. For example, if the continued pretraining stage requires 12 hours using FSDP on four A100 GPUs, we calculate this step as requiring 48 hours. For physician labeling hours, we asked physicians to estimate the average time they spent on the tasks."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "4.10 Model Inference Cost Estimation",
            "text": "To facilitate a fair and apples-to-apples comparison, we calculated the inference costs for both proprietary and open-source models based on API calls. In this context, the total annual inference cost is calculated as follows:\nHere,  represents the total annual inference cost.  denotes the price per input token, while  refers to the average number of input tokens per request. Similarly,  indicates the price per output token, and  represents the average number of output tokens per request. The term  stands for the total number of annual requests.\n\nFor open-source models, we assumed deployment on Fireworks.ai, a company that offers serverless inference for customized LLMs. We sourced pricing information from the websites of Google AI [64  ###reference_b64###], OpenAI [65  ###reference_b65###], and Fireworks.ai [66  ###reference_b66###] in May 2024 for on-demand API calls. Detailed pricing information can be found in Supplementary Method 7. We assumed an average of 3,000 input tokens and 1,000 output tokens per request for clinical note generation. This estimation likely contains redundancy and leaves room for prompt engineering, given that in a cohort of real-world family medicine clinical encounters, the average lengths per dialogue and note are 1505 and 683 tokens, respectively [23  ###reference_b23###].\n\nAn important consideration for production is ensuring adequate throughput for LLMs. As an example, Gemini 1.0 Pro has a rate limit of 360 requests per minute, while LLaMA-Clinic, deployed with the \"Developer\" plan from Fireworks.ai, allows 600 requests per minute. We consider this default rate limit acceptable for our calculations, as shown in Figure 5  ###reference_###. For example, 1 million annual requests translate into an average of approximately 5.7 requests per minute, using the formula:\nassuming an 8-hour workday. However, this calculation does not account for peak demand, which would necessitate system redundancy in a production environment. In addition, there are other technical factors to consider for deployment in production, such as latency and throughput variance [35  ###reference_b35###], which were not included in our analysis."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Data availability",
            "text": "Access to MIMIC-IV can be requested at https://physionet.org/content/mimiciv/, which requires a signed safe usage agreement. ACI-BENCH dataset can be accessed at https://github.com/wyim/aci-bench. Dialogue-G, physician preference data, and the final test data can be found at https://github.com/hanyin88/llama-clinic."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Code availability",
            "text": "Scripts for this work were written in Python. They are available with accompanied documentation at https://github.com/hanyin88/llama-clinic."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Acknowledgement",
            "text": "This research was supported by NSF awards SCH-2205289, IIS-2034479, and SCH-2014438. The funder played no role in the study design, data collection, analysis, and interpretation of data, or the writing of this manuscript. This research is also supported by the Mayo Clinic Health System Southwest Minnesota Region Protected Research Grant."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Competing interests",
            "text": "The authors declare no competing interests."
        },
        {
            "section_id": "9",
            "parent_section_id": null,
            "section_name": "Author Contributions",
            "text": "H.W. designed, conducted, and analyzed the results of experiments. H.W. and C.G. wrote the original draft. J.S. obtained funding and computing resources for the project. L.B., Q.X., and H.G. provided human preference labeling for RLHF and wrote notes for the physician reader study. M.E., K.I., and H.K. conducted the physician reader study. H.K. also performed the harm evaluation. H.W. and C.O performed and validated inference cost analysis. All authors contributed to the conceptualization of the research questions. All authors reviewed, revised, and approved the final manuscript."
        }
    ],
    "url": "http://arxiv.org/html/2405.00715v4",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.1",
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "2.5",
            "2.6",
            "2.7",
            "2.8"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "2.4",
            "2.5",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2405.00715v4",
        "paper_title": "Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning",
        "research_background": "### Motivation\n\nThe paper is driven by several compelling factors:\n\n1. **Transformative Potential of LLMs**: Recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, but their application in the medical domain remains nascent.\n  \n2. **Limitations of Proprietary LLMs**: While proprietary LLMs like GPT-4 and Med-PaLM show impressive capabilities in clinical NLP tasks, they come with constraints such as limited flexibility for domain-specific fine-tuning, issues of HIPAA compliance, data security, cost, and transparency of training data.\n\n3. **Promise of Open-Source LLMs**: The advent of powerful open-source LLMs offers opportunities for domain-specific fine-tuning in the medical field, presenting a cost-effective and flexible alternative for practical applications such as clinical note generation.\n\n4. **Clinical Relevance**: Clinical note documentation is a significant burden for healthcare practitioners, making it a natural application for the generative capabilities of LLMs.\n\n### Research Problem\n\nThe primary research question the paper seeks to address is: \"How can an open-source LLM be best adapted for the specific use case of clinical note generation?\" The study aims to tackle several challenges:\n\n- Ensuring the generated clinical notes are of high quality and meet real-world clinical application standards.\n- Exploring effective techniques for domain- and task-specific adaptation of open-source LLMs.\n- Overcoming limitations in current research, such as reliance on lexical similarity metrics for evaluation, variations in ground-truth reference notes, and limited fine-tuning datasets.\n- Utilizing advanced training strategies, including reinforcement learning, which remain largely unexplored in this context.\n\n### Relevant Prior Work\n\nThe paper draws on and builds upon several streams of prior research:\n\n1. **Capabilities of Proprietary LLMs**: Studies demonstrating the effectiveness of GPT-4 and Med-PaLM in clinical tasks, though with noted limitations in flexibility and compliance [3 ###reference_b3###, 4 ###reference_b4###, 5 ###reference_b5###].\n\n2. **Open-Source LLMs in Medical NLP**: Previous work on open-source models like Meditron, which performed well on benchmarks but mostly focused on medical knowledge injection rather than practical clinical applications [11 ###reference_b11###, 12 ###reference_b12###, 13 ###reference_b13###, 14 ###reference_b14###, 15 ###reference_b15###, 16 ###reference_b16###].\n\n3. **Clinical Text Summarization**: Research indicating that LLM-generated summaries can be preferred over human ones for completeness and accuracy [5 ###reference_b5###].\n\n4. **Workshops on Clinical Note Generation**: The 2023 ACL ClinicalNLP and CLEFImage workshops that initiated clinical note generation from patient-doctor conversations, revealing limitations in metrics, ground-truth quality, and fine-tuning approaches [19 ###reference_b19###, 20 ###reference_b20###, 21 ###reference_b21###].\n\n5. **Reinforcement Learning in LLMs**: General advancements in the NLP field suggesting the potential of reinforcement learning, though underexplored in medical applications. \n\nBy addressing these gaps, this study seeks to enhance the practical utility of LLMs in clinical settings, offering a robust framework for the adaptation and deployment of open-source models for clinical note generation.",
        "methodology": "The proposed method seeks to leverage open-source large language models (LLMs) to generate high-quality outpatient notes from recorded patient-doctor dialogue, akin to notes written by clinicians. This is particularly relevant given the increasing use of both in-person and virtual scribes, along with automatic speech recognition devices in healthcare settings.\n\n### Key Components of the Method:\n\n1. **Focus Areas**: The methodology concentrates on generating the \u201cSubjective\u201d and \u201cAssessment and Plan\u201d sections of outpatient notes rather than the \u201cObjective\u201d section. This decision is based on feedback from physician co-authors who indicated that detailed discussion of the \"Objective\" section, which includes physical examination results, is impractical during clinical encounters.\n2. **Objective Data Handling**: Much of the \u201cObjective\u201d data, such as laboratory and imaging results, are often directly integrated into Electronic Medical Records (EMR). Therefore, generating clinical notes for these sections is relatively straightforward and sometimes unnecessary.\n3. **Usage of Prompts**: For each patient-doctor dialogue, two separate prompts are created\u2014one for generating the \u201cSubjective\u201d section and another for the \u201cAssessment and Plan\u201d section. These prompts are then fed into the LLM to generate the respective parts of the clinical note.\n\n### Innovations:\n\n- **On-Policy Reinforcement Learning**: The methodology incorporates on-policy reinforcement learning to adapt the LLM for the specific task of generating expert-level clinical notes. This adaptation process ensures the model continually improves based on the feedback received during its application.\n- **Cost-Effectiveness**: A key aim of this approach is to make the clinical note generation process cost-effective, leveraging open-source models rather than more expensive, proprietary solutions.\n  \nThis combination of prompts management and reinforcement learning adaptation can potentially streamline the generation of different sections of clinical notes, focusing on the most clinically valuable parts while leveraging modern AI techniques to enhance efficiency and accuracy.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup\nThe primary experiment aimed to align the output of the LLaMA-2 model with that of Gemini Pro for clinical note generation, using Supervised Fine-Tuning (SFT) and Reinforcement Learning with AI Feedback (RLAIF). The evaluation metric used was the ROUGE-1 score, which measures lexical similarity to reference notes created by Gemini Pro. A ROUGE-1 score above 0.5 is generally considered good. The main datasets involved were:\n\n- **MIMIC-IV**: Used for continued pretraining.\n- **Discharge-long**: A long form of discharge summaries.\n- **Discharge-short**: A more concise form of discharge summaries.\n\nBaseline models included:\n- **Unmodified LLaMA-2**: The base model without any adaptations.\n- **Vanilla chat model**: The base model specifically tuned for dialogue without incorporation of domain-specific data initially.\n\n#### Main Results\n- **Unmodified LLaMA-2 vs. Vanilla Chat Model**: The vanilla chat model outperformed the base model, demonstrating better instruction-following capabilities out-of-the-box.\n- **Performance after Continued Pretraining**: Incorporation of MIMIC-IV discharge summaries for continued pretraining impaired the chat model\u2019s ability to follow instructions.\n- **Impact of SFT**: Supervised Fine-Tuning (SFT) significantly enhanced performance, particularly for the chat model. SFT mitigated the adverse effects of pretraining on out-of-distribution corpora.\n- **Boost from RLAIF**: Reinforcement Learning with AI Feedback (RLAIF) using DistillDirect further improved the model's performance, although training instability, a known issue in RL, was frequently encountered.\n\nManual reviews by physician authors highlighted the high quality of \u201cSubjective\u201d sections across models; however, the \u201cAssessment and Plan\u201d sections required further improvement for accuracy and conciseness in medical reasoning."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the impact of continued pretraining on model performance and stability within the task of generating clinical notes from patient-doctor dialogues.",
            "experiment_process": "The training loss curve across different experimental setups was analyzed. Models were pretrained using two datasets: Discharge-long and Discharge-short. Spikes in training loss were noted, and models without such spikes were used for further studies, namely supervised fine-tuning (SFT) and reinforcement learning with AI feedback (RLAIF).",
            "result_discussion": "The lowest training loss was achieved with the Discharge-long dataset, approximately 0.9, whereas the Discharge-short dataset saw a training loss of around 1.4. Despite these differences, both chat and base models exhibited similar loss trajectories. Training loss spikes posed recovery challenges, which were documented in supplementary figures. Stability issues with various training strategies were acknowledged, linking the SFT and RLAIF experiments directly to models that bypassed these instabilities.",
            "ablation_id": "2405.00715v4.No1"
        },
        {
            "research_objective": "Align the output of LLaMA-2 with that of Gemini Pro through Supervised Fine-Tuning (SFT) and Reinforcement Learning with AI Feedback (RLAIF), and evaluate the model's lexical similarity to reference notes.",
            "experiment_process": "Used ROUGE-1 scores to measure lexical similarity against reference notes created by Gemini Pro. Evaluated the performance of vanilla LLaMA-2 vs. modified versions post-SFT and RLAIF. Various training datasets (Discharge-long and Discharge-short) and checkpoint selections were explored for their impact on model performance.",
            "result_discussion": "Unmodified LLaMA-2 chat models initially outperformed base models. However, continued pretraining with out-of-distribution corpora hindered the chat model's instruction-following abilities. SFT significantly enhanced performance, particularly in chat models, while RLAIF further refined the outputs, reducing hallucinations and improving format adherence. The Discharge-short dataset consistently yielded better performance despite higher pretraining loss. Post-RLAIF, the quality in the 'Subjective' sections was noted to be high while 'Assessment and Plan' sections required further improvement.",
            "ablation_id": "2405.00715v4.No2"
        },
        {
            "research_objective": "Introduce and validate DistillDirect as an improved method for Reinforcement Learning with AI Feedback (RLAIF) for clinical note generation.",
            "experiment_process": "DistillDirect method was implemented, ensuring on-policy learning by sampling responses from the current policy model and comparing them against teacher model responses (Gemini Pro). Training involved three rounds of DistillDirect on a single Nvidia A100 GPU, with specified learning rates, optimizers, and hyperparameters. Performance was benchmarked against both offline, off-policy, and online, on-policy RLAIF setups detailed in comparative analyses.",
            "result_discussion": "DistillDirect maintained strictly on-policy training, validated by manual review. Results showed positive implications for on-policy training, offering improvements over conventional distilled DPO methods. Computational constraints were addressed with optimizations in the training setup, including using the trl library and specific configurations for memory and performance. Empirical benchmarks supported enhanced model performance in clinical note generation.",
            "ablation_id": "2405.00715v4.No3"
        }
    ]
}