{
    "title": "Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian",
    "abstract": "In this paper, we focus on generating a synthetic question answering (QA) dataset using an adapted Translate-Align-Retrieve method. Using this method, we created the largest Serbian QA dataset of more than 87K samples, which we name SQuAD-sr. To acknowledge the script duality in Serbian, we generated both Cyrillic and Latin versions of the dataset. We investigate the dataset quality and use it to fine-tune several pre-trained QA models.\n\nThe results show that our model exceeds zero-shot baselines, but fails to go beyond human performance. We note the advantage of using a monolingual pre-trained model over multilingual, as well as the performance increase gained by using Latin over Cyrillic. By performing additional analysis, we show that questions about numeric values or dates are more likely to be answered correctly than other types of questions. Finally, we conclude that SQuAD-sr is of sufficient quality for fine-tuning a Serbian QA model, in the absence of a manually crafted and annotated dataset.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Extractive Question Answering (usually referred to as Question Answering or QA) is one of the most common tasks in Natural Language Processing (NLP). It assesses the Reading Comprehension (RC) ability of a machine by setting an objective to extract the required information from a text provided in natural language. Given the context and question, the goal is to extract a span from the context which represents the best possible answer to the posed question. Contrary to Generative Question Answering, Extractive Question Answering performs the extraction of the answer without changing word order and form in the extracted span.\n\nIn recent years, there has been enormous progress in every aspect of NLP. With the introduction of Transformers, many models which compare to or even exceed human performance have been released. By introducing new fine-tuning techniques and scaling model size up to more than a trillion parameters, current models act as an all-in-one solution to all NLP tasks. However, due to the computational requirements for these Large Language Models (LLM), smaller models that focus on a single task remain relevant to this day. In this paper, we focus on Transformer-based models with parameter counts ranging from 110M to 270M, enabling the possibility to train and run these models without supercomputers.\n\nDue to its omnipresence in both the real and digital world, English is used as the default language in NLP research. Except for other languages with large speaker populations such as Chinese or French, there is a substantial gap in model performance on low-resource languages, regardless of the specific NLP task. Additionally, aside from unlabeled natural text, there are little to no datasets suitable for fine-tuning a model for a specific task. The absence of labeled high-quality data aggravates the process of releasing an NLP model for low-resource languages. To acquire a dataset comparable to existing English ones in terms of both size and quality, a lot of investment is required. Hiring crowd workers to manually label the data takes time and results in significant costs. For those reasons, the idea is posed to craft a synthetic dataset based on already existing ones.\n\nIn this paper, we fine-tune a QA model in Serbian. To overcome the issue of data shortage, we turn to synthetic dataset generation methods. We follow the idea of the Translate-Align-Retrieve method from previous research to which we propose changes to make it more suitable for Serbian. By applying the method to SQuAD v1.1, we obtain a dataset that we utilize to fine-tune several models. We analyze the dataset and compare the performance of acquired models on the XQuAD dataset translated to Serbian.\n\nThe contributions of this work are as follows:\n- We publish the largest question answering dataset in Serbian.\n- We release the best-performing model acquired from our research.\n- We perform extensive analysis on both the dataset and the fine-tuning design choices and report the conclusions."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "II Related work",
            "text": "Undoubtedly, English is the\nlanguage with the most available resources for extractive question answering. The most prominent question answering dataset is the Stanford Question Answering Dataset (SQuAD) v1.1 [6  ###reference_b6###], often used as a benchmark for pre-trained models. Datasets such as NewsQA [8  ###reference_b8###] and Natural Questions [9  ###reference_b9###] follow a similar structure and open domain property, as opposed to domain-specific datasets such as SubjQA [10  ###reference_b10###]. There are also monolingual datasets for some of the other languages with large speaker communities\u2014SberBERT [11  ###reference_b11###] in Russian or FQuAD [12  ###reference_b12###] in French\u2014which have all been collected by employing crowd workers.\nThe advancement of available solutions has prompted researchers to make the QA task more difficult, by adding samples with unanswerable questions to the datasets. An example is SQuAD v2.0 [13  ###reference_b13###], which contains over 50K unanswerable questions. However, in this research we focus on the simpler version of the problem, leaving unanswerable questions handling as future work.\nSome attempts to address the issue of data scarcity in languages other than English have been made by crafting multilingual question answering datasets, such as MLQA [14  ###reference_b14###] or XQuAD [7  ###reference_b7###]. These datasets are intended to be used primarily for assessing zero-shot cross-lingual question answering, but due to the lack of other datasets, some researchers evaluate their solutions on portions of these datasets that contain samples in the required language. Since these datasets cover only a handful of languages\u2014neither of which is Serbian\u2014we manually translate XQuAD to Serbian and use it to evaluate our models.\nOther researchers have also explored the idea of crafting a synthetic dataset to tackle the data scarcity issue. In [15  ###reference_b15###], the authors use only samples in which answer translation can be matched to a span of a context translation. Mozannar et al. [16  ###reference_b16###] utilize synthetic dataset to train a weak QA system, and manually crafted dataset for further fine-tuning. In [15  ###reference_b15###], edit distance is utilized to find context spans that correspond to translated answers. Finally, [5  ###reference_b5###] proposes Translate-Align-Retrieve (TAR) method which utilizes word alignments to extract answer span from the context. We recognize the TAR method as the most suitable for Serbian, due to frequent form changes in the answer translation, and further adapt it to increase the synthetic samples quality.\nTo the best of our knowledge, the QA task in Serbian hasn\u2019t been tackled directly. However, besides the zero-shot approach, we can use multi-purpose Large Language Models (LLMs) such as Llama-2 [17  ###reference_b17###]. Although these models prove to be somewhat effective, their size makes them unusable on the average computer. In this paper, we turn to the approach of fine-tuning a BERT-like pre-trained model to acquire an effective Serbian QA system.\nRegarding pre-trained models, several multilingual variants have been released in previous research [18  ###reference_b18###, 4  ###reference_b4###]. Also, a BERT variant trained on Serbo-Croatian macro-language [3  ###reference_b3###] named BERTi\u0107 has been released publicly. We test the impact of the pre-trained model by fine-tuning both monolingual and multilingual variants and report the results."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "III Methodology",
            "text": "Our work is split into two main parts: dataset synthesis and model fine-tuning. By performing a method described in Chapter III-A  ###reference_### we acquire the largest Serbian QA dataset, which we name SQuAD-sr. Addressing the script duality property of Serbian, we acquire the dataset in both Cyrillic and Latin. Fine-tuning provides us with several models which we evaluate and report the results in Chapter V  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "III-A Dataset Synthesis",
            "text": "To obtain the synthetic dataset, we start with the data provided in SQuAD v1.1. This is a publicly available QA dataset with more than 87K high-quality samples in its train split. The data is crowdsourced by having manual workers provide questions and answer spans from contexts scraped from Wikipedia. The dataset is often used [4  ###reference_b4###, 18  ###reference_b18###, 19  ###reference_b19###] for fine-tuning and evaluation of pre-trained models, due to its quality, size, and variety of topics included.\nUsing the Translate-Transliterate-Align-Retrieve method inspired by [5  ###reference_b5###], we convert each triple  from English to Serbian , where  and  represent context, question and answer, respectively."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "III-A1 Translate",
            "text": "Before the translation, we split each context into single sentences to improve the translation quality. We use Punkt sentence tokenizer implementation in NLTK toolkit [20  ###reference_b20###]. Sentences are then processed by trimming whitespaces. We translate all article titles, contexts, and questions from the dataset. It\u2019s important to note that answers acquired by translating  are unsuitable because answers are required to keep the same form as in the given context. In later steps, we extract answers from .\nTo perform the translation, we utilize the publicly available Neural Machine Translation (NMT) model NLLB-200-1.3B from Meta AI [21  ###reference_b21###]. When translating to Serbian, the model outputs sentences in Cyrillic, which is suspected to affect the model quality. To address this issue, we introduce a Transliterate step, which we mark as optional."
        },
        {
            "section_id": "3.1.2",
            "parent_section_id": "3.1",
            "section_name": "III-A2 Transliterate",
            "text": "Fine-tuning a model on Cyrillic text might negatively impact the results because pre-trained models are usually trained on a much larger portion of text in Latin. Since in Serbian Cyrillic and Latin bear equal status, we introduce Transliterate step to obtain the sentences in Latin and mark\nit as optional. In Chapter V  ###reference_### we investigate and discuss the impact of the script on the evaluation results.\nTo perform the transliteration, we utilize the publicly available Cyrillic-transliteration module [22  ###reference_b22###]."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "III-A3 Align",
            "text": "To extract  from  we need information about word-to-word correspondence for each sentence in  and . The task of finding such correspondence is named Word Alignment and is considered a legacy subtask of word-based Statistical Machine Translation (SMT). It consists of finding which word from a sentence in  acts as a translation of each word from a sentence in , and vice versa.\nTo obtain word alignment results we utilize Efficient Low-Memory Aligner (eflomal) [23  ###reference_b23###], a publicly available unsupervised word alignment tool based on Gibbs sampling with a Bayesian extension of the IBM alignment models. Before applying eflomal, we create word tokens for each sentence using TreeBank tokenizer from the NLTK toolkit [20  ###reference_b20###], and therefore structure the data according to the required format.\nTo further improve the alignments we apply grow-diag-final-and heuristics to the eflomal results. We use the implementation from the fast-align repository [24  ###reference_b24###]."
        },
        {
            "section_id": "3.1.4",
            "parent_section_id": "3.1",
            "section_name": "III-A4 Retrieve",
            "text": "The final step utilizes previously acquired translations and alignments to obtain the synthetic dataset. We employ a specific strategy depending on the text sequence - context, question, or answer.\nContexts.\nTranslated contexts are obtained by concatenating corresponding individually translated sentences. Additionally, we compute mappings  and  which map the first character index of each word to the word index, and the word index of each word to the first character index, respectively.\nFinally, to extract the answer span from  context alignments are required. These are computed from previously acquired sentence alignments of individual sentence pairs contained in  and . For each sentence pair, we increment its sentence alignments by the total number of word tokens contained in previous sentence pairs. Naturally, we count English word tokens to increment English alignments, and Serbian word tokens to increment Serbian alignments.\nQuestions. We use question translations from the translation step without any alterations.\nAnswers. We leverage previously computed context alignments, as well as  and  to extract the answers from the contexts. First, we use  to get word indices of each word contained in . Then, for each word index, we employ context alignment to determine the corresponding Serbian word index to which each word from  is mapped. If no words from  are mapped to Serbian we drop the sample. By calculating the smallest and the largest indices of Serbian words, we determine the words which are contained in . Finally, we employ  to extract the first character index of the first word."
        },
        {
            "section_id": "3.1.5",
            "parent_section_id": "3.1",
            "section_name": "III-A5 SQuAD-sr",
            "text": "The method above provides us with a synthetic dataset containing 87175 examples\u2014only 424 examples less than SQuAD v1.1. We compare it to SQuAD v1.1 and Serbian XQuAD split in terms of size and average sequence length and report the results in Table I  ###reference_###. We note the differences between average context lengths. The context length difference between SQuAD v1.1 and the Latin version of SQuAD-sr arguably stems from the differences between English and Serbian. The difference between average context lengths of Latin SQuAD-sr and Serbian XQuAD indicates longer contexts in XQuAD, and consequently harder evaluation examples.\nTo assess the dataset quality, we manually inspect a portion of SQuAD-sr. We focus on examining samples that significantly deviate from the mean in terms of context, question or answer length, as well as additional 500 random samples.\nThe usage of the NMT system is noticeable in both contexts and questions. In some samples translation error also propagates to the answer, making it poorly translated. By inspecting long contexts, questions, and answers, we notice that the NMT model sometimes tends to output the same token or short sequence of tokens multiple times.\nThe answer extraction method yields decent results. However, by inspecting samples with short answers, we notice that the method sometimes extracts punctuation marks instead of the desired word.\nDespite the occasional errors, we are satisfied with the final state of SQuAD-sr and use it as is to fine-tune our models."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "III-B Fine-tuning",
            "text": "When developing NLP systems, fine-tuning a pre-trained model for downstream tasks is considered standard practice. We apply this approach by utilizing SQuAD-sr to fine-tune neural networks for question answering. We consider several Transformer-based models [1  ###reference_b1###], which differ in the approach and data used in pre-training. We select two multilingual models - mBERT [18  ###reference_b18###] and XLM-R [4  ###reference_b4###], as well as BERTi\u0107 [3  ###reference_b3###] which can be considered monolingual since it has been trained on the Serbo-Croatian macro-language. Fine-tuning is performed using model and training implementations in the HuggingFace ecosystem."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "IV Experimental setup",
            "text": "To properly investigate the influence of design choices, we standardize the fine-tuning process. The same hyperparameters presented in Table II are used for each experiment. We use Nvidia Tesla P100 GPU and train each model for approximately 4 hours.\n\nFirst, we compare our models to the ones we fine-tuned using the zero-shot learning approach. Then, the influence of selecting a monolingual vs. multilingual model is examined. Finally, we investigate how the choice of script (Cyrillic vs. Latin) affects the performance of our model.\n\nFor our metrics, we use F1 Score, which is the most common metric in extractive question answering. F1 Score is more forgivable and allows a partial overlap. We use the implementation of the available metric from HuggingFace."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "IV-A Evaluation Dataset",
            "text": "Unfortunately, there are no datasets suitable for the evaluation of Serbian QA systems. To overcome this issue, we select a multilingual dataset named XQuAD, which consists of 1190 samples extracted from SQuAD v1.1 dev split professionally translated to 12 different languages, and translate it to Serbian. We translate contexts using Google Translate and manually evaluate and correct them. Questions are manually translated without the help of any machine translation software. In our opinion, evaluating the models on this dataset provides us with reliable scores which can be used to compare the models."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "The results are presented in Table III  ###reference_###. We achieve the best performance with BERTi\u0107-SQuAD-sr with 82.97 % F1 on the Latin version of Serbian XQuAD. On Cyrillic, the best model is XLM-R-SQuAD-sr with 78.08 % F1 Score. The results provide insight into the dataset quality, as well as the influence of script and pre-trained model selection."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Comparison against zero-shot models and human baseline",
            "text": "To acquire the baseline models, we fine-tune mBERT and XLM-R on SQuAD v1.1 using the same procedure as our other models. The models are evaluated on Serbian XQuAD split in Latin. We note the slight improvement of XLM-R over mBERT, which is in accordance with [4  ###reference_b4###].\n\nAll our models achieve better results than the model baselines. This confirms the quality of SQuAD-sr, in both its Cyrillic and Latin forms, and justifies the idea of using a synthetic dataset to fine-tune a QA model. The biggest improvement over mBERT and XLM-R is achieved by Latin BERTi\u0107-SQuAD-sr in terms of F1 score.\n\nDue to the lack of human performance results in Serbian, we compare our models to English results on SQuAD v1.1 reported in [6  ###reference_b6###]. Humans outperform our model, leaving room for further development of Serbian QA models."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Monolingual vs. multilingual pre-trained models",
            "text": "To investigate the impact of the choice between the monolingual and multilingual pre-trained model, we fine-tune multiple models on our SQuAD-sr. For the monolingual model, we select BERTi\u0107 [3 ###reference_b3###], the best-performing monolingual model in Serbian. We test it against mBERT and XLM-R. The results show a significant performance increase when using the monolingual model fine-tuned on the Latin dataset. Similar results are shown in [12 ###reference_b12###], confirming the importance of having a monolingual model for fine-tuning."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Cyrillic vs. Latin script",
            "text": "Finally, we show the difference in results acquired by fine-tuning models on Cyrillic and Latin datasets. We achieve performance increases in all models when fine-tuning on the Latin dataset. The main reason is arguably the fact that all considered models use a much smaller portion of Cyrillic data for pre-training, which affects both vocabulary building and representation learning stages of pre-training. The largest difference is reported on BERTi\u0107-SQuAD-sr with an increase on the Latin dataset. The reason this model performs significantly worse on Cyrillic is presumably the vocabulary size of only 32K tokens, which is in our opinion not enough to address the Cyrillic data. Other models have larger vocabulary sizes helping them to address the Cyrillic data. Additionally, they use Russian text among others for pre-training, which makes the Cyrillic portion of the data much larger and results in more learned Cyrillic tokens."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Analysis",
            "text": "To gain better insight into the model\u2019s errors, we follow a similar performance analysis approach as in [12]. We divide samples contained in Serbian XQuAD into categories based on the question type. We use seven categories: Who (Ko, Koji, Koje, Koja), What (\u0160ta), How (Kako), When (Kad, Kada), Where (Gde), How many (Koliko, Koliki, Kolika) and Other. We run the experiment using BERTi\u0107-SQuAD-sr on the Latin version of the dataset and report the results.\n\nOur model achieves the best performance on questions containing words When and How many. This is expected, due to the question\u2019s unambiguity and well-established answer forms. When-questions clearly refer to dates and How many-questions require numeric value as an answer\u2014these relations are arguably easy to learn which results in much larger scores than the average. We also note the same average answer length of predictions and targets.\n\nOn the other hand, the model performs significantly worse on What- and Where-questions. What-questions are arguably more difficult than others because they require much deeper reading comprehension, resulting in lower scores. Poor performance on Where-questions comes as a surprise at first, due to question unambiguity\u2014they usually refer to a location. Although the form isn\u2019t as strict as in When-questions, the difference between the scores is much larger than expected. We believe that poor performance on these questions stems from two factors: answers to these questions are usually in genitive which makes them harder to be recognized by the model, and they are usually preceded by a preposition which is not always extracted. In our opinion, these obstacles cannot be overcome with a synthetic training dataset.\n\nWe note interesting results on How-questions. This is due to the property of Serbian to pose questions using How (Kako) to ask for a name or a location (Kako se zove\u2026?, Kako glasi\u2026?, etc.). Contrary to Where-questions, How-questions referring to a location don\u2019t require answers in the genitive, which results in much higher scores.\n\nFinally, we note the positive correlation between answer length and performance\u2014our model achieves better performance on questions that require shorter answers. This behavior is expected and justifies the usage of answer length as a question difficulty indicator."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "VI Conclusion",
            "text": "In this work, we presented SQuAD-sr, the largest QA dataset in Serbian with more than 87K samples. By focusing on synthetic dataset crafting methods, we managed to acquire SQuAD-sr much faster and with almost no investments, while still maintaining a reasonable quality. We utilized it to fine-tune several QA models. Our experiments showed that monolingual pre-trained models are more suitable than multilingual ones, and that the Latin dataset is better for fine-tuning those models. Finally, by performing a question-type classification, we gained an insight into what questions can be asked with more confidence about the answer. Finally, we make both Latin and Cyrillic versions of the dataset publicly available, as well as BERTi\u0107-SQuAD-sr, our best-performing model."
        }
    ],
    "url": "http://arxiv.org/html/2404.08617v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.1.4",
            "3.1.5",
            "3.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.08617v1",
        "paper_title": "Synthetic Dataset Creation and Fine-Tuning of Transformer Models for Question Answering in Serbian",
        "research_background": "### Motivation:\nThe motivation for this paper originates from the growing field of Natural Language Processing (NLP) and the specific task of Extractive Question Answering (QA). This task plays a critical role in assessing machine reading comprehension by requiring the extraction of precise information from given texts. However, despite significant progress in NLP, much of the advancements and datasets have been predominantly centered around high-resource languages like English. This leaves a substantial performance gap for low-resource languages such as Serbian, particularly due to the lack of labeled high-quality datasets that are essential for fine-tuning models for specific tasks.\n\n### Research Problem:\nThe primary research problem addressed in this paper is the shortage of suitable datasets for fine-tuning QA models in Serbian, which is a low-resource language. Creating such datasets manually is both time-consuming and costly, making it impractical. Therefore, the paper proposes and evaluates a synthetic dataset generation method to overcome this shortage, aiming to produce a high-quality, large-scale dataset comparable to existing English datasets.\n\n### Relevant Prior Work:\n1. **Transformers**: The baseline for modern NLP modeling, introduced by [1 ###reference_b1###]. Transformers have enabled models to achieve or exceed human performance in various tasks through advanced fine-tuning techniques and large parameter counts.\n   \n2. **Scale of Models**: Refers to the expansion of model sizes up to more than a trillion parameters, as highlighted by [2 ###reference_b2###]. Despite their performance, these Large Language Models (LLMs) require significant computational resources, retaining the relevance of smaller, more task-specific models with fewer parameters.\n\n3. **Translate-Align-Retrieve Method**: This method from [5 ###reference_b5###] serves as inspiration for creating synthetic datasets. It involves translating and aligning QA pairs from high-resource languages to low-resource languages, then retrieving the retrieved span in the target language.\n\n4. **SQuAD v1.1**: A benchmark QA dataset in English [6 ###reference_b6###], used as a basis for generating the synthetic Serbian QA dataset.\n\n5. **XQuAD**: A multilingual QA dataset translated to Serbian [7 ###reference_b7###], utilized for evaluating the performance of the fine-tuned models.\n\nBy leveraging and modifying these methods and resources, the paper aims to publish a comprehensive QA dataset for Serbian and achieve significant model performance improvements.",
        "methodology": "**Methodology:**\n\nOur work is split into two main parts: dataset synthesis and model fine-tuning. \n\n**Dataset Synthesis**: By performing a method described in Chapter III-A [###reference_###], we acquire the largest Serbian QA dataset, which we name SQuAD-sr. Addressing the script duality property of Serbian, we acquire the dataset in both Cyrillic and Latin.\n\n**Model Fine-Tuning**: Fine-tuning provides us with several models which we evaluate and report the results in Chapter V [###reference_###].",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Experiment Setup:**\n\n- **Hyperparameters:** The same standardized hyperparameters presented in Table II are used across all experiments to ensure consistency and fair comparison.\n  \n- **Hardware:** All models are trained using an Nvidia Tesla P100 GPU for approximately 4 hours.\n\n- **Comparison Points:**\n  1. Models trained using fine-tuning are compared to those using a zero-shot learning approach.\n  2. The performance of monolingual models is compared against multilingual models.\n  3. Models using Cyrillic script are compared to those using Latin script to understand the influence of script choice.\n\n- **Evaluation Metrics:** \n  - **Exact Match (EM):** Measures strict accuracy by checking if the prediction exactly matches the ground truth answer.\n  - **F1 Score:** Measures a balance between precision and recall, allowing for partial matches between the prediction and the ground truth.\n\nThese metrics are implemented from the HuggingFace library.\n\n**Main Experimental Results:**\n\nThe results can be found in Table 1, which shows the performance of the models based on different criteria: zero-shot vs. fine-tuned, monolingual vs. multilingual, and Cyrillic vs. Latin script. The key observations from the results are:\n\n- **Fine-tuning vs. Zero-shot:** Models that were fine-tuned demonstrated significant improvements in both Exact Match and F1 Scores compared to those that used a zero-shot learning approach.\n  \n- **Monolingual vs. Multilingual Models:** Monolingual models generally performed better in terms of both EM and F1 Scores compared to multilingual models.\n\n- **Script Choice (Cyrillic vs. Latin):** Models using the Cyrillic script showed better performance metrics compared to those using the Latin script, highlighting the impact of script choice on model effectiveness."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "The study aims to compare the fine-tuning performance of different QA models on a synthetic Serbian dataset, examining the effect of monolingual versus multilingual models and the influence of script choice (Cyrillic vs. Latin).",
            "experiment_process": "The standardized fine-tuning process involves using the same hyperparameters for each experiment, as specified in Table II. Models are trained on an Nvidia Tesla P100 GPU for approximately 4 hours. The experiments compare various models on the impact of monolingual vs. multilingual pre-training and the choice of script. Exact Match (EM) and F1 Score metrics are used for evaluation, with the implementation from HuggingFace. The performance is evaluated on the benchmark XQuAD dataset translated into Serbian.",
            "result_discussion": "BERTi\u0107-SQuAD-sr achieved the highest performance with 73.91% EM and 82.97% F1 on the Latin Serbian XQuAD, while the best model for Cyrillic was XLM-R-SQuAD-sr with 67.58% EM and 78.08% F1. This demonstrates the higher quality of the Latin dataset and the significant impact of monolingual model fine-tuning over multilingual models. The study confirms SQuAD-sr's efficacy despite not surpassing human performance.",
            "ablation_id": "2404.08617v1.No1"
        },
        {
            "research_objective": "The study aims to evaluate the performance of models fine-tuned with the newly created SQuAD-sr dataset against zero-shot models and a human baseline.",
            "experiment_process": "mBERT and XLM-R are fine-tuned on SQuAD v1.1 and evaluated on the Serbian XQuAD dataset in Latin, using the same procedure as other models.",
            "result_discussion": "All fine-tuned models outperform zero-shot baselines, validating the quality of SQuAD-sr. BERTi\u0107-SQuAD-sr shows the most improvement, with 15.31% EM and 12.83% F1 over mBERT. However, humans still outperform the models by roughly 8% in EM and F1, indicating room for improvement in Serbian QA models.",
            "ablation_id": "2404.08617v1.No2"
        },
        {
            "research_objective": "The aim is to determine the impact of using monolingual versus multilingual pre-trained models for fine-tuning on a Serbian QA dataset.",
            "experiment_process": "The study fine-tunes multiple models on SQuAD-sr, selecting BERTi\u0107 for the monolingual model, and comparing it against mBERT and XLM-R.",
            "result_discussion": "Results indicate a significant performance boost when using the monolingual model BERTi\u0107 on the Latin dataset, with a 6.33% EM and 4.89% F1 increase over the best multilingual model, XLM-R-SQuAD-sr. This underscores the importance of monolingual models for fine-tuning.",
            "ablation_id": "2404.08617v1.No3"
        },
        {
            "research_objective": "The study investigates how the choice between Cyrillic and Latin scripts affects the performance of fine-tuned QA models.",
            "experiment_process": "The study fine-tunes models on both Cyrillic and Latin versions of the SQuAD-sr dataset and compares the performance differences.",
            "result_discussion": "Results consistently show better performance on Latin datasets across all models due to smaller Cyrillic pre-training portions. BERTi\u0107-SQuAD-sr shows the largest gap, with an 18.49% EM and 17.3% F1 increase on the Latin dataset, suggesting insufficient Cyrillic vocabulary size in pre-training.",
            "ablation_id": "2404.08617v1.No4"
        },
        {
            "research_objective": "To understand the performance variability of the model on different question types within the Serbian QA task.",
            "experiment_process": "Model performance is analyzed by categorizing questions from Serbian XQuAD into seven types: Who, What, How, When, Where, How many, and Other. The BERTi\u0107-SQuAD-sr model is tested on the Latin dataset to gather results.",
            "result_discussion": "The model performs best on When and How many questions due to their straightforward nature, but struggles with What and Where questions due to complex comprehension and genitive case challenges. Notably, Serbian-specific properties lead to better scores on How questions than elsewhere. Furthermore, there is a positive correlation between shorter answers and higher performance.",
            "ablation_id": "2404.08617v1.No5"
        }
    ]
}