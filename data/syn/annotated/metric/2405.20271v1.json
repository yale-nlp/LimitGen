{
    "title": "ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections",
    "abstract": "Parameter-efficient finetuning (PEFT) has become ubiquitous to adapt foundation models to downstream task requirements while retaining their generalization ability. However, the amount of additionally introduced parameters and compute for successful adaptation and hyperparameter searches can explode quickly, especially when deployed at scale to serve numerous individual requests. To ensure effective, parameter-efficient, and hyperparameter-robust adaptation, we propose the ETHER transformation family, which performs Efficient fineTuning via HypErplane Reflections. By design, ETHER transformations require a minimal number of parameters, are less likely to deteriorate model performance, and exhibit robustness to hyperparameter and learning rate choices. In particular, we introduce ETHER and its relaxation ETHER+, which match or outperform existing PEFT methods with significantly fewer parameters (times lower than LoRA or OFT) across multiple image synthesis and natural language tasks without exhaustive hyperparameter tuning. The code is available at \n\nhttps://github.com/mwbini/ether.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Recently, large-scale foundation models have demonstrated impressive general-purpose capabilities across both generative and discriminative tasks, showing extensive flexibility and strong performance when further adapted to different, more specialized tasks such as instruction following or controlled image synthesis. While impressive, these capabilities come with parameter counts increasing into the billions. To allow for affordable and scalable model adaptation that can serve large and diverse client bases, various techniques have been introduced in the literature.\n\nThese techniques range from full finetuning to just a few layers of the pretrained model, concatenating additional learning modules, and more recently to adapters on the network weights with lightweight learnable transformations. The latter have proven particularly effective, introducing no inference latency, fewer adaptation parameters, and strong performance. Conceptually, these methods finetune on smaller datasets to adapt to downstream task and data requirements, without compromising too much on the costly pretraining and incurring concept and semantic drifts by catastrophically overwriting pretrained weights.\n\nTreading the line for a suitable trade-off between adaptation and retention of the foundational model capabilities presents itself as a difficult task to tackle, often requiring costly tuning of hyperparameters such as learning rates. This problem is acknowledged explicitly in various studies aiming to preserve Euclidean weight distances between pretrained and finetuned models, and implicitly with approaches opting for both lower learning rates (at the cost of more tuning iterations) and inclusion of tuning parameters via summation.\n\nIn this work, we propose Efficient fineTuning via HypErplane Reflections (ETHER) - a new family of weight transformations, efficient in parameter count while preserving model abilities and being robust in convergence and learning rate choices. By default, ETHER transformations frame the tuning process as a search for suitable hyperplanes, along which weight vectors can be reflected based on the orthogonal Householder transformation. This keeps the distance to the transformation neutral element - the identity matrix - constant by construction and improves training stability while reducing the chance of deteriorating model performance. In addition, being built from single vectors, Householder transformations allow for efficient block-parallel matrix multiplication with minimal performance trade-offs.\n\nHowever, situations may arise where the hard distance restriction of ETHER can prove suboptimal (such as for subject-driven image generation, where finegrained subject-specific semantics need to be retained). As such, we augment the ETHER family with ETHER+ - a relaxation on the default ETHER method. More precisely, ETHER+ derives from the Householder transformation, but breaks the orthogonality and constant distance constraints, introducing multiple hyperplanes that can interact with a weight vector. As a result, ETHER+ allows for more controlled and finegrained adaptation, while still having a bounded distance to the transformation neutral element, and retaining the ETHER benefits of high parameter-efficiency, training stability, and hyperparameter robustness.\n\nIndeed, across subject-driven image generation, controlled image synthesis, natural language understanding and instruction tuning tasks, we find that ETHER and especially ETHER+ match and outperform existing methods using only a few additional tuning parameters - all while presenting stronger learning rate robustness compared to other methods and consequently requiring minimal hyperparameter tuning to achieve strong performance."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Parameter-Efficient Finetuning (PEFT). PEFT of pretrained models has evolved over the past years, involving finetuning protocols and the concatenation of learnable modules (Houlsby et al., 2019; Lester et al., 2021; Li & Liang, 2021; Pfeiffer et al., 2020; Guo et al., 2021) to more recent reparametrization of network weights with efficient transformations (Qiu et al., 2023; Hu et al., 2022; Kopiczko et al., 2023; Valipour et al., 2023; Zhang et al., 2023c). These approaches have demonstrated convincing trade-offs between adaptation quality, additional parameters, and inference latency. LoRA (Hu et al., 2022) modifies network weights by adding the result of a learnable, low-rank matrix product. Various extensions of LoRA include QLora (Dettmers et al., 2023) using quantized weights, AdaLoRA (Zhang et al., 2023c) with dynamic rank adjustment, and VeRA (Kopiczko et al., 2023) employing low-rank frozen random projections and trainable vectors to minimize parameter counts.\n\nControlling Diffusion Generative Models. Diffusion-based generative models exhibit strong compositional generation (Rombach et al., 2022; Mukhopadhyay et al., 2023; Podell et al., 2023b; Karthik et al., 2023; Saharia et al., 2022). Gal et al. (2022) and Ruiz et al. (2023) popularized personalized generation, enabling models to produce variations of user-provided samples. Following DreamBooth (Ruiz et al., 2023), other research (Liu et al., 2023b; Richardson et al., 2023; Zhang et al., 2023e) expanded on this concept. ControlNet (Zhang et al., 2023b) demonstrated model controllability through external signals like semantic and depth maps or face landmarks via additional layers, although this results in higher inference latency. Our work introduces an approach prioritizing robustness and parameter efficiency through hyperplane reflections.\n\nInstruction Tuning Language Models. Large Language Models (LLMs) have demonstrated remarkable generalization across numerous tasks (Zhao et al., 2023; Zhang et al., 2023d; OpenAI, 2023; Touvron et al., 2023a). Nevertheless, the default training objective often does not align with downstream task requirements and intentions. Instruction Tuning (Wang et al., 2023; Zhang et al., 2023d; Longpre et al.; Taori et al., 2023) addresses this mismatch by fine-tuning LLMs with additional (Instruction, Output) pairs to align the model with human preferences explicitly. This enhances capabilities and controllability while avoiding costly retraining (K\u00f6pf et al., 2023). Recently, methods based on LoRA (Hu et al., 2022) have been proposed for efficient control (Dettmers et al., 2023; Xu et al., 2023; Chen et al., 2023b; Valipour et al., 2023; Kopiczko et al., 2023). This work introduces a robust alternative that enhances parameter efficiency and learning rate robustness."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We first discuss adapter-based PEFT in \u00a73.1, before describing and motivating the use of hyperplane reflections in ETHER (\u00a73.2). To encourage flexibility in trainable control and adaptation, we propose a simple, yet effective relaxation ETHER+ in \u00a73.3. Finally, \u00a73.4 describes block-diagonal ETHER for improved computational efficiency.\n\nThe most commonly deployed form of PEFT with an adapter is Low-rank Adaptation (LoRA, Hu et al. (2022)). LoRA parametrizes a change of pretrained weights as a matrix product of two low-rank matrices for specified dimensions. When the rank is reduced, this approach can significantly decrease the required tuning parameters compared to full finetuning. In addition, the adaptation can be absorbed into the pretrained weights during inference to avoid additional latency.\n\nHowever, finetuning with LoRA can incur significant, potentially catastrophic weight changes. To ensure better preservation of pretrained model weights, Qiu et al. (2023) propose Orthogonal Finetuning (OFT). OFT proposes the usage of multiplicative orthogonal transformations on the model weights, aiming to retain pairwise weight angles.\n\nTo work in practice, Qiu et al. (2023) require the construction of the orthogonal matrix via a Cayley parametrization, where the matrix is skew-symmetric. Notice that by using this parametrization, they limit the range of possible orthogonal matrices to those with determinant 1.\n\nTo make OFT more parameter efficient, the orthogonal matrix is built in a block-diagonal fashion, made up of smaller blocks of specified sizes.\n\nThe final OFT transformation on the forward pass can then be described with block-diagonal matrices. The trainable parameters are the matrices that compose the skew-symmetric matrices. For finetuning, the skew-symmetric matrices are initialized as zero, ensuring the identity transformation at the beginning of finetuning."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Preliminaries",
            "text": "The most commonly deployed form of PEFT with an adapter is Low-rank Adaptation (LoRA, Hu et al. (2022)). LoRA parametrizes a change of pretrained weights as a matrix product of two low-rank matrices, i.e., for ,  and . When rank , this can significantly reduce the required tuning parameters compared to full finetuning. Additionally,  can be absorbed into  during inference to avoid additional latency.\n\nFinetuning with LoRA can lead to significant changes in weights. To better preserve pretrained model weights, Qiu et al. (2023) propose Orthogonal Finetuning (OFT). OFT suggests using multiplicative orthogonal transformations on the model weights. By retaining pairwise weight angles, certain properties can remain unaffected.\n\nFor practical implementation, Qiu et al. (2023) propose constructing the orthogonal matrix  via a Cayley parametrization , where  is skew-symmetric. This parametrization limits the range of possible orthogonal matrices to those with determinant 1, excluding reflections, which motivates ETHER.\n\nTo make OFT more parameter efficient, the orthogonal matrix  is built in a block-diagonal fashion, composed of smaller blocks  of size .\n\nThe final OFT transformation on the forward pass can be described with block-diagonal . The trainable parameters are the  matrices  that compose  - more specifically, the matrices  that build the skew-symmetric matrices  for .\n\nFor finetuning, the  are initialized as zero, ensuring that  and consequently  at the beginning of finetuning."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "ETHER: Finetuning with Hyperplane Reflections",
            "text": "Fundamentally, ETHER (Efficient fineTuning via HypErplane Reflections) sets up weight transformations as hyperplane reflections. These reflections can be obtained via the Householder transformation matrix with the hyperplane unit normal vector and the corresponding outer product. The reflection can be easily intuited when applied to a weight vector: Transformation effectively subtracts twice the component of projected on, thereby reflecting it with respect to the hyperplane defined by. By construction, hyperplane reflections are well-suited for the efficient finetuning of pretrained models, as they keep the distance to the transformation neutral element - the identity matrix - constant, which minimizes the risk of divergence from the pretrained model and deterioration of model performance. This can be easily shown by computing the Frobenius norm of the difference between the Householder matrix H and the identity matrix I: The above equation leverages the fact that for any matrix and that with and having unit length, one can simply write (with).\n\nSince the finetuning process simply consists of finding the optimal directions of the reflection hyperplanes with bounded deviations from the transformation neutral element, it allows for (i) a very low number of extra parameters corresponding to the unit vectors, and (ii) the usage of high learning rates, as the risk of divergence is minimized. This allows for general learning rate robustness and encourages fast convergence by default, as consistently high learning rates can be selected; reducing computational resources required to achieve good performance.\n\nInterestingly, as this transformation is orthogonal, it falls under the umbrella of orthogonal transformations motivated in OFT (Qiu et al., 2023). OFT leverages the Cayley parametrization of orthogonal matrices, which only produces determinant 1 matrices. By construction, this excludes Householder matrices from OFT, which have determinant. However, as noted above, it is indeed in this particular setting and through the use of Householder transformations that high parameter efficiency, strong pretraining retention, and learning rate robustness arise.\n\nThese findings partly motivate the exploration of a relaxed variant of the Householder reflection, demonstrating that loosening the orthogonality constraint not only maintains good performance but can even lead to enhanced results."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Relaxing Orthogonality in ETHER",
            "text": "While finetuning via hyperplane reflections has several promising qualities as highlighted above, there is no free lunch. In particular, situations may arise where the strength of the transformation and inherent deviation from the identity may be too large by default, such as for potentially more nuanced tasks like subject-driven generation (Ruiz et al., 2023). To allow for more nuanced transformations while retaining beneficial properties of ETHER - parameter efficiency and learning rate robustness through bounded deviations from the transformation neutral element - we propose the ETHER+ relaxation with unit vectors. This is a simple variation of the Householder transformation that now allows for interaction between two distinct hyperplanes. This helps to control the transformation strength as vectors can weaken or even cancel each other out to return the identity transformation in the limit where the vectors meet certain criteria. In addition, the transformation distance remains bounded, as the relaxed variant always has a certain property. This follows immediately from the triangle inequality of norms. Due to the weaker strength of this new transformation, we apply it both on the left and right of the weight matrix, such that the forward pass becomes more complex. Consequently, ETHER+ effectively leverages a sequence of hyperplane interactions that no longer have to retain length to allow for more nuanced weight adjustment while still minimizing the risk of diverging from the pretrained model."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Efficient ETHER through Block-Parallelism",
            "text": "In multiplicative finetuning like OFT or ETHER, further computational load is introduced through additional matrix multiplications. To mitigate this issue, we introduce a block-diagonal formulation of ETHER similar to block-diagonal OFT. For this, we break down the Householder transformation into its corresponding block-diagonal variant, with each -th block-plane parameterized by . Each -th block now only affects the corresponding -th block-row in the weight matrix . This means we can split  into  sub-blocks , each uniquely altered by its corresponding  counterpart. As a result, the full weight transformation can now be separated into smaller block-specific operations, reducing the overall number of computations. Furthermore, these operations can now be fully block-parallelized, significantly increasing training speed.\n\nIn terms of computations, for each full-matrix-multiplication between  and  of sizes  and  respectively,  multiplications and  additions are necessary, accounting for  operations. With our block-parallel scheme, we reduce these to  block-specific  multiplications and  additions, resulting in  operations.\n\nFurthermore, with each block being built from a single vector of dimension , ETHER transformations\u2019 construction ensures that the total number of trainable parameters remains constant for any  number of blocks. This stands in contrast to block-diagonal OFT, where the use of higher block counts was introduced to minimize the number of parameters while introducing noticeable decreases in adaptation performance. Instead, for block-diagonal ETHER, we find performance to be consistent over increasing block counts, allowing for an improved computational footprint with negligible performance decrease."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Intriguing Properties of ETHER",
            "text": "This section investigates and highlights the bounded distance and non-deteriorating nature of ETHER/ETHER+ in more detail while providing insights into its favorable learning rate robustness and the reliable use of high learning rates for fast convergence. For completeness, we also report here comparisons with the unconstrained Naive method, to better show the impact of orthogonality as proposed by Qiu et al. (2023), and how our method provides much stronger robustness. Finally, we include a discussion on the parameter efficiency. For all experiments in this section, please see \u00a75.1 for relevant implementation details.\n\nNon-Deteriorating Nature. Because both ETHER and ETHER+ are upper-bounded in their possible perturbation over the pretrained weight matrices (as measured for example by the distance to the transformation neutral element, the identity matrix), finetuning with both methods will guarantee suitable results for most hyperparameter choices. This is easily visualized in Fig. 3 by looking at generation samples after perturbing Stable Diffusion with randomly sampled transformations for each approach - OFT, ETHER and ETHER+ - respectively. While ETHER uses a fixed-distance transformation (c.f. Eq. 2) that introduces a noticeable change (but still retaining semantics), ETHER+ can obtain both fine-grained visual control as well as stronger semantic changes. Conversely, unbounded methods like OFT catastrophically deteriorate a model\u2019s generative abilities as the perturbation strength increases. This results in a much more controlled generation setting for ETHER and ETHER+ finetuning. This is also depicted quantitatively in Fig. 4, which shows distances between the learned transformation and the transformed weights (at convergence) to the identity matrix and the pretrained weights, respectively, as a function of the learning rate. As can be seen, larger learning rate values for OFT and Naive finetuning (OFT without orthogonality constraints) result in distances that are orders of magnitude higher than those of ETHER and ETHER+, leading to catastrophic deterioration and model collapse.\n\nLearning Rate and Hyperparameter Robustness. Practically, the non-deteriorating nature of ETHER and ETHER+ manifests in learning rate robustness during finetuning. As the risks of divergence and collapse are minimized, training stability becomes much less dependent on the choice of learning rate. This is seen when evaluating performance (e.g. mIoU for controllable image synthesis in Fig. 5) and model convergence (Fig. 6) against learning rates. For non-ETHER methods, Fig. 5 shows significant performance drops for high learning rates, while Fig. 6 reveals fast convergence speeds for ETHER+ with learning rates covering multiple magnitudes, much more general than e.g. OFT. This means that not only can good performance be guaranteed for most learning rate choices, but fast convergence as well, with competitive results already after the first epoch. Since ETHER also only introduces a single hyperparameter, the number of diagonal blocks, which marginally impacts performance (c.f. \u00a73.4), ETHER methods become very attractive for practical usage, as the need for grid-search and cautious low learning rate training for good performance (c.f. \u00a71) is reduced.\n\nParameter Efficiency. Finally, we provide a more detailed exploration on the parameter efficiency of ETHER-based methods. Let \\( l \\) be the number of finetuned layers, \\( n_i \\) and \\( n_j \\) the respective weight dimensions for \\( \\Theta^{(i,j)} \\). Then the parameter complexity for OFT can be written as \\( O(h^2l) \\) (Qiu et al., 2023) with \\( h \\) number of diagonal blocks. Similarly, for LoRA we get \\( O(rhl) \\), while for ETHER and ETHER+ we only have \\( O(hl) \\) and \\( O(1) \\) respectively. With respect to both LoRA and OFT, this omits at the very least the rank multiplier \\( r \\), or a potentially quadratic scaling. As already motivated in Sec. 3, this results in incredibly efficient finetuning while achieving comparable or stronger performances. For example, when finetuning Stable Diffusion as done above, ETHER and ETHER+ use 120 times and 30 times fewer parameters than OFT respectively."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Benchmark Experiments",
            "text": "We first investigate generative model adaptation in Sec. 5.1, with a focus on subject-driven image synthesis (\u00a75.1.1) and controllable image synthesis (\u00a75.1.2) following recent works (Qiu et al., 2023; Liu et al., 2023a). Sec. 5.2 then correspondingly investigates language model adaptation, looking at both natural language understanding (\u00a75.2.1) and instruction tuning (\u00a75.2.2)."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "ETHER for Image-generative Model Adaptation",
            "text": "For our experiments on diffusion-based generative models, we apply the finetuning methods on the pretrained Stable Diffusion-v1.5 (Rombach et al., 2022 ###reference_b53###), following the setting from OFT (Qiu et al., 2023 ###reference_b50###). Our experiments follow best practices and hyperparameter choices for each method. For implementation details, please refer to App. C ###reference_###."
        },
        {
            "section_id": "5.1.1",
            "parent_section_id": "5.1",
            "section_name": "5.1.1 Subject-driven Generation",
            "text": "We first deploy ETHER and ETHER+ on subject-driven generation following Ruiz et al. (2023); Qiu et al. (2023); finetuning the generative model for each of the 30 subjects and 25 prompts. For each combination, we generate four images, and measure image quality via a DINO (Caron et al., 2021) and a CLIP image encoder (Radford et al., 2021), text-prompt fidelity via a CLIP text encoder, and image diversity using LPIPS (Zhang et al., 2018).\n\nQuantitative Results. Results are shown in Tab. 2. On subject-driven generation, we find competitive performance for both image quality, text-prompt fidelity, and image diversity, particularly for ETHER+. Most importantly, we achieve this performance while only utilizing a fraction of tuning parameters; with ETHER+ only introducing a small percentage of additional parameters compared to other methods.\n\nAs hypothesized in Sec. 3, for nuanced finetuning, ETHER\u2019s transformation strength seems to be too high to retain key semantic concepts in subject-driven generation, falling short in image quality with respect to other methods, despite achieving strong image diversity and text-prompt fidelity."
        },
        {
            "section_id": "5.1.2",
            "parent_section_id": "5.1",
            "section_name": "5.1.2 Controllable Image Generation",
            "text": "This section applies ETHER for controllability of Stable Diffusion following Qiu et al. (2023) for the Semantic Map to Image (S2I) task on ADE20K (Zhou et al., 2018). We use the trainable encoder from ControlNet (Zhang et al., 2023b) for the control signal and perform finetuning on the Stable Diffusion weights only. We report a baseline with just the control signal encoder to highlight relative gains through finetuning.\n\nEvaluations are performed on 2000 images generated from the validation set using mean Intersection-over-Union (mIoU) and accuracy of semantic maps over generated images using UperNet-101 (Xiao et al., 2018) pretrained on ADE20K. Finally, we measure the similarity between generated and original images via FID (Heusel et al., 2018). We note that LoRA did not provide good results even for larger hyperparameter grids and was therefore omitted from Tab. 3 (more details in App. F).\n\nQuantitative Results. Results are depicted in Tab. 3, and clearly demonstrate competitive control with both ETHER and ETHER+. Unlike subject-driven image generation, we find that ETHER performs on the same level as OFT multiplicative finetuning while using over fewer parameters. Introducing magnitude re-fitting to OFT yields only limited gains while adding parameters.\n\nSimilar to Tab. 2 for subject-driven image generation, we find that for controllable image synthesis, the ETHER+ relaxation provides additional performance gains. Taking into account the more robust and faster convergence, this presents ETHER+ as a practically attractive finetuning alternative."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "ETHER for Language Models Adaptation",
            "text": "To understand the applicability of the ETHER transformation family in the language domain, we follow Liu et al. (2023a)'s and Hu et al. (2022)'s experimental setup. For fair comparisons, we run grid searches over the most relevant hyperparameters in common value ranges. For additional implementation details, please refer to App. C."
        },
        {
            "section_id": "5.2.1",
            "parent_section_id": "5.2",
            "section_name": "5.2.1 Natural Language Understanding",
            "text": "We begin by deploying ETHER and ETHER+ on the widely utilized GLUE benchmark, finetuning a pretrained DeBERTaV3-base model, from which we report the baselines\u2019 results. GLUE comprises various English sentence understanding tasks, such as inference tasks (MNLI, QNLI, RTE), classification of sentiment (SST-2) or correct English grammatical structures (CoLA), and semantic similarity and equivalence prediction (MRPC, QQP, STS-B). CoLA scores report the Matthews correlation coefficient, MNLI matched accuracy, and STS-B average correlation. All other tasks are evaluated on accuracy.\n\nQuantitative Results. Results show that ETHER and ETHER+ match and even outperform previous methods with significantly fewer parameters. For example, ETHER outperforms the second-best BOFT on the RTE inference task or equivalence prediction on MRPC while using just one-ninth of the parameters. ETHER+ sets both the best performance on STS-B and particularly the highest overall score using less than half of the parameters of BOFT.\n\nThese results provide additional support for the practical viability of ETHER transformations, now for natural language adaptation - being a strong, but much more parameter-efficient competitor."
        },
        {
            "section_id": "5.2.2",
            "parent_section_id": "5.2",
            "section_name": "5.2.2 Instruction Tuning",
            "text": "Our instruction tuning experiments make use of Llama-2-7B (Touvron et al., 2023b) as pretrained model, finetuning it on the Alpaca dataset (Taori et al., 2023) for one epoch. To operate on a consumer GPU, we truncate the maximum sequence length to 256 and use bfloat16 precision (Kalamkar et al., 2019).\n\nWe evaluate 0-shot performance of our instruction-tuned model on (i) Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) with 57 different tasks in four different subjects (STEM, Humanities, Social Sciences, Others); (ii) the AI2 Reasoning Challenge (ARC) (Clark et al., 2018), a common-sense reasoning dataset of questions from science grade exams; (iii) TruthfulQA (Lin et al., 2022) comprising 817 questions spanning 38 categories testing how much the model (wrongly) relies on imitation of human text to answer.\n\nQuantitative Results. Results in Tab. 5 show significant improvements over other comparable finetuning approaches using ETHER. Across all metrics, the Llama-2-7B baseline is consistently surpassed by significant margins. Despite being the most parameter-efficient method, ETHER outperforms all baselines with comparable number of parameters, such as the recently introduced VeRA (Kopiczko et al., 2023) with rank, and LoRA rank. Surprisingly, increasing the rank of VeRA leads to a decrease in performance, while LoRA rank shows better results but is still outperformed on MMLU despite having more parameters.\n\nOn the other hand, ETHER+ surpasses all other methods across all benchmarks, while having fewer parameters than LoRA rank."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Hyperspherical Energy for Effective PEFT",
            "text": "To test this assumption, we have included an OFT control baseline (Naive), which does not utilize orthogonality constraints, on the same finetuning settings in which OFT was proposed. Results at convergence, as reported in Tab. 6, do not show significant differences, while actually introducing the overhead of computing the Cayley parametrizations (which also involve computing the inverse of a matrix). We also included the Naive baseline in the learning rate robustness studies in Fig. 4 and Fig. 5, showcasing that while differences are present for high learning rates, the optimal working range remains unaltered.\n\nIn contrast, on these same evaluations, our newly proposed ETHER transformation family, by introducing a boundary on the Euclidean distance on the transformation side, achieves stronger performance and greater robustness. This is especially true for the non-orthogonal ETHER+, which alters the overall results even more than Naive. \n\nThis evidence diminishes the role of other factors and instead emphasizes the greater importance of the Euclidean distance, establishing the ETHER family as a favorable option in multiplicative finetuning settings."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions",
            "text": "Our paper introduces the ETHER family of transformations for parameter-efficient finetuning. Based on the Householder formulation of hyperplane reflections, ETHER methods frame finetuning as a search for unit normal vectors that define hyperplanes along which weight vectors are reflected. In doing so, ETHER (and its relaxation ETHER+ for more finegrained adaptation) fix (or upper bound) the distance of learned transformations from the identity matrix (the transformation neutral element), thereby minimizing the risk of finetuning divergence. Put together, ETHER methods operate more parameter-efficiently than other PEFT methods (e.g., around 10-100 times less than LoRA or OFT), have higher learning rate robustness and encourage fast convergence. Consequently, ETHER transformations require less expansive hyperparameter searches to achieve good performance, making them very attractive for practical deployment.\n\nLimitations. Of course, there is no free lunch. While both ETHER and its relaxation ETHER+ show strong results with few parameters across a broad range of tasks, increasing the expressive power of the transformation is not as straightforward as in other methods, such as LoRA, where one can adjust the rank parameter to more closely approximate full finetuning. Moreover, multiplicative methods introduce a computational overhead during training compared to additive methods. Thanks to our block-parallel scheme, we make significant progress towards closing the gap between multiplicative and additive approaches; however, multiplicative methods still lag behind. This introduces a trade-off between parameter efficiency and computational overhead when achieving similar performance levels."
        }
    ],
    "url": "http://arxiv.org/html/2405.20271v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.1.1",
            "5.1.2",
            "5.2",
            "5.2.1",
            "5.2.2",
            "5.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "5.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.20271v1",
        "paper_title": "ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections",
        "research_background": "### Motivation\n\nThe paper is motivated by the impressive capabilities of large-scale foundation models across a range of tasks, which come at the cost of substantial parameter counts, often reaching billions. Adapting these models is a significant challenge, particularly when it must be done affordably and at scale. Traditional fine-tuning methods either require substantial computational resources or risk overwriting pretrained weights, disrupting the model's learned knowledge. Additionally, achieving an effective balance between retaining foundational capabilities while adapting to new tasks is difficult and often necessitates costly hyperparameter tuning.\n\n### Research Problem\n\nThe primary research problem addressed in this paper is finding a more efficient and robust method to adapt large-scale models. The goal is to preserve the models' capabilities while ensuring they can be fine-tuned with fewer parameters, less computation, and minimal hyperparameter tuning. Existing methods, even those introducing lightweight transformations, often require stringent hyperparameter settings or result in high computational overhead, which the proposed method aims to overcome.\n\n### Relevant Prior Work\n\nThe relevant prior work includes:\n- **Full Finetuning:** Techniques where the entire model or just specific layers are fine-tuned (e.g., Zhao et al., 2024; Zhang et al., 2023a; Stojanovski et al., 2022; Kornblith et al., 2019).\n- **Concat Modules:** Approaches like adding additional learning modules to existing models (e.g., Houlsby et al., 2019; Pfeiffer et al., 2020; Mou et al., 2023).\n- **Lightweight Transformations:** Recent methods that use adapters for lightweight learnable transformations without much inference latency (e.g., Qiu et al., 2023; Hu et al., 2022; Kopiczko et al., 2023; Valipour et al., 2023).\n- **Euclidean Distance Measures:** Techniques aimed at preserving Euclidean weight distances to maintain knowledge (e.g., Li et al., 2018; Chen et al., 2023a; Gouk et al., 2021).\n- **Hyperspherical Energy (HE):** Metrics and transformations like OFT that focus on hyperspherical energy as an alternative measure to Euclidean distance, aiming for robust finetuning with orthogonal transformations (Qiu et al., 2023).\n\n### ETHER and ETHER+\n\nThe proposed ETHER method introduces weight transformations using hyperplane reflections based on the Householder transformation, aimed at keeping distance to the transformation neutral element constant and improving training stability. The paper also introduces ETHER+, an augmented variant that allows for more controlled and fine-grained adaptation by breaking the orthogonality constraint, leading to strong performance across various tasks while maintaining parameter efficiency and training robustness. Both methods are shown to outperform existing approaches, providing benefits in terms of computational efficiency and reduced need for hyperparameter tuning. The effectiveness of non-orthogonal transformations is further explored, questioning the strict emphasis on orthogonality promoted in prior work.",
        "methodology": "### Methodology\n\n#### Adapter-Based PEFT (Parameter-Efficient Fine-Tuning)\nThe foundational technique for parameter-efficient fine-tuning (PEFT) utilized in this study is the Low-rank Adaptation (LoRA) method. LoRA, as proposed by Hu et al. (2022 ###reference_b20###), involves modifying pretrained weights  using a matrix product of two low-rank matrices, denoted as:\n\n\\[ \\Delta W = A B \\]\n\nHere, \\( A \\) and \\( B \\) are low-rank matrices where the rank \\( r \\) is substantially lower than the dimensions of the full matrix, i.e., \\( A \\in \\mathbb{R}^{d \\times r} \\) and \\( B \\in \\mathbb{R}^{r \\times d} \\). This approach drastically reduces the number of parameters to be tuned while maintaining model performance. Importantly, during inference, the adaptation matrices can be merged into the pretrained weight matrix to avoid added computational overhead.\n\nWhile effective, LoRA can occasionally result in substantial, potentially deleterious changes to model weights, necessitating enhanced strategies for preserving pretrained model structure.\n\n#### Hyperplane Reflections in ETHER\nETHER aims to mitigate significant weight changes seen in LoRA by implementing hyperplane reflections. This concept stems from Orthogonal Finetuning (OFT), which was introduced by Qiu et al. (2023 ###reference_b50###). OFT employs multiplicative orthogonal transformations on model weights to maintain the hyperspherical energy (HE) of the weight space, thereby preserving the intrinsic properties of the pretrained model.\n\nHowever, OFT uses Cayley parametrization to construct the orthogonal matrix \\( R \\):\n\n\\[ R = (I + A)(I - A)^{-1} \\]\n\nwhere \\( A \\) is skew-symmetric. This parametrization has a limitation: it only produces orthogonal matrices with determinant 1, omitting those with determinant -1, which include reflections.\n\nETHER addresses this gap by incorporating hyperplane reflections, expanding the scope of maintainable orthogonal transformations to better preserve model integrity.\n\n#### Relaxation with ETHER+\nTo further boost the model's adaptability and control during fine-tuning, we present ETHER+, an enhanced version that introduces additional flexibility. This modification allows for a more adaptive response to training dynamics while maintaining efficiency.\n\n#### Block-Diagonal ETHER for Computational Efficiency\nTo improve the computational efficiency of our method, we propose a block-diagonal structure for constructing orthogonal matrices, similar to the strategy used in OFT. The orthogonal matrix \\( R \\) is designed as block-diagonal, composed of smaller blocks \\( R_i \\) of size \\( r \\times r \\):\n\n\\[ R = \\text{block-diag}(R_1, R_2, ..., R_k) \\]\n\nEach block \\( R_i \\) is derived from skew-symmetric matrices \\( A_i \\), where each \\( A_i \\) is parameterized by trainable parameters initialized at zero to start with \\( R_i = I \\). This ensures that initially, \\( R = I \\), safeguarding the original weights at the onset of fine-tuning.\n\n#### Summary\nTo summarize, ETHER introduces hyperplane reflections in a PEFT setting to maintain the pretrained model's weights' integrity by preserving hyperspherical energy while leveraging block-diagonal matrix structures for computational efficiency. ETHER+ enhances the model's flexibility through a relaxation mechanism, making fine-tuning more efficient and effective.",
        "main_experiment_and_results": "In the main experiment setup of the paper \"ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections,\" the experiments are divided into distinct parts focusing on generative model adaptation and language model adaptation.\n\n### Datasets and Tasks:\n1. **Generative Model Adaptation (Sec. 5.1):**\n   - **Subject-Driven Image Synthesis (Sec. 5.1.1):** This section focuses on adapting generative models to synthesize images around specific subjects. The exact datasets are not detailed in the provided text, but it references following recent works for methodology.\n   - **Controllable Image Synthesis (Sec. 5.1.2):** This section caters to generating images with controllable attributes following methodologies from Qiu et al., 2023 and Liu et al., 2023a.\n\n2. **Language Model Adaptation (Sec. 5.2):**\n   - **Natural Language Understanding (Sec. 5.2.1):** Tasks under this segment involve standard natural language processing datasets, although specific datasets are not detailed in the provided text.\n   - **Instruction Tuning (Sec. 5.2.2):** This part emphasizes tuning language models for better adherence to given instructions or commands. Specific datasets are not mentioned.\n\n### Baselines:\nThe experiments compare the proposed ETHER method against recent state-of-the-art methodologies in both generative and language model adaptation fields. Specifically, comparisons are drawn with techniques referenced in Qiu et al., 2023 and Liu et al., 2023a.\n\n### Evaluation Metrics:\nThe evaluation metrics for the performance of ETHER in both generative and language model adaptations are not explicitly stated in the provided text. However, common metrics in these fields generally include:\n- For image synthesis: Inception Score (IS), Fr\u00e9chet Inception Distance (FID), and user studies or qualitative assessments.\n- For natural language understanding: accuracy, F1 score, and other task-specific benchmarks.\n- For instruction tuning: task completion rates, response accuracy, and adherence to instructions.\n\n### Main Experimental Results:\nAlthough specific numeric results and detailed quantitative comparisons are not provided, the main experimental outcomes can generally be summarized as follows:\n- **Generative Model Adaptation:** The ETHER method demonstrates improved performance in both subject-driven and controllable image synthesis compared to the baselines.\n- **Language Model Adaptation:** ETHER provides enhanced natural language understanding capabilities and better performance in instruction tuning tasks relative to existing state-of-the-art methodologies.\n\nThe results collectively indicate that finetuning large-scale models using hyperplane reflections and considering orthogonality and hyperspherical energy (as explored further in Sec. 5.3) result in more efficient and effective model adaptations across various tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate and highlight the bounded distance and non-deteriorating nature of ETHER/ETHER+, and their learning rate robustness and parameter efficiency in finetuning applications.",
            "experiment_process": "The study compared ETHER and ETHER+ against the unconstrained Naive method and OFT for tasks such as controllable image synthesis. Experiments involved perturbing Stable Diffusion with randomly sampled transformations (OFT, ETHER, ETHER+) and analyzing the results via visual samples (Fig. 3), distance metrics (Fig. 4), and performance metrics like mIoU (Fig. 5). Convergence speeds were evaluated using learning rates covering multiple magnitudes (Fig. 6). Parameter efficiency was analyzed by comparing the parameter complexity of various methods.",
            "result_discussion": "ETHER and ETHER+ demonstrated significant robustness and non-deteriorating behavior compared to OFT, especially at higher learning rates. The methods provided controlled generation settings and fast convergence with good performance guarantees. Parameter efficiency analyses revealed that ETHER and ETHER+ used 120 times and 30 times fewer parameters than OFT, respectively, while achieving comparable or better performance.",
            "ablation_id": "2405.20271v1.No1"
        },
        {
            "research_objective": "Evaluate the practical utility of Hyperspherical Energy (HE) retention in improving finetuning stability and performance.",
            "experiment_process": "The study included an OFT control baseline (Naive) with the same finetuning settings to test the assumption that orthogonality and unaltered HE lead to better stability and performance. Comparative analysis was performed by measuring convergence, learning rate robustness, and HE values during training (Tab. 6, Figs. 4, 5, and 7).",
            "result_discussion": "Results showed no significant differences between the Naive baseline and OFT in terms of performance at convergence, challenging the orthogonality and HE stability assumption. The study highlighted that ETHER transformations, particularly ETHER+, achieved stronger performance and greater robustness, emphasizing the importance of Euclidean distance over HE. This establishes ETHER as a more favorable option for multiplicative finetuning.",
            "ablation_id": "2405.20271v1.No2"
        }
    ]
}