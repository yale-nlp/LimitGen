{
    "title": "Clarinet: Augmenting Language Models to Ask Clarification Questions for Retrieval",
    "abstract": "Users often make ambiguous requests that require clarification. We study the problem of asking clarification questions in an information retrieval setting, where systems often face ambiguous search queries and it is challenging to turn the uncertainty in the retrieval model into a natural language question. We present Clarinet, a system that asks informative clarification questions by choosing questions whose answers would maximize certainty in the correct candidate. Our approach works by augmenting a large language model (LLM) to condition on a retrieval distribution, finetuning end-to-end to generate the question that would have maximized the rank of the true candidate at each turn. When evaluated on a real-world retrieval dataset of users searching for books, our system outperforms traditional heuristics such as information gain on retrieval success by 17% and vanilla prompted LLMs by 39% relative.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Natural language is a flexible interface for users to interact with systems, but language is inherently ambiguous, and users themselves may not know what they want. As a result, systems must handle underspecified queries. We study this in an information retrieval setting, where search ambiguity is a well-studied challenge. While modern large language models (LLMs) can ask coherent clarification questions, they do not always ask questions that elicit information specifically about what the model is uncertain about. This is particularly challenging in the case of retrieval, where it is unclear how to integrate the conversational abilities of an LLM with the external database or the retrieval system that represents the uncertainty over search candidates.\n\nIn contrast, approaches that use principled information theoretic measures such as information gain or KL utility can use the retriever distribution to evaluate the right questions to ask and explicitly select questions that reduce uncertainty. However, these methods require expensive inference-time generation and evaluation of potential questions and have not yet been able to scale beyond toy settings. In this work, we investigate whether we can learn to ask good questions simply by distilling the search over good questions into an end-to-end model.\n\nWe present Clarinet, a framework for learning to ask clarification questions for information retrieval. Our approach works by augmenting a language model to condition on the retriever distribution and then finetuning the system end-to-end to generate informative questions. We select informative questions by simulating interactions with a prompted LLM that acts as a user proxy, training only on questions that would have significantly increased the confidence in the true item if answered. In contrast to heuristic-based methods, Clarinet distills the expensive, explicit search over questions at inference time into the model.\n\nThen, given user responses to clarification questions, we summarize the interaction history into a language posterior, a single natural language query describing what the system knows about the user\u2019s desired item. We use this query to re-rank candidate items from the database.\n\nWe evaluate our approach on a real-world dataset of users asking for help on an online forum (Goodreads) to find books they vaguely recall from a database of thousands of items, e.g., \u201cwritten by a former journalist, takes place in NYC, involves a necklace.\u201d We evaluate systems interactively against a simulated user, which we implement as a prompted LLM that can answer clarification questions with oracle access to the true item. Compared to a purely dialogue-based approach that generates clarification questions prompted only with the dialogue history, our system asks better questions. Our model also outperforms heuristic-based approaches that select a clarification question from a candidate pool with information gain or KL utility, while being much simpler and cheaper at inference time."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Interactive NLP systems have used clarification questions as way to gather more information from users in settings such as classifying Yu et al. (2019  ###reference_b16###), conversational question answering Rao and Daum\u00e9 III (2018  ###reference_b11###), and visually grounded 20 questions games White et al. (2021  ###reference_b15###). To select informative questions, much of the work draws heuristics that maximize expected information gain from simulated answers.\nSome of the works shift the clarification question generation and selection from using rule-based and heuristic methods to reinforcement learning. Rao and Daum\u00e9 III (2019  ###reference_b12###) extended their utility used in Rao and Daum\u00e9 III (2018  ###reference_b11###) in a reinforcement setting to generate useful and relevant questions. Meanwhile, Pyatkin et al. (2022  ###reference_b10###) presented an interactive system that asks relevant and informative clarification questions to learn salient contexts of a social or moral situation. Their approach to question generation utilized reinforcement learning, aiming to optimize the generation of questions that elicit responses containing morally relevant contexts.\nIn a more practical scenario, Zamani et al. (2020  ###reference_b17###) suggests both supervised and reinforcement learning models to generate clarifying questions that aid users in formulating search queries in search engines. Additionally, they explore techniques for generating potential answers for each clarifying question, allowing users to conveniently choose from a predefined set of answers. However, if the users don\u2019t know what they are looking for, the predefined answers may not be very helpful."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "We train and evaluate our system in a \u201ctip of the tongue\u201d (TOT) search setting, where users are searching for items they vaguely remember but for which they cannot formulate precise queries. To train and evaluate clarification question generation, we use the dataset from Bhargav et al. (2022  ###reference_b1###), which contains multi-turn interactions from community forums where users post queries searching for books where they were unable to find using conventional search engines. We filter the data specifically about book queries, resulting in 784 interactions, which we split into 156 for training and 628 for evaluation. The retrieval database consists of a corpus of 1877 documents.\nIn general, Clarinet is agnostic to the choice of retrieval model. In our experiments, we use Dense Passage Retriever (DPR) Karpukhin et al. (2020  ###reference_b5###) as the retriever over the book database.\nWe finetune the retriever for this task with the larger scale WhatsThatBook dataset from Lin et al. (2023  ###reference_b7###), which contains 11,552 initial query-document pairs for TOT book retrieval.\nWe use dot-product similarity between the DPR representation of the book information (book title, author, metadata, synopsis) and the DPR representation of a query synthesized by our Clarinet model (as described in Section 3.1  ###reference_###) to rank candidate books.\nAt each turn, we generate a pool of 50 candidate clarification questions by prompting GPT-3 (gpt-3.5-turbo-0613) with the user\u2019s initial query (dataset queries from real users), the dialogue history of clarification questions and answers, and top three items in the retriever distribution. We generate 20 candidate clarification questions for each turn by sampling with temperature 0.8.\nFor each candidate question, we simulate a response with a user simulator model. We implement the user simulator as a GPT-3 model that has access to the true item and its description, and is prompted to answer questions about the item vaguely. Refer to Appendix B  ###reference_### for the prompt.\nFor each initial training query, make a run where the question at each turn is uniformly sampled from the candidate pool generated by GPT-3, and the retrieval distribution is updated through explicit posterior. At the end of each turn, calculate and record the ranking of the target book.\nIn contrast to methods like EIG that operate at inference time, we can directly train on the question that places the target\u2019s rank high or improves the rank the most.\nOnly the questions that help the system to rank the target book as top 10, or help increase the rank by 10 will be used in the training. Users\u2019 initial queries, past interactions, and the information of top book candidates (book candidates with retrieval probabilities that add up to 50% [no more than 3 books]) will be served as the model input.\n###figure_2### To produce an updated belief distribution over the retrieval candidates after each turn, we need to integrate all the information that the user has provided thus far. We prompt another GPT-3 model to synthesize a description of the retrieval candidate given the interaction history thus far, which can be thought of as a posterior belief over the true item, represented in language. We then use this language posterior as the search query to produce a new candidate distribution, instead of the initial query provided by the user.\nWe train Flan-T5-base Chung et al. (2022  ###reference_b2###) to generate the selected clarification questions.\nThe model takes in the user\u2019s initial query, the interaction history of questions and answers, and the information of the top 50% (no more than 3) confident retrieval candidates to generate the next clarification question.\nFor the books dataset, the information for each retrieval candidate includes the book title, author, published dates, and description. We additionally include the current ranking and retrieval probability, concatenating all the information as text to form the full context for a book.\nWe use a Fusion-in-Decoder architecture (FiD) Izacard and Grave (2020  ###reference_b4###), where we concatenate the information for each retrieval candidate with the initial query and interaction history and feed it into the encoder independently.\nThe encoded candidates are then concatenated, which the decoder then attends to in order to generate clarification questions.\nWe train the model for 10k gradient steps using the Adam optimizer with a learning rate of , batch size 8, and dropout 0.1. We evaluate the models at intervals of 500 steps and select the checkpoint with the best BLEU score on a held-out validation.\n###figure_3###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Asking Clarification Questions with Clarinet",
            "text": "The retrieval dataset itself consists of initial (underspecified) queries from real users, and the true item they were searching for.\nTo train a Clarinet model, we transform a retrieval dataset into an interaction dataset by synthesizing a series of dialogue interactions with a user simulator model. Each interaction is seeded with a real user query from the dataset; we then generate a series of clarification questions and answers from the user simulator model.\nAt each turn, we generate a pool of candidate questions and filter for the most informative questions for finding the user\u2019s desired items\u2014i.e., the questions whose answers would increase the rank of the true candidate the most.\nThe clarification dataset thus consists of (interaction history , ) for each informative question  out of the candidate pool.\nWe finetune a LLM to generate these questions  conditioned on the interaction history and retrieval distribution.\nFor the TOT dataset, we run five 10-turn games for each initial user query.\nAt inference-time, we directly sample from the model to generate questions to ask the user. We update the retrieval distribution after each user response by condensing the interaction history thus far into a language posterior, a single retrieval query that we use to re-retrieve results from the database.\nAt each turn, we generate a pool of 50 candidate clarification questions by prompting GPT-3 (gpt-3.5-turbo-0613) with the user\u2019s initial query (dataset queries from real users), the dialogue history of clarification questions and answers, and top three items in the retriever distribution. We generate 20 candidate clarification questions for each turn by sampling with temperature 0.8.\nFor each candidate question, we simulate a response with a user simulator model. We implement the user simulator as a GPT-3 model that has access to the true item and its description, and is prompted to answer questions about the item vaguely. Refer to Appendix B  ###reference_###  ###reference_### for the prompt.\nFor each initial training query, make a run where the question at each turn is uniformly sampled from the candidate pool generated by GPT-3, and the retrieval distribution is updated through explicit posterior. At the end of each turn, calculate and record the ranking of the target book.\nIn contrast to methods like EIG that operate at inference time, we can directly train on the question that places the target\u2019s rank high or improves the rank the most.\nOnly the questions that help the system to rank the target book as top 10, or help increase the rank by 10 will be used in the training. Users\u2019 initial queries, past interactions, and the information of top book candidates (book candidates with retrieval probabilities that add up to 50% [no more than 3 books]) will be served as the model input.\n###figure_4### To produce an updated belief distribution over the retrieval candidates after each turn, we need to integrate all the information that the user has provided thus far. We prompt another GPT-3 model to synthesize a description of the retrieval candidate given the interaction history thus far, which can be thought of as a posterior belief over the true item, represented in language. We then use this language posterior as the search query to produce a new candidate distribution, instead of the initial query provided by the user.\nWe train Flan-T5-base Chung et al. (2022  ###reference_b2###  ###reference_b2###) to generate the selected clarification questions.\nThe model takes in the user\u2019s initial query, the interaction history of questions and answers, and the information of the top 50% (no more than 3) confident retrieval candidates to generate the next clarification question.\nFor the books dataset, the information for each retrieval candidate includes the book title, author, published dates, and description. We additionally include the current ranking and retrieval probability, concatenating all the information as text to form the full context for a book.\nWe use a Fusion-in-Decoder architecture (FiD) Izacard and Grave (2020  ###reference_b4###  ###reference_b4###), where we concatenate the information for each retrieval candidate with the initial query and interaction history and feed it into the encoder independently.\nThe encoded candidates are then concatenated, which the decoder then attends to in order to generate clarification questions.\nWe train the model for 10k gradient steps using the Adam optimizer with a learning rate of , batch size 8, and dropout 0.1. We evaluate the models at intervals of 500 steps and select the checkpoint with the best BLEU score on a held-out validation.\n###figure_5###"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In our experiments, we aim to answer the following questions:  \nHow does our method compare to well-studied methods for asking both quantitatively (how effectively do our system\u2019s questions help us narrow down the user\u2019s item) and qualitatively (how do our system\u2019s questions differ)?  \nHow important is conditioning on information from the retriever vs. purely dialogue- or prompting-based approaches that reason purely in text and generate questions conditioned only on the dialogue history?  \nWhat types of questions are helpful in increasing confidence in the target and ultimately achieving successful retrieval?  \n\nIn this section, we present the system\u2019s empirical performance by evaluating it in a book retrieval setting. In one retrieval game, the system will be given an initial query that vaguely describes the book that the user is looking for. The system will then be allowed to ask 9 clarification questions to identify the book (a total of 10 turns including the initial query).  \nWe run experiments with a simulated user, which we implement as a GPT-3 model prompted with oracle information about the target book, removing the title so that the user simulator cannot output the name of the target item outright. We prompt the user simulator to mimic a user searching for the book with a vague memory about the content.  \n\nWe compare model performance on cumulative retrieval success, where an interaction up to turn is counted as successful if the correct item is retrieved at any of the turns and non-cumulative retrieval success, where the interaction is successful only if the correct item is identified at turn.  \nWe additionally show trends for mean reciprocal rank (MRR), the average reciprocal rank of the correct item, to contextualize model performance.  "
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": "We compare Clarinet to a purely dialogue-based approach that randomly generates a question conditioned only on the dialogue history, without access to the retriever distribution. This model simply randomly samples a question from the pool of candidate questions generated by GPT-3, prompted with the initial query and dialogue history. Additionally, we implement two information theoretic approaches to clarification question generation frequently used in prior work Rao and Daum\u00e9 III (2018  ###reference_b11###); Yu et al. (2019  ###reference_b16###); White et al. (2021  ###reference_b15###). These approaches typically generate a pool of candidate questions at each turn, using different notions of question usefulness to select the best question to ask. In contrast to Clarinet, because these approaches explicitly evaluate candidate questions, they are much more expensive at inference time. We implement question selection based on expected information gain and Kullback-Liebler (KL) divergence, which we describe in more detail below."
        },
        {
            "section_id": "4.1.1",
            "parent_section_id": "4.1",
            "section_name": "4.1.1 Expected Information Gain (EIG)",
            "text": "Expected information gain (EIG) selects clarification questions that are most likely to yield the most information, in expectation over potential user responses. Formally, at turn we want to choose the question that optimizes:\n\nwhere  is the model\u2019s current posterior belief over the true item  with the information up to turn .\n\n is the information gain (or equivalently, entropy reduction) in the candidate distribution  from observing answer , given that we asked :\n\nThus, to find the most informative question, the optimization problem can be simplified to:\n\nwhere the answer distribution is obtained by marginalizing over the current belief distribution  (subscript  omitted):\n\nTo compute the answer distribution , we answer every question  for every candidate  with a pre-trained Flan-T5-base model. The answer probabilities  are estimated by softmaxing over the summation of logits of tokens across every generation step."
        },
        {
            "section_id": "4.1.2",
            "parent_section_id": "4.1",
            "section_name": "4.1.2 KL Divergence",
            "text": "The previous heuristic function helps pick the question candidate with the highest expected information gain. However, when there are only a few books with high confidence, the question selector using EIG will only try to select a question that differentiates between the top candidates. As a result, the selected questions in the subsequent turns can become very similar. Therefore, we\u2019d also like a heuristic function that selects the question that is likely to change the current belief distribution.\n\nTo estimate the posterior after observing an answer to a question, we answer every question for every candidate item with a pretrained Flan-T5-base model, like the EIG baseline. The posterior is estimated by computing the cosine similarity between the answers calculated from different (question, book) pairs, where the language embedding is based on the referenced answer."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Results",
            "text": "In Figure 2, we show that Clarinet achieves higher retrieval success than the dialogue-only baseline that randomly selects questions given only the dialogue context, as well as outperforming the information theoretic approaches. The performance of random question selection plateaus after a certain number of turns."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "###figure_7### We evaluate the effect of summarizing the interaction so far into language posterior, compared to maintaining a explicit posterior belief distribution that is updated at each turn with Bayes\u2019 Rule, i.e. . We compare random question selection with each of these posterior belief representations. As shown in Figure 4  ###reference_###, the language posterior has significantly higher retrieval performance. We observe that the MRR of the true item decreases with more questions when using an explicit posterior, despite the fact that more information should theoretically always help the model improve its belief about the true item. Qualitatively, we observe that the explicit posterior fails because it is less robust to errors, which may compound at each point in the system or over course of several turns. For example, if the human or user simulator provides inaccurate information to the system or if the retrieval model fails to interpret a particular user response, a single turn can have a large impact on the belief distribution that is hard to correct.\nAdditionally, single turn responses to clarification questions are more out-of-distribution for the retriever, whereas the language posterior synthesizes the interaction history into something that looks like a hypothetical query.\nOne downside of the language posterior that we observed qualitatively was that initial queries, which were often much longer than responses to clarification questions, were over-represented in the language posterior. Future work may improve gains in the early turns by emphasizing or upweighting the question responses when retrieving in the early turns.\nWe also compare the performance of another variant of our model as well as a fine-tuned FLAN-T5-base model. In figure 3  ###reference_### the variant annotated with \"top 10\" is trained with only the questions that help the retriever rank the target book as top 10, as opposed to the \"delta\" model that is trained with the questions that either help the retriever ranks the target as top 10 or increase the target\u2019s rank by 10. The better performance of the \"delta\" model suggests that the questions that help increase the target\u2019s rank are as important as questions that give the target a high absolute rank. The finetuned t5 model is trained with the same dataset and parameters as the \"delta\" model except that the initial query, interactions, and top book information will be put into a single text string and fed into the encoder.\nThe comparison between our model with FiD architecture and the fine-tuned t5 suggests that it is very hard for the model to explicitly represent uncertainty given text descriptions with small data. Instead, encoding the passages separately would help the model ask a more useful question that could more efficiently identify the target."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Qualitative Analysis",
            "text": "To understand how the kinds of clarification questions that models ask may differ, we label every 100 questions generated by each model with GPT-3, prompting it to label each question as one of four categories or \u201cother.\u201d\nAccording to figure 5  ###reference_###, the clarification questions generated by Clarinet are shown qualitatively different than the other methods. Specifically, it does not generate many binary questions as it might learn that the binary questions are not providing as much information. Additionally, Clarinet generates more questions that are specific to characters or events than the other methods. This behavior could potentially show that events and scenes are one of the determining factors that distinguish between books.\nAs Figure 5  ###reference_### suggests, there are also notable differences between the information gain and KL divergence heuristics for question selection. The KL divergence approach tends to ask more binary questions and fewer describe-type questions than EIG question selector. The behavior is reasonable as KL heuristic likes to select questions that could potentially shift the distribution a lot. A binary (yes-no) question is more likely to change the distribution than an open-ended question as the answer to the binary question is polarized. In general, different question-asking behaviors may be effective in different contexts; while KL performed better on retrieval accuracy in our settings, open-ended questions provide more information than a binary question in theory. The type of behavior that is preferable may depend on how the user responds to queries, the robustness of the retrieval system, among other qualities."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Additional Analysis",
            "text": "Next, we investigate the effect of the language posterior, FiD architecture, and gain in rank for train-time question selection.\nWe evaluate the effect of summarizing the interaction so far into language posterior, compared to maintaining a explicit posterior belief distribution that is updated at each turn with Bayes\u2019 Rule, i.e. . We compare random question selection with each of these posterior belief representations. As shown in Figure 4  ###reference_###  ###reference_###, the language posterior has significantly higher retrieval performance. We observe that the MRR of the true item decreases with more questions when using an explicit posterior, despite the fact that more information should theoretically always help the model improve its belief about the true item. Qualitatively, we observe that the explicit posterior fails because it is less robust to errors, which may compound at each point in the system or over course of several turns. For example, if the human or user simulator provides inaccurate information to the system or if the retrieval model fails to interpret a particular user response, a single turn can have a large impact on the belief distribution that is hard to correct.\nAdditionally, single turn responses to clarification questions are more out-of-distribution for the retriever, whereas the language posterior synthesizes the interaction history into something that looks like a hypothetical query.\nOne downside of the language posterior that we observed qualitatively was that initial queries, which were often much longer than responses to clarification questions, were over-represented in the language posterior. Future work may improve gains in the early turns by emphasizing or upweighting the question responses when retrieving in the early turns.\nWe also compare the performance of another variant of our model as well as a fine-tuned FLAN-T5-base model. In figure 3  ###reference_###  ###reference_### the variant annotated with \"top 10\" is trained with only the questions that help the retriever rank the target book as top 10, as opposed to the \"delta\" model that is trained with the questions that either help the retriever ranks the target as top 10 or increase the target\u2019s rank by 10. The better performance of the \"delta\" model suggests that the questions that help increase the target\u2019s rank are as important as questions that give the target a high absolute rank. The finetuned t5 model is trained with the same dataset and parameters as the \"delta\" model except that the initial query, interactions, and top book information will be put into a single text string and fed into the encoder.\nThe comparison between our model with FiD architecture and the fine-tuned t5 suggests that it is very hard for the model to explicitly represent uncertainty given text descriptions with small data. Instead, encoding the passages separately would help the model ask a more useful question that could more efficiently identify the target."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Discussion & Conclusion",
            "text": "Traditional heuristic approaches like EIG are often shown useful in the context of closed-form question selections where there is only a fixed number of vocabulary/answers and a limited amount of questions. The computation time increases dramatically if the answer becomes open-ended. Although the retrieval performance using KL as the question selection function is decent, it needs to estimate the resulting confidence distribution of every question in the candidate pool. It will be very costly and inefficient to run in a real-time interactive system.\n\nWe presented an interactive retrieval system that helps users retrieve books by asking open-ended clarification questions, finetuning a LLM to generate informative questions end-to-end. Our Clarinet model adopts an architecture that encodes the query, interactions, and passages separately so that the model could learn to ask questions that help identify the target more quickly with limited training data. We show that this approach can effectively distill the search over questions into the model, resulting in much cheaper inference while outperforming methods like EIG and KL that explicitly evaluate the usefulness of clarification questions at inference time."
        }
    ],
    "url": "http://arxiv.org/html/2405.15784v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.1.1",
            "4.1.2",
            "4.2",
            "5",
            "5.1",
            "5.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5",
            "5.1",
            "5.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.15784v1",
        "paper_title": "Clarinet: Augmenting Language Models to Ask Clarification Questions for Retrieval",
        "research_background": "### Paper\u2019s Motivation:\nThe paper is motivated by the inherent challenge of handling underspecified and ambiguous user queries in natural language interactions, particularly in the domain of information retrieval. While modern large language models can generate coherent questions, they often fail to ask questions that effectively reduce the uncertainty in the search process. This motivation stems from the need to improve search accuracy and efficiency through better question-asking mechanisms.\n\n### Research Problem:\nThe core research problem the paper addresses is how to integrate the conversational abilities of large language models (LLMs) with retrieval systems to ask clarification questions that effectively reduce uncertainty in information retrieval. The objective is to develop a method that can generate high-quality clarification questions that help refine user queries and thereby improve retrieval accuracy without the expensive computation costs traditionally associated with heuristic-based methods.\n\n### Relevant Prior Work:\n1. **Search Ambiguity in Information Retrieval:**\n   - Keyvan and Huang (2022) have studied the challenges associated with search ambiguity in information retrieval.\n\n2. **Capabilities and Limitations of LLMs:**\n   - Existing large language models can generate coherent questions (general observation), but these questions often do not focus on the model's uncertainty.\n\n3. **Information-Theoretic Approaches:**\n   - Prior methods using principled information-theoretic measures such as information gain and KL utility have been proposed to evaluate and select the right questions:\n     - Oaksford and Chater (1994)\n     - Van Rooy (2004)\n     - Nelson et al. (2010)\n     - Rothe et al. (2017)\n     - Hawkins and Goodman (2017)\n   - However, these methods require expensive inference-time computation and have not scaled beyond toy settings.\n\n4. **Example Dataset and Domain of Application:**\n   - Bhargav et al. (2022) introduced a real-world dataset from Goodreads where users ask for help locating vague recollections of books, providing a relevant context for evaluating clarification question models.\n\n### Contribution of the Paper:\nThe paper introduces Clarinet, a novel framework that augments a language model to condition on the retriever distribution and finetunes it end-to-end to generate informative clarification questions. This method distills the search over good questions into a model, making it simpler and cheaper at inference time. The approach integrates user interactions and summarization of interaction history to improve retrieval accuracy, demonstrating superior performance compared to existing dialogue-based and heuristic-based methods in a specified dataset scenario.",
        "methodology": "The methodology section describes a system named Clarinet designed to aid users in a \"tip of the tongue\" (TOT) search setting. Specifically, it helps users who need to recall and identify items, such as books, for which they can only remember vague details. The methodology consists of several key steps and components, detailed as follows:\n\n1. **Dataset and Training Split**:\n   - **Dataset**: Utilizes a dataset from Bhargav et al. (2022) containing multi-turn interactions from community forums focused on book queries, resulting in 784 interactions.\n   - **Split**: The dataset is divided into 156 interactions for training and 628 for evaluation.\n   - **Retrieval Database**: Comprises a corpus of 1877 book-related documents.\n\n2. **Retriever Model**:\n   - **Choice of Retriever**: While Clarinet is model-agnostic, the experiments use the Dense Passage Retriever (DPR) by Karpukhin et al. (2020).\n   - **Finetuning**: The DPR model is finetuned using the larger scale WhatsThatBook dataset, which contains 11,552 initial query-document pairs specifically for TOT book retrieval tasks.\n\n3. **Clarinet Model**:\n   - **Query Synthesis and Ranking**: Uses dot-product similarity between DPR representations of book information (titles, authors, metadata, synopsis) and DPR representations of synthesized queries from the Clarinet model for ranking candidate books.\n\n4. **Clarification Question Generation**:\n   - **Generation Process**: \n     - At each turn, a pool of 50 candidate clarification questions is generated using GPT-3 (gpt-3.5-turbo-0613), based on the user\u2019s initial query and dialogue history.\n     - 20 candidate questions are generated per turn by sampling with a temperature of 0.8.\n   - **User Simulation**:\n     - For each candidate question, responses are simulated using a GPT-3-based user simulator model with access to the true item and its description.\n   - **Training and Evaluation**:\n     - Uniform sampling from the candidate pool and updating the retrieval distribution through explicit posterior are performed.\n     - Only effective questions that improve the ranking of the target book significantly are included in training data.\n     - User interactions and information on top candidate books serve as model input.\n\n5. **Ranking Iteration**:\n   - A GPT-3 model synthesizes a description of the retrieval candidate given the interaction history to produce a posterior belief over the true item.\n   - This description is used as the search query to update the candidate distribution.\n\n6. **Question Generation Model**:\n   - **Model Choice**: Trains the Flan-T5-base model to generate selected clarification questions.\n   - **Inputs**: User's initial query, interaction history, and top retrieval candidates' information (limited to three candidates, who collectively hold a retrieval confidence of 50%).\n   - **Architecture**: Utilizes the Fusion-in-Decoder (FiD) architecture, where each retrieval candidate's information is processed through the encoder independently, and the encoded candidates are concatenated for the decoder to generate clarification questions.\n   - **Training Parameters**:\n     - The model undergoes 10,000 gradient steps with the Adam optimizer using a learning rate, batch size of 8, and dropout of 0.1.\n     - Evaluation occurs at intervals of 500 steps, with the best BLEU score on a held-out validation set determining the checkpoint selection.\n\nThis methodological framework indicates that Clarinet iteratively updates the question and retrieval processes to improve the user's ability to identify the correct item based on vague memory through clarification dialogues, leveraging advanced language models and specialized retrieval techniques.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Objective:** The main objective is to evaluate the ability of the system to ask effective clarification questions in order to improve book retrieval accuracy. The experiment aims to answer three primary questions:\n1. How does the proposed method compare to established methods both quantitatively and qualitatively in asking effective questions?\n2. The importance of conditioning on information from the retriever versus purely dialogue- or text-prompting based approaches.\n3. The types of questions that are most beneficial in increasing confidence in identifying the target book and achieving successful retrieval.\n\n**Experimental Context:** The evaluation is conducted in a book retrieval setting, where the system is given a vague initial query about a book and is allowed to ask up to 9 clarification questions (making a total of 10 turns including the initial query) to identify the target book.\n\n**User Simulation:** A simulated user is implemented using GPT-3, which is prompted with oracle information about the target book\u2014but crucially, the title is removed to prevent the simulator from directly providing the name of the target item. This setup mimics a user searching for a book with a vague memory of its content.\n\n**Datasets:** The dataset includes vague queries about books for which clarification questions can be asked. The exact dataset details (size, source, etc.) are not provided in the section, but it involves book-related queries and information.\n\n**Baselines:** While the specific baselines are not enumerated in the section, it's implied that the system's performance is compared against other well-studied methods for asking clarification questions in retrieval tasks.\n\n**Evaluation Metrics:**\n1. **Top-1 Retrieval Accuracy:** Measures the accuracy of retrieving the correct book as the top candidate.\n2. **Cumulative Retrieval Success:** Counts an interaction as successful if the correct item is retrieved at any turn up to the current turn.\n3. **Non-Cumulative Retrieval Success:** Counts an interaction as successful only if the correct item is the top candidate at the final turn.\n4. **Mean Reciprocal Rank (MRR):** Evaluates the average reciprocal rank of the correct item to provide additional context beyond top-1 accuracy.\n\n### Main Experimental Results\n\nThe results for the main experiment are focused on the retrieval success and quality of the questions generated by the system:\n\n- **Top-1 Retrieval Accuracy:** The system's ability to identify the correct book as the top candidate based on the clarification questions asked.\n- **Cumulative Retrieval Success:** The success rate of retrieving the correct book throughout the interaction turns.\n- **Non-Cumulative Retrieval Success:** The success rate of retrieving the correct book specifically at the final turn.\n- **Mean Reciprocal Rank (MRR):** Provides a nuanced view of the ranking accuracy of the retrieved book.\n\nThe system's performance on these metrics is compared to other established methods to highlight its effectiveness in improving book retrieval through effective clarification questions. Specific numerical results and comparisons are not detailed in this section but would provide insights into the system's relative performance across different metrics and conditions."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Evaluate the effect of summarizing interaction history into a language posterior versus maintaining an explicit posterior belief distribution updated at each turn using Bayes' Rule.",
            "experiment_process": "We compare random question selection using two posterior belief representations: a language posterior (summarizing interactions into language) and an explicit posterior belief distribution (updated at each turn with Bayes' Rule). The comparison is quantitatively analyzed through Mean Reciprocal Rank (MRR) of the true item and qualitatively observed for robustness to errors. Additionally, single turn responses, out-of-distribution occurrences, and over-representation of initial queries are studied qualitatively.",
            "result_discussion": "The language posterior has significantly higher retrieval performance than the explicit posterior. The MRR of the true item decreased with more questions using the explicit posterior, indicating a failure in robustness to errors. Single turn responses to clarification questions using explicit posterior compounded errors, showing the language posterior synthesizes interaction history better by appearing like a hypothetical query. A downside observed with the language posterior was the over-representation of initial lengthy queries early on.",
            "ablation_id": "2405.15784v1.No1"
        },
        {
            "research_objective": "Compare the retrieval performance of a variant model trained with different question selection criteria versus a fine-tuned FLAN-T5-base model.",
            "experiment_process": "We run comparative evaluations using two model variants: one annotated with 'top 10' (trained with questions helping rank the target book in the top 10) versus the 'delta' model (trained with questions that either rank the target as top 10 or boost its rank by 10). The FLAN-T5-base model is fine-tuned on the same dataset with the initial query, interactions, and top book information concatenated into a single text string for input to the encoder.",
            "result_discussion": "The 'delta' model outperformed the 'top 10' variant, indicating that questions boosting the target rank significantly affect performance. The FLAN-T5-base model, when compared to our FiD architecture model, demonstrated that representing uncertainty explicitly with text descriptions is challenging with small datasets. Instead, encoding passages separately as per FiD architecture facilitated asking more useful questions.",
            "ablation_id": "2405.15784v1.No2"
        }
    ]
}