{
    "title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs",
    "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs.\nDifficulties lie in assessing the factuality of free-form responses in open domains.\nAlso, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress.\nTo mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs.\nOpenFactCheck consists of three modules:\n(i) CustChecker allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims,\n(ii) LLMEval, a unified evaluation framework assesses LLM\u2019s factuality ability from various perspectives fairly, and (iii) CheckerEval is an extensible solution for gauging the reliability of automatic fact-checkers\u2019 verification results using human-annotated datasets.\nOpenFactCheck is publicly released at https://github.com/yuxiaw/OpenFactCheck.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large language models (LLMs) have demonstrated impressive capabilities in generating naturally-sounding answers over a broad range of human inquiries.\nHowever, GPT-4 (OpenAI, 2023  ###reference_b24###) and other text generation models still frequently produce content that deviates from real-world facts (Bang et al., 2023  ###reference_b2###; Borji, 2023  ###reference_b4###; Guiven, 2023  ###reference_b12###).\nThis degrades the system performance and undermines its reliability, representing a significant bottleneck in the deployment (Chuang et al., 2023  ###reference_b7###; Geng et al., 2023  ###reference_b10###).\nMany studies have explored evaluating and improving the factuality of LLMs Lee et al. (2022  ###reference_b16###); Chuang et al. (2023  ###reference_b7###); Shi et al. (2023  ###reference_b26###); Chen et al. (2023  ###reference_b5###).\nTwo challenges have been identified for evaluation: (1) it is difficult to assess the factuality of open-domain free-form responses, and (2) different papers use different evaluation datasets and measurements, rendering them hard to compare and hampering future progress Wang et al. (2024  ###reference_b29###).\nTo mitigate these issues, we introduce an open-sourced fact-checking system OpenFactCheck.\n###figure_1### It includes three modules as shown in Figure 1  ###reference_###. CustChecker allows users to customize an automatic fact-checker and to verify free-form documents to alleviate the first problem.\nA unified LLM factuality evaluation module LLMEval applies seven factuality-specific benchmarks to assess the LLM factuality ability from different aspects and then produces a report to illustrate the weakness and offer improvement advice, tackling the second challenge.\nWe further incorporate CheckerEval that assesses the verification accuracy of fact-checkers, equipped with a leaderboard in terms of accuracy, latency, and costs, aiming to encourage the development of advanced automatic fact-checking systems.\nThe three modules collaborate and help each other.\nThe results of human verification derived from LLMEval can be used as the benchmark for evaluating the accuracy of automated fact-checkers. Simultaneously, the most effective checker identified in CheckerEval can be deployed for automated fact-checking tasks.\nEach fact-checker in CheckerEval can be an implementation in CustChecker. Complex user inquiries may be considered as potential candidates included the factuality assessment dataset utilized in LLMEval.\nUsers can tailor their checkers according to their specific needs, such as domain specialization, cost-effectiveness, or rapid processing, and identify factual errors for both human-written text (a claim or document) and the outputs of LLMs.\nLLM researchers and practitioners can directly submit their LLM responses to our LLMEval platform by downloading our question set.\nSubsequently, we conduct evaluations to assess the model\u2019s factual accuracy and to generate a report analyzing the model performance from multiple aspects.\nSimilarly, developers who seek to evaluate and to compare the efficacy of their fact-checking systems to other ones fairly can upload their checker\u2019s verification outcomes to CheckerEval.\nThen, our system will show the ranking information in the leaderboard after evaluating under the same measurements.\nFrom the perspective of developers and contributors to this open-source project, we encourage extensive implementation of unique, effective, and robust claim processors, retrievers and verifiers within fact-checking pipelines, collections of challenging questions that LLMs tend to make factual errors, and human-annotated fine-grained verification examples. We believe that this will help to promote and to advance future research on LLM factuality.\nTo sum, this work investigates three research questions in LLM factuality evaluation:\nhow to effectively identify factual errors in an LLM response;\nhow to systematically evaluate the factuality ability of an LLM;\nwhich automatic fact-checker is the best, and which component dominates the final verification accuracy.\nWe initiate an open-source project and develop a preliminary version implementing the three modules, which is anticipated to serve as a stepping stone to facilitate future endeavors in this domain."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Background",
            "text": "Fact-checking is the task of assessing whether claims made in writing are manipulated or true.\nObservations above motivate us to integrate these three closely-related components into one platform to facilitate (i) users to flexibly configurate an automatic fact-checking system to verify the factuality of claims and documents, (ii) LLM developers to evaluate LLM\u2019s factuality under the same measurement scale including the task, dataset, and metrics, and (iii) researchers to assess the fact-checkers reliability under the fine-grained annotated benchmarks."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Fact-checking Systems",
            "text": "Many recent papers have described automatic fact-checking systems used to evaluate the factuality of LLM responses, such as RARR, FactScore, FacTool, CoVe, and Factcheck-GPT Gao et al. (2022  ###reference_b9###); Min et al. (2023  ###reference_b22###); Chern et al. (2023  ###reference_b6###); Dhuliawala et al. (2023  ###reference_b8###); Wang et al. (2023  ###reference_b28###).\nEach checker has unique characteristics designed for specific domains or scenarios.\nRARR verifies a document as a whole and can generate an attribution report to explain factual errors. FactScore retrieves evidence from an offline Wikipedia dump mainly for the biography. FacTool is friendly to users with low latency, CoVe completely depends on the capability of LLMs, and Factcheck-GPT has a fine-grained pipeline to localize intermediate errors.\nUnlike the above work, our aim is to enable the easy customization of a fact-checker according to users\u2019 requirements and application scenarios, e.g., offline settings with a limited budget, by simply clicking dropdowns to choose offline retrievers and verifiers supported by small models without calling APIs.\nDespite different designs and implementations of various checkers, they generally consist of three modules: (1) claim processor, which extracts context-independent atomic claims from a document, (2) evidence retriever, which searches related passages from the Internet or database, and then ranks them by relevance, and (3) verifier, which determines the claim/document factuality based on the collected evidence (Guo et al., 2022  ###reference_b13###; Li et al., 2023b  ###reference_b18###; Wang et al., 2024  ###reference_b29###).\nTo this end, we first unify different fact-checking systems into a unified pipeline with the three modules. Then, given a module, users can select a developed module from various implementations or develop one by themselves.\nIn addition, the framework supports easy migration of existing fact-checking systems to our demo.\nHowever, the verification results of automatic fact-checkers are not necessarily accurate.\nHow to evaluate and improve the accuracy of automated fact-checkers is critical. The accuracy of automatic fact-checkers serves as a confidence and a reliability signal for the verification results."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Automatic Evaluation of Fact-Checking Systems",
            "text": "How accurate are current fact-checking systems? Can they effectively serve as proxies for evaluating the factual accuracy of language models?\nExisting automatic fact-checking studies often first collect a set of human-annotated (LLM response, extracted claims, factuality of the claims), and then quantify the effectiveness of their systems by comparing the final verification results (i.e., whether a claim or a document is factually true or false) to human-annotated labels Min et al. (2023  ###reference_b22###); Chern et al. (2023  ###reference_b6###); Dhuliawala et al. (2023  ###reference_b8###).\nRecent work on long-form factuality in LLMs also demonstrates a statistically significant correlation between the outputs by automatic fact-checkers and labels by human annotators (Wei et al., 2024  ###reference_b30###).\nThus, we merge four human-annotated LLM factuality datasets including FacTool-QA, FELM-WK, Factcheck-Bench, and HaluEval, and then compare them to the results of automatic fact-checkers to assess the performance of fact-checking systems."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "LLM Factuality Evaluation",
            "text": "Relevance vs. Factuality There are subtle differences between evaluating LLM\u2019s general performance and factuality.\nQuestion answering (QA) datasets over various domains are always used for general performance evaluation (Hendrycks et al., 2021  ###reference_b14###). The focus is on judging whether the question is answered correctly. If the model\u2019s response contains the correct answer, it counts; otherwise, it is void even though the response presents all facts.\nWhile research on factuality concentrates more on whether the response presents facts aligning with world knowledge even if some statements were irrelevant to the question.\nTherefore, instead of using datasets for general performance assessment, we selected seven datasets that are specifically collected for factuality evaluation.\nThey were selected to cover as diverse potential factual errors as possible, including aspects of vulnerabilities of snowballing hallucinations, awareness of self-uncertainty, robustness to false-premise questions, fresh questions with answers changing fast over time, and free-form responses spanning distinct domains, topics, and tasks.\nAfter evaluations over datasets from different perspectives, an analysis report will be generated to demonstrate the advantage and weaknesses of the model and the potential solutions to improve.\nObservations above motivate us to integrate these three closely-related components into one platform to facilitate (i) users to flexibly configurate an automatic fact-checking system to verify the factuality of claims and documents, (ii) LLM developers to evaluate LLM\u2019s factuality under the same measurement scale including the task, dataset, and metrics, and (iii) researchers to assess the fact-checkers reliability under the fine-grained annotated benchmarks."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Design and Implementation",
            "text": "Our OpenFactCheck platform is implemented by a Python server, a web user interface, and a database, deployed via AWS.\nThe Python backend can also be used as a Python toolkit, allowing easy and flexible development.\nOpenFactCheck\u2019s design emphasizes two principles: (i) customizability and extensibility for both users and developers, and (ii) compatibility with existing methods and datasets.\nIt consists of three modules: CustChecker, LLMEval, and CheckerEval. Below, we present the detailed design and implementation of each component.\nWe unify diverse fact-checking systems as a procedure with three steps, abstracted into three classes: a claim_processor, a retriever, and a verifier.\nThe instances of the three classes are sequentially linked into a pipeline, solving the following tasks: (1) decomposing a document into atomic claims, (2) collecting relevant passages given a claim, and (3) making a true/false judgment given both the claim and the evidence as input (see Figure 2  ###reference_###). We refer to them as task solvers.\nThe implementation of a task solver can be flexible, just ensuring that the input and the output are aligned with the abstract class definitions. For example, evidence can be retrieved by calling SerpAPI or by searching Wikipedia using BM25, but we must return a list of relevant passages given an input claim.\nMoreover, task solvers in our pipeline are not hardcoded, but can be configured through a yaml configuration file. Thus, users can combine task-solver implementations from different frameworks (e.g., using Factcheck-GPT\u2019s claim processor, RARR\u2019s retriever, and FacTool\u2019s verifier) and start the verification from any step. For example, users can start from the step of retrieval when the input document does not need decomposition.\nThis functionality is achieved using a message-passing mechanism that uses a success flag as the message indicates whether the current task solver successfully executes and returns the expected output.\nThe success flag passes through the configured order of the pipeline, guaranteeing that the output of the preceding solver fits the input for the current solver, otherwise error warning will be issued.\nPractically, the input and the output parameter names for the task solvers are defined in the configuration file. To link different solvers into a pipeline, one only needs to ensure that the current solver output name matches the input name of the succeeding solver. A dictionary format fact-checking-state is kept throughout the pipeline to store all information in the verification.\nInspired by Fairseq (Ott et al., 2019  ###reference_b25###), our framework is designed to be highly extendable by treating any third-party task solvers as plugins.\nAs long as the developed task solvers adhere to our class interface definitions, they can be imported and used in our framework.\nIn summary, due to the customizable and extendable nature of CustChecker, general users of OpenFactCheck can utilize it as a fully functional application with web-based user interfaces.\nAdvanced developers have the flexibility to use it as a library, allowing them to develop their task solvers or to integrate existing solvers.\nWe collect a set of questions by gathering questions from seven existing corpora that is collected deliberately to assess LLM\u2019s factuality, including Snowball (Zhang et al., 2023a  ###reference_b34###), SelfAware (Yin et al., 2023  ###reference_b33###), FreshQA (Vu et al., 2023  ###reference_b27###), FacTool (Chern et al., 2023  ###reference_b6###), FELM-WK (Chen et al., 2023  ###reference_b5###), Factcheck-GPT (Wang et al., 2023  ###reference_b28###) and FactScore-Bio, a total of 6,480 examples shown in Table 1  ###reference_###, referring to FactQA (see dataset details in Appendix A.1  ###reference_###).\nEach example includes the following fields: question, domain, topic, ability to test, task and source, where the domain and the topic are identified using GPT-4 based on the (question, reference response), and others are either inherited or summarized from the original datasets.111We used GPT-4 response as a reference response for a question as it is more likely to provide a relevant and correct answer, assisting the identification of domains and topics.\nTo concretely analyze models\u2019 vulnerability, we tag three labels for each question from the perspective of the knowledge domain, the topic, and the potential error type if a LLM generates a factually incorrect response.\nDomains include a general domain, law, biomedical, clinical, science and so on. Given a domain, we further define fine-grained topics for the question.\nThree common error types are included.\nType1: Knowledge error is the most common error, occurring when the model produces hallucinated or inaccurate information due to lacking relevant knowledge.\nThat is, LLMs lack relevant knowledge or internalize false knowledge in the pre-training stage or in the problematic alignment process.\nHowever, LLMs do not know what they do not know, sometimes overestimate their capacities and confidently output unknown information, leading to false responses.\nMitigating such errors require: (a) learning and correcting parametric knowledge through the curation of corpora used in pre-training, supervised fine-tuning (SFT) and alignment, (b) augmenting by external knowledge in inference, (c) calibrating models to be aware of unknowns, and (d) configuring the decoding strategies (sample/beam-search, temperature), balancing diversity and accuracy Zhang et al. (2023b  ###reference_b35###); Wang et al. (2024  ###reference_b29###).\nType2: Over-commitment error occurs when the model fails to recognize the falsehoods (or jokes) inherent in the prompt or previously-generated context, and provides an inaccurate or inappropriate response.\nThe left-to-right generation strategy used by LLMs poses potential risks that LLMs sometimes over-commit to the false premise in the context, even when they recognize they are incorrect (Zhang et al., 2023b  ###reference_b35###).\nTo address this issue, engineering better prompts is helpful, such as explicitly instructing models to first detect false premises in the prompt (Vu et al., 2023  ###reference_b27###) and asking the same question in a different way (Is 10733 a prime number?  What are the factors of 10733? Let\u2019s think step-by-step.)\nType3: Disability error happens when the model is unable to search up-to-date information to correctly answer questions whose answers change over time, e.g., What is today\u2019s gas price in New York (fast-changing), Who is the current spouse of Elon Musk (slow-changing).\nRetrieving external knowledge and augmenting it in the context would help.\nNote that we do not consider reasoning errors that arise when a claim employs flawed reasoning or faulty logic, and irrelevant error that denotes that the content is unrelated to the prompt (Chen et al., 2023  ###reference_b5###). The former highlights LLM\u2019s reasoning ability, which is more reflected in math and reasoning tasks, and the latter has more to do with response\u2019s helpfulness or human preference.\nThey are important in LLM evaluation, and may implicitly influence factuality, but we will first focus on explicit causes, leaving implicit claims for future work.\nFor questions that can be answered as Yes/No or have a short gold answer, we perform exact matching between the model responses and the gold standard answer to judge whether the response is factually correct or not, and then to calculate accuracy, such as for Snowball and SelfAware.\nThere are no short gold answers in FreshQA, and thus we use the FreshEval proposed in Vu et al. (2023  ###reference_b27###) to evaluate the correctness of model\u2019s responses, in which few-shot in-context learning based on GPT-4 is used. We use a strict evaluation criterion, considering an answer to be correct only if all the claims in the response are factually true and also up-to-date.\nFor open-domain questions from the other four datasets with free-form and long responses, in which there are no gold standard answers, we use automatic fact-checking systems augmented with retrieved world-knowledge evidence to judge the correctness at the claim-level as well as at the document level."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "CustChecker",
            "text": "CustChecker allows users to customize a fact-checking system by selecting a claim processor, a retriever, and a verifier in web pages. The current version currently supports the following fact-checking systems: RARR, FacTool and Factcheck-GPT (Gao et al., 2022  ###reference_b9###; Chern et al., 2023  ###reference_b6###; Wang et al., 2023  ###reference_b28###).\nUsers input either human-written text or outputs of LLMs into the box (see Figure 4  ###reference_###), and then the fact-checker defined above will process and detect factual errors, showing the verification results including evidence, judgment, and explanations.\n###figure_2### We unify diverse fact-checking systems as a procedure with three steps, abstracted into three classes: a claim_processor, a retriever, and a verifier.\nThe instances of the three classes are sequentially linked into a pipeline, solving the following tasks: (1) decomposing a document into atomic claims, (2) collecting relevant passages given a claim, and (3) making a true/false judgment given both the claim and the evidence as input (see Figure 2  ###reference_###  ###reference_###). We refer to them as task solvers.\nThe implementation of a task solver can be flexible, just ensuring that the input and the output are aligned with the abstract class definitions. For example, evidence can be retrieved by calling SerpAPI or by searching Wikipedia using BM25, but we must return a list of relevant passages given an input claim.\nMoreover, task solvers in our pipeline are not hardcoded, but can be configured through a yaml configuration file. Thus, users can combine task-solver implementations from different frameworks (e.g., using Factcheck-GPT\u2019s claim processor, RARR\u2019s retriever, and FacTool\u2019s verifier) and start the verification from any step. For example, users can start from the step of retrieval when the input document does not need decomposition.\nThis functionality is achieved using a message-passing mechanism that uses a success flag as the message indicates whether the current task solver successfully executes and returns the expected output.\nThe success flag passes through the configured order of the pipeline, guaranteeing that the output of the preceding solver fits the input for the current solver, otherwise error warning will be issued.\nPractically, the input and the output parameter names for the task solvers are defined in the configuration file. To link different solvers into a pipeline, one only needs to ensure that the current solver output name matches the input name of the succeeding solver. A dictionary format fact-checking-state is kept throughout the pipeline to store all information in the verification.\nInspired by Fairseq (Ott et al., 2019  ###reference_b25###  ###reference_b25###), our framework is designed to be highly extendable by treating any third-party task solvers as plugins.\nAs long as the developed task solvers adhere to our class interface definitions, they can be imported and used in our framework.\nIn summary, due to the customizable and extendable nature of CustChecker, general users of OpenFactCheck can utilize it as a fully functional application with web-based user interfaces.\nAdvanced developers have the flexibility to use it as a library, allowing them to develop their task solvers or to integrate existing solvers."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "LLMEval",
            "text": "We observed that studies assessing language models\u2019 factuality or evaluating whether the methods are effective to mitigate model hallucinations use different datasets and metrics.\nThis makes it difficult to compare, in the same conditions, the factuality of different models as well as to compare the effectiveness of different factuality enhancement approaches.\nMoreover, a lot of prior work applied datasets such as MMLU (Hendrycks et al., 2021  ###reference_b14###), StrategyQA (Geva et al., 2021  ###reference_b11###) and HotpotQA (Yang and et al., 2018  ###reference_b32###) to evaluate model\u2019s factuality.\nThese datasets tend to focus on evaluating the general performance, rather than factuality.\nTo this end, we first collect a dataset FactQA by gathering a large number of factual questions that probe diverse factual errors and span across a spectrum of domains, to fairly evaluate LLMs\u2019 factuality under the same criteria.\nWe collect a set of questions by gathering questions from seven existing corpora that is collected deliberately to assess LLM\u2019s factuality, including Snowball (Zhang et al., 2023a  ###reference_b34###  ###reference_b34###), SelfAware (Yin et al., 2023  ###reference_b33###  ###reference_b33###), FreshQA (Vu et al., 2023  ###reference_b27###  ###reference_b27###), FacTool (Chern et al., 2023  ###reference_b6###  ###reference_b6###), FELM-WK (Chen et al., 2023  ###reference_b5###  ###reference_b5###), Factcheck-GPT (Wang et al., 2023  ###reference_b28###  ###reference_b28###) and FactScore-Bio, a total of 6,480 examples shown in Table 1  ###reference_###  ###reference_###, referring to FactQA (see dataset details in Appendix A.1  ###reference_###  ###reference_###).\nEach example includes the following fields: question, domain, topic, ability to test, task and source, where the domain and the topic are identified using GPT-4 based on the (question, reference response), and others are either inherited or summarized from the original datasets.111We used GPT-4 response as a reference response for a question as it is more likely to provide a relevant and correct answer, assisting the identification of domains and topics.\nTo concretely analyze models\u2019 vulnerability, we tag three labels for each question from the perspective of the knowledge domain, the topic, and the potential error type if a LLM generates a factually incorrect response.\nDomains include a general domain, law, biomedical, clinical, science and so on. Given a domain, we further define fine-grained topics for the question.\nThree common error types are included.\nType1: Knowledge error is the most common error, occurring when the model produces hallucinated or inaccurate information due to lacking relevant knowledge.\nThat is, LLMs lack relevant knowledge or internalize false knowledge in the pre-training stage or in the problematic alignment process.\nHowever, LLMs do not know what they do not know, sometimes overestimate their capacities and confidently output unknown information, leading to false responses.\nMitigating such errors require: (a) learning and correcting parametric knowledge through the curation of corpora used in pre-training, supervised fine-tuning (SFT) and alignment, (b) augmenting by external knowledge in inference, (c) calibrating models to be aware of unknowns, and (d) configuring the decoding strategies (sample/beam-search, temperature), balancing diversity and accuracy Zhang et al. (2023b  ###reference_b35###  ###reference_b35###); Wang et al. (2024  ###reference_b29###  ###reference_b29###).\nType2: Over-commitment error occurs when the model fails to recognize the falsehoods (or jokes) inherent in the prompt or previously-generated context, and provides an inaccurate or inappropriate response.\nThe left-to-right generation strategy used by LLMs poses potential risks that LLMs sometimes over-commit to the false premise in the context, even when they recognize they are incorrect (Zhang et al., 2023b  ###reference_b35###  ###reference_b35###).\nTo address this issue, engineering better prompts is helpful, such as explicitly instructing models to first detect false premises in the prompt (Vu et al., 2023  ###reference_b27###  ###reference_b27###) and asking the same question in a different way (Is 10733 a prime number?  What are the factors of 10733? Let\u2019s think step-by-step.)\nType3: Disability error happens when the model is unable to search up-to-date information to correctly answer questions whose answers change over time, e.g., What is today\u2019s gas price in New York (fast-changing), Who is the current spouse of Elon Musk (slow-changing).\nRetrieving external knowledge and augmenting it in the context would help.\nNote that we do not consider reasoning errors that arise when a claim employs flawed reasoning or faulty logic, and irrelevant error that denotes that the content is unrelated to the prompt (Chen et al., 2023  ###reference_b5###  ###reference_b5###). The former highlights LLM\u2019s reasoning ability, which is more reflected in math and reasoning tasks, and the latter has more to do with response\u2019s helpfulness or human preference.\nThey are important in LLM evaluation, and may implicitly influence factuality, but we will first focus on explicit causes, leaving implicit claims for future work.\nFor questions that can be answered as Yes/No or have a short gold answer, we perform exact matching between the model responses and the gold standard answer to judge whether the response is factually correct or not, and then to calculate accuracy, such as for Snowball and SelfAware.\nThere are no short gold answers in FreshQA, and thus we use the FreshEval proposed in Vu et al. (2023  ###reference_b27###  ###reference_b27###) to evaluate the correctness of model\u2019s responses, in which few-shot in-context learning based on GPT-4 is used. We use a strict evaluation criterion, considering an answer to be correct only if all the claims in the response are factually true and also up-to-date.\nFor open-domain questions from the other four datasets with free-form and long responses, in which there are no gold standard answers, we use automatic fact-checking systems augmented with retrieved world-knowledge evidence to judge the correctness at the claim-level as well as at the document level."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "CheckerEval",
            "text": "Automatic fact-checking systems aim to identify whether a claim or a document is factually correct or not with/without references, but the results are not necessarily correct.\nTo assess the accuracy of automatic fact-checkers, we gather four LLM factuality benchmarks with human-annotated factual labels for three levels of granularity text: claims/segments/documents given (question, ChatGPT response) pairs, including FacTool, FELM-WK, Factcheck-GPT and HaluEval, a total of 4,835 examples, and we refer to them as FactBench.\nWe use precision, recall, and F1-score with respect to the True and False claims and documents to evaluate the effectiveness of fact-checking systems.\nThis method regards the system as a whole, only assessing the final verification results, i.e., whether a claim or a document is true or false.\nWithout the evaluation of intermediate results, it is difficult to localize which step eventually results in the erroneous factual judgment for claims Wang et al. (2023  ###reference_b28###).\nWe will incorporate the evaluation of intermediate results throughout the fact-checking pipelines in future updates."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Web Client",
            "text": "To enhance the user interaction experience, the corresponding UI interfaces have been developed for all functionalities within the demo. Specifically, we develop a web client based on Streamlit, which consists of four interfaces, each corresponding to one of the three modules, along with a leaderboard.\nCustChecker interface in Figure 4  ###reference_### primarily communicates with the CustChecker backend (\u00a73.1  ###reference_###). Users can freely select different combinations of claim processors, retrievers, and verifiers. When given an input document or claim, the CustChecker back-end executes the fact-checking pipeline. The final verification results and the intermediate processing outcomes are then presented on the page to users for reference.\nLLMEval page in Figure 5  ###reference_### corresponds to the module of LLMEval (\u00a73.2  ###reference_###). Users first download our predefined question set and then use their LLM for inference. After uploading the model responses, the system forwards them to background tasks, using our LLMEval for evaluation. Afterwards, a comprehensive report is generated and emailed to the user, notifying them of the availability of the report\u2019s PDF for download. Moreover, if users consent to publish the evaluation results, we display them on the corresponding leaderboard page.\nCheckerEval page in Figure 6  ###reference_### corresponds to the module of CheckerEval (\u00a73.3  ###reference_###), evaluating the performance of fact-checking systems.\nUsers can download claims or documents to be checked from this page, and then use their fact-checking system to predict factuality. The results including True/False, time, and USD costs are subsequently uploaded. We evaluate the submitted fact-checker results based on the ground truth labels of the human-annotated datasets, we rank and we display them on the leaderboard.\nLeaderboard page in Figure 7  ###reference_### is maintained for both the LLM factuality evaluation and the automatic fact-checking system evaluation. This leaderboard is updated in real time, allowing users to track their performance and to compare it to others. The leaderboard is accessible from the main page, providing a comprehensive overview of the system\u2019s performance.\nThe design principle of our web client is to invoke these functional modules in the form of third-party independent applications, without excessively intervening in the system\u2019s architecture. Consequently, our system is made available to users in the form of a library, a command-line toolkit, and a web application."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we first evaluate the performance of different models, and then we assess the accuracy of different automatic fact-checking systems in different settings.\n###figure_3### ###figure_4### ###figure_5### On the Snowball dataset, we observe high error rates: >80% for LLaMA-2 and 65.5% for GPT-4, similar to the evaluation results on GPT-3.5-Turbo presented in Zhang et al. (2023a  ###reference_b34###).\nHowever, when justifying previously generated content, GPT-4 can identify 87% of its own mistakes.\nTherefore, in these cases, errors are mostly attributed to the over-committing to the previously generated false context, rather than to large knowledge gaps in LLMs.\nThis phenomenon is referred to as hallucination snowballing: an LLM over-commits to early mistakes, leading to more mistakes that it otherwise would not have made.\nIts prevalence in generative models leads to factual errors for simple facts.\nHigher precision than recall is achieved on SelfAware across three models when setting a positive label as unanswerable.\nThis reveals that many truly unanswerable questions are incorrectly recognized as answerable, implying that models are always not aware of what they do not know.\nPoor performance on questions with rapidly changing answers (FreshQA) illustrates the inherent challenge of retrieving up-to-date information for LLMs.\nWe used FacTool equipped with Serper and GPT-3.5-Turbo to automatically evaluate the factuality of free-form responses over prompts in FacTool-QA, FELM-WK, and Factcheck-Bench.\nThe results are shown in Figure 3  ###reference_###, where we can make several interesting observations:\nThe percentage of true claims is in the range of 89%-94%, revealing that the vast majority of claims are verified as true.\nThe questions in FacTool-QA are relatively more challenging for the three LLMs to answer correctly than for the other two datasets, leading to a relatively lower percentage of true claims. The apparent lower number of false claims in FacTool-QA stems from its smaller dataset size, where 50 is less than 94 and 184.\nGPT-4 has the best factuality performance with a smaller number of false claims and higher percentage of true claims, followed by LLaMA-2 13 and then 7B;\nThe cost for automatic evaluation mainly depends on the number of atomic claims and the price of the backend models used in FacTool. It spends $0.02 for an atomic claim on average.\nOverall, snowballing hallucination, over-commitment to false premise, difficulty in identifying unknown knowledge and answering with up-to-date information are still challenging issues for LLMs.\nFor general open-domain questions, on average less than 10% of the claims are factually incorrect in LLM responses.\nThis somehow implies that models may poorly understand questions and their own knowledge, but they can generate correctly documents.\nThis is aligned with the recent finding that what an LLM can generate, it may not understand (West et al., 2023  ###reference_b31###).\nAdditionally, it is costly to evaluate open-domain answers even if based on automatic fact-checkers,  $30 for 100 responses based on GPT-3.5-Turbo.\nTo ensure that all checking systems verify the same sets of annotated claims with factual labels, we skip the step of extracting atomic claims from the documents.\nAll fact-checking systems get a claim as an input, and they are expected to output whether or not the claim is true.\nRecent fact-checking frameworks such as FactScore, FacTool, Factcheck-GPT and commercial retrieval-augmented generative models such as Perplexity.ai are evaluated, with evidence retrieved from Wikipedia articles or web pages, as well as with various LLM-based verifiers that judge the factuality of a claim based on their internal knowledge and retrieved evidence as a reference.\nIn Table 5  ###reference_###, we observe that automatic fact-checking systems struggle to detect false claims. Across the three datasets we experiment with, it is consistently more arduous for these systems to differentiate false claims compared to identifying true ones.\nThis challenge may arise from the tendency of returning invalid evidence for false claims.\nRetrieving evidence from the web using Serper (Google search engine results) is more effective than sourcing related passages from Wikipedia articles using BM25, given that a wider array of effective evidence is accessible on open web pages for open-domain questions.\nThe verification accuracy of an LLM-based verifier primarily relies on the capabilities of the LLM and the effectiveness of the prompts used. For instance, the overall performance of GPT-4 surpasses that of both LLaMA-3-8B and GPT-3.5-Turbo, and thus the verification results (Factcheck-GPT) outperform those of FacTool, FactScore and Perplexity.ai, despite all of them utilizing evidence sourced from the web.\nWhile Factcheck-GPT exhibits superior effectiveness, it is associated with considerable latency and substantial costs (see Table 6  ###reference_###).\nLatency and cost are largely contingent upon the implementation strategy.\nFor instance, FacTool adopts asynchronous processing and leverages Serper ($0.001 per search) in conjunction with GPT-3.5-Turbo, rendering it faster and more economical compared to Factcheck-GPT. Notably, Factcheck-GPT uses SerpAPI ($0.015 per search) alongside GPT-4, where the cost of the most affordable GPT-4 model is 20 times that of GPT-3.5-Turbo (see Figure 8  ###reference_###).\nIn summary, the efficacy of automated fact-checking systems is fundamentally dependent on implementation factors such as choice of search tool, prompts, and backend LLMs. This is primarily driven by engineering considerations."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "LLaMA-2 and GPT-4 Evaluation",
            "text": "Based on questions/instructions in FactQA, we collected responses from LLaMA-2 (7B, 13B) and GPT-4.\nAs shown in Table 3  ###reference_###, the responses of GPT-4 tend to be shorter than that of LLaMA-2.\nOn the Snowball dataset, we observe high error rates: >80% for LLaMA-2 and 65.5% for GPT-4, similar to the evaluation results on GPT-3.5-Turbo presented in Zhang et al. (2023a  ###reference_b34###  ###reference_b34###).\nHowever, when justifying previously generated content, GPT-4 can identify 87% of its own mistakes.\nTherefore, in these cases, errors are mostly attributed to the over-committing to the previously generated false context, rather than to large knowledge gaps in LLMs.\nThis phenomenon is referred to as hallucination snowballing: an LLM over-commits to early mistakes, leading to more mistakes that it otherwise would not have made.\nIts prevalence in generative models leads to factual errors for simple facts.\nHigher precision than recall is achieved on SelfAware across three models when setting a positive label as unanswerable.\nThis reveals that many truly unanswerable questions are incorrectly recognized as answerable, implying that models are always not aware of what they do not know.\nPoor performance on questions with rapidly changing answers (FreshQA) illustrates the inherent challenge of retrieving up-to-date information for LLMs.\nWe used FacTool equipped with Serper and GPT-3.5-Turbo to automatically evaluate the factuality of free-form responses over prompts in FacTool-QA, FELM-WK, and Factcheck-Bench.\nThe results are shown in Figure 3  ###reference_###  ###reference_###, where we can make several interesting observations:\nThe percentage of true claims is in the range of 89%-94%, revealing that the vast majority of claims are verified as true.\nThe questions in FacTool-QA are relatively more challenging for the three LLMs to answer correctly than for the other two datasets, leading to a relatively lower percentage of true claims. The apparent lower number of false claims in FacTool-QA stems from its smaller dataset size, where 50 is less than 94 and 184.\nGPT-4 has the best factuality performance with a smaller number of false claims and higher percentage of true claims, followed by LLaMA-2 13 and then 7B;\nThe cost for automatic evaluation mainly depends on the number of atomic claims and the price of the backend models used in FacTool. It spends $0.02 for an atomic claim on average.\nOverall, snowballing hallucination, over-commitment to false premise, difficulty in identifying unknown knowledge and answering with up-to-date information are still challenging issues for LLMs.\nFor general open-domain questions, on average less than 10% of the claims are factually incorrect in LLM responses.\nThis somehow implies that models may poorly understand questions and their own knowledge, but they can generate correctly documents.\nThis is aligned with the recent finding that what an LLM can generate, it may not understand (West et al., 2023  ###reference_b31###  ###reference_b31###).\nAdditionally, it is costly to evaluate open-domain answers even if based on automatic fact-checkers,  $30 for 100 responses based on GPT-3.5-Turbo."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Evaluating Fact-Checking Systems",
            "text": "We investigate automatic fact-checking systems in three aspects: accuracy, latency, and costs. We evaluate the verification performance in multiple settings across different fact-checking frameworks, evidence sources, and verifiers. Pipeline and core component modules of different fact-checking frameworks are basically similar, including obtaining atomic claims, collecting evidence and verifying correctness; all thus, while the implementations are different. For example, in terms of how to extract atomic claims, RARR does not include this step. Recent fact-checking frameworks such as FactScore, FacTool, Factcheck-GPT and commercial retrieval-augmented generative models such as Perplexity.ai are evaluated, with evidence retrieved from Wikipedia articles or web pages, as well as with various LLM-based verifiers that judge the factuality of a claim based on their internal knowledge and retrieved evidence as a reference. In Table 5, we observe that automatic fact-checking systems struggle to detect false claims. Across the three datasets we experiment with, it is consistently more arduous for these systems to differentiate false claims compared to identifying true ones. This challenge may arise from the tendency of returning invalid evidence for false claims. Retrieving evidence from the web using Serper (Google search engine results) is more effective than sourcing related passages from Wikipedia articles using BM25, given that a wider array of effective evidence is accessible on open web pages for open-domain questions. The verification accuracy of an LLM-based verifier primarily relies on the capabilities of the LLM and the effectiveness of the prompts used. For instance, the overall performance of GPT-4 surpasses that of both LLaMA-3-8B and GPT-3.5-Turbo, and thus the verification results (Factcheck-GPT) outperform those of FacTool, FactScore and Perplexity.ai, despite all of them utilizing evidence sourced from the web. While Factcheck-GPT exhibits superior effectiveness, it is associated with considerable latency and substantial costs. Latency and cost are largely contingent upon the implementation strategy. For instance, FacTool adopts asynchronous processing and leverages Serper ($0.001 per search) in conjunction with GPT-3.5-Turbo, rendering it faster and more economical compared to Factcheck-GPT. Notably, Factcheck-GPT uses SerpAPI ($0.015 per search) alongside GPT-4, where the cost of the most affordable GPT-4 model is 20 times that of GPT-3.5-Turbo. In summary, the efficacy of automated fact-checking systems is fundamentally dependent on implementation factors such as choice of search tool, prompts, and backend LLMs. This is primarily driven by engineering considerations."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "We proposed OpenFactCheck, a unified, easy-to-use and extensible toolkit for LLM factuality evaluation.\nOpenFactCheck establishes a unified framework with clearly defined modules and flexible configurations to support the customization of automatic fact-checking systems.\nFrom the application level, OpenFactCheck allows general users to check whether a claim and a document are factual or not. This proposed framework could also facilitate LLM practitioners and developers to effectively and efficiently evaluate the factuality of their LLMs from various perspectives and to assess the accuracy of automatic fact-checkers.\nOur extensive experiments indicate that more than 90% of the claims generated by LLMs in response to open-domain questions are factually correct.\nNevertheless, models encounter challenges when addressing some straightforward questions such as Is 7411 a prime number?\nThis difficulty is attributed to recent findings suggesting that LLMs demonstrate weaker comprehension abilities relative to their generation capabilities.\nAdditionally, prevalent fact-checking systems struggle to identify false claims, with the retrieval of pertinent evidence posing a significant bottleneck. The latency and the cost associated with these systems primarily hinge on implementation strategies.\nIn the future, we will continue to integrate new techniques, features, and evaluation benchmarks to OpenFactCheck to facilitate the research progress of LLM fact-checking."
        }
    ],
    "url": "http://arxiv.org/html/2405.05583v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "4",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.05583v1",
        "paper_title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs",
        "research_background": "The paper, \"OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs,\" is motivated by the need to address the frequent generation of content by large language models (LLMs) that deviates from real-world facts. This issue hampers the models' performance and reliability, presenting a significant barrier to their deployment in real-world applications. Despite many efforts to evaluate and improve the factuality of LLMs, two primary challenges persist: (1) the difficulty in assessing the factuality of open-domain free-form responses and (2) the lack of standardization in evaluation datasets and measurements across different studies, making comparisons difficult and impeding progress in the field.\n\nTo tackle these challenges, the paper introduces OpenFactCheck, an open-sourced fact-checking system consisting of three interlinked modules: \n1. **CustChecker**, which allows the customization of automatic fact-checkers to verify free-form documents, addressing the first challenge.\n2. **LLMEval**, a unified LLM factuality evaluation module that applies seven benchmarks to assess LLM factuality from various perspectives, providing reports that highlight weaknesses and suggest improvements. This module aims to address the second challenge.\n3. **CheckerEval**, which evaluates the verification accuracy of fact-checkers and incorporates a leaderboard to encourage the development of advanced automatic fact-checking systems.\n\nRelevant prior work includes studies on the factuality of LLMs and the challenges identified in their evaluation (Lee et al., 2022; Chuang et al., 2023; Shi et al., 2023; Chen et al., 2023). The paper acknowledges the frequent factual inaccuracies in current models (GPT-4 by OpenAI, 2023; Bang et al., 2023; Borji, 2023; Guiven, 2023) and the issues these pose for system performance and reliability (Chuang et al., 2023; Geng et al., 2023).\n\nThe paper aims to advance the research on LLM factuality by addressing three main research questions:\n1. How to effectively identify factual errors in LLM responses.\n2. How to systematically evaluate the factuality ability of LLMs.\n3. Which automatic fact-checker is the best and which component dominates the final verification accuracy.\n\nBy developing the three modules within OpenFactCheck, the authors aim to create a robust foundation to facilitate further research and development in LLM factuality evaluation.",
        "methodology": "### Methodology: OpenFactCheck\n\n**Platform Overview:**\n*Implementation:* OpenFactCheck is built with three primary components - a Python server, a web interface, and a database, all deployed using AWS. The Python backend doubles as a toolkit, facilitating flexible development.\n\n*Design Principles:*\n1. **Customizability and Extensibility:** The platform is designed for easy adaptation and expansion by both users and developers.\n2. **Compatibility:** It ensures compatibility with existing fact-checking methods and datasets.\n\n**Key Components:**\n1. **CustChecker**\n2. **LLMEval**\n3. **CheckerEval**\n\n**Unified Fact-Checking Procedure:**\nThe platform standardizes various fact-checking systems into a three-step process, sequentially coordinated through:\n- **claim_processor:** Decomposes a document into individual claims.\n- **retriever:** Collects relevant information or passages based on the claim.\n- **verifier:** Assesses the truthfulness of the claims using the gathered evidence.\n\n**Flexibility in Implementation:**\n- Methods for evidence retrieval can vary, e.g., using SerpAPI or Wikipedia\u2019s BM25 search.\n- Configurability via a yaml file allows integration of different implementations (like Factcheck-GPT\u2019s claim processor or RARR\u2019s retriever).\n- Users can initiate verification from any step, depending on their needs, e.g., starting at retrieval if no decomposition is required.\n\n**Message-Passing Mechanism:**\n- A success flag passed along the sequence ensures each solver's output is compatible with the next solver\u2019s input.\n- Configuration files define solver input and output parameter names, linking different solvers seamlessly into a pipeline.\n\n**Integration and Extensibility:**\n- Inspired by Fairseq, third-party task solvers are treated as plugins, adhering to class interface definitions for easy import and integration.\n- OpenFactCheck can be used as a full application via web interfaces or as a library allowing advanced development.\n\n**Dataset and Evaluation:**\n- The platform uses questions from seven existing corpora (e.g., Snowball, SelfAware, FreshQA, etc.), totaling 6,480 examples.\n- Each question is labeled based on domain, topic, knowledge attribute, task, and source.\n- **Domain Identification:** GPT-4 helps identify domains and topics for each question.\n- Error Analysis: Three types of errors are tagged:\n  - **Type1: Knowledge Error:** Incorrect information due to lack of knowledge or hallucinated data.\n  - **Type2: Over-commitment Error:** Over-commitment to falsehoods or jokes in the prompt.\n  - **Type3: Disability Error:** Inability to retrieve up-to-date information, leading to incorrect answers.\n\n**Error Mitigation Strategies:**\n- Enhancing learning with curated corpora during pre-training and fine-tuning.\n- Augmenting knowledge during inference.\n- Calibrating models, configuring decoding strategies, and engineering better prompts.\n- Reassessing prompts and using multiple question formats.\n\n**Evaluation Methods:**\n- Yes/No or short-answer questions are evaluated based on exact matches with the gold standard answers.\n- Long and open-domain responses are evaluated using automatic fact-checking systems with retrieved evidence.\n\nIn essence, OpenFactCheck presents a flexible, customizable, and extensible framework for evaluating the factuality of responses generated by LLMs, integrating a thorough error analysis and helping to improve the accuracy and reliability of these models.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n**Main Experiment Setup:**\n\n1. **Datasets:**\n   - **Snowball dataset:** Used to evaluate error rates and the phenomenon of hallucination snowballing.\n   - **SelfAware dataset:** Used to measure precision and recall, specifically setting a positive label as unanswerable.\n   - **FreshQA dataset:** Focuses on questions with rapidly changing answers to challenge the LLMs' ability to retrieve up-to-date information.\n   - **FacTool-QA, FELM-WK, Factcheck-Bench:** Used with FacTool to evaluate the factuality of free-form responses.\n\n2. **Baselines:**\n   - Models evaluated include **LLaMA-2, GPT-4**, and earlier versions such as **GPT-3.5-Turbo.**\n   - **Fact-checking frameworks:** FactScore, FacTool, Factcheck-GPT, and Perplexity.ai.\n   - Verification systems using web search tools like Serper and Wikipedia-based BM25.\n\n3. **Evaluation Metrics:**\n   - **Error rates:** Percentage of factually incorrect responses.\n   - **Precision and recall:** Particularly with a focus on unanswerable questions.\n   - **Percentage of true and false claims:** Across different datasets.\n   - **Cost and latency:** Financial and time-based efficiency in automated fact-checking processes.\n\n**Main Experimental Results:**\n\n1. **Error Rates on Snowball Dataset:**\n   - **LLaMA-2:** >80% error rate.\n   - **GPT-4:** 65.5% error rate.\n   - Observation that GPT-4, despite recognizing 87% of its own mistakes, often over-commits to incorrect contexts, a phenomenon called \"hallucination snowballing.\"\n\n2. **SelfAware Performance:**\n   - Higher precision than recall when marking questions as unanswerable.\n   - Indicative that models often incorrectly determine they have the answer to unanswerable questions.\n\n3. **FreshQA Observations:**\n   - Poor performance on questions with rapidly changing answers.\n   - Challenges in retrieving up-to-date information.\n\n4. **Fact-checking Efficacy on FacTool-QA, FELM-WK, Factcheck-Bench:**\n   - True claims range between 89%-94%.\n   - FacTool-QA posed more significant challenges for LLMs compared to the other datasets.\n   - **GPT-4:** Best factuality performance; possesses fewer false claims and higher percentages of true claims compared to LLaMA-2 and others.\n\n5. **Automatic Fact-checking Systems:**\n   - Difficulty in detecting false claims across datasets.\n   - Superior results when retrieving evidence from open web pages (Serper) rather than Wikipedia (BM25).\n   - **Factcheck-GPT:** Greatest verification accuracy among evaluated systems but higher costs and latency.\n   - **FacTool:** More cost-effective and faster due to asynchronous processing and use of cheaper search tools like Serper with cheaper LLMs (GPT-3.5-Turbo versus Factcheck-GPT's GPT-4).\n\nIn summary, the main experiment highlights persistent challenges for LLMs like hallucination snowballing, difficulties in identifying unknown knowledge, and issues with retrieving up-to-date information. Automated fact-checking systems vary significantly in effectiveness, cost, and latency based on their implementation, search tools, and underlying LLMs."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of different models in factual accuracy and to assess the accuracy of various automatic fact-checking systems.",
            "experiment_process": "The experiment first evaluates the factual accuracy on datasets including Snowball, SelfAware, and FreshQA, utilizing LLaMA-2 and GPT-4. This also involves using automatic evaluation tools like FacTool equipped with Serper and GPT-3.5-Turbo. The evaluation metrics include error rates and the ability to identify true versus false claims. Verification tools such as Factcheck-GPT and commercial models like Perplexity.ai are also evaluated using retrieved evidence from sources like Wikipedia and open web pages. These systems are assessed based on accuracy, latency, and costs.",
            "result_discussion": "GPT-4 generally outperforms LLaMA-2 and has a good ability to identify its own mistakes, reducing hallucination snowballing. However, both models exhibit high error rates with LLaMA-2 performing worse. FreshQA dataset illustrates the challenge of retrieving up-to-date information. Fact-checking systems show a higher percentage of true claims verified as true (89%-94%) but struggle more with false claims. GPT-4 provides better factuality performance but incurs higher latency and costs compared to other models. The accuracy of fact-checking systems is highly influenced by the implementation strategy, including the choice of search tools and backend models.",
            "ablation_id": "2405.05583v1.No1"
        },
        {
            "research_objective": "To evaluate the performance of LLaMA-2 and GPT-4 based on factual questions from FactQA and to understand the error types and their implications.",
            "experiment_process": "The responses from LLaMA-2 (7B, 13B) and GPT-4 were collected based on questions from FactQA. Error rates were observed and analyzed alongside the potential error types (knowledge error, over-commitment error, and disability error). The evaluation also included verifying correctness of the responses using tools like FacTool with Serper, and GPT-3.5-Turbo over different datasets.",
            "result_discussion": "GPT-4 performs better than LLaMA-2 with lower error rates. Both models face issues with over-commitment to false contexts, leading to 'hallucination snowballing'. The models also struggle with questions requiring up-to-date information. The evaluation tools revealed that while a high percentage of true claims are verified correctly, identifying false claims remains challenging. Costs associated with automatic evaluation can be significant, highlighting the need for efficient implementation strategies.",
            "ablation_id": "2405.05583v1.No2"
        },
        {
            "research_objective": "To investigate the accuracy, latency, and costs of various automatic fact-checking systems across different settings.",
            "experiment_process": "The experiment evaluates different fact-checking frameworks (FactScore, FacTool, Factcheck-GPT, and Perplexity.ai) using three datasets including Factcheck-Bench, FacTool-QA, and FELM-WK. The systems process claims extracted from documents and assess their veracity based on internal knowledge and retrieved evidence. Metrics for evaluation include verification performance, latency, and costs.",
            "result_discussion": "Automatic fact-checking systems consistently face difficulty in detecting false claims. GPT-4 based verifiers showed superior performance but at higher costs and latency. Evidence retrieval from open web sources using Serper is more effective than using passages from Wikipedia. The effectiveness of these systems is significantly influenced by the chosen implementation strategy, including search tools, prompts, and LLMs. FacTool, which uses asynchronous processing and more cost-effective search options, showed better cost efficiency, while Factcheck-GPT presented higher effectiveness with substantial latency and costs.",
            "ablation_id": "2405.05583v1.No3"
        }
    ]
}