{
    "title": "Using Contextual Information for Sentence-level Morpheme Segmentation",
    "abstract": "Recent advancements in morpheme segmentation primarily emphasize word-level segmentation, often neglecting the contextual relevance within the sentence. In this study, we redefine the morpheme segmentation task as a sequence-to-sequence problem, treating the entire sentence as input rather than isolating individual words. Our findings reveal that the multilingual model consistently exhibits superior performance compared to monolingual counterparts. While our model did not surpass the performance of the current state-of-the-art, it demonstrated comparable efficacy with high-resource languages while revealing limitations in low-resource language scenarios.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The problem of morpheme segmentation deals with decomposing a word into a sequence of morphemes that represent the smallest meaningful unit of words like prefixes, suffixes, and root words. For example, the word pokers can be decomposed into its morphemes as poke@@er@@s, where @@ represents separations between morphemes.\nRecent advances in morpheme segmentation focus mostly on word-level segmentation (Batsuren et al., 2022  ###reference_b1###; Peters and Martins, 2022  ###reference_b5###) and do not take the context of the word in the sentence into consideration. Instead, we focus on the task of sentence-level morpheme segmentation in which the context of the word in a sentence is taken into account for morpheme segmentation.\nThe SIGMORPHON 2022 Shared Task on Morpheme Segmentation (Batsuren et al., 2022  ###reference_b1###) formulates the task of morpheme segmentation into two subtasks: word-level segmentation and sentence-level segmentation. Although many submissions in this task show significant improvements over baselines on both subtasks (Rouhe et al., 2022  ###reference_b7###; Wehrli et al., 2022  ###reference_b12###; Peters and Martins, 2022  ###reference_b5###), most submitted approaches for sentence-level segmentation subtask ignore the context of words by design treating the problem as word-level morpheme segmentation, i.e. they treat each word in a sentence as a separate example.\nHowever, in many languages, the context of a word might provide a piece of meaningful information to disambiguate the morphology of the word (Batsuren et al., 2022  ###reference_b1###) and help improve morpheme segmentation.\nConsider the following sentences in Mongolian and their corresponding morpheme segmentation:\n\u0413\u044d\u0440\u0442 \u044d\u043c\u044d\u044d \u0445\u043e\u043e\u043b \u0445\u0438\u0439\u0432.  \u0413\u044d\u0440 @@\u0442 \u044d\u043c\u044d\u044d \u0445\u043e\u043e\u043b \u0445\u0438\u0439\u0445 @@\u0432.\n\u0411\u0438 \u04e9\u0434\u04e9\u0440 \u044d\u043c\u044d\u044d \u0443\u0443\u0441\u0430\u043d.  \u0411\u0438 \u04e9\u0434\u04e9\u0440 \u044d\u043c @@\u044d\u044d \u0443\u0443\u0445 @@\u0441\u0430\u043d.\nIn the above examples, the word \u044d\u043c\u044d\u044d has different meanings in each sentence. In the first sentence, it means grandmother and is not segmentable. However, in the second one, it means medicine and is segmentable as \u044d\u043c @@\u044d\u044d. Therefore, the context of a word in a sentence is an important factor for determining how the word should be segmented into its morphemes.\nWe focus on the task of sentence-level morpheme segmentation while taking the whole sentence in context and implement a sequence-to-sequence transformer model (Vaswani et al., 2017  ###reference_b10###) inspired by DeepSPIN-3 (Peters and Martins, 2022  ###reference_b5###) which is the winner of word-level morpheme segmentation in the SIGMORPHON 2022 Shared Task on Morpheme Segmentation (Batsuren et al., 2022  ###reference_b1###).\nWhile most existing methods treat sentence-level morpheme segmentation as a zero-shot solution of word-level morpheme segmentation (Batsuren et al., 2022  ###reference_b1###), we treat each sentence as a whole as one training example to preserve its context and treat the problem as a sequence to sequence generation task. The sentence-level dataset provided for the shared task consists of three languages: Czech, English, and Mongolian.\nWe perform various experiments with monolingual and multilingual sequence-to-sequence transformer models and show that the multilingual model generally performs better over monolingual models, especially for low-resource languages. Additionally, we experiment with data augmentation in which we increase the training dataset by combining samples for word-level morpheme segmentation. We also experiment with upsampling the sentence-level data given the lack of enough training data for low-resource languages like Czech and Mongolian. Although we are not able to outperform the winners of the shared task, our results are close for high resource languages like English (F1 score: 95.10) but relatively underperform for low-resource languages like Czech (F1 score: 75.79) and Mongolian (F1 score: 72.54)."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "We implement sequence to sequence transformer model similar to DeepSPIN-3 (Peters and Martins, 2022  ###reference_b5###) for sentence-level morpheme segmentation while treating each sentence as one training example.\nOur implementation is done using fairseq (Ott et al., 2019  ###reference_b4###).\nThis sequence-to-sequence task setting for sentence-level morpheme segmentation is similar to machine translation settings but differs in the sense that the source language corresponds to sentence examples and the target language corresponds to the sentence in which the words are segmented into their corresponding morphemes.\nWe train monolingual and multilingual models with the same transformer architecture.\nThe overall approach for both monolingual and multilingual models are described below."
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Tokenization",
            "text": "We use Google\u2019s SentencePiece tokenizer with subword regularization using Unigram Language Model (ULM) (Kudo, 2018  ###reference_b3###).\nULM is a top-down technique where the model is initially initialized with a vast vocabulary of overlapping sub-words and a score is generated based on expectation maximization for each sub-word.\nUp until the anticipated vocabulary size is attained, the lowest-scoring sub-words are trimmed.\nThe SentencePiece tokenizer is trained using both source and target data, and all tokens for sentence-level data are pre-computed before training as expected by fairseq\u2019s training workflow.\nThe vocabulary size for English is chosen to be 6000 to 8000 for English and 1000 to 5000 for Czech and Mongolian depending upon the data augmentation which could allow for higher vocabulary sizes.\nAdditionally, for multilingual model, we use the vocabulary size to be 9000 to 12000 so as to take into account a large number of diverse tokens present in multiple languages."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Model Architecture",
            "text": "Our transformer model (Vaswani et al., 2017  ###reference_b10###) consists of 6 encoder and 6 decoder blocks with 8 multi-head attentions.\nWe use an embedding size of 256 and dropout of 0.3 determined through hyperparameter tuning on the dev set.\nThe size of each feed-forward layer is 1024 which is also determined through hyperparameter tuning on the dev set.\nWe use identical architecture for all three languages: Czech, English, and Mongolian."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Training",
            "text": "We train for a maximum of 400,000 updates and perform early stopping based on the validation loss.\nWe use learning rate warm-up for 4000 steps for different languages determined through hyperparameter tuning.\nWe also use inverse square root learning rate scheduling during our training. We use a batch size of 8192 for all the experiments.\nOur model is trained using entmax-loss (Peters et al., 2019  ###reference_b6###) with alpha of 1.5 similar to Peters and Martins (2022  ###reference_b5###) since it seemed to show better performance compared to cross-entropy loss in our preliminary experiments.\nThe entmax loss is a general family of loss functions encompassing the cross-entropy loss as a special case (when alpha=1). Using values greater than 1 for alpha, the entmax loss allows for sparse gradients with non-zero values occurring only on the gold label and/or other labels that receive non-zero probability. This means that completely-peaked probability distributions are possible, in contrast to the denser distributions observed at alpha=1.\u201d\nThis makes entmax loss a better choice over cross-entropy loss in many applications.\nHowever, entmax loss is computationally more expensive to compute than cross-entropy loss.\nWe therefore use existing implementation of entmax loss rather than implementing it from scratch."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Inference and Evaluation",
            "text": "For generating morpheme segmentation for each sentence, we use beam search with a beam size of 5.\nThe tokenizer trained using the training data is used to tokenize sentences during inference.\nAdditional postprocessing is done to clean up raw output from the model to obtain final segmentation.\nWe use F1 score as our primary metric to evaluate the morpheme segmentation at the sentence-level.\nThis means that the generated segmentation for a sentence is considered to be correct only if all the words in that sentence is segmented correctly.\nPrecision, recall and Levenshtein distance, although computed, have not been reported in this paper.\n###table_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We evaluate our approach on the sentence-level morpheme segmentation dataset provided by Batsuren et al. (2022  ###reference_b1###).\nThis dataset consists of train, dev and test samples in Czech, English and Mongolian languages as shown in Table 2  ###reference_###.\nWe experiment with monolingual models (separate models for each language) and multilingual model (one model for all languages) that are discussed below."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Monolingual Experiments",
            "text": "We experiment with monolingual models in which we train three different models separately on Czech, English and Mongolian datasets.\nWe conduct few different experiments using the sequence to sequence transformer model as described in 2  ###reference_### using only the sentence-level dataset, augmenting with word dataset and upsampling the dataset."
        },
        {
            "section_id": "3.1.1",
            "parent_section_id": "3.1",
            "section_name": "3.1.1 Sentence-level Dataset Only",
            "text": "In this experiment, we take the sentence-level dataset in three languages and train the sequence to sequence transformer model for each of these languages.\nThe results are shown in the Table 1  ###reference_### (second row).\nWe observe that for high resource language like English, the F1 score is 87.94 while drops significantly for low-resource language likes Czech (F1 score: 22.62) and Mongolian (F1 score: 21.00)."
        },
        {
            "section_id": "3.1.3",
            "parent_section_id": "3.1",
            "section_name": "3.1.3 Word-level Dataset Augmentation",
            "text": "Another way to increase the number of training data we employ is to augment the sentence-level training set with word-level dataset from their corresponding languages.\nThe results with word-level datset augmentation are shown in Table 1  ###reference_### (fourth row).\nWe observe that augmenting the word-level dataset significantly improves the F1 score for English to 92.46 compared to 87.94 obtained without any upsampling or word-level dataset augmentation.\nHowever, there is only small improvement for Czech language (from 22.62 without any upsampling or word-level dataset augmentation to 25.87 in this case).\nSurprisingly, augmenting the word-level dataset reduced the performance for Mongolian language.\nThis could be because of the prevalence of context-dependent morpheme segmentation examples like the one discussed in Sec. 1  ###reference_###.\nWe believe that understanding the core reason for this drop in performance might require knowledge of Mongolian language\u2019s morphology."
        },
        {
            "section_id": "3.1.4",
            "parent_section_id": "3.1",
            "section_name": "3.1.4 Word Augmentation with Upsampling",
            "text": "We combine word-level data augmentation and then perform upsampling similar to Sec. 3.1.2  ###reference_.SSS2### and use this dataset to train the transformer model.\nThe results are shown in Table 1  ###reference_### (fifth row).\nWe observe that this achieves the highest overall F1 score of 95.10 for English. We also see improvements for Czech with F1 scores of 46.62.\nAlthough there is improvement for Mongolian with F1 score of 38.00 compared to word-level dataset augmentation, this is still lower than F1 score of 39.40 obtained with upsampling, and therefore this improvement is most likely only contributed by upsampling rather than word-level dataset augmentation which most likely slightly hampered the performance instead."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Multilingual Experiments",
            "text": "We experiment with multilingual model in which we concatenate training datasets from Czech, English and Mongolian into one large multilingual dataset and train one tokenizer model and one sequence-to-sequence transformer on this multilingual dataset.\nWe do not use any language identifier token to identify the samples from different languages and instead perform simple concatenation only.\nWe also experiment with word-level dataset augmentation and then perform upsampling on this augmented multilingual dataset since this approach demonstrated better performance with monolingual models.\nThe results are shown in Table 1  ###reference_### (middle block).\nWe observe that the multilingual model outperforms monolingual models for Czech and Mongolian in which the F1 scores are 75.79 and 72.54 respectively. This shows that Czech and Mongolian languages significantly benefited from multilingual settings.\nFor English, the performance drops slightly to 92.93 in multilingual settings from 95.10 in monolingual settings. Overall, we observe that low-resource languages see much more performance improvements in multilingual settings as expected."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Many NLP applications require tokenization of words into subwords. However, most subword tokenization approaches like Byte-Pair Encoding (BPE) (Gage, 1994  ###reference_b2###; Sennrich et al., 2016  ###reference_b9###) and Unigram Language Model (ULM) (Kudo, 2018  ###reference_b3###) ignore the morphophology of words during tokenization and instead tokenize words based on statistical co-occurence of subwords.\nBatsuren et al. (2022  ###reference_b1###) proposed SIGMORPHON 2022 Shared Task on Morpheme Segmentation citing the necessity of linguistically motivated subword tokenization in which many approaches demonstrated state-of-the-art performance over existing baseline systems like WordPiece (Schuster and Nakajima, 2012  ###reference_b8###), ULM (Kudo, 2018  ###reference_b3###) and Morfessor2 (Virpioja et al., 2013  ###reference_b11###).\nFor example, DeepSPIN-2 and DeepSPIN-3 (Peters and Martins, 2022  ###reference_b5###) collectively demonstrate a superior performance on word-level morpheme segmentation on all 9 languages considered on the shared task.\nAUUH (Rouhe et al., 2022  ###reference_b7###), CLUZH (Wehrli et al., 2022  ###reference_b12###) and others also show significantly improved performance over baselines for both subtasks.\nHowever, most submitted approaches ignore the context of words in a sentence for sentence-level morpheme segmentation.\nOur approach formulates sentence-level morpheme segmentation by considering sentence as a whole to preserve the context (including the unofficial highest scorer DeepSPIN-sent) and performs morpheme segmentation without separating the sentence into individual words before morpheme segmentation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "Our results show that our approach with multilingual model shows better performance over monolingual models.\nAdditionally, the performance improvement is more significant for low-resource languages like Czech and Mongolian than English for which comparatively large amount of training data was available.\nWe also show that word-level dataset augmentation and data upsampling show improved performance particularly for low resource languages. In this regard, we emphasize that simple techniques like data upsampling could serve as a first step for low-resource languages in some NLP tasks.\nOverall, although our results do not outperform state of the art, they are competitive for high-resource languages like English and have the potential for improved performance upon tackling issues with regards to the lack of enough data for low-resource languages like Czech and Mongolian.\nFuture directions of this research could try to tackle lack of enough data for low-resource languages by investigating techniques like semi-supervised learning in which a trained model is used to generate labels for newly collected data and then integrate this newly labelled data to expand the training data if the predicted probabilities for the labels cross certain threshold. This could help generate new labelled data that could then be used to train new models. Additionally, our multilingual model do not distinguish between language samples. Adding an additional language identifier token in data based on language could potentially help improve the performance of multilingual model further."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"S2.T1.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S2.T1.1.1.1.2\">Czech</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S2.T1.1.1.1.3\">English</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S2.T1.1.1.1.4\">Mongolian</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.2.2.1\">Monolingual Sequence-to-Sequence Transformer</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.2.2.2\">22.62</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.2.2.3\">87.94</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.2.2.4\">21.00</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.3.3.1\">\u00a0\u00a0\u2003\u2003+ Upsampling</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.3.3.2\">37.58</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.3.3.3\">88.62</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.3.3.4\">39.40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.4.4.1\">\u00a0\u00a0\u2003\u2003+ Word Augmentation</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.4.4.2\">25.87</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.4.4.3\">92.46</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.4.4.4\">16.76</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.5.5.1\">\u00a0\u00a0\u2003\u2003+ Word Augmentation + Upsampling</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.5.5.2\">46.62</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.5.5.3.1\">95.10</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.5.5.4\">38.00</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.6.6.1\">Multilingual Sequence-to-Sequence Transformer</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.6.6.2\">66.77</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.6.6.3\">88.54</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.6.6.4\">68.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.7.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.7.7.1\">\u00a0\u00a0\u2003\u2003+ Word Augmentation + Upsampling</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.7.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.7.7.2.1\">75.79</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.7.7.3\">92.93</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.7.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.7.7.4.1\">72.54</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.8.8.1\">Official highest score in <cite class=\"ltx_cite ltx_citemacro_citet\">Batsuren et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15436v2#bib.bib1\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.8.8.2\">91.99</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.8.8.3\">96.31</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.8.8.4\">82.88</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S2.T1.1.9.9.1\">DeepSPIN-sent <cite class=\"ltx_cite ltx_citemacro_citep\">(Peters and Martins, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15436v2#bib.bib5\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S2.T1.1.9.9.2\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.T1.1.9.9.2.1\">93.23</em></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S2.T1.1.9.9.3\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.T1.1.9.9.3.1\">98.24</em></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S2.T1.1.9.9.4\"><em class=\"ltx_emph ltx_font_italic\" id=\"S2.T1.1.9.9.4.1\">83.59</em></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>F1 scores obtained for sentence-level test sets for Czech, English and Mongolian languages in monolingual and multilingual settings (top two blocks) along with highest scores of official shared task <cite class=\"ltx_cite ltx_citemacro_citep\">(Batsuren et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15436v2#bib.bib1\" title=\"\">2022</a>)</cite> winners (AUUH_B <cite class=\"ltx_cite ltx_citemacro_citep\">(Rouhe et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15436v2#bib.bib7\" title=\"\">2022</a>)</cite> for English and CLUZH-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wehrli et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.15436v2#bib.bib12\" title=\"\">2022</a>)</cite> for Czech and Mongolian ) and DeepSPIN-3 (unofficial highest scorer). The bold numbers are highest scores for our implementation, and italicized numbers are highest overall scores.</figcaption>\n</figure>",
            "capture": "Table 1: F1 scores obtained for sentence-level test sets for Czech, English and Mongolian languages in monolingual and multilingual settings (top two blocks) along with highest scores of official shared task (Batsuren et\u00a0al., 2022) winners (AUUH_B (Rouhe et\u00a0al., 2022) for English and CLUZH-3 (Wehrli et\u00a0al., 2022) for Czech and Mongolian ) and DeepSPIN-3 (unofficial highest scorer). The bold numbers are highest scores for our implementation, and italicized numbers are highest overall scores."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S2.T2.1.1.1.1\">Language</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T2.1.1.1.2\">train</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T2.1.1.1.3\">dev</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T2.1.1.1.4\">test</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T2.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T2.1.2.1.1\">Czech</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T2.1.2.1.2\">1000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T2.1.2.1.3\">500</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T2.1.2.1.4\">500</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T2.1.3.2.1\">English</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T2.1.3.2.2\">11007</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T2.1.3.2.3\">1783</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T2.1.3.2.4\">1845</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S2.T2.1.4.3.1\">Mongolian</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S2.T2.1.4.3.2\">1000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S2.T2.1.4.3.3\">500</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S2.T2.1.4.3.4\">500</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Break down of sentence-level morpheme segmentation for Czech, English and Mongolian in train, dev and test set.</figcaption>\n</figure>",
            "capture": "Table 2: Break down of sentence-level morpheme segmentation for Czech, English and Mongolian in train, dev and test set."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T3.1.1.1.1\">Language</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.1.1.1.2\">Number of Samples</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.1.2.1.1\">Czech</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S3.T3.1.2.1.2\">38682</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.1.3.2.1\">English</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S3.T3.1.3.2.2\">577374</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.1.4.3.1\">Mongolian</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S3.T3.1.4.3.2\">18966</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Break down of word-level morpheme segmentation dataset for Czech, English and Mongolian used to augment sentence-level dataset.</figcaption>\n</figure>",
            "capture": "Table 3: Break down of word-level morpheme segmentation dataset for Czech, English and Mongolian used to augment sentence-level dataset."
        }
    },
    "image_paths": {},
    "references": [
        {
            "1": {
                "title": "The SIGMORPHON 2022 shared task on morpheme segmentation.",
                "author": "Khuyagbaatar Batsuren, G\u00e1bor Bella, Aryaman Arora, Viktor Martinovi\u0107,\nKyle Gorman, Zden\u011bk \u017dabokrtsk\u1ef3, Amarsanaa Ganbold,\n\u0160\u00e1rka Dohnalov\u00e1, Magda \u0160ev\u010d\u00edkov\u00e1,\nKate\u0159ina Pelegrinov\u00e1, et al. 2022.",
                "venue": "arXiv preprint arXiv:2206.07615.",
                "url": null
            }
        },
        {
            "2": {
                "title": "A new algorithm for data compression.",
                "author": "Philip Gage. 1994.",
                "venue": "C Users Journal, 12(2):23\u201338.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Subword regularization: Improving neural network translation models\nwith multiple subword candidates.",
                "author": "Taku Kudo. 2018.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 66\u201375,\nMelbourne, Australia. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "4": {
                "title": "fairseq: A fast, extensible toolkit for sequence modeling.",
                "author": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,\nDavid Grangier, and Michael Auli. 2019.",
                "venue": "In Proceedings of NAACL-HLT 2019: Demonstrations.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Beyond characters: Subword-level morpheme segmentation.",
                "author": "Ben Peters and Andr\u00e9 FT Martins. 2022.",
                "venue": "In Proceedings of the 19th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology, pages 131\u2013138.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Sparse sequence-to-sequence models.",
                "author": "Ben Peters, Vlad Niculae, and Andr\u00e9 FT Martins. 2019.",
                "venue": "arXiv preprint arXiv:1905.05702.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Morfessor-enriched features and multilingual training for canonical\nmorphological segmentation.",
                "author": "Aku Rouhe, Stig-Arne Gr\u00f6nroos, Sami Virpioja, Mathias Creutz, and Mikko\nKurimo. 2022.",
                "venue": "In Proceedings of the 19th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology. The Association for\nComputational Linguistics.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Japanese and korean voice search.",
                "author": "Mike Schuster and Kaisuke Nakajima. 2012.",
                "venue": "In 2012 IEEE international conference on acoustics, speech and\nsignal processing (ICASSP), pages 5149\u20135152. IEEE.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Neural machine translation of rare words with subword units\nproceedings of the 54th annual meeting of the association for computational\nlinguistics (volume 1: Long papers) 1715\u20131725 association for computational\nlinguistics https://www. aclweb. org/anthology.",
                "author": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.",
                "venue": "P16-1162.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Attention is all you need.",
                "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.",
                "venue": "Advances in neural information processing systems, 30.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Morfessor 2.0: Python implementation and extensions for morfessor\nbaseline.",
                "author": "Sami Virpioja, Peter Smit, Stig-Arne Gr\u00f6nroos, Mikko Kurimo, et al. 2013.",
                "venue": null,
                "url": null
            }
        },
        {
            "12": {
                "title": "Cluzh at sigmorphon 2022 shared tasks on morpheme segmentation and\ninflection generation.",
                "author": "Silvan Wehrli, Simon Clematide, and Peter Makarov. 2022.",
                "venue": "In Proceedings of the 19th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology, pages 212\u2013219.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.15436v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "4"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.1.4",
            "3.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1.1",
            "3.1.2",
            "3.1.3",
            "3.1.4",
            "3.2"
        ]
    },
    "research_context": {
        "paper_id": "2403.15436v2",
        "paper_title": "Using Contextual Information for Sentence-level Morpheme Segmentation",
        "research_background": "### Paper's Motivation\nThe motivation behind this paper lies in the challenge of morpheme segmentation, which involves breaking down words into their smallest meaningful units like prefixes, suffixes, and roots. The authors are particularly interested in moving beyond the traditional focus on word-level segmentation to consider the context of words within sentences. This approach promises to better handle the nuanced, context-sensitive nature of morphology in many languages.\n\n### Research Problem\nThe primary research problem addressed in this paper is sentence-level morpheme segmentation that accounts for the context of a word within its sentence. Traditional methods and most submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation tend to treat sentence-level segmentation as isolated word-level segmentation, ignoring valuable contextual information. The authors aim to improve morpheme segmentation by utilizing entire sentence contexts, particularly for languages where context significantly influences word morphology.\n\n### Relevant Prior Work\n1. **Word-Level Morpheme Segmentation**:\n   - **Batsuren et al., 2022** and **Peters and Martins, 2022** (###reference_b1### and ###reference_b5###): These works focus on word-level morpheme segmentation and have shown significant advancements. Both were heavily referenced in the context of the SIGMORPHON 2022 Shared Task.\n   \n2. **SIGMORPHON 2022 Shared Task**:\n   - **Batsuren et al., 2022** (###reference_b1###): This shared task highlighted the distinction between word-level and sentence-level morpheme segmentation, showing that understanding the context within a sentence can provide meaningful information in certain languages.\n   - Submissions **Rouhe et al., 2022** (###reference_b7###), **Wehrli et al., 2022** (###reference_b12###), and **Peters and Martins, 2022** (###reference_b5###): These submissions showcased significant improvements over baselines but still mainly approached sentence-level segmentation as isolated word-level tasks.\n\n3. **Sequence-to-Sequence Models**:\n   - **Vaswani et al., 2017** (###reference_b10###): The concept of the transformer model, which the authors seek to adapt for sentence-level morpheme segmentation.\n   - **DeepSPIN-3 by Peters and Martins, 2022** (###reference_b5###): This specific model was the top performer in the word-level morpheme segmentation task in the SIGMORPHON 2022 Shared Task, and serves as inspiration for the authors\u2019 sequence-to-sequence transformer model adapted for sentence-level segmentation.\n\nBy leveraging the strengths and limitations outlined in previous works, the authors aim to better utilize sentence-level context, thereby contributing to more effective morpheme segmentation models.",
        "methodology": "Methodology: We implement a sequence-to-sequence transformer model similar to DeepSPIN-3 (Peters and Martins, 2022) for sentence-level morpheme segmentation, where each sentence is treated as a single training example. Our implementation utilizes the fairseq toolkit (Ott et al., 2019).\n\nKey components and innovations:\n1. **Transformer Model Adoption**: Our approach leverages a sequence-to-sequence transformer model akin to DeepSPIN-3. This allows us to handle complex dependencies in sentence structures effectively.\n2. **Sentence-Level Approach**: The model is trained at the sentence level, treating each sentence as one training example. This is akin to the setup in machine translation tasks where the source language comprises the sentence examples and the target language consists of sentences segmented into morphemes.\n3. **Monolingual and Multilingual Models**: We construct and train both monolingual and multilingual models using the same transformer architecture. This facilitates comparison and evaluation of the models\u2019 effectiveness across different languages.\n\nMethod Details:\n- **Training Examples**: Source sentences are processed as complete units, enhancing the context available for segmenting words into morphemes.\n- **DeepSPIN-3 Basis**: Our method capitalizes on the strengths of the DeepSPIN-3 model, which has proven efficacy in sequence-to-sequence tasks.\n- **Fairseq Implementation**: Our model implementation is conducted using fairseq, a robust toolkit for sequence modeling, which streamlines the development and training processes.\n\nThe integration of these components and approaches aims to improve the accuracy and efficiency of morpheme segmentation by leveraging sentence-level contextual information through advanced transformer architectures.",
        "main_experiment_and_results": "We evaluate our approach on the sentence-level morpheme segmentation dataset provided by Batsuren et al. (2022). This dataset consists of train, dev, and test samples in Czech, English, and Mongolian languages. We experiment with monolingual models (separate models for each language) and a multilingual model (one model for all languages).\n\n**Datasets**: \n- The dataset includes train, dev, and test samples in Czech, English, and Mongolian.\n\n**Baselines**:\n- Monolingual models: Separate models trained and evaluated on each language individually.\n- Multilingual model: A single model trained on the combined dataset of all three languages.\n\n**Evaluation Metrics**:\n- The performance of the models is evaluated using standard segmentation metrics.\n\n**Main Experimental Results**:\n- The results of our experiments demonstrate the effectiveness of using contextual information for morpheme segmentation. \n- The multilingual model generally outperforms the monolingual models due to its ability to leverage shared information across languages.\n\nOverall, the main experiment highlights the advantage of incorporating contextual information and using a multilingual approach for morpheme segmentation tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To evaluate the performance of a sequence-to-sequence transformer model trained exclusively on sentence-level datasets in multiple languages.",
            "experiment_process": "Separate sequence-to-sequence transformer models were trained on sentence-level datasets for English, Czech, and Mongolian. The evaluation of these models' performance was based on F1 scores.",
            "result_discussion": "The F1 score for English was 87.94, but it dropped significantly for low-resource languages, with Czech scoring 22.62 and Mongolian scoring 21.00.",
            "ablation_id": "2403.15436v2.No1"
        },
        {
            "research_objective": "To determine the effect of data upsampling on the performance of sentence-level morpheme segmentation across different languages.",
            "experiment_process": "The sentence-level training set was upsampled 100 times for Czech and Mongolian and 10 times for English. The performance was compared using F1 scores.",
            "result_discussion": "The results for all three languages improved; however, the gap in F1 scores between the high-resource language, English, and the low-resource languages, Czech and Mongolian, remained significant.",
            "ablation_id": "2403.15436v2.No2"
        },
        {
            "research_objective": "To investigate the impact of augmenting the sentence-level training dataset with word-level datasets on the performance of morpheme segmentation.",
            "experiment_process": "The sentence-level dataset was augmented with word-level datasets from the corresponding languages. The transformer model's performance was evaluated using F1 scores.",
            "result_discussion": "Augmenting the word-level dataset significantly improved the F1 score for English from 87.94 to 92.46. The F1 score for Czech improved slightly from 22.62 to 25.87. However, the performance for Mongolian dropped, potentially due to the context-dependent nature of the language's morphology.",
            "ablation_id": "2403.15436v2.No3"
        },
        {
            "research_objective": "To evaluate the effectiveness of combining word-level data augmentation and upsampling on sentence-level morpheme segmentation.",
            "experiment_process": "The training set was augmented with word-level data and then upsampled following the method in Section 3.1.2. The model was evaluated using F1 scores.",
            "result_discussion": "This combination yielded the highest F1 score of 95.10 for English and improved results for Czech with an F1 score of 46.62. For Mongolian, the F1 score was 38.00, which was still lower than the 39.40 obtained with only upsampling, indicating that the improvement was primarily due to upsampling rather than augmentation.",
            "ablation_id": "2403.15436v2.No4"
        },
        {
            "research_objective": "To explore the performance advantages of a multilingual model over monolingual models for sentence-level morpheme segmentation.",
            "experiment_process": "A single tokenizer model and a sequence-to-sequence transformer model were trained on a concatenated multilingual dataset comprising Czech, English, and Mongolian, with no language identifier tokens. The impact of word-level dataset augmentation followed by upsampling was also tested.",
            "result_discussion": "The multilingual model outperformed monolingual models for Czech and Mongolian, with F1 scores of 75.79 and 72.54, respectively. For English, the performance slightly dropped to 92.93 from 95.10 in monolingual settings. This shows that low-resource languages benefit significantly more in multilingual settings.",
            "ablation_id": "2403.15436v2.No5"
        }
    ]
}