{
    "title": "Refinement of an Epilepsy Dictionary through Human Annotation of Health-related posts on Instagram",
    "abstract": "Objective \u2014\nTo (1) identify health-related terms used on social media posts that do not precisely match the health-related meaning of terms in a biomedical dictionary, (2) decide which terms need to be removed in order to improve the quality of the dictionary in the scope of biomedical text mining tasks, (3) evaluate the effect of removing imprecise terms on such tasks, and (4) discuss how human-centered annotation complements automated annotation in social media mining for biomedical purposes.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Social media data, such as text, hashtags, or images in posts, allow researchers to gain unprecedented access to study human cohorts. It is now possible to quantitatively measure very large populations for their individual or collective experiences, behaviors, perceptions, and emotions [1  ###reference_b1###, 2  ###reference_b2###, 3  ###reference_b3###, 4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###].\nSocial media is more than a source of information or a valuable communication tool for its users [7  ###reference_b7###]\u2014it is also a valuable resource of information about human health and well-being [1  ###reference_b1###].\nIt has been used to track and predict various public health issues [8  ###reference_b8###], such as mental health disorders [9  ###reference_b9###, 10  ###reference_b10###], adverse drug reaction (ADR) [11  ###reference_b11###, 12  ###reference_b12###], drug-drug interactions (DDI) [13  ###reference_b13###, 14  ###reference_b14###, 15  ###reference_b15###], and substance abuse [16  ###reference_b16###, 17  ###reference_b17###].\nInstagram is one of the most popular social network services in the world; in 2021, Instagram reached more than 1 billion users worldwide [18  ###reference_b18###].\nThrough an application for smartphones or tablets, users can share photos and videos, often accompanied by long captions.\nAlthough most research on social media has been focused on data from Twitter (now named X) or Facebook , Instagram has great potential for social media research given its increasing number of users.\nIt has already shown its potential for large-scale social media analysis and monitoring of public health issues, such as DDI and ADR, uncovering behavioral pathology and associations between drugs and symptoms in depression [15  ###reference_b15###].\nIt can even be used as a tool for communication and support between patients and healthcare providers [19  ###reference_b19###], and other health-related applications [1  ###reference_b1###].\nFor these reasons, the work described here is focused on data from Instagram, but our methodology and results are applicable to other social media from Twitter to Reddit [20  ###reference_b20###].\nDespite the benefits of social media analysis for public health, social media research has not focused much on people with epilepsy (PWE), a chronic noncommunicable brain disorder and one of the most common neurological diseases [21  ###reference_b21###].\nIn the U.S., more than three million adults have epilepsy, and about 470,000 children were diagnosed with active epilepsy in 2015 [21  ###reference_b21###, 22  ###reference_b22###, 23  ###reference_b23###].\nEpilepsy-relevant Facebook pages and Twitter accounts play an essential role in providing information about drugs or correcting misconceptions or epilepsy stigma on online platforms [24  ###reference_b24###, 25  ###reference_b25###].\nFurthermore, our team has shown that even small cohorts of epilepsy patients on Facebook can inform experts about relevant behaviors involved in rare outcomes, such as sudden death in epilepsy [6  ###reference_b6###].\nTherefore, more research on PWE and their caregivers\u2019 online behaviors on social media, from epilepsy-specific online groups to general-purpose platforms, is needed\u2014especially to better understand their complex symptoms and medication schedules, including DDI and ADR.\nTo support such a research agenda, it is essential to develop automated annotation pipelines to mine and detect biomedical signals from large-scale social media data in general [1  ###reference_b1###, 26  ###reference_b26###].\nAt the core of such pipelines is the construction of biomedical dictionaries to tag relevant terminology in social media posts.\nTypically these are produced from databases and named entity recognition tools that were developed for scientific discourse, such as papers with experimental evidence available on PubMed [15  ###reference_b15###, 1  ###reference_b1###, 27  ###reference_b27###, 28  ###reference_b28###].\nHowever, it is unclear whether biomedical dictionaries built from scientific discourse and evidence are fit for the informal discourse and particularities of discussions on Instagram and other social media that are relevant for epilepsy (or other conditions of medical interest).\nTo address that question, here we present a human-centered dictionary refinement methodology and analysis tailored to tag clinically relevant terminology for the study of epilepsy cohorts on Instagram .\nFor comparison, OpenAI\u2019s GPT series models, as representatives of Large Language Model (LLMs), were also used as an alternative annotation process, though leading to worse results than human annotators in our analysis.\nIn addition to producing a focused biomedical dictionary for social media, our manual annotation effort demonstrates that false positive terms (clinically relevant terms used in a clinically irrelevant context) with high frequency exist in social media discourse.\nIn other words, dictionaries built from biomedical terminology appropriate for the scientific literature, contain terms that are used in social media with other meanings.\nMoreover, those terms bias the knowledge inferences that automated pipelines might produce.\nIndeed, we demonstrate that the removal of just a few high frequency false-positive dictionary terms improves the biomedical knowledge extracted from the epilepsy cohort on Instagram, thus highlighting the importance of human annotation to improve the quality of cohort-specific social media analysis."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Data and Methods",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Dictionary Construction",
            "text": "Our dictionary includes terms related to drugs, allergens, medical terms, and natural products, including cannabis. We construct this dictionary following [15  ###reference_b15###, 29  ###reference_b29###]. We recall the deails of that construction below.\nWe obtained these terms from a variety of existing medical ontologies and data sources.\nDrug, allergen, and food terms are retrieved from Drugbank (v.5.1.0) [30  ###reference_b30###].\nMedical terms including, but not limited to, disease symptoms and drug side effects are obtained from MedDRA (v.15) [31  ###reference_b31###].\nNatural products are retrieved from MedlinePlus [32  ###reference_b32###] and TCMGeneDIT [33  ###reference_b33###].\nFor cannabis, we manually added commonly referred terms and slang, such as \u2018Mary Jane\u2019 and \u2018420\u2019, to our dictionary, as detailed in [15  ###reference_b15###].\nIn addition, epilepsy terms commonly used by patients on Internet forums were manually added using a C-value [34  ###reference_b34###] tokenizer on the Epilepsy.com discussion forums.\nThese epilepsy-related terms include mentions such as \u2018VNS\u2019 (i.e. Vagus Nerve Stimulator) and were validated by an epilepsy specialist and matched to MedDRA codes.\nWe distinguish dictionary terms into four categories: Allergens, Drugs, Medical Terms, and Natural Products.\nAllergens include food names, ingredients, and animals (e.g., Orange, Duck);\nDrugs include medicine and chemical compounds (e.g., Diazepam);\nMedical Terms include status and conditions of putative medical relevance, such as physical, psychological, or physiological features (e.g., Headache, Feeling hot);\nNatural Products consist of plants and their extracted elements (e.g., Rose).\nImportantly, synonyms are possible for each term. Therefore, all those are matched\u2014as child terms\u2014to a unique parent (preferred) term.\nFor instance, Weed, Mary jane, 420 and Cannabis are all synonyms of the parent term Cannabis.\nThe parent term is also included as a child term for completeness of synonym lists.\nDrug names are treated similarly, whereby we keep the chemical name as parent terms (e.g. Diazepam), and all known commercial names (as extracted from DrugBank [30  ###reference_b30###]) as child terms (e.g. Valium).\nSome data sources of our dictionary already have term hierarchy, and we used it as the base of our parent term mapping (for example, the \u201cpreferred term\u201d in MedDRA is mapped to the parent term in our dictionary).\nTable 1  ###reference_### lists examples of the extracted posts, terms, and their parent term.\nDrug brand names sometimes have very common names in the English language, which may increase the number of false-positive hits (e.g., Nighttime is a common word and a synonym for the drug Benadryl).\nTo account for that, we matched terms in our dictionary to the expected occurrence of such terms in the Brown Corpus [35  ###reference_b35###].\nTerms very commonly used in the daily English language were then ranked and removed.\nAfter this collection and initial automatic curation procedure, our dictionary contains 176,278 terms, of which 105,345 are Drugs, 66,961 are Medical Terms, 2,797 are Allergens, and 1,175 are Natural Products."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Data Collection and Post Tagging",
            "text": "Since June 2016, the collection of Instagram data via the platform\u2019s API has been limited due to a company policy change.\nHere we use publicly available data from Instagram posts ranging from 2011 to early 2016 when they were collected and securely stored according to the platform\u2019s terms on our servers [15  ###reference_b15###].\nThe data is comprised of the entire timelines (all time-stamped public posts) of Instagram users who produced at least one post mentioning a hashtag (i.e., #) with a drug name (or synonym) known to treat epilepsy.\nThe (parent) drug names we used to retrieve timelines include carbamazepine, clobazam, diazepam, lacosamide, lamotrigine, levetiracetam, oxcarbazepine, as well as all their brand name (child term) synonyms (e.g., Valium).\nIn addition, we added all user timelines that mentioned the epilepsy-associated hashtag \u2018#seizuremeds\u2019 which is commonly used among PWE on discussion forums such as Epilepsy.com.\nThis resulted in the collection of the entire timelines of a cohort of 9,890 users, comprising 8,496,124 posts.\nDuplicate posts from regrams were removed.\nIn order to protect user privacy, we did not extract demographic information from the collected accounts.\nThe caption field of all collected posts was subsequently tagged with the dictionary terms according to an automatic multi-word lexical matching pipeline, resulting in a total of 979,683 dictionary term matches on the more than 8 million Instagram posts on the dataset."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Results",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Human-centered Annotation",
            "text": "Automatic lexical matching with biomedical dictionaries built from scientific discourse and evidence is not necessarily contextually accurate when tagging social media posts. Indeed, dictionary terms used in social media discourse may refer to alternate meanings without any putative clinical relevance. For instance, the term \u2018hot\u2019 has multiple meanings depending on context, ranging from \u2018having a fever\u2019 to \u2018sexual appeal\u2019. Naturally, we are only interested in the potential clinical relevance of dictionary term usage. Therefore, we need to refine the dictionary terms to improve the accuracy of term matches in biomedical social media analytics. That is, the goal is to reduce false positive term matches, such as in the example above.\n\nThe number and proportion of matches per dictionary category is:  = 874 (29.7%; 108 parent terms),  = 204 (6.9%; 64 parent terms),   = 1,647 (55.9%; 268 parent terms), and   = 222 (7.5%; 26 parent terms). \n\nThis sample was subsequently used in our annotation workflow which comprises two rounds:\nData scientists in training as well as epilepsy researchers participated in the annotation. They decided whether terms were appropriately matched (true-positive), inappropriately matched (false-positive), or unclear to determine (e.g., a term with unclear meaning).\nAnnotations were compared, achieving a good inter-rater reliability rating (Cohen\u2019s Kappa=0.634) [36  ###reference_b36###]."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Identifying Ambiguous Terms",
            "text": "After the full annotation review step of the human-centered annotation process, 839 (28.5%) false-positive term matches were observed\u2014by annotators considering and inferring the context of the post where the match occurred from text alone. Examples of posts with false-positive matches are shown in Table 3  ###reference_###.\nIn one case (top row), from context, annotators infer that the dictionary term for Orange (as fruit it is in the\nallergen\ncategory) is actually used to mean the color orange.\nSimilarly, in another post (bottom row), rose was actually identified as a typo for ros\u00e9 (wine), thus assigned to an incorrect parent dictionary term.\nFocusing on each of the four term categories, the human-centered annotation revealed the following false-positive rates (including unclear cases):\nAllergen: 166 out of 874 (19%).\nThese include Orange, Apple and Ginger which were frequently found to indicate a color, name (brand, pet, place, etc.), or toys, respectively.\nDrug: 35 out of 204 (17%).\nThe match for Valium (brand name for diazepam) had the highest false positive rate in this category.\nSince this drug brand name is frequently used as a metaphor on social media (unlike in the scientific literature), a discourse feature human annotators can easily infer (unlike most automatic methods).\nMedical term: 571 out of 1,647 (35%). This is the category with the largest observed false-positive rate (including unclear cases), due to the possible broad meanings in which these terms were frequently used.\nFor instance, Hot resolves to the Feeling hot parent term, which clinically means to be \u201chaving or feeling a relatively high temperature\u201d (fever).\nHowever, in the sampled posts, Hot often meant \u201csexually excited\u201d or \u201cnewly made\u201d [37  ###reference_b37###]\u2014another case of very distinct usage between social media and scientific discourse.\nNatural product: 67 out of 222 (30%).\nExamples include Rose, Rosa, and Daphne, where the inferred context often indicated one\u2019s name or color.\nGiven the false-positive rates observed for term matches of the post sample, the next step is to identify which terms are most ambiguous and should be removed from the dictionary in order to refine and tailor it to epilepsy discourse on social media, our problem domain.\nFigure 2  ###reference_### charts the false-positive rate against the frequency of all parent terms evaluated by the manual annotation workflow described above.\nThe most concerning terms are naturally those that occur very frequently in our data and simultaneously have a high false-positive rate\u2014those on the top and right of Figure 2  ###reference_###.\nTable 4  ###reference_### lists the top 8 such child terms, as well as their respective parent terms.\nFor instance, the terms Hot, Cold and Euphoria are frequent matches in posts, but most of the time without any putative medical significance with false-positive rates above 90%, since they typically occur in contexts without any relationship to the intended meaning of the term in the biomedical dictionary.\nThe same can be said for the terms Rose and Orange, though less frequently (fewer than 30 matches) but still unacceptably high false-positive rates above 80%.\nIn contrast, while the term Cannabis appears very frequently in our samples (144 total matches for all of its child terms), it is most often used in relevant contexts (126 matches (88%)). In Figure 2  ###reference_###, we show two of the synonyms of Cannabis: 420 and weed.\n###figure_3### To obtain a new refined dictionary we established a criterion to remove terms that maximize false-positive rate and occur frequently enough in the human-annotated post sample.\nWe first selected terms with false-positive rates  and frequency  (red and blue dashed lines in Figure 2  ###reference_###, respectively), subsequently ranking them by frequency.\nThis resulted in the removal of 8 terms listed in Table 4  ###reference_###:\nHot, Cold, Euphoria, Valium, Death, Rose, Orange, and Ginger.\nAmong the 8 terms, we found that most occurrences of Hot were not about feeling elevated temperature but about feeling excitement or describing something as popular. On the contrary, Cold was mostly used as a term about temperature, not about the illness known as the Common Cold. Likewise, the meaning of Euphoria was often not related to one\u2019s feeling of joy, since it was frequently used in club music-relevant contexts that may indicate a musical genre, a music album, or a brand. As for Valium, if the word was mentioned with other terms related to drugs, it was easier to determine whether it is a true- or false-positive term. However, almost half of the posts that mentioned it did not provide enough context. In the remaining half, it was often used as a metaphor for something boring (e.g. example on the second row of table 3  ###reference_###). Finally, Rose, Orange, and Ginger in the posts were frequently used as proper nouns, such as a person, an area, or a pet.\nNotice that we could have removed more than these 8 terms with a false-positive rate above 50% (red dashed line in Figure 2  ###reference_###).\nHowever, such terms have low frequencies of matches in our sample, which means that the false-positive rate was estimated with few human-annotated matches and potentially subjected to fluctuations from small sample size.\nThus, to be conservative given that terms originate from established medical dictionaries, we retained them.\nIn any case, as we show next, the impact of removing these 8 is much larger than removing other terms with similar frequency."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Impact of Removing Ambiguous Terms",
            "text": "After dictionary refinement, we evaluate the impact of term removal on the eigenvector-centrality\nof networks whose edge weights are obtained by tallying dictionary co-mentions in posts (their construction, especially how we dealt with parent terms and child terms, is detailed in SI Section B.1  ###reference_###).\nThese co-mention networks can be seen as associative knowledge structures that characterize the discourse of social media cohorts [38  ###reference_b38###, 39  ###reference_b39###, 15  ###reference_b15###, 26  ###reference_b26###].\nThe co-mention networks is built on the level of parent terms in the sense that each node is a parent term covering several synonyms in the dictionary.\nIn such associative knowledge networks, removing terms from the dictionary potentially reduces the set of nodes that comprise them (each parent term is represented as a node and the term removal is done in the level of child term). More importantly, removing terms also affects the edge weights between nodes, as co-mention counts are altered. Since the structure of connections is altered, all inferences one can make from these networks\u2014such as information retrieval, link prediction, community structure, shortest paths [39  ###reference_b39###]\u2014can be affected.\nWe chose eigenvector centrality because it is a popular measurement of node importance that accounts for indirect influence between nodes on undirected graphs, thus allowing us to assess the network-level impact of term removal [40  ###reference_b40###].\nAnother popular node centrality is PageRank centrality. Although it is powerful on directed graphs, it is highly correlated with (or almost exactly the same as) node degree when applied to undirected networks [41  ###reference_b41###, 42  ###reference_b42###]. Therefore, since our co-mention networks are undirected, we choose eigenvector centrality rather than PageRank to assess the effect of removing terms.\nThe top 20 terms ranked by eigenvector centrality, before and after dictionary refinement, are shown in table 5  ###reference_###.\nTop eigenvector centrality terms before the refinement include terms not particularly relevant to epilepsy, such as Cocoa or Tattoo.\nHowever, after dictionary refinement, not onlydo parent terms like Feeling hot disappear from the top, since their main child terms were no longer present in the dictionary, but we see that all the top 10 terms are associated with epilepsy, as attested by our epilepsy specialists.\nFor instance, Depression, a clinical diagnosis often co-morbid with epilepsy, jumps from 9th (0.040) to the top ranked centrality score term (0.599), closely followed by Anxiety.\nThis suggests that our dictionary refinement improved the quality of the top eigenvector centrality terms, bringing epilepsy-related terms to a more central role in the knowledge network.\n###figure_4### To investigate whether the observed changes on the top eigenvector centrality terms are simply due the high frequency of the 8 removed terms, we computed a null model experiment in which 8 terms with similar frequencies but smaller false positive rates (below ) are randomly sampled 1,000 times and removed, with subsequent computation of eigenvector centrality of the resulting networks.\nBecause the top  lists before and after dictionary refinement do not have the same set of terms, common rank difference metrics, such as Spearman\u2019s coefficient and Kendall\u2019s  cannot be directly applied.\nIn Figure 3  ###reference_###, we compare the top  terms by eigenvector centrality (of the dictionary term co-mention networks), before and after removing 8 dictionary terms, by counting the common elements ratio (CER) between the top  lists for each case.\nIt is very clear that the impact of removing the 8 terms with false positive rate above  (via human-annotation) is much higher than when we remove 8 random terms with same or higher frequency, but lower false-positive rate.\nFor  (the top 10 with largest eigenvector centrality), only  of the terms remain after removal of the 8 terms with largest false positive rate per human annotation. Whereas, in the case of random removal of 8 terms with similar frequency, on average,  of the terms remain unaffected in the top 10.\nThe difference between the selected terms and the null model remains large for all values of , even if CER increases.\nWhile the impact of removing random terms almost saturates after , at 91% CER, the impact of removing the 8 terms with high false positive rate remains significant for all , with over 20% different terms even for .\nAdditionally, we also use Fagin\u2019s generalized Kendall\u2019s distance , a metric specially designed to compare top  ranked lists [43  ###reference_b43###]. We use it to quantify the impact on rank of removing the 8 ambiguous terms by computing the distance between the eigenvector centrality rankings obtained before and after removal.\n is comparable only to lists with the same sizes (the same ), with a higher value denoting a higher rank difference, with the penalty of missing elements considered.\nThe value of  obtained using our refined dictionary (denoted ) for various values of  is given in Table 6  ###reference_###, alongside the average distance obtained by removing 8 similarly frequent random terms (denoted ).\nTable 6  ###reference_### shows that the 8 ambiguous terms selected through human annotation have statistically significantly larger impact on eigenvector centrality rankings (higher ) beyond the top 20 nodes than removal of similar random sets of terms. Though removing random high frequency terms could have a similar impact on the top 10 terms list in relatively rare cases (about 14% of the time), we observed no case (out of 1,000 samples) in which this occurred for the top 20 terms or beyond.\nMoreover, the difference between  for the 8 ambiguous terms and the average  for the randomly removed terms increases dramatically as the list size  increases.\nThe null model comparison, with both CER and Fagin\u2019s generalized Kendall\u2019s distance for rank comparison, demonstrates that removal of terms revealed as ambiguous by human annotation, have much higher impact on the knowledge network of biomedical terms than removing random terms with the same frequency (but lower false positive rate).\nIn other words, the high false-positive rate terms, estimated per human annotation, are likely confusing the co-mention network with many spurious edges that are not coherent with the remaining biomedical knowledge captured by the co-mention networks. Thus, removing them has a high impact on the network because their co-mention associations are quite different from those of other terms in dictionary.\nIndeed, random removals of terms with similarly high frequency are not as impactful, because many of the associations the random terms induce are more coherent with the structure of other biomedical term associations.\nIn summary, the network global impact of removing human annotated high false-positive rate terms highlights the value of human context-specific annotation."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Comparing Social Media with Medical and Scientific Discourse",
            "text": "At the onset, the hypothesis behind our study is that while there is much discourse on general-purpose social media platforms that is of biomedical relevance, it is expressed differently than scientific discourse and unfolds simultaneously with many other contexts that are not relevant to health. Thus, human-centered dictionary refinement is needed to remove ambiguous terms in such platforms.\nTo emphasize this need, and how different general-purpose social media discourse is, we go beyond Instagram and deploy both the original and the refined version of our dictionary in other data sources: epilepsy-related abstracts from PubMed, epilepsy-related clinical trials from clinicaltrials.gov  ###reference_clinicaltrials.gov###, Twitter (now named X) posts from users who have mentioned epilepsy related terms, and discussion forums from Epilepsy.com  ###reference_Epilepsy.com###. Detailed descriptions of data harvesting and network construction of additional data sources are available in SI Appendix B  ###reference_### under each data source section.\nThe first two additional data sources pertain to scientific discourse.\nBoth Instagram and Twitter data sources represent epilepsy related users\u2019 speech on a general-purpose social media. The last data source is a form of social media that is focused on a single topic of medical relevance\u2014epilepsy, in this case.\nIn Table 7  ###reference_### we compare the impact of removing the 8 high false positive terms using Fagin\u2019s generalized Kendall\u2019s distance as in Table 6  ###reference_###.\nTwitter, being a general purpose social media like Instagram, received an impact approximately a half of Instagram received after terms removal, as measured by Fagin\u2019s generalized Kendall\u2019s distance.\nOther data sources received much less impact. Networks built from clinicaltrials.gov  ###reference_clinicaltrials.gov### data and the Epilepsy.com  ###reference_Epilepsy.com### forums data set received zero impact on top 20 terms list and at most 6% of the impact as Instagram on other lists. PubMed, being a much larger text corpus, received only at most 22% of the impact as Instagram.\nThis is also supported by Table S2  ###reference_###, Table S4  ###reference_###, Table S9  ###reference_###, and Table S11  ###reference_### in SI, in which we show that aside from Twitter, for all other data sources, there are only inconsequential impacts on the top 20 terms ranked by eigenvector centrality\u2014except for the disappearance of those 8 terms we removed.\nIn other words, unlike in Instagram and Twitter , those 8 terms are either not ambiguous at all or their ambiguous usages do not affect the network analysis greatly in scientific database (PubMed and Clinical trials) nor in epilepsy-specific social media (Epilepsy Foundation Forums) discourse.\nThis suggests that general-purpose social media platforms such as Instagram are much noisier for biomedical surveillance, and for epilepsy research in particular, since users tend to post about many distinct aspects of their lives.\nIn contrast, in the Epilepsy Foundation (EF) forums users center discourse around their condition, which naturally makes it a rich resource for epilepsy research.\nThe same can be said for the chosen PubMed articles or Clinical Trials, where the scientific context is focused on epilepsy.\nTherefore, the impact of removing the same 8 terms from the knowledge networks of both the EF forums and the PubMed abstracts is much less pronounced in comparison to Instagram ."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "The disagreement between GPT-4 and human annotators is significant",
            "text": "Complementary to our human annotation efforts, we tested the feasibility of using a large language model instead of human annotators in the labeling process of our proposed dictionary refinement workflow. We assigned the same labeling task to OpenAI\u2019s latest and most advanced model, GPT-4 (version 1106 in 2023) and its precesssor GPT-3.5 via OpenAI\u2019s API, using the same guidelines we provided to human annotators [44  ###reference_b44###, 45  ###reference_b45###].\nWe find significant disagreement between GPT-4 and the decisions of human annotators.\nWe use OpenAI\u2019s API to ask OpenAI\u2019s GPT-3.5 (version gpt-3.5-turbo-1106) and GPT-4 (version gpt-4-1106-preview) to do the same labeling task as our human annotators.\nWe converted the human annotation guidelines (detailed in section 3.1  ###reference_### and summarized in fig. S2  ###reference_###), initially designed for humans and presented to them in slides, into a text-based prompt for the AI, undergoing several rounds of refinement for optimization.\nFor human annotators, the term matches to be annotated are highlighted in color, while for the LLM\u2019s task, these terms are marked with asterisks (*) on both sides.\nThis method serves as a text-based alternative to visual highlighting, and is commonly used in prompting LLMs.\nFor each term tagged on a post, we query the LLM using two prompts: one system prompt based on the annotation guidelines, and one user prompt containing the entire text of post with the highlighted term.\nThe LLM was instructed to provide a response in json format, with both the decision label and justification for the classification.\nThe model\u2019s response was then collected and parsed.\nFor term matches that the LLM returned different label than human annotators, we used the justification text generated by the model to suggest possible causes for the disagreement.\nFor each round of our prompt iteration, we used 200 tagged terms to test the performance.\nDuring those tests, GPT-3.5 consistently showed inferior performance compared to GPT-4, so we did not run a full-scale evaluation for GPT-3.5, choosing to focus on GPT-4 only.\nFor the final full-scale test, we sent 1,500 term matches to GPT-4 (corresponding to 1,500 rows in the tables for human annotators), using our final version of the prompt.\nTreating the consensus of two human annotators as the ground truth and grouping all \u201cUncertain\u201d or \u201cDisagree\u201d (if the two annotators gave different annotations) labels with the negative label, we find a Matthews correlation coefficient (MCC) of  for GPT-4\u2019s classification results. This improves slightly to  if we discard all \u201cUncertain\u201d or \u201cDisagree\u201d from both GPT-4 and human annotators\u2019 annotation.\nIn particular, GPT-4 tended to label correctly matched terms (\u201cTrue Positive\u201d in the annotation guideline and \u201cmatch\u201d for short in the following tables) as incorrectly matched (\u201cmismatch\u201d for short in the tables). In other words, it is more strict in designating a term as being used in a putatively clincally relevant way.\nAs shown in Table 8  ###reference_###, for the same set of 1,500 term matches, there are only 29 cases where the master annotator determined it as \u201cmismatch\u201d while the annotator 2 thought it was a \u201cmatch\u201d, in comparison, there are 285 cases where GPT-4 thought it was \u201cmismatch\u201d and the annotator 2 thought it was a \u201cmatch\u201d."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this paper, we show the importance of manual curation in developing a biomedical dictionary to study epilepsy-related discussions on social media.\nDespite its cultural importance and global reach, Instagram , the social media we study has not been a focus of social media research.\nIn general, most biomedical research on social media focuses on platforms with freely and easily accessible data, such as Twitter (now named X).\nIn addition, such studies are either interested in a few drugs or medical-related terms [46  ###reference_b46###, 47  ###reference_b47###, 48  ###reference_b48###], or on conditions with less associated stigma.\nVery few studies so far have analyzed the social media discourse of people with epilepsy and their caregivers [6  ###reference_b6###] using biomedical dictionaries.\nThe use of dictionary-based methods to study biomedical discourse, however, is not new.\nIn general, they have been used for knowledge discovery and applied to large corpus of clinician-written notes over the course of clinical examination, and more recently extracted from electronic health records [49  ###reference_b49###, 50  ###reference_b50###, 51  ###reference_b51###, 52  ###reference_b52###].\nAs much as these dictionaries have been shown to work well in clinical settings, they have not been tailored to the informal and broad discourse of social media.\nTherefore, our goal here using human annotation is to identify and evaluate the impact of false-positive dictionary terms matched to social media data.\nWe were able to identify and remove terms with high false-positive rates and that were not removed in our initial automated curated process, thus highlighting the importance of a human-in-the-loop throughout the curation process.\nHuman annotation has been widely used to analyze and understand sentiments, behaviors, perceptions, languages, or relations of people in social media [53  ###reference_b53###, 54  ###reference_b54###].\nEven though automated annotation techniques are able to identify and match terms faster than human annotation [55  ###reference_b55###], our results support prior literature showing human annotation is necessary when extracting term meaning from social media posts [56  ###reference_b56###, 57  ###reference_b57###].\nTherefore, balancing between the speed of automated annotation and the reliability and validity of human annotation can be one of key approaches to optimal results [56  ###reference_b56###, 58  ###reference_b58###, 59  ###reference_b59###].\nWe identified what kinds of terms tend to mean differently than their biomedical meaning through human annotation and how often they have been used in social media posts.\nThe false positive matches were from the different meanings of words. Proper nouns were one of the most common cases.\nBesides, words were often used as metaphors, which cannot be learned from the English dictionary.\nThese results imply that future research on social media with biomedical dictionaries should be cautious about terms with multiple meanings to perform a more precise analysis.\nAnother noise source in term tagging on social media is lack of context information.\nThere were cases where our annotators found it difficult to determine the true meaning of the words due to lack of context.\nIt would be impossible for machine based automated methods to determine whether those term matches should be tagged or not.\nIf there are abundant cases among them that the term was actually not used with the biomedical relevant meaning, they would introduce the same kinds of noise as the ambiguous terms we identified in this study, to the text mining analysis.\nOur results of dictionary refinement via human annotation also demonstrate how a dictionary with ambiguous terms can have a great impact on downstream data analysis.\nWe showed that removing a few terms with high false-positive rates from the dictionary, guided by human annotation, could significantly change the results of network analysis.\nWe found that the impacts of the refined dictionary were not limited to the terms we removed.\nThere is no reason to believe that eigenvector centrality is the only analysis that will benefit from dictionary refinement.\nTrivially, network analysis based on node weight generated by node centrality based methods could be affected.\nIn addition, in future studies, we can also test the impact of dictionary refinement on other network analysis, including, but not limited to community detection and link prediction, all the way to impact on actual application like information recommendation system.\nThe great difference in the impact of the same dictionary refinement on different data sources also suggests potential network-based methods for identifying low-quality terms in the dictionary.\nThose low quality terms could be either ambiguous terms we discussed in this study or terms should not be in the dictionary in the first place.\nIf our assumption is correct, the majority of the knowledge structure and spurious connections generated by noise shall have distinctively different structure patterns.\nEither seperating the noisy pattern from the majority knowledge structure directly, or simply removing nodes and measuring the impact like we did for the dictionary refinement, could be potentially feasible approaches to identify the low-quality terms.\nWe also found that some data corpus benefit more from this dictionary refinement than other data corpus, suggesting the need for dictionary refinement should tailor to each individual research topic and data corpus.\nThe four additional data sources besides Instagram may have different sets of their own ambiguous terms distinct from the 8 identified for Instagram . Indeed, the manual annotation focused only on Instagram posts and it is possible that a similar approach could be useful for refining dictionaries specific to the EF forums, PubMed abstracts, Clinical Trials and even Twitter .\nDue to the associated cost of manual annotation, it is beyond the scope of this work to do so.\nThe variation in ambiguous terms across datasets may be attributable to the inherent characteristics and the qualitative distinctions of each data corpus.\nLanguage usage and richness levels could be different between social media, online health communities, and medical resources. For instance, polysemous words in online health communities and medical resources have much higher chances of being used with their medical-relevant and professional meanings than their casual meanings than in social media.\nIdentifying these differences may benefit future researchers in improving the quality of biomedical signal analysis on different types of datasets.\nEspecially, it would be essential to be cautious about the noisy dataset, such as data from social media. Using deep learning models, multi-corpus training, and normalization could be other ways to improve the performance of social media mining regarding the differences between those resources [60  ###reference_b60###].\nIn Section 3  ###reference_###, we have shown that GPT-4, as a representative of the latest LLM, could not replace human annotators in this task.\nThere are several factors that contribute to this performance difference.\nThe most significant factor we observed was that the GPT-4 often stayed within the narrow definition of some terms in the guidelines.\nFor example, in the guidelines, we mentioned that some terms in our dictionary were imported from medical dictionaries like MedDRA.\nHuman annotators could understand why \u201cmarriage\u201d and \u201ctattoo\u201d are in MedDRA in the first place, and why \u201corange\u201d as food can be an allergen, then labeled the term matches as \u201ctrue positive\u201d when they thought it was appropriate, while GPT-4 believed that the term was not used as a medical term and therefore should be labeled as \u201cfalse positive\u201d.\nA few counterexamples in the prompt can help, but cannot eliminate such kinds of bias.\nGPT-3.5 did much worse than GPT-4 in this kind of bias (see SI Section C.3  ###reference_###).\nFor both models, it was hard to enumerate all possible term types for which the model may fail.\nIn this paper, we discuss the results of using GPT series models as a direct replacement for human annotators.\nHowever, it is not the purpose of this paper to find the optimal workflow to use LLM on these types of tasks.\nStill, we gained some valuable experience about prompt engineering for the labeling task in this paper and we recommend people to spend considerable time on prompt engineering if they aim to use LLM for similar tasks.\nThe time spent on prompt engineering should be included into the time cost in project management.\nThe prompt engineering is not just about writing one good prompt from scratch, but is more about performing a comprehensive test to catch the \u201cslips\u201d or the bias of LLM in some types of terms, then trying to correct the bias in many iterations of the prompt.\nThis could require substantial amounts of human annotated data, depending on the goal of the research, with more accurate estimation on the term mislabeling need more data, and coarser estimation requires less.\nSome sampling methods could be used to reduce the size of data set needed, but in principle cannot improve the coverage of corner cases.\nIf sufficient human annotated data are readily available, fine-tuning the model could potentially be more effective than prompt engineering.\nTaking into account all those steps, the use of LLM for term labeling still requires substantial time investment if accuracy is a priority.\nAlthough current LLMs are not perfect replacements for human annotators, the research on LLM is a fast developing field with better models coming out every year.\nWe should not underestimate the potential of LLM in the future application of social media mining.\nBearing in mind the prerequisites of conducting comprehensive validations prior to the deployment of LLMs, several promising approaches are currently emerging that utilize LLMs to enhance term tagging in the immediate or foreseeable future.\nOne possible hybrid approach is discussed above: first uses human annotation to align LLM output, then LLM could be used to annotate larger data corpus. LLM\u2019s annotation will then be used for dictionary refinement. In this way, we accumulate much more annotated data, thus providing better coverage for low-frequency terms and leading to potentially better dictionary refinement, with the same amount of human annotation data.\nIn the future, LLM could also be used to tag each occurrence of the term directly for the entire text corpus without dictionary refinement, so the \u201ccorrect\u201d usage of high false positive rate terms can be tagged and contribute to downstream analysis, in contrast to discarding all matches of high false-positive rate terms.\nHowever, even if current LLMs are sufficiently accurate, their high operational costs still prohibit large-scale applications. This limitation is likely to be alleviated in the future."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "This research identified the terms in our dictionaries with higher frequencies and false-positive rates in an epilepsy cohort on Instagram by using the human annotation method.\nOn comparison, GPT series models cannot replace human annotation in the annotation process.\nWe also identified which terms should be removed from our dictionary to obtain more epilepsy-relevant results from network analysis. Additionally, we identify the fundamental cause of incorrect term matches\u2014having different meanings that are often used\u2014and specific cases. Finally, through validation of the impact of the refined dictionary on eigenvector centrality analysis of the co-mention networks, we demonstrated the impacts and complementary role of the human annotation method to the automated annotation technique.\nOur results demonstrate that human-centered dictionary refinement should be performed when conducting social media mining of biomedical relevance, and epilepsy in particular. It remains to be determined if this approach is also necessary for analyzing other types of scientific or patient-centered textual resources.\nOur study has utilized a large number of terms in medical dictionaries and focused on Instagram , which may have significant potentials for large-scale social media analysis. Other social media studies on medical corpus have more focused on Twitter and Facebook , and most of them have selected a few specific medical terms in their analyses.\nOur results and implications may help future research in our research communities, including the areas of bioinformatics and health informatics, by improving analysis of medical topics on social media speeches.\nUltimately, this research would contribute to developing knowledge graphs that more truthfully represent the underlying knowledge structure, which can be used for personalized information systems that can support PWE, PWEC. And the workflow could also be applied to other diseases and support other disease cohorts."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Acknowledgement",
            "text": "This work was partially funded by National Institutes of Health, National Library of Medicine Program, grant 01LM011945-01 (LMR, AM, XW, RBC, and WRM), a Fulbright Commission fellowship (LMR), the NSF-NRT grant 1735095 \u201cInterdisciplinary Training in Complex Networks and Systems\u201d (LMR, AM, XW and RBC), and Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia, grant PTDC-MEC-AND-30221-2017 (RBC)\nThe funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."
        }
    ],
    "url": "http://arxiv.org/html/2405.08784v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2.1",
            "2.2"
        ],
        "main_experiment_and_results_sections": [
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.3",
            "3.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.08784v1",
        "paper_title": "Refinement of an Epilepsy Dictionary through Human Annotation of Health-related posts on Instagram",
        "research_background": "The paper is motivated by the increasing use and vast potential of social media data for public health research. With a specific focus on Instagram\u2014an immensely popular platform with over 1 billion users as of 2021\u2014the study aims to leverage this medium to gather detailed insights into the experiences and behaviors of people with epilepsy (PWE). While prior research has extensively exploited data from platforms like Twitter and Facebook, Instagram remains underutilized despite its rich data offerings.\n\nThe research problem addressed by this study centers on the inadequacy of existing biomedical dictionaries\u2014originally developed for scientific literature\u2014in capturing the informal and context-specific language used in social media posts relevant to epilepsy. The primary issue is that these dictionaries, when applied to social media, often result in false positives, i.e., clinically relevant terms being used in non-clinically relevant contexts. This leads to inaccurate biomedical knowledge extraction and potentially flawed inferences in automated annotation pipelines.\n\nThe related work highlights several key points:\n1. **Utilization of Social Media for Public Health**: Social media has been employed to track and predict health issues such as mental health disorders, adverse drug reactions (ADR), drug-drug interactions (DDI), and substance abuse.\n2. **Epilepsy on Social Media**: Specific platforms like Facebook and Twitter have been utilized to provide information and correct misconceptions about epilepsy, demonstrating their utility in capturing relevant patient behaviors and outcomes.\n3. **Challenges with Current Biomedical Dictionaries**: Dictionaries developed from scientific discourse often fail when applied to the informal language on social media, creating a need for a more tailored approach.\n\nThe paper builds on prior work by proposing a human-centered approach to refining biomedical dictionaries for epilepsy-specific research on Instagram. In doing so, it aims to address the gaps identified in leveraging social media data effectively to study PWE and ensure more accurate and relevant data annotation, thereby improving the quality of social media-based biomedical research.",
        "methodology": "## Refinement of an Epilepsy Dictionary through Human Annotation of Health-related Posts on Instagram\n\n### Methodology\n\n#### Dictionary Construction\nThe proposed method involves the construction and refinement of an epilepsy dictionary containing terms related to drugs, allergens, medical terms, and natural products, with an emphasis on including terms like cannabis. This dictionary was constructed following methodologies detailed in [15 ###reference_b15###, 29 ###reference_b29###] and was populated with terms sourced from various existing medical ontologies and data sources.\n\nKey sources include:\n- **Drugbank (v.5.1.0)** [30 ###reference_b30###] for drugs, allergens, and food-related terms.\n- **MedDRA (v.15)** [31 ###reference_b31###] for medical terms, including disease symptoms and drug side effects.\n- **MedlinePlus** [32 ###reference_b32###] and **TCMGeneDIT** [33 ###reference_b33###] for natural products.\n- **Cannabis-related terms** were manually added, including common slang such as \u2018Mary Jane\u2019 and \u2018420\u2019, referenced from [15 ###reference_b15###].\n\n#### Inclusion of Epilepsy Terms\nEpilepsy-specific terminology was manually curated by analyzing posts on the Epilepsy.com discussion forums using a C-value tokenizer [34 ###reference_b34###]. Terms such as \u2018VNS\u2019 (Vagus Nerve Stimulator) were identified, validated by an epilepsy specialist, and matched to MedDRA codes.\n\n#### Categorization\nDictionary terms were categorized into four specific groups:\n1. **Allergens**: Food names, ingredients, and animals (e.g., Orange, Duck).\n2. **Drugs**: Medications and chemical compounds (e.g., Diazepam).\n3. **Medical Terms**: Physical, psychological, or physiological conditions (e.g., Headache, Feeling hot).\n4. **Natural Products**: Plants and their extracts (e.g., Rose).\n\nEach term in the dictionary was matched as a child term to a unique parent (preferred) term to handle synonyms efficiently. For example, terms like Weed, Mary Jane, and Cannabis were all synonyms under the parent term Cannabis, with the parent term included as a child term for thorough synonym representation. Similarly, drug names retained their chemical name as the parent term (e.g., Diazepam) and included all known commercial names (e.g., Valium) as child terms, extracted from DrugBank [30 ###reference_b30###].\n\n#### Hierarchical Structure and Synonym Management\nThe dictionary leveraged existing term hierarchies from data sources (e.g., \u201cpreferred term\u201d in MedDRA) as the basis for parent term mapping. This hierarchical structure ensured that the most accurate and relevant terminology was retained and effectively organized.\n\n#### Handling Common Names and False-Positives\nTo mitigate false positives due to drug brand names overlapping with common English language terms (e.g., Nighttime, a synonym for Benadryl), the terms were cross-checked against their expected occurrence in the Brown Corpus [35 ###reference_b35###]. Highly common terms in daily language were ranked and removed from the dictionary.\n\n#### Outcome\nFollowing this rigorous collection and automatic curation process, the resultant dictionary encompassed a total of 176,278 terms, categorized as follows:\n- **Drugs**: 105,345 terms\n- **Medical Terms**: 66,961 terms\n- **Allergens**: 2,797 terms\n- **Natural Products**: 1,175 terms",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Experiment Setup:**\nThe primary goal of the experiment is to refine an epilepsy-related dictionary to improve the accuracy of term matches in biomedical social media analytics, specifically on Instagram. The challenge addressed is that terms used in social media discourse may have alternate meanings not relevant to their clinical context. To achieve this, a manual annotation workflow was implemented to identify false positives among dictionary terms used in Instagram posts about epilepsy.\n\n**Datasets:**\n- The dataset comprises over 8 million Instagram posts initially, but due to the impracticality of manually annotating all, a sample was taken.\n- The sample includes 1,771 posts containing at least one matched term, resulting in 2,947 matches linked to 466 unique parent terms.\n- Breakdown of matches per dictionary category: \n  - 874 matches (29.7%; 108 parent terms)\n  - 204 matches (6.9%; 64 parent terms)\n  - 1,647 matches (55.9%; 268 parent terms)\n  - 222 matches (7.5%; 26 parent terms)\n\n**Annotation Workflow:**\n1. **Initial Annotation & Guideline Refinement:**\n   - A subset of 292 posts with 499 dictionary matches was used to test the annotation process and refine guidelines.\n   - The guidelines helped annotators decide if a matched term was used in its intended biomedical dictionary sense (true-positive) or not (false-positive).\n   - Each sampled post was assigned to two annotators for review, and discrepancies were discussed to revise the guidelines accordingly.\n\n2. **Full Annotation Review:**\n   - Using the refined guidelines, the remaining 1,771 posts and their 2,947 matches were reviewed by two annotators independently.\n   - Annotators included data scientists in training and epilepsy researchers.\n   - Annotations were categorized as true-positive, false-positive, or unclear.\n   - The process achieved good inter-rater reliability, indicated by a Cohen\u2019s Kappa of 0.634.\n\n**Evaluation Metrics:**\n- Inter-rater reliability was evaluated using Cohen\u2019s Kappa, with a result of 0.634, showcasing substantial agreement between annotators.\n\n**Main Experimental Results:**\n- The annotation workflow refined the original epilepsy-related dictionary by identifying terms that were frequently matched incorrectly in the context of Instagram posts.\n- This refinement aims to reduce false positive term matches in biomedical social media analytics.\n- The specific numbers showing the distribution of matches across various dictionary categories highlight the prevalence of false positives in social media discourse.\n\nOverall, this manual annotation workflow, involving both initial guideline refinement and full post review, provided a systematic approach to enhancing the contextual accuracy of biomedical dictionaries used in analyzing social media content."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To refine a biomedical dictionary by identifying and removing imprecise health-related terms used ambiguously in social media posts, particularly focusing on posts related to epilepsy on Instagram.",
            "experiment_process": "Following a full review of human-centered annotations, 839 false-positive term matches were found, representing 28.5% of cases. Annotators identified ambiguous terms such as 'Orange' (used for color), 'Rose' (typo for 'ros\u00e9'), and 'Valium' (used metaphorically). False-positive rates were calculated for four term categories: allergen (19%), drug (17%), medical term (35%), and natural product (30%). Terms with high false-positive rates were then plotted against their frequency, with the most problematic terms identified and slated for removal. A criterion based on maximizing false-positive rates and frequency was established, leading to the removal of 8 terms: Hot, Cold, Euphoria, Valium, Rose, Orange, Ginger, and Death.",
            "result_discussion": "Removing these terms significantly reduced the ambiguity in the dictionary, as evidenced by a drop in false-positive matches. Terms with high false-positive rates, such as 'Euphoria' and 'Hot', often lacked medical significance within the social media context. The refined dictionary improved the relevance of terms, presenting a more accurate mapping to epilepsy discourse.",
            "ablation_id": "2405.08784v1.No1"
        },
        {
            "research_objective": "To evaluate the impact of removing ambiguous terms on the eigenvector centrality of associative knowledge networks built from social media posts mentioning epilepsy.",
            "experiment_process": "Eigenvector centrality, which measures node importance based on indirect influence in undirected graphs, was chosen to quantify the impact of term removal. Co-mention networks of dictionary terms were created from social media posts. Top terms by eigenvector centrality were listed before and after the removal of ambiguous terms. A null model experiment was also conducted, where 8 frequent but less ambiguous terms were randomly removed 1,000 times to compare the impact with the removal of human-annotated ambiguous terms. Metric comparisons included common elements ratio (CER) and Fagin\u2019s generalized Kendall\u2019s distance.",
            "result_discussion": "Removing the 8 ambiguous terms led to a substantial shift in the network, with top centrality terms becoming more relevant to epilepsy (e.g., Depression and Anxiety). The refined dictionary had a greater impact on eigenvector centrality compared to random removals, indicating that human-analyzed ambiguous terms disrupt the biomedical knowledge network more significantly. The null model supported this finding, as random high-frequency term removal had less impact, affirming the significance of the refined dictionary.",
            "ablation_id": "2405.08784v1.No2"
        },
        {
            "research_objective": "To compare the discourse of general-purpose social media platforms like Instagram and Twitter with scientific and focused medical platforms regarding the impact of removing ambiguous terms.",
            "experiment_process": "Both original and refined dictionaries were applied to data from Instagram, Twitter, PubMed epilepsy-related abstracts, clinicaltrials.gov epilepsy clinical trials, and Epilepsy.com forums. The impact of term removal was assessed on the knowledge networks of each data source using Fagin's generalized Kendall\u2019s distance, quantifying changes in top terms ranked by eigenvector centrality after term removal. Data harvesting and network construction details were provided in supplementary sections.",
            "result_discussion": "General-purpose social media like Instagram and Twitter showed significant impact from term removal, indicating high noise levels in biomedical relevance. In contrast, scientific databases and specialized forums had minimal impact, suggesting more focused and relevant discourse. This highlights the necessity of human-centered annotation for refining biomedical dictionaries used in general-purpose social media mining.",
            "ablation_id": "2405.08784v1.No3"
        }
    ]
}