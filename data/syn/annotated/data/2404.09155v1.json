{
    "title": "Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding",
    "abstract": "Recent studies have highlighted the effectiveness of tensor decomposition methods in the Temporal Knowledge Graphs Embedding (TKGE) task. However, we found that inherent heterogeneity among factor tensors in tensor decomposition significantly hinders the tensor fusion process and further limits the performance of link prediction. To overcome this limitation, we introduce a novel method that maps factor tensors onto a unified smooth Lie group manifold to make the distribution of factor tensors approximating homogeneous in tensor decomposition. We provide the theoretical proof of our motivation that homogeneous tensors are more effective than heterogeneous tensors in tensor fusion and approximating the target for tensor decomposition based TKGE methods. The proposed method can be directly integrated into existing tensor decomposition based TKGE methods without introducing extra parameters. Extensive experiments demonstrate the effectiveness of our method in mitigating the heterogeneity and in enhancing the tensor decomposition based TKGE models111Our code is available at https://github.com/dellixx/tkbc-lie.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Knowledge graphs (KGs) are data structures that encapsulate knowledge triples of real-world entities and their interrelationships, and are widely used to improve information retrieval Liang et al. (2023  ###reference_b17###), reasoning Xu et al. (2023  ###reference_b31###), Q&A Hu et al. (2021  ###reference_b7###), etc. Temporal knowledge graphs (TKGs) extend this paradigm by introducing timestamps into knowledge triplets to reflect the validity of facts over time and provide a deeper understanding and analysis of dynamic changes in the facts. Due to the data incompleteness in both KGs and TKGs, researchers propose many KG embedding (KGE) and TKG embedding (TKGE) methods to predict the missing facts, thereby enhancing the richness and accuracy of the KGs and TKGs. This work mainly focuses on TKGE.\n###figure_1### As the interest in TKG grows, researchers proposed many TKGE methods and greatly promoted the development of TKG.\nConcerning the success of tensor decomposition in KGE Nickel et al. (2016  ###reference_b21###); Trouillon et al. (2016  ###reference_b26###); Lacroix et al. (2018  ###reference_b11###), recent works Lacroix et al. (2020  ###reference_b10###); Xu et al. (2021  ###reference_b30###); Li et al. (2023  ###reference_b15###) further extended tensor decomposition into TKGE and obtained very excellent performance.\nThese works demonstrated that tensor decomposition can guarantee full expressiveness under specific embedding dimensionality bounds in TKG, thus enhancing the link prediction.\nHowever, existing TKGE methods based on tensor decomposition suffer from inherent heterogeneity among factor tensors. Recent research Wu et al. (2020  ###reference_b29###); Li et al. (2021  ###reference_b16###) also highlights the intrinsic heterogeneity in TKGs, specifically in terms of entity and temporal heterogeneity. According to our analysis, the heterogeneity of entity, relation and timestamp originates from their semantic roles within the knowledge graphs. That is, the entities represent the static components of the graph, the relations delineate the interactions among the entities, and the timestamp characterizes the temporal aspects of these interactions, specifying when they occur and their duration. This heterogeneity leads to the learned factor tensor expliciting different distributions in TKGE, as shown in Figure 1  ###reference_###(a). This further limits the tensor fusion in TKGE models and lowers the link prediction accuracy. More discussion about heterogeneity can be found in Appendix A  ###reference_###.\nTherefore, it is necessary to address the heterogeneity for tensor decomposition-based TKGE methods to enhance link prediction. To this target, we propose to map the factor tensors onto a unified smooth Lie group manifold to make the distribution of factor tensors approximating homogeneous in tensor decomposition, as shown in Figure 1  ###reference_###(b).\nSince the manifold in Lie group looks the same at every point and all tangent spaces at any point are alike Sol\u00e0 et al. (2018  ###reference_b23###), the factor tensors mapped by the Lie group have a smooth and unified distribution, which mitigates the heterogeneity among the factor tensors. We provide the theoretical proof of our motivation that homogeneous factor tensors are more effective in approximating the target compared to heterogeneous factor tensors in TKGE models in Sec. 4.1  ###reference_###. We integrate the proposed method into several existing tensor decomposition based TKGE models and conduct extensive experiments to evaluate its effectiveness. The experimental results present the heterogeneity among factor tensors in TKGE methods and illustrate that the proposed method brings significant performance improvement. This confirms the effectiveness of our method in alleviating the heterogeneity. Our contributions are summarized as follows:\nTo the best of our knowledge, we are the first to investigate the negative effect of the heterogeneity among the factor tensors for tensor decomposition based TKGE models and propose to enhance these models by diminishing the heterogeneity via Lie group manifold.\nWe provide the theoretical proof of our motivation that homogeneous factor tensors are more effective than heterogeneous factor tensors in approximating the target in TKGE.\nOur proposed method can be directly integrated into the tensor decomposition based TKGE models without introducing any additional parameters, and extensive experiments on several TKGE models demonstrate its effectiveness and generalization."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Static Knowledge Graph Embedding Drawing inspiration from the concept of translation invariance featured in word2vec Mikolov et al. (2013  ###reference_b20###), TransE assesses the relations between entities and their links by calculating the distance from  to  using standard  or  norms, where  and  are the vectors that represent the starting and ending entities, and  represents the linking relation. Following TransE, TransH Wang et al. (2014  ###reference_b28###), TransR Lin et al. (2015  ###reference_b18###), and TransD Ji et al. (2015  ###reference_b9###) introduce various mapping ways and thus refine these embeddings for better KGE representation. ComplEx Trouillon et al. (2016  ###reference_b26###) employs 3-th order tensor decomposition to capture the interactions within KGs. TorusE Ebisu and Ichise (2018  ###reference_b5###) utilizes a torus (a donut-shaped manifold) for its embeddings. TorusE introduces a torus, which is a compact Abelian Lie group, and defines distance functions on the torus. The torus can be considered as a collection of multiple Lie groups. Instead, we map the factor tensors to the Lie group space, thus mitigating the distributional heterogeneity among them.\nTemporal Knowledge Graph Embedding\nIn TKGE models, the temporal information is added, and the scoring function is calculated for the quadruples to assess their reasonableness. Therefore, most TKGE models use existing KGE models as a foundation. TTransE Leblay and Chekol (2018  ###reference_b13###) extends TransE and encodes time stamps  as translations same as relations. Hence, the score function of TTransE is denoted as . Furthermore, TA-TransE Garc\u00eda-Dur\u00e1n et al. (2018  ###reference_b6###) encode timestamps based on TransE. RotateQVS Chen et al. (2022  ###reference_b4###) uses quaternion embeddings to represent both entities and relations. Recently, BoxTE Messner et al. (2022  ###reference_b19###) models the TKGE based on a box embedding model BoxE Abboud et al. (2020  ###reference_b1###).\nTensor Decomposition Based Temporal Knowledge Graph Embedding\nTComplEx Lacroix et al. (2020  ###reference_b10###) and TNTComplEx Lacroix et al. (2020  ###reference_b10###) expand upon the ComplEx model by executing a fourth-order tensor decomposition in temporal knowledge graphs (TKGs). This method offers a more nuanced understanding of the temporal dimensions in knowledge graphs. TeLM Xu et al. (2021  ###reference_b30###) utilizes the asymmetric geometric product, a method that allows for a more sophisticated and expressive representation of temporal relationships and entities. TeAST Li et al. (2023  ###reference_b15###) maps relations onto Archimedean spiral timelines, and ensures that relations occurring simultaneously are placed on the same timeline, with all relations evolving over time. The above works are all based on tensor decomposition to optimize the TKGE representation. In this paper, we focus on exploring the problem of the heterogeneity of factor tensors in tensor decomposition based TKGE models."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Background and Notation",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "TKGE Task",
            "text": "Given a TKG, let  denote the set of entities,  denote the set of relations, and  denote the set of timestamps. A TKG can be defined as a collection of quadruples , where , ,  and  denote the subject entity, relation, object entity and timestamp, respectively. The TKGE task aims to accurately learn embedded representations of entities, relations and timestamps to facilitate predictions of missing entities in TKGs. Specifically, it involves predicting the object entity  given a tuple , or conversely, predicting the subject entity  for a tuple , thereby capturing the dynamic nature of relationships over time. In this paper, we denote the relation in TKG as  and the rank in mathematics as ."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Tensor Decomposition for TKGE",
            "text": "In the existing tensor decomposition based TKGE methods, each relation quadruple is represented by a -valued 4-th order tensor  Lacroix et al. (2020  ###reference_b10###). This representation allows each element  to indicate that at a specific time , there is a relationship  between entities  and . In link prediction, tensor decomposition algorithms learn to infer a predicted tensor  that approximates the ground truth , as\nwhere rank ,  denotes the tensor product.  is scaling factor. , ,  and  denote the subject entity, relation, object entity and timestamp factor tensors. Tensor decomposition based TKGE methods aim to optimize the factor tensors to make  as close as possible to the tensor  and thus achieve more accurate link prediction."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "This paper employs Lie group manifold to diminish the heterogeneity of factor tensors in tensor decomposition based TKGE models and thus improves the performance of these models. In Sec. 4.1  ###reference_###, we provide the theoretical proof of our motivation that homogeneous tensors are more effective than heterogeneous tensors in approximating the target for tensor decomposition based TKGE methods. In Sec. 4.2  ###reference_###, we explain why Lie groups can mitigate the heterogeneity among factor tensors, and describe how to map the factor tensors to the Lie group space.\nIn Sec. 4.3  ###reference_###, we introduce a Logarithmic Mapping \u2018\u2019 operation and alleviate the heterogeneity among factors by minimizing the difference between the original factor tensors and the mapped factor tensors in the Lie group through the N3 regularization in the loss function."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Theoretical Analysis of Homogeneous vs. Heterogeneous Factor Tensors",
            "text": "Proposition 1. Homogeneous factor tensors (, , , ) with a low rank can effectively approximated  while heterogeneous factor tensors (, , , ) require a higher rank to approximate  in TKGE.\n\nProof. Given a 4th-order tensor decomposition in TKGE\nwhere  is the rank of the decomposition,  is scaling factor, and  are factor tensors.\nIf the factor tensors (, , , ) are homogenous,\nthere are a common set of basis vectors  between these factor tensors. Based on this homogeneity, the column vectors of each factor matrix can be expressed as linear combinations of the basis vectors in .\nwhere , and , , , and  are the coefficient vectors for the -th component in their respective factor matrices.\nConsequently, we can get\n\nGiven the homogeneity among factor tensors, we can further obtain the representation as follows\n\nwhere , , , and  are the scalar coefficients corresponding to the projection of the -th component onto the -th basis vector in their respective factor matrices. The set of basis vectors  are orthogonal to each other, ensuring that each dimension represented by these vectors is independent. Both  and  can be considered as constants. Thus, we can get a reduced-rank tensor  as\nwhich captures the essence of the homogeneity within the factor matrices. The rank of , denoted by , is less than the original rank .\nThus, homogeneous factor tensors (, , , ) with a low rank can effectively approximate the target quadruple tensor .\nIn contrast, if the factor tensors (, , , ) are highly heterogeneous, they exhibit distinct semantics and distributions characteristics. This implies that these factor tensors cannot be effectively approximated by a simple, small number of rank-1 tensors. Each rank-1 tensor can be viewed as a representation of a specific pattern or feature.\nFor these heterogeneous tensors, the patterns they capture, or the semantics they represent within the data necessitate a larger number of rank-1 tensors to capture their diverse characteristics individually. Hence, the heterogeneous factor tensors (, , , ) require a higher rank or even full rank to approximate  in TKGE. Since higher rank means more parameters to estimate and more computation, homogeneous factor tensors are more effective than heterogeneous factor tensors in approximating the target for tensor decomposition based TKGE methods.\n###figure_2###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Mitigating Heterogeneity via Lie Group",
            "text": "In TKGE, the underlying reason why highly heterogeneous factor tensors require a higher rank to approximate the target vector is that the heterogeneity among factor tensors can limit the fusion process of subsequent computations, which can be analogous to the multimodal fusion process Chen and Zhang (2020  ###reference_b3###). Therefore, it is crucial to mitigate the heterogeneity among factor tensors to approximate the target tensor more efficiently. Our motivations for choosing Lie groups to mitigate heterogeneity in TKGE are as follows.\nFirstly, Lie groups are adept at maintaining structural integrity and handling data\u2019s dynamic nature over time, making them a suitable choice for TKGE. The application of Lie groups in fields like robotics Sol\u00e0 et al. (2018  ###reference_b23###), machine learning Sommer et al. (2020  ###reference_b24###); Lee and Civera (2022  ###reference_b14###), and computer vision Teed and Deng (2021  ###reference_b25###) underscores its capability to model complex geometric transformations effectively. Secondly, as shown in Figure 2  ###reference_###, Lie group is a mathematical structure that simultaneously satisfies the axioms of a group and the properties of a smooth manifold. It is like a curved, smooth hyper-surface, with no edges or spikes, embedded in a space of higher dimension. The smoothness of the manifold implies the existence of a unique tangent space at each point. In a Lie group, the manifold looks the same at every point, and therefore all tangent spaces at any point are alike. Thus, the factor tensors mapped by the Lie group have a smooth and unified distribution, which further mitigates the heterogeneity among the factor tensors.\nIn this study, we map the factor tensors to the same Lie group space and make the factor tensors have a unified distribution to mitigate the heterogeneity among them. To facilitate the description of our method, we employ factor tensor of rank 4 in the subsequent discussions.\nGiven a factor tensor  of rank 4 and map it to the Lie group  space, we get\nwhere  denots Lie group mapping operation.  is a rotation matrix in , denoted as\n\nAccordingly, given a quadruple (, , , ) in TKG, its corresponding factor tensors are (, , , ) in the tensor decomposition based TKGE models. We map these four factor tensors onto Lie group space, and we get the rotation matrices\n\nWhen generalized to  rank,  can be denoted as a  rotation matrix. In an -dimensional space, a Givens rotation is performed by fixing  dimensions and applying a rotation transformation within the plane formed by the remaining two dimensions. This effectively isolates the rotation to a specific two-dimensional subspace. The Givens rotation matrix  for rotating the -th and -th coordinates in -dimensional space is given by:\nwhere the rotation occurs in the plane spanned by the -th and -th basis vectors. The matrix is an identity matrix except for the four elements , , , and , which form the  rotation block within the larger matrix. Hence, we focus on the  base rotation matrices in the code implementation."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": ""
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Datasets",
            "text": "To evaluate the effectiveness of the proposed method, we evaluate our method on two popular TKGE benchmark datasets. ICEWS14 and ICEWS05-15 Garc\u00eda-Dur\u00e1n et al. (2018  ###reference_b6###) are both extracted from the Integrated Crisis Early Warning System (ICEWS) dataset Lautenschlager et al. (2015  ###reference_b12###), which consists of temporal sociopolitical facts starting from 1995. ICEWS14 consists of sociopolitical events in 2014 and ICEWS05-15 involves events occurring from 2005 to 2015. ICEWS14 is a fine temporal granularity dataset, while ICEWS05-15 has a wider temporal granularity relative to ICEWS14. See Appendix B  ###reference_### for summary statistics of the dataset and more discussion of the dataset."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Evaluation Protocol",
            "text": "In this research, we follow the previous works Lacroix et al. (2020  ###reference_b10###); Xu et al. (2021  ###reference_b30###); Li et al. (2023  ###reference_b15###) to evaluate our method. Specifically, to evaluate the quality of the ranking for each test quadruples, we calculate all possible substitutions for the subject and object entities, denoted as  and , where , . After that, we sort the score of candidate quadruples under the time-wise filtered settings Lacroix et al. (2020  ###reference_b10###); Xu et al. (2021  ###reference_b30###); Li et al. (2023  ###reference_b15###). The performance is evaluated using standard evaluation metrics, including Mean Reciprocal Rank (MRR) and Hits@. The Hits@ metric measures the percentage of correct entities in the top  predictions. Higher values of MRR and Hits@ indicate better performance. Hits ratio with cut-off values . In this paper, we utilize H@ to denote Hits@ for convenience."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Experimental Setup",
            "text": "We implement our method based on the existing training framework222https://github.com/facebookresearch/tkbc  ###reference_###. All experiments are trained on a single NVIDIA Tesla A100. The hyperparameters used in the experiment are consistent with the optimal hyperparameters of the original paper report. The best models are selected by early stopping (threshold of 10) on the validation datasets. The max epoch is 200. We report the average results on the test set for five runs. To ensure a fair validation of the effectiveness of our method, we employ the same hyperparameter configuration in both the before and after comparison experiments.\nAccording to Lie group mapping described in Sec. 4.2  ###reference_###, it is essential to ensure that the rank  of the matrix can be satisfied by the square root of  is an integer in our method. This requirement arises from the implementation of the matrix logarithm map for TcomplEx, TNTcomplEx and TeAST. The rank of TeLM needs to be satisfied by the square root of  is an integer."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results and Analysis",
            "text": ""
        },
        {
            "section_id": "6.1",
            "parent_section_id": "6",
            "section_name": "Main Results",
            "text": "In our experiments, we validate the effectiveness of our proposed method for dealing with TKGs heterogeneity in tensor decomposition on ICEWS14 and ICEWS05-15 datasets. The improvements are marked in red in Table 1  ###reference_###, highlighting the advancements over the baselines. When our method is applied to different tensor decomposition based TKGE models, they all achieve meaningful improvements in different metrics. This significant improvement confirms our Proposition 1, in which homogeneous factor tensors can be effectively approximated  with a low rank. Additionally, the ICEWS05-15 dataset validates the effectiveness of our method in mitigating data heterogeneity, with TeAST notably exhibiting an average improvement of 10.3 points.\nIn conclusion, the experimental results provide robust evidence supporting Proposition 1. The experimental outcomes validate the theoretical framework of our study and demonstrate that our novel method effectively alleviates data heterogeneity in tensor decomposition and enhances the link prediction performance of these models. More experiments on the large TKG dataset GDELT can be found in Appendix C  ###reference_###."
        },
        {
            "section_id": "6.2",
            "parent_section_id": "6",
            "section_name": "Quantitative Analysis on Heterogeneity",
            "text": "In this section, we perform a quantitative analysis to prove the effectiveness of our proposed method. For any factor tensors  and , the skew-symmetric matrices in  are given by\nWe define the difference between  and  in  to be denoted as .\nThe relationship between the set of skew-symmetric matrices  obtained from the mapping of a set of vectors  can be described using the operations in the Lie algebra, such as computing their Lie brackets. This process of mapping vectors to  and then to  transforms them into elements with a unified algebraic structure, mitigating the differences in structural distribution between them.\nTo quantify the structural differences between the skew-symmetric matrices  and  in , we consider the Frobenius norm of their difference\n\nThis norm provides a measure of the difference between the corresponding matrices.\nBased on the above quantitative formulas, we evaluate on the representative models TComplEx and TNTComplEx. Specifically, we calculate the difference between entity and relation, entity and timestamp, and relation and timestamp. As shown in Table 2  ###reference_###, we calculate their average distance difference on ICEWS14.\nAs shown in Table 2  ###reference_###, for the standard TComplEx model, the average difference in distance between entities and relations is 15.71, between entities and timestamps is 7.61, and between relations and timestamps is 15.72. These results demonstrate significant differences in quantification between different types of embeddings.\nFurther, when our method  is employed in the TComplEx model, we observe a reduction in all three distance difference. This significant improvement points to the effectiveness of our method in mitigating the heterogeneity among different factor tensors. Similarly, the results of the TNTComplEx model support the above statement."
        },
        {
            "section_id": "6.3",
            "parent_section_id": "6",
            "section_name": "Visualisation Analysis",
            "text": "To further verify that our method can effectively mitigate the heterogeneity between factor embeddings, we utilize t-SNE Van der Maaten and Hinton (2008  ###reference_b27###) to visualize the learned entity, relation, and timestamp embeddings. As shown in Figure 3  ###reference_###, we can observe that the learned factor embeddings through our method exhibit a trend towards homogeneity. This further demonstrates the inherent heterogeneity present among different types of embeddings in TKGE based on tensor decomposition. Our method effectively mitigates this issue, demonstrating that meaningful performance improvements can be achieved.\n###figure_3### ###figure_4### ###figure_5### ###figure_6###"
        },
        {
            "section_id": "6.4",
            "parent_section_id": "6",
            "section_name": "Effect of Rank",
            "text": "In this work, we compare the performance of the standard TeAST Li et al. (2023  ###reference_b15###) model with our proposed TeAST model enhanced by  across different rank values on ICEWS14. As shown in Figure 4  ###reference_###, we observe that the performance of both models improves with the increase of rank values. However, after the rank value reaches 800, the pace of performance improvement slows down. This is because the representation capacity of the model reaches saturation at a certain level, beyond which the marginal benefits of increasing rank diminish. Additionally, higher rank values might lead to overfitting, especially in cases of sparse data, negatively affecting the model\u2019s generalization ability on unseen data.\n###figure_7### ###figure_8### ###figure_9### ###figure_10###"
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we are the first to introduce methods to mitigate heterogeneity in factor tensors within tensor decomposition-based TKGE models. We reveal that the heterogeneity primarily stems from diverse semantic content among elements (entities, relation and timestamps), which impedes the effective fusion of factor tensors and limits link prediction accuracy. We prove that homogeneous tensors are more effective than heterogeneous tensors in tensor fusion and approximating the target for tensor decomposition based TKGE methods. Our method maps factor tensors onto a smooth Lie group manifold to standardize their distribution and mitigate heterogeneity without increasing model complexity. Our experimental results demonstrate the effectiveness of this method in mitigating tensor heterogeneity and enhancing performance. We hope that this work can offer fresh insights for research in the field of TKGE."
        }
    ],
    "url": "http://arxiv.org/html/2404.09155v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5.1",
            "5.2",
            "5.3",
            "6.1",
            "6.2",
            "6.3",
            "6.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "6.1",
            "6.2",
            "6.4"
        ]
    },
    "research_context": {
        "paper_id": "2404.09155v1",
        "paper_title": "Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding",
        "research_background": "### Paper's Motivation:\n\nThe motivation behind this paper stems from the recognition that Temporal Knowledge Graph Embedding (TKGE) methods encounter significant challenges due to inherent heterogeneity among factor tensors derived from the inherent roles of entities, relations, and timestamps within Knowledge Graphs (KGs) and Temporal Knowledge Graphs (TKGs). The paper highlights that this heterogeneity negatively impacts tensor decomposition models used in TKGE, leading to reduced accuracy in link prediction tasks. The authors aim to address this issue by proposing a novel approach that maps these heterogeneous factor tensors onto a unified smooth Lie group manifold to homogenize their distributions, thereby enhancing the overall performance of TKGE models.\n\n### Research Problem:\n\nThe primary research problem addressed in this paper is the adverse effect of heterogeneity among factor tensors in tensor decomposition-based TKGE methods. Specifically, the problem arises because the different semantic roles of entities (static components), relations (interactions), and timestamps (temporal aspects) within TKGs lead to varied distributions of factor tensors, which hampers the tensor fusion process and decreases link prediction accuracy. The authors propose a solution to this problem by utilizing Lie group manifolds to map the factor tensors onto a homogeneous distribution.\n\n### Relevant Prior Work:\n\n1. **Knowledge Graph Embedding (KGE) and TKG Embedding (TKGE)**:\n    - Researchers have developed numerous KGE and TKGE methods to predict missing facts and improve the completion and accuracy of KGs and TKGs (Liang et al., 2023; Xu et al., 2023; Hu et al., 2021).\n\n2. **Tensor Decomposition in KGE and TKGE**:\n    - Tensor decomposition techniques have shown great success in KGE (Nickel et al., 2016; Trouillon et al., 2016; Lacroix et al., 2018).\n    - Recent works extended tensor decomposition methods to TKGE, achieving excellent performance (Lacroix et al., 2020; Xu et al., 2021; Li et al., 2023).\n\n3. **Heterogeneity in TKGs**:\n    - It has been highlighted that intrinsic heterogeneity, particularly in terms of entity and temporal aspects, exists in TKGs and negatively affects performance (Wu et al., 2020; Li et al., 2021).\n\nBy reviewing these prior studies, the authors provide a comprehensive backdrop for their claim that the heterogeneity among factor tensors is a significant issue in current TKGE models, necessitating the development of a method to homogenize these distributions to improve model efficacy.",
        "methodology": "**Methodology:**\n\nThis paper employs **Lie group manifold** to diminish the heterogeneity of factor tensors in tensor decomposition-based Temporal Knowledge Graph Embedding (TKGE) models, thereby enhancing their performance. Here's a breakdown of the proposed method and its key components:\n\n1. **Theoretical Proof of Motivation (Sec. 4.1 ###reference_###):**\n   - The paper begins by establishing a theoretical basis for its method, demonstrating that homogeneous tensors are inherently more effective than heterogeneous tensors for approximating targets in tensor decomposition-based TKGE methods.\n\n2. **Lie Groups for Mitigating Heterogeneity (Sec. 4.2 ###reference_###):**\n   - The next section outlines why and how Lie groups are effective in addressing the heterogeneity among factor tensors. Leverage the properties of Lie groups to create a mapping from the original tensor space to a Lie group space, which inherently diminishes heterogeneity.\n\n3. **Logarithmic Mapping Operation (Sec. 4.3 ###reference_###):**\n   - The method introduces a \u2018Logarithmic Mapping\u2019 operation. This operation involves minimizing the difference between the original factor tensors and the tensors mapped to the Lie group space through logarithmic mapping.\n   - This minimization process is incorporated into the model's loss function using **N3 regularization** to effectively reduce heterogeneity among the factor tensors.\n\nThese components collectively form a novel approach to tensor decomposition in TKGE models, incorporating advanced mathematical structures (Lie groups) and custom regularization techniques to improve model performance by addressing the heterogeneity issue.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n#### Experiment Setup:\n- **Datasets:**\n  - **ICEWS14:** Contains sociopolitical events from the year 2014.\n  - **ICEWS05-15:** Includes events occurring between 2005 and 2015.\n  Both datasets are derived from the Integrated Crisis Early Warning System (ICEWS).\n\n#### Evaluation:\n- **Baselines:** \n  - Not explicitly mentioned here, but typical baselines for TKGE tasks often include methods such as TransE, DistMult, or more advanced temporal knowledge graph embeddings like TComplEx or HyTE.\n  \n- **Evaluation Metrics:**\n  - The metrics commonly used in such experiments include Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@N (for example, Hits@1, Hits@3, Hits@10).\n\n#### Main Experimental Results:\n- The results section would generally present a comparative analysis of the proposed method against the baselines on the given datasets, demonstrating the effectiveness of the proposed Lie group manifold approach in handling heterogeneity in factor tensors for TKGE. Specific numerical results and possibly tables or graphs would highlight improvements in metrics such as MRR and Hits@N, showcasing superior performance on ICEWS14 and ICEWS05-15 datasets.\n\nThe detailed summary statistics and additional details regarding the datasets can be found in Appendix B of the paper."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To validate the effectiveness of the proposed method for dealing with TKGs heterogeneity in tensor decomposition and to demonstrate improvements in link prediction performance.",
            "experiment_process": "The method was validated on ICEWS14 and ICEWS05-15 datasets. The application of the method was extended to different tensor decomposition based TKGE models. Various metrics were used to measure improvements, with specific attention to the low-rank approximation of homogeneous factor tensors. The method's effectiveness was further validated using the ICEWS05-15 dataset.",
            "result_discussion": "The experimental results showed significant improvements in metrics when the method was applied, with notable advancements marked in red. The ICEWS05-15 dataset validation showed an average improvement of 10.3 points with the TeAST model. These results confirm the theoretical framework and effective mitigation of data heterogeneity, enhancing the link prediction performance of the models.",
            "ablation_id": "2404.09155v1.No1"
        },
        {
            "research_objective": "To perform a quantitative analysis to prove the effectiveness of the proposed method in mitigating structural heterogeneity among factor tensors.",
            "experiment_process": "Skew-symmetric matrices for factor tensors and their differences were computed using Frobenius norm. The distances between different types of factor tensors (entity-relation, entity-timestamp, relation-timestamp) were calculated on the ICEWS14 dataset using the representative models TComplEx and TNTComplEx. Results before and after applying the proposed method were compared.",
            "result_discussion": "For the standard TComplEx model, average distance differences were 15.71 (entity-relation), 7.61 (entity-timestamp), and 15.72 (relation-timestamp). After applying the proposed method, significant reductions in all three distance differences were observed, demonstrating the method's effectiveness. Similar reductions were observed for the TNTComplEx model.",
            "ablation_id": "2404.09155v1.No2"
        },
        {
            "research_objective": "To compare the performance of the TeAST model with and without the proposed method across different rank values.",
            "experiment_process": "The standard TeAST model and the enhanced version with the proposed method were tested across different rank values on the ICEWS14 dataset. Performance improvements were observed and analyzed up to a rank value of 800.",
            "result_discussion": "Both models' performance improved with increasing rank values, but beyond rank 800, the improvement pace slowed due to model representation capacity saturation. Higher ranks eventually caused overfitting, affecting the model's generalization on unseen data.",
            "ablation_id": "2404.09155v1.No3"
        }
    ]
}