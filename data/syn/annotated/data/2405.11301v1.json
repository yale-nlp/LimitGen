{
    "title": "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models",
    "abstract": "Fine-grained image classification, particularly in zero/few-shot scenarios, presents a significant challenge for vision-language models (VLMs), such as CLIP. These models often struggle with the nuanced task of distinguishing between semantically similar classes due to limitations in their pre-trained recipe, which lacks supervision signals for fine-grained categorization. This paper introduces CascadeVLM, an innovative framework that overcomes the constraints of previous CLIP-based methods by effectively leveraging the granular knowledge encapsulated within large vision-language models (LVLMs).\nExperiments across various fine-grained image datasets demonstrate that CascadeVLM significantly outperforms existing models, specifically on the Stanford Cars dataset, achieving an impressive 85.6% zero-shot accuracy.\nPerformance gain analysis validates that LVLMs produce more accurate predictions for challenging images that CLIPs are uncertain about, bringing the overall accuracy boost. Our framework sheds light on a holistic integration of VLMs and LVLMs for effective and efficient fine-grained image classification.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The vision-language model (VLM) landscape is evolving, highlighted by models like CLIP (Radford et al., 2021  ###reference_b37###) and its significant zero/few-shot classification capabilities (Zhou et al., 2022b  ###reference_b56###). Despite these advances, fine-grained image classification remains challenging, particularly in distinguishing closely related sub-classes (Ren et al., 2023a  ###reference_b38###), such as the watercress and the wallflower in Figure 1  ###reference_###, due to the subtle visual differences between classes.\nTo enhance the zero/few-shot fine-grained classification performance of CLIP, recent methods have concentrated on advanced prompt engineering (Zhou et al., 2021  ###reference_b54###, 2022a  ###reference_b55###) and the enhancement of pre-training supervision (Li et al., 2023b  ###reference_b30###; Singh et al., 2023  ###reference_b43###). Additionally, to enrich CLIP\u2019s prompt context, Menon and Vondrick (2022  ###reference_b33###) introduced a technique whereby GPT3 generates detailed class descriptions for enhancing CLIP models.\nHowever,\nthis method cannot handle visually similar classes as the generated descriptions tend to be nearly identical, offering limited benefits for fine-grained classification tasks.\n###figure_1### In this paper, we instead resort to large vision language models (LVLMs), to better capitalize on their extensive world knowledge.\nA straightforward way is to directly concatenate all the candidate classes as a prompt for LVLMs. However, the limited capacity for long-context modeling (Zhao et al., 2023  ###reference_b52###) poses challenges, particularly evident when LVLMs grapple with a large candidate image class set. As shown in Figure 1  ###reference_###, an advanced open-sourced LVLM, Qwen-VL (Bai et al., 2023  ###reference_b3###), still suffers from a dramatic accuracy decline when the candidate categories increased from 5 to 102.\nTo tackle this, we employ CLIP models for locating potential candidate classes to ease the burden for LVLMs.\nDue to the contrastive pre-training objective, CLIP is capable of finding a set of possible classes, evidenced by the relatively higher top-K prediction accuracy.\nFor example, in the Flowers102 dataset, CLIP(ViT-B/32) gives a 68.7% Top-1 accuracy, yet the Top-10 accuracy is significantly boosted to 89.9%. This characteristic validates our motivation to holistically integrate CLIP and LVLM.\nMotivated by our previous exploration, in this paper, we introduce CascadeVLM, which integrates the complementary capabilities of CLIP-like models and LVLMs to perform fine-grained image classification. The key idea is to leverage the CLIP-like models as a class filter for LVLMs to fulfill the LVLM potentials.\nBesides, the results can be further enhanced by leveraging the in-context learning (Dong et al., 2022  ###reference_b13###) of LVLMs to perform few-shot learning. Moreover, the overall inference efficiency can be improved by adopting an entropy threshold as a heuristic mechanism to evaluate the necessity of deploying LVLMs, achieving a dynamic early exiting (Xin et al., 2020  ###reference_b49###; Li et al., 2021b  ###reference_b25###).\nWe conducted zero/few-shot experiments across various fine-grained image datasets, and the evaluation results demonstrate that CascadeVLM surpasses other individual models in performance. For instance, CascadeVLM achieved an 85.6% accuracy rate on the Stanford Cars dataset. Additionally, we utilized two advanced pretrained CLIP-like models as backbones for the iNaturalist and SUN397 datasets, achieving superior results compared to the individual backbone models.\nFurthermore, our analysis meticulously dissects CascadeVLM\u2019s performance improvements, spotlighting the model\u2019s sophisticated handling of entropy thresholds to balance computational demands with accuracy. This dual-focused investigation not only clarifies the operational dynamics of CascadeVLM but also its adeptness in integrating with CLIP-like architectures and LVLM for refined final predictions, underlining the model\u2019s comprehensive adaptability and strategic efficiency.\nIn summary, the contributions of this paper could be summarized as follows:\n(1) We propose the CascadeVLM framework which effectively combines CLIP-like VLMs and LVLMs for zero/few-shot fine-grained image classification.\n(2) Experiment on six relevant datasets reveals that CascadeVLM achieves performance on par with advanced zero/few-shot methods. Our analysis provides insights into the holistic integration of VLMs and LVLMs\n###figure_2###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "In this section, we delineate the methodology underpinning our CascadeVLM framework, which is structured into two steps. (1) The first step involves candidate selection facilitated by the CLIP model. (2) The second step encompasses the application of zero-shot or few-shot prediction techniques using large vision-language models (LVLMs). For zero-shot prediction, LVLMs directly engage in classification based on these pre-selected candidates. For few-shot situations, we use the in-context learning strategy to augment the semantic context.\nZero-shot learning Socher et al. (2013  ###reference_b44###) enables models to predict unseen classes without specific training examples, leveraging pre-existing knowledge from broader contexts or related tasks. This method is particularly beneficial in data-scarce scenarios, where it effectively infers new categories despite limited training data.\nIn the context of our CascadeVLM framework, zero-shot prediction is executed after identifying the top- candidate classes using CLIP. The LVLM then selects one candidate , as the final prediction. Here, we generalize the process of LVLM prediction as function , given the input image  and the top- candidate set :\nThe zero-shot prediction phase in our CascadeVLM framework highlights LVLMs\u2019 proficiency in utilizing pre-trained knowledge for unseen data while adeptly managing contextual complexities. Focusing on a select set of candidates, the proposed methods coude effectively address the intricacies of fine-grained classification, ensuring precise and dependable outcomes even without class-specific examples.\nIn the Few-Shot Prediction phase of our CascadeVLM framework, we capitalizes on LVLMs\u2019 in-context learning Brown et al. (2020  ###reference_b6###) ability, where additional relevant samples significantly enhance performance, allowing LVLMs to deepen their understanding and improve predictive accuracy.\nIn the integration of few-shot learning within our cascade framework, we undertake a two-step process for candidate categories set :\nStep 1: Context Generation:\nIn this initial phase, for each category  in , we randomly select an example image  from the training dataset, and design a prompt to contextualize the input image  for the LVLMs. Here, each candidate class  and its corresponding example image  are integrated with the prompt template, creating a contextual framework for the LVLMs. We denote this assemblage of prompts and images form the contextual basis as  in the subsequent step of our methodology. For instance, within the context of the GPT4-V scenario, the contextual basis denoted as  is formulated in Table 1  ###reference_###.\nStep 2 - Prediction with Contextual Information:\nIn this step, the context  is integrated with the input image  and fed into the LVLMs. This integration enables the LVLMs to utilize the rich contextual information embedded in  to enhance and refine its predictive process for the image . Consequently, the final classification outcome, denoted as , emerges from this enriched inferential framework. The process can be mathematically represented as:\nwhere  represents the LVLM prediction based on provided image , the top- candidate set  and the context set ."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "LVLMs Prediction with Candidate Set",
            "text": "In this section, we seek to leverage Large Vision-Language Models (LVLMs) for the final classification within our CascadeVLM framework. Capitalizing on a subset of candidates pre-selected by CLIP, LVLMs overcome the challenge of extensive context and improve prediction accuracy through adaptable zero-shot and few-shot learning strategies, tailored to the data-sparse environments.\nZero-shot learning Socher et al. (2013  ###reference_b44###  ###reference_b44###) enables models to predict unseen classes without specific training examples, leveraging pre-existing knowledge from broader contexts or related tasks. This method is particularly beneficial in data-scarce scenarios, where it effectively infers new categories despite limited training data.\nIn the context of our CascadeVLM framework, zero-shot prediction is executed after identifying the top- candidate classes using CLIP. The LVLM then selects one candidate , as the final prediction. Here, we generalize the process of LVLM prediction as function , given the input image  and the top- candidate set :\nThe zero-shot prediction phase in our CascadeVLM framework highlights LVLMs\u2019 proficiency in utilizing pre-trained knowledge for unseen data while adeptly managing contextual complexities. Focusing on a select set of candidates, the proposed methods coude effectively address the intricacies of fine-grained classification, ensuring precise and dependable outcomes even without class-specific examples.\nIn the Few-Shot Prediction phase of our CascadeVLM framework, we capitalizes on LVLMs\u2019 in-context learning Brown et al. (2020  ###reference_b6###  ###reference_b6###) ability, where additional relevant samples significantly enhance performance, allowing LVLMs to deepen their understanding and improve predictive accuracy.\nIn the integration of few-shot learning within our cascade framework, we undertake a two-step process for candidate categories set :\nStep 1: Context Generation:\nIn this initial phase, for each category  in , we randomly select an example image  from the training dataset, and design a prompt to contextualize the input image  for the LVLMs. Here, each candidate class  and its corresponding example image  are integrated with the prompt template, creating a contextual framework for the LVLMs. We denote this assemblage of prompts and images form the contextual basis as  in the subsequent step of our methodology. For instance, within the context of the GPT4-V scenario, the contextual basis denoted as  is formulated in Table 1  ###reference_###  ###reference_###.\nStep 2 - Prediction with Contextual Information:\nIn this step, the context  is integrated with the input image  and fed into the LVLMs. This integration enables the LVLMs to utilize the rich contextual information embedded in  to enhance and refine its predictive process for the image . Consequently, the final classification outcome, denoted as , emerges from this enriched inferential framework. The process can be mathematically represented as:\nwhere  represents the LVLM prediction based on provided image , the top- candidate set  and the context set ."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Speed-up via Adaptive Entropy Threshold",
            "text": "In our CascadeVLM framework, we introduce an adaptive entropy-based approach aimed at enhancing inference speed, reducing the computational load on LVLMs, and accelerating overall throughput. The entropy  of the probability distribution, a measure of uncertainty or predictability within the distribution, is calculated as follows:\nThis computation serves as a critical decision point in our methodology. If the calculated entropy  falls below a predefined threshold, it signifies a high confidence level in the top-1 category as determined by CLIP. In such cases, we expedite the process by directly outputting this top-1 category, thereby bypassing the need for further LVLM processing. Conversely, if  exceeds the threshold, indicating a lower level of confidence and greater uncertainty, we proceed to the subsequent steps involving LVLMs for refined classification. This adaptive mechanism effectively balances speed and accuracy, streamlining the framework while ensuring reliable classification outcomes."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we rigorously evaluate the performance of our CascadeVLM framework across diverse benchmarks. Initially, we detail the experimental setup in Section 3.1  ###reference_###, followed by an in-depth analysis of the framework\u2019s efficacy in zero-shot learning scenarios in Section 3.2  ###reference_###, and subsequently in few-shot learning contexts in Section 3.3  ###reference_###.\n###table_1### For experimental evaluation, we employed various CLIP models in combination with specific Large Vision-Language Models (LVLMs). The experiments utilized one of the CLIP variants CLIP ViT-B/32, ViT-B/16, or ViT-L/14 alongside either Qwen-VL-Chat (Bai et al., 2023  ###reference_b3###) or GPT-4V (OpenAI, 2023  ###reference_b36###) as the LVLM. Additionally, we explore the framework\u2019s adaptability by integrating it with two robust CLIP-like models, MAWS-CLIP (Singh et al., 2023  ###reference_b43###) and CLIP (ViT-G/14) pre-trained with OpenCLIP (Ilharco et al., 2021  ###reference_b19###) using the Laion2B (Schuhmann et al., 2022  ###reference_b41###) dataset, as backbones cascading with GPT-4V.\nWe utilize a collection of datasets each offering unique characteristics and significance for fine-grained image classification, as summarized in Table 2  ###reference_###. These datasets include Flowers102 (Nilsback and Zisserman, 2008  ###reference_b34###), StanfordCars (Krause et al., 2013  ###reference_b21###), FGVC Aircraft (Maji et al., 2013  ###reference_b32###), BirdSnap (Berg et al., 2014  ###reference_b5###), SUN39 (Xiao et al., 2010  ###reference_b48###), and iNaturalist(2018) (Horn et al., 2018  ###reference_b18###), collectively encompassing a wide range of categories.\nIn our experimental evaluation, we benchmark the performance of various CLIP variants both individually and within our cascade framework. This comparative analysis allows us to demonstrate the enhancement in classification accuracy achieved by integrating these models into the CascadeVLM framework. Moreover, we extend our comparison to include diverse methodologies, providing a comprehensive evaluation of our proposed approach."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Experimental Settings",
            "text": "For experimental evaluation, we employed various CLIP models in combination with specific Large Vision-Language Models (LVLMs). The experiments utilized one of the CLIP variants CLIP ViT-B/32, ViT-B/16, or ViT-L/14 alongside either Qwen-VL-Chat (Bai et al., 2023  ###reference_b3###  ###reference_b3###) or GPT-4V (OpenAI, 2023  ###reference_b36###  ###reference_b36###) as the LVLM. Additionally, we explore the framework\u2019s adaptability by integrating it with two robust CLIP-like models, MAWS-CLIP (Singh et al., 2023  ###reference_b43###  ###reference_b43###) and CLIP (ViT-G/14) pre-trained with OpenCLIP (Ilharco et al., 2021  ###reference_b19###  ###reference_b19###) using the Laion2B (Schuhmann et al., 2022  ###reference_b41###  ###reference_b41###) dataset, as backbones cascading with GPT-4V.\nWe utilize a collection of datasets each offering unique characteristics and significance for fine-grained image classification, as summarized in Table 2  ###reference_###  ###reference_###. These datasets include Flowers102 (Nilsback and Zisserman, 2008  ###reference_b34###  ###reference_b34###), StanfordCars (Krause et al., 2013  ###reference_b21###  ###reference_b21###), FGVC Aircraft (Maji et al., 2013  ###reference_b32###  ###reference_b32###), BirdSnap (Berg et al., 2014  ###reference_b5###  ###reference_b5###), SUN39 (Xiao et al., 2010  ###reference_b48###  ###reference_b48###), and iNaturalist(2018) (Horn et al., 2018  ###reference_b18###  ###reference_b18###), collectively encompassing a wide range of categories.\nIn our experimental evaluation, we benchmark the performance of various CLIP variants both individually and within our cascade framework. This comparative analysis allows us to demonstrate the enhancement in classification accuracy achieved by integrating these models into the CascadeVLM framework. Moreover, we extend our comparison to include diverse methodologies, providing a comprehensive evaluation of our proposed approach."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Zero-shot Learning Results",
            "text": "Table 3  ###reference_### showcases the zero-shot prediction capabilities of CascadeVLM, demonstrating its superior performance across several benchmarks. Remarkably, without necessitating any training, CascadeVLM outperforms established methods such as CoOp, CoCoOp, and POMP. Moreover, it is comparable to and often outperforms the more strongly supervised method FLIP, which leverages fine-grained supervision for training. Given the flexibility of CascadeVLM, there is potential for further accuracy enhancements by employing FLIP as the underlying architecture. Consequently, we propose an experimental framework that integrates two advanced pre trained CLIP-like models as the backbone to explore this potential.\nTable 4  ###reference_### presents the results of experiments that applied two advanced CLIP-like models, MAWS CLIP and OpenCLIP (ViT-G/14), cascading to GPT-4V to challenging datasets: iNaturalist and SUN397, with random samples of 500 from each due to budget and rate limit constraints. The imbalanced nature of iNaturalist typically poses challenges for CLIP models. However, the findings reveal significant performance enhancements when utilizing a cascade framework. Specifically, MAWS CLIP achieved improvements of 6.8% on iNaturalist and 3.6% on SUN397. Similarly, OpenCLIP\u2019s performance increased by 5.2% on iNaturalist and 3.2% on SUN397. These improvements highlight the cascade framework\u2019s adaptability and effectiveness in enhancing the capabilities of CLIP-like models."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Few-shot Learning Results",
            "text": "In our initial explorations, we assessed Qwen-VL\u2019s capacity for few-shot learning within fine-grained image classification domains. However, it became apparent that Qwen-VL struggled to optimally utilize in-context demonstrations and instructions in this setting. Consequently, we turned our focus to GPT-4V, anticipating its better alignment with our framework\u2019s requirements.\nOur experiments with GPT-4V were limited to a random subset of 200 samples per dataset due to budget and rate limit constraints. Here we utilize top-k (k=5) for few-shot experiments and yield even more pronounced improvements in predictive accuracy. For instance, with few-shot learning applied, the Flower102 dataset achieved an impressive 94.5% accuracy, while the StanfordCars dataset attained 88.5%. These results not only reaffirm the effectiveness of our cascade framework but also highlight its adaptability and efficiency in leveraging few-shot learning for fine-grained classification tasks."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Analysis",
            "text": "In this section, we explore various aspects of CascadeVLM, highlighting the underlying reasons for its enhanced performance, the trade-off of the entropy threshold, and more. More investigations can be found in the Appendix B  ###reference_###.\n###figure_3###"
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Performance Gain Analysis",
            "text": "This analysis seeks to demonstrate why cascading CLIP with an LVLM model leads to improved accuracy in classification tasks. Using the Flowers102 dataset as a case study, we assess the performance of CLIP and the enhancement brought by LVLM. The margin (Settles, 2009  ###reference_b42###), i.e., the difference between the top1 and top2 probability scores from CLIP, serves as an indicator of the model\u2019s certainty about its prediction, where smaller margins suggest greater ambiguity in the image classification.\nWe divide the range of margins into five intervals, from 0 to 1, to analyze the effects systematically. The data reveals that GPT-4V, representing LVLM, significantly outperforms CLIP with margins less than 0.4, where CLIP experiences confusion.\nThis is evident in the consistently high ACC for GPT-4V in these instances. When the margin exceeds 0.6, the ACC for CLIP improves, indicating that the model is more confident and accurate in its predictions, thus reducing the gap in performance between CLIP and LVLM. The accompanying Figure 3  ###reference_### illustrates this trend, with the ACC gap decreasing sharply as the margin increases. This pattern suggests that while LVLM provides a significant advantage in cases of high ambiguity, the benefit tapers off as CLIP\u2019s confidence in its classifications rises.\n###figure_4###"
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Sensitivity of Option Orders",
            "text": "As delineated in Tables 3  ###reference_### and 5  ###reference_###, we find, surprisingly, that the arrangement of options provided by CLIP plays a pivotal role in the efficacy of Language-Vision Language Models (LVLMs). While it may seem a minor detail to supply LVLMs with the entire class set from CLIP, this procedure is significantly impactful. For example, as shown in Table 3  ###reference_###, presenting all classes in a random order to Qwen results in an average accuracy of only 17.6%. Conversely, when the classes are organized according to the probabilities assigned by CLIP, there is a notable enhancement in performance. Specifically, in the case where CLIP(ViT-L/14) cascades with Qwen, offering a fully ordered class set, there is a substantial accuracy increase of 33.9% across various datasets. This improvement is consistently observed with GPT-4V as well, where organizing the classes based on CLIP\u2019s guidance boosts accuracy by 8.8% across the datasets."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Inference Efficiency Analysis",
            "text": "This subsection critically evaluates the efficacy of implementing an entropy threshold within the CascadeVLM framework. Functioning as a heuristic determinant, this threshold crucially dictates the juncture at which processing shifts from CLIP\u2019s initial evaluation to the computationally demanding LVLM analysis. This strategic integration plays a pivotal role in augmenting inference speed, adeptly balancing expeditious processing with the need for in-depth LVLM processing. Our experiments, conducted in a 1GPU (V100) environment, are illustrated in Figure 4  ###reference_###. Results indicate a direct correlation between an increase in entropy threshold and heightened inference speed, albeit at the cost of reduced accuracy.\n###figure_5###"
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Towards Explainable Predictions",
            "text": "Understanding the rationale behind the predictions in image classification provides valuable insights for a robust decision (Chen et al., 2019a  ###reference_b8###).\nEmploying the cascade framework, we design an explanation prompt that inquires why the LVLM prioritizes one option over others.\nFor instance, as depicted in Figure 5  ###reference_###, CLIP initially predicts an incorrect answer.\nThe LVLM not only corrects this but also explains its prediction, allowing us to dissect the LVLM\u2019s reasoning ability.\nThis enables us to differentiate between the terms titmouse which is CLIP\u2019s prediction, and parus major, which is the LVLM\u2019s corrected choice.\nThis case highlights the great potential of our CascadeVLM beyond producing accurate classification results."
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Case Study",
            "text": "Our case study analysis presents an examination of three distinct scenarios encountered during experimentation with CLIP ViT-L/14 and Qwen as the LVLM in a  setting.\nCase 1 illustrates a scenario where CLIP\u2019s top-1 prediction is incorrect; however, the ground truth is present within its top-5 predictions. Leveraging the LVLM\u2019s discernment, the accurate class\u2014green-winged dove\u2014is selected.\nCase 2 depicts a situation where, despite CLIP\u2019s inclusion of the correct answer\u2014striped owl\u2014in its top-5 predictions, the LVLM fails to identify it correctly. This instance highlights potential areas for refinement within the LVLM\u2019s decision-making process.\nCase 3 demonstrates a complete misalignment where both CLIP and LVLM fail to recognize the correct class within the top-5 predictions, leading to a compounded error in the final outcome.\nThese cases underscore the nuanced complexities of fine-grained image classification and reaffirm the necessity for integrated approaches like CascadeVLM to capitalize on the strengths of both CLIP and LVLMs. They also provide valuable insights into the decision-making dynamics of the models, offering pathways for future enhancements.\n###figure_6###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Building vision language models (VLMs) for understanding the multi-modal world has been an active research area. Pilot studies leverage pre-training concepts from NLP (Devlin et al., 2019  ###reference_b12###), learning shared representations across modalities from mixed visual and language inputs (Li et al., 2019  ###reference_b27###; Tan and Bansal, 2019  ###reference_b46###; Su et al., 2020  ###reference_b45###; Chen et al., 2019b  ###reference_b10###; Li et al., 2020  ###reference_b28###). Among these, Radford et al. (2021  ###reference_b37###) introduced CLIP, a contrastive language-image pre-training framework that employs language as supervision, demonstrating potential for multi-modal tasks and inspiring subsequent variants for improvement (Jia et al., 2021  ###reference_b20###; Li et al., 2022b  ###reference_b29###, 2023b  ###reference_b30###, 2021a  ###reference_b24###, a  ###reference_b23###).\nThe evolution of large language models like ChatGPT (OpenAI, 2022  ###reference_b35###) has motivated the development of large vision language models (LVLMs), combining powerful vision encoders like CLIP with large language models such as LLaMa (Touvron et al., 2023  ###reference_b47###) and Vicuna (Chiang et al., 2023  ###reference_b11###). Achieved through large-scale modality alignment training on image-text pairs (Alayrac et al., 2022  ###reference_b1###; Awadalla et al., 2023  ###reference_b2###) and supervised fine-tuning on multi-modal instruction tuning datasets (Liu et al., 2023  ###reference_b31###; Li et al., 2023a  ###reference_b26###), resulting LVLMs like GPT-4V (OpenAI, 2023  ###reference_b36###) and Qwen-VL (Bai et al., 2023  ###reference_b3###) exhibit promising perceptual and cognitive abilities (Yang et al., 2023  ###reference_b51###) for engaging user queries. This paper identifies limitations in CLIP and LVLMs for fine-grained image recognition and proposes the CascadeVLM framework to effectively enhance prediction accuracy by harnessing the advantages of both models.\nFine-grained image recognition, involving categorization into subordinate classes within a broader category, such as cars (Krause et al., 2013  ###reference_b21###) and aircraft models (Maji et al., 2013  ###reference_b32###), demands fine-grained feature learning. Previous work explores diverse strategies, including local-global interaction modules with attention mechanisms (Fu et al., 2017  ###reference_b16###; Zheng et al., 2017  ###reference_b53###), end-to-end feature encoding with specialized training objectives (Dubey et al., 2018  ###reference_b14###; Chang et al., 2020  ###reference_b7###), and the incorporation of external knowledge bases or auxiliary datasets (Chen et al., 2018  ###reference_b9###; Xu et al., 2018  ###reference_b50###). These approaches offer potential enhancements similar to our CLIP model, which we identify as a future exploration for improved performance.\nRecent studies have enhanced the CLIP primarily via prompt engineering and pre-training techniques.\nIn prompt engineering, CoOp (Zhou et al., 2021  ###reference_b54###) introduces an innovative method by learning context words as continuous vectors. Extending this idea, CoCoOp (Zhou et al., 2022a  ###reference_b55###) incorporates a lightweight neural network to generate input-specific tokens for images, further improving model performance. POMP Li et al. (2023b  ###reference_b30###) proposes pre-training a general soft prompt on the ImageNet-21K dataset for universal visual tasks.\nBesides, Menon and Vondrick (2022  ###reference_b33###) instead employs GPT-4 to generate better descriptive prompts for classification, enriching the prompting context for CLIP models.\nIn the realm of pre-training, MAWS (Singh et al., 2023  ###reference_b43###) combines Masked Autoencoder (MAE) pre-training with weakly supervised learning, significantly enhancing the learning efficacy. Similarly, FLIP (Li et al., 2023b  ###reference_b30###) increases prediction accuracy by masking substantial portions of image patches, facilitating the processing of more image-text pairs within the same timeframe and boosting performance across various tasks.\nDifferent from previous studies, our CascadeVLM explores the integration of LVLMs for leveraging their world knowledge to effectively handle similar classes."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we propose CascadeVLM, harnessing the advantages of CLIP and LVLMs for fine-grained image classification.\nBy utilizing CLIP for selecting the potential candidate class, LVLM can make more accurate predictions for image classes with subtle differences.\nExperimental results on four benchmarks demonstrate the effectiveness of our proposed framework. Further extension to the few-shot setups showcases the great potential of the cascading framework to leverage the in-context learning ability of LVLMs."
        }
    ],
    "url": "http://arxiv.org/html/2405.11301v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5"
        ],
        "methodology_sections": [
            "2",
            "2.1",
            "2.2",
            "2.3"
        ],
        "main_experiment_and_results_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.2",
            "3.3"
        ]
    },
    "research_context": {
        "paper_id": "2405.11301v1",
        "paper_title": "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models",
        "research_background": "### Motivation\n\nThe paper is motivated by the persistent challenges in fine-grained image classification, especially in distinguishing visually similar sub-classes. Despite the advancements in vision-language models (VLMs), such as CLIP, which exhibit strong zero/few-shot classification capabilities, they struggle with fine-grained distinctions due to subtle visual differences.\n\n### Research Problem\n\nThe core research problem addressed in the paper is enhancing zero/few-shot fine-grained image classification performance. Current methods leveraging vision-language models like CLIP struggle with visually similar classes, even when combining advanced prompt engineering and pre-training supervision. The significant accuracy drop encountered when expanding the candidate class set in large vision language models (LVLMs) due to their limited capacity for long-context modeling also underscores the inadequacy in handling large sets of visually similar classes.\n\n### Relevant Prior Work\n\n1. **CLIP and Few-Shot Capabilities**:\n   - Radford et al. (2021) highlighted CLIP\u2019s zero/few-shot classification capabilities.\n   - Zhou et al. (2022b) further discussed its potential.\n\n2. **Challenges in Fine-Grained Classification**:\n   - Ren et al. (2023a) emphasized the challenge in distinguishing closely related sub-classes in fine-grained image classification.\n\n3. **Enhancements to CLIP**:\n   - Methods focusing on advanced prompt engineering: Zhou et al. (2021, 2022a).\n   - Enhancement of pre-training supervision: Li et al. (2023b); Singh et al. (2023).\n   - Menon and Vondrick (2022) proposed using GPT-3 for generating detailed class descriptions to enrich CLIP\u2019s prompt context, but this approach showed limitations for fine-grained classification due to near-identical descriptions for visually similar classes.\n\n4. **Challenges with LVLMs**:\n   - Zhao et al. (2023) and Bai et al. (2023) indicated the difficulty of LVLMs in handling a large candidate class set, with noticeable accuracy drop when candidate categories are increased.\n\n### Proposed Solution\n\nTo address the outlined challenges, the authors propose CascadeVLM, a framework that integrates CLIP-like models and LVLMs. Here, CLIP models are first used to locate potential candidate classes (acting as a class filter), and then the LVLMs are employed to capitalize on their extensive world knowledge for refined predictions. This integration is intended to enhance the performance of zero/few-shot fine-grained image classification tasks. Additionally, they leverage the in-context learning capability of LVLMs for few-shot learning and employ an entropy threshold mechanism to improve inference efficiency dynamically. \n\n### Experimental Validation\n\nThe authors conducted experiments on various fine-grained image datasets, demonstrating that CascadeVLM outperforms individual models, with significant accuracy improvements such as an 85.6% accuracy rate on the Stanford Cars dataset. They also compared the results using advanced pretrained CLIP-like models on the iNaturalist and SUN397 datasets, achieving superior results.",
        "methodology": "### Methodology: Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models\n\n#### Overview\nThe CascadeVLM framework is meticulously crafted with a two-step approach for enhancing fine-grained image classification. It leverages powerful vision-language models (VLMs), particularly focusing on how they handle zero-shot and few-shot learning.\n\n#### Step 1: Candidate Selection with CLIP\nThe first step is crucial: it involves the employment of the CLIP model to select a set of candidate classes relevant to the input image. This selection narrows the large search space to a more manageable subset, which facilitates more precise and efficient subsequent predictions by the VLMs.\n\n#### Step 2: Prediction with Large Vision-Language Models (LVLMs)\n\n##### Zero-Shot Prediction\nFor zero-shot prediction, the process is direct but potent. After the CLIP model identifies the top-k candidate classes, the LVLM uses this pre-selected set to make a final classification. Importantly, zero-shot learning as described by Socher et al. (2013) enables the LVLM to tackle classes it hasn\u2019t been specifically trained on, by leveraging broad contextual knowledge:\n\\[ \\text{Zero-Shot Prediction Function: } \\mathcal{Z}(I, C_k) \\]\nHere, \\( \\mathcal{Z} \\) is the prediction function of the LVLM based on the input image \\( I \\) and the candidate set \\( C_k \\).\n\n##### Few-Shot Prediction\nFew-shot learning within CascadeVLM involves a sophisticated in-context learning strategy, as highlighted by Brown et al. (2020). This is particularly beneficial when additional, albeit minimal, training samples are available.\n\nThe few-shot process includes two sub-steps:\n\n###### Step 1: Context Generation\nFor each potential category in the candidate set \\( C_k \\), a random example image \\( x_c \\) is selected from the training data. This example is merged with a prompt to generate a comprehensive context:\n\\[ P_c(x_i, C_k) \\]\nThese prompts and images create a robust contextual basis, aiding the LVLM in understanding and classifying the input image \\( I \\). \n\n###### Step 2: Prediction with Contextual Information\nIntegrating the generated context with the input image, the LVLM uses this rich, contextual backdrop to refine its predictions:\n\\[ \\mathcal{F}(I, C_k, P_c) \\]\nHere, \\( \\mathcal{F} \\) denotes the prediction function of the LVLM, incorporating the input image \\( I \\), the candidate set \\( C_k \\), and the contextual prompts \\( P_c \\).\n\n#### Conclusion\nThe CascadeVLM framework combines the strengths of zero-shot and few-shot prediction methodologies via cascaded VLMs. By first constraining the candidate space with CLIP and then applying advanced LVLMs using pre-trained knowledge and contextual augmentation, the model significantly advances the precision and reliability of fine-grained image classifications.",
        "main_experiment_and_results": "For the main experiment, we assess the performance of our CascadeVLM framework through rigorous evaluation across multiple benchmarks. We use different variants of the CLIP (Contrastive Language-Image Pre-training) model, specifically:\n\n- CLIP ViT-B/32\n- CLIP ViT-B/16\n- CLIP ViT-L/14\n\nThese CLIP models are paired with either Qwen-VL-Chat (Bai et al., 2023) or GPT-4V (OpenAI, 2023) as the Large Vision-Language Models (LVLMs). \n\n#### Datasets\nWe utilize a wide array of datasets for fine-grained image classification, each offering distinct characteristics:\n\n- Flowers102 (Nilsback and Zisserman, 2008)\n- StanfordCars (Krause et al., 2013)\n- FGVC Aircraft (Maji et al., 2013)\n- BirdSnap (Berg et al., 2014)\n- SUN39 (Xiao et al., 2010)\n- iNaturalist(2018) (Horn et al., 2018)\n\nThese datasets collectively cover a wide range of categories crucial for evaluating fine-grained image classification.\n\n#### Evaluation Metrics\nThe primary evaluation metric for our comparative analysis is classification accuracy. This includes assessing the individual performance of various CLIP variants and comparing it within the CascadeVLM framework. Moreover, we extend our comparison to include diverse methodologies to provide a comprehensive evaluation of our proposed approach.\n\n#### Main Experimental Results\nThe experimental results indicate a notable enhancement in classification accuracy when employing the CascadeVLM framework. By comparing the performance of various CLIP variants within this cascade setting, we demonstrate:\n- Enhanced classification accuracy achieved through the integration of these models.\n\nThese results underscore the efficacy of the CascadeVLM framework in improving fine-grained image classification, validated across a diverse array of datasets."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Explore the potential for improving zero-shot learning accuracy in fine-grained image classification using pre-trained CLIP-like models cascaded with GPT-4V.",
            "experiment_process": "Two advanced CLIP-like models, MAWS CLIP and OpenCLIP (ViT-G/14), were used as backbones within a cascade framework that incorporates GPT-4V. The experiments were conducted on iNaturalist and SUN397 datasets with a random subset of 500 samples each, selected due to budget and rate limit constraints. The performance was compared against traditional CLIP models, focusing on imbalanced datasets like iNaturalist.",
            "result_discussion": "The findings reveal significant performance improvements using the cascade framework: MAWS CLIP achieved 6.8% and 3.6% improvements on iNaturalist and SUN397, respectively, while OpenCLIP saw enhancements of 5.2% on iNaturalist and 3.2% on SUN397. These results underscore the framework's effectiveness and adaptability in improving the capabilities of CLIP-like models for zero-shot learning in fine-grained image classification.",
            "ablation_id": "2405.11301v1.No1"
        },
        {
            "research_objective": "Evaluate the effectiveness of the GPT-4V model for few-shot learning in fine-grained image classification tasks.",
            "experiment_process": "Initial assessments were made using Qwen-VL, which underperformed in few-shot settings, leading to the selection of GPT-4V. Due to budget and rate limitations, the experiments were conducted on a random subset of 200 samples per dataset. The few-shot experiments applied a top-k (k=5) approach and evaluated performance on datasets such as Flower102 and StanfordCars.",
            "result_discussion": "The experiments yielded significant improvements in predictive accuracy: Flower102 achieved 94.5% accuracy, while StanfordCars reached 88.5%. These outcomes not only emphasize the cascade framework's efficacy but also highlight its efficiency and adaptability in leveraging GPT-4V for few-shot learning in fine-grained classification domains.",
            "ablation_id": "2405.11301v1.No2"
        }
    ]
}