{
    "title": "1 Introduction",
    "abstract": "In this paper, we present a novel approach for text independent phone-to-audio alignment based on phoneme recognition, representation learning and knowledge transfer. Our method leverages a self-supervised model (wav2vec2) fine-tuned for phoneme recognition using a Connectionist Temporal Classification (CTC) loss, a dimension reduction model and a frame-level phoneme classifier trained thanks to forced-alignment labels (using Montreal Forced Aligner) to produce multi-lingual phonetic representations, thus requiring minimal additional training. We evaluate our model using synthetic native data from the TIMIT dataset and the SCRIBE dataset for American and British English, respectively. Our proposed model outperforms the state-of-the-art (charsiu) in statistical metrics and has applications in language learning and speech processing systems. We leave experiments on other languages for future work but the design of the system makes it easily adaptable to other languages.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Pronunciation is pivotal for language acquisition and effective communication. However, it often receives insufficient attention in second language (L2) education, leading to persistent challenges for learners in achieving clarity. Technology, particularly Computer-Assisted-Language-Learning (CALL) has emerged as a significant aid in supporting L2 pronunciation development across formal and informal learning environments Chapelle (2009  ###reference_b1###). Recent advancements in deep learning offer promising avenues for enhancing language learning by providing unlimited targeted and immediate feedback, addressing the class time and size limitations preventing educators from differentiating their pedagogy on such an individual skill.\nTo improve pronunciation, integrating speech processing technologies with traditional teaching methods can provide a comprehensive approach to language learning. One fundamental task in this domain is text independent phone-to-audio alignment, a process that involves aligning phonetic representations with corresponding audio signals without relying on pre-determined text. This task is essential for accurately mapping the phonemes to their acoustic representations, contributing to the development of precise and effective speech processing technologies in language learning Golonka et al. (2014  ###reference_b2###); Tits and Broisson (2023  ###reference_b3###).\nText independent phone-to-audio alignment faces a significant challenge due to the difficulty in obtaining extensive and well-annotated datasets. A possible solution to this challenge entails using established systems (like text dependent phone-to-audio alignment systems) to extract temporal information from publicly available speech datasets. This approach can be refined through the application of transfer learning and self-supervised learning methodologies in the development of a solution.\nTransfer learning Tan et al. (2018  ###reference_b4###), is a method within the field of deep learning. Its approach involves pre-training a model on a large dataset and subsequently fine-tuning it on a smaller dataset tailored to the specific task. This methodology has demonstrated considerable success across various domains, particularly in the context of self-supervised learning.\nIn Self-supervised learning Jaiswal et al. (2020  ###reference_b5###) a model is trained to autonomously learn representations of input data without the need for explicit supervision. This proves particularly advantageous when labeled data is either limited or entirely unavailable. Within the field of speech technology, the application of self-supervised learning through transfer learning has proven invaluable in addressing several complex scenarios. For example, in Automatic Speech Recognition (ASR) for low resource languages Zoph et al. (2016  ###reference_b6###), where annotated data may be scarce, transfer learning allows the utilization of pre-trained models on more data-abundant languages, adapting them effectively to the target language. Likewise, in emotional or expressive speech synthesis Tits et al. (2020  ###reference_b7###, 2019  ###reference_b8###, 2021  ###reference_b9###), speech emotion recognition Tits et al. (2018  ###reference_b10###), voice conversion Zhou et al. (2022  ###reference_b11###) and pronunciation assessment Hu et al. (2015  ###reference_b12###).\nIn this paper, we take full advantage of state-of-the-art methodologies in deep learning, self-supervised learning, and phonetic representation to present a novel approach to text independent phone-to-audio alignment.\nMost state-of-the-art self-supervised systems perform well on American English which means that other varieties of English get penalised and the model is biased towards American English. The rationale to develop the system proposed in the paper was the pedagogical needs to create a system that performs equally well on other variants of English as it does on American English. For our system, we use the self-supervised model Wav2Vec2, fine-tuned for phoneme recognition using CTC loss. We integrate this with a dimensional reduction model based on Principal Component Analysis (PCA) and a frame-level phoneme classifier. The resulting model pipeline produces a vector of probabilities for every audio frame. From the same model we also extract predicted phonemes and their boundaries and hence use this information for a text independent phone alignment system.\nThe major contributions of this paper are as follows: First,\nwe propose a text independent phoneme alignment system using self-supervised learning. This not only advances the state-of-the-art in this specific task but also opens avenues for broader applications in language learning and speech processing systems. Second, this system functions effectively with diverse English language variations (British) and is capable of accommodating various languages, making it language-independent."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Phone-to-Audio Alignment",
            "text": "Text independent phone-to-audio alignment involves predicting a sequence of phones and their temporal locations within speech signal without prior linguistic information, such as a known text or phone sequence.\nIn contrast, text dependent phone-to-audio alignment utilizes text information to align a phone sequence generated from a grapheme-to-phoneme model applied to textual inputs.\nHidden Markov Models (HMMs) have traditionally played a prominent role in aligning phonetic and temporal information, including but not limited to Kaldi Povey et al. (2011  ###reference_b13###) and HTK Young et al. (2002  ###reference_b14###). Forced alignment systems using HMMs include Gentle111https://lowerquality.com/gentle/  ###reference_lowerquality.com/gentle/### and ProsodyLab Gorman et al. (2011  ###reference_b15###). However, such models face limitations when attempting to predict both phones and phone boundaries simultaneously. The challenges for HMMs become apparent when confronted with long-range dependencies within a sequence. However, they struggle to effectively capture and understand the broader context of a word within a sentence. As a result, this hinders their ability to grasp nuanced linguistic relationships and context. Another reason why HMMs may not perform well is due to the incorrect phonetic transcriptions due to variations in conversational speech.\nRecent developments have witnessed a shift towards deep learning models like Recurrent Neural Networks (RNNs) Koizumi et al. (1996  ###reference_b16###) and models trained with CTC loss M\u00fcller et al. (2017  ###reference_b17###), for phoneme recognition. This shift towards deep learning models allows us to efficiently predict both phones and phone boundaries simultaneously."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Phoneme Recognition",
            "text": "Deep learning models are known for their ability to capture complex patterns in data and are frequently employed for phoneme recognition. CTC loss is a popular training criterion for such models in this context. It allows the model to learn the alignment between the input speech signal and the corresponding phoneme sequence, even when the temporal correspondence is not provided during training. Wav2Vec2 Conneau et al. (2020  ###reference_b18###); Xu et al. (2021  ###reference_b19###) stands out as a self-supervised model that has been fine-tuned specifically for phoneme recognition using the CTC loss.\nOne of the advantages of the Wav2Vec2 approach is its ability to generate multi-lingual phonetic representations. By leveraging self-supervised learning during pre-training, the model learns to extract robust features from speech signals, capturing phonetic information that is broadly applicable across different languages. Furthermore, the fine-tuning process with CTC loss refines the model\u2019s ability to map these learned representations to specific phoneme sequences."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Systems Predicting Phones and Boundaries",
            "text": "Charsiu Zhu et al. (2022  ###reference_b20###) is recognized for its unique capability to predict both phones and phone boundaries. This integrated approach contributes to a more comprehensive understanding of the speech signal. While HMMs can perform similar tasks, they are often employed more prevalently for forced-alignment tasks, particularly in text-dependent scenarios. The emphasis on forced alignment implies that these systems are commonly utilized when the corresponding phoneme or phone sequence is available.\nTherefore, in this paper, we conduct a comparative analysis between our proposed model and the text independent aspect of the charsiu model, recognized as the state-of-the-art for this task. Our methodology, detailed in the following section, combines self-supervised learning with phoneme recognition."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Our Proposed Method",
            "text": "Our proposed method is an innovative approach, using wav2vec2, Principal Component Analysis (PCA) for dimensional reduction and frame-level phoneme classification, offers a robust text independent phone-to-audio alignment. The system is explained in details in subsection 3.3 and the system\u2019s architecture is depicted in Figure 1. The model\u2019s robustness is demonstrated through evaluation using synthetic native data, using the TIMIT Garofolo (1993  ###reference_b21###) and SCRIBE dataset. Thanks to its versatility, we expect that our method will find applications in language learning and speech processing systems.\n###figure_1###"
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Pre-trained Self-Supervised Model",
            "text": "In this Section, we explain what open-source model we use as a basis222https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft  ###reference_lsr-53-espeak-cv-ft### in order to leverage learned cross-lingual representations of speech adapted towards the phonetic space.\nThe basis of this model is a self-supervised model (wav2vec2) trained to extract latent representations from the audio data. The wav2vec2 model is trained on a large dataset of unlabeled speech data, using a contrastive predictive coding loss function to learn a representation that is capable of predicting future audio frames and can then be utilized for downstream tasks. The pre-training loss is defined as Baevski et al. (2020  ###reference_b22###):\nWhere,  is contrastive loss,  is diversity loss and  is the hypertuned parameter\nIn the above equation,  represents the fixed temperature, and sim signifies the cosine similarity between context representations and quantized latent speech representations. The term  resembles the Softmax Function, but it employs cosine similarity instead of a score. For ease of optimization, the negative logarithm of the fraction is also applied.  is the context network output centered over masked time step  and  is the true quantized latent speech representation.\nIn this paper we use the version of the model pre-trained on cross-lingual speech data (53 different languages) Conneau et al. (2020  ###reference_b18###) that was then fine-tuned Xu et al. (2021  ###reference_b19###) for the task of phoneme sequence prediction using a CTC loss, and the result is an open-source 333https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft  ###reference_lsr-53-espeak-cv-ft###.\nThe model itself thus predict sequences of phoneme labels, but no phone boundaries. What we propose in the following sections is an approach for leveraging the learned cross-lingual speech representations of this model that was already oriented to a phonetic space thanks to the CTC fine-tuning. The goal of this approach is to require limited amount of data and to be less biased towards American English accent compared to existing models."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we describe the experiments conducted to evaluate the performance of our proposed phone-to-audio alignment model."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Text independent phone-to-audio alignment on TIMIT",
            "text": "In evaluating the performance of our text independent phone-to-audio alignment model, we conducted a comprehensive comparison with the state-of-the-art model, charsiu Zhu et al. (2022  ###reference_b20###).\nIn the charsiu model the authors compare two systems: text independent phone-to-audio alignment and text dependent phone-to-audio alignment using Wav2Vec2. The Wav2Vec2-FS is a semi-supervised model which learns alignment using contrastive learning and a forward sum loss. The second model, Wav2Vec2-FC, is a frame classification model trained on labels for forced alignment, capable of both forced alignment and text-independent segmentation. The evaluation of both the systems for charsiu has been performed on the TIMIT dataset. It has been used for this evaluation because of the availability of human annotations especially phoneme level. We use a similar approach to evaluate the same metrics for our model.\nThe assessment is based on statistical measures, namely precision, recall, F1 score, and r-value. In the referenced work, the authors present their best-performing model, W2V2-FC-32k-Libris, as can be observed in Table 1.\nIn our case, for the task of phoneme recognition, we compare our model with the text independent W2V2-FC-10ms charsiu model. The statistical metrics for our proposed model outperform that of the charsiu model. We compare the TIMIT test dataset to assess our model and we observe from Table 1 that the r-value for our model deteriorates in performance. Apart from r-value, all the other metrics: Precision, Recall and F1 value have shown to perform well for our model.\nThe tests for our model are also performed on the TIMIT dataset but has the possibility to extend to other datasets with real speech and also other languages.\nThe r-value, which is known as the Pearson correlation coefficient measures the similarity between the ground truth phonemes and the predicted phonemes. Some of the reasons for low r-value could be alignment errors, variability in the pronunciation of speakers or changes in speaking style within the dataset. However, we need more experiments to confirm this hypothesis."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Text-independent phone-to-audio alignment on SCRIBE",
            "text": "A second part of the experiments in our proposed model is to evaluate it on British English.\nThe SCRIBE444https://www.phon.ucl.ac.uk/resource/scribe/  ###reference_/### dataset is a small corpus of read speech and spontaneous speech specializing in British English. It consists of 200 \u2019phonetically rich\u2019 sentences and 460 \u2019phonetically compact\u2019 sentences. The \u2019phonetically rich\u2019 sentences are phonetically balanced. The \u2019phonetically compact\u2019 sentences are based on a British version of the MIT compact sentences (as in TIMIT). There are 45 files in the dataset of 30-50s containing 5-10 sentences.\nThe audio files and the phoneme annotations needed some processing before we could start using the dataset. The phonemes are in SAMPA form and needed to be converted to the English Arpabet. Even after the conversion from SAMPA to Arpabet, there were symbols that we could not retrieve so we filtered them out.\nWe evaluated the charsiu model and our proposed model using SCRIBE and achieved the metrics that are depicted in Table 2.\nUpon close examination of the results presented in Table 2, it becomes evident that the metrics associated with our proposed model exhibit a greater uniformity in values when compared to charsiu. Furthermore, these metrics generally surpass those of charsiu, albeit with the exception of precision. The uniformity observed in our model\u2019s metrics can be attributed to the utilization of a more balanced reduced frame dataset during training, which serves to mitigate biases and yield more consistent outcomes. Moreover, the superior quality of audio files within the SCRIBE dataset, resembling professional studio recordings with minimal noise, likely contributes to the heightened metrics observed for both charsiu and our proposed model when contrasted with the TIMIT dataset.\nConsequently, our system demonstrates enhanced generalization across various accents, laying the foundation for potential expansion to other languages. The key lies in leveraging the underlying Wav2vec2 XLSR framework, and the methodology employed with MAILABS can seamlessly be replicated with different datasets, opening avenues for utilizing non-native English accents and broader linguistic applications."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Discussion",
            "text": "In this paper, we introduced an innovative text independent phone alignment system designed to be language-independent. Our method harnesses a self-supervised model (wav2vec2) fine-tuned for phoneme recognition through a CTC loss, alongside a dimensional reduction model (PCA) and a frame-level phoneme classifier. Through experiments using synthetic native data, we assessed our model\u2019s performance on both American and British English, benchmarking it against the state-of-the-art charsiu model. Encouragingly, our results demonstrate robustness and applicability to diverse accents, such as British English.\nHowever, certain limitations are acknowledged. Firstly, the reducer and classifier have been trained on a restricted amount of data specific to native English, posing a constraint. Secondly, our model relies on forced-aligned datasets of native speech for effective learning, introducing another limitation. Thirdly, the SCRIBE dataset that we used is a smaller dataset of British English as compared to the TIMTI dataset. Lack of well-annotated datasets, especially on phoneme level is one of our biggest challenges.\nFuture research directions could explore the incorporation of datasets containing non-native English data. This involves re-training the shallow reducer and classifier models using non-native speech data. Furthermore, extending our approach to languages beyond English and evaluating its performance with real-world data from language learners presents an intriguing avenue for exploration. Our work lays the foundation for further experiments in the realm of text independent phone-to-audio alignment, especially in the context of non-native English.\nPart of this work was done during the project REDCALL that is partially funded by a FIRST Entreprise Docteur program from SPW Recherche555https://recherche.wallonie.be/.\nPart of this work was done during the project DEEPCALL that is partially funded by a Win4Doc program from SPW Recherche.\n-\\extralength0cm\nReferences"
        }
    ],
    "url": "http://arxiv.org/html/2405.02124v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2.1",
            "2.2",
            "2.3"
        ],
        "methodology_sections": [
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4.1",
            "4.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.3",
            "4.1",
            "4.2"
        ]
    },
    "research_context": {
        "paper_id": "2405.02124v1",
        "paper_title": "1 Introduction",
        "research_background": "### Paper's Motivation:\nThe primary motivation of the paper is to address the persistent challenges faced by second language (L2) learners in achieving clear pronunciation. Traditional language education often lacks sufficient focus on pronunciation, which is crucial for effective communication and language acquisition. With advancements in technology, particularly in deep learning and Computer-Assisted-Language-Learning (CALL), there is an opportunity to enhance pronunciation learning by integrating speech processing technologies. The paper aims to leverage these advancements to provide individualized, targeted feedback, which overcomes the limitations of class time and size in conventional education settings.\n\n### Research Problem:\nThe research problem tackled in this paper is the difficulty of achieving accurate text independent phone-to-audio alignment, which is essential for precisely mapping phonetic representations to their acoustic realizations. This task is complicated by the challenge of obtaining extensive and well-annotated datasets. The goal is to develop a system that performs effective phoneme alignment without relying on pre-determined text, particularly one that works well across different English language variations (notably British English) and can be adapted to various languages.\n\n### Relevant Prior Work:\n1. **CALL and CSLL**: The integration of computer-assisted methodologies in language learning has been significant in supporting pronunciation development (Chapelle, 2009)###reference_b1###.\n2. **Speech Processing Technologies**: Essential foundational work in the alignment of phonetic representations and audio signals has underscored the importance of precise speech processing for effective language learning (Golonka et al., 2014)###reference_b2###; (Tits and Broisson, 2023)###reference_b3###.\n3. **Transfer Learning**:\n   - Demonstrated success across various domains, with a particular impact on self-supervised learning by leveraging pre-trained models on large datasets before fine-tuning on task-specific smaller datasets (Tan et al., 2018)###reference_b4###.\n4. **Self-Supervised Learning in Speech Technologies**:\n   - Effective in situations with limited or unavailable labeled data, important for tasks in speech technology such as automatic speech recognition for low resource languages and expressive speech synthesis (Jaiswal et al., 2020)###reference_b5###, (Zoph et al., 2016)###reference_b6###.\n   - Specific applications include emotional or expressive speech synthesis (Tits et al., 2020)###reference_b7###; (Tits et al., 2019)###reference_b8###; (Tits et al., 2021)###reference_b9###, speech emotion recognition (Tits et al., 2018)###reference_b10###, voice conversion (Zhou et al., 2022)###reference_b11###, and pronunciation assessment (Hu et al., 2015)###reference_b12###.\n\n### Novel Contribution:\nThe paper presents a novel approach by using a self-supervised learning model (Wav2Vec2) fine-tuned for phoneme recognition and integrating it with a PCA-based dimensional reduction model and a frame-level phoneme classifier to produce text independent phoneme alignment. This system achieves significant advancements in the field by:\n1. Offering an accurate and efficient text independent phoneme alignment system.\n2. Ensuring the system performs well across different English variations, with a particular focus on British English.\n3. Providing a scalable solution that can extend to various languages, enhancing its applicability in diverse language learning and speech processing contexts.",
        "methodology": "Methodology: Our proposed method is an innovative approach, using wav2vec2, Principal Component Analysis (PCA) for dimensional reduction and frame-level phoneme classification, offers a robust text independent phone-to-audio alignment. The system is explained in details in subsection 3.3 and the model\u2019s robustness is demonstrated through evaluation using synthetic native data, using the TIMIT Garofolo (1993) and SCRIBE dataset. Thanks to its versatility, we expect that our method will find applications in language learning and speech processing systems.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets\nThe main experiment is conducted using the **TIMIT dataset**, which is chosen due to its availability of human annotations, especially at the phoneme level.\n\n#### Baselines\nThe primary baseline model for comparison is the **charsiu model** presented in Zhu et al. (2022). Specifically, comparisons are drawn against the following configurations of the charsiu model:\n- **Wav2Vec2-FC-10ms**: A frame classification model trained on labels for forced alignment and capable of text-independent segmentation.\n\n#### Evaluation Metrics\nThe evaluation metrics used to assess the performance of the models include:\n- **Precision**\n- **Recall**\n- **F1 score**\n- **r-value (Pearson correlation coefficient)**: Measures the similarity between the ground truth phonemes and the predicted phonemes.\n\n### Main Experimental Results\nThe results focus on comparing the statistical metrics of the proposed model against the charsiu model. The following observations are noted:\n- The proposed model outperforms the charsiu model in **Precision**, **Recall**, and **F1 score**.\n- The **r-value** for the proposed model shows a deterioration in performance compared to the charsiu model. This might be due to alignment errors, variability in pronunciation of speakers, or changes in speaking style within the dataset.\n\nThese findings suggest that while the proposed model excels in phoneme recognition metrics like precision, recall, and F1 score, there is room for improvement in achieving better alignment as indicated by the lower r-value. More experiments are recommended to investigate the causes of this lower r-value."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine the optimal level of variance retention in dimensionality reduction for generating a frame-level phoneme classifier that balances noise filtration and classifier performance.",
            "experiment_process": "Experimenting with three levels of variance retention (99%, 95%, and 90%), using PCA on speech representations extracted from a self-supervised model fine-tuned for phoneme recognition. The MAILABS Solak dataset (American and British English) is employed to train a dimension reduction model and a frame classifier. Data balancing ensures unbiased training across phonemes. A K-Nearest Neighbors (KNN) classifier with 10 neighbors produces a probability matrix for phoneme classification. Frames with maximum posterior probabilities form the final phoneme sequences, filtered by a threshold of 0.5 to remove low-probability phonemes.",
            "result_discussion": "Retaining 95% variance in PCA provided the best classification results, likely due to effective noise filtration and creating a manageable space for the classifier. The integration of the reduction model with the classifier model effectively managed the mapping between learned representations and phonetic units, ensuring computational efficiency while handling complex data relationships.",
            "ablation_id": "2405.02124v1.No1"
        },
        {
            "research_objective": "To evaluate the performance of the proposed text-independent phone-to-audio alignment model compared to the state-of-the-art charsiu model on the TIMIT dataset.",
            "experiment_process": "Comparison of statistical metrics (precision, recall, F1 score, and r-value) of the proposed model and charsiu\u2019s W2V2-FC-10ms model using the human-annotated TIMIT dataset. The datasets for both models are processed similarly, and the results are displayed in Table 1.",
            "result_discussion": "The proposed model outperforms the charsiu model in precision, recall, and F1 score but shows inferior results in the r-value metric. The low r-value might be due to alignment errors, speaker pronunciation variability, or speaking style differences in the dataset, though additional experiments are required to confirm these hypotheses.",
            "ablation_id": "2405.02124v1.No2"
        },
        {
            "research_objective": "To evaluate the performance of the proposed text-independent phone-to-audio alignment model on the British English SCRIBE dataset and compare it with the charsiu model.",
            "experiment_process": "Evaluation of both models (proposed and charsiu) on the SCRIBE dataset, which includes converting phoneme annotations from SAMPA to Arpabet and filtering irretrievable symbols. Metrics are compared as depicted in Table 2.",
            "result_discussion": "The proposed model achieves more uniform and generally superior metrics compared to charsiu, except for precision. The uniformity is attributed to balanced training data, and superior audio quality in SCRIBE contributes to better performance metrics. Results indicate that the system generalizes well across various accents and is adaptable to other languages, leveraging the Wav2vec2 XLSR framework and the MAILABS methodology.",
            "ablation_id": "2405.02124v1.No3"
        }
    ]
}