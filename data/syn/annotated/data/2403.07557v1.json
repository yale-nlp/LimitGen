{
    "title": "SIFiD: Reassess Summary Factual Inconsistency Detection with LLM",
    "abstract": "Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documents.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Document summarization, the process of distilling key information from extensive texts, has become indispensable across various real-world applications, propelled by advancements in Natural Language Generation (NLG) Pilault et al. (2020  ###reference_b11###); Ma et al. (2022  ###reference_b9###). The advent of Large Language Models (LLMs) Brown et al. (2020  ###reference_b1###); Ouyang et al. (2022  ###reference_b10###); Touvron et al. (2023  ###reference_b14###) has notably enhanced models\u2019 capabilities to generate natural and factually consistent summaries Chang et al. (2023  ###reference_b2###). However, the rapid evolution in summarization techniques may lead to factually inconsistent summaries which are very close to facts Zhang et al. (2023  ###reference_b16###). Such inconsistencies could pose significant challenges, resulting in hallucinations that traditional detection models struggle to identify. As LLMs evolve, there is a critical demand for more robust methods to detect factual inconsistencies, leveraging the advanced capabilities of LLMs themselves.\nLuo et al. (2023  ###reference_b8###) were among the first to utilize LLMs for the detection of factual inconsistencies, employing a universal zero-shot prompt across various benchmarks in SummaC Laban et al. (2022  ###reference_b7###) and inputting the full document along with its summary into GPT-3.5 for evaluation. Despite these innovations, their approach was limited by the plain application, early GPT-3.5 model\u2019s constraints and a lack of adaptation to the specific requirements of different benchmarks. Consequently, their method did not achieve superior performance compared to existing models, such as those detailed in the SummaC paper.\nThis paper revisits the challenge of inconsistency detection in document summarization through zero-shot inference with LLMs, specifically examining the latest versions of GPT-3.5 and GPT-4 on the SummaC dataset. We aim to set up new LLM-based baselines for research in this domain. Moreover, we introduce a novel methodology, SIFiD (Summary Inconsistency Detection with Filtered Document), designed to significantly enhance the efficiency and effectiveness of factual inconsistency detection. SIFiD focuses on identifying crucial sentences within documents by evaluating their entailment scores or semantic similarity with summary sentences, subsequently retaining only the most relevant sentences for further analysis. This approach not only refines the assessment of factual consistency but also reduces the computational resources required for evaluation by decreasing the number of input tokens.\n###figure_1### Our comprehensive evaluation on the SummaC dataset reveals that, while the updated GPT-3.5 model still falls short of outperforming traditional baseline methods, GPT-4 significantly excels in detecting factual inconsistencies. The integration of SIFiD further amplifies GPT-4\u2019s detection capabilities, highlighting the potency of our proposed method. To support continued research and collaboration in this field, we make our code available open source at Anonymous, fostering advancements and exploration in factual inconsistency detection."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "The evaluation of summary factual consistency has traditionally relied on methods such as Question Answering and Question Generation (QAG) Wang et al. (2020  ###reference_b15###); Durmus et al. (2020  ###reference_b3###); Scialom et al. (2021  ###reference_b13###), synthetic classifiers Kry\u015bci\u0144ski et al. (2020  ###reference_b6###), and pairing-based approaches Goodrich et al. (2019  ###reference_b4###); Goyal and Durrett (2020  ###reference_b5###). These methodologies focus on identifying discrepancies between documents and their summaries. Laban et al. (2022  ###reference_b7###) later demonstrated that Natural Language Inference (NLI) could be effectively employed for inconsistency detection at appropriate levels of text granularity, thereby advancing the field of summary inconsistency detection.\nThe emergence of Large Language Models (LLMs) has recently shifted the focus towards integrating these models into the assessment of summary factual consistency. Luo et al. (2023  ###reference_b8###) pioneered the application of GPT-3.5 for this purpose, tailoring prompts to various evaluation tasks including summary factual inconsistency detection, summary ranking, and consistency evaluation. Despite this innovative approach, the early iteration of GPT-3.5, coupled with an insufficient detection methodology, did not yield improvements over conventional techniques in identifying factual inconsistencies.\nIn our research, we revisit the approach proposed by Luo et al. (2023  ###reference_b8###), employing the most recent versions of GPT-3.5 and GPT-4. We integrate these advanced LLMs with our newly developed Summary Inconsistency Detection with Filtered Document (SIFiD) method. This combination aims to enhance the accuracy and efficiency of factual inconsistency detection, leveraging the state-of-the-art capabilities of LLMs to set new benchmarks in the field."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Approach",
            "text": "In this section, we detail our approach to reevaluating summary factual consistency using the latest GPT models and introduce the novel SIFiD method."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Summary Factual Inconsistency Detection with Large Language Models",
            "text": "As underscored in the Introduction, leveraging Large Language Models (LLMs) for detecting summary factual inconsistencies is crucial to addressing the challenges posed by rapidly improving document summarization capabilities. While Luo et al. (2023  ###reference_b8###) were pioneers in utilizing LLMs for this task, their methodology was constrained by the plain application, the limitations of early GPT models and a lack of differentiation in benchmark requirements. Our objective is to reevaluate this detection process using the most recent GPT models and a refined prompt template for the Polytope benchmark.\nInitially, we applied the prompt template used by Luo et al. (2023  ###reference_b8###) to assess the performance of GPT-3.5 Turbo and GPT-4 Turbo on SummaC. Recognizing the distinct requirements of Polytope benchmark in SummaC, we crafted a tailored prompt template to better suit Polytope and reevaluated the models\u2019 performance. The revised prompt template is detailed below:\nDecide if the following summary have any of the specified problems in relation to the corresponding article.\nThe problems are categorized as omission, addition, or inaccuracy. Omission means Key point is missing from the summary. Addition means Unnecessary and irrelevant snippets from the Article are included in the summary. Inaccuracy means some information in the summary is not supported by the article.\nArticle:\n{{ Article }}\nSummary:\n{{ Summary }}\nIf the summary has any of the above problems, answer \u2019No\u2019. Otherwise, answer \u2019Yes\u2019. Answer (Yes or No):\nComparing with the original prompt, we let the model detect omission, addition, and inaccuracy summary to fit the annotation of Polytope. With the experiments above, we set a new baseline for summary factual inconsistency detection with LLMs."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Scorer",
            "text": "We use one of the two distinct scoring mechanisms to evaluate the relevance between document sentences and summary sentences.\nEntailment Scorer: We adopt the entailment scoring approach as proposed by Laban et al. (2022  ###reference_b7###), which utilizes a Natural Language Inference (NLI) model Schuster et al. (2021  ###reference_b12###). The net entailment score is calculated by , where  and  are the initial entailment score and contradiction score directly calculated by the NLI model on . The net entailment score reflects the degree to which the summary sentence is supported by the document sentence without contradiction.\nSemantic Similarity Scorer: For assessing semantic similarity, we leverage the sentence-transformers library to generate embeddings for both document and summary sentences, denoted as  and , respectively. The cosine similarity between these embeddings serves as the measure of semantic similarity, which is , where  quantifies the semantic closeness between the document and summary sentences. This metric enables us to identify and assess the degree of semantic overlap."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Method\nCoGenSum\nXsumFaith\nPolytope\nFactCC\nSummEval\nFRANK\nAvg.\n\n\n\nDAE\n63.4\n50.8\n62.8\n75.9\n70.3\n61.7\n64.2\n\nFEQA\n61.0\n56.0\n57.8\n53.6\n53.8\n69.9\n58.7\n\nQuestEval\n62.6\n62.1\n70.3\n66.6\n72.5\n82.1\n69.4\n\nSummaC-ZS\n70.4\n58.4\n62.0\n83.8\n78.7\n79.0\n72.1\n\nSummaC-Conv\n64.7\n66.4\n62.7\n89.5\n81.7\n81.6\n74.43\n\nLuo et al. (2023  ###reference_b8###)\n63.3\n64.7\n56.9\n74.7\n76.5\n80.9\n69.5\n\n    +CoT\n74.3\n63.1\n61.4\n79.5\n83.3\n82.6\n74.0\n\nGPT-3.5 Turbo\n59.9\n67.6\n41.0(57.9)\n71.3\n81.4\n80.2\n66.9(69.7)\n\n    +CoT\n65.2\n62.3\n49.5(59.1)\n79.1\n77.4\n81.4\n69.2(70.8)\n\nSIFiD-Entailment\n65.5\n63.9\n37.5\n81.0\n79.0\n81.6\n68.1\n\n    +CoT\n65.7\n60.3\n52.7\n82.3\n79.3\n81.6\n70.3\n\nSIFiD-Similarity\n65.4\n64.7\n35.3\n76.0\n74.5\n80.1\n66.0\n\n    +CoT\n64.3\n59.7\n52.8\n81.7\n76.6\n80.4\n69.2\n\nGPT-4 Turbo\n80.9\n61.0\n66.0(60.9)\n89.6\n88.0\n87.4\n78.8(78.0)\n\n    +CoT\n80.2\n66.4\n62.1(61.4)\n87.8\n86.2\n85.6\n78.1(78.0)\n\nSIFiD-Entailment\n82.8\n58.9\n74.4\n89.4\n87.5\n86.1\n79.9\n\n    +CoT\n83.2\n60.6\n61.7\n89.4\n87.1\n85.8\n78.0\n\nSIFiD-Similarity\n83.1\n60.2\n71.0\n90.6\n86.8\n87.7\n79.9\n\n    +CoT\n82.9\n65.0\n69.3\n91.3\n84.6\n86.0\n79.8\nIn this section, we detail the experiments conducted with GPT models and the SIFiD method on SummaC Laban et al. (2022  ###reference_b7###). We evaluated the performance of GPT-3.5, GPT-4, and SIFiD against a range of state-of-the-art approaches, including traditional methods such as DAE Goyal and Durrett (2020  ###reference_b5###), FEQA Durmus et al. (2020  ###reference_b3###), QuestEval Scialom et al. (2021  ###reference_b13###), SummaC-ZS, SummaC-Conv Laban et al. (2022  ###reference_b7###), and an LLM-based method proposed by Luo et al. (2023  ###reference_b8###).\nFollowing previous research Luo et al. (2023  ###reference_b8###); Laban et al. (2022  ###reference_b7###), we report the balanced accuracy for SummaC. The experimental results were obtained from Luo et al. (2023  ###reference_b8###). Our experiments utilized gpt-3.5-turbo-1106 and gpt-4-1106-preview222https://platform.openai.com/docs/models. For the SIFiD configuration, we applied  for entailment-based filtering and  for semantic similarity-based filtering, observing a 61.3% and 67% sentence removal rate on average across benchmarks, respectively. We use all-mpnet-base-v2 for sentence-transformers."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Results and Analysis",
            "text": "The experimental outcomes are summarized in Table 1  ###reference_###, leading to several insights on LLM-based summary factual inconsistency detection:\nPrefer GPT-4 Over GPT-3.5. Analysis indicates that previous LLM-based methods, though superior to many traditional techniques, underperform compared to SummaC-Conv. This discrepancy is attributed to the limited capabilities of the GPT-3.5 model. Our reevaluation with the GPT-3.5 Turbo model yielded results similar to those of Luo et al. (2023  ###reference_b8###). However, substituting GPT-3.5 with GPT-4 Turbo significantly enhanced performance, from 69.7 to 78.0, underscoring GPT-4\u2019s advanced language comprehension.\nAdopt Benchmark-Specific Prompt Templates. The effectiveness of a single prompt template across different benchmarks is limited due to the unique requirements of each benchmark. Traditional methods typically incorporate benchmark-specific training, which mitigates task variance. In contrast, LLMs rely on the provided instructions, necessitating tailored prompt templates. Adjusting the prompt template for Polytope increased GPT-4\u2019s performance from 60.9 to 66.0, elevating the overall average to 78.8. However, this adjustment resulted in a performance decline for GPT-3.5 on Polytope, from 57.9 to 41.0, highlighting GPT-3.5\u2019s inferior prompt comprehension.\nEnhanced Performance with SIFiD on GPT-4. Integrating SIFiD with GPT-4 further improved its performance to 79.9. SIFiD\u2019s selective filtering of sentences enhances document relevance to the summary, simplifying factual inconsistency detection. This approach did not yield similar benefits for GPT-3.5, possibly due to its reduced efficacy in processing less fluent filtered documents.\nMixed Results with Chain-of-Thought (CoT). Applying CoT techniques did not uniformly benefit all methods. While GPT-3.5 saw improvements, GPT-4\u2019s performance declined, suggesting GPT-4\u2019s innate proficiency in inconsistency detection without CoT. Additionally, CoT might introduce biases that could negatively influence outcomes."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we advance the field of LLM-based summary factual inconsistency detection by evaluating the performance of the latest GPT models, thereby establishing new benchmarks for future research. We introduce SIFiD, a novel, efficient, and effective approach that computes a relevance matrix at the sentence level between the document and its summary. This method filters out irrelevant sentences from the document before employing LLMs for inconsistency detection. Our experimental findings on the SummaC dataset demonstrate that SIFiD significantly enhances the performance of advanced GPT models in detecting factual inconsistencies, highlighting its potential to facilitate more accurate and resource-efficient research in this domain."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Experiment results on <span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T1.2.1\">SummaC</span>. Values in brackets represent balanced accuracy without redesigned prompt template. \u201c+CoT\u201d means using chain-of-thought method.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.3\" style=\"width:303.5pt;height:207.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-111.6pt,76.3pt) scale(0.57634,0.57634) ;\">\n<p class=\"ltx_p\" id=\"S4.T1.3.1\"><span class=\"ltx_text\" id=\"S4.T1.3.1.1\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T1.3.1.1.1\" style=\"width:526.7pt;height:360pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\" id=\"S4.T1.3.1.1.1.1\"><span class=\"ltx_text\" id=\"S4.T1.3.1.1.1.1.1\" style=\"color:#000000;\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.3.1.1.1.1.1.1\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.1.1\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.2.1\">CoGenSum</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.3.1\">XsumFaith</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.4.1\">Polytope</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.5.1\">FactCC</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.6.1\">SummEval</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.7.1\">FRANK</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.1.1.8.1\">Avg.</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.2.1\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T1.3.1.1.1.1.1.1.2.1.1\">DAE</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.3.1.1.1.1.1.1.2.1.2\">63.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.3.1.1.1.1.1.1.2.1.3\">50.8</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.3.1.1.1.1.1.1.2.1.4\">62.8</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.3.1.1.1.1.1.1.2.1.5\">75.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.3.1.1.1.1.1.1.2.1.6\">70.3</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.3.1.1.1.1.1.1.2.1.7\">61.7</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.3.1.1.1.1.1.1.2.1.8\">64.2</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.3.2\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.3.2.1\">FEQA</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.3.2.2\">61.0</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.3.2.3\">56.0</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.3.2.4\">57.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.3.2.5\">53.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.3.2.6\">53.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.3.2.7\">69.9</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.3.2.8\">58.7</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.4.3\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.4.3.1\">QuestEval</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.4.3.2\">62.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.4.3.3\">62.1</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.4.3.4\">70.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.4.3.5\">66.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.4.3.6\">72.5</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.4.3.7\">82.1</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.4.3.8\">69.4</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.5.4\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.1.1\">SummaC</span>-ZS</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.2\">70.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.3\">58.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.4\">62.0</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.5\">83.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.6\">78.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.7\">79.0</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.5.4.8\">72.1</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.6.5\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.1.1\">SummaC</span>-Conv</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.2\">64.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.3.1\">66.4</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.4\">62.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.5\">89.5</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.6\">81.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.7\">81.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.6.5.8\">74.43</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.7.6\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.7.6.1\"><cite class=\"ltx_cite ltx_citemacro_citet\">Luo et\u00a0al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.07557v1#bib.bib8\" title=\"\">2023  ###reference_b8###</a>)</cite></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.7.6.2\">63.3</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.7.6.3\">64.7</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.7.6.4\">56.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.7.6.5\">74.7</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.7.6.6\">76.5</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.7.6.7\">80.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.7.6.8\">69.5</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.8.7\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.8.7.1\">\u00a0\u00a0\u00a0\u00a0+CoT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.8.7.2\">74.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.8.7.3\">63.1</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.8.7.4\">61.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.8.7.5\">79.5</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.8.7.6\">83.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.8.7.7\">82.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.8.7.8\">74.0</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.9.8\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.9.8.1\">GPT-3.5 Turbo</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.9.8.2\">59.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.9.8.3\">67.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.9.8.4\">41.0(57.9)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.9.8.5\">71.3</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.9.8.6\">81.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.9.8.7\">80.2</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.9.8.8\">66.9(69.7)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.10.9\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.10.9.1\">\u00a0\u00a0\u00a0\u00a0+CoT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.10.9.2\">65.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.10.9.3\">62.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.10.9.4\">49.5(59.1)</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.10.9.5\">79.1</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.10.9.6\">77.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.10.9.7\">81.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.10.9.8\">69.2(70.8)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.11.10\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.11.10.1\">SIFiD-Entailment</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.11.10.2\">65.5</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.11.10.3\">63.9</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.11.10.4\">37.5</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.11.10.5\">81.0</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.11.10.6\">79.0</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.11.10.7\">81.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.11.10.8\">68.1</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.12.11\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.12.11.1\">\u00a0\u00a0\u00a0\u00a0+CoT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.12.11.2\">65.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.12.11.3\">60.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.12.11.4\">52.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.12.11.5\">82.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.12.11.6\">79.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.12.11.7\">81.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.12.11.8\">70.3</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.13.12\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.13.12.1\">SIFiD-Similarity</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.13.12.2\">65.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.13.12.3\">64.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.13.12.4\">35.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.13.12.5\">76.0</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.13.12.6\">74.5</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.13.12.7\">80.1</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.13.12.8\">66.0</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.14.13\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.14.13.1\">\u00a0\u00a0\u00a0\u00a0+CoT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.14.13.2\">64.3</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.14.13.3\">59.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.14.13.4\">52.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.14.13.5\">81.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.14.13.6\">76.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.14.13.7\">80.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.14.13.8\">69.2</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.15.14\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.1\">GPT-4 Turbo</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.2\">80.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.3\">61.0</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.4\">66.0(60.9)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.5\">89.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.6.1\">88.0</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.7\">87.4</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.1.1.1.1.1.15.14.8\">78.8(78.0)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.16.15\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.1\">\u00a0\u00a0\u00a0\u00a0+CoT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.2\">80.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.3.1\">66.4</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.4\">62.1(61.4)</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.5\">87.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.6\">86.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.7\">85.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.16.15.8\">78.1(78.0)</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.17.16\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.1\">SIFiD-Entailment</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.2\">82.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.3\">58.9</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.4.1\">74.4</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.5\">89.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.6\">87.5</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.7\">86.1</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.17.16.8.1\">79.9</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.18.17\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.1\">\u00a0\u00a0\u00a0\u00a0+CoT</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.2.1\">83.2</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.3\">60.6</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.4\">61.7</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.5\">89.4</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.6\">87.1</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.7\">85.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.18.17.8\">78.0</span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.19.18\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.1\">SIFiD-Similarity</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.2\">83.1</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.3\">60.2</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.4\">71.0</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.5.1\">90.6</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.6\">86.8</span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.7.1\">87.7</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1.1.19.18.8.1\">79.9</span></span></span>\n<span class=\"ltx_tr\" id=\"S4.T1.3.1.1.1.1.1.1.20.19\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T1.3.1.1.1.1.1.1.20.19.1\">\u00a0\u00a0\u00a0\u00a0+CoT</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.1.1.1.1.1.20.19.2\">82.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.1.1.1.1.1.20.19.3\">65.0</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.1.1.1.1.1.20.19.4\">69.3</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.1.1.1.1.1.20.19.5\">91.3</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.1.1.1.1.1.20.19.6\">84.6</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.1.1.1.1.1.20.19.7\">86.0</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.1.1.1.1.1.20.19.8\">79.8</span></span>\n</span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n</figure>",
            "capture": "Table 1: Experiment results on SummaC. Values in brackets represent balanced accuracy without redesigned prompt template. \u201c+CoT\u201d means using chain-of-thought method."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.07557v1_figure_1.png",
            "caption": "Figure 1: An illustration of SIFiD. The Score could either be entailment score or semantic cosine similarity."
        }
    },
    "references": [
        {
            "1": {
                "title": "Language models are few-shot learners.",
                "author": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.",
                "venue": "Advances in neural information processing systems, 33:1877\u20131901.",
                "url": null
            }
        },
        {
            "2": {
                "title": "A survey on evaluation of large language models.",
                "author": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023.",
                "venue": "ACM Transactions on Intelligent Systems and Technology.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization.",
                "author": "Esin Durmus, He He, and Mona Diab. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055\u20135070.",
                "url": null
            }
        },
        {
            "4": {
                "title": "Assessing the factual accuracy of generated text.",
                "author": "Ben Goodrich, Vinay Rao, Peter J Liu, and Mohammad Saleh. 2019.",
                "venue": "In proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 166\u2013175.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Evaluating factuality in generation with dependency-level entailment.",
                "author": "Tanya Goyal and Greg Durrett. 2020.",
                "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592\u20133603.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Evaluating the factual consistency of abstractive text summarization.",
                "author": "Wojciech Kry\u015bci\u0144ski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332\u20139346.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Summac: Re-visiting nli-based models for inconsistency detection in summarization.",
                "author": "Philippe Laban, Tobias Schnabel, Paul Bennett, and Marti A Hearst. 2022.",
                "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Chatgpt as a factual inconsistency evaluator for text summarization.",
                "author": "Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023.",
                "venue": null,
                "url": "http://arxiv.org/abs/2303.15621"
            }
        },
        {
            "9": {
                "title": "Multi-document summarization via deep learning techniques: A survey.",
                "author": "Congbo Ma, Wei Emma Zhang, Mingyu Guo, Hu Wang, and Quan Z Sheng. 2022.",
                "venue": "ACM Computing Surveys, 55(5):1\u201337.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Training language models to follow instructions with human feedback.",
                "author": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.",
                "venue": "Advances in Neural Information Processing Systems, 35:27730\u201327744.",
                "url": null
            }
        },
        {
            "11": {
                "title": "On extractive and abstractive neural document summarization with transformer language models.",
                "author": "Jonathan Pilault, Raymond Li, Sandeep Subramanian, and Christopher Pal. 2020.",
                "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9308\u20139319.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Get your vitamin C! robust fact verification with contrastive evidence.",
                "author": "Tal Schuster, Adam Fisch, and Regina Barzilay. 2021.",
                "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 624\u2013643, Online. Association for Computational Linguistics.",
                "url": "https://doi.org/10.18653/v1/2021.naacl-main.52"
            }
        },
        {
            "13": {
                "title": "Questeval: Summarization asks for fact-based evaluation.",
                "author": "Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. 2021.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594\u20136604. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Llama: Open and efficient foundation language models.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.",
                "venue": "arXiv preprint arXiv:2302.13971.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Asking and answering questions to evaluate the factual consistency of summaries.",
                "author": "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020.",
                "url": null
            }
        },
        {
            "16": {
                "title": "Siren\u2019s song in the ai ocean: a survey on hallucination in large language models.",
                "author": "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023.",
                "venue": "arXiv preprint arXiv:2309.01219.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.07557v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3.1",
            "4",
            "4.1"
        ]
    },
    "research_context": {
        "paper_id": "2403.07557v1",
        "paper_title": "SIFiD: Reassess Summary Factual Inconsistency Detection with LLM",
        "research_background": "The motivation, research problem, and relevant prior work for the paper titled \"SIFiD: Reassess Summary Factual Inconsistency Detection with LLM\" can be summarized as follows:\n\n### Motivation\nThe motivation behind this research is driven by the increasing reliance on document summarization facilitated by advancements in Natural Language Generation (NLG) technologies. While Large Language Models (LLMs) have significantly improved the generation of factually consistent summaries, the rapid development in summarization techniques also introduces the risk of generating summaries that are factually inconsistent, even if they appear accurate at first glance. Detecting these inconsistencies is crucial as they can lead to significant challenges, including hallucinated information that is difficult for existing models to identify.\n\n### Research Problem\nThe primary research problem addressed in this paper is the detection of factual inconsistencies in document summaries generated by LLMs. This issue becomes particularly pressing as traditional models struggle to detect subtle factual discrepancies due to their evolving natural language generation capabilities. Previous approaches, such as the one by Luo et al. (2023), have employed LLMs like GPT-3.5 for this task but have faced limitations, including plain application and constraints of early model versions, which prevented them from outperforming traditional methods. Therefore, there is a need for more robust and efficient methods for factual inconsistency detection in summaries using the advanced capabilities of the latest LLMs.\n\n### Relevant Prior Work\nThe paper builds upon several key pieces of prior work:\n1. **Document Summarization Techniques**: Previous advancements in NLG and summarization techniques have been highlighted by studies such as Pilault et al. (2020) and Ma et al. (2022).\n2. **Large Language Models**: The capabilities of LLMs, notably GPT-3.5 and GPT-4, which have shown significant promise in generating natural language and maintaining factual consistency, as discussed by Brown et al. (2020), Ouyang et al. (2022), and Touvron et al. (2023).\n3. **Factual Inconsistency Detection Approaches**: Initial efforts by Luo et al. (2023) to employ LLMs for factual inconsistency detection using a zero-shot prompting approach across benchmarks like SummaC, though it faced limitations in performance.\n4. **Benchmark Datasets**: The use of the SummaC dataset for evaluating factual consistency in summaries, as established by Laban et al. (2022).\n\nThe authors propose a novel methodology named SIFiD, which enhances detection by focusing on identifying and retaining only the most relevant sentences from documents based on entailment scores or semantic similarity to the summary. This approach significantly improves the efficiency and effectiveness of the evaluation process, particularly when integrated with the latest models like GPT-4.\n\nIn conclusion, the paper aims to set new benchmarks for LLM-based factual inconsistency detection techniques while also providing an open-source tool to encourage further research and collaboration in this critical domain.",
        "methodology": "Sure, I'll provide a description of the proposed method or model based on the methodology section provided:\n\n---\n\n**SIFiD: Reassess Summary Factual Inconsistency Detection with LLM**\n\n**Methodology:**\nIn this section, we detail our approach to reevaluating summary factual consistency using the latest GPT models and introduce the novel **SIFiD method**.\n\n---\n\n**Proposed Method or Model:**\n\nThe methodology centers around the innovative **SIFiD method** for detecting factual inconsistencies in summaries. This approach leverages the advanced capabilities of the latest GPT models to reassess and improve the accuracy of factual consistency evaluations. \n\n*Key Components and Innovations:*\n- **Use of Advanced GPT Models:** The method implements cutting-edge GPT models, which are among the most recent advancements in large language models (LLMs). These models are instrumental in achieving greater accuracy and nuanced understanding in textual analysis.\n- **Summary Factual Consistency Reevaluation:** SIFiD focuses on the crucial task of reassessing the factual consistency of summaries. Unlike traditional methods, this approach enhances the detection of errors and discrepancies that might be present in summarized content.\n\nOverall, the combination of state-of-the-art GPT models with a method specifically designed for factual consistency evaluation marks the primary novelty of the SIFiD method.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n#### Datasets:\nThe experiments were conducted on a collection of datasets including CoGenSum, XsumFaith, Polytope, FactCC, SummEval, and FRANK. These datasets are varied in content and complexity, making them suitable for assessing the performance of fact-checking models in summary evaluation.\n\n#### Baselines:\nThe study compared the performance of the proposed methods (SIFiD-Entailment and SIFiD-Similarity) and several versions of GPT models (GPT-3.5 Turbo and GPT-4 Turbo) against a range of state-of-the-art approaches:\n1. **DAE** - Goyal and Durrett (2020)\n2. **FEQA** - Durmus et al. (2020)\n3. **QuestEval** - Scialom et al. (2021)\n4. **SummaC-ZS and SummaC-Conv** - Laban et al. (2022)\n5. **Luo et al. (2023)**\n\n#### Evaluation Metrics:\nPerformance was evaluated using balanced accuracy for the SummaC experiments. This metric ensures an equal emphasis on both precision and recall, providing a fair assessment of the models' factual inconsistency detection capabilities.\n\n### Main Experimental Results\n\nThe results highlighted the performance across different models and methods on the specified datasets. Here are the outcomes summarized:\n\n#### Models without Chain-of-Thought (CoT):\n- **GPT-3.5 Turbo**:\n  - Achieved an average balanced accuracy of 66.9% (69.7% with an alternative scoring).\n\n- **SIFiD-Entailment**:\n  - Reached an average balanced accuracy of 68.1%.\n\n- **SIFiD-Similarity**:\n  - Achieved an average balanced accuracy of 66.0%.\n\n- **GPT-4 Turbo**:\n  - Achieved the highest average balanced accuracy of 78.8% (or 78.0% with an alternative scoring).\n\n- **SummaC-ZS** and **SummaC-Conv**:\n  - SummaC-ZS had an average accuracy of 72.1%.\n  - SummaC-Conv achieved 74.43%.\n\n#### Models with Chain-of-Thought (CoT):\n- **GPT-3.5 Turbo + CoT**:\n  - Improved to an average balanced accuracy of 69.2% (70.8% with an alternative scoring).\n\n- **SIFiD-Entailment + CoT**:\n  - Slightly improved to an average balanced accuracy of 70.3%.\n\n- **SIFiD-Similarity + CoT**:\n  - Demonstrated an improvement, reaching 69.2%.\n\n- **GPT-4 Turbo + CoT**:\n  - Achieved an average balanced accuracy of 78.1% (with an alternative scoring staying stable at 78.0%).\n\nAdditionally, the SIFiD-Similarity model showed consistent performance improvements when paired with CoT over most datasets.\n\n### Conclusions\nThe experiments establish that GPT-4 Turbo and SIFiD models, especially with entailment-based and similarity-based filtering, outperform many traditional and recent state-of-the-art models. Notably, the integration of Chain-of-Thought with these models further enhances their factual inconsistency detection accuracy."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Reevaluate summary factual inconsistency detection using the most recent GPT models and a refined prompt template for the Polytope benchmark.",
            "experiment_process": "Initially, the prompt template used in Luo et al. (2023) was applied to GPT-3.5 Turbo and GPT-4 Turbo on SummaC. Recognizing the distinct requirements of the Polytope benchmark, a tailored prompt template was crafted: classifiers for omissions, additions, and inaccuracies were added. This template directs the model to decide if key points are missing (omission), unnecessary snippets are included (addition), or if some information in the summary is unsupported (inaccuracy). The performances of these models under the original and revised template were then compared.",
            "result_discussion": "The revised prompt template led to higher performance with GPT-4, signifying the advanced capabilities of GPT-4 over GPT-3.5 for handling tailored templates. This adjustment created a new baseline for summary factual inconsistency detection with LLMs.",
            "ablation_id": "2403.07557v1.No1"
        },
        {
            "research_objective": "Assess the performance of GPT models and the SIFiD method on SummaC and set benchmarks against state-of-the-art approaches.",
            "experiment_process": "Experiments were carried out using GPT-3.5, GPT-4, and SIFiD with entailment-based and semantic similarity-based filtering against traditional methods (DAE, FEQA, QuestEval, SummaC-ZS, SummaC-Conv) and Luo et al. (2023)'s LLM-based method. The balanced accuracy for SummaC was reported, and the evaluation settings included using specific configurations like 'gpt-3.5-turbo-1106', 'gpt-4-1106-preview', and sentence-transformers 'all-mpnet-base-v2'. SIFiD's entailment and similarity filtering resulted in average sentence removal rates of 61.3% and 67% across benchmarks, respectively.",
            "result_discussion": "Key findings were: GPT-4 significantly outperforms GPT-3.5, emphasizing its superior language comprehension. Benchmark-specific prompt templates proved crucial for enhancing performance, especially with complex benchmarks like Polytope. SIFiD significantly improved GPT-4's accuracy in factual inconsistency detection, but did not yield similar improvements for GPT-3.5. Finally, Chain-of-Thought (CoT) techniques offered mixed results, improving GPT-3.5\u2019s performance but not benefiting or even reducing GPT-4's performance, suggesting that GPT-4's proficient baseline capabilities.",
            "ablation_id": "2403.07557v1.No2"
        }
    ]
}