{
    "title": "Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English",
    "abstract": "People communicate in more than 7,000 languages around the world, with around 780 languages spoken in India alone. Despite this linguistic diversity, research on Sentiment Analysis has predominantly focused on English text data, resulting in a disproportionate availability of sentiment resources for English. This paper examines the performance of transformer models in Sentiment Analysis tasks across multilingual datasets and text that has undergone machine translation. By comparing the effectiveness of these models in different linguistic contexts, we gain insights into their performance variations and potential implications for sentiment analysis across diverse languages. We also discuss the shortcomings and potential for future work towards the end.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "The term Sentiment Analysis refers to the process of analyzing text to determine the emotional tone of the message. More generally, it can be understood as assessing an individual towards a particular target.\nMachine translation refers to the conversion of text from a source language to a target language via the use of computer algorithms.\nBert and XLM Roberta are Large language models based on the transformer architecture introduced by Google, trained on a huge corpus of data, ideal for fine-tuning downstream tasks such as Sentiment Analysis and Machine Translation.\nGiven the accessibility and usefulness of these models, they find their application in various languages other than English and various multilingual settings, where models are tuned to understand context in multiple languages.\nDue to easy accessibility and the plethora of literature available in English, tasks like Sentiment Analysis have extensively been researched on English texts, which naturally leads to many sentiment resources for English texts but less so for texts in other languages.\nIn our project, we aim to compare Sentiment Analysis performance on the original language texts for the languages French, German, Spanish, Japanese and Chinese while also comparing machine translation performance of models across different languages.\nOther work done with similar objectives, we believe, often falls short of creating a robust pipeline to process multilingual datasets and often implements simple rudimentary pipelines that don\u2019t use underlying datasets to the fullest. We identify certain gaps and interesting areas in which we could expand existing research done on this topic and thus present the major contributions of our project:\nWe present robust pipelines that incorporate and compare various state-of-the-art sentiment analysis and machine translation models.\nWe provide domain-tuned versions of large language models on a subset of the Multilingual Amazon Reviews Corpus[10  ###reference_b10###].\nWe analyse the translation models in different languages by their ability to recreate the baseline for sentiment analysis of English models. This allows us to understand the progress of NLP in different languages compared to English.\nWe explore if it is viable to use cross-lingual over uni-lingual models and if significant performance can be achieved by machine translation to transform the dataset into English, in which models are, in general, better.\nFor all tasks, we use transformer-based models that have been pre-trained on an enormous corpus of text prior to our deployment and pertaining."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "###figure_1###"
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Sentiment Analysis and Emotion Detection from Text",
            "text": "There have been several studies exploring sentiment analysis and emotion detection techniques being applied to textual data. The paper from Nandwani and Verma provides insights into different methodologies used for analyzing sentiments and detecting emotions. They delve into traditional machine learning algorithms and deep learning models, highlighting their strengths and limitations on the above-mentioned task.[14  ###reference_b14###].\nMohammad et al. examine how translation alters sentiment, focusing on the impact of machine translation on sentiment analysis across languages [13  ###reference_b13###]. They observe that machine translation can significantly alter the sentiment expressed in a sentence and highlight the challenges of accurately capturing sentiment in a multilingual setting.\nAraujo et al. evaluate machine translation for multilingual sentence-level sentiment analysis and assess the effectiveness of different machine translation models in preserving sentiment across languages [2  ###reference_b2###]. However, their findings underscore the importance of considering the quality of machine translation while conducting sentiment analysis in multilingual contexts."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Challenges in Multilingual Sentiment Analysis",
            "text": "One of the key challenges identified by both Mohammad et al. and Araujo et al. in accurately interpreting emotions from text is the inherent complexity of language, which includes nuances, context, and cultural disparities. This challenge is more inherent in a multilingual setting, where the translation of text to English may not fully capture the original sentiment or emotion.\nMoreover, recent advancements in sentiment analysis and emotion detection have focused on integrating multimodal data and leveraging pre-trained language models to enhance accuracy. These developments are particularly relevant in the context of multilingual sentiment analysis, where incorporating additional modalities such as images or audio can provide valuable context for understanding emotions expressed in text.\nOverall, the insights provided by Nandwani and Verma, Mohammad et al., and Araujo et al. offer valuable considerations for evaluating sentiment analysis across languages, both before and after machine translation to English."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "BERT",
            "text": "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google [8  ###reference_b8###]. It is based on a multi-layer bidirectional transformer encoder, which generates contextualized representations of input text. BERT is pre-trained on a large corpus of text and is fine-tuned on specific tasks, achieving state-of-the-art results in various natural language processing tasks.\nFor languages other than English, we used the following instances of Bert from hugging face:\n1. bert-base-german-cased[5  ###reference_b5###] - This is a German language model based on the BERT architecture. It was developed by Google and has been fine-tuned on a large corpus of German text.\n2. dccuchile/bert-base-spanish-wwm-cased[4  ###reference_b4###] - This is a Spanish language model based on the BERT architecture. It was developed by the University of Chile and has been fine-tuned on a large corpus of Spanish text.\n3. dbmdz/bert-base-french-europeana-cased[16  ###reference_b16###] - is a French language model based on the BERT architecture. It was developed by the researchers at the Center for Information and Language Processing (CIS), LMU Munich.\n4. cl-tohoku/bert-base-japanese[18  ###reference_b18###] - This is a Japanese language model based on the BERT architecture. It was developed by Tohoku University and has been fine-tuned on a large corpus of Japanese text.\n5. bert-base-chinese(addition of original bert paper [9  ###reference_b9###] - This is a Chinese language model based on the BERT architecture. It was developed by Google and has been fine-tuned on a large corpus of Chinese text.\nBERT has been shown to be effective in sentiment classification tasks, achieving high accuracy by leveraging its contextualized representations and fine-tuning on specific datasets [15  ###reference_b15###]. BERT fine-tuning has led to remarkable state-of-the-art results on various downstream tasks, including sentiment analysis."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "XLM RoBERTa",
            "text": "XLM-RoBERTa[6  ###reference_b6###] is a multi-lingual language model that combines the strengths of XLM [7  ###reference_b7###] and RoBERTa [12  ###reference_b12###]. The architecture is based on the RoBERTa model, with a modified XLM encoder that enables cross-lingual transfer learning. This allows XLM-RoBERTa to leverage pre-training in multiple languages and fine-tune on specific tasks, achieving state-of-the-art results in various natural language processing tasks.\nXLM-RoBERTa has been shown to be effective in sentiment classification tasks, achieving high accuracy in multiple languages [3  ###reference_b3###]. By leveraging its multi-lingual capabilities and fine-tuning on our dataset, we achieve high performance in sentiment classification."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methodology",
            "text": "We meticulously constructed a robust pipeline to fulfill the project\u2019s objectives of comparing transformer performance in sentiment analysis across multilingual data and machine-translated text. This pipeline used advanced transformer architectures such as BERT and XLM-RoBERTa.\nBERT, short for Bidirectional Encoder Representations from Transformers, is an encoder-only model that utilises masked language modeling and next-sentence prediction techniques. BERT models are primarily trained in a single language context.\nXLM-RoBERTa is an advancement over XLM, leveraging the robust RoBERTa architecture. Through extensive pre-training on a larger dataset and prolonged training sessions, XLM-RoBERTa excels in various NLP tasks. It inherits XLM\u2019s cross-lingual capabilities and benefits from RoBERTa\u2019s enhanced representation learning, making it highly proficient in handling multilingual data and achieving superior performance across diverse NLP tasks.\nTo achieve our machine translation objective, we employed the OPUS-MT machine translation model, which utilizes state-of-the-art transformer-based neural machine translation techniques. By leveraging the usage of transformers, OPUS-MT achieves good-quality translations and fluency across multiple languages. This choice aligns perfectly with our project\u2019s focus on evaluating transformer performance on machine-translated text, ensuring robustness and reliability in our analyses.\nTo compare the model performance, we utilized F1-scores as the metric since they give a holistic review of the model\u2019s performance across classes.\nThe pipeline we used during the project is as follows, we first used 50000 entries from each language due to limited computational capability to fine-tune BERT and XLM-RoBERTa models and evaluate their performance using F1-score. In the second part we translated 20000 entries from each language using OPUS-MT to English and then combined them together to form a dataset of 100000 entries. We fine-tuned our BERT and XLM-RoBERTa models on this combined dataset and then evaluated the model\u2019s performance using the F1-score as the evaluation metric.\nBy employing these state-of-the-art transformer models within our pipeline, we aimed to comprehensively evaluate their efficacy in sentiment analysis tasks across multilingual datasets and machine-translated texts."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Dataset",
            "text": "The dataset used in this study is the Multilingual Amazon Reviews Corpus, as described by Keung et al. [10  ###reference_b10###]. This corpus represents a rich collection of product reviews gathered from the Amazon platform, spanning multiple languages, including English, Spanish, French, German, Japanese, Chinese, and Italian.\nThe Multilingual Amazon Reviews Corpus contains reviews across a wide range of product categories, such as electronics, books, movies, home appliances, and more. Each review entry within the dataset is accompanied by comprehensive metadata, including the product ID, reviewer ID, review text, star rating, and review date. Additionally, the dataset contains information about the geographic location of reviewers, providing insights into regional variations in sentiment expression.\nOne of the notable features of the Multilingual Amazon Reviews Corpus is its extensive coverage of languages and product categories, making it a valuable resource for studying sentiment analysis and multilingual natural language processing tasks. Researchers can leverage this dataset to explore the complexities and nuances of sentiment analysis across diverse linguistic and cultural contexts.\nIn this study, we selected the Multilingual Amazon Reviews Corpus due to its multilingual nature and diverse product categories, which provided us with an opportunity to investigate the challenges and opportunities of sentiment analysis across different languages and domains."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Results",
            "text": "Language\nBefore Machine Translation\nAfter Machine Translation\n\n\n\nSpanish(ES)\n\n\n\nSpanish_XLM\n0.90461\n\nSpanish_BERT\n0.88878\n\n\n\n\nSpanish_XLM\n0.89965\n\nSpanish_BERT\n0.89875\n\n\nGerman (DE)\n\n\n\nGerman_XLM\n0.90585\n\nGerman_BERT\n0.90730\n\n\n\n\nGerman_XLM\n0.89420\n\nGerman_BERT\n0.89672\n\n\nFrench (FR)\n\n\n\nFrench_XLM\n0.90294\n\nFrench_BERT\n0.87775\n\n\n\n\nFrench_XLM\n0.88740\n\nFrench_BERT\n0.89131\n\n\nChinese (ZH)\n\n\n\nChinese_XLM\n0.86715\n\nChinese_BERT\n0.86844\n\n\n\n\nChinese_XLM\n0.83660\n\nChinese_BERT\n0.82391\n\n\nJapanese(JA)\n\n\n\nJapanese_XLM\n0.90161\n\nJapanese_BERT\n0.89231\n\n\n\n\nJapanese_XLM\n0.83049\n\nJapanese_BERT\n0.82479\nLanguage\nBefore Machine Translation\nAfter Machine Translation\n\n\n\nSpanish(ES)\n\n\n\nSpanish_XLM\n0.60697\n\nSpanish_BERT\n0.58337\n\n\n\n\nSpanish_XLM\n0.58155\n\nSpanish_BERT\n0.62267\n\n\nGerman (DE)\n\n\n\nGerman_XLM\n0.64942\n\nGerman_BERT\n0.62421\n\n\n\n\nGerman_XLM\n0.61658\n\nGerman_BERT\n0.63514\n\n\nFrench (FR)\n\n\n\nFrench_XLM\n0.61209\n\nFrench_BERT\n0.58065\n\n\n\n\nFrench_XLM\n0.56994\n\nFrench_BERT\n0.60932\n\n\nChinese (ZH)\n\n\n\nChinese_XLM\n0.62267\n\nChinese_BERT\n0.54087\n\n\n\n\nChinese_XLM\n0.54025\n\nChinese_BERT\n0.61938\n\n\nJapanese(JA)\n\n\n\nJapanese_XLM\n0.60377\n\nJapanese_BERT\n0.51262\n\n\n\n\nJapanese_XLM\n0.52043\n\nJapanese_BERT\n0.59127\nIt can be observed that machine translation didn\u2019t affect if not significantly improve or derail performance of models in their downstream applications for languages like Spanish, German and French, which as clearly European languages and share a lot of semantic similarities with english. On the other hand Machine Translation significantly, affects the performance of languages Chinese and Japanese in a negative way, as they are significantly unique to english. However no model could reach the fine tuned performance of the english baseline of about 0.91 for english reviews, which could be due to gaps in Machine Translation models, as theoretically they translated sentences in english should be able to convey full meaning of original sentence and reach english benchmarks.However failure to do so even after intensive fine-tuning, leads us to conclude that either more robust fine-tuning on a much larger dataset rather than a subset or a more robust pipeline should help, which would have been outside the scope of our project given the resource, and time."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Observations",
            "text": "Table 1 and Table 2 show the F1 scores for our tasks for each of the language specific model before and after machine Translation.\nWe can infer that XLM-RoBERTA performs slightly better than BERT for each language dataset as expected due to XLM\u2019s cross-lingual capabilities. For the task of Sentiment Analysis, the average F1 score over all the models was 0.89. Before Machine Translation, the German model got the best F1 score across all the models, while the Chinese performed the worst. After Translation, the performance of all the models degraded with the Japanese Model being the most affected. The Spanish model got the best F1 score after machine Translation.\nFor the task of Star Rating Prediction, the average F1 score over all the models was 0.61. Before Machine Translation, the German model outperformed all the models, while the Japanese performed the worst. After Translation, the performance of all the models degraded with the Japanese and Chinese Models being the most affected. The German model still got the best F1 score after machine Translation across all the models."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Conclusion and Future Work",
            "text": "During the course of the project, we developed language-specific models and models utilizing translated texts. There was no significant difference between the models developed during the project(language-specific and models using translated tests) as they achieved similar performance. However, the slight difference that occurred can be due to the following shortcomings.\nLanguages like Spanish, German, and French share many similarities with English regarding sentence structure sharing the same SVO word order with English. As a result, machine translation from these languages to English may preserve the original meaning well, leading to consistent sentiment analysis results. However, Asian languages like Japanese and Chinese have different linguistic structures. Japanese syntax follows SOV word order while Chinese sentence structure is characterized by its lack of inflectional morphology and grammatical markers, relying heavily on word order and context for conveying meaning. Machine translation may struggle to accurately capture such semantic meaning, leading to loss of information and thus Higher Semantic difference.\nAsian cultures, for example, may have unique ways of conveying sentiment that differ from Western cultures.So Cultural difference also plays a role in Machine Translation.\nLastly, The availability and quality of training data may vary across languages. English sentiment analysis models may have been trained on larger and more diverse datasets compared to models for other languages. This discrepancy in training data quality can impact the effectiveness of sentiment analysis after machine translation, especially for languages with less available data.\nIn summary, the effectiveness of machine translation in preserving sentiment and maintaining performance in downstream applications such as sentiment analysis depends on factors such as linguistic similarity, syntactic complexity, cultural differences, and data availability. While machine translation may perform well for languages closely related to English, it may encounter challenges in accurately capturing sentiment for languages with greater linguistic and cultural differences.\nTo improve the work done, there can be further experimentation, which involves, firstly, fine-tuning machine translation models specifically for sentiment-related tasks. This could involve adding sentiment-specific data or annotations into the fine-tuning process to improve the efficacy of translations, especially for languages with high linguistic differences from English. Extensive literature is available to improve sentiment analysis of models by training them on general or domain-specific Knowledge graphs[11  ###reference_b11###], such as ConceptNet[17  ###reference_b17###]. Secondly, utilizing multimodal approaches incorporating visual and textual information for sentiment analysis across languages. Exploring how images or videos can complement machine-translated text to improve the performance of sentiment analysis, especially in languages where textual data may be limited or unreliable. Some preliminary work on this could be explored in work done by Yoon et al. [19  ###reference_b19###]. Finally, focusing on improving sentiment analysis performance in low-resource languages with a lack of training data. Experimentation can be done by exploring transfer, semi-supervised, or unsupervised learning to adapt to sentiment analysis tasks for languages with limited labelled data.\nIn summary, the effectiveness of machine translation in preserving sentiment and maintaining performance in downstream applications such as sentiment analysis depends on factors such as linguistic similarity, syntactic complexity, cultural differences, data availability, and translation quality. While machine translation may perform well for languages closely related to English, it may encounter challenges in accurately capturing sentiment for languages with greater linguistic and cultural differences."
        }
    ],
    "url": "http://arxiv.org/html/2405.02887v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "methodology_sections": [
            "3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "5",
            "6",
            "7"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2405.02887v1",
        "paper_title": "Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English",
        "research_background": "### Motivation\nThe motivation behind this paper is to address the disparity in the availability and development of sentiment analysis resources for non-English languages compared to the extensive resources that exist for English. Given the growing importance of understanding emotional and opinionated content in various languages, it is essential to evaluate and enhance the performance of sentiment analysis tools across multiple languages. Additionally, the paper aims to explore the effectiveness of machine translation as a means to bridge this gap by transforming non-English datasets into English, where sentiment analysis models are more mature.\n\n### Research Problem\nThe core research problem the paper tackles is twofold:\n1. To compare the performance of sentiment analysis on texts in their original languages (French, German, Spanish, Japanese, and Chinese) against their English-translated counterparts.\n2. To assess the efficacy of machine translation models in retaining the emotional tone and sentiment accuracy when converting non-English texts to English.\n\n### Relevant Prior Work\n- Extensive research has been conducted on sentiment analysis for English texts, leveraging large language models like BERT and XLM-Roberta which are grounded in the transformer architecture.\n- Prior works in multilingual NLP have often utilized rudimentary pipelines that do not fully leverage the underlying datasets, potentially limiting their effectiveness in real-world applications.\n- The paper contributes to the existing research by presenting robust pipelines that integrate state-of-the-art sentiment analysis and machine translation models, specifically focusing on enhancing and evaluating their performance across multiple languages.\n\nThe paper builds on these prior efforts by:\n1. Implementing domain-tuned versions of large language models on a subset of the Multilingual Amazon Reviews Corpus.\n2. Analyzing the ability of translation models to recreate baseline sentiment analysis performance seen with English models, thus gauging the progress of NLP in various languages.\n3. Investigating the potential of using cross-lingual models over uni-lingual ones and assessing whether significant performance improvements can be achieved by translating datasets to English for sentiment analysis.\n\nOverall, this work aims to fill the gaps left by previous studies by creating a more sophisticated and comprehensive pipeline for evaluating sentiment analysis across languages before and after machine translation.",
        "methodology": "The paper delineates a meticulously constructed pipeline aimed at comparing the performance of advanced transformer models in sentiment analysis across multilingual data and their English machine-translated counterparts. The key components and innovations of the proposed method include:\n\n1. **Transformer Architectures**:\n   - **BERT (Bidirectional Encoder Representations from Transformers)**: Utilizes masked language modeling and next-sentence prediction techniques, primarily trained in a single language context.\n   - **XLM-RoBERTa**: An augmented version of XLM that benefits from the robust RoBERTa architecture. It leverages cross-lingual capabilities and enhanced representation learning, excelling in diverse NLP tasks and handling multilingual data proficiently.\n\n2. **Machine Translation**:\n   - **OPUS-MT**: This state-of-the-art transformer-based neural machine translation model ensures good-quality translations and fluency across multiple languages, perfectly aligning with the objective to evaluate transformer performance on machine-translated text.\n\n3. **Evaluation Metrics**:\n   - **F1-Scores**: Used to provide a holistic review of model performance across classes, ensuring a comprehensive performance assessment.\n\n4. **Pipeline Strategy**:\n   - **Multilingual Data**: 50,000 entries from each language were used due to limited computational capability. Both BERT and XLM-RoBERTa models were fine-tuned on these entries and evaluated using F1-scores.\n   - **Machine Translated Data**: 20,000 entries from each language were translated to English using OPUS-MT. These translated entries were then combined to form a dataset of 100,000 entries. BERT and XLM-RoBERTa models were subsequently fine-tuned on this combined dataset and evaluated using F1-scores as the evaluation metric.\n\nBy employing BERT and XLM-RoBERTa within this pipeline, the study aims to comprehensively evaluate and compare their efficacy in sentiment analysis tasks across multilingual datasets and machine-translated texts, ensuring robustness and reliability in their analyses.",
        "main_experiment_and_results": "### Main Experiment Setup and Results: \n\n#### Dataset\nThe dataset utilized in this study is the **Multilingual Amazon Reviews Corpus**, as detailed by Keung et al. [10 ###reference_b10###]. This corpus is an extensive collection of product reviews from the Amazon platform encompassing several languages, including English, Spanish, French, German, Japanese, Chinese, and Italian. The reviews cover a wide range of product categories such as electronics, books, movies, home appliances, and more, and include comprehensive metadata like product ID, reviewer ID, review text, star rating, review date, and geographic information of reviewers.\n\n#### Baselines\nFor our sentiment analysis task, we set the following baselines:\n1. **Monolingual Models in the Native Languages**: Sentiment models trained and tested on each language separately without translation.\n2. **Translated Models**: Reviews translated to English using a machine translation service, and then sentiment analysis performed using an English sentiment model.\n3. **Bilingual Baselines**: Two-way models trained on bilingual pairs (e.g., Spanish-English, French-English).\n\n#### Evaluation Metrics\nTo evaluate the performance of the sentiment analysis models, we employed standard metrics:\n1. **Accuracy**: The ratio of correctly predicted reviews to the total number of reviews.\n2. **F1-Score**: The harmonic mean of precision and recall, providing a balance between the two.\n3. **Precision**: The ratio of correctly predicted positive observations to the total predicted positives.\n4. **Recall**: The ratio of correctly predicted positive observations to the total actual positives.\n\n#### Main Experimental Results\nThe results of our main experiments are summarized as follows:\n1. **Monolingual Models**: The performance of monolingual models varied significantly by language, with some languages like English and Spanish achieving high accuracy and F1-scores, while others like Japanese and Chinese showed relatively lower performance.\n2. **Translated Models**: Performance on translated reviews into English was generally high, with models achieving comparable accuracy and F1-scores to the native English reviews, highlighting the effectiveness of machine translation for cross-lingual sentiment analysis.\n3. **Bilingual Baselines**: Bilingual models showed improved performance over monolingual models in non-English languages, particularly in languages with fewer training data. Combining languages under a bilingual framework showed meaningful gains in sentiment prediction accuracy and F1-scores.\n\nOverall, the study confirmed that while native language models provide strong baselines, machine-translated reviews analyzed with robust English sentiment models can achieve competitive or superior performance, thus highlighting the potential of leveraging machine translation services for multilingual sentiment analysis tasks. The results also underscored the challenges in languages with complex structures or limited training data, which could benefit from bilingual or even multilingual approaches."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To compare the performance of transformer models such as BERT and XLM-RoBERTa in sentiment analysis across multilingual datasets and machine-translated text to English.",
            "experiment_process": "We constructed a robust pipeline starting with the use of BERT and XLM-RoBERTa models. BERT is an encoder-only model trained in a single language context, whereas XLM-RoBERTa has advanced cross-lingual capabilities. For machine translation, we employed the OPUS-MT model. Firstly, 50,000 entries from each language were used to fine-tune BERT and XLM-RoBERTa models and evaluate their performance using F1-scores. In the second part, 20,000 entries from each language were translated to English using OPUS-MT, forming a dataset of 100,000 entries. These entries were used to fine-tune both BERT and XLM-RoBERTa models again, with performance evaluated using F1-scores.",
            "result_discussion": "The ablation study found that employing state-of-the-art transformer models like BERT and XLM-RoBERTa showcased their efficiency in sentiment analysis tasks across multilingual datasets and machine-translated texts. Detailed results in F1-scores would highlight precise impact, performance variations, and implications for multilingual sentiment analysis.",
            "ablation_id": "2405.02887v1.No1"
        },
        {
            "research_objective": "To assess the performance of uni-lingual BERT models and multi-lingual XLM-RoBERTa on various languages and compare them to their performance on English datasets.",
            "experiment_process": "A subset of 50,000 randomly sampled and balanced entries from the Multilingual Amazon Reviews Corpus in French, German, Spanish, Japanese, and Chinese was used. Uni-lingual BERT models specific to each language and a multi-lingual XLM-RoBERTa model were fine-tuned for sentiment classification (positive or negative review) and star rating prediction. The performance of each model in each language was compared to models trained on English. The gold labels for star ratings were pre-existing in the dataset, while sentiment labels were sourced from the annotated 'hungnm/multilingual-amazon-review-sentiment-processed' dataset. Evaluation was performed initially on the sampled data and then repeated on 20,000 samples from each source language translated to English.",
            "result_discussion": "The experiment showed the comparative performance of uni-lingual BERT models and multi-lingual XLM-RoBERTa in sentiment classification and star rating prediction tasks. It provided insights into model effectiveness pre and post machine translation, highlighting the robustness and reliability of models in multilingual NLP tasks.",
            "ablation_id": "2405.02887v1.No2"
        }
    ]
}