{
    "title": "General Purpose Verification for Chain of Thought Prompting",
    "abstract": "Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency. We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation. The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint. To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier. We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets. Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities of performing a diverse range of tasks by framing them as text generation (Brown et al., 2020  ###reference_b6###; Chowdhery et al., 2022  ###reference_b9###; Touvron et al., 2023  ###reference_b33###; OpenAI, 2023  ###reference_b24###; Bubeck et al., 2023  ###reference_b7###, inter alia). Chain-of-Thought prompting Nye et al. (2021  ###reference_b23###); Wei et al. (2022  ###reference_b38###); Chowdhery et al. (2022  ###reference_b9###) further improved their performance on challenging reasoning tasks using a simple trick of generating intermediate steps before giving the final answer allowing the LLM to spread computation over more tokens Goyal et al. (2023  ###reference_b14###).\nHowever, this approach lacks a mechanism to rectify errors in reasoning. While LLMs may eventually reach the correct answer, they might do so via incorrect intermediate reasoning steps, or worse, never reach the correct answer due to earlier mistakes Turpin et al. (2023  ###reference_b34###).\nTo illustrate this, we provide a concrete example in Figure 1  ###reference_###, where the final answer is correct, but the intermediate steps are (i) irrelevant Shi et al. (2023  ###reference_b29###), (ii) contradicting previous steps M\u00fcndler et al. (2023  ###reference_b22###), and (iii) with mathematical errors Patel et al. (2021  ###reference_b26###).\nRecent work Yao et al. (2023  ###reference_b40###); Xie et al. (2023  ###reference_b39###); Pan et al. (2023  ###reference_b25###) has attempted to alleviate these problems by employing a search mechanism or a self-correction mechanism in the spirit of \"System 2\" thinking. Other directions include training a dataset-specific verifier to improve the performance when aggregating multiple reasoning chains Li et al. (2023  ###reference_b18###).\nHowever, all these approaches have dataset-specific adaptations and don\u2019t generalize out-of-the-box.\nIn this work, we explore if catching early mistakes in reasoning chains through problem-agnostic verification can improve reasoning in LLMs. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency and use models, called verifiers, to test for each principle. Each verifier operates on a step Uesato et al. (2022  ###reference_b35###) generated from the step-by-step manner of Chain-of-Thought prompting and assigns a score to that step. We design the verifiers to operate at this granularity so they can detect intermediate mistakes and discourage the LLM from committing to an erroneous reasoning chain. To further steer the generation towards better steps, we use the perplexity of the reasoning step as an additional verifier. We then explore various ways, including Self-Consistency Wang et al. (2022  ###reference_b36###), to aggregate verifier scores and report their downstream task performance.\nWe make the following contributions: (i) we propose a general framework for guiding reasoning in LLMs using verifiers which offers the flexibility to use a problem-agnostic implementation across any reasoning task but also offers the adaptability to use task- and dataset-specific implementations, and\n(ii) we show how using our proposed verifiers can improve reasoning outcomes in LLMs and can also improve existing ensembling techniques like Self-Consistency.\nImportantly, our work is not intended to be an exploration on the best way to use a computational budget to achieve a desired performance, but an exploration of whether the LLM are capable (even if inefficiently) of detecting their own mistakes together with a simple recovering mechanism.\n###figure_1###"
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "We focus here only on LLM-based approaches, and divide previous related work according to\n(i) the generalizability of the prompts used, and\n(ii) how the final answer is generated.\nIn prior work, the prompts used can be categorized based on their level of generality. Some approaches utilize a singular prompt, applying it uniformly across a wide spectrum of datasets and tasks.\nWei et al. (2022  ###reference_b38###) proposed chain-of-thought prompting with in-context examples.\nKojima et al. (2022  ###reference_b17###) then explored zero-shot prompts capable of exhibiting similar behaviors.\nOther recent works explore using LLMs to self-evaluate Yin et al. (2023  ###reference_b42###) and potentially improve upon their generation with the resulting feedback Saunders et al. (2022  ###reference_b27###); Chen et al. (2023  ###reference_b8###); Pan et al. (2023  ###reference_b25###); Shinn et al. (2023  ###reference_b30###).\nBai et al. (2022  ###reference_b3###) use an LLM with in-context examples to detect and edit the responses of a chat model that are harmful or toxic. Madaan et al. (2023  ###reference_b21###) proposes a framework to iteratively self-improve the generations of a LLM.\nYao et al. (2023  ###reference_b40###) tightly integrates an LLM with custom dataset-specific prompts to act as a guiding mechanism in the underlying search space. Hao et al. (2023  ###reference_b15###) expands on this by using a Monte Carlo tree search strategy.\nOther recent work questioned the extent to which using an LLM to evaluate and improve its own generations is viable Huang et al. (2023  ###reference_b16###), a conclusion which we observed as well in our preliminary work and sidestepped by re-sampling instead of asking the LLM to refine.\nImportantly, previous work explored self-evaluation through the lens of task-specific evaluation and prompts, a direction that inherently constrains the broader utility of Large Language Models (LLMs) as general-purpose reasoners. On the other hand, our approach follows a distinct trajectory: we deliberately eschew the use of prompts tailored to individual datasets or tasks.\nA second dimension is that of how the model arrives at the final solution, where we distinguish between methods that take a linear approach Wei et al. (2022  ###reference_b38###); Kojima et al. (2022  ###reference_b17###); Goyal et al. (2023  ###reference_b14###) from the methods that do not Yao et al. (2022  ###reference_b41###); Wang et al. (2023  ###reference_b37###); Long (2023  ###reference_b19###).\nBy linear approaches, we refer to those methods where the final answer is generated token-by-token in one go. On the other hand, non-linear approaches typically include a search mechanism Xie et al. (2023  ###reference_b39###); Yao et al. (2023  ###reference_b40###); Besta et al. (2023  ###reference_b5###) or a self-reflection process Madaan et al. (2023  ###reference_b21###); Pan et al. (2023  ###reference_b25###).\nFor example, recent work explored tightly integrating the LLM to act as a guiding mechanism in the underlying search space Xie et al. (2023  ###reference_b39###); Yao et al. (2023  ###reference_b40###). This involves one LLM generating candidate steps while another LLM assigns single float value as a value score. This value score is derived from an LLM with a dataset-specific prompt and in-context examples.\nWithin this dimension, our approach aligns with the non-linear paradigm. We leverage verifiers to evaluate each step in the solution-generation process, with the overarching aim of guiding the generation towards solutions that receive high scores, as determined by the verifiers. Differently from previous work on self-evaluation, we explore a setting of self-evaluation that is problem-agnostic."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Proposed Method",
            "text": "###figure_2### We propose a novel approach that seamlessly integrates with any given Large Language Model\u2019s (LLM) solution generation process, at both step generation and step evaluation Wei et al. (2022  ###reference_b38###); Wang et al. (2022  ###reference_b36###); Kojima et al. (2022  ###reference_b17###). Our approach consists of two components: a solution generator  and a set of verifiers , where each verifier specializes in a particular qualitative aspect of reasoning. The solution generator  is responsible for generating candidate steps, and each verifier  is responsible for checking whether the candidate step is in compliance with the specific reasoning property. We explore properties that are generally applicable to a wide range of reasoning tasks. We provide an illustrative example in Figure 2  ###reference_###.\nThis section is further organized as follows. We describe the notations and abstractions used in Section 3.1  ###reference_###, the solution generation component in Section 3.2  ###reference_###, the proposed verifiers in Section 3.3  ###reference_###, the procedure to obtain fine-grained scores for reasoning chains in Section 3.4  ###reference_### and how we use verifiers in aggregate in Section 3.5  ###reference_###."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Notation",
            "text": "In the following, we define the notation we adopt throughout the paper.\nA token as  where Vocab represents the set of possible tokens defined by a given vocabulary. We use  to represent a given text, with  denoting the set of all potential texts of varying lengths. Under this notation, we represent a problem as  and a reasoning step as , both presented in free-text form. A solution generator  in the form of a function that takes text as input and returns text as output: . We interpret the output text as a sequence of reasoning steps . For simplicity, we define a reasoning step as the sequence of tokens until a new line, similar to Uesato et al. (2022  ###reference_b35###). A verifier  implemented a function that takes text as input and returns an indicator: . The returned value represents whether the reasoning step satisfies the verifier constraint () or not ().\nWe will next describe the Solution Generator and Step Verification components."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Solution Generation",
            "text": "The solution generator (typically an LLM) operates over a prompt  and generates a sequence of tokens as output: .\nFor our purpose, we concentrate on the correctness at the level of a reasoning step, instead of individual tokens.\nA reasoning step, as defined in this work, represents the sequence of tokens up to the occurrence of a new line Uesato et al. (2022  ###reference_b35###).\nThe next reasoning step can then be generated by conditioning the generator  on both the question  and on the sequence of previously generated reasoning steps , which is initially empty. This conditioning can be expressed as follows: .\nFor the solution generation process, we adopt the zero-shot prompt used in Kojima et al. (2022  ###reference_b17###) which simply appends \"Let\u2019s think step by step\" to the problem question to elicit a chain-of-thought like behavior in the model\u2019s response without any annotated exemplars Wei et al. (2022  ###reference_b38###). Nevertheless, our proposed method is agnostic to the specific implementation of the solution generator."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Step Verification",
            "text": "Our research aims to investigate whether specifying a subset of conditions that an ideal reasoning chain should satisfy and then employing these conditions to score the corresponding reasoning chain can result in improved performance on downstream reasoning tasks.\nTo this end, we explore three general and necessary (but not sufficient) conditions that a given step should satisfy in order for the resulting solution to be sound from a reasoning perspective: (i) Relevance, (ii) Mathematical Accuracy (if applicable), and (iii) Logical Coherence. In this work, we use for our verifiers a set of LLMs provided with a detailed instruction of the task and constraints.111Prompts available in Appendix B  ###reference_###\nWe then map the output of the LLM to  based on its content. If the relevance verifier generates \u201cnot relevant\u201d, for example, we interpret this as a score of .\nImportantly, to verify the generalizability of our proposed methodology, we keep the implementation of our verifiers fixed for all reasoning tasks and for all datasets. We provide an illustrative example of the verifiers we use in Figure 2  ###reference_###.\nWe provide in-context exemplars for the Mathematical Accuracy verifier due to challenges in the LLM\u2019s ability to generate a valid intermediate structured output, a requirement of this verifier\u2019s setup.222We use the same in-context exemplars for all the Mathematical datasets.\nIn addition to the scores from the aforementioned three verifiers, we use the perplexity score of a reasoning step as an additional verifier, in order to encourage the final solution towards text deemed more likely by the LLM.\nWe explain each verifier in greater detail below."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Relevance",
            "text": "The first verifier in our proposed framework is the Relevance verifier, where the goal is to constrain a reasoning step to contribute to the construction of a meaningful solution narrative. We provide a relevant and an irrelevant example in Figure 2  ###reference_###.\nWe begin with a question, a sequence of previous steps, and a candidate step. The Relevance verifier assesses the candidate reasoning steps for their relevance to the problem at hand. In the example provided, calculating how much Mr. Doe spent is irrelevant because the problem asks for how much Mr. Benson spent and there is no connection between them.\nWe acknowledge the inherent subjectivity and nuance associated with determining the relevance of a given reasoning step. However, there are instances where it becomes evident that a reasoning step is distinctly irrelevant, deviating from the coherent solution narrative. For instance, some reasoning steps may veer into speculative or unrelated content, which our Relevance verifier aims to identify."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Mathematical Accuracy",
            "text": "The Mathematical Accuracy constraint enforces the need for each reasoning step to contain correct mathematical calculations. We implement this in a similar manner to Tool-based approaches Schick et al. (2023  ###reference_b28###), working as follows. First, we extract the mathematical formulas (if present) from a sentence as structured output, as depicted in the Verifier\u2019s intermediate output field corresponding to the Mathematical Accuracy constraint in Figure 2  ###reference_###. For each mathematical calculation present, we extract the left-hand side (lhs), the right-hand side (rhs), and the operator (op).\nThen, we programmatically execute the extracted formulas (if any) and compare them using the extracted operator."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Logical Consistency",
            "text": "A third condition for a logically sound argument we use is for the reasoning steps to not contradict each other M. and Mckeon (1941  ###reference_b20###). To this end, we introduce Logical Consistency as our third verifier.\nThis verifier operates over the previous steps and the current candidate step.\nFor example, in Figure 2  ###reference_###, the candidate step - Mr. Benson received a discount of $3. contradicts one of the previous steps, as one of the previous steps already established that Mr. Benson received a discount of $4."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Constraint Satisfaction Score",
            "text": "Except for the Perplexity verifier, all our proposed verifiers output a binary value, representing whether a given reasoning step satisfies the given constraint or not. For example, if the Relevance verifier gives a score of  for a given reasoning step , this means that the given step is deemed as relevant.\nSince the underlying implementation In order to reduce the variance and get a more fine-grained score , we use the expected value: , which we approximate using sampling. Since each verifier is implemented with an LLM, we can sample multiple generations, map each one to a binary value , and then average."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Using Verifiers in Aggregate",
            "text": ""
        },
        {
            "section_id": "3.5.1",
            "parent_section_id": "3.5",
            "section_name": "3.5.1 Scoring a Reasoning Chain",
            "text": "Given a verifier , we extend the concept of a score for a given reasoning step  to the score for a given (partial or not) reasoning chain  by aggregating the scores over each of its constituent reasoning steps.\nFormally, we extend the verifier\u2019s scores to that of a reasoning chain , where we first obtain a score for each , resulting in the following score vector:\n, and then aggregate.\nA low-scoring reasoning step does not necessarily render the entire reasoning chain wrong, but it does increase the likelihood of inaccuracies. To combine these scores, we employ the geometric mean as a milder alternative to the min operator in our aggregation process.\nWe obtain a single score for a given reasoning chain  and a set of verifiers  by aggregating over the scores of each verifier  on .\nOur proposed framework allows for the customization of each verifier\u2019s contribution during aggregation. We use a weighted arithmetic mean, as defined below.\nWe set  for perplexity and  for all the others.\nWe selected  for perplexity based on preliminary experiments on the train partition of GSM8k and CSQA 2.0.\nImportantly, we use the same weights for all our experiments."
        },
        {
            "section_id": "3.5.2",
            "parent_section_id": "3.5",
            "section_name": "3.5.2 Ensembling Methods using the Verifiers",
            "text": "Ensembling techniques work by aggregating the solution of multiple reasoning chains to obtain a final solution. For example, Self-Consistency Wang et al. (2022  ###reference_b36###) randomly samples a given number of reasoning chains, and then performs a majority vote on the final answer.\nInstead of resorting to a majority voting mechanism over randomly sampled reasoning chains, we propose to leverage the scores obtained from our proposed verifiers to do the selection and the weighting."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "We use Falcon333Specifically, we use Falcon-40B-Instruct Almazrouei et al. (2023  ###reference_b2###) as our base LLM, as it was one of the largest and most capable open-source model family freely available at the time of the experiments.444Open source according to https://opensource.org/ We use the same model for both solution generation and solution verification. For solution generation, we use the zero-shot prompt from Kojima et al. (2022  ###reference_b17###). For verification, we use different prompts for each verifier. We include the prompts we used in the Appendix B  ###reference_###.\nWe perform experiments spanning 4 reasoning tasks: Math, Commonsense, Symbolic, and Other, and 9 datasets: BigBench Date Understanding bench authors (2023  ###reference_b4###) (Other), CommonsenseQA Talmor et al. (2019  ###reference_b31###), CommonsenseQA 2.0 Talmor et al. (2021  ###reference_b32###) and Strategy Geva et al. (2021  ###reference_b11###) (Commonsense), Coinflip and Last Letter Concatenation Wei et al. (2022  ###reference_b38###) (Symbolic), GSM8k Cobbe et al. (2021  ###reference_b10###), SVAMP Patel et al. (2021  ###reference_b26###), and AddSub Kojima et al. (2022  ###reference_b17###) (Math). We provide an example from each dataset in Appendix A.1  ###reference_###.555For Last Letter Concatenation, we use only  words instead of .\nWe use the standard evaluation metrics as previous work, which is Accuracy score computed between the gold answer and the predicted answer.\nAll our proposed verifiers are dataset-agnostic and we use the same prompts for all our experiments. Due to computational constraints, we use the mathematical accuracy verifier only for the math datasets.\nIn an attempt to minimize the impact of the underlying search strategy for the step-by-step solution, we adopt the following approach: we first sample 40 reasoning chains for each problem and for each dataset, then use the scores resulting from our proposed verifiers to guide our selection process.\nWe analyze the contributions of the verifiers by comparing the performance of the proposed method against the following baselines: (i) Random Chains, where we use the LLM to sample a solution. We use the same prompt as Kojima et al. (2022  ###reference_b17###), and (ii) best-of N sampling Adiwardana et al. (2020  ###reference_b1###); Wang et al. (2022  ###reference_b36###), where we sample a total of  reasoning chains and select the one with the lowest perplexity. Our motivation for using these two baselines is two-fold. First, we want to allow both the baselines and our proposed method to have access to the same candidate reasoning chains. Secondly, it has been observed in Wang et al. (2022  ###reference_b36###) that Best-of N sampling performs better than greedy decoding, especially for large N.\nWe conduct the following experiments: (i) Single chain analysis, where we use the resulting scores of each reasoning chain to select a single reasoning chain, (ii) Self-Consistency, where we aggregate multiple reasoning chains, and (iii) Single chain analysis with incomplete chain scoring, where we only score the reasoning chains based on the an initial % (or number) of the reasoning steps."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experimental Setting",
            "text": "We use Falcon333Specifically, we use Falcon-40B-Instruct Almazrouei et al. (2023  ###reference_b2###  ###reference_b2###) as our base LLM, as it was one of the largest and most capable open-source model family freely available at the time of the experiments.444Open source according to https://opensource.org/ We use the same model for both solution generation and solution verification. For solution generation, we use the zero-shot prompt from Kojima et al. (2022  ###reference_b17###  ###reference_b17###). For verification, we use different prompts for each verifier. We include the prompts we used in the Appendix B  ###reference_###  ###reference_###.\nWe perform experiments spanning 4 reasoning tasks: Math, Commonsense, Symbolic, and Other, and 9 datasets: BigBench Date Understanding bench authors (2023  ###reference_b4###  ###reference_b4###) (Other), CommonsenseQA Talmor et al. (2019  ###reference_b31###  ###reference_b31###), CommonsenseQA 2.0 Talmor et al. (2021  ###reference_b32###  ###reference_b32###) and Strategy Geva et al. (2021  ###reference_b11###  ###reference_b11###) (Commonsense), Coinflip and Last Letter Concatenation Wei et al. (2022  ###reference_b38###  ###reference_b38###) (Symbolic), GSM8k Cobbe et al. (2021  ###reference_b10###  ###reference_b10###), SVAMP Patel et al. (2021  ###reference_b26###  ###reference_b26###), and AddSub Kojima et al. (2022  ###reference_b17###  ###reference_b17###) (Math). We provide an example from each dataset in Appendix A.1  ###reference_###  ###reference_###.555For Last Letter Concatenation, we use only  words instead of .\nWe use the standard evaluation metrics as previous work, which is Accuracy score computed between the gold answer and the predicted answer.\nAll our proposed verifiers are dataset-agnostic and we use the same prompts for all our experiments. Due to computational constraints, we use the mathematical accuracy verifier only for the math datasets.\nIn an attempt to minimize the impact of the underlying search strategy for the step-by-step solution, we adopt the following approach: we first sample 40 reasoning chains for each problem and for each dataset, then use the scores resulting from our proposed verifiers to guide our selection process.\nWe analyze the contributions of the verifiers by comparing the performance of the proposed method against the following baselines: (i) Random Chains, where we use the LLM to sample a solution. We use the same prompt as Kojima et al. (2022  ###reference_b17###  ###reference_b17###), and (ii) best-of N sampling Adiwardana et al. (2020  ###reference_b1###  ###reference_b1###); Wang et al. (2022  ###reference_b36###  ###reference_b36###), where we sample a total of  reasoning chains and select the one with the lowest perplexity. Our motivation for using these two baselines is two-fold. First, we want to allow both the baselines and our proposed method to have access to the same candidate reasoning chains. Secondly, it has been observed in Wang et al. (2022  ###reference_b36###  ###reference_b36###) that Best-of N sampling performs better than greedy decoding, especially for large N.\nWe conduct the following experiments: (i) Single chain analysis, where we use the resulting scores of each reasoning chain to select a single reasoning chain, (ii) Self-Consistency, where we aggregate multiple reasoning chains, and (iii) Single chain analysis with incomplete chain scoring, where we only score the reasoning chains based on the an initial % (or number) of the reasoning steps."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Single Chain",
            "text": "In this experiment, we assess how the scores generated by our proposed verifiers are correlated with the likelihood of a reasoning chain reaching the correct final answer.\nFor this purpose, we conduct the following experiment: from the  sampled reasoning chains, we select the highest-scoring chain based on our proposed verifiers\u2019 scores.\nWe present our results in Table 1  ###reference_###, comparing our proposed approach with two baselines: Random Chains and best-of N sampling.\nWe make the following remarks.\nFirst, over all datasets, our proposed method performs better than selecting a reasoning chain at random, with improvements ranging from 1.18 points (Strategy) to 25.68 points (Last Letter), with an average improvement of 12.63.\nSecond, we remark that our proposed method outperforms the best reasoning chain according to perplexity (Low PPL Chains) in over 65% of the cases ( out of  datasets). This means that our proposed verification procedure provides valuable information beyond what is captured by simply selecting the lowest perplexity chains.\nWe note that this trend does not hold true for Symbolic Reasoning, where for both datasets investigated (Coinflip and Last Letter) the Low PPL Chain is better than the one selected according to our proposed verifier. An exploration over Coinflip revealed that steps where the coin has not been flipped received, on average, a lower relevance score, although this information is relevant. We leave the exploration of better verifiers for Symbolic Reasoning to future work.\nAll in all, the average improvement of our proposed method over the reasoning chain with the lowest perplexity is 1.43 points.\n###figure_3### ###figure_4### ###figure_5###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Self-Consistency",
            "text": "In this experiment, we explore how well our proposed method leverages ensemble techniques, particularly Self-Consistency Wang et al. (2022  ###reference_b36###).\nWe start with the same set of  reasoning paths and employ different selection strategies to evaluate their effectiveness: (i) we randomly sample from these paths (Random Chains), (ii) we select chains with the lowest perplexity from this set (Low PPL Chains), and (iii) we choose the reasoning chains with high scores, as determined by the verifiers, from these 40 paths (Proposed (weighted)).\nUnlike the original majority vote approach, we empirically found that weighting each reasoning chain by their verifier scores yields slightly better results. However, it is worth noting that this improvement does not hold when using only perplexity, as shown in Wang et al. (2022  ###reference_b36###).\nWe show in Figure 3  ###reference_### the behavior of our proposed method. We make two observations: First, our proposed method is able to leverage ensembling techniques, showing consistent performance gains as the number of reasoning chains increases. Second, we remark that our proposed method scales better, consistently outperforming the baselines.\nWe further investigate the performance impact of weighted voting, utilizing scores from our proposed verifiers, against the standard majority-voting approach. Specifically, we apply both voting methods to the identical set of reasoning chains, initially selected at random.\nWe found that using the scores of our proposed verifiers to do a weighted voting improves over the majority voting in over 96% of the cases.666\nDue to space constraints, we include the resulting plots in Appendix D  ###reference_###."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Verifying Incomplete Reasoning Chains",
            "text": "In our prior experiments, our proposed verifiers evaluated complete reasoning chains. Now, we explore their effectiveness when applied exclusively to the initial reasoning steps. This experiment provides insights into the potential utility of our proposed method in an \"online\" setting, where the reasoning step-level evaluation is employed to guide the search for good reasoning chains without fully generating multiple candidate solutions.\nWe assess the impact of using the verifiers for varying percentages of reasoning steps, denoted as X% along the X-axis of our line plot in Figure 4  ###reference_###.\nWe remark that the final performance increases with the % of steps verified and that verifying only the first 20% of the steps is sufficient to increase the final performance beyond that of random chains.\nSince knowing beforehand the total number of reasoning steps is unrealistic, we also experiment with only verifying a given number of the initial reasoning steps. Due to space limitations we include these results in Appendix F  ###reference_###. Additionally, we include in Table 5  ###reference_### the resulting performance when verifying between  and All reasoning steps over all datasets. In  cases, the performance increases even when verifying only the first step. When verifying the first two steps, the final performance increases in all the cases.\n###figure_6### ###figure_7### ###figure_8###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Contributions of each Verifier",
            "text": "In this experiment, we assess the contribution to the final performance of each of our verifiers: (1) Low Step Perplexity, (2) Relevance, (3) Mathematical Accuracy (if applicable), (4) Logical Consistency.\nFirst, we observe that each individual verifier is meaningfully contributing towards the final solution. For example, for the math datasets (Table 2(a)  ###reference_st1###), employing any verifier improves the final performance, with improvements ranging from  to . All in all, using as little as a single verifier improves the final performance in over 89% of the cases.77735/39 Secondly, we remark that combining all the verifiers gives further improvements, beyond those obtained by using a single verifier, suggesting that each verifier is adding meaningful and non-overlapping information. We note that there is a notable exception to this trend, where for the Symbolic Reasoning tasks (and for the Strategy dataset), a distinct combination of verifiers (i.e. only Perplexity) attains a better score than using all the verifiers. We provide more comprehensive results covering a wider range of verifier combinations in Appendix E  ###reference_###."
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Human Evaluation",
            "text": "While the proposed verifiers meaningfully contribute to the final performance on the reasoning downstream tasks, we perform a human evaluation study to assess: (1) how well they correlate with human judgment, (2) how reliably concepts such as logical consistency or relevance can be evaluated by humans, and (3) the percentage of errors not captured by the proposed principles.\nWe randomly sample candidate reasoning chains for 3 datasests: GSM8k, CSQA2.0, and Coinflip. We then distribute these to 8 human annotators and ask them to annotate each reasoning chain according to the following four criteria: (i) Relevance: is this step helpful in reaching the final solution, (ii) Mathematical Accuracy: are the mathematical calculations (if any) correct, (iii) Consistency: is the current step consistent with the previous steps, and (iv) Overall Correctness. We include inter-annotator agreement scores in the Appendix G  ###reference_###.\nWe compute pearson correlation scores (the correlation scores are in  between human assessments and the scores proposed by our verifiers and summarize the results in Figure 5  ###reference_###. We draw the following conclusions.\nFirst, we remark that each verifier exhibits a significant () and positive correlation (although small for Relevance and Logical Consistency) with human judgment.\nGiven the empirical improvements we observed when using the verifiers and the low correlations between the verifiers and the humans, one might wonder whether the improvements we have empirically observed are reasonable.\nTo address this concern, we conducted additional experiments on artificially generated data, where we had precise control over the correlation values. We provided additional details in Appendix H  ###reference_###. This experiment showed that the improvements we have observed are expected and even small correlations can statistically differentiate better reasoning chains from worse ones on average.\nWe leave the exploration of verifiers that correlate more strongly with human judgment to future work.\nSecond, we observed a large variance in the inter-annotator agreement score, which we hypothesize to come from different reasoning styles between different humans.\nThis variation is reminiscent of findings in toxicity research, where annotator backgrounds influence their judgments Goyal et al. (2022  ###reference_b13###).888We first observed this in the pilot study, after which we improved the clarity of the guidelines. We show all the inter-annotator agreements over each of the four attributes in Appendix G  ###reference_###.\nLastly, we found that less than 2% of the errors marked by the annotators are not captured by one of the three principles explored.\n###figure_9###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this study, we explore the applicability of a general-purpose verification procedure that can be readily applied to a wide range of reasoning tasks. This verification procedure consists of task- and dataset-agnostic verifiers designed to operate at the reasoning step-level and is inspired from fundamental principles that underlie sound reasoning: (1) Relevance (2) Mathematical Accuracy and (3) Logical consistency. On top of these, we leverage the perplexity of the reasoning step to steer the LLM towards high-quality solutions.\nWe evaluated our proposed approach across four distinct reasoning tasks, spanning nine datasets. Our results consistently demonstrate that the adoption of our proposed verifiers leads to notable performance improvements when compared to randomly sampled reasoning chains. Most notably, our approach outperforms the lowest perplexity reasoning chain in over 6 out of the 9 datasets we tested, indicating that the proposed verifiers provide additional information beyond what is captured by the perplexity measure of the reasoning chain.\nOverall, we found that large language models are capable of finding their own mistakes, albeit this capability is currently noisy. We leave the exploration of better and more cost-efficient verifiers to future work."
        }
    ],
    "url": "http://arxiv.org/html/2405.00204v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3",
            "3.3.4",
            "3.4",
            "3.5",
            "3.5.1",
            "3.5.2"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.5",
            "4.6"
        ]
    },
    "research_context": {
        "paper_id": "2405.00204v1",
        "paper_title": "General Purpose Verification for Chain of Thought Prompting",
        "research_background": "**Paper's Motivation:**\n\nThe paper is motivated by the observation that while Large Language Models (LLMs) using chain-of-thought prompting have shown impressive capabilities in text generation and reasoning tasks, they still lack mechanisms to rectify errors in their reasoning process. Chain-of-thought prompting allows LLMs to generate intermediate steps to solve complex tasks, but these steps can contain errors that prevent reaching the correct final answer or make the reasoning path invalid even if the final answer is correct. This limitation needs addressing to enhance the reasoning abilities of LLMs.\n\n**Research Problem:**\n\nThe primary research problem tackled in this paper is whether introducing problem-agnostic verification mechanisms to identify and rectify early mistakes in reasoning chains can improve the reasoning capabilities of LLMs. The authors aim to develop a general framework using verifiers to test for (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency in the intermediate steps of the reasoning process, ensuring these models produce more accurate and logically sound outputs across various tasks.\n\n**Relevant Prior Work:**\n\n1. **LLM Capabilities and Chain-of-Thought Prompting:**\n   - Large Language Models' success in multiple tasks (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023; OpenAI, 2023; Bubeck et al., 2023).\n   - Chain-of-thought prompting to improve reasoning performance by generating intermediate steps (Nye et al., 2021; Wei et al., 2022; Chowdhery et al., 2022; Goyal et al., 2023).\n\n2. **Challenges in Reasoning Steps:**\n   - The problem of incorrect intermediate steps that might lead to correct final answers or prevent reaching the correct answer (Turpin et al., 2023).\n\n3. **Recent Mitigating Approaches:**\n   - Approaches using search mechanisms and self-correction inspired by \"System 2\" thinking (Yao et al., 2023; Xie et al., 2023; Pan et al., 2023).\n   - Training dataset-specific verifiers to improve the aggregation of reasoning chains (Li et al., 2023).\n\nThis work aims to go beyond dataset-specific solutions by proposing a general framework applicable to various reasoning tasks, improving the logic and accuracy of the reasoning steps generated by LLMs through verification mechanisms.",
        "methodology": "**General Purpose Verification for Chain of Thought Prompting**\n\n**Methodology:**\n\nWe propose a novel approach that seamlessly integrates with any given Large Language Model\u2019s (LLM) solution generation process, operating at both the step generation and step evaluation stages as suggested by Wei et al. (2022), Wang et al. (2022), and Kojima et al. (2022).\n\nOur method consists of two key components:\n\n1. **Solution Generator**:\n   - Responsible for generating candidate steps.\n\n2. **Verifiers**:\n   - A set of verifiers, each specializing in a particular qualitative aspect of reasoning.\n   - Each verifier checks whether the candidate step complies with a specific reasoning property.\n\nWe focus on properties that are broadly applicable across various reasoning tasks. \n\nThe section is organized as follows:\n- **Section 3.1** - Describes the notations and abstractions employed.\n- **Section 3.2** - Details the solution generation component.\n- **Section 3.3** - Introduces the proposed verifiers.\n- **Section 3.4** - Outlines the procedure to obtain fine-grained scores for reasoning chains.\n- **Section 3.5** - Explains how verifiers are used in the aggregate.\n\nThis detailed breakdown ensures a comprehensive understanding of our approach, emphasizing its adaptability to diverse reasoning tasks by embedding validation at each step in the generation process.",
        "main_experiment_and_results": "## Main Experiment Setup and Results\n\n### Main Experiment Setup\n\n**Model and Prompts:**\n- **Base LLM:** Falcon-40B-Instruct, selected for being one of the largest and most capable open-source models available at the time of the experiments.\n- **Solution Generation:** Zero-shot prompt from Kojima et al. (2022).\n- **Verification:** Different prompts were used for each verifier, detailed in Appendix B.\n\n**Tasks and Datasets:**\n- **Reasoning Tasks:** Math, Commonsense, Symbolic, and Other.\n- **Datasets:**\n  - **Math:** GSM8k, SVAMP, AddSub.\n  - **Commonsense:** CommonsenseQA, CommonsenseQA 2.0, StrategyQA.\n  - **Symbolic:** Coinflip, Last Letter Concatenation.\n  - **Other:** BigBench Date Understanding.\n- Examples from each dataset are provided in Appendix A.1.\n\n**Evaluation Metrics:**\n- **Metric:** Accuracy score, measured between the gold answer and the predicted answer, consistent with previous works.\n\n**Proposed Verifiers:**\n- **Dataset-Agnostic:** Verifiers applicable across different datasets.\n- **Mathematical Accuracy Verifier:** Exclusively used for math datasets due to computational constraints.\n\n**Procedure:**\n- **Reasoning Chains:** \n  - Generate 40 reasoning chains per problem for each dataset.\n  - Use verifiers to score and guide the selection process.\n  \n**Baselines:**\n1. **Random Chains:** Solutions sampled using the LLM with the same prompt as used in Kojima et al. (2022).\n2. **Best-of N Sampling:** After sampling multiple reasoning chains, select the one with the lowest perplexity score. This approach was chosen based on evidence that it performs better than greedy decoding for large N.\n\n### Main Experimental Results\n\n- The results section compares the proposed verifier method against the aforementioned baselines:\n  - **Single Chain Analysis:** Verifiers were used to evaluate individual reasoning chains to select a single best-performing chain.\n  - **Self-Consistency:** Multiple reasoning chains were aggregated for evaluation.\n  - **Single Chain Analysis with Incomplete Chain Scoring:** Reasoning chains were scored based on an initial portion of the reasoning steps.\n\n- **Findings:** Detailed comparative performance analyses were provided, demonstrating how the proposed method performed relative to random chain sampling and best-of N sampling baselines. This analysis underscores the effectiveness and robustness of the proposed verifiers across different datasets and reasoning tasks."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To determine how the scores generated by the proposed verifiers correlate with the likelihood of a reasoning chain reaching the correct final answer.",
            "experiment_process": "From the 40 sampled reasoning chains, the highest-scoring chain based on the proposed verifiers' scores is selected. The results are compared against two baselines: Random Chains and best-of N sampling.",
            "result_discussion": "The proposed method consistently performs better than random chain selection, with an average improvement of 12.63 points. It also outperforms the lowest perplexity chains in over 65% of the cases, suggesting that the verification procedure provides valuable information beyond simple perplexity.",
            "ablation_id": "2405.00204v1.No1"
        },
        {
            "research_objective": "To explore the effectiveness of using ensemble techniques, particularly Self-Consistency, with the proposed method.",
            "experiment_process": "Starting with 40 reasoning paths, different selection strategies are used: (i) Random Chains, (ii) Low PPL Chains, and (iii) Proposed (weighted), where reasoning chains are weighted by their verifier scores. The performance of weighted voting is also compared against standard majority voting.",
            "result_discussion": "The proposed method shows consistent performance gains with increased reasoning chains and scales better than baselines. Weighted voting with verifier scores performs better than majority voting in over 96% of the cases.",
            "ablation_id": "2405.00204v1.No2"
        },
        {
            "research_objective": "To evaluate the effectiveness of the proposed verifiers when applied only to initial reasoning steps.",
            "experiment_process": "The performance impact is assessed by verifying varying percentages of reasoning steps. The results of verifying from the first step to all reasoning steps over all datasets are recorded.",
            "result_discussion": "Final performance increases with the percentage of steps verified. Verifying the first 20% of steps already increases performance beyond random selection, and verifying just the first two steps improves performance in all cases.",
            "ablation_id": "2405.00204v1.No3"
        },
        {
            "research_objective": "To determine the contribution of each proposed verifier to the final performance.",
            "experiment_process": "Evaluation involves the use of four verifiers: Low Step Perplexity, Relevance, Mathematical Accuracy (if applicable), and Logical Consistency. Performance is compared using individual verifiers and their combinations.",
            "result_discussion": "Each individual verifier contributes meaningfully to the final performance. Combining verifiers results in further improvements, with certain exceptions for Symbolic Reasoning tasks. Combining all verifiers generally provides better results than using any single verifier.",
            "ablation_id": "2405.00204v1.No4"
        },
        {
            "research_objective": "To assess the correlation of proposed verifiers with human judgment, and to evaluate the human reliability in assessing logical consistency and relevance.",
            "experiment_process": "Human evaluation is conducted on reasoning chains from three datasets: GSM8k, CSQA2.0, and Coinflip. Eight annotators judge each chain on four criteria: Relevance, Mathematical Accuracy, Consistency, and Overall Correctness. Pearson correlation scores between human assessments and verifier scores are computed.",
            "result_discussion": "Each verifier shows a significant positive correlation with human judgment. Despite low correlation values, the verifiers can still statistically differentiate better reasoning chains. Inter-annotator agreement varies significantly, suggesting differences in reasoning styles. Less than 2% of errors marked by annotators are not captured by the proposed principles.",
            "ablation_id": "2405.00204v1.No5"
        }
    ]
}