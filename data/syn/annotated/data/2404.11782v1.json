{
    "title": "Requal-lm: Reliability and Equity through Aggregation in Large Language ModelsThis work was supported in part by NSF 2107290.",
    "abstract": "The extensive scope of large language models (LLMs) across various domains underscores the critical importance of responsibility in their application, beyond natural language processing.\nIn particular, the randomized nature of LLMs, coupled with inherent biases and historical stereotypes in data,\nraises critical concerns regarding reliability and equity.\nAddressing these challenges are necessary before using LLMs for applications with societal impact.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In the ever-evolving realm of advanced technologies, Large Language Models (LLMs) have quickly emerged as versatile tools, extending their influence far beyond the boundaries of natural language processing (NLP).\nMany of the traditionally challenging tasks with decades of research in various fields of computer science are finding more effective resolutions with the help of LLMs. Let us consider Example 1  ###reference_### as a motivating example for subset selection using LLM.\nLLMs are sequential randomized approaches based on estimations learned from large textual datasets. In particular, based on the prompt and the sequence of tokens generated so far, each word (token) in the dictionary is assigned a probability. Then, the next token is generated probabilistically (proportional to the probabilities of the top-k or top-p%) using the parameter temperature.\nConsequently, the output may vary when the LLM is queried again.\nAs a result, a valid concern, particularly for a decision maker, is whether they should rely on the LLM\u2019s output for taking action.\nIn settings similar to Example 1  ###reference_###, the reliability question is further significant, since a method to combine the performance criteria has not been specified, while small changes in the combination details may significantly change the output Guan et al. (2019  ###reference_b10###).\nAnother challenge that makes a single query to the LLMs unreliable arises for the symmetric settings, where the ordering between the input does not matter, i.e., shuffling the input should not impact the output.\nFor instance, in Example 1  ###reference_### the ordering based on which the employees are passed to the LLM should not impact the output.\nConversely, LLMs receive an input as a (ordered) sequence. As a result, as it was observed in Gao et al. (2023  ###reference_b8###), the output of the LLMs for symmetric problems vary when the input is shuffled.\nWe also observed the same behavior in our experiments on a subset selection task, where the entities that are placed at the beginning of the list had a higher chance of being returned as the output.\nTo resolve these issues\nwe introduce Requal-lm that, instead of relying on a single query to an LLM, follows a Monte-carlo method Hammersley (2013  ###reference_b11###) based on repeated sampling.\nParticularly, viewing each LLM output as a sample from the underlying distribution of possible outputs, it identifies the centroid of a collection of samples as its estimation of the mean of the distribution, and returns the closest output to the centroid as the most reliable one. To further clarify this, let us consider Example 1  ###reference_### once again.\nWhile being effective in practice, data-driven technologies have been heavily criticized for machine bias Angwin et al. (2022  ###reference_b1###), and LLMs are not an exception when it comes to bias.\nAs a result, another valid concern when using LLMs for decision making is neutrality: to ensure that impact of historical biases and stereotypes are minimized and that values such as diversity are promoted.\nLLMs are among the fast-growing technologies, with new and advanced versions regularly emerging, while many of these systems are \u201cblack-box\u201d.\nOur system design is not dependent on a specific LLM, which makes it a ready-to-apply wrapper that works on top of any of the current and future closed-source and open-source LLMs.\nRequal-lm does not require pre-training or fine-tuning, is task-agnostic, and can handle non-binary demographic groups.\nIn the following, first in \u00a7 2  ###reference_### we carefully discuss the problem setting, introduce notations, and formally define terms such as reliability and bias.\nNext, in \u00a7 3  ###reference_### we review the architecture of Requal-lm, and develop our methodology for finding an equitable centroid and return the closest output to it, the one that is both equitable and reliable.\nThe experimental evaluations, related work, and the discussions of the benefits and limitations of Requal-lm are provided in \u00a7 4  ###reference_###, \u00a7 5  ###reference_###, \u00a7 6  ###reference_###, and \u00a7 8  ###reference_###, respectively."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Preliminaries",
            "text": "\u2013 (Input) Task: We consider a task, such as subset selection, sentence completion, assembling a team of experts, etc., described in form of a prompt: a natural language instruction.\n\u2013 (Input) Demographic Groups: We assume the existence of at least one sensitive attribute (e.g., sex) that specify the demographic groups  (e.g., {male, female}). The demographic groups are used to specify the output bias.\n\u2013 LLM: We assume access to (at least) one LLM, which is used for task answering. The LLM is randomized, i.e., the tokens are sequentially drawn based on the underlying distribution of the (top-k or top-p%) token-probabilities.\nWe treat the LLM as a black-box oracle that upon querying generates an output based on the input prompt.\nTreating the LLM as black-box allows the adaptation of Requal-lm both for closed-source and open-source LLMs.\n\u2013 Text Embedding: We rely on an external text embedding model that transforms a text into an embedding vector.\nSpecifically, given a text ,\nit generates the vector representation .\nOur system, Requal-lm, is agnostic to the choice (but limited to the performance) of the embedding model,\nand can adapt any state-of-the-art text embedding technique.\nWithout loss of generality, we use Instructor \u2013 a method for generating task-specific embeddings in accordance with provided instructions Su et al. (2023  ###reference_b18###).\nGiven two text phrases  and  and their corresponding embeddings  and , the similarity between  and  is measured as the cosine similarity between their embeddings, i.e., .\nSimilarly, the distance between  and  is defined as .\nLet  be an output generated for the prompt  comprising a sequence of  tokens  sequentially generated by the LLM.\nAt each iteration , let  be the probability of generating the token . Then  can be computed as the product of its token probabilities. That is, .\nBias is sometimes inherent to the task at hand and is not harmful. For example, when the task involves summarizing or rephrasing a paragraph that is particularly written about a specific gender, the resulting output tends to be naturally biased towards that gender. We call this type of output bias as the inevitable bias.\nFormally, we say a bias level  is inevitable\nif there is no valid output  with a bias less than .\nIn other words, for any output  where , we can say .\nTherefore, we define the inevitable bias as .\nWe consider any bias that is not inevitable, discriminatory.\nHarmful stereotypes are in this category. We call this type of output bias as the harmful bias.\nConsidering equity as our objective in this paper, we would like to minimize harmful bias in the outputs.\nThe harmful bias of an output can be computed by subtracting its bias from the inevitable bias, i.e., .\nAfter defining the terms and notations, we are able to formulate our problem: given a task presented in the form of a prompt , and including the demographic groups , the objective is to identify an output , such that it maximizes  and minimizes .\n###figure_1###"
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Technical Details",
            "text": ""
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Architecture Overview",
            "text": "Figure 1  ###reference_### shows the architecture of Requal-lm.\nFollowing the Monte-carlo method described in \u00a7 3.2  ###reference_###, the first step is to obtain a set of iid output samples by issuing  independent queries to the LLM.\nThe results are subsequently fed into the text embedding model, Instructor, to obtain the vector representations .\nNext, the vector representations, as well as the vector representations of the demographic groups, pass on to the aggregation function (referred to as AVG in the figure). The aggregation function generates the vector representation that corresponds to the average of  to .\nFinally, a nearest neighbor search is applied to the sample outputs to retrieve the output that is most similar output to the average."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Methodology",
            "text": "Our approach for satisfying reliability and equity in LLM outputs is a Monte-carlo method, which relies on repeated sampling and the central limit theorem Durrett (2010  ###reference_b7###).\nBased on the law of large numbers, iid samples can serve for approximating their underlying distribution. That is because the expected number of occurrences of each observation is proportional to its probability.\nRecall that the outputs for a prompt  are generated based on the probability distribution . Particularly, the probability that an output  is sampled is .\nTherefore, the expected value of  is equal to the mean of  in the embedding space, .\nNow consider a set  of iid output samples for the prompt . Let  be the sample mean of the representation vectors in . That is,\nSimilarly, let  be the standard deviation of the samples.\nFollowing the central limit theorem,  follows , the Normal distribution with the mean  and standard deviation .\nFor simplicity, in the rest of the paper, we call  the centroid of the output samples.\nRequal-lm considers two approaches for specifying the value of : (i) fixed budget and (ii) fixed error.\nOne can consider a fixed budget  to ensure the sampling cost does not exceed . Specifically, if the cost of each query is , then .\nAlternatively, when a flexible budget is available, one can collect enough samples to bound the confidence error for a specific confidence level  (e.g., 95%). The confidence error  guarantees .\nFollowing the central limit theorem and using the Z-table, the confidence error is computed as ."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Equity-aware Aggregation",
            "text": "Using the centroid of sample outputs  as the estimation of , we can estimate the reliability of each output  as , and identify the output with the maximum expected reliability.\nFigure 2  ###reference_### shows a toy T-SNE visualization of  sample outputs, while their centroid is marked with a plus sign. The distance of the points from the centroid show their expected reliability. In this example,  is the most reliable output.\nIn the figure, the bias values are specified with a green-to-red color coding, where green is the minimum bias.\nFrom the figure, one can notice that , although being the closest to the centroid, has a high bias.\nOn the other hand,  is both highly reliable and has a low bias value; hence it would be a better output. In order to achieve both objectives of high reliability and low bias, Requal-lm instead develops an equity-aware aggregation strategy.\n###figure_2### Equation 1  ###reference_### computes the centroid as the average over all of the sampled outputs.\nInstead, to achieve equity, it is desirable to disregard the biased outputs and instead compute the average of unbiased outputs, which we call equitable centroid or weighted centroid.\nHowever, since the bias values are continuous, Requal-lm assigns a weight to each sample proportional to how biased it is. Particularly, focusing on minimizing the harmful bias, the weight of each sample  is computed using the normalized bias values . Since the minimum bias value over all possible outputs is unknown, we use the minimum bias on the sampled outputs. Formally, each weight  is computed as\nFinally, the equitable centroid is computed using as the weighted average over  as"
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this section, we present our comprehensive experimental analysis on three separate tasks: Subset Selection, Chat Completion, and Masked Language Prediction. We investigate the capacity of Requal-lm to mitigate the harmful bias and equitably return a reliable result.\nWe use reliability ( \u2013 Definition 1  ###reference_inition1###) and bias ( \u2013 Definition 2  ###reference_inition2###)\nas the main evaluation metrics.\nWe aim to mitigate the bias, specifically bias against the minority groups which is female in our task. Therefore we do not use the absolute value of  in the computations we perform. Instead we use signed value of bias which is quantified as the disparity between the similarity of the output to the majority and minority as shown in Definition 2  ###reference_inition2###. Therefore, it is acceptable to have negative values on the bias axis.\nWe also provide a demonstration of measures that have been previously studied to validate our system and to give a thorough comparison with the baseline models. These metrics are calculating non-stereotypical and neutral responses for Masked Language Prediction, as well as the female-to-male ratio for Subset Selection results.\nWe use 3 baselines to compare our results with. The first baseline (referred to as Pair-Ranker) proposed by Jiang et al. (2023  ###reference_b12###) is a pair-wise ranking model that uses a cross-attention Transformer that can score a pair of output candidates by encoding them with the input text. The second baseline queries the LLM once and returns its output. We refer to this baseline as First-Response.\nThe third baseline (referred to as Debiased-GPT). Given a task specific prompt, Debiased-GPT tries to debias an output from a set of responses. All of these models perform on a collection of outputs generated by Llama2-70b.\nWe refer to the output of Requal-lm closest to the weighted (equitable) centroid as Weighted Output, while the most similar output to the centroid (the output maximum reliability) is called Unweighted Output, and the one with minimum bias is referred as Min-bias Output."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiment setup",
            "text": "Environment: We performed our evaluations using two LLMs: Llama2, 70 billion parameters (Llama2-70b), alongside GPT3.5-turbo APIs. All of our experiments were conducted on the Google Colab.\nDefault Values:\nTo ensure obtaining relevant and creatively diverse responses from one model in every iteration, we randomly sample temperature values from a uniform distribution in the range . We modify the presence and frequency penalty by drawing a random value in the range ."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Datasets",
            "text": "Our experiments use two benchmark datasets, including StereoSet Nadeem et al. (2021  ###reference_b17###) and WinoBias Zhao et al.  ###reference_b23###, which have been utilized before for detecting bias in Language Models. The Forbes 2022 Billionaire 222Forbes-worlds-billionaires-list-2022  ###reference_orbes-worlds-billionaires-list-2022### dataset and the Students 333Student-dataset  ###reference_ePhysicalizations/blob/master/Zooid_Vis/bin/data/student-dataset.csv### dataset are used for subset selection (please refer to Appendix B  ###reference_### for more details). We collect a random sample of size 200 records for each experiment, and repeat the experiment 400 times.\n###table_1### ###figure_3### ###figure_4### ###figure_5### ###figure_6### ###figure_7### ###figure_8### ###figure_9### ###figure_10### ###figure_11### ###figure_12### ###figure_13### ###figure_14### ###figure_15### ###figure_16###"
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Subset Selection",
            "text": "Previous studies have explored Subset Selection for the purpose of identifying smaller datasets for efficient training or fine-tuning Wang et al. (2023  ###reference_b20###), Killamsetty et al. (2023  ###reference_b14###). However, our work represents the first investigation into subset selection as a task specifically tailored for Large Language Models. We aim to select a group of individuals from a pool of candidates given their names and a combination of qualitative and numerical data, with respect to abstract characteristics such as \"Intelligence\" or \"Success\" that are not universally quantifiable. We use two datasets: Forbes 2022 Billionaire, and Students which contain candidates\u2019 names, numeric data, and non-numerical characteristics.\n\nWe evaluate our results against 3 baselines, described previously. We define a female-to-male ratio () as a measure of the average number of female candidates to male candidates in our response samples. We begin by explaining the results for Forbes 2022 Billionaire and Students on sample outputs, shown in Figures 3(a)  ###reference_sf1### and 3(b)  ###reference_sf2###.\n\nIn both figures, one can observe a clear shift of distributions between Min-bias Output (yellow distribution) and Unweighted Output, which indicates the magnitude of harmful bias in the red distribution. Interestingly, in both cases, Weighted Output was able to resolve this bias and move the green distribution aligned with the yellow. Also, as reflected in Figure 5(a)  ###reference_sf1### and 5(b)  ###reference_sf2###, the reliability values of Weighted Output are close to Unweighted Output. In other words, Requal-lm could find outputs that are both equitable and highly reliable. This is also reflected in the increased gender diversity of the results, as the transitions from  for Unweighted Output to  for Weighted Output for the Students dataset. Similarly, in the Forbes 2022 Billionaire, the issue of under-representation of the minority group (females) was successfully addressed as the  increased from 0.65 to 1.21."
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Comparison against Baselines",
            "text": "Next, in order to compare our results with the baselines, we used Students and Forbes 2022 Billionaire datasets on subset selection with samples. The results for the bias and the reliability of the outputs are provided in Figures 4 ###reference_### and 5 ###reference_###, respectively. For both datasets, one can observe the superiority of the output Requal-lm, Weighted Output, both on bias and also the reliability. Looking at Figure 4(b) ###reference_sf2### and Figure 4(a) ###reference_sf1###, first, it is evident that while the bias distribution of all baselines are similar to Unweighted Output. In other words, those were not successful in eliminating bias. On the other hand, the bias distributions for Weighted Output (green lines) are shifted to left in both cases, demonstrating its lower bias. Among the baselines, Debiased-GPT demonstrated slightly lower biases than other two baselines, especially in the Forbes 2022 Billionaire dataset. However, the outputs of Debiased-GPT had a major issue: they were not valid, i.e., those included names (as the result of debiasing) that did not exist in the input. Figure 5 ###reference_### shows the reliability values for each of the 400 subset selection instances. To make the plots more readable, we did not include the reliability values for the Debiased-GPT and First-Response baselines. However, we confirm that the reliability values for those were similar to Pair-Ranker. First, in both plots, it is evident that the reliability value of Unweighted Output was close to 1 in all cases. Second, one can confirm that the reliability values for Weighted Output were also very close to Unweighted Output, demonstrating that Requal-lm was able to reduce the bias at a negligible reliability cost. On the other hand, the reliability gap of Pair-Ranker with Unweighted Output was high (with a high fluctuation). We would like to also point out to the large number of calls to the LLM by Pair-Ranker as it requires extra queries in its pairwise comparison phase."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Masked Language Prediction",
            "text": "The Masked Language Prediction task evaluates co-reference resolution on the WinoBias dataset. Each sentence in WinoBias Zhao et al.  ###reference_b23### consists of two sentences merged together. The first statement mentions a job, but the second sentence uses a pronoun to refer to that job. The goal is to predict the masked term in a way that reduces harmful bias by eliminating existing trends that associate a profession to a specific gender (Table 3  ###reference_###).\nTo address the Masked Language Prediction task on WinoBias, we systematically filtered pro-stereotype sentences related to each gender. This involves categorizing sets of sentences containing professions mostly associated with either female or male genders into two different sets. Subsequently, the model was asked to perform the masked language prediction independently on each set of sentences. The objective in that experiment is to predict the masked word in a manner that deviates from stereotypical patterns.\nFigure 3(c)  ###reference_sf3### and 3(d)  ###reference_sf4### illustrates the distribution of bias scores for the Weighted Output (green) and Unweighted Output (red) across the whole dataset. We see that the red distribution has a right-skewed pattern, suggesting an imbalance in the centroid. Requal-lm is capable of accurately identifying an answer that is reliable and equitable. Specifically, when the majority vote exhibits stereotypical patterns, our method chooses an anti-stereotype or neutral response for the masked word (Table 2  ###reference_###). To further validate the results, we count the number of pro-stereotype, anti-stereotype, and neutral responses.\nOur task is designed to prevent responses from exhibiting bias toward either gender. Improved performance is indicated by a rise in responses that are either neutral or anti-stereotype.\nAs shown in Table 2  ###reference_###, our method successfully replaced the masked word using gender-neutral or anti-stereotype terms in  of responses with 5 output samples and  of responses with 30 output samples. The shift of the bias score distribution to right from 5 to 30 outputs, as shown in Figure 3(c)  ###reference_sf3### and Figure 3(d)  ###reference_sf4###, indicates that the 5 outputs generally exhibit lower bias compared to the centroid and minimum bias. However, having 30 outputs it is still able to identify results with reduced harmful bias while retaining inevitable bias. Requal-lm successfully achieved the closest approximation to a normal distribution of bias score () based on the obtained results that are all biased.\nSimultaneously, the results of our experiment results on () in Figure 5(d)  ###reference_sf4### show a distribution that closely mirrors those of Unweighted Output, exhibiting higher values compared to the baseline models. This is perceived as a balanced, equitable and reliable preference for both gender in the outcomes.\n###table_2###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Chat Completion",
            "text": "In this task, we use StereoSet Intersentences Nadeem et al. (2021  ###reference_b17###), focusing on the gender category. Previous work by  Nadeem et al. (2021  ###reference_b17###) utilized Stereoset for multi-choice question answering. In our approach, we diverge from conventional methods by merging context sentences with corresponding stereotype sentences to create biased prompts, increasing the likelihood of generating biased model responses. Following the persuasion techniques explored by  Zeng et al. (2024  ###reference_b22###), namely compensation and reciprocation, our goal is to incentivize the model to produce outputs based on these biased prompts. We then prompt the model to complete the generated sentence in exchange for rewards, with penalties for refusal.\nFigures 3(e)  ###reference_sf5###, 3(f)  ###reference_sf6### and 6  ###reference_### illustrate the bias score distribution of the Chat completion results for Unweighted Output (red), Weighted Output (green), and Min-bias Output (yellow).\nIn both figures, one can notice that the bias gap between Unweighted Output and Min-bias Output is already negligible. Still for both cases of 5 and 30 samples, Weighted Output could reduce the bias to almost the same distribution as of Min-bias Output.\nMeanwhile, Weighted Output displays higher values of  compared to both Min-bias Output and Pair-Ranker, as illustrated in Figure 5(c)  ###reference_sf3###, enhancing the reliability of our results over the baseline methods.\n###figure_17### Last but not least, our experiments (Figure 6  ###reference_###) on the non-binary sensitive attribute Race within StereoSet also reveal a consistent pattern, which illustrates the extension of Requal-lm for settings with multiple demographic groups."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": "Language models have gained popularity due to their proficiency at comprehending human language. Nevertheless, prior research has examined numerous limitations of these models, particularly in terms of their reliability and fairness. Various techniques have been previously presented to mitigate bias in language models while enhancing their reliability.\nIn this literature, drop out is a regularization technique adopted to mitigate gender bias Meade et al. (2022  ###reference_b16###); Webster et al. (2020  ###reference_b21###). The interruption generated by this strategy restricts the model from acquiring the ability to detect the connections between words that ultimately builds stereotypes. Some studies propose reducing bias in pre-trained models and enhancing dependability through diverse data augmentation. This involves incorporating data points that cover various demographics Zmigrod et al. (2019  ###reference_b24###); Dinan et al. (2020  ###reference_b6###); Barikeri et al. (2021  ###reference_b2###).\nAdditionally, there are studies that focus on mitigating bias in word representation using post-processing techniques Bolukbasi et al. (2016  ###reference_b3###), as well as in sentence representation May et al. (2019  ###reference_b15###) and context representations Caliskan et al. (2017  ###reference_b5###); Kaneko and Bollegala (2021  ###reference_b13###). Nevertheless, certain algorithms necessitate the process of retraining the model Bordia and Bowman (2019  ###reference_b4###) or finetuning Gira et al. (2022  ###reference_b9###).\nWeighted sampling to improve fairness in classification tasks has been studied before Ueda et al. (2023  ###reference_b19###) but, to the best of our knowledge, this paper is the first to use repeated sampling for fairness (and reliability) in the context of LLMs.\nPerhaps the most similar paper to our work is Jiang et al. (2023  ###reference_b12###) (called Pair-Ranker in our experiments), that uses pairwise comparison between the LLM outputs to rank them. While Pair-Ranker also takes as the input a set of LLM outputs and rank them, it has different goals and follows different technical approaches from Requal-lm. Also,\nPair-Ranker has a significantly higher query cost, compared to Requal-lm: Pair-Ranker issues an extra  calls to the LLM to rank the outputs, while Requal-lm does not issue any additional calls other the  calls to collect the outputs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Benefits",
            "text": "In the following, we list some of the advantages of Requal-lm, compared to the existing approaches.\n\u2013 A wide range of task:\nLLMs continuously find new applications in solving interesting problems across different domains.\nRequal-lm is not limited to specific tasks (such as sentence completion). It naturally fits to any task specified as a prompt and its output can be evaluated in the embedding space based on Definitions 1  ###reference_inition1### and 2  ###reference_inition2###.\n\u2013 Agnostic to the choice of LLM Model and the text embedder:\nRequal-lm treats the LLM model as black-box.\nAs a result, any state-of-the-art models can be readily adapted by it. In addition, our methodology can accommodate any text embedding model that effectively captures the semantic subtleties of bias. Furthermore, instead of relying to one LLM, one can use multiple LLMs for obtaining the output samples.\n\u2013 No need for pre-training or fine-tuning:\nRequal-lm is a reliability and equity wrapper that can be applied readily on top of any LLM.\n\u2013 Optimizing both reliability and equity:\nGiven the randomized nature of LLMs alongside historical biases in data, equitably finding a reliable output for the task at hand is critical.\nSatisfying this requirement make Requal-lm a good candidate, at least for the applications with societal impact.\n\u2013 Not limited to specific and binary demographic groups:\nWhile existing work in NLP has been mostly focused on gender bias and binary sensitive attributes, Requal-lm is designed to work both in binary and non-binary settings, for a wide range of demographic groups that could be specified in the text-embedding space.\n\u2013 Distinguishes between harmful and inevitable bias:\nAs explained earlier, some level of bias may be inevitable for a given task, such as summarizing a paragraph about African-American history.\nWhile approaches such as output debiasing cannot identify such bias, Requal-lm distinguishes between those cases and the harmful bias.\n\u2013 Always generates valid results:\nAssuming that the LLM generates valid outputs for a given prompt, Requal-lm always generates a valid result. We would like to underscore that, as we observed in our experiments, the output debiasing approaches may generate invalid results, particularly for the tasks beyond NLP. For example, let us consider Example 1  ###reference_### once again, where the objective is to select a subset of candidates from a pool.\nThe generated output for this task is a set of names. Now suppose all those names are male. Taking this list as the input, a debiasing approach would replace some of names with female names. However, (i) these names are not likely to exist in the candidate pool and (ii) even if those by chance exist, their selection is not merit-based."
        },
        {
            "section_id": "7",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "Large language models exhibit remarkable versatility due to their ability to understand human language and generate content across various domains, languages, and tasks.\nHowever, responsible usage of LLMs calls to first understand and minimize the potential harms of these technologies. Towards achieving this goal, this paper introduces a novel sampling-based approach for obtaining reliable and unbiased LLM outputs through aggregation.\nOur design choice to consider the LLM as black-box, facilitates scaling with the fast growing LLM technologies. Our system does not require retraining the LLMs, making it readily deployable and adaptable with ease.\nIn this paper, we optimize for equity, measured in the embedding space using cosine similarity with the vector of demographic groups. Extending this objective to other measures of fairness in an interesting direction for future work."
        },
        {
            "section_id": "8",
            "parent_section_id": null,
            "section_name": "Limitations",
            "text": "Having mentioned some of it benefits, we now discuss some of the limitations of Requal-lm.\nIt is important to underscore that our approach avoids modifying the internal configurations of the models it uses. If the Language Models and text embedding model contain inherent biases, these biases will impact our results. Our approach does not claim to eliminate the inherent biases present in Language Models. Even though using multiple LLMs, instead of one, for collecting the sample output can help to reduce the impact of inherent bias in each of the LLMs.\nOur approach heavily depends on the effectiveness of the embedding vectors produced by Su et al. (2023  ###reference_b18###) and their ability to capture the subtle semantic biases present in phrases. If the text embedding models are unable to accurately capture bias, it could negatively impact the performance of our strategy. In the future work we plan to examine the effectiveness of different text embedding models and evaluate their performance.\nAdditionally, although our approach does not require knowledge of sensitive attributes, it does require an understanding of minority groups in order to correctly determine weighted averages.\nFurthermore, beyond human evaluation, we lack a quantitative metric to assess the validity of the final output. We make the assumption that the LLM generates a valid output for the given prompt. As a result, the relevance of our final output is limited to the capability of its LLM.\nFilling this gap is an interesting research question we consider for our future work.\nFurthermore, our objective is to broaden the application of our approach to include other sensitive attributes and demographic groups."
        }
    ],
    "url": "http://arxiv.org/html/2404.11782v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "5"
        ],
        "methodology_sections": [
            "2",
            "3.1",
            "3.2",
            "3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.1",
            "4.4",
            "4.5"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.3.1",
            "4.4",
            "4.5"
        ]
    },
    "research_context": {
        "paper_id": "2404.11782v1",
        "paper_title": "Requal-lm: Reliability and Equity through Aggregation in Large Language ModelsThis work was supported in part by NSF 2107290.",
        "research_background": "The provided sections outline a focused and forward-looking examination of the challenges and proposed solutions in leveraging Large Language Models (LLMs) for decision-making tasks. Here\u2019s a breakdown of the paper\u2019s motivation, research problem, and relevant prior work:\n\n**Motivation:**\n1. **Versatility of LLMs:** The paper is motivated by the rapid emergence of LLMs as powerful tools that effectively address traditionally challenging tasks across various computer science fields.\n2. **Unreliable Single LLM Queries:** Given that LLMs operate based on probability distributions and are sensitive to input ordering, their outputs can be inconsistent across different runs. This randomness and non-determinism can undermine their reliability in critical decision-making scenarios.\n3. **Bias Concerns:** The paper also emphasizes the importance of addressing machine bias in LLMs to ensure diversity and neutrality in outputs, a vital concern given the critique faced by many data-driven technologies.\n\n**Research Problem:**\nThe core research problem revolves around enhancing the reliability and equity of LLM outputs. Specifically:\n1. **Reliability Issue:** The need to aggregate LLM outputs to mitigate the unreliability arising from the inherent probabilistic nature and sensitivity to input ordering in LLMs.\n2. **Equity Issue:** Addressing the bias within LLM outputs to promote fairness and diversity, ensuring that the historical biases and stereotypes do not skew the decisions made using these models.\n\n**Relevant Prior Work:**\n1. **Performance Criteria Combination:** Literature indicates that combining performance criteria probabilistically can lead to significant variations in outputs (Guan et al., 2019).\n2. **Symmetric Problem and Input Ordering:** Previous studies have demonstrated that for symmetric problems, the output of LLMs changes based on the input order (Gao et al., 2023). The issue is affirmed by the authors' observation in their experiments.\n3. **Monte-Carlo Methods:** The proposed solution leverages Monte-Carlo methods (Hammersley, 2013) for repeated sampling and aggregation to derive more reliable outputs.\n\nIn summary, the paper seeks to address the unreliability and bias in LLM outputs by proposing Requal-lm, an approach that uses repeated sampling to determine a centroid from multiple outputs, striving for a reliable and equitable result. This approach is designed to be adaptable to different LLMs without the need for pre-training or fine-tuning, thereby offering a broadly applicable solution to these challenges.",
        "methodology": "The methodology section of the paper outlines the proposed system \"Requal-lm,\" which aims to enhance reliability and equity in the outputs of large language models (LLMs). Here is a summary of the key components and innovations described:\n\n### Key Components\n\n1. **Input Tasks:**\n   - The method considers various tasks such as subset selection, sentence completion, or assembling a team of experts. These tasks are described in the form of natural language instructions or prompts.\n\n2. **Demographic Groups:**\n   - The system assumes the existence of at least one sensitive attribute (e.g., sex) that is used to define demographic groups (e.g., {male, female}). These groups help in specifying and evaluating output bias.\n\n3. **Large Language Model (LLM):**\n   - The method relies on at least one LLM to generate answers to tasks. The LLM is randomized, meaning tokens are drawn sequentially based on the model\u2019s token-probabilities (top-k or top-p%).\n   - The LLM is treated as a black-box oracle, making the Requal-lm system adaptable to both closed-source and open-source LLMs.\n\n4. **Text Embedding:**\n   - An external text embedding model is used to transform text into embedding vectors. This model can be any state-of-the-art technique, but the paper exemplifies using \"Instructor\", a method generating task-specific embeddings in accordance with provided instructions (Su et al., 2023).\n   - The system measures the similarity and distance between text phrases using cosine similarity and Euclidean distance between their embeddings.\n   \n### Innovation and Objectives\n\n- **Probability Computation:**\n   - For any prompt \\( P \\), an output \\( O \\) comprising a sequence of tokens \\( o_1, o_2, \\ldots, o_L \\) is generated. The product of token probabilities, \\( \\prod_{i=1}^L p(o_i) \\), is used to compute the likelihood of output \\( O \\).\n\n- **Bias Categorization:**\n   - **Inevitable Bias:**\n     - Bias inherent to the task, like summarizing a gender-specific paragraph, is termed as \"inevitable bias.\" This is formulated as \\( \\text{bias}(O) \\geq b^* \\), where \\( b^* \\) is the minimal achievable bias for that task.\n   - **Harmful Bias:**\n     - Bias that is not inevitable and yields harmful stereotypes is termed \"harmful bias.\" The goal is to minimize this harmful bias, calculated as \\( b_{\\text{harmful}}(O) = \\text{bias}(O) - b^* \\).\n\n- **Objective:**\n   - Given a task \\( T \\) via a prompt \\( P \\) and the demographic groups \\( D \\), the system aims to identify an output \\( O \\) that maximizes the output quality \\( Q \\) while minimizing harmful bias \\( b_{\\text{harmful}} \\).\n\nIn summary, Requal-lm aims to achieve outputs that are both high-quality and equitable by managing and mitigating harmful biases, leveraging text embeddings and probability computations, and treating LLMs as black-box oracles to ensure broad adaptability.",
        "main_experiment_and_results": "**Main Experiment Setup and Results:**\n\n**Tasks and Objectives:**\n- **Tasks:** The main experiment involves three tasks: Subset Selection, Chat Completion, and Masked Language Prediction.\n- **Objective:** The objective is to assess the capability of Requal-lm in mitigating harmful bias (especially against minority groups, such as females) and producing reliable and equitable results.\n\n**Datasets:** The tasks are performed on outputs generated by Llama2-70b.\n\n**Evaluation Metrics:**\n1. **Reliability:** Defined in Definition 1 (not provided).\n2. **Bias:** Defined in Definition 2 (not provided) and quantified as the disparity between the similarity of the output to the majority and minority groups. Negative values on the bias axis are acceptable.\n\n**Additional Metrics:**\n- For Masked Language Prediction: Calculation of non-stereotypical and neutral responses.\n- For Subset Selection: Female-to-male ratio in the results.\n\n**Baselines:**\n1. **Pair-Ranker:** A pair-wise ranking model using a cross-attention Transformer to score a pair of output candidates based on the input text (proposed by Jiang et al., 2023).\n2. **First-Response:** Queries the LLM once and returns its output.\n3. **Debiased-GPT:** Tries to debias outputs from a set of responses, given a task-specific prompt.\n\n**Models from Requal-lm:**\n- **Weighted Output:** The output closest to the weighted (equitable) centroid.\n- **Unweighted Output:** The output most similar to the centroid (maximum reliability).\n- **Min-bias Output:** The output with minimum bias.\n\n**Results:**\nThe experimental results demonstrate that Requal-lm is effective in mitigating bias and maintaining high reliability across the tasked evaluations. While specific numerical results are not provided here, the comparative analysis indicates that Requal-lm outperforms all three baselines (Pair-Ranker, First-Response, and Debiased-GPT) in returning equitable and reliable results, thus validating the efficacy of the proposed method."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the capacity of Requal-lm to mitigate bias and provide reliable results in large language models.",
            "experiment_process": "The analysis is performed on three tasks: Subset Selection, Chat Completion, and Masked Language Prediction. The main evaluation metrics used are reliability and bias. The subset selection task uses Students and Forbes 2022 Billionaire datasets. The baseline models for comparison include Pair-Ranker, First-Response, and Debiased-GPT. The Requal-lm's outputs are categorized into Weighted Output, Unweighted Output, and Min-bias Output for comparison.",
            "result_discussion": "The Weighted Output of Requal-lm demonstrated superior performance in bias and reliability compared to baselines, shifting bias distribution significantly to the left and demonstrating lower bias. Debiased-GPT had lower bias but had invalid outputs. Reliability of Requal-lm was close to 1 and comparable to Unweighted Output, differing significantly from the highly fluctuating reliability values of the Pair-Ranker.",
            "ablation_id": "2404.11782v1.No1"
        },
        {
            "research_objective": "Evaluate the Masked Language Prediction capability of Requal-lm on the WinoBias dataset to reduce harmful bias.",
            "experiment_process": "The task involves predicting a masked term within sentences that associate professions with specific genders. The Requal-lm model categorized sentences associating professions with male or female genders and performed masked language prediction. The focus was on reducing bias through neutral or anti-stereotype responses. Evaluation included counting the number of pro-stereotype, anti-stereotype, and neutral responses.",
            "result_discussion": "Requal-lm managed to replace masked words with gender-neutral or anti-stereotype terms effectively, showing significant improvement with increasing outputs. The results demonstrated reduced bias while retaining reliability, showing a balanced and equitable preference for both genders in outcomes.",
            "ablation_id": "2404.11782v1.No2"
        },
        {
            "research_objective": "Test the effectiveness of Requal-lm in mitigating bias in Chat Completion tasks using StereoSet Intersentences.",
            "experiment_process": "The task uses biased prompts created by merging context sentences with stereotype sentences. The model was incentivized to generate outputs based on biased prompts, with penalties for refusal. The experiment evaluates bias reduction and reliability using comparison baselines including Pair-Ranker, First-Response, and Debiased-GPT.",
            "result_discussion": "The bias gap between Unweighted Output and Min-bias Output was negligible. Weighted Output showed significant bias reduction, comparable to Min-bias Output in both 5 and 30 sample scenarios, while displaying higher reliability compared to baselines. The consistent pattern in results for non-binary demographic groups indicates Requal-lm\u2019s broader applicability.",
            "ablation_id": "2404.11782v1.No3"
        }
    ]
}