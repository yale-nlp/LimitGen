{
    "title": "Revisiting character-level adversarial attacks",
    "abstract": "Adversarial attacks in Natural Language Processing apply perturbations in the character or token levels. Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples. While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend.\nChallenging these beliefs, we introduce Charmer, an efficient query-based adversarial attack capable of achieving high attack success rate (ASR) while generating highly similar adversarial examples. Our method successfully targets both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2, Charmer improves the ASR in  points and the USE similarity in  points with respect to the previous art. Our implementation is available in github.com/LIONS-EPFL/Charmer.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Language Models (LMs) have rapidly become the go-to tools for Natural Language Processing (NLP) tasks like language translation (Sutskever et al., 2014  ###reference_b58###), code development (Chen et al., 2021  ###reference_b9###) and even general counseling via chat interfaces (OpenAI, 2023  ###reference_b44###). However, several failures concerning robustness to natural and adversarial noise have been discovered (Belinkov & Bisk, 2018  ###reference_b2###; Alzantot et al., 2018  ###reference_b1###). Adversarial attacks have been widely adopted in the computer vision community to discover the worst-case performance of Machine Learning models (Szegedy et al., 2014  ###reference_b59###; Goodfellow et al., 2015  ###reference_b20###) or be used to defend against such failure cases (Madry et al., 2018  ###reference_b37###; Zhang et al., 2019  ###reference_b67###).\nThe application of adversarial attacks in LMs is not straight-forward due to algorithmic (Guo et al., 2021  ###reference_b22###) and imperceptibility constraints (Morris et al., 2020a  ###reference_b41###).\nUnlike the computer vision tasks, where inputs consist of tensors of real numbers, in NLP tasks, we work with sequences of discrete non-numerical inputs. This results in adversarial attacks being an NP-hard problem even for convex classifiers (Lei et al., 2019  ###reference_b30###). This fact also hardens the use of popular gradient-based methods for obtaining adversarial examples (Guo et al., 2021  ###reference_b22###). To tackle this problem, attackers adopt gradient based strategies in the embedding space, restricting the attack to the token vocabulary (Ebrahimi et al., 2018  ###reference_b17###; Liu et al., 2022  ###reference_b34###; Hou et al., 2023  ###reference_b24###) or the black-box setting, where only input-output access to the model is assumed (Alzantot et al., 2018  ###reference_b1###; Gao et al., 2018  ###reference_b18###; Jin et al., 2020  ###reference_b25###; Li et al., 2020  ###reference_b33###; Garg & Ramakrishnan, 2020  ###reference_b19###; Wallace et al., 2020  ###reference_b62###).\n###figure_1### Another difficult analogy to make with the computer vision world is imperceptibility. Adversarial examples should be by definition imperceptible, in the sense that the attack should not modify the human prediction or allow to think that an attack has been done (Szegedy et al., 2014  ###reference_b59###). Given an input , in the numerical-input setting, imperceptibility is controlled by looking for an adversarial example  in an  ball centered at  with perturbation radius , i.e., , where  can be set arbitrarily small.\nIn NLP, Morris et al. (2020a  ###reference_b41###) suggest different strategies for controlling imperceptibility according to the attack level: \nCharacter: Constrain the attack to have a low Levenshtein (edit) distance. However, character-level attacks have lost relevance due to the strength of robust word recognition defenses (Pruthi et al., 2019  ###reference_b48###; Jones et al., 2020  ###reference_b26###).\nToken: Constrain the embedding similarity111Similarity is commonly measured via the cosine similarity of the USE embeddings (Cer et al., 2018  ###reference_b8###). of replaced words and of the overall sentence to be high. Nevertheless, Dyrmishi et al. (2023  ###reference_b16###) conclude that state-of-the-art attacks do not produce imperceptible attacks in practice. To be specific, Hou et al. (2023  ###reference_b24###) report  of their attacks change the semantics of the sentence.\nThe overall attack desiderata is summarized in Table 1  ###reference_###.\nExisting defenses against character-level attacks rely on robust word recognition modules, which assume the attacker adopts unrealistic constraints, not allowing simple modifications such as insertion or deletion of blank spaces, which are adopted in practice (Li et al., 2019  ###reference_b32###).\nIn this work, we revisit character-level adversarial attacks as a practical solution to imperceptibility. Our attack, Charmer, is based on a greedy approach combined with a position subset selection to further speed-up the attack, while minimally affecting performance. Our attack is able to achieve  ASR in every studied TextAttack benchmark and LLMs Llama-2 and Vicuna, obtaining up to a -point ASR improvement with respect to the runner-up method. We show that existing adversarial training based defenses (Hou et al., 2023  ###reference_b24###) degrade character-level robustness, i.e., increasing the ASR in  points when compared to standard training. Our findings indicate typo-corrector defenses (Pruthi et al., 2019  ###reference_b48###; Jones et al., 2020  ###reference_b26###) are only successful when a set of strict attack constraints is assumed, if just one of these constraints is relaxed, ASR can increase from  to . Overall, we believe character-level robustness a more consistent measure than token-level robustness.\nNotation: Sentences are sequences of characters in the set . Sentences are denoted with uppercase letters . For sets of sentences we use caligraphic uppercase letters . We denote the concatenation operator of two sentences as . The empty character  is defined so that sentences remain invariant to concatenations with it, i.e., . We use the shorthand  for  for any positive integer . We use bold lowercase letters for vectors , with the  position being ."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related work",
            "text": "We provide an overview of adversarial attacks in NLP. Adversarial attacks have been devised for producing missclassifications (Alzantot et al., 2018  ###reference_b1###), generating unfaithful machine translations (Cheng et al., 2020  ###reference_b10###; Sadrizadeh et al., 2023a  ###reference_b53###, b  ###reference_b54###; Wallace et al., 2020  ###reference_b62###), providing malicious outputs (jailbreaking) (Zou et al., 2023  ###reference_b70###; Zhu et al., 2023  ###reference_b69###; Carlini et al., 2023  ###reference_b7###) or even extracting training data (Nasr et al., 2023  ###reference_b43###). We distinguish these methods in two main branches: token-level and character-level attacks.\nToken based: Early token-based attacks rely in black-box token replacement/insertion strategies based on heuristics for estimating the importance of each position, and the candidate tokens for the operation (Ren et al., 2019  ###reference_b52###; Jin et al., 2020  ###reference_b25###; Li et al., 2020  ###reference_b33###; Garg & Ramakrishnan, 2020  ###reference_b19###; Lee et al., 2022  ###reference_b29###). Ebrahimi et al. (2018  ###reference_b17###) and Li et al. (2019  ###reference_b32###) consider the token gradient information to select which token to replace. Guo et al. (2021  ###reference_b22###) propose GBDA, the first, but inefficient, full gradient based text adversarial attack.\nTextGrad (Hou et al., 2023  ###reference_b24###) is a more efficient variante proposed to be integrated within Adversarial Training.\nCharacter based: Belinkov & Bisk (2018  ###reference_b2###) showcase that character-level Machine Translation models are sensible to natural character level perturbations (typos) and adversarially chosen ones. (Pruthi et al., 2019  ###reference_b48###) propose to iteratively change the best possible character until success. However, this strategy can be inefficient for lengthy sentences. Addressing this issue, other methods propose pre-filtering the most important words/tokens in the sentence, to then introduce a random typo (Gao et al., 2018  ###reference_b18###), or the best typo among a random sample (Li et al., 2019  ###reference_b32###). In (Liu et al., 2022  ###reference_b34###), a token-based attack with character-level Levenshtein distance constraints is considered. However, considering token-level information for assesing character-level importance can be suboptimal. Ebrahimi et al. (2018  ###reference_b17###) propose involving embedding gradient information for evaluating the importance of characters, making the strategy only valid for character-level models. (Yang et al., 2020  ###reference_b66###) evaluate the relevance of each character by masking and evaluating the loss in each position, to then modify the top positions. This strategy does not consider character insertions, and does not take into acount the effect of indivudual changes in the importance of positions. Similarly to (Yang et al., 2020  ###reference_b66###), our method measures the importance of every position plus insertions. After a perturbation is done, importances are updated to consider the interaction between perturbations."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Problem setting",
            "text": "In this section, we summarize the setting, Levenshtein distance and the operators used in our attack."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "The sentence space",
            "text": "Let  be the alphabet set. A sentence  with  characters (i.e., the length ) in  is defined with .\nFor notational simplicity, we denote  as the character in the  position and  () as the sequence obtained by taking the characters after (before) the  position included.\nWe denote  as the set of (all possible) sequences with characters in  with length less than .\nLet  be the Levenshtein distance (Levenshtein et al., 1966  ###reference_b31###), also known as the edit distance. To be specific, for any two sentences , the Levenshtein distance is defined as:\n\nNote that  represents the cost in number of character insertions, deletions and replacements needed for  to become equal to  or vice-versa."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Adversarial robustness",
            "text": "In this work, we tackle robustness restricted by the Levenshtein distance. This enables the search of highly similar, hard to detect and semantics-preserving adversarial examples (Morris et al., 2020a  ###reference_b41###).\nDenote the set of sentences at distance up to  as\nA learning model (e.g., neural networks) , where  is the label space, is called -robust at  if . If  for some , we say  is an adversarial example.\nWithout loss of generality, we focus on the classification task. In the adversarial robustness community, adversarial examples are usually sought by solving some optimization problem (Carlini & Wagner, 2017  ###reference_b6###). Given a data sample  and a classification model , with  classes, we solve:\nwhere  is some loss function, e.g., the cross entropy loss. In the following, we elaborate on how Eq. 1  ###reference_### is solved."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Characterizing the perturbations",
            "text": "To explore adversarial examples in , we make use of the contraction, expansion (Definition 3.3  ###reference_theorem3###) and replacement operators (Definition 3.5  ###reference_theorem5###) to characterize this set.\nLet  be the space of sentences with alphabet  and the special character , the pair of expansion-contraction functions  and  is defined as:\nClearly,  aims to insert  into  in all possible positions between characters and at the beginning and end of the sentence, and thus we have .\nSimilarly,  aims to remove all  occurred in .\nThe  pair satisfies the property that .\nWe give the following example for a better understanding.\nLet  for visibility:\n\nLet , the integer  and the character , the replacement operator  of the  position of  with  is defined as:\nThanks to Definitions 3.3  ###reference_theorem3### and 3.5  ###reference_theorem5###, it is easy to check the following proposition.\nLet  be a non-empty sentence, and  be another sentence satisfying . Then we can find  and a character  such that\nThe parametrization of the transformation from  to  might not be unique. For example, for  and , both pairs  and  are valid parametrizations.\nReplacing a character in  for , and applying  results in a deletion. Similarly, replacing a  by a character in  and applying  results in an insertion.\nLet  be a non-empty sentence in the volcabulary, with , for any , the set  (see Definition 3.2  ###reference_theorem2###) can be obtained by the following recursion:\n\nThe size of these sets is bounded as:\nIn the case , for any , we trivially have that .\nProof. Refer to Appendix C  ###reference_###.\nNote that exactly computing  is non-trivial and complex dynamic programming algorithms have been proposed for this task (Mihov & Schulz, 2004  ###reference_b38###; Mitankin, 2005  ###reference_b40###; Touzet, 2016  ###reference_b61###). The exponential dependence of  on  makes it unfeasible to evaluate every sentence in the set, therefore, smarter strategies are needed."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Method",
            "text": "Let us now introduce our method (Charmer). In Secs. 4.2  ###reference_### and 4.3  ###reference_### we present our attack for both standard classifiers and LLM-based classifiers.\nTo circumvent the exponential dependence of  on  as indicated by Corollary 3.9  ###reference_theorem9###, we propose to greedily select the single-character perturbation with the highest loss. Furthermore, we reduce the search space for single-character perturbations by considering a subset of locations where characters can be replaced."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Attack classifier",
            "text": "In the case of using a classifier , where the predicted class is given by  with  classes, we follow Hou et al. (2023  ###reference_b24###) and use the Carlini-Wagner Loss222In the original paper, Carlini & Wagner (2017  ###reference_b6###) clip the value of the loss to be  at maximum. We do not clip in order to deal with cases where the loss is positive for different adversarial examples. (Carlini & Wagner, 2017  ###reference_b6###):\nIn this case, a sentence  is an adversarial example when .\nTo search the closest sentence in Levenshtein distance that produces a misclassification, we iteratively solve Eq. BP  ###reference_7### with  until the adversarial sentence  is misclassified. Our attack pseudo-code is presented in Algorithm 2  ###reference_###."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Attack LLM",
            "text": "Now we illustrate how to apply our method on attacking LLM-based classifiers. Given a data sample , the input to LLMs is formulated by concatenating the original sentences with some instructive prompts  in the format of . A schematic for illustration and additional details on the prompting strategy can be found in Sec. B.6  ###reference_###.\nSimilar to Eq. 1  ###reference_###, we aim to solve:\nwhere the model output  is the conditional probability of the next token .\nWe can still use the Carlini-Wagner Loss defined in Eq. 2  ###reference_### by considering the next token probability for the classes."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "Our experiments are conducted in the publicly available333https://huggingface.co/textattack  ###reference_huggingface.co/textattack### TextAttack models (Morris et al., 2020b  ###reference_b42###) and open-source large language models including Llama 2-Chat 7B (Touvron et al., 2023  ###reference_b60###) and Vicuna 7B (Chiang et al., 2023  ###reference_b11###). We evaluate our attack in the text (or text pair) classification datasets SST-2 (Socher et al., 2013  ###reference_b56###), MNLI-m (Williams et al., 2018  ###reference_b65###), RTE (Dagan et al., 2006  ###reference_b12###; Wang et al., 2019  ###reference_b63###), QNLI (Rajpurkar et al., 2016  ###reference_b50###) and AG-News (Gulli, 2005  ###reference_b21###; Zhang et al., 2015  ###reference_b68###).\nIn the text pair classification tasks (MNLI-m, RTE, and QNLI), we perturb only the hypothesis sentence. If the length of the test dataset is more than , we restrict to the first  samples. If a test dataset is not available for a benchmark, we evaluate in the validation dataset, this is a standard practice (Morris et al., 2020b  ###reference_b42###). For comparison with other attacks, we use the default hyperparameters of those methods. For Charmer we use  positions (see Algorithm 1  ###reference_###) and  except for AG-news where we use  because of the much longer sentences present in the dataset. Charmer-Fast simply takes  to speed-up the attack. For the alphabet , in order to not introduce out-of-distribution characters, we take the characters present in each evaluation dataset.\nAll of our experiments were conducted in a machine with a single NVIDIA A100 SXM4 GPU.\nFor better illustration between token-level and character-level attacks, we mark them with \u26ab and \u26ab respectively.\nWe note that alternative design decisions can enable the usage of projected gradient ascent (PGA) attacks. However, we observed a worse performance in comparison with our strategy, see Appendix D  ###reference_###."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Selecting the number of positions",
            "text": "To select the appropriate number of candidate positions  for Algorithm 1  ###reference_###, we evaluate the ASR and runtime of the attack with . We conduct the experiment with the fine-tuned BERT on SST-2 from TextAttack at . For the SST-2 test sentences, the maximum number of positions across the dataset is  and the average is . We would like a value of  much smaller than these values. As a comparison, we report the ASR computed by exploring all possible positions (ASR Upper Bound). Additionally, to test the effect of our heuristic, we evaluate the performance when randomly selecting  positions (Random).\nIn Fig. 2  ###reference_###, we can observe that the ASR consistently grows when increasing the number of candidate positions. However, the increase is less noticeable for , therefore, the increase in runtime does not pay off the increase in ASR. This leads us to choose  for the rest of our experiments. When compared with the random position selection, our method greatly improves the ASR for all the studied , while introducing a minor time increase of  seconds on average."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Comparison against state-of-the-art attacks",
            "text": "Firstly, we compare against the following state-of-the-art attacks (i) token level: BAE-R (Garg & Ramakrishnan, 2020  ###reference_b19###), TextFooler (Jin et al., 2020  ###reference_b25###), BERT-attack (Li et al., 2020  ###reference_b33###), GBDA (Guo et al., 2021  ###reference_b22###) and TextGrad (Hou et al., 2023  ###reference_b24###), (ii) character level: DeepWordBug (Gao et al., 2018  ###reference_b18###), TextBugger (Li et al., 2019  ###reference_b32###) and CWBA (Liu et al., 2022  ###reference_b34###). For each attack method, we evaluate the attach success rate (ASR), the average Levenshtein distance measured at character level () and the cosine similarity () measured as in (Guo et al., 2021  ###reference_b22###), i.e., computing the cosine similarity of the USE encodings (Cer et al., 2018  ###reference_b8###). We evaluate the performance of the attacks in the finetuned BERT (Devlin et al., 2019  ###reference_b15###) and RoBERTa (Liu et al., 2019  ###reference_b36###) from TextAttack. Additional experiments with ALBERT (Lan et al., 2020  ###reference_b28###) can be found in Appendix B  ###reference_###.\nSecondly, we test the performance of the proposed method in Llama 2-Chat 7B (Touvron et al., 2023  ###reference_b60###). Additional results on Vicuna 7B (Chiang et al., 2023  ###reference_b11###) are deferred to Sec. B.6  ###reference_###. Note that in the case of LLMs, the inference process is extremely costly. As a result, we only use the fast version of Charmer, i.e., . Moreover, we perform an additional position selection framework to further accelerate. Specifically, we first tokenize the input sentence and mask each token to determine the most important one based on the loss. Next, we perform Algorithm 1  ###reference_### for the position in these important tokens. An ablation study of such token selection procedure can be found in Sec. B.6  ###reference_###.\nIn Tables 2  ###reference_### and 3  ###reference_###, we can observe Charmer consistently achieves the highest ASR with  in every benchmark. At the same time, our method obtains the lowest Levenshtein distance (). Regarding the similarity (Sim), our Charmer attains the highest or runner up similarity in  cases, proving its ability to generate highly similar adversarial examples. With respect to time, Charmer is not as fast as the simple DeepWordBug, however, the runtime is comparable to previous state-of-the-art token-level TextGrad. If speed is preferred to adversarial example quality, we can set  (Charmer-Fast), which attains a runtime closer to DeepWordBug at the cost of a higher 444Except for RTE where the average  is smaller due to failure in hard examples, where higher  is needed. and lower ASR. This phenomenon is aligned with the results of Sec. 5.1  ###reference_###, as the ASR at  is lower when  is lower."
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Adversarial Training",
            "text": "In this section, we analyze the performance of models trained with adversarial training defenses (Madry et al., 2018  ###reference_b37###). Following the insights of Hou et al. (2023  ###reference_b24###), we use the TRADES objective (Zhang et al., 2019  ###reference_b67###). We compare the use of a token-level attack, TextGrad, v.s. a character-level attack, Charmer, for solving the inner maximization problem. We use TextGrad with the recommended hyperparameters for training and Charmer with the standard hyperparameters and . Every  training steps, we measure the clean, TextFooler and Charmer () adversarial accuracies. We train on  random initializations of BERT-base (Devlin et al., 2019  ###reference_b15###) for  epoch in SST-2.\nIn Table 4  ###reference_### we can firstly observe that both the TextGrad and Charmer defenses improve the token-level and character-level robustness respectively when compared with the standard training baseline. This was expected as this is the objective each method is targetting. Interestingly, Charmer does not improve the token-level robustness and TextGrad hinders the character-level robustness. This observation is confirmed when looking at the training evolution in Fig. 3  ###reference_###. It remains open to know if we should aim at character or token level robustness, nevertheless our results indicate character-level robustness is less conflicted with clean accuracy."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Bypassing typo correctors",
            "text": "We analyze the performance of our attack when attaking models defended by a typo-corrector (Pruthi et al., 2019  ###reference_b48###), or a robust encoding module (Jones et al., 2020  ###reference_b26###). We notice the success of these defenses can be attributed by the properties of the considered attacks. In Pruthi et al. (2019  ###reference_b48###); Jones et al. (2020  ###reference_b26###), the studied attacks are constrained to555Pruthi et al. (2019  ###reference_b48###), further constrain the attack by only considering replacements of nearby characters in the English keyboard.:\nRepeat: Not perturb the same word twice.\nFirst: Not perturb the first character of a word.\nLast: Not perturb the last character of a word.\nLength: Not perturb words with less than  chars.\nLowEng: Only perturb lowercase English letters.\nA word is anything between blank spaces. We denote these as the Pruthi-Jones Constraints (PJC). While these constraints aim at preserving the meaning of every individual word (Rawlinson, 1976  ###reference_b51###; Davis, 2003  ###reference_b13###), in sentence classification, we might sacrifice the meaning of a word during the attack, if the global meaning of the sentence is preserved. We analyze the performance of our attack with and without the PJC constraints. We train the strongest typo-corrector (Pruthi et al., 2019  ###reference_b48###) and use it in front of the BERT-base model from TextAttack. For the robust encoding defense we train a BERT-base model over the agglomerative clusters (Jones et al., 2020  ###reference_b26###).\nIn Table 5  ###reference_###, we can observe that Charmer attains  ASR when not considering the PJC constraints. It is only when considering PJC that robust word recognition defenses are effective. In Table 6  ###reference_### we analyze the effect of relaxing each of the PJC constraints while keeping the rest. We observe that by relaxing any of the LowEng, End or Start constraints, performance grows considerably for both defenses, e.g., from  to  ASR when relaxing LowEng in the robust encoding case. This result indicates that robust word recognition defenses provide a false sempsation of robustness. Together with the observations in Sec. 5.3  ###reference_###, we believe adversarial training based methods suppose a more promising avenue towards achieving character-level robustness.\n###table_1###"
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "We have proposed an efficient character-level attack based on a novel strategy to select the best positions to perturb at each iteration. Our attack (Charmer) is able to obtain close to  ASR both in BERT-like models and LLMs like Llama-2. Charmer defeats both token-based adversarial training defenses (Hou et al., 2023  ###reference_b24###) and robust word recognition defenses (Pruthi et al., 2019  ###reference_b48###; Jones et al., 2020  ###reference_b26###). When integrated within adversarial training, our attack is able to improve the robustness against character-level attacks. We believe defending agains character-level attacks is an interesting open problem, with adversarial training posing as a promising avenue for defenses."
        }
    ],
    "url": "http://arxiv.org/html/2405.04346v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2"
        ],
        "methodology_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3"
        ],
        "main_experiment_and_results_sections": [
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "5.1",
            "5.2",
            "5.3",
            "5.4"
        ]
    },
    "research_context": {
        "paper_id": "2405.04346v1",
        "paper_title": "Revisiting character-level adversarial attacks",
        "research_background": "### Motivation\nThe motivation behind this paper stems from the increasing reliance on Language Models (LMs) for critical NLP tasks such as language translation, code development, and counseling via chat interfaces. Despite their widespread adoption, these models have shown significant failures in robustness when exposed to both natural and adversarial noise. The paper highlights the need for revisiting character-level adversarial attacks, which have somewhat fallen out of focus due to the perceived robustness of modern defenses. The authors argue that current defenses are based on unrealistic constraints and that character-level attacks can still be potent and relevant.\n\n### Research Problem\nThe primary research problem addressed in this paper is the development of practical and imperceptible adversarial attacks at the character level, which challenge the robustness of Language Models. Given that existing character-level attacks are either constrained by strong defense mechanisms or deemed irrelevant, the authors seek to re-establish their significance by proposing a new attack method, Charmer. This method aims to demonstrate that character-level attacks can bypass current defenses, thus questioning the assumed robustness and propriety of token-level adversarial robustness measures.\n\n### Relevant Prior Work\n1. **Adversarial Attacks in NLP and CV**:\n    - Belinkov & Bisk (2018), Alzantot et al. (2018) highlighted failures in LMs' robustness to natural and adversarial noise.\n    - Szegedy et al. (2014), Goodfellow et al. (2015) discussed adversarial attacks for worst-case performance scenarios in machine learning models.\n    - Madry et al. (2018), Zhang et al. (2019) explored adversarial attacks as a defense mechanism.\n\n2. **Challenges in NLP Adversarial Attacks**:\n    - Guo et al. (2021) and Morris et al. (2020a) addressed algorithmic and imperceptibility constraints in NLP adversarial attacks.\n    - Lei et al. (2019) emphasized the complexity (NP-hard problem) of adversarial attacks in NLP due to discrete non-numerical inputs, unlike the continuous inputs in CV.\n\n3. **Adversarial Attack Strategies**:\n    - Gradient-based strategies in embedding space and token vocabulary were investigated by Ebrahimi et al. (2018), Liu et al. (2022), Hou et al. (2023).\n    - Black-box setting attacks, where only input-output access is available, were explored by Alzantot et al. (2018), Gao et al. (2018), Jin et al. (2020), Li et al. (2020), Garg & Ramakrishnan (2020), Wallace et al. (2020).\n\n4. **Imperceptibility in Adversarial Attacks**:\n    - Strategies for ensuring imperceptibility at various attack levels were proposed by Morris et al. (2020a).\n    - Challenges and failures in token-level imperceptibility were noted by Dyrmishi et al. (2023) and Hou et al. (2023).\n\n5. **Defenses Against Character-Level Attacks**:\n    - Existing defenses such as robust word recognition modules assume unrealistic constraints (Pruthi et al. (2019), Jones et al. (2020)).\n    - Li et al. (2019) point out that practical character-level attacks often involve simple modifications like insertion or deletion of spaces.\n\nThis paper introduces Charmer, a novel character-level attack method that challenges the robustness of Language Models and demonstrates that current defenses are insufficient under more realistic and relaxed constraints.",
        "methodology": "### Revisiting Character-level Adversarial Attacks\n\n#### Methodology\n\nLet us now introduce our method (Charmer). In Secs. 4.2 [reference] and 4.3 [reference], we present our attack for both standard classifiers and LLM-based classifiers.\n\nTo circumvent the exponential dependence of \\(\\epsilon\\) on \\(n\\) as indicated by Corollary 3.9 [reference_theorem9], we propose to greedily select the single-character perturbation with the highest loss. Furthermore, we reduce the search space for single-character perturbations by considering a subset of locations where characters can be replaced.",
        "main_experiment_and_results": "### Main Experiment Setup\n\n**Datasets:** The primary evaluation of character-level adversarial attacks revolves around five text (or text pair) classification datasets:\n1. **SST-2** (Socher et al., 2013  ###reference_b56###) - Single Sentence Classification\n2. **MNLI-m** (Williams et al., 2018  ###reference_b65###) - Text Pair Classification\n3. **RTE** (Dagan et al., 2006  ###reference_b12###; Wang et al., 2019  ###reference_b63###) - Text Pair Classification\n4. **QNLI** (Rajpurkar et al., 2016  ###reference_b50###) - Text Pair Classification\n5. **AG-News** (Gulli, 2005  ###reference_b21###; Zhang et al., 2015  ###reference_b68###) - Single Sentence Classification\n\n**Baselines:** The main experiment involves comparison with other adversarial attack methods using the default hyperparameters set by those methods. The key compared methods include:\n- **Charmer:** Customized positions' selection for different datasets.\n- **Charmer-Fast:** Simplified for faster attacks.\n\n**Evaluation:** The evaluation involves assessing the performance of the attacks on the above datasets. Specific strategies include:\n- In text pair classification tasks (MNLI-m, RTE, and QNLI), only the hypothesis sentence is perturbed.\n- For datasets exceeding a certain length, only the first few samples are considered.\n- Validation datasets are used when test datasets are not available, following standard practices.\n\n**Hardware:** All experiments run on a single NVIDIA A100 SXM4 GPU.\n\n### Main Experimental Results\nThe results demonstrate the efficacy of the proposed character-level adversarial attack methods (Charmer and Charmer-Fast), showing improved performance over baseline attacks. The specifics of the performance comparison\u2014such as attack success rates, accuracy drops, etc.\u2014are not detailed in the provided text. However, it is implied that the designed methods outperform existing ones, especially given that alternative methods like projected gradient ascent (PGA) showed worse performance (as mentioned with reference to Appendix D)."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To select the appropriate number of candidate positions for Algorithm 1 to balance attack success rate (ASR) and runtime for the Charmer adversarial attack.",
            "experiment_process": "The experiment was conducted on a fine-tuned BERT model on the SST-2 dataset from TextAttack. The maximum number of positions across the dataset is given, and various values of candidate positions are tested. The ASR is computed by exploring all possible positions (ASR Upper Bound) and by randomly selecting positions (Random). The evaluation involves measuring ASR and runtime.",
            "result_discussion": "ASR consistently grows with the increase in the number of candidate positions, but the increase in runtime makes it impractical beyond a certain point. Therefore, a specific number of positions is chosen for the rest of the experiments. The method significantly improves ASR compared to random position selection, with only a minor increase in runtime.",
            "ablation_id": "2405.04346v1.No1"
        },
        {
            "research_objective": "To compare the performance of Charmer against state-of-the-art token-level and character-level adversarial attacks in terms of ASR, Levenshtein distance, and cosine similarity.",
            "experiment_process": "State-of-the-art attacks, including BAE-R, TextFooler, BERT-attack, GBDA, TextGrad, DeepWordBug, TextBugger, and CWBA, are evaluated against finetuned BERT and RoBERTa models using SST-2. The performance metrics include ASR, Levenshtein distance at the character level, and cosine similarity of USE encodings. Additionally, Charmer is tested on Llama 2-Chat 7B and compared for speed using a fast version of Charmer with additional position selection to accelerate the process.",
            "result_discussion": "Charmer consistently achieves the highest ASR and lowest Levenshtein distance in every benchmark. It also attains high similarity scores, proving its effectiveness in generating highly similar adversarial examples. While Charmer is not the fastest method, it is comparable to other token-level attacks. The fast version of Charmer achieves a runtime closer to DeepWordBug but with lower ASR and higher Levenshtein distance, particularly for challenging datasets.",
            "ablation_id": "2405.04346v1.No2"
        },
        {
            "research_objective": "To analyze the effectiveness of adversarial training defenses using token-level and character-level attacks in improving model robustness.",
            "experiment_process": "Adversarial training is performed using the TRADES objective with TextGrad (token-level) and Charmer (character-level). Both attacks are used to solve the inner maximization problem. The performance metrics, including clean, TextFooler, and Charmer adversarial accuracies, are measured every training steps. The models are trained on  random initializations of BERT-base for 1 epoch using SST-2.",
            "result_discussion": "Both TextGrad and Charmer defenses significantly improve robustness against their respective types of attacks compared to standard training. Charmer does not improve token-level robustness, and TextGrad hinders character-level robustness. The results suggest that character-level robustness is less conflicted with clean accuracy.",
            "ablation_id": "2405.04346v1.No3"
        },
        {
            "research_objective": "To evaluate the robustness of typo-corrector and robust encoding defenses against the Charmer adversarial attack.",
            "experiment_process": "The performance of Charmer is tested with and without the Pruthi-Jones Constraints (PJC), which are designed to preserve the meaning of individual words. The experiments are performed using a typo-corrector and robust encoding module in front of a BERT-base model. The constraints include not perturbing the same word twice, not perturbing the first/last character of a word, limiting perturbation to words with  chars or more, and perturbing only lowercase English letters.",
            "result_discussion": "Charmer achieves high ASR when not considering PJC constraints. The robust word recognition defenses are only effective when PJC constraints are applied. Relaxing any of these constraints significantly increases ASR against both defenses. These results indicate that robust word recognition defenses provide a false sense of robustness and suggest that adversarial training methods may offer a more promising approach to achieving character-level robustness.",
            "ablation_id": "2405.04346v1.No4"
        }
    ]
}