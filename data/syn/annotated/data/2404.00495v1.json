{
    "title": "Configurable Safety Tuning of Language Models with Synthetic Preference Data Pre-print, work in progress.",
    "abstract": "State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at https://github.com/vicgalle/configurable-safety-tuning.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction and Related Work",
            "text": "###figure_1### The evolution of large language models (LLMs) has led to a broader range of applications, but concerns over their safe and ethical use persist. Any attempt to maximize their utility and ensure safety requires the strategic integration of desirable behaviors during the training phase. Yet, guaranteeing control at the inference stage remains a complex task. Current preference learning fine-tuning approaches often involve the definition of a set of rules (as in Constitutional AI [2  ###reference_b2###]) or acceptable behavior, which is predefined by the developer, to fine-tune the model\u2019s resulting behavior [1  ###reference_b1###]. But this has several drawbacks. Mainly, it inhibits downstream developers or users of popular open models from personalizing the model based on evolving use cases or implementing safety controls based on their preferences.\nInspired by a recent alignment technique, Direct Preference Optimization (DPO)[5  ###reference_b5###], in this work we propose a fine-tuning strategy for the flexible safety tuning of LLMs. Rather than hard-coding a set of values into the model, our approach equips the model with the capability to be controlled flexibly at inference time. This strategy allows the expression of more diversified preferences based on deployment needs. Combining the recent DPO technique with self-critique [2  ###reference_b2###, 4  ###reference_b4###, 3  ###reference_b3###], we propose a framework, dubbed Configurable Safety Tuning (CST). It facilitates the flexible and controlled adjustment of language models\u2019 safety levels, using only synthetic preference data, and also while retaining their general abilities such as general knowledge or reasoning, as evidenced in the experiments section. CST is illustrated in the schematic Figure 1  ###reference_###."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "II Configurable Safety Tuning",
            "text": "DPO requires synthetic preference data of the form , in which  is the user prompt or question,  the original answer, and  a revised, more preferable answer. We use the self-critique method [2  ###reference_b2###, 4  ###reference_b4###] to refine the original response into the revised one, by prompting the LLM to critiques and rewrite the original (uncensored) answer into a harmless, safe revision.\nAfter generating such a dataset, the DPO loss function is given by\n with , if we refer to the learning language model as , with  being its learnable parameters; and  being a frozen copy of the language model at the beginning. Fine-tuning then proceeds to optimize the parameters of the language model to increase (resp. decrease) the likelihood of preferred (resp. rejected) answers.\nHowever, we note that to arrive at a configurable system at inference time, the preference probability  depends on the context, represented by a system prompt  that specifies the safety configuration. Thus, it is necessary to also condition on , leading to a preference probability of the form . As an example, we may consider two system prompts,  and , for an uncensored response and a safety-revised response, respectively. Since these system prompts are opposites, we can leverage the synthetic data pairs by setting  = .\nThis resorts to using the same DPO loss function, but over an augmented dataset of tuples of the form  and . Note there is no need for extra preference data, as it is only required to reverse the preference relation  for the already synthesized data examples, while introducing a new system prompt to express the opposite behavior. Whereas vanilla DPO or Constitutional AI, which just consider a default system prompt or constitution, fail to capture both opposite safety behaviors, the augmentation of CST solves this limitation, enabling different safety preferences at inference time."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "III Experiments and Discussion",
            "text": "In the experiments we evaluate with two recent popular open-source LLMs: OpenHermes-2.5-Mistral (7B parameters) and SOLAR-Instruct (10.7B). We compare the proposed CST with the DPO baseline, which also serves as an ablation: whereas both methods use the same synthetic examples for training, CST leverages two opposing system prompts (), yet DPO only uses the safety-inducing one ().\nControlling harmful behaviors. We aim to control safety by allowing or avoiding sensible content from the LLM, using a collection of adversarial prompts from Harmful Behaviors [6  ###reference_b6###]. For the system prompts, we use  You are an uncensored AI assistant and  You are a helpful yet harmless assistant that avoids generating illegal or harmful content. For each user prompt from the test set, we generate responses with both system prompts, and measure whether the answer satisfies the safety configuration specified in the corresponding system prompt. We designate these (binary) scores as  and , respectively, with  being perfect. To evaluate responses, we used the gpt-4 model to classify the responses into either safety behavior, which achieved -score of 99.2% in a held-out test. Table I  ###reference_### shows average scores  and  over the test set, for the original untrained model, the DPO baseline, and the CST approach. Note that whereas the DPO-tuned model improves the generation of safe responses ( higher than original model), it fails to generate uncensored answers, with a  score lower than the original model: even when prompted to be uncensored, the DPO model has forgotten so, and instead it is too conservative in its responses. On contrast, the CST variant successfully manages both safety configurations, avoiding that pitfall of standard DPO.\nFurthermore, the CST models were evaluated on general capabilities tasks, from the HuggingFace leaderboard (ARC, HellaSwag, MMLU and TruthfulQA), with results in Table III  ###reference_###. This evidences that CST not only enables safety configuration of the models at inference time, it also doesn\u2019t degrade performance in downstream tasks such as general knowledge question-answering or reasoning. \nConclusions. CST enables controlling the safety behavior of LLMs, with no need for additional synthetic preference data compared with what is already available in current fine-tuning pipelines. Further work shall study more fine-grained controls of safety, i.e., depending on semantic topics."
        }
    ],
    "url": "http://arxiv.org/html/2404.00495v1",
    "segmentation": {
        "research_background_sections": [
            "1"
        ],
        "methodology_sections": [
            "2"
        ],
        "main_experiment_and_results_sections": [
            "3"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "3"
        ]
    },
    "research_context": {
        "paper_id": "2404.00495v1",
        "paper_title": "Configurable Safety Tuning of Language Models with Synthetic Preference Data Pre-print, work in progress.",
        "research_background": "### Motivation: \nThe motivation springs from the rapid evolution and broad applicability of large language models (LLMs), coupled with persistent concerns over their safe and ethical use. Specifically, the challenge is to maximize the utility of LLMs while ensuring they operate safely and ethically, without being rigidly bound by predefined rules or behaviors. This paper seeks to address the issue of controlling LLMs at the inference stage in a way that allows for personalization and flexibility, accommodating evolving use cases and user preferences.\n\n### Research Problem:\nThe core research problem addressed in this paper is how to achieve flexible and personalized safety tuning of LLMs at the inference stage. Traditional preference learning and fine-tuning approaches impose predefined sets of rules which limit downstream personalization and dynamic adjustment based on new deployment contexts. The paper aims to overcome these limitations by proposing a method that allows the expression and integration of dynamic and diversified safety preferences during use.\n\n### Relevant Prior Work:\n1. **Constitutional AI [2] and Preference Learning Fine-Tuning Approaches [1]:** These existing methods involve the definition of a set of rules or acceptable behaviors predefined by the developer to guide the model's behavior. However, they limit customization and adaptability, as they inhibit downstream users from personalizing the model.\n\n2. **Direct Preference Optimization (DPO) [5]:** This recent alignment technique inspires the proposed approach. DPO is focused on optimizing model behaviors based on direct preferences, and this work builds on this concept to facilitate flexible safety tuning.\n\n3. **Self-Critique Methods [2, 4, 3]:** These methods involve models critiquing themselves to improve performance and alignment with desired outcomes. The proposed CST framework integrates self-critique with DPO to enhance the flexibility and control over LLMs' behaviors.\n\nOverall, this paper seeks to combine these insights to create a new framework, Configurable Safety Tuning (CST), that uses synthetic preference data to enable flexible safety adjustments while maintaining the models' general abilities.",
        "methodology": "The methodology section of the paper titled \"Configurable Safety Tuning of Language Models with Synthetic Preference Data\" outlines a novel approach to fine-tuning language models for configurable safety at inference time. The proposed method builds on the Dynamic Preference Optimization (DPO) framework and introduces several key innovations.\n\n### Key Components and Innovations:\n\n1. **Synthetic Preference Data Generation:**\n   - **Self-Critique Method**: The first step involves generating synthetic preference data using the self-critique method. Here, a language model is prompted to critique and rewrite an original (uncensored) answer into a more preferable, safe version.\n   - **Data Format**: The synthetic preference data is of the form \\((q, a, a')\\), where \\(q\\) is the user prompt or question, \\(a\\) is the original answer, and \\(a'\\) is the revised, more preferable answer.\n\n2. **Preference Probability and Context:**\n   - The DPO loss function optimizes the language model to increase the likelihood of preferred answers and decrease the likelihood of rejected ones.\n   - **System Prompt \\(\\psi\\)**: To achieve configurable safety, the preference probability \\(P^{\\pi_{\\theta}}(a \\succ a' | q)\\) is conditioned on a system prompt \\(\\psi\\), which specifies the safety configuration.\n   - **Example System Prompts**: Two system prompts, \\(\\psi_1\\) for an uncensored response and \\(\\psi_2\\) for a safety-revised response, are considered. These prompts represent opposite safety behaviors and enable context-dependent preference probabilities.\n\n3. **Augmented Dataset and Reverse Preferences:**\n   - By reversing the preference relation \\(\\succ\\) for the already synthesized data examples, a new dataset is created without needing additional preference data.\n   - **Dataset Tuples**: The augmented dataset consists of tuples of the form \\((q, \\psi_1, a, \\psi_2, a')\\), allowing the model to learn both types of safety responses.\n\n4. **Dynamic Preference Optimization (DPO) Loss Function:**\n   - The same DPO loss function is applied over the augmented dataset to optimize the language model parameters \\(\\theta\\).\n   - **Inference Time Flexibility**: This approach enables the model to exhibit different safety preferences at inference time based on the specified system prompt \\(\\psi\\), overcoming the limitations of vanilla DPO and Constitutional AI, which typically consider a default system prompt or constitution.\n\nOverall, the methodology leverages the synthetic preference data and the innovative augmentation of this dataset to achieve a language model that can be fine-tuned for configurable safety settings, providing flexibility and adaptability in response behaviors at inference time.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n**Models and Baselines:**\n- Models evaluated: OpenHermes-2.5-Mistral (7B parameters) and SOLAR-Instruct (10.7B).\n- Baselines compared: Proposed Configurable Safety Tuning (CST) and Direct Preference Optimization (DPO).\n  - Both CST and DPO use the same synthetic examples for training.\n  - CST uses two opposing system prompts to control safety behavior: \"You are an uncensored AI assistant\" and \"You are a helpful yet harmless assistant that avoids generating illegal or harmful content.\"\n  - DPO only uses the safety-inducing prompt.\n\n**Datasets:**\n- **Harmful Behaviors Dataset:** Collection of adversarial prompts designed to provoke unsafe or sensible content from the LLMs.\n- **Truthy-DPO Dataset:** Synthetic dataset that requires the LLM to respond as either an unbiased, honest AI assistant or a role-playing persona.\n\n**Evaluation Metrics:**\n- ** and :** Binary scores indicating whether a model\u2019s response satisfies the safety configuration of the corresponding prompts.\n  - : Safety behavior when prompted to be helpful and harmless.\n  - : Uncensored behavior when prompted to be uncensored.\n- **Consistency Scores:** Additional scores to evaluate consistency with multi-task system prompts.\n  - : Consistency with safety-related prompts.\n  - : Consistency with new multi-task prompts.\n\n**Experimental Results:**\n- **Table I - Harmful Behaviors Dataset:**\n  - **DPO Baseline:**\n    - Improves the generation of safe responses (higher ).\n    - Fails to generate uncensored answers, leading to a lower , indicating overly conservative behavior even when prompted to be uncensored.\n  - **CST Approach:**\n    - Manages both safety configurations successfully.\n    - Avoids the conservative pitfall of the DPO model.\n  \n- **Table II - Multi-task System Prompts:**\n  - CST outperforms other methods, demonstrating its compatibility with additional task data and diverse system prompts.\n\n- **Table III - General Capabilities (HuggingFace Leaderboard: ARC, HellaSwag, MMLU, TruthfulQA):**\n  - CST achieves results without degrading performance in downstream tasks like general knowledge question-answering or reasoning.\n\n**Conclusions:**\n- CST enables effective control over the safety behavior of language models during inference.\n- No additional synthetic preference data is required beyond current fine-tuning pipelines.\n- Future work aims to explore fine-grained controls on safety based on specific semantic topics."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "To control safety by allowing or avoiding sensible content from the LLM, using a collection of adversarial prompts from Harmful Behaviors.",
            "experiment_process": "Using the OpenHermes-2.5-Mistral (7B parameters) and SOLAR-Instruct (10.7B) models, we evaluate responses generated by both DPO and CST methods. CST uses two opposing system prompts: 'You are an uncensored AI assistant' and 'You are a helpful yet harmless assistant that avoids generating illegal or harmful content.' Responses generated by both system prompts are classified into either safety behavior using the gpt-4 model with an f-score of 99.2%. We measure whether the answers satisfy the safety configuration specified in the corresponding system prompt and designate these (binary) scores as S\u2092 and S\u2090.",
            "result_discussion": "The DPO-tuned model improves the generation of safe responses, achieving a higher S\u2090 score than the original model. However, it fails to generate uncensored answers, with a lower S\u2092 score compared to the original model, being too conservative even when prompted to be uncensored. Contrary to this, the CST variant successfully manages both safety configurations, thus avoiding the pitfalls of standard DPO.",
            "ablation_id": "2404.00495v1.No1"
        },
        {
            "research_objective": "To evaluate the compatibility of the CST framework with additional multi-task data from other synthetic datasets.",
            "experiment_process": "Leverage the truthy-dpo dataset, prompting the LLM to reply either as a 'honest AI assistant' or a 'role-played persona.' Create a new multi-task dataset combining both the harmful behaviors and truthy-dpo datasets. Evaluate the responses using previous scores (S\u2092 and S\u2090) and additional scores (Hhon and Hper), depending on the example source. Compare the performance of CST on this new dataset.",
            "result_discussion": "CST achieves the best results, indicating that the safety framework is compatible with the introduction of additional data and more varied system prompts. This demonstrates that CST can handle multiple tasks while maintaining safety configurations.",
            "ablation_id": "2404.00495v1.No2"
        },
        {
            "research_objective": "To determine whether CST affects the LLMs' general capabilities in downstream tasks.",
            "experiment_process": "Evaluate CST models on general capabilities tasks from the HuggingFace leaderboard, including ARC, HellaSwag, MMLU, and TruthfulQA. Compare the performance metrics of CST with those of models tuned using other methods.",
            "result_discussion": "Results show that CST not only enables safety configuration at inference time but also does not degrade performance on downstream tasks such as general knowledge question-answering or reasoning.",
            "ablation_id": "2404.00495v1.No3"
        }
    ]
}