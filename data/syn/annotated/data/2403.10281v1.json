{
    "title": "Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning",
    "abstract": "In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop 111https://defactify.com/factify3.html, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "In an era characterized by an overwhelming influx of information facilitated by the internet and social media, the verification of facts has emerged as an increasingly critical challenge. The proliferation of digital platforms has democratized content creation and dissemination, yet it has also paved the way for the rapid spread of misinformation, disinformation, and misleading content. This phenomenon poses a significant threat to the integrity of information consumed by individuals, communities, and societies at large.\nThe need for robust fact verification mechanisms has become increasingly evident. The ability to distinguish between accurate, credible information and falsehoods has become essential in ensuring informed decision-making, fostering a well-informed citizenry, and preserving the fabric of democratic societies. Fact verification serves as a cornerstone in the pursuit of truth, objectivity, and reliability in an information landscape fraught with distortions and inaccuracies.\nIn recent years, the advent of large language models has revolutionized natural language understanding, enabling machines to comprehend and generate human-like text at an unprecedented scale. Among the myriad applications of these models, one paramount challenge they face is the discernment of factual accuracy in claims against available evidence, due to the inherent limitations and complexities of language understanding. Thus we undertake the task of classifying claims as supported, refuted, or neutral based on provided evidence, advancing the capabilities of language models towards more nuanced and accurate comprehension.\nThis research paper explores the intricate process of enhancing large language models to navigate the terrain of claim verification, employing a spectrum of methodologies. Fine-tuning, recognized for its superior performance, serves as a cornerstone method in our investigation. This paper goes beyond by delving into the significance of in-context learning, which we consider to also encompass the wider spectrum of prompt engineering and prompt tuning, elucidating its role in augmenting models\u2019 understanding of nuanced linguistic contexts. Additionally, it examines the effectiveness and limitations of feature extraction techniques in capturing crucial information relevant to claim classification. Furthermore, the paper explores the benefits of ensemble learning approaches, synthesizing diverse model outputs to enhance classification accuracy and reliability.\nOur approach draws substantial influence from the pioneering work of Du et al.\u2019s Pre-CoFactv2 model [1  ###reference_b1###], as presented during the Factify 2 challenge [2  ###reference_b2###]. We were impressed by the performance of parameter-efficient fine-tuning on the DeBERTa model, and thought to take one step further, dedicating our efforts to a comprehensive fine-tuning of the advanced DeBERTaV3 [3  ###reference_b3###]. The outcome showcases substantial improvement across question answering and text classification tasks when compared to all other assessed methodologies. We hereby introduce our resultant model as Pre-CoFactv3, symbolizing our continuum of innovation derived from our predecessors\u2019 pioneering endeavors.\nJudging by evaluation results, our fine-tuning method outperforms in-context learning and human baseline, when discerning between support, refute and neutral correlations between claim and evidence text, reaching 86 percent accuracy overall on internal validation. When using external validation against other models, our method comes out as state-of-the-art, leading ahead of the next contender by 52 percent.\nBy synthesizing insights from varied methodologies and their contributions, this research aims to contribute significantly to the ongoing discourse on refining large language models for factual inference, paving the way for more robust and reliable natural language understanding in the domain of claim verification."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Works",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "Traditional Methods",
            "text": "In the landscape of fake news detection, traditional methods relied on rule-based approaches, focusing on specific words or phrases linked to misinformation. Models such as Na\u00efve Bayes, Support Vector Machines (SVM), and Decision Trees exemplify these conventional techniques [4  ###reference_b4###][5  ###reference_b5###]. However, their vulnerability to phrasing variations and evolving tactics limited their effectiveness."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Deep Learning Revolution in NLP",
            "text": "The Deep Learning Revolution in Natural Language Processing (NLP) introduced advanced feature engineering and integrated sophisticated techniques. Notable methodologies like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Long Short-Term Memory (LSTM) marked this transformative phase. For instance, Kaliyar et al. [6  ###reference_b6###] harnessed deep CNN for multi-layered feature extraction, while Bahad et al. [7  ###reference_b7###] introduced the bi-directional LSTM-RNN architecture with the GLoVe word embedding for effective fake news detection."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "Cross-Modality",
            "text": "To address the dynamic nature of misinformation, researchers explored cross-domain learning, integrating NLP knowledge from related domains. This led to the examination of multi-modal data analysis, incorporating text, images, and social media network information. Models like EANN (event adversarial neural network) by Wang et al. [8  ###reference_b8###] and MCAN (multimodal co-attention network) by Wu et al. [9  ###reference_b9###] exemplify these cross-modal approaches. However, due to the unimodal nature of the Factify5WQA dataset [10  ###reference_b10###] (text only), our focus remains on unimodal models."
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "The Power of Pre-trained Large Language Models",
            "text": "In recent times, the evolution of Transformer [11  ###reference_b11###] has revolutionized the landscape of natural language processing and information extraction. Pre-trained Large Language Models, exemplified by BERT [12  ###reference_b12###] and GPT [13  ###reference_b13###], reveal profound capabilities in comprehending and generating human-like text, proving particularly adept at identifying fake news.\nIn line with the methodology outlined by Du et al. [1  ###reference_b1###], we leverage the simplicity and effectiveness inherent in various pre-trained models. Our architecture systematically tests different pre-trained models, thereby amplifying efficiency in training and learning patterns from the dataset."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "Methods",
            "text": "We leverage Pre-trained Large Language Models (LLMs) to generate embeddings for the claim, evidence, claim answer, and evidence answer. Subsequently, we freeze the parameters of the Pre-trained LLMs and exclusively train the last embedding layer.\nIn the subsequent step, we employ four embeddings that participate in co-attention with each other in pairs, resulting in six pairs of co-attention. These co-attention pairs comprise (1) claim and evidence, (2) claim and claim answer, (3) claim and evidence answer, (4) evidence and claim answer, (5) evidence and evidence answer, and (6) claim answer and evidence answer. The co-attention block utilized here is a variant of the multi-head self-attention block, akin to the encoder in Transformer [11  ###reference_b11###]. This block accepts two embeddings as inputs to facilitate the learning of interactions and relations between them. Subsequently, mean aggregation is employed to merge all the outcomes into a single embedding , representing the corresponding text.\nInformed by the insights of Gao et al. [15  ###reference_b15###], our model leverages the length of the text and other common textual attributes in NLP to extract important information from diverse perspectives. Additionally, inspired by Zhang et al. [16  ###reference_b16###], we incorporated the calculation of similarity between text and question-answer pairs, aiming to harness the significance of relevance and coherence to enhance our model\u2019s capabilities. The details of our model\u2019s feature extractors are illustrated in Table 1  ###reference_###.\nOur initial focus involves computing common features in NLP for our datasets. Building on the methodology outlined in [17  ###reference_b17###], we calculate 11 features from each given text. These features include word length, the count of capital words, stopwords, quotes, and more. As our text sources encompass claim, evidence, question, claim answer to the question, and evidence answer to the question, we utilize these 11 metrics to comprehensively evaluate each text source. This results in a total of 55 features in the initial phase, significantly enabling our model to capture basic information distilled from the text.\nFurthermore, we delve into the calculation of sentence similarity. Capitalizing on the flexibility afforded by various measurements of semantic and text similarities, we employ SimCSE, MPNet, The Fuzz, TF-IDF, and ROUGE to assess the similarity of given text pairs. Specifically, we evaluate the similarity between claim-evidence pairs and their corresponding question-answer pairs, resulting in a total of 10 features.\nIn summary, our approach involves the computation of essential text information and the evaluation of sentence similarity, resulting in a total of 65 features. Subsequently, we normalize these features and transform them into embeddings, denoted as .\nThe classifier is implemented as a simple two-layer Multilayer Perceptron (MLP). We directly concatenate the embeddings from the Pre-trained LLMs  and the Feature Extractor , using the concatenated result as input for the classifier. Finally, the classifier produces the probability distribution over each label , where  and .\nThe loss function is cross-entropy and is defined as follows:\nFor further details on FakeNet, please refer to [1  ###reference_b1###]."
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Pre-CoFactv3 Overview",
            "text": "###figure_1### Figure 1  ###reference_### illustrates the overview of our Pre-CoFactv3 framework. Owing to the absence of claim answers, evidence answers, and labels in the testing dataset, we have divided the overall process into two distinct phases: (1) Question Answering and (2) Text Classification. In the first phase, questions are answered by the information derived from both the claim and evidence. Subsequently, in the second phase, the system uses the claim, evidence, and answers obtained from the initial phase to predict the appropriate label, which can be categorized as Support, Neutral, or Refute."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "Question Answering",
            "text": "In the Question Answering phase, the input comprises a set denoted by , where . Here,  represents the claim,  denotes the evidence, and  signifies a list of questions associated with the given claim and evidence. The output is represented by a set denoted as , where  and . In this context,  corresponds to the list of claim answers, and  represents the list of evidence answers generated by the Question Answering module.\nWithin the Question Answering module, we employ two distinct methodologies: In-Context Learning and Fine-tuning Large Language Models (LLMs).\nOur initial experimentation focuses on In-Context Learning, with detailed insights provided in Section A.1  ###reference_###."
        },
        {
            "section_id": "3.2.1",
            "parent_section_id": "3.2",
            "section_name": "3.2.1 Fine-tuning Large Language Models (LLMs)",
            "text": "In the Fine-tuning of LLMs, we integrate two sets of LLMs. The first set involves LLMs fine-tuned using the SQuAD 2.0 dataset [14  ###reference_b14###], accessible on Hugging Face. Concurrently, the second set comprises pre-trained LLMs subsequently fine-tuned on the FACTIFY5WQA dataset [10  ###reference_b10###], specifically tailored for the question-answering task. The objective of the question-answering task is to input the context (claim or evidence) and subsequently identify the index corresponding to the location of the answer within that context. The formulation is as below:\nComprehensive experimental results will be presented in Section 4.4  ###reference_###."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Text Classification",
            "text": "In the Text Classification phase, the input is represented as , encompassing the claim , evidence , question , and the claim answer  and evidence answer  obtained from the preceding Question Answering phase. The output of the Text Classification module is the predicted label, which falls into one of the categories: Support, Neutral, or Refute.\nWithin the Text Classification module, three distinct methodologies are employed: In-Context Learning, FakeNet, and Fine-tuning Large Language Models (LLMs).\nOur initial experimentation centers around In-Context Learning, and comprehensive insights into this approach are elucidated in Section A.2  ###reference_###.\nWe leverage Pre-trained Large Language Models (LLMs) to generate embeddings for the claim, evidence, claim answer, and evidence answer. Subsequently, we freeze the parameters of the Pre-trained LLMs and exclusively train the last embedding layer.\nIn the subsequent step, we employ four embeddings that participate in co-attention with each other in pairs, resulting in six pairs of co-attention. These co-attention pairs comprise (1) claim and evidence, (2) claim and claim answer, (3) claim and evidence answer, (4) evidence and claim answer, (5) evidence and evidence answer, and (6) claim answer and evidence answer. The co-attention block utilized here is a variant of the multi-head self-attention block, akin to the encoder in Transformer [11  ###reference_b11###  ###reference_b11###]. This block accepts two embeddings as inputs to facilitate the learning of interactions and relations between them. Subsequently, mean aggregation is employed to merge all the outcomes into a single embedding , representing the corresponding text.\nInformed by the insights of Gao et al. [15  ###reference_b15###  ###reference_b15###], our model leverages the length of the text and other common textual attributes in NLP to extract important information from diverse perspectives. Additionally, inspired by Zhang et al. [16  ###reference_b16###  ###reference_b16###], we incorporated the calculation of similarity between text and question-answer pairs, aiming to harness the significance of relevance and coherence to enhance our model\u2019s capabilities. The details of our model\u2019s feature extractors are illustrated in Table 1  ###reference_###  ###reference_###.\nOur initial focus involves computing common features in NLP for our datasets. Building on the methodology outlined in [17  ###reference_b17###  ###reference_b17###], we calculate 11 features from each given text. These features include word length, the count of capital words, stopwords, quotes, and more. As our text sources encompass claim, evidence, question, claim answer to the question, and evidence answer to the question, we utilize these 11 metrics to comprehensively evaluate each text source. This results in a total of 55 features in the initial phase, significantly enabling our model to capture basic information distilled from the text.\nFurthermore, we delve into the calculation of sentence similarity. Capitalizing on the flexibility afforded by various measurements of semantic and text similarities, we employ SimCSE, MPNet, The Fuzz, TF-IDF, and ROUGE to assess the similarity of given text pairs. Specifically, we evaluate the similarity between claim-evidence pairs and their corresponding question-answer pairs, resulting in a total of 10 features.\nIn summary, our approach involves the computation of essential text information and the evaluation of sentence similarity, resulting in a total of 65 features. Subsequently, we normalize these features and transform them into embeddings, denoted as .\nThe classifier is implemented as a simple two-layer Multilayer Perceptron (MLP). We directly concatenate the embeddings from the Pre-trained LLMs  and the Feature Extractor , using the concatenated result as input for the classifier. Finally, the classifier produces the probability distribution over each label , where  and .\nThe loss function is cross-entropy and is defined as follows:\nFor further details on FakeNet, please refer to [1  ###reference_b1###  ###reference_b1###]."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 FakeNet",
            "text": "Building upon the foundations laid by the previous work, Pre-CoFactv2 [1  ###reference_b1###], we have introduced a novel framework named FakeNet. Figure 2  ###reference_### provides an overview of FakeNet.\n###figure_2### We leverage Pre-trained Large Language Models (LLMs) to generate embeddings for the claim, evidence, claim answer, and evidence answer. Subsequently, we freeze the parameters of the Pre-trained LLMs and exclusively train the last embedding layer.\nIn the subsequent step, we employ four embeddings that participate in co-attention with each other in pairs, resulting in six pairs of co-attention. These co-attention pairs comprise (1) claim and evidence, (2) claim and claim answer, (3) claim and evidence answer, (4) evidence and claim answer, (5) evidence and evidence answer, and (6) claim answer and evidence answer. The co-attention block utilized here is a variant of the multi-head self-attention block, akin to the encoder in Transformer [11  ###reference_b11###  ###reference_b11###  ###reference_b11###]. This block accepts two embeddings as inputs to facilitate the learning of interactions and relations between them. Subsequently, mean aggregation is employed to merge all the outcomes into a single embedding , representing the corresponding text.\nInformed by the insights of Gao et al. [15  ###reference_b15###  ###reference_b15###  ###reference_b15###], our model leverages the length of the text and other common textual attributes in NLP to extract important information from diverse perspectives. Additionally, inspired by Zhang et al. [16  ###reference_b16###  ###reference_b16###  ###reference_b16###], we incorporated the calculation of similarity between text and question-answer pairs, aiming to harness the significance of relevance and coherence to enhance our model\u2019s capabilities. The details of our model\u2019s feature extractors are illustrated in Table 1  ###reference_###  ###reference_###  ###reference_###.\nOur initial focus involves computing common features in NLP for our datasets. Building on the methodology outlined in [17  ###reference_b17###  ###reference_b17###  ###reference_b17###], we calculate 11 features from each given text. These features include word length, the count of capital words, stopwords, quotes, and more. As our text sources encompass claim, evidence, question, claim answer to the question, and evidence answer to the question, we utilize these 11 metrics to comprehensively evaluate each text source. This results in a total of 55 features in the initial phase, significantly enabling our model to capture basic information distilled from the text.\nFurthermore, we delve into the calculation of sentence similarity. Capitalizing on the flexibility afforded by various measurements of semantic and text similarities, we employ SimCSE, MPNet, The Fuzz, TF-IDF, and ROUGE to assess the similarity of given text pairs. Specifically, we evaluate the similarity between claim-evidence pairs and their corresponding question-answer pairs, resulting in a total of 10 features.\nIn summary, our approach involves the computation of essential text information and the evaluation of sentence similarity, resulting in a total of 65 features. Subsequently, we normalize these features and transform them into embeddings, denoted as .\nThe classifier is implemented as a simple two-layer Multilayer Perceptron (MLP). We directly concatenate the embeddings from the Pre-trained LLMs  and the Feature Extractor , using the concatenated result as input for the classifier. Finally, the classifier produces the probability distribution over each label , where  and .\nThe loss function is cross-entropy and is defined as follows:\nFor further details on FakeNet, please refer to [1  ###reference_b1###  ###reference_b1###  ###reference_b1###]."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Fine-tuning Large Language Models (LLMs)",
            "text": "Conversely, we explore an alternative approach by fine-tuning the entire pre-trained LLMs on the text classification task. In this configuration, the input is a sequence formed by concatenating the claim, evidence, question, claim answer, and evidence answer. The output of the model is the probability distribution over the three labels: Support, Neutral, or Refute."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Ensemble",
            "text": "Inspired by the approach outlined in [1  ###reference_b1###], we incorporate ensemble learning to amalgamate informative knowledge from various models, aiming to enhance the overall predictive performance. Four distinct ensemble methods have been designed:\nWeighted sum with labels:\nPower weighted sum with labels:\nPower weighted sum with two models:\nPower weighted sum with three models:\nIn methods 1 and 2, the terms , , and  denote the new probabilities for Support, Neutral, and Refute, respectively. Meanwhile, , , and  represent the probabilities for Support, Neutral, and Refute obtained from model . In methods 3 and 4, the notation  signifies the new probabilities for all labels, while  denotes the probabilities for all labels derived from model .\nThe experimental results will be shown in Section 4.5  ###reference_###."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Experiments",
            "text": "In this experiment, we aim to compare the results across different LLMs, namely:\nThe roberta-large [20  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###] 222https://huggingface.co/deepset/roberta-large-squad2\nThe deberta-v3-large [3  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###] 333https://huggingface.co/deepset/deberta-v3-large-squad2\nThe roberta-large [20  ###reference_b20###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###].\nThe roberta-large [20  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###].\nFine-tuning is conducted by the Hugging Face Trainer API on the Question Answering task444https://huggingface.co/docs/transformers/tasks/question_answering. We employ BLEU scores for both claim answer and evidence answer, taking the average of the two as the metric. The results are presented in Table 2  ###reference_###. Surprisingly, deberta-v3-large [3  ###reference_b3###] fine-tuned solely with the FACTIFY5WQA [10  ###reference_b10###] dataset achieves the best performance. This outcome underscores the significance of fine-tuning on a dataset that matches the task\u2019s characteristics. Furthermore, it is worth noting that since a majority of the answers are short in length, they may yield lower BLEU scores with 4-gram precision. Finally, we use LLM 4 for testing.\n###table_1### To compare various Pre-trained LLMs, we employ seven LLMs available on Hugging Face. The results are displayed in Table 3  ###reference_###.\n###table_2### Two noteworthy findings have emerged from our experiments. Firstly, the model size does not exhibit a strictly positive correlation with performance. For instance, deberta-v3-large outperforms deberta-v3-base, but deberta-xlarge does not surpass deberta-large. Secondly, deberta-v3-large achieves the highest accuracy, prompting its selection for the next experiment.\nIn the ensuing experiment, our objective is to assess whether the inclusion of features can enhance performance. We select 65 features as input and apply various normalization techniques. The detailed results are presented in Table 4  ###reference_###.\n###table_3### Our findings reveal that the inclusion of features normalized between -1 and 1 results in the best performance, albeit with a slight improvement of 0.19%.\nIn the fine-tuning process, we employ the microsoft/deberta-v3-large [3  ###reference_b3###] 555https://huggingface.co/microsoft/deberta-v3-large as the Pre-trained LLMs. The training parameters are configured with an epoch of 8, a batch size of 4, and a learning rate of 0.00002. Fine-tuning is executed using the Hugging Face Trainer API on the Text Classification task 666https://huggingface.co/docs/transformers/tasks/sequence_classification, and various alterations in the input and length are made to facilitate performance comparison. The result is shown in Table 5  ###reference_###.\nSurprisingly, the performance with only text as input surpasses that of using text, question, and answer as input. This prompts a question: Is the inclusion of question and answer truly beneficial for text classification? Additionally, we observe that longer text lengths correspond to improved performance. The model with a claim length of 1600 and evidence length of 10000 achieves the highest accuracy, markedly surpassing FakeNet."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Experiments Environment",
            "text": "All experiments were executed on a machine equipped with 32 AMD EPYC 7302 16-Core CPUs, 2 NVIDIA RTX A5000 GPUs, and 252GB of RAM. Python serves as our primary programming language. The source code is available at https://github.com/AndyChiangSH/Pre-CoFactv3."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Dataset",
            "text": "We utilize the dataset FACTIFY5WQA [10  ###reference_b10###][18  ###reference_b18###] provided by the AAAI-24 Workshop Factify 3.0. This dataset is designed for fact verification, with the task of determining the veracity of a claim based on given evidence. In our specific task, we augment this dataset by introducing questions that entail the answers derived from the claim and evidence, thus framing the fact verification as an entailment problem. The dataset consists of 10,500 samples for training, 2,250 samples for validation, and 2,250 samples for testing, totaling 15,000 samples. Each sample in the training and validation sets includes fields for claim, evidence, question, claim answer, evidence answer, and label. For the testing set, the fields include claim, evidence, and question.\nClaim: the statement to be verified.\nEvidence: the facts to verify the claim.\nQuestion: the questions generated from the claim by the 5W framework (who, what, when, where, and why).\nClaim answer: the answers derived from the claim.\nEvidence answer: the answers derived from the evidence.\nLabel: the veracity of the claim based on the given evidence, which is one of three categories: Support, Neutral, or Refute."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Evaluation Metric",
            "text": "The official competition metric for Factify 3.0 involves computing the average BLEU score for answers to questions derived from both the claim and evidence. A prediction is deemed correct only if this score exceeds a predefined threshold and the label is accurate. The final accuracy is then calculated as the percentage of correct predictions.\nFollowing this, in the Question Answering task, we employ the BLEU score with 4-gram precision [19  ###reference_b19###]. In the case of the Text Classification task, accuracy serves as the evaluation metric. Detailed experimental results are provided in the ensuing sections."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Question Answering",
            "text": "In this experiment, we aim to compare the results across different LLMs, namely:\nThe roberta-large [20  ###reference_b20###  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###] 222https://huggingface.co/deepset/roberta-large-squad2\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###] 333https://huggingface.co/deepset/deberta-v3-large-squad2\nThe roberta-large [20  ###reference_b20###  ###reference_b20###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###].\nThe roberta-large [20  ###reference_b20###  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###].\nFine-tuning is conducted by the Hugging Face Trainer API on the Question Answering task444https://huggingface.co/docs/transformers/tasks/question_answering. We employ BLEU scores for both claim answer and evidence answer, taking the average of the two as the metric. The results are presented in Table 2  ###reference_###  ###reference_###. Surprisingly, deberta-v3-large [3  ###reference_b3###  ###reference_b3###] fine-tuned solely with the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###] dataset achieves the best performance. This outcome underscores the significance of fine-tuning on a dataset that matches the task\u2019s characteristics. Furthermore, it is worth noting that since a majority of the answers are short in length, they may yield lower BLEU scores with 4-gram precision. Finally, we use LLM 4 for testing.\n###table_4###"
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Fine-tuning Large Language Models (LLMs)",
            "text": "In this experiment, we aim to compare the results across different LLMs, namely:\nThe roberta-large [20  ###reference_b20###  ###reference_b20###  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###  ###reference_b14###] 222https://huggingface.co/deepset/roberta-large-squad2\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###  ###reference_b14###] 333https://huggingface.co/deepset/deberta-v3-large-squad2\nThe roberta-large [20  ###reference_b20###  ###reference_b20###  ###reference_b20###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###  ###reference_b3###] model fine-tuned by the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###].\nThe roberta-large [20  ###reference_b20###  ###reference_b20###  ###reference_b20###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###].\nThe deberta-v3-large [3  ###reference_b3###  ###reference_b3###  ###reference_b3###] model fine-tuned by the SQuAD 2.0 [14  ###reference_b14###  ###reference_b14###  ###reference_b14###] and the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###].\nFine-tuning is conducted by the Hugging Face Trainer API on the Question Answering task444https://huggingface.co/docs/transformers/tasks/question_answering. We employ BLEU scores for both claim answer and evidence answer, taking the average of the two as the metric. The results are presented in Table 2  ###reference_###  ###reference_###  ###reference_###. Surprisingly, deberta-v3-large [3  ###reference_b3###  ###reference_b3###  ###reference_b3###] fine-tuned solely with the FACTIFY5WQA [10  ###reference_b10###  ###reference_b10###  ###reference_b10###] dataset achieves the best performance. This outcome underscores the significance of fine-tuning on a dataset that matches the task\u2019s characteristics. Furthermore, it is worth noting that since a majority of the answers are short in length, they may yield lower BLEU scores with 4-gram precision. Finally, we use LLM 4 for testing.\n###table_5###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Text Classification",
            "text": "In this section, we will present experimental results for the text classification task. To compare various Pre-trained LLMs, we employ seven LLMs available on Hugging Face. The results are displayed in Table 3. Two noteworthy findings have emerged from our experiments. Firstly, the model size does not exhibit a strictly positive correlation with performance. For instance, deberta-v3-large outperforms deberta-v3-base, but deberta-xlarge does not surpass deberta-large. Secondly, deberta-v3-large achieves the highest accuracy, prompting its selection for the next experiment.\n\nIn the fine-tuning process, we employ the microsoft/deberta-v3-large as the Pre-trained LLMs. The training parameters are configured with an epoch of 8, a batch size of 4, and a learning rate of 0.00002. Fine-tuning is executed using the Hugging Face Trainer API on the Text Classification task, and various alterations in the input and length are made to facilitate performance comparison. The result is shown in Table 5.\n\nSurprisingly, the performance with only text as input surpasses that of using text, question, and answer as input. This prompts a question: Is the inclusion of question and answer truly beneficial for text classification? Additionally, we observe that longer text lengths correspond to improved performance. The model with a claim length of 1600 and evidence length of 10000 achieves the highest accuracy, markedly surpassing FakeNet."
        },
        {
            "section_id": "4.5.1",
            "parent_section_id": "4.5",
            "section_name": "4.5.1 FakeNet",
            "text": "During the training of FakeNet, specific configurations were employed. The batch size was set to 24, the learning rate to 0.00005, and the number of epochs depended on the specific LLMs used. The input dimension for text and answer was 1024, the hidden dimension of FakeNet was set to 256, and the number of co-attention heads was specified as 2. Additionally, the input dimensions of the Pre-trained LLM and the Feature Extractor were set to 256 and 32, respectively. To compare various Pre-trained LLMs, we employ seven LLMs available on Hugging Face. The results are displayed in Table 3 ###reference_### ###reference_### ###reference_###. ###table_8### Two noteworthy findings have emerged from our experiments. Firstly, the model size does not exhibit a strictly positive correlation with performance. For instance, deberta-v3-large outperforms deberta-v3-base, but deberta-xlarge does not surpass deberta-large. Secondly, deberta-v3-large achieves the highest accuracy, prompting its selection for the next experiment. In the ensuing experiment, our objective is to assess whether the inclusion of features can enhance performance. We select 65 features as input. The detailed results are presented in Table 4 ###reference_### ###reference_### ###reference_###. ###table_9###"
        },
        {
            "section_id": "4.5.2",
            "parent_section_id": "4.5",
            "section_name": "4.5.2 Fine-tuning Large Language Models (LLMs)",
            "text": "In the fine-tuning process, we employ the microsoft/deberta-v3-large as the Pre-trained LLMs. The training parameters are configured with an epoch of 8, a batch size of 4, and a learning rate of 0.00002. Fine-tuning is executed using the Hugging Face Trainer API on the Text Classification task, and various alterations in the input and length are made to facilitate performance comparison. The result is shown in Table 5.\n\nSurprisingly, the performance with only text as input surpasses that of using text, question, and answer as input. This prompts a question: Is the inclusion of question and answer truly beneficial for text classification? Additionally, we observe that longer text lengths correspond to improved performance. The model with a claim length of 1600 and evidence length of 10000 achieves the highest accuracy, markedly surpassing FakeNet."
        },
        {
            "section_id": "4.5.3",
            "parent_section_id": "4.5",
            "section_name": "4.5.3 Ensemble",
            "text": "The experimental results for the four ensemble methods mentioned in Section 3.3.3  ###reference_.SSS3### are provided in Table 6  ###reference_###. Our examination indicates that the ensemble method, utilizing a power weighted sum with three models (two Fine-tuned LLMs and one FakeNet), achieves the highest accuracy. Hence, we have selected this ensemble model as our best result. Figure 3  ###reference_### displays the confusion matrix for these three models and the ensemble model. The ensemble approach demonstrates the ability to capitalize on the strengths of each individual model, leading to an overall enhancement in accuracy. However, it is noteworthy that all models exhibit difficulty in accurately identifying instances of \"Support,\" thereby limiting the extent of performance improvement in this particular category. ###figure_3###"
        },
        {
            "section_id": "4.6",
            "parent_section_id": "4",
            "section_name": "Baselines",
            "text": ""
        },
        {
            "section_id": "4.6.1",
            "parent_section_id": "4.6",
            "section_name": "4.6.1 In-Context Learning Baseline",
            "text": "The In-Context Learning baseline uses the ChatGPT API to generate replies with the best prompt we could come up with, more details about prompt engineering in A.2  ###reference_###. Around one hundred claim-evidence pairs were fed into the Large Language Model, and the respective labels were extracted from their replies.\nThe result in Table 7  ###reference_### seemed skewed towards support, but after further analysis, we found it rather difficult to differentiate between support and neutral, even as humans. We take the lower percentages of correct neutral labels to mean the prompt we were using favored support when in an ambiguous state. Overall, it does not outperform the human baseline, and falls way short of our Pre-CoFactv3 model."
        },
        {
            "section_id": "4.6.2",
            "parent_section_id": "4.6",
            "section_name": "4.6.2 Human Baseline",
            "text": "To assess the effectiveness of our model, we established a human baseline through a questionnaire created using the dataset. The objective was to gauge how well our model performs compared to human beings in identifying fake news. For simplicity, we selected 10 entities from the dataset and distributed them to 20 individuals, asking them to categorize the claim news. The results are presented in the Table 7  ###reference_###.\nUpon analysis, the accuracy of human identification was found to be 44%, significantly lower than our model\u2019s performance. Moreover, it became apparent that, concerning the Refute metric, a significant number of participants struggled to provide correct answers. This challenge likely stems from the ambiguous boundary between Neutral and Refute metric. This observation serves as a foundation for illustrating the efficacy of our model in surpassing human judgment in fake news identification."
        },
        {
            "section_id": "4.7",
            "parent_section_id": "4",
            "section_name": "Testing Result",
            "text": ""
        },
        {
            "section_id": "4.7.1",
            "parent_section_id": "4.7",
            "section_name": "4.7.1 Final Submission",
            "text": "In our final submissions, we present three versions of our work, and the corresponding testing accuracy is provided in Table 8  ###reference_###. It is noteworthy that the performance of the ensemble model did not meet our expectations, possibly indicating overfitting issues. Given that the fine-tuned LLM for both Question Answering and Text Classification demonstrates the highest testing accuracy, our observation leads us to assert that \"Fine-tuning is all you need.\"\n###table_10###"
        },
        {
            "section_id": "4.7.2",
            "parent_section_id": "4.7",
            "section_name": "4.7.2 Leaderboard",
            "text": "Table 9  ###reference_### displays the leaderboard for the Factify 3.0 Workshop. We are delighted to announce that our team, Trifecta, won first place in the workshop. Our performance surpassed the baseline by an impressive 103%, maintaining a lead over the second competitor by 70%.\n###table_11###"
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "Limitations & Discussions",
            "text": "Fine-tuning a model demands a wealth of annotated data for effective supervised training, and any shortfall in labeled data can detrimentally impact the training outcomes. Moreover, the current dataset is confined to English data, restricting its applicability across different languages.\nThe real-world data introduces added complexities, with nuances in semantics and incomplete information posing challenges to the model\u2019s recognition capabilities, such as using a pre-built database of evidence to fact-check claims made on social media and other information-sharing platforms. While this brings into question whether previously collected evidence could confirm the news as it happens, it could very well be used to prune incoming data, lessening the load for further examination, whether by more complex models or human intervention.\nIn the fine-tuning process, parameters like model size, max length, and batch size are intricately tied to GPU memory constraints. Exploring options such as expanding GPU memory or adopting Parameter-Efficient Fine-Tuning (PEFT) raises intriguing questions for future research, particularly in relation to their potential to amplify recognition accuracy."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusion",
            "text": "In this paper, we introduced Pre-CoFactv3, a framework composed of two integral parts: (1) Question Answering and (2) Text Classification. For the Question Answering task, we employed In-Context Learning and Fine-tuned Large Language Models (LLMs) to answer questions based on the claim and evidence. In the Text Classification task, our approach encompassed In-Context Learning, the FakeNet model, and Fine-tuned LLMs to predict label categories.\nOur experiments demonstrated the efficacy of our diverse approaches, encompassing the comparison of various Pre-trained LLMs, the development of the FakeNet model, and the exploration of different ensemble methods. Additionally, we established two baseline models for In-Context Learning and Human performance.\nThe culmination of our efforts resulted in the success of our team, Trifecta, securing the first-place position in the AAAI-24 Factify 3.0 Workshop. Our performance surpassed the baseline by an impressive 103%, maintaining a substantial lead over the second competitor by 70%.\nIn summary, our accomplishment validates the effectiveness of our approach and the thoughtful integration of diverse techniques. As we continue to advance in the field of fact verification, our experiences offer valuable insights and lessons for the broader research community."
        }
    ],
    "appendix": [
        {
            "section_id": "Appendix 1",
            "parent_section_id": null,
            "section_name": "Appendix A In-Context Learning",
            "text": ""
        }
    ],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S3.T1.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.2.1\">Features</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.1\" rowspan=\"4\">\n<span class=\"ltx_text\" id=\"S3.T1.1.2.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.2.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.2.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.2.1.1.1.1.1\">Common Features</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.2.1.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.2.1.1.1.2.1\">in NLP</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.2\">Character count</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.3\">Word count</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.4\">Count of capital characters</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.1\">Count of capital words</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.2\">Count of punctuation</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.3\">Count of words in quotes</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.1\">Sentence count</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.2\">Count of unique words</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.3\">Count of hashtags</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.5.1\">Count of mentions</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.5.2\">Count of stopwords</td>\n<td class=\"ltx_td\" id=\"S3.T1.1.5.3\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.1.6.1\" rowspan=\"2\">\n<span class=\"ltx_text\" id=\"S3.T1.1.6.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.6.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.6.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.6.1.1.1.1.1\">Similarity between</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.6.1.1.1.2\">\n<span class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.6.1.1.1.2.1\">Text Pair</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.6.2\">SimCse</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.6.3\">MPNet</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.6.4\">The Fuzz</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.7.1\">TF-IDF</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T1.1.7.2\">Rouge</td>\n<td class=\"ltx_td ltx_border_bb\" id=\"S3.T1.1.7.3\"></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Selected features in our Feature Selector are 11 prevalent features in NLP and 5 common metrics for semantic textual similarities.</figcaption>\n</figure>",
            "capture": "Table 1: Selected features in our Feature Selector are 11 prevalent features in NLP and 5 common metrics for semantic textual similarities."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1\">LLMs</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1\">Claim Answer (BLEU)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.3.1\">Evidence Answer (BLEU)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.1\">Average (BLEU)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.2.1\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.2\">0.3543</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.3\">0.3006</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.4\">0.3275</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.3.1\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.2\">0.3586</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3\">0.3178</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.4\">0.3382</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.4.1\">3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.2\">0.5230</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.3\">0.3361</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.4\">0.4296</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.1.1\">4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.2\">0.5248</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.3.1\">0.3963</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.5.4.1\">0.4605</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.6.1\">5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.6.2.1\">0.5323</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.3\">0.3518</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.4\">0.4421</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.1.7.1\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.7.2\">0.5268</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.7.3\">0.3873</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.7.4\">0.4571</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Experiment result of Compare LLMs on the Question Answering task.</figcaption>\n</figure>",
            "capture": "Table 2: Experiment result of Compare LLMs on the Question Answering task."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1\">Pre-trained LLMs</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.2.1\">Epoch</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.3.1\">Accuracy</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.2.1\">bert-large-uncased <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib12\" title=\"\">12</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.2.2\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.3\">0.7040</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.3.1\">gpt2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib21\" title=\"\">21</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.3.2\">30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.3\">0.6813</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.4.1\">t5-large <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib22\" title=\"\">22</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.4.2\">10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.3\">0.6991</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.5.1\">microsoft/deberta-large <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib23\" title=\"\">23</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.5.2\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3\">0.7498</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.6.1\">microsoft/deberta-xlarge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib23\" title=\"\">23</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.6.2\">20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.3\">0.7440</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.7.1\">microsoft/deberta-v3-base <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib3\" title=\"\">3</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.7.2\">15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.3\">0.7364</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.8.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.8.1.1\">microsoft/deberta-v3-large</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib3\" title=\"\">3</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.8.2\">15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.8.3.1\">0.7542</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Experiment results of different Pre-trained LLMs in FakeNet.</figcaption>\n</figure>",
            "capture": "Table 3: Experiment results of different Pre-trained LLMs in FakeNet."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1\">Features</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.2.1\">Accuracy</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.1.2.1\">no features</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.2.2\">0.7542</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.1.3.1\">features not normalized</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.3.2\">0.5880</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.1.4.1\">features normalized between 0 and 1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.4.2\">0.7502</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.5.1.1\">features normalized between -1 and 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.1.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.5.2.1\">0.7556</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Comparing features and various normalizations.</figcaption>\n</figure>",
            "capture": "Table 4: Comparing features and various normalizations."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T5\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T5.1\" style=\"width:433.6pt;height:156.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(17.4pt,-6.3pt) scale(1.08714345136519,1.08714345136519) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.1.1\">Input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.2.1\">Claim</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.3.1\">Evidence</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.4.1\">Question</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.5.1\">Evidence Answer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T5.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.6.1\">Claim Answer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.7.1\">Accuracy</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.2\">\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T5.1.1.2.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.2.2.1\">Length</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.2.3.1\">Length</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.2.4.1\">Length</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.2.5.1\">Length</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.2.6.1\">Length</span></td>\n<td class=\"ltx_td\" id=\"S4.T5.1.1.2.7\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.3.1\">text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.2\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.3\">1000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.3.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.3.7\">0.8044</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.4.1\">text</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.2\">400</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.3\">4000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.4.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.4.7\">0.8396</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.5.1\">text</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.2\">800</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.3\">8000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.5.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.5.7\">0.8462</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.6.1\">text</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.2\">1600</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.3\">10000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.5\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.6.6\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.6.7.1\">0.8502</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.7.1\">question + answer</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.4\">50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.5\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.1.7.6\">100</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.1.7.7\">0.6311</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T5.1.1.8.1\">text + question + answer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.8.2\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.8.3\">1000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.8.4\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.8.5\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T5.1.1.8.6\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.8.7\">0.7849</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>The performance comparison between different alterations in the input and length.</figcaption>\n</figure>",
            "capture": "Table 5: The performance comparison between different alterations in the input and length."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T6.1\" style=\"width:433.6pt;height:95.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(11.9pt,-2.6pt) scale(1.05782296414906,1.05782296414906) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T6.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.1.1\">Ensemble Methods</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.2.1\">Model 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.3.1\">Model 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T6.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.4.1\">Model 3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.5.1\">Accuracy</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.2.1.1\">Weighted sum with labels</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.1.2.2\">Fine-tuned LLM 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.1.2.3\">Fine-tuned LLM 2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.2.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.1.2.5\">0.8564</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.1.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.3.1.1\">Power weighted sum with labels</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.1.3.2\">Fine-tuned LLM 1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.1.3.3\">Fine-tuned LLM 2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.1.3.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.1.3.5\">0.8587</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.4.1.1\">Power weighted sum with two models</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.1.4.2\">Fine-tuned LLM 1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.1.4.3\">Fine-tuned LLM 2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.1.4.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.1.4.5\">0.8609</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T6.1.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.5.1.1\">Power weighted sum with three models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.1.1.5.2\">Fine-tuned LLM 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.1.1.5.3\">Fine-tuned LLM 2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T6.1.1.5.4\">FakeNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.1.1.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.5.5.1\">0.8644</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>The experimental results for the four ensemble methods, where Fine-tuned LLM 1 is the fine-tuned LLM with a claim length of 800 and evidence length of 8000, Fine-tuned LLM 2 is the fine-tuned LLM with a claim length of 1600 and evidence length of 10000, and FakeNet is the deberta-v3-large training with features normalized between -1 and 1.</figcaption>\n</figure>",
            "capture": "Table 6: The experimental results for the four ensemble methods, where Fine-tuned LLM 1 is the fine-tuned LLM with a claim length of 800 and evidence length of 8000, Fine-tuned LLM 2 is the fine-tuned LLM with a claim length of 1600 and evidence length of 10000, and FakeNet is the deberta-v3-large training with features normalized between -1 and 1."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T7\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T7.1\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" id=\"S4.T7.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T7.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.2.1\">Support</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T7.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.3.1\">Neutral</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T7.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.4.1\">Refute</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T7.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.1.5.1\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.1.2.1\">In-Context Learning Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.2\">0.7500</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.3\">0.2857</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.4\">0.3333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.2.5\">0.4300</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.3.1\">Human Baseline</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.2\">0.5500</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.3\">0.6000</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.4\">0.1333</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.3.5\">0.4400</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt\" id=\"S4.T7.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.4.1.1\">Pre-CoFactv3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S4.T7.1.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.4.2.1\">0.8000</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S4.T7.1.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.4.3.1\">0.9133</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S4.T7.1.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.4.4.1\">0.8800</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S4.T7.1.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.4.5.1\">0.8644</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Accuracy of Human Baseline, In-Context Learning Baseline, and Pre-CoFactv3 with different labels.</figcaption>\n</figure>",
            "capture": "Table 7: Accuracy of Human Baseline, In-Context Learning Baseline, and Pre-CoFactv3 with different labels."
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T8.1\">\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T8.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.1.1.1\">Submission</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.1.2.1\">Question Answering</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T8.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.1.3.1\">Text Classification</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T8.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.1.4.1\">Testing Accuracy</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T8.1.2.1\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.2\">Fine-tuned LLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T8.1.2.3\">FakeNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T8.1.2.4\">0.6880</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.1.3.1\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.2\">Fine-tuned LLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T8.1.3.3\">Fine-tuned LLM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T8.1.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.1.3.4.1\">0.6956</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T8.1.4.1\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.1.4.2\">Fine-tuned LLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T8.1.4.3\">Ensemble</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T8.1.4.4\">0.6080</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>The testing accuracy of three submissions.</figcaption>\n</figure>",
            "capture": "Table 8: The testing accuracy of three submissions."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T9.1\">\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T9.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.1.1\">Team Name</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T9.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.2.1\">Testing Accuracy (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T9.1.2.1\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T9.1.2.2\">0.3422 (0%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T9.1.3.1\">Jiankang Han</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.1.3.2\">0.4547 (33%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T9.1.4.1\">SRL_Fact_QA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T9.1.4.2\">0.4551 (33%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt\" id=\"S4.T9.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.5.1.1\">Trifecta (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S4.T9.1.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.5.2.1\">0.6956 (103%)</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>The leaderboard for the Factify 3.0 Workshop.</figcaption>\n</figure>",
            "capture": "Table 9: The leaderboard for the Factify 3.0 Workshop."
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T10\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Table of comparison answers between GPT-3.5 and ground truth in question answering task.</figcaption>\n<table class=\"ltx_tabular\" id=\"A1.T10.1\">\n<tr class=\"ltx_tr\" id=\"A1.T10.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A1.T10.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.1.1.1\">Category</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T10.1.1.2\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"A1.T10.1.1.2.1\">Claim</span></td>\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" id=\"A1.T10.1.1.3\" style=\"width:71.1pt;\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T10.1.1.4\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"A1.T10.1.1.4.1\">Evidence</span></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"A1.T10.1.1.5\" style=\"width:71.1pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T10.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.2.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.2.2\" style=\"width:71.1pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.2.2.1\">GPT-3.5</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T10.1.2.3\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"A1.T10.1.2.3.1\">Ground Truth</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.2.4\" style=\"width:71.1pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A1.T10.1.2.4.1\">GPT-3.5</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.2.5\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"A1.T10.1.2.5.1\">Ground Truth</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T10.1.3.1\">Question 1</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.3.2\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.3.2.1\">Micah Richards.</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T10.1.3.3\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.3.3.1\">Micah Richards</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.3.4\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.3.4.1\">The context does not specify anyone spending an entire season at Aston Villa without playing a single game.</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.3.5\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.3.5.1\">Richards</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T10.1.4.1\">Question 2</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.4.2\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.4.2.1\">Zero.</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T10.1.4.3\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.4.3.1\">a single game</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.4.4\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.4.4.1\">The context does not provide information on the number of games Micah Richards played at Aston Villa.</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.4.5\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.4.5.1\">0</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T10.1.5.1\">Question 3</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.5.2\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.5.2.1\">Aston Villa.</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"A1.T10.1.5.3\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.5.3.1\">: at Aston Vila</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.5.4\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.5.4.1\">Micah Richards spent an entire season at Aston Villa without playing a game.</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T10.1.5.5\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.5.5.1\">Aston Villa</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T10.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T10.1.6.1\">Question 4</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"A1.T10.1.6.2\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.6.2.1\">The context does not specify the exact duration of Micah Richards\u2019 time at Aston Villa without playing.</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T10.1.6.3\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.6.3.1\">an entire season</p>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"A1.T10.1.6.4\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.6.4.1\">The context does not specify the duration of Micah Richards\u2019 time at Aston Villa.</p></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"A1.T10.1.6.5\" style=\"width:71.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T10.1.6.5.1\">remained at the club</p>\n</td>\n</tr>\n</table>\n</figure>",
            "capture": "Table 10: Table of comparison answers between GPT-3.5 and ground truth in question answering task."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T11\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>Table of prompt template candidates to represent In-Context Learning for text classification tasks.</figcaption>\n<table class=\"ltx_tabular\" id=\"A1.T11.1\">\n<tr class=\"ltx_tr\" id=\"A1.T11.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T11.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T11.1.1.1.1\">ID</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A1.T11.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T11.1.1.2.1\">Descriptor</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"A1.T11.1.1.3\" style=\"width:256.1pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_top\" id=\"A1.T11.1.1.3.1\">Prompt</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.2.1\">1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.2.2\">Zero-Shot</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.2.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.2.3.1\">System Prompt: given claim text and evidence text, determine whether the evidence refutes, supports or is neutral against the claim.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.3.1\">2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.3.2\">Few-Shot <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.3.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.3.3.1\">System Prompt: given claim text and evidence text, determine the probabilities that the evidence supports, refutes and is neutral against the claim. At the end, return a label with the highest probability. Here are examples of each as the highest probability:\n<br class=\"ltx_break\"/>1.\n<br class=\"ltx_break\"/>\"claim\": \"The Washington Metro has a frequency of 4 minutes on the Red Line and 9 minutes on other lines , during rush hour .\",\n<br class=\"ltx_break\"/>\"evidence\": \"Trains run more frequently during rush hours on all lines , with scheduled peak hour headways of 3 minutes on the Red Line and 6 minutes on all other lines .\",\n<br class=\"ltx_break\"/>\"label\": \"Refute\"\n<br class=\"ltx_break\"/>2.\n<br class=\"ltx_break\"/>\"claim\": \"Scientists have developed a more accurate way to determine dogs\u2019 ages, rather than multiplying human years by seven. https://t.co/u4E7BJuQ4U\",\n<br class=\"ltx_break\"/>\"evidence\": \"By Francesca Giuliani-Hoffman, CNNUpdated 2:57 PM ET, Sat July 4, 2020 (CNN)How do you compare a dog\u2019s age to that of a person? A popular method says you should multiply the dog\u2019s age by 7 to compute how old Fido is in \u1e27uman years. \"\u0308,\n<br class=\"ltx_break\"/>\"label\": \"Support\"\n<br class=\"ltx_break\"/>3.\n<br class=\"ltx_break\"/>\"claim\": \"More than 400,000 copies of Good Girl Gone Bad were shipped to different parts of the world .\",\n<br class=\"ltx_break\"/>\"evidence\": \"It was certified quintuple platinum by Music Canada , denoting shipments of more than 500,000 copies .\",\n<br class=\"ltx_break\"/>\"label\": \"Neutral\"\n<br class=\"ltx_break\"/>Please answer to the best of your abilities.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.4.1\">3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.4.2\">Probability</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.4.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.4.3.1\">System Prompt: given claim text and evidence text, determine the probabilities that the evidence supports, refutes and is neutral against the claim.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.5.1\">4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.5.2\">Summarized</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.5.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.5.3.1\">System Prompt: given claim text and evidence text, summarize the evidence first, then determine the percentage-wise probabilities that the evidence supports, refutes, and is neutral against the claim. Finish with either {Support}, {Refute} or {Neutral}.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.6.1\">5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.6.2\">Defined</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.6.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.6.3.1\">System Prompt: given claim text and evidence text, determine the probabilities that the evidence supports, refutes and is neutral against the claim. Support means both are related and is likely true. Refute means both are related but is likely false. Neutral means both are unrelated.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.7.1\">6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.7.2\">Characterized</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.7.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.7.3.1\">System Prompt: Pretend you are a human fact checker tasked with the following: given claim text and evidence text, determine the probabilities that the evidence supports, refutes and is neutral against the claim.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.8.1\">7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.8.2\">Chain of Thought <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib25\" title=\"\">25</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.8.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.8.3.1\">System Prompt: given claim text and evidence text, summarize the evidence first, then determine the percentage-wise probabilities that the evidence supports, refutes and is neutral against the claim. Remember to walk us through the thought process. Finish with either {Support}, {Refute} or {Neutral}.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.9.1\">8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.9.2\">Socratic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10281v1#bib.bib26\" title=\"\">26</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.9.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.9.3.1\">System Prompt: given a claim text and evidence text, find five reasons from evidence that supports claim and five reasons from evidence that refutes claim. Give each reason a likelihood probability and how important it is to the validity of the claim. Multiply the two values with each other, turn the refute reasons negative, and add all six reasons together. Return Support if it is higher than 0.1, or Refute if it is lower than -0.1, or Neutral if it is in between.</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T11.1.10.1\">9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T11.1.10.2\">With QA</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"A1.T11.1.10.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.10.3.1\">System Prompt: given claim, evidence, question, answer to question by claim, answer to question by evidence, from 0 to 100, give probabilities that the evidence supports, refutes or is neutral against the claim. Finish the response with the most likely choice overall in brackets .</p>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T11.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T11.1.11.1\">10</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T11.1.11.2\">QA Only</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"A1.T11.1.11.3\" style=\"width:256.1pt;\">\n<p class=\"ltx_p ltx_align_top\" id=\"A1.T11.1.11.3.1\">System Prompt: given question, answer to question by claim, answer to question by evidence, from 0 to 100, give probabilities that the evidence supports, refutes or is neutral against the claim. Finish the response with the most likely choice overall in brackets .</p>\n</td>\n</tr>\n</table>\n</figure>",
            "capture": "Table 11: Table of prompt template candidates to represent In-Context Learning for text classification tasks."
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"A1.T12\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>Table of prompt templates and their performance on test instances of text classification.</figcaption>\n<table class=\"ltx_tabular\" id=\"A1.T12.1\">\n<tr class=\"ltx_tr\" id=\"A1.T12.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A1.T12.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T12.1.1.1.1\">ID</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"A1.T12.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T12.1.1.2.1\">Descriptor</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T12.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T12.1.1.3.1\">Test 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T12.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T12.1.1.4.1\">Test 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T12.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T12.1.1.5.1\">Test 3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.2.1\">1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.2.2\">Zero-Shot</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.2.3\">X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.2.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.2.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.3.1\">2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.3.2\">Few-Shot</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.3.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.3.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.3.5\">X</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.4.1\">3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.4.2\">Probability</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.4.3\">O</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.4.4\">O</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.4.5\">X</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.5.1\">4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.5.2\">Summarized</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.5.3\">X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.5.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.5.5\">X</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.6.1\">5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.6.2\">Defined</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.6.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.6.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.6.5\">X</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.7.1\">6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.7.2\">Characterized</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.7.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.7.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.7.5\">X</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.8.1\">7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.8.2\">Chain of Thought</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.8.3\">X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.8.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.8.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.9.1\">8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.9.2\">Socratic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.9.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.9.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.9.5\">X</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T12.1.10.1\">9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"A1.T12.1.10.2\">With QA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.10.3\">X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.10.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T12.1.10.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T12.1.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A1.T12.1.11.1\">10</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T12.1.11.2\">QA Only</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T12.1.11.3\">X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T12.1.11.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T12.1.11.5\">-</td>\n</tr>\n</table>\n</figure>",
            "capture": "Table 12: Table of prompt templates and their performance on test instances of text classification."
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.10281v1_figure_1.png",
            "caption": "Figure 1: The overview of our Pre-CoFactv3 framework"
        },
        "2": {
            "figure_path": "2403.10281v1_figure_2.png",
            "caption": "Figure 2: The overview of FakeNet"
        },
        "3": {
            "figure_path": "2403.10281v1_figure_3.png",
            "caption": "Figure 3: The confusion matrix for these three models and the ensemble model"
        }
    },
    "references": [
        {
            "1": {
                "title": "Team triple-check at factify 2: Parameter-efficient large foundation models with feature representations for multi-modal fact verification,",
                "author": "W.-W. Du, H.-W. Wu, W.-Y. Wang, W.-C. Peng,",
                "venue": "arXiv preprint arXiv:2302.07740 (2023).",
                "url": null
            }
        },
        {
            "2": {
                "title": "Factify 2: A multimodal fake news and satire news dataset,",
                "author": "S. Suryavardan, S. Mishra, P. Patwa, M. Chakraborty, A. Rani, A. Reganti, A. Chadha, A. Das, A. Sheth, M. Chinnakotla, A. Ekbal, S. Kumar,",
                "venue": "in: Proceedings of DeFactify 2: Second Workshop on Multimodal Fact-Checking and Hate Speech Detection, CEUR, 2023.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing,",
                "author": "P. He, J. Gao, W. Chen,",
                "venue": "arXiv preprint arXiv:2111.09543 (2021).",
                "url": null
            }
        },
        {
            "4": {
                "title": "Information credibility on twitter,",
                "author": "C. Castillo, M. Mendoza, B. Poblete,",
                "venue": "in: Proceedings of the 20th international conference on World wide web, 2011, pp. 675\u2013684.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Enquiring minds: Early detection of rumors in social media from enquiry posts,",
                "author": "Z. Zhao, P. Resnick, Q. Mei,",
                "venue": "in: Proceedings of the 24th international conference on world wide web, 2015, pp. 1395\u20131405.",
                "url": null
            }
        },
        {
            "6": {
                "title": "Fndnet\u2013a deep convolutional neural network for fake news detection,",
                "author": "R. K. Kaliyar, A. Goswami, P. Narang, S. Sinha,",
                "venue": "Cognitive Systems Research 61 (2020) 32\u201344.",
                "url": null
            }
        },
        {
            "7": {
                "title": "Fake news detection using bi-directional lstm-recurrent neural network,",
                "author": "P. Bahad, P. Saxena, R. Kamal,",
                "venue": "Procedia Computer Science 165 (2019) 74\u201382.",
                "url": null
            }
        },
        {
            "8": {
                "title": "Eann: Event adversarial neural networks for multi-modal fake news detection,",
                "author": "Y. Wang, F. Ma, Z. Jin, Y. Yuan, G. Xun, K. Jha, L. Su, J. Gao,",
                "venue": "in: Proceedings of the 24th acm sigkdd international conference on knowledge discovery & data mining, 2018, pp. 849\u2013857.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Multimodal fusion with co-attention networks for fake news detection,",
                "author": "Y. Wu, P. Zhan, Y. Zhang, L. Wang, Z. Xu,",
                "venue": "in: Findings of the association for computational linguistics: ACL-IJCNLP 2021, 2021, pp. 2560\u20132569.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Overview of factify5wqa: Fact verification through 5w question-answering,",
                "author": "S. Suresh, A. Rani, P. Patwa, A. Reganti, V. Jain, A. Chadha, A. Das, A. Sheth, A. Ekbal,",
                "venue": "in: proceedings of DeFactify 3.0: third workshop on Multimodal Fact-Checking and Hate Speech Detection, CEUR, 2024.",
                "url": null
            }
        },
        {
            "11": {
                "title": "Attention is all you need,",
                "author": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, I. Polosukhin,",
                "venue": "Advances in neural information processing systems 30 (2017).",
                "url": null
            }
        },
        {
            "12": {
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding,",
                "author": "J. Devlin, M.-W. Chang, K. Lee, K. Toutanova,",
                "venue": "arXiv preprint arXiv:1810.04805 (2018).",
                "url": null
            }
        },
        {
            "13": {
                "title": "Language models are few-shot learners,",
                "author": "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al.,",
                "venue": "Advances in neural information processing systems 33 (2020) 1877\u20131901.",
                "url": null
            }
        },
        {
            "14": {
                "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text,",
                "author": "P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang,",
                "venue": "arXiv e-prints (2016) arXiv:1606.05250. arXiv:1606.05250.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Logically at factify 2022: Multimodal fact verification,",
                "author": "J. Gao, H.-F. Hoffmann, S. Oikonomou, D. Kiskovski, A. Bandhakavi,",
                "venue": "arXiv preprint arXiv:2112.09253 (2021).",
                "url": null
            }
        },
        {
            "16": {
                "title": "Ino at factify 2: Structure coherence based multi-modal fact verification,",
                "author": "Y. Zhang, Z. Tao, X. Wang, T. Wang,",
                "venue": "arXiv preprint arXiv:2303.01510 (2023).",
                "url": null
            }
        },
        {
            "17": {
                "title": "FACTIFY-5WQA: 5W aspect-based fact verification through question answering,",
                "author": "A. Rani, S. T. I. Tonmoy, D. Dalal, S. Gautam, M. Chakraborty, A. Chadha, A. Sheth, A. Das,",
                "venue": "in: A. Rogers, J. Boyd-Graber, N. Okazaki (Eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Toronto, Canada, 2023, pp. 10421\u201310440. URL: https://aclanthology.org/2023.acl-long.581. doi:10.18653/v1/2023.acl-long.581.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Bleu: a method for automatic evaluation of machine translation,",
                "author": "K. Papineni, S. Roukos, T. Ward, W.-J. Zhu,",
                "venue": "in: Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311\u2013318.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Roberta: A robustly optimized bert pretraining approach,",
                "author": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov,",
                "venue": "arXiv preprint arXiv:1907.11692 (2019).",
                "url": null
            }
        },
        {
            "20": {
                "title": "Language models are unsupervised multitask learners,",
                "author": "A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,",
                "venue": "OpenAI blog 1 (2019) 9.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer,",
                "author": "C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu,",
                "venue": "The Journal of Machine Learning Research 21 (2020) 5485\u20135551.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Deberta: Decoding-enhanced bert with disentangled attention,",
                "author": "P. He, X. Liu, J. Gao, W. Chen,",
                "venue": "arXiv preprint arXiv:2006.03654 (2020).",
                "url": null
            }
        },
        {
            "23": {
                "title": "Chain-of-thought prompting elicits reasoning in large language models,",
                "author": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, D. Zhou,",
                "venue": "in: S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh (Eds.), Advances in Neural Information Processing Systems, volume 35, Curran Associates, Inc., 2022, pp. 24824\u201324837. URL: https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Llama 2: Open foundation and fine-tuned chat models,",
                "author": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al.,",
                "venue": "arXiv preprint arXiv:2307.09288 (2023).",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.10281v1",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.2.1",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.4",
            "4.4.1",
            "4.5",
            "4.5.1",
            "4.5.2",
            "4.5.3",
            "4.6.1",
            "4.6.2",
            "4.7.1",
            "4.7.2"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "4",
            "4.4",
            "4.4.1",
            "4.5",
            "4.5.1",
            "4.5.2",
            "4.5.3"
        ]
    },
    "research_context": {
        "paper_id": "2403.10281v1",
        "paper_title": "Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning",
        "research_background": "### Motivation\nIn today's digital age, the internet and social media have dramatically increased the volume of readily accessible information, making the verification of facts a critical challenge. The growth of digital platforms has allowed widespread content creation and dissemination, but it has also led to the rapid spread of misinformation, disinformation, and misleading content. This trend threatens the quality of information consumed by individuals and communities and poses a risk to democratic societies. Therefore, robust mechanisms for fact verification have become essential to enable informed decision-making, maintain a well-informed citizenry, and uphold the integrity of information.\n\n### Research Problem\nThe primary research problem tackled in this paper is the development of advanced methodologies to classify claims as supported, refuted, or neutral based on provided evidence. Given the proliferation of false and misleading information, there is a need to enhance large language models to discern factual accuracy accurately. The complexity and limitations of language understanding pose significant challenges in achieving this goal.\n\n### Relevant Prior Work\nThe study draws significant influence from existing research and methodologies in the field:\n1. **Du et al.'s Pre-CoFactv2 Model**: Recognized for its use of parameter-efficient fine-tuning on the DeBERTa model during the Factify 2 challenge. The impressive performance of this methodology inspired this research to extend its scope.\n2. **DeBERTaV3**: This paper utilizes the advanced DeBERTaV3 model for comprehensive fine-tuning, aiming to surpass the performance achieved with earlier versions.\n3. **Methodologies such as Fine-Tuning, In-Context Learning, and Feature Extraction**: These techniques are explored to navigate the complexities of claim verification, with a focus on enhancing the nuanced understanding of linguistic contexts.\n4. **Ensemble Learning Approaches**: Investigated for synthesizing diverse model outputs to improve classification accuracy and reliability.\n\nThe resultant model introduced in this research, named Pre-CoFactv3, demonstrates a significant improvement across question answering and text classification tasks. The evaluation results indicate that the fine-tuning method outperforms in-context learning and even human baselines, establishing a new state-of-the-art in the domain of claim verification.",
        "methodology": "### Methodology Overview\n\n#### Model Architecture\nThe proposed method leverages pre-trained Large Language Models (LLMs) for generating embeddings of key textual elements: claim, evidence, claim answer, and evidence answer. The key innovation lies in the treatment of these embeddings and subsequent stages of fine-tuning and integration.\n\n#### Embedding Generation and Freezing\n- **Pre-trained LLM Utilization**: Embeddings for the claim, evidence, claim answer, and evidence answer are generated using pre-trained LLMs.\n- **Parameter Freezing**: The parameters of the pre-trained LLMs are frozen. Only the last embedding layer is trained, ensuring that the robust, pre-trained knowledge remains incorporated while adapting the model to the specific task.\n\n#### Co-Attention and Interaction Learning\n- **Co-Attention Mechanism**: The embeddings are then processed through a co-attention mechanism. This involves creating pairs of embeddings that participate in co-attentions:\n  1. Claim and Evidence\n  2. Claim and Claim Answer\n  3. Claim and Evidence Answer\n  4. Evidence and Claim Answer\n  5. Evidence and Evidence Answer\n  6. Claim Answer and Evidence Answer\n\n- **Co-Attention Block**: A modified version of the multi-head self-attention block (similar to the Transformer encoder) is utilized. Each block takes two embeddings as inputs to learn interactions and relationships.\n\n- **Mean Aggregation**: The results from the co-attention operations are aggregated with mean pooling to obtain a single, cohesive embedding representing the text.\n\n#### Feature Extraction and Text Characteristics\n- **Common NLP Features**: Inspired by methodologies in prior work, we compute 11 standard NLP features for each text source, including word length, capital words count, stopwords, and quotes. Given the multiple text sources (claim, evidence, question, claim answer, evidence answer), we derive a total of 55 features.\n  \n- **Similarity Calculation**: We incorporate various semantic similarity measures using tools like SimCSE, MPNet, The Fuzz, TF-IDF, and ROUGE to compute similarities between claim-evidence pairs and corresponding question-answer pairs. This step adds an additional 10 features focused on text relevance and coherence.\n\n#### Feature Integration and Classification\n- **Feature Normalization and Embedding**: The combined 65 features from the extracted text characteristics and similarity measures are normalized and transformed into embeddings.\n  \n- **Classifier Design**: A simple two-layer Multilayer Perceptron (MLP) classifier is employed. The embeddings from the pre-trained LLMs and the feature extractor are concatenated to form the input for the MLP classifier.\n  \n- **Output**: The classifier produces a probability distribution over the possible labels. The cross-entropy loss function is used for optimization.\n\n### Summary\nThe methodology emphasizes a hybrid approach, integrating robust embeddings from pre-trained LLMs, detailed feature extraction from standard NLP characteristics, and advanced similarity measures. The integration of these diverse informational sources through a co-attention mechanism and a straightforward MLP classifier results in a refined model adept at fact verification.",
        "main_experiment_and_results": "### Main Experiment Setup and Results\n\n#### Experiment Setup:\n\n1. **Models Evaluated:**\n   - **roberta-large:** Fine-tuned with:\n     - SQuAD 2.0\n     - FACTIFY5WQA\n   - **deberta-v3-large:** Fine-tuned with:\n     - SQuAD 2.0\n     - FACTIFY5WQA\n   - **Combined fine-tuning of SQuAD 2.0 and FACTIFY5WQA:** Both roberta-large and deberta-v3-large.\n\n2. **Fine-Tuning Protocol:**\n   - Conducted using the Hugging Face Trainer API on the Question Answering task.\n   - BLEU scores are used to evaluate both claim answer and evidence answer.\n\n3. **Evaluation Metrics:**\n   - The main evaluation metric is the average BLEU score for both claim answer and evidence answer.\n\n4. **Datasets:**\n   - **SQuAD 2.0**\n   - **FACTIFY5WQA**\n\n#### Main Experimental Results:\n\n1. **Comparison Across LLMs:**\n   - **deberta-v3-large** fine-tuned solely on FACTIFY5WQA achieved the best performance.\n   - Highlights the significance of fine-tuning on a dataset that aligns closely with the task's characteristics.\n\n2. **Findings on Model Size:**\n   - There is no strictly positive correlation between model size and performance.\n   - Example: deberta-v3-large outperformed deberta-v3-base, whereas deberta-xlarge did not surpass deberta-large.\n\n3. **Optimal Model and Performance:**\n   - deberta-v3-large achieved the highest accuracy among the tested models.\n   - Performance improvement with longer text lengths, where the model with a claim length of 1600 and evidence length of 10000 achieved the highest accuracy.\n\nThese results underscore the importance of fine-tuning on specific datasets, suggesting that a dataset matching the task can enhance model performance. Additionally, the experiment reveals that model size is not the only determinant of performance; dataset alignment and text length also play crucial roles."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Assess the impact of different fine-tuning datasets on the performance of LLMs for the Question Answering task.",
            "experiment_process": "We compared the results across different LLMs fine-tuned on SQuAD 2.0 and FACTIFY5WQA datasets. The LLMs used in the comparison were roberta-large and deberta-v3-large models. Fine-tuning was conducted using the Hugging Face Trainer API on the Question Answering task, and BLEU scores were used to evaluate both claim answer and evidence answer, taking the average of the two as the metric.",
            "result_discussion": "Results indicated that deberta-v3-large fine-tuned solely with the FACTIFY5WQA dataset achieved the best performance. This highlights the importance of fine-tuning on a dataset that aligns with the task's characteristics. However, since most answers are short, this could result in lower BLEU scores with 4-gram precision.",
            "ablation_id": "2403.10281v1.No1"
        },
        {
            "research_objective": "Evaluate the effect of text input length and the inclusion of questions and answers on text classification performance using LLMs.",
            "experiment_process": "The microsoft/deberta-v3-large model was fine-tuned with an epoch of 8, a batch size of 4, and a learning rate of 0.00002. Fine-tuning was executed on the Text Classification task with variations in input content (text only vs. text, question, and answer) and text length (claim length of 1600 and evidence length of 10000).",
            "result_discussion": "Surprisingly, better performance was achieved with only text as input compared to using text, question, and answer, questioning the benefit of including questions and answers for text classification. Longer text lengths also correlated with improved performance, with the chosen model outperforming FakeNet.",
            "ablation_id": "2403.10281v1.No2"
        },
        {
            "research_objective": "Determine the effect of including additional features and various normalization techniques on text classification performance.",
            "experiment_process": "65 features were selected as input, and different normalization techniques were applied. Results were evaluated and compared.",
            "result_discussion": "Inclusion of features normalized between -1 and 1 led to the best performance, albeit with only a slight improvement of 0.19%.",
            "ablation_id": "2403.10281v1.No3"
        },
        {
            "research_objective": "Analyze the efficacy of ensemble methods for the text classification task.",
            "experiment_process": "Four ensemble methods were tested, involving two fine-tuned LLMs and one FakeNet, with results evaluated using accuracy metrics.",
            "result_discussion": "The ensemble method using a power-weighted sum with the three models achieved the highest accuracy, capitalizing on the strengths of each model. Nevertheless, the models struggled with accurately identifying 'Support' instances, limiting performance improvement in this category.",
            "ablation_id": "2403.10281v1.No4"
        }
    ]
}