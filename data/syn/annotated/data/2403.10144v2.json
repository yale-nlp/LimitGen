{
    "title": "NLP Verification: Towards a General Methodology for Certifying Robustness",
    "abstract": "Deep neural networks (DNNs) have exhibited substantial success in the field of Natural Language Processing (NLP). As these systems are increasingly integrated into real-world applications, ensuring their safety and reliability becomes a primary concern. There are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Computer Vision had pioneered the use of formal verification for neural networks for such scenarios and developed common verification standards and pipelines. In contrast, NLP verification methods have only recently appeared in the literature. While presenting sophisticated algorithms in their own right, these papers have not yet crystallised into a common methodology. They are often light on the pragmatical issues of NLP verification, and the area remains fragmented.",
    "sections": [
        {
            "section_id": "1",
            "parent_section_id": null,
            "section_name": "Introduction",
            "text": "Deep neural networks (DNNs) have demonstrated remarkable success at addressing challenging problems in various areas, such as Computer Vision (CV) [1  ###reference_b1###] and Natural Language Processing (NLP) [2  ###reference_b2###, 3  ###reference_b3###].\nHowever, as DNN-based systems are increasingly deployed in safety-critical applications [4  ###reference_b4###, 5  ###reference_b5###, 6  ###reference_b6###, 7  ###reference_b7###, 8  ###reference_b8###, 9  ###reference_b9###], ensuring their safety and security becomes paramount. Current NLP systems cannot guarantee either truthfulness, accuracy, faithfulness, or groundedness of outputs given an input query, which can lead to different levels of harm.\nNLP domain is the requirement of a chatbot to correctly disclose non-human identity, when prompted by the user to do so. Recently there have been several pieces of legislation proposed that will enshrine this requirement in law [10  ###reference_b10###, 11  ###reference_b11###].\nIn order to be compliant with these new laws, in theory the underlying DNN of the chatbot (or the sub-system responsible for identifying these queries) must be 100% accurate in its recognition of such a query. However, a central theme of generative linguistics going back to von Humboldt, is that language is \u2018an infinite use of finite means\u2019, i.e there exists many ways to say the same thing. In reality the questions can come in a near infinite number of different forms, all with similar semantic meanings. For example: \u201cAre you a Robot?\u201d, \u201cAm I speaking with a person?\u201d, \u201cAm i texting to a real human?\u201d, \u201cAren\u2019t you a chatbot?\u201d.\nFailure to recognise the user\u2019s intent and thus failure to answer the question correctly could potentially have legal implications for designers of these systems [10  ###reference_b10###, 11  ###reference_b11###].\nSimilarly, as such systems become widespread in their use, it may be desirable to have guarantees on queries concerning safety critical domains, for example when the user asks for medical advice. Research has shown that users tend to attribute undue expertise to systems [12  ###reference_b12###, 7  ###reference_b7###] potentially causing real world harm [13  ###reference_b13###] (e.g. \u2018Is it safe to take these painkillers with a glass of wine?\u2019).\nHowever, a question remains on how to ensure that NLP systems can give formally guaranteed outputs, particularly for scenarios that require maximum control over the output.\nOne possible solution has been to apply formal verification techniques to deep neural networks (DNN), which aims at ensuring that for every possible input, the output generated by the network satisfies the desired properties.\nOne example has already been given above, i.e. guaranteeing that a system will accurately disclose its non-human identity.\nThis example is an instance of the more general problem of DNN robustness verification, where the aim is to guarantee that every point in a given region of the embedding space is classified correctly.\nConcretely, given a network , one first defines subspaces  of the vector space m. For example, one can define \u201c\u201d or \u201c\u201d111The terminology will be made precise in Example 1  ###reference_mple1###.\naround all input vectors given by the dataset in question (in which case the number of  will correspond to the number of samples in the given dataset). Then, using a separate verification algorithm , we verify whether  is robust for each , i.e. whether  assigns the same class for all vectors contained in .\nNote that each  is itself infinite (i.e. continuous), and thus  is usually based on equational reasoning, abstract interpretation or bound propagation (see related work in Section 2  ###reference_###).\nThe subset of  for which  is proven robust, forms the set of verified subspaces of the given vector space (for ). The percentage of verified subspaces\nis called the verification success rate (or verifiability). Given , we say a DNN  is more verifiable than  if  has higher verification success rate on .\nDespite not providing a formal guarantee about the entire embedding space, this result is useful as it provides guarantees about the behaviour of the network over a large set of unseen inputs.\nExisting verification approaches primarily focus on computer vision (CV) tasks, where images are seen as vectors in a continuous space and every point in the space corresponds to a valid image. In contrast, sentences in NLP form a discrete domain222In this paper, we work with textual representations of sentences. Raw audio input can be seen as continuous, but this is out of scope of this paper., making it challenging to apply traditional verification techniques effectively.\nIn particular, taking an NLP dataset  to be a set of sentences  written in natural language, an embedding  is a function that maps a sentence to a vector in m. The resulting vector space is called the embedding space. Due to discrete nature of the set , the reverse of the embedding function  is undefined for some elements of m. This problem is known as the \u201cproblem of the embedding gap\u201d. Sometimes, one uses the term to more generally refer to any discrepancies that  introduces, for example, when it maps dissimilar sentences close\nin m. We use the term in both mathematical and NLP sense.\nMathematically, the general (geometric) \u201cDNN robustness verification\u201d approach of defining and verifying subspaces of m should work, and some prior works exploit this fact. However, pragmatically, because of the embedding gap, usually only a tiny fraction of vectors contained in the verified subspaces map back to valid sentences.\nWhen a verified subspace contains no or very few sentence embeddings, we say that verified subspace has low generalisability. Low generalisability may render verification efforts ineffective for practical applications.\nFrom the NLP perspective, there are other, more subtle, examples where the embedding gap can manifest. Consider an example of a subspace containing sentences that are semantically similar to the sentence: \u2018i really like too chat to a human. are you one?\u2019.\nSuppose we succeed in verifying a DNN to be robust on this subspace.\nThis provides a guarantee that the DNN will always identify sentences in this subspace as questions about human/robot identity.\nBut suppose the embedding function \nwrongly embeds sentences belonging to an opposite class into this subspace. For example, the LLM Vicuna [14  ###reference_b14###] generates the following sentence as a rephrasing of the previous one: Do you take pleasure in having a conversation with someone?. Suppose our verified subspace contained an embedding of this sentence too, and thus our verified DNN identifies this second sentence to belong to the same class as the first one. However, the second sentence is not a question about human/robot identity of the agent! When we can find such an example, we say that it falsifies the verification guarantee for the subspace it is contained in.\nAlternatively, we say that the subspace is falsifiable.\nRobustness verification in NLP is particularly susceptible to this problem, because we cannot cross the embedding gap in the opposite direction as the embedding function is not invertible. This means it is difficult for humans to understand what sort of sentences are captured by a given subspace.\nWe start by showing, through a series of experiments, that purely geometric approaches to NLP verification (such as those based on the  [15  ###reference_b15###]) suffer from the verifiability-generalisability trade-off: that is, when one metric improves, the other deteriorates. Figure 1  ###reference_### gives a good idea of the problem: the smaller the s are, the more verifiable they are, and less generalisable. To the best of our knowledge, this phenomenon has not been reported in the literature before (in the NLP context).\nWe propose a general method for measuring generalisability of the verified subspaces, based on algorithmic generation of semantic attacks on sentences included in the given verified semantic subspace.\n###figure_1### ###figure_2### ###figure_3### ###figure_4### An alternative method to the purely geometric approach\nis to construct subspaces of the embedding space based on the semantic perturbations of sentences (first attempts to do this appeared in [16  ###reference_b16###, 17  ###reference_b17###, 18  ###reference_b18###, 19  ###reference_b19###]). Concretely, the idea is to form each  by embedding a sentence  and  semantic perturbations of  into the real vector space and enclosing them inside some geometric shape. Ideally, this shape should be the convex hull around the  embedded sentences (see Figure 1  ###reference_###), however calculating convex hulls with sufficient precision is computationally infeasible for high number of dimensions. Thus, simpler shapes, such as hyper-cubes and hyper-rectangles are used in the literature.\nWe propose a novel refinement of these ideas, by including the method of a hyper-rectangle rotation in order to increase the shape precision (see Figure 1  ###reference_###). We will call the resulting shapes semantic subspaces (in contrast to those obtained purely geometrically).\nA few questions have been left unanswered in the previous work [16  ###reference_b16###, 17  ###reference_b17###, 18  ###reference_b18###, 19  ###reference_b19###]. Firstly, because generalisability of the verified subspaces is not reported in the literature, we cannot know whether the prior semantically-informed approaches are better in that respect than purely geometric methods. If they are better in both verifiability and generalisability, it is unclear whether the improvement should be attributed to:\nthe fact that verified semantic subspaces simply have an optimal volume (for the verifiability-generalisability trade-off), or\nthe improved precision of verified subspaces that comes from using the semantic knowledge.\nThrough a series of experiments, we confirm that semantic subspaces are more verifiable and more generalisable than their geometric counterparts. Moreover, by comparing the volumes of the obtained verified semantic and geometric subspaces, we show that the improvement is partly due to finding an optimal size of subspaces (for the given embedding space), and partly due to improvement in shape precision.\nThe second group of unresolved questions concerns robust training regimes in NLP verification that is used as means of improving verifiability of subspaces in prior works [16  ###reference_b16###, 17  ###reference_b17###, 18  ###reference_b18###, 19  ###reference_b19###].\nIt was not clear what made robust training successful:\nwas it because additional examples generally improved the precision of the decision boundary? (in which case dataset augmentation would have a similar effect);\nwas it because adversarial examples specifically improved adversarial robustness (in which case simple  PGD attacks would have a similar effect); or\ndid the knowledge of semantic subspaces play the key role?\nThrough a series of experiments we show that the latter is the case.\nIn order to do this, we formulate a semantically robust training\nmethod that uses projected gradient descent on semantic subspaces (rather than on  as the famous PGD algorithm does [20  ###reference_b20###]).\nWe use different forms of semantic perturbations, at character, word and sentence levels (alongside the standard PGD training and data augmentation) to perform semantically robust training.\nWe conclude that semantically robust training generally wins over the standard robust training methods. Moreover, the more sophisticated semantic perturbations we use in semantically robust training, the more verifiable the neural network will be obtained as a result (at no cost to generalisability).\nFor example, using the strongest form of attack (the polyjuice attack [21  ###reference_b21###]) in semantically robust training, we obtain DNNs that are more verifiable irrespective of the way the verified sub-spaces are formed.\nAs a result, we arrive at a fully parametric approach to NLP verification that disentangles the four components:\nchoice of the semantic attack (on the NLP side),\nsemantic subspace formation in the embedding space (on the geometric side),\nsemantically robust training (on the machine learning side),\nchoice of the verification algorithm (on the verification side).\nWe argue that, together with the new generalisability metric, this approach opens the way for more principled evaluation of performance of NLP verification methods that accounts for the effects of the embedding gap; and generation of more transparent NLP verification benchmarks.\nWe implement a tool ANTONIO that generates NLP verification benchmarks based on the above choices.\nThis paper is the first to use a complete SMT-based verifier (namely Marabou [22  ###reference_b22###]) for NLP verification.\nWe test the theoretical results by suggesting an NLP verification pipeline, a general methodology that starts with NLP analysis of the dataset and obtaining semantically similar perturbations that together characterise the semantic meaning of a sentence; proceeds with embedding of the sentences into the real vector space and defining semantic subspaces around embeddings of semantically similar sentences; and culminates with using these subspaces for both training and verification.\nThis clear division into stages allows us to formulate practical NLP methods for\nminimising the effects of the embedding gap. In particular, we show that\nthe quality of the generated sentence perturbations maybe improved through the use of human evaluation, cosine similarity and ROUGE-N.\nWe introduce the falsifiability metric as an effective practical way to measure the quality of the embedding functions. Through a detailed case study, we show how geometric and NLP intuitions can be put together at work towards obtaining DNNs that are more verifiable over better generalisable and less falsifiable semantic subspaces. Perhaps more importantly, the proposed methodology opens the way for transparency in reporting NLP verification results, \u2013 something that this domain will benefit from if it reaches the stage of practical deployment of NLP verification pipelines.\nPaper Outline. From here, the paper proceeds as follows. Section 2  ###reference_### gives an extensive literature review encompassing DNN verification methods generally, and NLP verification methods in particular. The section culminates with distilling a common \u201cNLP verification pipeline\u201d encompassing the existing literature. Based on the understanding of major components of the pipeline, the rest of the paper focuses on improving understanding or implementation of its components. Section 3  ###reference_### formally defines the components of the pipeline in a general mathematical notation, which abstracts away from particular choices of sentence perturbation, sentence embedding, training and verification algorithms. The central notion the section introduces is that of geometric and semantic subspaces. The next Section 4  ###reference_### makes full use of this general definition, and shows that semantic subspaces play a pivotal role in improving verification and training of DNNs in NLP. This section formally defines the generalisability metric and considers the problem of generalisability-verifiability trade-off. Through thorough empirical evaluation, it shows that a principled approach to defining semantic subspaces\ncan help to improve both generalisability and verifiability of DNNs, thus reducing the effects of the trade-off. The final Section 5  ###reference_### further tests the NLP verification pipelines using state-of-the-art NLP tools, and analyses the effects of the embedding gap from the NLP perspective, in particular it introduces a method of measuring falsifiability of semantic subspaces and reporting this metric alongside verifiability and generalisability. Section 6  ###reference_### concludes the paper and discusses future work."
        },
        {
            "section_id": "2",
            "parent_section_id": null,
            "section_name": "Related Work",
            "text": ""
        },
        {
            "section_id": "2.1",
            "parent_section_id": "2",
            "section_name": "DNN Verification",
            "text": "Formal verification is an active field across several domains including hardware [23  ###reference_b23###, 24  ###reference_b24###], software languages [25  ###reference_b25###], network protocols [26  ###reference_b26###] and many more [27  ###reference_b27###]. However, it was only recently that this\nbecame applicable to the field of machine learning [28  ###reference_b28###].\nAn input query to a verifier consists of a subspace within the embedding space and a target subspace of outputs, typically a target output class.\nThe verifier then returns either true, false or unknown. True indicates that there exists an input within the given input subspace whose output falls within the given output subspace, often accompanied by an example of such input. False indicates that no such input exists.\nSeveral verifiers are popular in DNN verification and competitions [29  ###reference_b29###, 30  ###reference_b30###, 31  ###reference_b31###, 32  ###reference_b32###].\nWe can divide them into 2 main categories: complete verifiers which return true/false and incomplete verifiers which return true/unknown.\nWhile complete verifiers are always deterministic, incomplete verifiers may be probabilistic.\nUnlike deterministic verification, probabilistic verification is not sound and a verifier may incorrectly output true with a very low probability (typically 0.01%).\nComplete Verification based on Linear Programming & SMT solving.\nThis group of verification methods [33  ###reference_b33###, 28  ###reference_b28###, 22  ###reference_b22###] is built upon the observation that feed-forward neural networks are defined by the sequential composition of affine transformations and non-linear activation functions.\nWhen the activation functions are piecewise linear\n(e.g. ReLU), the DNN can be encoded by conjunctions and disjunctions\nof linear inequalities and thus linear programming algorithms can be directly applied to solve the satisfiability problem, yielding a solution to complete verification.\nA state-of-the-art tool is Marabou [22  ###reference_b22###], which answers queries about neural networks and their properties in the form of constraint satisfaction problems. Marabou takes the network as input and first applies multiple pre-processing steps to infer bounds for each node in the network. It applies the algorithm ReLUplex [28  ###reference_b28###], a combination of Simplex [34  ###reference_b34###] search over linear constraints, modified to work for networks with piece-wise linear activation functions. With time, Marabou grew into a complex prover with multiple heuristics supplementing the original ReLUplex algorithm [22  ###reference_b22###], for example it now includes abstract interpretation and MILP-based algorithms which we survey below.\nIncomplete Verification based on Abstract Interpretation takes inspiration from the domain of abstract interpretation, and mainly uses linear relaxations on ReLU neurons, resulting in an over-approximation of the initial constraint.\nAbstract interpretation was first developed by Cousot and Cousot [35  ###reference_b35###] in 1977. It formalises the idea of abstraction of mathematical structures, in particular those involved in the specification of properties and proof methods of computer systems [36  ###reference_b36###] and it has since been used in many applications [37  ###reference_b37###].\nSpecifically, for DNN verification, this technique can model the behaviour of a network using an abstract domain that captures the possible range of values the network can output for a given input.\nAbstract interpretation-based verifiers can define a lower bound and an upper bound of the output of each ReLU neuron as linear constraints, which define a region called ReLU polytope that gets propagated through the network.\nOne can use interval bound propagation (IBP) [38  ###reference_b38###, 39  ###reference_b39###, 40  ###reference_b40###, 41  ###reference_b41###].\nThe strength of IBP-based methods lies in their efficiency; they are faster than alternative approaches and demonstrate superior scalability. However, their primary limitation lies in the inherently loose bounds they produce [39  ###reference_b39###]. This drawback becomes particularly pronounced in the case of deeper neural networks, typically those with more than 10 layers [42  ###reference_b42###], where they cannot certify non-trivial robustness due to the amplification of over-approximation.\nOther methods that are less efficient but produce tighter bounds are based on polyhedra abstraction, such as CROWN [43  ###reference_b43###] and DeepPoly [44  ###reference_b44###], or based on multi-neuron relaxation, such as PRIMA [45  ###reference_b45###].\nOne of the most mature tools in this category is ERAN [46  ###reference_b46###], which can be used for complete verification, but its main purpose is deterministic incomplete verification through abstract interpretation (DeepPoly) and multi-neuron relaxation (PRIMA).\nMILP-based approaches [47  ###reference_b47###, 48  ###reference_b48###, 49  ###reference_b49###] encode the verification problem as a mixed-integer linear programming problem, in which the constraints are linear inequalities and the objective is represented by a linear function.\nThus, the DNN verification problem can be precisely encoded as a MILP problem.\nFor example, ERAN [46  ###reference_b46###], which is mainly used as an incomplete verifier combines abstract interpretation with the MILP solver GUROBI [50  ###reference_b50###].\nERAN uses abstract domains with custom multi-neuron relaxations to support fully-connected, convolutional, and residual networks with ReLU, Sigmoid, Tanh, and Maxpool activations.\nBaB-based verification [51  ###reference_b51###, 52  ###reference_b52###, 53  ###reference_b53###, 54  ###reference_b54###, 55  ###reference_b55###, 56  ###reference_b56###, 57  ###reference_b57###] relies on the piecewise linear property of DNNs: since each ReLU neuron outputs ReLU() = max{,} is piecewise linear, we can consider its two linear pieces ,  separately.\nA BaB verification approach, as the name suggests, consists of two parts: branching and bounding. It first applies incomplete verification to derive a lower bound and an upper bound, then, if the lower bound is positive it terminates with \u2018verified\u2019, else, if the upper bound is non-positive it terminates with \u2018not verified\u2019 (bounding). Otherwise, the approach recursively chooses a neuron to split into two branches (branching), resulting in two linear constraints. Then bounding is applied to both constraints and if both are satisfied the verification terminates, otherwise the other neurons are split recursively. When all neurons are split, the branch will contain only linear constraints, and thus the approach applies linear programming to compute the constraint and verify the branch.\nIt is important to note that BaB approaches themselves are neither inherently complete nor incomplete. BaB is an algorithm for splitting problems into sub-problems and requires a solver to resolve the linear constraints. The completeness of the verification depends on the combination of BaB and the solver used.\nMulti-Neuron Guided Branch-and-Bound (MN-BaB) [54  ###reference_b54###] is a state-of-the-art neural network verifier that builds on the tight multi-neuron constraints proposed in PRIMA [58  ###reference_b58###] and leverages these constraints within a BaB framework to yield an efficient, GPU based dual solver.\nAnother state-of-the-art tool is -CROWN [59  ###reference_b59###, 56  ###reference_b56###], a neural network verifier based on an efficient linear bound propagation framework and branch-and-bound. It can be accelerated efficiently on GPUs and can scale to relatively large convolutional networks (e.g.,  parameters).\nIt also supports a wide range of neural network architectures (e.g., CNN, ResNet, and various activation functions).\nBaB-based methods are more scalable than solver-based approaches, however they introduce a level of abstraction and sacrifice precision in favor of scalability. For example GCP-CROWN [57  ###reference_b57###] extracts convex constraints from MILP solvers and integrates them in linear inequality propagation, which can be viewed as leveraging multi-neuron relaxations in branch-and-bound complete verification.\nProbabilistic incomplete verification approaches add random noise to smooth models, and then derive certified robustness for these smoothed models.\nThis field is commonly referred to as Randomised Smoothing,\ngiven that these approaches provide probabilistic guarantees of robustness, and all current probabilistic verification techniques are tailored for smoothed models [60  ###reference_b60###, 61  ###reference_b61###, 62  ###reference_b62###, 63  ###reference_b63###, 64  ###reference_b64###, 65  ###reference_b65###].\nGiven that this work focuses on deterministic approaches, here we only report the existence of this line of work without going into details.\nNote that these existing verification approaches primarily focus on computer vision tasks, where images are seen as vectors in a continuous space and every point in the space corresponds to a valid image, while sentences in NLP form a discrete domain, making it challenging to apply traditional verification techniques effectively.\nIn this work we use both an abstract interpretation-based incomplete verifier (ERAN [46  ###reference_b46###]) and an SMT-based complete verifier (Marabou [22  ###reference_b22###]) in order to demonstrate the effect that the choice of a verifier may bring, and demonstrate common trends."
        },
        {
            "section_id": "2.2",
            "parent_section_id": "2",
            "section_name": "Robust Training",
            "text": "Verifying DNNs poses significant challenges if they are not appropriately trained. The fundamental issue lies in the failure of DNNs, including even sophisticated models, to meet essential verification properties, such as robustness [66  ###reference_b66###].\nTo enhance robustness, various training methodologies have been proposed. It is noteworthy that, although robust training by projected gradient descent [67  ###reference_b67###, 20  ###reference_b20###, 68  ###reference_b68###] predates verification, contemporary approaches are often related to, or derived from, the corresponding verification methods by optimizing verification-inspired regularization terms or injecting specific data augmentation during training.\nIn practice, after robust training, the model usually achieves higher certified robustness and is more likely to satisfy the desired verification properties [66  ###reference_b66###]. Thus, robust training is a strong complement to robustness verification approaches.\nRobust training techniques can be classified into several large groups:\ndata augmentation [69  ###reference_b69###],\nadversarial training [67  ###reference_b67###, 20  ###reference_b20###] including property-driven training [70  ###reference_b70###, 71  ###reference_b71###],\nIBP training [39  ###reference_b39###, 72  ###reference_b72###] and other forms of\ncertified training [73  ###reference_b73###], or\na combination thereof [74  ###reference_b74###, 66  ###reference_b66###].\nData augmentation involves the creation of synthetic examples through the application of diverse transformations or perturbations to the initial training data. These generated instances are then incorporated into the original dataset to enhance the training process.\nAdversarial training entails identifying worst-case examples at each epoch during the training phase and calculating an additional loss on these instances. State of the art adversarial training involves projected gradient descent algorithms such as FGSM [67  ###reference_b67###] and PGD [20  ###reference_b20###].\nCertified training methods focus on providing mathematical guarantees about the model\u2019s behaviour within certain bounds. Among them, we can name IBP\ntraining [39  ###reference_b39###, 72  ###reference_b72###] techniques, which impose intervals or bounds on the predictions or activations of the model, ensuring that the model\u2019s output lies within a specific range with high confidence.\nNote that all techniques mentioned above can be categorised based on whether they primarily augment the data (such as data augmentation) or augment the loss function (as seen in adversarial, IBP and certified training).\nAugmenting the data tends to enhance generalisation and is efficient, although it may not help against stronger adversarial attacks. Conversely, methods that manipulate the loss functions directly are more resistant to strong adversarial attacks but often come with higher computational costs. Ultimately, the choice between altering data or loss functions depends on the specific requirements of the application and the desired trade-offs between performance, computational complexity, and robustness guarantees."
        },
        {
            "section_id": "2.3",
            "parent_section_id": "2",
            "section_name": "NLP robustness",
            "text": "There exists a substantial body of research dedicated to enhancing the adversarial robustness of NLP systems [75  ###reference_b75###, 76  ###reference_b76###, 77  ###reference_b77###, 78  ###reference_b78###, 79  ###reference_b79###, 80  ###reference_b80###, 81  ###reference_b81###]. These efforts aim to mitigate the vulnerability of NLP models to adversarial attacks and improve their resilience in real-world scenarios [76  ###reference_b76###, 77  ###reference_b77###] and mostly employ data augmentation techniques [82  ###reference_b82###, 83  ###reference_b83###].\nIn NLP, we can distinguish perturbations based on three main criteria:\nwhere and how the perturbations occur,\nwhether they are algorithmically generated (vs. generated by humans or LLMs) and\nwhether they are adversarial (as opposed to random).\nIn particular, perturbations can occur at the character, word, or sentence level [84  ###reference_b84###, 85  ###reference_b85###, 86  ###reference_b86###] and may involve deletion, insertion, swapping, flipping, substitution with synonyms, concatenation with characters or words, or insertion of numeric or alphanumeric characters [87  ###reference_b87###, 88  ###reference_b88###, 89  ###reference_b89###].\nFor instance, in character level adversarial attacks, Belinkov et al. [90  ###reference_b90###] introduce natural and synthetic noise to input data, while Gao et al. [91  ###reference_b91###] and Li et al. [92  ###reference_b92###] and Li et al. [92  ###reference_b92###] identify crucial words within a sentence and perturb them accordingly. For word level attacks, they can be categorised into gradient-based [87  ###reference_b87###, 93  ###reference_b93###], importance-based [94  ###reference_b94###, 95  ###reference_b95###], and replacement-based [96  ###reference_b96###, 97  ###reference_b97###, 98  ###reference_b98###] strategies, based on the perturbation method employed.\nMoreover, Moradi et al. [99  ###reference_b99###] introduce algorithmic non-adversarial perturbations at both the character and word levels. They utilise a rule-based method to generate these perturbations, simulating various types of noise typically caused by spelling mistakes, typos, and other similar errors.\nIn sentence level adversarial attacks, some perturbations [100  ###reference_b100###, 101  ###reference_b101###] are created so that they do not impact the original label of the input and can be incorporated as a concatenation in the original text. In such scenarios, the expected behaviour from the model is to maintain the original output, and the attack can be deemed successful if the label/output of the model is altered.\nAdditionally, non-algorithmic sentence perturbations can be obtained through prompting language models based methods [21  ###reference_b21###, 14  ###reference_b14###] to generate rephrases of the inputs.\nBy augmenting the training data with these perturbed examples, models are exposed to a more diverse range of linguistic variations and potential adversarial inputs. This helps the models to generalise better and become more robust to different types of adversarial attacks.\nTo help with this task, the NLP community has gathered a dataset of adversarial attacks named AdvGLUE [102  ###reference_b102###], which aims to be a principled and comprehensive benchmark for NLP robustness measurements.\nIn this work we employ a PGD-based adversarial training as the method to enhance the robustness and verifiability of our models against gradient-based adversarial attacks. For non-adversarial perturbations, we create algorithmic perturbations at the character and word level as in Moradi et al. [99  ###reference_b99###] and non-algorithmic perturbations at the sentence level using PolyJuice [21  ###reference_b21###] and Vicuna [14  ###reference_b14###].\nWe thus cover most combinations of the three choices above (bypassing only human-generated adversarial attacks as these do not admit systematic evaluation which is important for this study).\n###table_1### ###table_2###"
        },
        {
            "section_id": "2.4",
            "parent_section_id": "2",
            "section_name": "Previous NLP Verification Approaches",
            "text": "Although DNN verification studies have predominantly focused on computer vision, there is a growing body of research exploring the verification of NLP. This research can be categorised into three main approaches: IBP, abstract interpretation, and randomised smoothing.\nTables 1  ###reference_### and 2  ###reference_### shows a comparison of these approaches.\nTo the best of our knowledge, this paper is the first one to use an SMT-based verifier for this purpose, and compare it with an abstract interpretation-based verifier on the same benchmarks.\nNLP Verification via Interval Bound Propagation.\nThe first technique successfully adapted from the computer vision domain for verifying NLP models was the IBP. In the NLP approaches, IBP is used for both training and verification. Its aim is to minimise the upper bound on the maximum difference between the classification boundary and the input perturbation region by augmenting the loss function.\nThis facilitates the minimisation of the perturbation region in the last layer, ensuring it remains on one side of the classification boundary. As a result, the adversarial region becomes tighter and can be considered certifiably robust.\nNotably, Jia et al. [17  ###reference_b17###] proposed certified robust models on word substitutions in text classification. The authors employed IBP to optimise the upper bound over perturbations, providing an upper bound over the discrete set of perturbations in the word vector space.\nFurthermore, Huang et al. [18  ###reference_b18###] introduced a verification and verifiable training method with a tighter over-approximation in style of the Simplex algorithm [28  ###reference_b28###].\nTo make the network verifiable, they defined the convex hull of all the original unperturbed inputs as a space of perturbations.\nBy employing the IBP algorithm, they generated robustness bounds for each neural network layer.\nLater on, Welbl et al. [103  ###reference_b103###] differentiated from the previous approaches by using IBP to address the under-sensitivity issue. They designed and formally verified the \u2018under-sensitivity specification\u2019 that a model should not become more confident as arbitrary subsets of input words are deleted.\nRecently, Zhang et al. [19  ###reference_b19###] introduced Abstract Recursive Certification (ARC) to verify the robustness of LSTMs. ARC defines a set of programmatically perturbed string transformations to construct a perturbation space. By memorising the hidden states of strings in the perturbation space that share a common prefix, ARC can efficiently calculate an upper bound while avoiding redundant hidden state computations.\nFinally, Wang et al. [104  ###reference_b104###] improved on the work of Jia et al. by introducing Embedding Interval Bound Constraint (EIBC). EIBC is a new loss that constraints the word embeddings in order to tighten the IBP bounds.\nThe strength of IBP-based methods is their efficiency and speed, while their main limitation is the bounds\u2019 looseness, further accentuated if the neural network is deep.\nNLP Verification via Abstract Interpretation.\nAnother popular verification technique applied to various NLP models is based on abstract interpretation.\nOne notable contribution in this area is POPQORN [105  ###reference_b105###], which is the first work that gives robustness guarantees for RNN-based networks. They handle the challenging non-linear activation functions of complicated RNN structures (like LSTMs and GRUs) by bounding them with linear functions.\nLater on, Du et al. improve on POPQORN by introducing Cert-RNN [106  ###reference_b106###], a robust certification framework for RNNs that overcomes the limitations of POPQORN. The framework maintains inter-variable correlation and accelerates the non-linearities of RNNs for practical uses. Cert-RNN utilised Zonotopes [115  ###reference_b115###] to encapsulate input perturbations\nand can verify the properties of the output Zonotopes to determine certifiable robustness.\nThis results in improved precision and tighter bounds, leading to a significant speedup compared to POPQORN.\nIn contrast, Shi et al. [15  ###reference_b15###] focus on transformers with self-attention layers. They developed a verification algorithm that can provide a lower bound to ensure the probability of the correct label is consistently higher than that of the incorrect labels.\nAnalogously, Bonaert et al. [107  ###reference_b107###] propose DeepT, a certification method for large transformers.\nIt is specifically designed to verify the robustness of transformers against synonym replacement-based attacks. DeepT employs multi-norm Zonotopes to achieve larger robustness radii in the certification and can work with networks much larger than Shi et al.\nAbstract interpretation-based methods produce much tighter bounds than IBP-based methods, which can be used with deeper networks. However, they use geometric perturbations () instead of semantic perturbations.\nNLP Verification via Randomised Smoothing.\nRandomised smoothing [116  ###reference_b116###] is another technique for verifying the robustness of deep language models that has recently grown in popularity due to its scalability [108  ###reference_b108###, 109  ###reference_b109###, 110  ###reference_b110###, 111  ###reference_b111###, 112  ###reference_b112###, 113  ###reference_b113###, 114  ###reference_b114###].\nThe idea is to leverage randomness during inference to create a smoothed classifier that is more robust to small perturbations in the input. This technique can also be used to give certified guarantees against adversarial perturbations within a certain radius. Generally, randomized smoothing begins by training a regular neural network on a given dataset.\nDuring the inference phase, to classify a new sample, noise is randomly sampled from the predetermined distribution multiple times. These instances of noise are then injected into the input, resulting in noisy samples. Subsequently, the base classifier generates predictions for each of these noisy samples. The final prediction is determined by the class with the highest frequency of predictions, thereby shaping the smoothed classifier. To certify the robustness of the smoothed classifier against adversarial perturbations within a specific radius centered around the input, randomised smoothing calculates the likelihood of agreement between the base classifier and the smoothed classifier when noise is introduced to the input. If this likelihood exceeds a certain threshold, it indicates the certified robustness of the smoothed classifier within the radius around the input.\nThe main advantage of randomised smoothing-based methods is their scalability, indeed recent approaches are tested on larger transformer such as BERT and Alpaca.\nHowever, their main issue is that they are probabilistic approaches, meaning they give certifications up to a certain probability (e.g., 99.9%).\nIn this work we focus on deterministic approaches, hence we only report these works in Table 2  ###reference_### for completeness without delving deeper into each paper here. All randomised smoothing-based approaches use data augmentation obtained by semantic perturbations."
        },
        {
            "section_id": "2.5",
            "parent_section_id": "2",
            "section_name": "Datasets and Use Cases Used in NLP Verification",
            "text": "Existing NLP verification datasets. Table 3  ###reference_### summarises the main features and tasks of the datasets used in NLP verification.\nDespite their diverse origins and applications, the datasets in the literature are usually binary or multi-class text classification problems. Furthermore, datasets can be sensitive to perturbations, i.e. perturbations can have non-trivial impact on label consistency. For example, Jia et al. [17  ###reference_b17###] use IBP with the SNLI [117  ###reference_b117###]333A semantic inference dataset that labels whether one sentence entails, contradicts or is neutral to another sentence. dataset (see Tables 1  ###reference_### and 3  ###reference_###) to show that\nword perturbations (e.g. \u2018good\u2019 to \u2018best\u2019) can change whether one sentence entails another. Some works such as Jia et al. [17  ###reference_b17###] try to address this label consistency, while others do not.\nAdditionally, we find that the previous research on NLP verification does\nnot utilise safety critical datasets (which strongly motivates the choice of datasets in alternative verification domains), with the exception of Du et al. [106  ###reference_b106###] that use the Toxic Comment dataset [118  ###reference_b118###].\nThese papers do not provide detailed motivation as to why the dataset choices were made, however it could be due to the datasets being commonly used in NLP benchmarks (IMDB etc.).\n###table_3###"
        },
        {
            "section_id": "2.5.1",
            "parent_section_id": "2.5",
            "section_name": "2.5.1 Datasets Proposed in This Paper",
            "text": "In this paper we focus on two datasets from safety-critical applications that have not appeared in the NLP verification literature before. Both are driven by real-world use cases of safety-critical NLP applications, i.e. applications for which law enforcement and safety demand formal guarantees of \u201cgood\u201d DNN behaviour.\nChatbot Disclosure Dataset.\nThe first case study is motivated by new legislation which states that a chatbot must not mislead people about its artificial identity [11  ###reference_b11###, 10  ###reference_b10###]. Given that the regulatory landscape surrounding NLP models (particularly LLMs and generative AI) is rapidly evolving, similar legislation could be widespread in the future \u2013 with recent calls for the US Congress to formalise such disclosure requirements [127  ###reference_b127###]. The prohibition on deceptive conduct act may apply to the outputs generated by NLP systems if used commercially [128  ###reference_b128###], and at minimum a system must guarantee a truthful response when asked about its agency [129  ###reference_b129###, 130  ###reference_b130###]. Furthermore, the burden of this should be placed on the designers of NLP systems, and not on the consumers.\nOur first safety critical case is the R-U-A-Robot dataset [129  ###reference_b129###], a written English dataset consisting of 6800 variations on queries relating to the intent of \u2018Are you a robot?\u2019, such as \u2018I\u2019m a man, what about you?\u2019. The dataset was created via a context-free grammar template, crowd-sourcing and pre-existing data sources. It consists of 2,720 positive examples (where given the query, it is appropriate for the system to state its non-human identity), 3,400 negative/adversarial examples and 680 \u2018ambiguous-if-clarify\u2019 examples (where it is unclear whether the system is required to state its identity). The dataset was created to promote transparency which may be required when the user receives unsolicited phone calls from artificial systems. Given systems like Google Duplex [131  ###reference_b131###], and the criticism it received for human-sounding outputs [132  ###reference_b132###], it is also highly plausible for the user to be deceived regarding the outputs generated by other NLP-based systems [128  ###reference_b128###]. Thus we choose this dataset to understand how to enforce such disclosure requirements. We collapse the positive and ambiguous examples into one label, following the principle of \u2018better be safe than sorry\u2019, i.e. prioritising a high recall system.\nMedical Safety Dataset.\nAnother scenario one might consider is that inappropriate outputs of NLP systems have the potential to cause harm to human users [13  ###reference_b13###]. For example, a system may give a user false impressions of its \u2018expertise\u2019 and generate harmful advice in response to medically related user queries [7  ###reference_b7###]. In practice it may be desirable for the system to avoid answering such queries.\nThus we choose the Medical safety dataset [12  ###reference_b12###], a written English dataset consisting of 2,917 risk-graded medical and non-medical queries (1,417 and 1,500 examples respectively). The dataset was constructed via collecting questions posted on reddit, such as r/AskDocs. The medical queries have been labelled by experts and crowd annotators for both relevance and levels of risk (i.e. non-serious, serious to critical) following established World Economic Forum (WEF) risk levels designated for chatbots in healthcare [133  ###reference_b133###]. We merge the medical queries of different risk-levels into one class, given the high scarcity of the latter 2 labels to create an in-domain/out-of-domain classification task for medical queries. Additionally, we consider only the medical queries that were labelled as such by expert medical practitioners. Thus this dataset will facilitate discussion on how to guarantee a system recognises medical queries, in order to avoid generating medical output.\nAn additional benefit of these two datasets is that they are distinct semantically, i.e. the R-U-A-Robot dataset contains several semantically similar, but lexically different queries, while the medical safety dataset contains semantically diverse queries. For both datasets, we utilise the same data splits as given in the original papers, and refer to the final binary labels as positive and negative. The positive label in the R-U-A-Robot dataset implies a sample where it is appropriate to disclose non-human identity, while in the medical safety dataset it implies an in-domain medical query."
        },
        {
            "section_id": "2.6",
            "parent_section_id": "2",
            "section_name": "Our Work: Parametric Approach to NLP Verification Pipelines",
            "text": "To show relation of our work to the body of already existing work, we distill an \u201cNLP verification pipeline\u201d that is common across many related papers. Figure 2  ###reference_### shows the pipeline diagrammatically. It proceeds in stages:\nGiven an NLP dataset, generate semantic perturbations on sentences that it contains.\nThe semantic perturbations can be of different kinds: character, word or sentence level. IBP and randomised smoothing use word and character perturbations, abstract interpretation papers usually do not use any semantic perturbations.\nOur method allows to use all existing semantic perturbations, in particular, we implement character and word level perturbations as in Moradi et al. [99  ###reference_b99###], sentence level perturbations with PolyJuice [21  ###reference_b21###] and Vicuna.\nEmbed the semantic perturbations into continuous spaces. The cited papers use the word embeddings GloVe [98  ###reference_b98###], we use the sentence embeddings S-BERT and S-GPT.\nWorking on the embedding space, use geometric or semantic perturbations to define geometric or semantic subspaces around perturbed sentences. In IBP papers, semantic subspaces are defined as \u201cbounds\u201d derived from admissible semantic perturbations. In abstract interpretation papers, geometric subspaces are given by -cubes and  around each embedded sentence. Our paper generalises the notion of -cubes by defining \u201chyper-rectangles\u201d on sets of semantic perturbations. The hyper-rectangles generalise -cubes both geometrically and semantically, by allowing to analyse subspaces that are drawn around several (embedded) semantic perturbations of the same sentence. We could adapt our methods to work with hyper-ellipses and thus directly generalise  (the difference boils down to using  norm instead of  when computing geometric proximity of points), however hyper-rectangles are more efficient to compute, which determined our choice of shapes in this paper.\nUse the geometric/semantic subspaces to train a classifier to be robust to change of label within the given subspaces.\nWe generally call such training either robust training or semantically robust training, depending on whether the subspaces it uses are geometric or semantic.\nA custom semantically robust training algorithm\nis used in IBP papers, while abstract interpretation papers usually skip this step or use (adversarial) robust training.\nIn this paper, we adapt the famous PGD algorithm [20  ###reference_b20###] that was initially defined for geometric subspaces () to work with semantic subspaces (hyper-rectangles) to obtain a novel semantic training algorithm.\nUse the geometric/semantic subspaces to verify the classifier\u2019s behaviour within those subspaces. The papers [17  ###reference_b17###, 18  ###reference_b18###, 103  ###reference_b103###, 19  ###reference_b19###, 104  ###reference_b104###] use IBP algorithms and the papers  [105  ###reference_b105###, 15  ###reference_b15###, 106  ###reference_b106###, 107  ###reference_b107###] use abstract interpretation; in both cases it is incomplete and deterministic verification. We use SMT-based tool Marabou (complete and deterministic) and abstract-interpretation tool ERAN (incomplete and deterministic).\nTable 1  ###reference_### summarises differences and similarities of the above NLP verification approaches against ours.\nTo the best of our knowledge, we are the first to use SMT-based complete methods in NLP verification and we show how they achieve higher verifiability than abstract interpretation-based verification approaches, thanks to the increased precision of the ReLUplex algorithm proof search relative to bound propagation.\nFurthermore, our study is the first to demonstrate that the construction of semantic subspaces can happen independently of the choice of the training and verification algorithms. Likewise, although training and verification build upon the defined (semantic) subspaces, the actual choice of the training and verification algorithms can be made independently of the method used to define the semantic subspaces.\nThis separation, and the general modularity of our approach, facilitates a comprehensive examination and comparison of the two key components involved in any NLP verification process:\neffects of the verifiability-generalisability trade-off for verification with geometric and semantic subspaces;\nrelation between the volume/shape of semantic subspaces and verifiability of neural networks obtained via semantic training with these subspaces.\nThese two aspects have not been considered in the literature before."
        },
        {
            "section_id": "3",
            "parent_section_id": null,
            "section_name": "The Parametric NLP Verification Pipeline",
            "text": "This section presents a parametric NLP verification pipeline, shown in Figure 2  ###reference_### diagrammatically. We call it \u201cparametric\u201d because each component within the pipeline operates independently of the others and\ncan be taken as a parameter when studying other components.\nThe parametric nature of the pipeline allows for the seamless integration of state-of-the-art methods at every stage, and for more sophisticated experiments with those methods.\nNote that the outlined pipeline can be seen as a filter which can be applied on top of an NLP system or LLM (such as S-BERT and S-GPT) to certify intended DNN behavior for safety-critical input queries.\nThe following section provides a detailed exposition of the methodological choices made at each step of the pipeline.\n###figure_5###"
        },
        {
            "section_id": "3.1",
            "parent_section_id": "3",
            "section_name": "Semantic Perturbations",
            "text": "As discussed in Section 2.6  ###reference_###, we require semantic perturbations for creating semantic subspaces.\nTo do so, we consider three kinds of perturbations\n\u2013 i.e. character, word and sentence level. This systematically accounts for different variations of the samples.\nCharacter and word level perturbations are created via a rule-based method proposed by Moradi et al. [99  ###reference_b99###]\nto simulate different kinds of noise one could expect from spelling mistakes, typos etc. These perturbations are non-adversarial and can be generated automatically. Moradi et al. [99  ###reference_b99###]\nfound that NLP models are sensitive to such small errors, while in practice this should not be the case.\nCharacter level perturbations types include randomly inserting, deleting, replacing, swapping or repeating a character of the data sample. At the character level, we do not apply letter case changing, given it does not change the sentence-level representation of the sample.\nNor do we apply perturbations to commonly misspelled words, given only a small percentage of the most commonly misspelled words occur in our datasets.\nPerturbations types at the word level include randomly repeating or deleting a word, changing the ordering of the words, the verb tense, singular verbs to plural verbs or adding negation to the data sample. At the word level, we omit replacement with synonyms, as this is accounted for via sentence rephrasing. Negation is not done on the medical safety dataset, as it creates label ambiguities (e.g. \u2018pain when straightening knee\u2019  \u2018no pain when straightening knee\u2019), as well as singular plural tense and verb tense, given human annotators would experience difficulties with this task (e.g. rephrase the following in plural/ with changed tense \u2013 \u2018peritonsillar abscess drainage aftercare.. please help\u2019).\nFurther examples of character and word rule-based perturbations can be found in Tables 4  ###reference_### and 5  ###reference_###.\n###table_4### ###table_5### Sentence level perturbations. We experiment with two types of sentence level perturbations,\nparticularly due to the complicated nature of the medical queries (e.g. it is non-trivial to rephrase queries such as this \u2013 \u2018peritonsillar abscess drainage aftercare.. please help\u2019). We do so by either using Polyjuice [21  ###reference_b21###] or vicuna-13b444Using the following API: https://replicate.com/replicate/vicuna-13b/api  ###reference_/api###..\nPolyjuice is a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by fine-tuning GPT-2 on multiple datasets of paired sentences.\nVicuna is a state-of-the-art open source chatbot trained by fine-tuning LLaMA [134  ###reference_b134###] on user-shared conversations collected from ShareGPT 555https://sharegpt.com/  ###reference_sharegpt.com/###.\nFor Vicuna, we use the following prompt to generate variations on our data samples \u2018Rephrase this sentence 5 times: \u201c[Example]\u201d.\u2019\nFor example, from the sentence \u201cHow long will I be contagious?\u201d,\nwe can obtain \u201cHow many years will I be contagious?\u201d or \u201cWill I be contagious for long?\u201d and so on.\nWe will use notation  to refer to a perturbation algorithm abstractly."
        },
        {
            "section_id": "3.2",
            "parent_section_id": "3",
            "section_name": "NLP Embeddings",
            "text": "The next component of the pipeline is the embeddings. Embeddings play a crucial role in NLP verification as they map textual data into continuous vector spaces,\nin a way that should capture semantic relationships and contextual information.\nGiven the set of all strings, , an NLP dataset  is a set of sentences  written in natural language. The embedding  is a function that maps a string in  to a vector in m.\nThe vector space m is called the embedding space.\nIdeally,  should reflect the semantic similarities between sentences in , i.e. the more semantically similar two sentences  and  are, the closer the distance between  and  should be in m. Of course, defining semantic similarity in precise terms may not be tractable (the number of unseen sentences may be infinite, the similarity may be subjective and/or depend on the context). This is why, the state-of-the-art NLP relies on machine learning methods to capture the notion of semantic similarity approximately.\nCurrently, the most common approach to obtain an embedding function  is by training transformers [135  ###reference_b135###, 136  ###reference_b136###].\nTransformers are a type of DNNs that can be trained to map sequential data into real vector spaces and are capable of handling variable-length input sequences. They can also be used for other tasks, such as classification or sentence generation, but in those cases, too, training happens at the level of embedding spaces. In this work, a transformer is trained as a function  for some given .\nThe key feature of the transformer is the \u201cself-attention mechanism\u201d, which allows the network to weigh the importance of different elements in the input sequence when making predictions, rather than relying solely on the order of elements in the sequence. This makes them good at learning to associate semantically similar words or sentences. In this work we initially use Sentence-BERT [136  ###reference_b136###] and later add Sentence-GPT [137  ###reference_b137###] to embed sentences.\nUnfortunately, the relation between the embedding space and the NLP dataset is not bijective: i.e. each sentence is mapped into the embedding space, but not every point in the embedding space has a corresponding sentence. This problem is well-known in NLP literature [138  ###reference_b138###] and, as shown in this paper, is one of the reasons why verification of NLP\nis tricky.\nGiven an NLP dataset  that should be classified into  classes, the standard approach is to construct a function  that maps the embedded inputs to the classes. In order to do that, a domain specific classifier  is trained on the embeddings  and the final system will then be the composition of the two subsystems, i.e. ."
        },
        {
            "section_id": "3.3",
            "parent_section_id": "3",
            "section_name": "Geometric Analysis of Embedding Spaces",
            "text": "We now formally define geometric and semantic subspaces of the embedding space. Our goal is to define subspaces on the embedding space m by using an effective algorithmic procedure.\nWe will use notation  to refer to a subspace of the embedding space.\nA hyper-rectangle of dimension  is a list of points  such that a point  is a member if for every dimension  we have .\nWe start with an observation that, given an NLP dataset  that contains a finite set of sentences  belonging to the same class, and an embedding function , we can define an embedding matrix , where each row  is given by . We will use the notation  to refer to the th element of the vector , and  to refer to the element in the th row and th column of . Treating embedded sentences as matrices, rather than as points in the real vector space, makes many computations easier.\nWe can therefore define a hyper-rectangle for  as follows.\nGiven an embedding matrix , the -dimensional hyper-rectangle for  is defined as:\nTherefore given an embedding function , and a set of sentences , we can form a subspace  by constructing the embedding matrix, as described above, and forming the corresponding hyper-rectangle. To simplify notation, we will omit the application of  and from here on simply write .\nThe next example shows how the above definitions generalise the commonly known definition of the .\nOne of the most popular terms used in robust training [67  ###reference_b67###] and verification [66  ###reference_b66###] literature is\nthe . It is defined as follows.\nGiven an embedded input ,\na constant , and a distance function (L-norm) , the  around  of radius  is defined as:\nIn practice, it is common to use the  norm, which results in the  actually being a hyper-rectangle, also called , where .\nTherefore our construction  is a strict generalisation of .\nWe will therefore use the notation  to refer to the set of  around every sentence in the dataset.\nOf course, as we have already discussed in the introduction and Figure 1  ###reference_###, hyper-rectangles are not very precise, geometrically. A more precise shape would be a convex hull around  given points in the embedding space. Indeed literature has some definitions of convex hulls [139  ###reference_b139###, 140  ###reference_b140###, 141  ###reference_b141###]. However, none of them is suitable as they are computationally too expensive due to the time complexity of  where  is the number of inputs and  is the number of dimensions [139  ###reference_b139###]. Approaches that use under-approximations to speed up the algorithms [140  ###reference_b140###, 141  ###reference_b141###] do not work well in NLP scenarios, as under-approximated subspaces are so small that they contain near zero sentence embeddings."
        },
        {
            "section_id": "3.3.1",
            "parent_section_id": "3.3",
            "section_name": "3.3.1 Exclusion of Unwanted Sentences Via Shrinking",
            "text": "Another concern is that the generated hyper-rectangles may contain sentences from a different class.\nThis would make it unsuitable for verification.\nIn order to exclude all samples from the wrong class, we define a shrinking algorithm  that calculates a new subspace that is a subset of the original hyper-rectangle around , that only contains embeddings of sentences in  that are of class . Of course, to ensure this, the algorithm may have to exclude some sentences of class . The second graph of Figure 3  ###reference_### gives a visual intuition of how this is done.\n###figure_6### ###figure_7### ###figure_8### Formally, for each sentence  in  that is not of class , the algorithm performs the following procedure.\nIf  lies in the current hyper-rectangle , then for each dimension  we compute the distance whether  is closer to  or . Without loss of generality, assume  is closer.\nWe then compute the number of sentences of class  that would be excluded by replacing  with  in the hyper-rectangle where  is a small positive number (we use ). This gives us a penalty for each dimension , and we exclude  by updating the hyper-rectangle in the dimension that minimises this penalty.\nThe idea is to shrink the hyper-rectangle in the dimensions that exclude as few embedded sentences from the desired class  as possible666Note that this algorithm shrinks exactly one dimension by a minimal amount to exclude the unwanted embedded sentence. This choice keeps the algorithm fast while guaranteeing the subspace to retain the highest number of wanted inputs. However, it is not necessarily the best choice for verification: there might be cases where perturbations of the unwanted input are left inside after shrinking and, if the network classifies them correctly, the subspace can never be verified. For large subspaces, our algorithm might render verification unachievable and more clever algorithms should be explored and discussed.."
        },
        {
            "section_id": "3.3.2",
            "parent_section_id": "3.3",
            "section_name": "3.3.2 Exclusion of Unwanted Sentences Via Clustering",
            "text": "An alternative approach to excluding unwanted sentences, is to split the dataset up by clustering semantically similar sentences in the embedding space, and then computing the hyper-rectangles around each cluster individually, as shown in the last graph of Figure 3  ###reference_###.\nIn this paper we will use the k-means algorithm as a clustering. We will use the notation  to refer to the -clusters formed by applying it to dataset .\nWhile in our experiments we have found this is often sufficient to exclude unwanted sentences, it is not guaranteed to do so. Therefore, this method is combined with the shrinking algorithm in our experiments."
        },
        {
            "section_id": "3.3.3",
            "parent_section_id": "3.3",
            "section_name": "3.3.3 Eigenspace Rotation",
            "text": "A final alternative and computationally efficient way of reducing the likelihood that the hyper-rectangles will contain embedded sentences of an unwanted class, is to rotate them to better align to the distribution of the embedded sentences of the desired class in the embedding space. This motivates us to introduce the Eigenspace rotation.\nTo construct the tightest possible hyper-rectangle, we define a specific method of eigenspace rotation.\nAs shown in Figure 1  ###reference_### (C and D), our approach is to calculate a rotation matrix  such that the rotated matrix  is better aligned with the axes than , and therefore  has a smaller volume.\nBy a slight abuse of terminology, we will refer to  as the rotated hyper-rectangle, even though strictly speaking, we are rotating the data, not the hyper-rectangle itself.\nIn order to calculate the rotation matrix , we use singular value decomposition [142  ###reference_b142###].\nThe singular value decomposition of  is defined as , where  is a matrix of left-singular vectors,  is a matrix of singular values and  is a matrix of right-singular vectors and  denotes the conjugate transpose.\nIntuitively, the right-singular vectors  describe the\ndirections in which  exhibits the most variance. The main idea behind the definition of rotation is to align these directions of maximum variance with the standard canonical basis vectors.\nFormally, using , we can compute the rotation (or change-of-basis) matrix  that rotates the right-singular vectors onto the canonical standard basis vectors , where  is the identity matrix.\nTo do this, we observe that .\nWe thus obtain  as desired.\nAll hyper-rectangles constructed in this paper are rotated."
        },
        {
            "section_id": "3.3.4",
            "parent_section_id": "3.3",
            "section_name": "3.3.4 Geometric and Semantic Subspaces",
            "text": "We now apply the abstract definition of a subspace of an embedding space to concrete NLP verification scenarios. Once we know how to define subspaces for a selection of points in the embedding space, the choice remains how to choose those points. The first option is to use  around given embedded points, as Example 1  ###reference_mple1### defines. Since this construction does not involve any knowledge about the semantics of sentences, we will call the resulting subspaces geometric subspaces.\nThe second choice is to apply semantic perturbations to a point in , embed the resulting sentences, and then define a subspace around them. We will call the subspaces obtained by this method semantic perturbation subspaces, or just semantic subspaces for short.\nWe will finish this section with defining semantic subspaces formally.\nWe will use  to denote an algorithm for generating sentence perturbations of type , applied to an input sentence  in a random position.\nIn the later sections, we will use  to refer to the different types of perturbations illustrated in Tables 4  ###reference_### and 5  ###reference_###, e.g. character-level insertion, deletion, replacement.\nIntuitively, given a single sentence we want to generate a set of semantically similar perturbations and then construct a hyper-rectangle around them, as described in Definition 1  ###reference_###.\nThis motivates the following definitions.\nGiven a sentence , a number , and a type , the set  is the set of  semantic perturbations of type  generated from .\nWe will use the notation  to denote the new dataset generated by creating  semantic perturbations of type  around each sentence.\nGiven an embedding function , the semantic subspace for a sentence  is the subspace . We will refer to a set of such semantic hyper-rectangles over an entire dataset  as .\nTo illustrate this construction, let us consider the sentence : \u201cCan u tell me if you are a chatbot?\u201d. This sentence is one of  original sentences of the positive class in the dataset. From this single sentence, we can create six new sentences using the word-level perturbations from Table 5  ###reference_### to form . Once the seven sentences are embedded into the vector space, they form the hyper-rectangle . By repeating this construction for the remaining  sentences, we obtain the set of hyper-rectangles  for the dataset.\nGiven a sentence , we embed each sentence in  into m obtaining vectors  where ."
        },
        {
            "section_id": "3.3.5",
            "parent_section_id": "3.3",
            "section_name": "3.3.5 Measuring the Quality of Sentence Embeddings",
            "text": "One of our implicit assumptions in the previous sections, is that the embedding function  maps pairs of semantically similar sentences to nearby points in the embedding space.\nIn Section 5.5.2  ###reference_.SSS2###, we will evaluate the accuracy of this assumption using cosine similarity.\nThis metric measures how similar two vectors are in a multi-dimensional space by calculating the cosine of the angle between them:\nwhere  is the dot product and .\nThe resulting value ranges from  to . A value of  indicates that the vectors are parallel (highest similarity), while  means that the vectors are orthogonal (no similarity)."
        },
        {
            "section_id": "3.4",
            "parent_section_id": "3",
            "section_name": "Training",
            "text": "As outlined in Section 2.2  ###reference_###, robust training is essential for bolstering the robustness of DNNs; without it, their verifiability would be significantly diminished. This study employs two robust training methods, namely data augmentation and a custom PGD adversarial training, with the goal of discerning the factors contributing to the success of robust training and compare the effectiveness of these methods.\n\nAdversarial Training. In this training method, the traditional Projected Gradient Descent (PGD) algorithm [20  ###reference_b20###],\nis defined as follows.\nGiven a loss function , a step size  and a starting point  then the output of the PGD algorithm  after  iterations is defined as:\nwhere  is the projection back into the desired subspace .\nIn its standard formulation, the subspace \nis often an  (for some chosen ).\nIn this work, we modify the algorithm to work with custom-defined hyper-rectangles as the subspace.\nThe primary distinction between our customised PGD algorithm and the standard version lies in the definition of the step size. In the conventional algorithm, the step size is represented by a scalar  therefore representing a uniform step size in every dimension.\nIn our case the width of  in each dimension may vary greatly, therefore we transforms  into a vector in m, allowing the step size to vary by dimension.\nNote that the dot  between  and  becomes an element-wise multiplication.\nThe resulting customised PGD training seeks to identify the worst perturbations within the custom-defined subspace, and trains the given neural network to classify those perturbations correctly, in order to make the network robust to adversarial inputs in the chosen subspace."
        },
        {
            "section_id": "3.5",
            "parent_section_id": "3",
            "section_name": "Choice of Verification Algorithm",
            "text": "As stated earlier, our approach in this study involves the utilization of cutting-edge tools for DNN verification. Initially, we employ ERAN [143  ###reference_b143###], a state-of-the-art abstract interpretation-based method. This choice is made over IBP due to its ability to yield tighter bounds. Subsequently, we conduct comparisons and integrate Marabou [22  ###reference_b22###], a state-of-the-art complete verifier. This enables us to attain the highest verification percentage, maximizing the tightness of the bounds.\nWe will use notation  to refer to a verifier abstractly."
        },
        {
            "section_id": "4",
            "parent_section_id": null,
            "section_name": "Characterisation of Verifiable Subspaces",
            "text": "In this Section, we\nprovide key results in support of Contribution 1 formulated in the introduction:\nWe start with introducing the metric of generalisability of (verified) subspaces and set-up some baseline experiments.\nWe introduce the problem of the verifiability-generalisability trade-off in the context of geometric subspaces.\nWe show that, compared to geometric subspaces, the use of semantic subspaces helps to find a better balance between generalisability and verifiability.\nFinally, we show that adversarial training based on semantic subspaces results in DNNs that are both more verifiable and more generalisable than those obtained with other forms of robust training."
        },
        {
            "section_id": "4.1",
            "parent_section_id": "4",
            "section_name": "Metrics for Understanding the Properties of Embedding Spaces",
            "text": "Let us start with recalling the existing standard metrics used in DNN verification. Recall that we are given an NLP dataset , moreover we assume that each  is assigned a correct class from . We restrict to the case of binary classification in this paper for simplicity, so we will assume .\nFurthermore, we are given an embedding function , and a network .\nUsually  corresponds to the number of classes, and thus\nin case of binary classification, we have .\nAn embedded sentence  is classified as class  if the value of  in  is higher than all other classes.\nAccuracy. The most popular metric for measuring the performance of the network is the accuracy of , which is measured as a percentage of sentences in  that are assigned to a correct class by .\nNote that this metric only checks a finite number of points in m given by the dataset.\nVerifiability. A verifier  takes a network , a subspace  and its designated class  as an input, and outputs 1 if it can prove that  assigns all points in the subspace  to the class  and 0 otherwise. Consider a verification problem with multiple subspaces , where all the points in each subspace should be assigned to a specific class . In the literature, the most popular metric to measure success rate of the given verifier on  is verifiability:\nGiven a set of subspaces  each assigned to classes , then the verifiability is the percentage of such subspaces successfully verified:\nAll DNN verification papers that study such problems report this measure.\nNote that each subspace contains an infinite number of points.\nHowever, suppose we have a subspace  that verifiably consists only of vectors that are assigned to a class  by .\nBecause of the embedding gap, it is difficult to calculate how many valid unseen sentences outside of  will be mapped into  by , and therefore how much utility there is in verifying .\nIn an extreme case it is possible to have 100% verifiability and yet the verified subspaces will not contain any unseen sentences.\nGeneralisability.\nTherefore, we now introduce a third metric, generalisability, which is a heuristic for the number of semantically-similar unseen sentences captured by a given set of subspaces.\nGiven a set of subspaces  and a target set of embeddings  the generalisability of the subspaces is measured as the percentage of the embedded vectors that lie in the subspaces:\nIn this paper we will generate the target set of embeddings  as  where  is a dataset,  is the type of semantic perturbation,  is the number of perturbations and  is the embeddings of the set of semantic perturbations  around  generated using , as described in Section 3.3  ###reference_###.\nNote that  can be given by a collection of different perturbation algorithms and their kinds. The key assumption is that  contains valid sentences semantically similar to  and belonging to the same class.\nAssuming that membership of  is easy to compute, then this metric is also easy to compute as the set  is finite and of size , and therefore so is .\nNote that, unlike accuracy and verifiability, the generalisability metric does not explicitly depend on any DNN or verifier.\nHowever, in this paper we only study generalisability of verifiable subspaces, and thus the existence of a verified network  will be assumed.\nFurthermore, the verified subspaces we study in this paper will be constructed from the dataset via the methodology described in Definition 2  ###reference_inition2###."
        },
        {
            "section_id": "4.2",
            "parent_section_id": "4",
            "section_name": "Baseline Experiments for Understanding the Properties of Embedding Spaces",
            "text": "The methodology defined thus far has given basic intuitions about the modular nature of the NLP verification pipeline.\nBearing this in mind, it is important to start our analysis with the general study of basic properties of the embedding subspaces, which is our main interest in this paper, and suitable baselines.\nBenchmark datasets will be abbreviated as \u201cRUAR\u201d and \u201cMedical\u201d.\nWe use  to refer to the set of sentences in the training dataset with a positive class (i.e. a question asking the identity of the model, and a medical query respectively), and  to refer to the remaining sentences.\nFor a benchmark network , we train a medium-sized fully-connected DNN (with 2 layers of size (128, 2) and input size 30) using stochastic gradient descent and cross-entropy loss.\nThe main requirement for a benchmark network is its sufficient accuracy, see Table 6  ###reference_###.\n###table_6### For the choice of benchmark subspaces, we use the following two extreme sets of geometric subspaces:\nthe singleton set containing the maximal subspace  around all embedded sentences of the positive class  in . This is the largest subspace constructable with our methods, but we should assume that verifiability of such a subspace would be near . It is illustrated in the first graph of Figure 3  ###reference_###.\nthe set of minimal subspaces  given by  around each embedded sentence of class  in , where  is chosen to be sufficiently small to give very high verifiability. This is illustrated in the first graph of Figure 1  ###reference_###.\nWe first seek to understand the geometric properties (e.g. volume,  values) and verifiability figures for these two extremes."
        },
        {
            "section_id": "4.3",
            "parent_section_id": "4",
            "section_name": "Verifiability-Generalisability Trade-off for Geometric Subspaces",
            "text": "The number and average volume of the hyper-rectangles that will make up our verified subspaces are shown in Table 7  ###reference_###.\nGenerally, we use the following naming convention for our experiments:  denotes a hyper-rectangle obtained using a method .\nFor example, RUAR dataset contains  sentences of the positive class, and therefore the experiment  consisting of generating hyper-cubes around each positive sentence results in  hyper-cubes.\nUsing clustering, we obtain a set of , , ,  clusters denoted as  \u2013  and using the shrinking algorithm we obtain .\nNotice the consistent reduction of volume in Table 7  ###reference_###, from  to  -  and ultimately to .\nThere are several orders of magnitude between the largest and the smallest subspace.\n###table_7###"
        },
        {
            "section_id": "4.3.1",
            "parent_section_id": "4.3",
            "section_name": "4.3.1 Verifiability of Geometric Subspaces",
            "text": "Next, we pass each set of hyper-rectangles and the given network to the ERAN verifier and measure verifiability.\nTable 8  ###reference_### shows that, as expected, the shrunk hyper-rectangle  achieves 0% verifiability,\nand the various clustered hyper-rectangles (,  , ) achieve at most negligible verifiability.\nIn contrast, the baseline  achieves up to  verifiability.\nThis suggests that  is a good benchmark for a different extreme.\nTable 7  ###reference_### can give us an intuition of why  has notably higher verifiability than the other hyper-rectangles: the volume of  is several orders of magnitude smaller. We call this effect low verifiability of the high-volume subspaces.\n###table_8### Tables 7  ###reference_### and 8  ###reference_### suggest that smaller subspaces are more verifiable. One may also conjecture that they are less generalisable (as they will contain fewer embedded sentences). We now will confirm this via experiments; we are particularly interested in understanding how quickly generalisability deteriorates as verifiability increases."
        },
        {
            "section_id": "4.3.2",
            "parent_section_id": "4.3",
            "section_name": "4.3.2 Generalisability of Geometric Subspaces",
            "text": "To test generalisability, we algorithmically generate a new dataset  containing its semantic perturbations, using the method described in Section 3.1  ###reference_###.\nThe choice to use only positive sentences is motivated by the nature of the chosen datasets - both Medical and RUAR sentences split into:\na positive class, that contains sentences with one intended semantic meaning (they are medical queries, or they are questions about robot identity); and\na negative class that represents \u201call other sentences\u201d. These \u201cother sentences\u201d are not grouped by any specific semantic meaning and therefore do not form one coherent semantic category.\nHowever Section 5  ###reference_### will make use of  in the context of falsifiability of verified subspaces.\nFor the perturbation type , in this experiment\nwe take a combination of the different perturbations algorithms described in Section 3.1  ###reference_###.\nFor RUAR,  character insertion, character deletion, character replacement, character swapping,\ncharacter repetition, word deletion, word repetition, word negation, word singular/plural verbs, word order,\nword tense . For the Medical dataset,  character insertion, character deletion, character replacement, character swapping, character repetition, word deletion, word repetition, word negation, word singular/plural verbs, word order, word tense, sentence polyjuice .\nEach type of perturbation is applied 4 times on the given sentence in random places.\nThe resulting datasets of semantically perturbed sentences are therefore approximately two orders of magnitude larger than the original datasets (see Table 9  ###reference_###), and contain unseen sentences of similar semantic meaning to the ones present in the original datasets  and .\n###table_9### Table 9  ###reference_###\nshows that the most verifiable subspace  is the least generalisable. This means  may not contain any valid new sentences apart from the one for which it was formed! At the same time,  has up to  of generalisability at the expense of only up to  of verifiability (cf. Table 8  ###reference_###). The effect of the generalisability vs verifiability trade-off can thus be rather severe for geometric subspaces.\nThis experiment demonstrates the importance of using the generalisability metric: if one only took into account the verifiability of the subspaces one would choose , obtaining mathematically sound but pragmatically useless results.\nWe argue that this is a strong argument for including generalisability as a standard metric in reporting NLP verification results in the future."
        },
        {
            "section_id": "4.4",
            "parent_section_id": "4",
            "section_name": "Verifiability-Generalisability Trade-off for Semantic Subspaces",
            "text": "The previous subsection has shown that the verifiability-generalisability trade-off is not resolvable by geometric manipulations alone. In this section we argue that using semantic subspaces can help to improve the effects of the trade-off. The main hypothesis that we are testing is: semantic subspaces constructed using semantic-preserving perturbations are more precise, and this in turn improves both verifiability and generalisability.\nWe will use the construction given in Definition 2  ###reference_inition2###.\nAs Table 10  ###reference_### illustrates, we construct several semantic hyper-rectangles on sentences of the positive class using character-level (, , , , , ), word-level () and sentence-level perturbations ().\nThe subscripts char and word refer to the kind of perturbation algorithm, while del., ins., rep., repl., swap. and pj refer to the type of perturbation, where pj stands for Polyjuice (see Section 3.1  ###reference_###).\nNotice comparable volumes of all these shapes, and compare with .\n###table_10###"
        },
        {
            "section_id": "4.4.1",
            "parent_section_id": "4.4",
            "section_name": "4.4.1 Verifiability of Semantic Subspaces",
            "text": "We pass each set of hyper-rectangles and the network  to the verifiers ERAN and Marabou to measure\nverifiability of the subspaces.\nTable 11  ###reference_### illustrates the verification results obtained using ERAN.\nFrom the table,\nwe can infer that the verifiability of our semantic hyper-rectangles is indeed higher than that of the geometrically-defined hyper-rectangles (Table 8  ###reference_###). Furthermore, our semantic hyper-rectangles, while unable to reach the verifiability of , achieve notable higher verification than its counterpart of comparable volume .\nFrom this experiment, we conclude that not only volume, but also precision of the subspaces has an impact on their verifiability.\n###table_11### Following these results, Table 12  ###reference_### reports the verification results using Marabou instead of ERAN. As shown, Marabou is able to verify up to  (), while ERAN achieves at most . This shows that complete verification (Marabou) outperforms abstract interpretation (ERAN).\nOverall, the Marabou experiment confirms the trends of improved verifiability shown by ERAN and thus confirms our hypothesis about importance of shape precision.\n###table_12###"
        },
        {
            "section_id": "4.4.2",
            "parent_section_id": "4.4",
            "section_name": "4.4.2 Generalisability of Semantic Subspaces",
            "text": "It remains to establish whether the more verifiable semantic subspaces are also more generalisable.\nWhereas Table 9  ###reference_### compared the generalisability of  and  with that of , Table 13  ###reference_### compares their generalisability to the most verifiable semantic subspaces,  and .\nIt shows that these semantic subspaces are also the most generalisable, containing, respectively,  and  of the unseen sentences.\nWe thus infer that using semantic subspaces is effective for bridging the verifiability-generalisability gap, with precise subspaces performing somewhat better than  of the same volume; however both beating the smallest\n from Section 4.2  ###reference_### of comparable verifiability.\nBearing in mind that the verified hyper-rectangles only cover a tiny fraction of the embedding space, the fact that they contain up to  of randomly generated new sentences is an encouraging result, the likes of which have not been reported before.\n###table_13###"
        },
        {
            "section_id": "4.5",
            "parent_section_id": "4",
            "section_name": "Adversarial Training on Semantic Subspaces",
            "text": "In this section, we study the effects that adversarial\ntraining methods have on the verifiability of the previously defined subspaces in Tables 7  ###reference_### and 10  ###reference_###.\nBy comparing the effectiveness of the different training approaches described in Section 3.4  ###reference_###, we show in this section that adversarial training based on our new semantic subspaces is the most efficient.\nThree kinds of training are deployed in this section:\nNo robustness training - The baseline network is  from the previous experiments, which has not undergone any robustness training.\nData augmentation. We obtain three augmented datasets ,  and  where  is defined in Section 4.4  ###reference_###.\nThe subscripts char and word denote the type of perturbation as detailed in Tables 4  ###reference_### and 5  ###reference_###, while the subscript pj refers to the sentence level perturbations generated with Polyjuice.\nWe train the baseline architecture, using the standard stochastic gradient descent and cross entropy loss, on the augmented datasets, and obtain DNNs ,  and .\nPGD adversarial training with geometric and semantic hyper-rectangles. Instead of using the standard  as the PGD subspace , we use the various hyper-rectangles defined in Tables 7  ###reference_### & 10  ###reference_###.\nWe refer to a network trained with the PGD algorithm on the hyper-rectangle associated with experiment  as .\nFor example, for the previous experiment , we obtain the network  by adversarially training the benchmark architecture on the associated subspace .\nSee Tables 14  ###reference_### & 17  ###reference_### for full listing of the networks we obtain in this way. We call DNNs of second and third type robustly trained networks.\nWe keep the geometric and semantic subspaces from the previous experiments (shown in Table 10  ###reference_###) to compare how training affects their verifiability.\nFollowing the same evaluation methodology of experiments as in Sections 4.2  ###reference_### and 4.4.1  ###reference_.SSS1###, we use the verifiers ERAN and Marabou to measure verifiability of the subspaces.\nTable 14  ###reference_### reports accuracy of the robustly trained networks, while the verification results are presented in Tables 15  ###reference_### and 16  ###reference_###.\nFrom Table 14  ###reference_### we can see that networks trained with data augmentation achieve similar nominal accuracy to networks trained with adversarial training.\nHowever, the most prominent difference is exposed in Tables 15  ###reference_### and 16  ###reference_###: adversarial training effectively improves the verifiability of the networks, while data augmentation actually decreases it.\nSpecifically, the adversarially trained networks trained on semantic subspaces (, , ) achieved high verifiability, reaching up to  for RUAR and up to  for the Medical dataset. This constitutes a significant improvement of the verifiability results compared to . Looking at nuances, there does not seem to be a single winner subspace when it comes to adversarial training, and indeed in some cases  wins over more precise subspaces.\nAll of the subspaces in Table 10  ###reference_### have very similar volume, which accounts for improved performance across all experiments. The particular peaks in performance then come down to particularities of a specific semantic attack that was used while training.\nFor example, the best performing networks are those trained with Polyjuice attack, the strongest form of attack in our range. Thus, if the kind of attack is known in advance, the precision of hyper-rectangles can be further tuned.\n###table_14### ###table_15### ###table_16### ###table_17### ###table_18### As a final note, we report results from robust training using the subspaces from Section 4.2  ###reference_### in Table 7  ###reference_###.\nTable 17  ###reference_### reports the accuracy and the details of the robustly trained networks on those subspaces, while the verification results are presented in Table 18  ###reference_###.\nThese tables further demonstrate the importance of volume, and show that subspaces that are\ntoo big still achieve negligible verifiability even after adversarial training.\nGeneralisability of the shapes used in Tables 14  ###reference_### - 18  ###reference_### remains the same, see Tables 9  ###reference_###, 13  ###reference_###."
        },
        {
            "section_id": "5",
            "parent_section_id": null,
            "section_name": "NLP Case Studies",
            "text": "The purpose of this section is two-fold. Firstly, the case studies we present here apply the NLP Verification Pipeline set out in Section 2.6  ###reference_### using more modern NLP tools.\nNotably, in this section we try different LLMs to embed sentences and replace Polyjuice with the LLM vicuna-13b777Using the following API: https://replicate.com/replicate/vicuna-13b/api  ###reference_/api###., a state-of-the-art open source chatbot trained by fine-tuning LLaMA [134  ###reference_b134###] on user-shared conversations collected from ShareGPT 888https://sharegpt.com/  ###reference_sharegpt.com/###. For further details, please refer to Section 3.1  ###reference_###.\nIn order to be able to easily vary the different components of the NLP Verification pipeline, we use the tool ANTONIO [16  ###reference_b16###], shown in Figure 4  ###reference_###.\n###figure_9### Secondly, and perhaps more fundamentally, we draw attention to the fact that the correctness of the specification (i.e. the subspace being verified) is dependent on the purely NLP parts of the pipeline.\nIn particular, the parts that generate, perturb, and embed sentences.\nTherefore, the probability of the specification itself being wrong is higher than in many other areas of verification.\nThis aspect is largely ignored in NLP verification papers and, in this section, we show that using standard NLP methods may result in incorrect specifications and therefore compromising the practical value of the NLP verification pipelines.\nImagine a scenario where a DNN was verified on subspaces of a class  and then used to classify new, unseen sentences. There are two key assumptions that affect the correctness of the generated specifications:\nLocality of the Embedding Function - We have been using the implicit assumption that the embedding function maps semantically similar sentences to nearby points in the embedding space and dissimilar sentences to faraway points. If this assumption fails, the verified subspace may also contain the embeddings of unseen sentences that actually belong to a different class .\nSentence Perturbation Algorithm Preserves Semantics - Another assumption that most NLP verification papers make is that we can algorithmically generate sentence perturbations in a way that is guaranteed to retain their original semantic meaning.\nAll semantic subspaces of Section 4  ###reference_### are defined based on the implicit assumption that all perturbed sentences retain the same class as the original sentence!\nBut if this assumption fails, we will once again end up constructing semantic subspaces around embeddings of sentences belonging to different classes.\nGiven that it is plausible that one or both of these assumptions may fail, it is therefore wrong to assure the user that the fact that we have verified the subspace, guarantees that all sentences that embed into it, actually belong to  (even if the DNN is guaranteed to classify them as )!\nIn fact we will say that new sentences of class  that fall inside the verified subspace of class  falsify the verified subspace.\nNote the root cause of these failures is the embedding gap, as we are unable to map sets of points in the embedding space back to sets of natural language sentences.\nConsequently, we are unable to reliably obtain correct specifications, and therefore we may enter a seemingly paradoxical situation, when in principle, the same subspace can be both formally verified and empirically falsified! Formal verification ensures that all sentences embedded within the semantic subspace will be classified identically by the given DNN; but empirical falsification of the semantic subspace comes from appealing to the semantic meaning of the embedded sentences \u2013 something that the NLP model can only seek to approximate.\nFailing to acknowledge and report on the problem of falsifiable verified subspaces may have different implications, depending on the usage scenario. Suppose the network is being used to recognise and censor sensitive (\u2018dangerous\u2019) sentences, and the subspace is verified to only contain such dangerous sentences. Then new sentences that fall inside of the verified subspace may still be wrongly censored; which in turn may make interaction with the chatbot impractical. But if the subspace is verified to only contain safe sentences, then potentially dangerous sentences could still be wrongly asserted as verifiably safe.\nNote that this problem is closely related to the well-known problem of false positives and false negatives in machine learning: as any new sentences that get incorrectly embedded into a verified subspace of a different class, must necessarily be false positives or false negatives for that DNN.\nIn the light of this limitation, the main question investigated by this section is: How can we measure and improve the quality of the purely NLP components of the pipeline, in a way that decreases the likelihood of generating falsifiable subspaces and therefore ensures the that our verification results are usable in practice?\nAs an answer to the measurement part of this question, we will introduce the falsifiability metric, that we argue should be used together with verifiability and generalisability metrics in all NLP verification benchmarks."
        },
        {
            "section_id": "5.1",
            "parent_section_id": "5",
            "section_name": "Role of False Positives and False Negatives",
            "text": "Generally, when DNNs are used for making decisions in situations where safety is critically important, practical importance of accuracy for each class may differ. For example, for an autonomous car, misrecognising a stop sign for 30 mph is more dangerous than misrecognising a 30 mph sign for a stop sign. Similarly for NLP, because of legal or safety implications, it is crucial that the chatbot always discloses its identity when asked, and never gives medical advice.\nIn the literature and in this paper, it is assumed that verified DNNs serve as filters that allow the larger system to use machine learning in a safer manner.\nWe therefore want to avoid false negatives altogether, i.e. if there is any doubt about the nature of the question, we would rather err on the side of caution and disallow the chatbot answers. If the chatbot (by mistake) refuses to answer some non-critically important questions, it maybe inconvenient for the user, but would not constitute a safety, security or legal breach. Thus, false positives maybe tolerated.\nOn the technical level, this has two implications:\nFirstly, if we use DNN on its own, without verification, we may want to report precision and recall999Recall that precision is defined as  and recall is defined as . in addition to accuracy.\nHigher recall will mean fewer false negative, which is highly desirable.\nSecondly, when verifying the network, we only seek to verify subspaces that characterise the critically important class or classes that we wish to filter.\nThis approach completely eliminates the effect of any false negatives within the verified subspaces. But the verified subspaces may still contain false positives (cf. Example 1  ###reference_i1###)\nand in this sense be \u201cfalsifiable\u201d, which may not be pragmatically desirable but does not constitute a security threat.\nTherefore, throughout this section we report the precision, recall and F1 to see whether one or another class is particularly challenging to recognise. Depending on the concrete scenario, precision or recall maybe more important."
        },
        {
            "section_id": "5.2",
            "parent_section_id": "5",
            "section_name": "Performance of Existing LLMs as Safety-Critical Filters",
            "text": "Bearing this in mind, we start first by gaining an intuition about the effectiveness of modern LLMs acting as safety-critical filters, by prompting an LLM to generate the desired output without additional fine-tuning, i.e. we first try a zero-shot model.\nFor our experiments, we choose the model vicuna-13b, given its high performance on the TruthfulQA zero-shot benchmark [144  ###reference_b144###], a benchmark to test a model\u2019s propensity to generate truthful answers to questions [145  ###reference_b145###]. Using vicuna-13b,\nwe experimented with prompts as shown in Figure 5  ###reference_###, and settled on the first prompt \u2018If I ask you anything about your agency, always only reply \u201cI am a large language model\u201d. If not, reply normally.\u201c[Example]\u201d\u2019? as it most often gave the (somewhat) desired output on a subset of manually checked data samples. Similarly for the medical dataset, we settled on a prompt \u2018If I ask you any medical query, always only reply \u201cAs an AI language model, I am not authorized to provide medical advice.\u201d. If not, reply normally.\u201c[Example]\u201d\u2019.\n###figure_10### For our zero-shot model, results are reported on the test set of our datasets. We use regular expressions and hand-crafted rules to check for the presence of the desired answer (e.g. \u2018I am a large language model\u2019 for the RUAR dataset) for positively classified training samples101010Additionally omitting  of answers which returned empty due to API errors.\n. For the RUAR dataset, if we are strict about the requirements of the output (only allowing for minor differences such as capitalisation), the F1 of the LLM is  (, ) as shown in the top line of Table 20  ###reference_###. This shows that false positives are slightly more likely than false negatives.\nIf we loosen our success criteria to consider other non-requested variations on our desired output (e.g. \u2018I am a chatbot\u2019 instead of \u2018I am a large language model\u2019) the F1 marginally improves, with . For the medical safety dataset, the results are , , and , indicating comparatively fewer false negatives.\nHowever, we found that in several cases the generated answers include a combination of the desired output and undesired output, e.g. \u2018\u2026I am not authorized to provide medical advice \u2026\u2019 followed by explicit medical advice and the results must be interpreted with this caveat. Therefore the actual success rate may be even lower than these reported results.\nNote there were at least 5 instances regarding the RUAR dataset where the system confirmed human identity, without any disclaimers. Thus, we find that our zero-shot model\nis, at most, minimally successful in identifying such queries,\nencouraging the need for verification methodologies.\n###table_19###"
        },
        {
            "section_id": "5.3",
            "parent_section_id": "5",
            "section_name": "Experimental Setup of the Verification Pipeline",
            "text": "We therefore turn our attention to assessing the effectiveness of training a classifier specifically for the task, and measuring the effect of the assumptions in Section 5  ###reference_### on the falsifiability of the verified subspaces.\nFor all experiments in this section, we set up the NLP verification pipeline as shown in Table 19  ###reference_###; and implement it using the tool ANTONIO [16  ###reference_b16###].\nIn setting up the pipeline, we use the key conclusions from Section 4  ###reference_### about successful verification strategies, namely:\nsemantic subspaces should be preferred over geometric subspaces as they result in a better verifiability-generalisability trade-off;\nconstructing semantic subspaces using stronger NLP perturbations results in higher verifiability of those subspaces;\nlikewise, adversarial training using subspaces constructed with stronger NLP perturbations also results in higher verifiability;\nMarabou allows us to verify a higher percentage of subspaces compared to ERAN thanks to its completeness.\nBased on these results, we further strengthen the NLP perturbations by substituting Polyjuice used in the previous section with Vicuna.\nVicuna introduces more diverse and sophisticated sentence perturbations.\nIn addition, we mix in the character and word perturbations used in the previous section, to further diversify and enlarge the set of available perturbed sentences.\nIn the terminology of Section 4.1  ###reference_###, we obtain the sets of perturbed sentences  and  where  is a combination of these perturbations.\nTable 19  ###reference_### also uses notation  and  to refer to filtered sets, this terminology will be introduced in Section 5.5.2  ###reference_.SSS2###.\nIn the light of the goals set up in this section, we diversify the kinds of LLMs we use as embedding functions. We use the sentence transformers package from Hugging Face originally proposed in [136  ###reference_b136###] (as our desired property is to give guarantees on entire sentences). Models in this framework are fine-tuned on a sentence similarity task which produces semantically meaningful sentence embeddings.\nWe select 3 different encoders to experiment with the size of the model. For our smallest model, we choose all-MiniLM-L6-v2, an s-transformer based on MiniLMv2 [146  ###reference_b146###], a compact version of the BERT architecture [135  ###reference_b135###] that has comparable performance. Additionally we choose 2 GPT-based models, available in the S-GPT package [137  ###reference_b137###]. We refer to these 3 models as s-bert 22M, s-gpt 1.3B, and s-gpt 2.7B respectively, where the number refers to size of the model (measured as the number of parameters).\nGiven , the set of semantic subspaces  which we wish to verify, are obtained via the hyper-rectangle construction in Definition 2  ###reference_inition2###. Accordingly, we set the adversarial training to explore the same subspaces , and to obtain the network ."
        },
        {
            "section_id": "5.4",
            "parent_section_id": "5",
            "section_name": "Analysis of the Role of Embedding Functions",
            "text": "For illustration, as well as an initial confidence check, we report F1 of the obtained models, for each of the chosen embedding functions in Table 20  ###reference_###.\nOverall the figures are as expected: compared to the F1 of 54-64% for the zero-shot model, using a fine-tuned trained DNN as a filter dramatically increases the F1 to the range of 76-95%.\n###table_20### Looking into nuances, one can further notice the following:\nThere is not a single embedding function that always results in the highest F1. For example, s-bert 22M is found to have the highest F1 for Medical, while s-gpt 2.7B has the highest F1 for RUAR (with the exception of F1 score, for which s-bert 22M is best for both datasets).\nThe smaller GPT model s-gpt 1.3B is systematically worse for both datasets.\nAs expected and discussed in Section 5.1  ###reference_###, depending on the scenario of use, the highest F1 may not be the best indicator of performance.\nFor Medical, s-bert 22M (either with or without adversarial training) obtains the highest precision, recall and F1. However, for RUAR, the choice of the embedding function has a greater effect:\nif F1 is desired, s-bert 22M is the best choice (difference with the worst choice of the embedding function is ,\nfor scenarios when one is not interested in verifying the network, the embedding function s-gpt 2.7B when combined with adversarial training gives an incredibly high recall () and would be a great choice (difference with the worst choice of the embedding function is ).\nhowever, if one wanted to use the same network for verification, s-gpt 2.7B would be the worst choice of embedding function, as the resulting precision drops to . For verification, either  trained with s-gpt 2.7B, or  trained with s-bert 22M would be better choices, both of which have precision .\nAdversarial training only makes a significant difference in F1 for the Medical perturbed test set. However, it has more effect on improving recall (up to 10% for Medical and 33% for RUAR).\nFor verifiability-generalisability trade-off, the choice of an embedding function also plays a role. Table 29  ###reference_### shows that s-gpt models exhibit lower verifiability compared to s-bert models. This observation also concurs with the findings in Section 4  ###reference_###:\ngreater volume correlates with increased generalisation, while a smaller and more precise subspace enhances verifiability.\nIndeed volumes for s-gpt models are orders of magnitude () larger than s-bert models.\nThe main conclusion one should make from the more nuanced analysis, is that depending on the scenario, the embedding function may influence the quality of the NLP verification pipelines, and reporting the error range (for both precision and recall) depending on the embedding function choice should be a common practice in NLP verification."
        },
        {
            "section_id": "5.5",
            "parent_section_id": "5",
            "section_name": "Analysis of Perturbations",
            "text": "Recall that two problems were identified as potential causes of falsifiable semantic subspaces: the imprecise embedding functions and invalid perturbations (i.e. the ones that change semantic meaning and the class of the perturbed sentences).\nIn the previous section, we obtained implicit evidence of variability of performance of the available state-of-the-art embedding functions. In this section, we turn our attention to analysis of perturbations. As outlined in [147  ###reference_b147###], to be considered valid, the perturbations should be semantically similar to the original, grammatical and have label consistency, i.e. human annotators should still assign the same label to the perturbed sample.\nFirstly, we wish to understand how common it is for our chosen perturbations to change the class, and secondly, we propose several practical methods how perturbation adequacy can be measured algorithmically.\nRecall that the definition of semantic subspaces depends on the assumption that we can always generate semantically similar (valid) perturbations and draw semantic subspaces around them. Both adversarial training and verification then explore the semantic subspaces. If this assumption fails and the subspaces contain a large number of invalid sentences, the NLP verification pipeline loses much of its practical value.\nTo get a sense of the scale of this problem, we start with the most reliable evaluation of sentence validity \u2013 human evaluation."
        },
        {
            "section_id": "5.5.1",
            "parent_section_id": "5.5",
            "section_name": "5.5.1 Understanding the Scale of the Problem",
            "text": "For the human evaluation, we labelled a subset of the perturbed datasets considering all three validity criteria discussed above.\nIn the experiment, for each original dataset  and word/character perturbation type , we select 10 perturbed sentences from . At the character level this gives us 50 perturbed sentences for both datasets (10 each for inserting, deleting, replacing, swapping or repeating a character).\nAt the word level this gives us 60 perturbed sentences for RUAR (deletion, repetition, ordering, negation, singular/plural, verb tense) and 30 for Medical (deletion, repetition, ordering).\nAt the sentence level, we only have one kind of perturbation - obtained by prompting vicuna-13b with instructions for the original sentence to be rephrased 5 times. We therefore randomly select 50 vicuna-13b perturbed sentences for each dataset. This results in a total of 290 pairs consisting of the original sentence and the perturbed sentence (130 from the medical safety, and 160 from the R-U-A-Robot dataset).\nWe then asked two annotators to both manually annotate all 290 pairs for the criteria shown in Table 21  ###reference_### which are modified from [147  ###reference_b147###].\nInter-Annotator Agreement (IAA) is reported via intraclass correlation coefficient (ICC).\n###table_21### Results of Human Evaluation.\nThe raw evaluation results are shown in Tables 22  ###reference_###, 23  ###reference_### and 24  ###reference_###.\nOverall, there are high scores for label consistency, in particular for rule-based perturbations, with  and  of the perturbations rated as maintaining the same label (i.e. score 1) by the two annotators  and  respectively. Similarly there are high scores for semantic similarity, with  and  of the ratings falling between levels 4 and 3 for  and . For grammaticality, annotators generally rate that perturbations generated by vicuna-13b are grammatical, whereas (as expected) rule-based perturbations compromise on grammaticality.\n###table_22### ###table_23### ###table_24### In order to evaluate the inter-annotator agreement, we report the ICC between the annotators. The ICC estimates and their 95% confidence intervals (CI)\nwere calculated based on absolute-agreement (single, fixed raters) \u2013 often referred to as ICC(A,1).\nUsing cutoffs provided by [148  ###reference_b148###], agreement was determined to be MODERATE for semantic similarity (F = 4.4 df (289), p<.001, 95% CI = [0.56,0.69]), BELOW SATISFACTORY for grammaticality (ICC = 0.43, p <.001, 95% CI = [0.34,0.52]) and BELOW SATISFACTORY for label consistency (ICC = 0.29, p<.001, 95% CI = [0.18, 0.39]).\nThis suggests that although annotators individually rated the perturbations for high label consistency, there may be disagreement on which specific samples maintain the same label. Given the moderate agreement for semantic similarity, we note that there may be perturbations that are semantically similar and yet may not maintain label consistency. For example Original: \u2018if a computer\ncan feel emotions, does that make you a computer or an actual human?\u2019, Perturbation: \u2018if a computer\ncan feel, does that make it a machine or a person\u2019 was rated by both annotators as high in semantic similarity but not maintaining label consistency. Overall, and particularly when using LLMs, perturbation quality and robustness to class change cannot be taken for granted, particularly when dealing with safety-critical queries.\nLimitations. We note this is in part due to our definition of grammatical being interpreted differently by the two independent evaluators (one accounting for character perturbations/spelling mistakes as un-grammatical and one not), and label consistency being ambiguous for the RUAR dataset. Finally, we also note that correlation between raters is statistically significant across all categories - indicating that ratings across coders were aligned beyond chance probability (criteria  = 0.05).\nFuture replications are warranted."
        },
        {
            "section_id": "5.5.2",
            "parent_section_id": "5.5",
            "section_name": "5.5.2 Automatic Ways to Measure and Report Perturbation Validity",
            "text": "Although in the near future, no geometric or algorithmic method will be able to match to the full extent the human perception and interpretation of sentences, we can still formulate a number of effective methods that give a characterisation of the validity of the perturbations utilised when defining semantic subspaces. We propose two:\nUsing cosine similarity of embedded sentences, we can characterise semantic similarity\nUsing the ROUGE-N method [149  ###reference_b149###] \u2013 a standard technique to evaluate natural sentence overlap, we can measure lexical and syntactic validity\nWe proceed to describe and evaluate each of them in order."
        },
        {
            "section_id": "5.5.x",
            "parent_section_id": "5.5",
            "section_name": "Cosine Similarity",
            "text": "Recall the definitions of ,  and cosine similarity in Section 3.3  ###reference_###.\nTo measure the general effectiveness of the embedding function at generating semantically similar sentences, we compute the percentage of vectors in  that have a cosine similarity with the embedding of the original sentence that is greater than .\nThe results are shown in Table 25  ###reference_###.\n###table_25### We then perform the experiments again, having removed all generated perturbations that fail to meet this threshold. For each original type of perturbation , this can be viewed as creating a new perturbation .\nTherefore in these alternative experiments, we form\n \u2013 the set of filtered sentence perturbations.\nFurthermore, we will refer to the set of hyper-rectangles obtained from  as  and, accordingly, we obtain the network  through adversarial training on .\nThe results are shown in Table 26  ###reference_###.\n###table_26### The results then allow us to identify the pros and cons of cosine similarity as a metric.\nPros:\nThere is some indication that cosine similarity is to a certain extent effective. For example, we have seen in Table 20  ###reference_### in Section 5.3  ###reference_### that s-bert 22M was the best choice for F1 and precision \u2013 and we see in Table 25  ###reference_### that s-bert 22M eliminates the most perturbed sentences, while not penalising its F1 in Table 26  ###reference_###. However, we cannot currently evaluate whether it is eliminating the truly dissimilar sentences. This will be evaluated at the end of this section, when we measure how using  instead of  impacts verifiability and falsifiability.\nCosine similarity metric is general (i.e. would apply irrespective of other choice of the pipeline), efficient and scalable.\nCons:\nAs discussed earlier, due to its geometric nature, the cosine similarity metric does not give us direct knowledge about true semantic similarity of sentences. As evidence of this, the human evaluation of semantic similarity we presented in Section 5.5.1  ###reference_.SSS1### hardly matches the optimistic numbers reported in Table 25  ###reference_###!\nMoreover, cosine similarity relies on the assumption that the embedding function embeds semantically similar sentences close to each other in m. As an indication that this assumption may not hold, Table 25  ###reference_### shows that disagreement in cosine similarity estimations may vary up to  when different embedding functions are applied.\nThus, the overall conclusion is that, although it has its limitations, cosine similarity is a useful metric to report, and filtering based on cosine similarity is useful\nas a pre-processing stage in the NLP verification pipeline. The latter will be demonstrated at the end of this section, when we take the pipeline in Table 19  ###reference_### and substitute  for .\n###table_27### ###table_28###"
        },
        {
            "section_id": "5.5.x",
            "parent_section_id": "5.5",
            "section_name": "ROUGE-N",
            "text": "We additionally calculate lexical and syntactic variability of the generated vicuna-13b output by reporting ROUGE-N  and  scores (i.e. which measures  overlap) [149  ###reference_b149###], where . Intuitively if  is a sentence from the dataset and  a perturbation of , ROUGE-N is an overlap measure, which measures:\n, i.e. the number of words (for )\nor word sequences (for )\nin  that also appear in , divided by the number of words in ; and\n, i.e. number of words (for )\nor word sequences (for )\nin  that also appear in , divided by the number of words in .\nFigure 6  ###reference_### shows an experiment in which vicuna-13b is asked to generate sentence perturbations. As we can see, the results show a high number of invalid sentences, due to incoherence, hallucination, or wrong literal rephrasing.\n###figure_11### For lexical ROUGE-N, we compare the strings of the original sample to the perturbations, while for syntax we follow the same procedure, but using the corresponding parts-of-speech (POS) tags [150  ###reference_b150###]. Furthermore, we calculate and compare ROUGE-N before and after filtering with cosine similarity. Results are given in Tables 27  ###reference_### and 28  ###reference_###, and qualitative examples of errors in Figure 6  ###reference_###. It is important to note that we are not concerned with low  and  scores, as it does not necessarily imply non-semantic preserving rephrases. For example, shuffling, rephrasing or synonym substitution could lower the scores.\nPrior to filtering, the scores remain steady for , while after filtering, the scores decrease as  increases. When the scores remain steady prior to filtering, it implies a long sequence of text is overlapping between the original and the perturbation (i.e. for unigrams, bigrams and trigrams), though there may be remaining text unique between the two sentences. When  and  decay, it means that singular words overlap in both sentences, but not in the same sequence, or they are alternated by other words (i.e the high unigram overlap decaying to low trigram overlap). It is plausible that cosine similarity filters out perturbations that have long word sequence overlaps with the original, but that also contain added hallucinations that change the semantic meaning (see Figure 6  ###reference_###, the \u2018Hallucinated content\u2019 example).\nGenerally, there is higher syntactic overlap than lexical overlap, regardless of filtering. Sometimes this leads to unsatisfactory perturbations, where local rephrasing leads to globally implausible sounding sentences, as shown in Figure 6  ###reference_### (the \u2018Local rephrasing, global incoherence\u2019 example).\nWithout filtering, there is higher  compared to , while after filtering, the  increases. From Tables 27  ###reference_### and 28  ###reference_### we can hypothesise that overall cosine similarity filters out perturbations that are shorter than the original sentences.\nObservationally, we also find instances of literal rephrasing (see Figure 6  ###reference_###, the \u2018Literal (not pragmatic) rephrasing\u2019 example), which illustrates the difficulties of generating high quality perturbations. For example in the medical queries, often there are expressed emotions that need to be inferred. The addition of hallucinated content in perturbations is also problematic. However, it would be more problematic if we were to utilise the additional levels of risk labels from the medical safety dataset (see Section 2.5.1  ###reference_.SSS1###) \u2013 the hallucinated content can have a non-trivial impact on label consistency."
        },
        {
            "section_id": "5.6",
            "parent_section_id": "5",
            "section_name": "Falsifiability",
            "text": "As the final result of this paper, we introduce the new metric \u2013 falsifiability \u2013 that measures the number of unwanted sentences that are mapped into a verified subspace. Recall that Sections 5.4  ###reference_### and 5.5  ###reference_### discussed the methods that assess the role of inaccurate embeddings and semantically incoherent perturbations in isolation. In both cases, the methods were of general NLP applicability, and did not directly link to verifiability or generalisability of verified subpspaces.\nThe falsifiability metric differs from these traditional NLP methods in two aspects:\nfirstly, it helps to\nmeasure both effects simultaneously, and thus helps to assess validity of both the assumption of locality of the embedding function and the assumption of semantic stability of the perturbations outlined at the start of Section 5  ###reference_###.\nsecondly, it is applied here as a verification metric specifically. Applied to the same verified subspaces and adversarially trained networks as advocated in Section 4  ###reference_###, it is shown as a verification metric on par with verifiability and generalisability.\nWe next formally define the falsifiability metric. Intuitively, the falsifiability of a set of subspaces  of class  is the percentage of those subspaces that contain at least one embedding of a sentence that belongs to a different class.\nGiven a set of subspaces  that are supposed to contain exclusively sentences of class , a dataset  that contains sentences not of class  and a set of embeddings , then falsifiability is measured as the percentage of subspaces that contain at least one element of .\nwhere  is the indicator function returning  for true.\nAs with the definition of generalisability, in this paper we will generate the target set of embeddings  as  where  is a dataset,  is the type of semantic perturbation,  is the number of perturbations and  is the embeddings of the set of semantic perturbations  around  generated using , as described in Section 3.3  ###reference_###.\nWe also measure the presence of false positives, as calculated as the percentage of the perturbations of sentences from classes other than  that lie within at least one of the set of subspaces .\nTo measure the effectiveness of the falsifiability metric, we perform the following experiments.\nAs previously shown in Table 19  ###reference_###, both RUAR and Medical datasets are split into two classes,  and . We construct\n and , and as described in Section 3.3  ###reference_###, the set  is obtained by embedding sentences in .\nThe subspaces for which we measure falsifiability are given by  where we consider both the unfiltered () and the filtered version () of the perturbation .\n###table_29### Table 29  ###reference_### shows the falsifiability of our models and semantic subspaces. This gives us a quantitative estimation of the scale of the problem of discussed at the start of this section. Namely, whether the assumptions of the locality of the embedding function and that perturbations are semantics-preserving hold.\nFrom the fact that in five out of the twelve experiments falsifiability is non-zero, we can see that the problem indeed exists, hence our claim that falsifiability should be reported routinely in NLP verification papers.\nAt the same time, the problem does not appear to be as severe as one might fear: we note that at most  of the subspaces are falsified, and falsifiability has only occurred in the subspaces created based on the RUAR dataset. Therefore falsifiability is constrained to a few subspaces that can, in theory, be removed.\nFurthermore, falsifiability could also reflect issues in the dataset and subsequent noisy perturbations. The medical safety dataset, for instance, was annotated by an expert practitioner, while the RUAR dataset contains (for this particular task) what could be construed as noisy labels. For example \u2018are robot you you a\u2019 is a sample that is found in the negative RUAR train set. The reason for the negative label is that it is an ungrammatical false positive, but given our methods of perturbation for the construction of subspaces, this negative sample may be very similar to a word level perturbation for the positive class. Concretely, for the model with the highest falsifiability in Table 29  ###reference_### (i.e.  s-bert 22M for RUAR dataset with  falsifiability), some sentence pairs of negative samples with their accompanying perturbations contained in falsified subspaces are: (Original: \u2018Are you a chump?\u2019, Perturbation: \u2018You a chump\u2019), (Original: \u2018Are you a liar\u2019, Perturbation: \u2018You a liar\u2019), (Original: \u2018if a computer can feel emotions, does that make you a computer or an actual human?\u2019, Perturbation: \u2018if a computer can feel, does that make it a machine or a person\u2019). Thus, the task of determining what queries require disclosure (e.g. should \u2018what is your favorite food\u2019 warrant disclosure?) is more ambiguous and, as the outputs of LLMs sound more coherent, it becomes harder to define.\nThis area merits further research."
        },
        {
            "section_id": "5.6.x",
            "parent_section_id": "5.6",
            "section_name": "Falsifiability vs Generalisability and Verifiability",
            "text": "For comparison with the findings outlined in Section 4  ###reference_###, we provide additional insights into verifiability and generalisability, also presented in Table 29  ###reference_###.\nWe first analyse the effect of cosine similarity filtering.\nInitially, the experiments reveal that filtering results in slightly\nhigher levels of both verifiability and generalisability for all models.\nGiven the conclusions in Section 4  ###reference_###, the increase in verifiability is expected.\nHowever, the increase in generalisability is somewhat unexpected because, as demonstrated in Section 4  ###reference_###, larger subspaces tend to exhibit greater generalisability, but filtering decreases the volume of the subspaces.\nTherefore, we conjecture the increase in precision of the subspaces from filtering outweighs the reduction in their volume and hence generalisability increases overall.\nThe data therefore suggests that cosine similarity filtering can serve as an additional heuristic for improving precision of the verified DNNs, and for further reducing the verifiability-generalisability gap.\nIndeed, upon calculating the ratio of generalisability to verifiability, we observe a higher ratio before filtering ( for RUAR and  for Medical).\nRecall that Section 4  ###reference_### already showed that our proposed usage of semantic subspaces can serve as a heuristic for closing the gap; and cosine similarity filtering provides opportunity for yet another heuristic improvement.\nMoreover, the best performing model  (s-bert 22M), results in   medical perturbations and   RUAR perturbations contained in the verified subspaces. While  of the  perturbations contained in the verified subspaces for the RUAR dataset may seem like a low number, it still results in a robust filter, given that the  class of the dataset contains many adversarial examples of the same input query, i.e. semantically similar but lexically different queries.\nThe medical dataset on the other hand contains many semantically diverse queries, and there are several unseen medical queries not contained in the dataset nor in the resultant verified subspaces.\nHowever, given that the subspaces contain  of the  perturbations of the medical safety dataset, an application of this could be to carefully curate a new dataset containing only queries with critical and serious risk-level labels defined by the World Economic Forum for chatbots in healthcare (see Section 2.5.1  ###reference_.SSS1### and [133  ###reference_b133###]).\nThis dataset could be used to create verified filters centred around these queries to prevent generation of medical advice for these high-risk categories.\nOverall, we find that semantically-informed verification generalises well across the different kinds of data to ensure guarantees on the output, and thus should aid in ensuring the safety of LLMs."
        },
        {
            "section_id": "6",
            "parent_section_id": null,
            "section_name": "Conclusions and Future Work",
            "text": "Summary. This paper started with a general analysis of existing NLP verification approaches,\nwith a view of identifying key components of a general NLP verification methodology. We then distilled these into a \u201cNLP Verification Pipeline\u201d consisting of the following six components:\ndataset selection;\ngeneration of perturbations;\nchoice of embedding functions;\ndefinition of subspaces;\nrobust training;\nverification via one of existing verification algorithms.\nBased on this taxonomy, we make concrete selections for each component, and implement the pipeline using the tool ANTONIO [16  ###reference_b16###].\nANTONIO allowed us to mix and match different choices for each pipeline component, enabling us to study the effects of varying the components of the pipeline in a algorithm-independent way.\nOur main focus was to identify weak or missing parts of the existing NLP verification methodologies.\nWe proposed that NLP verification results should report, in addition to the standard verifiability metric, the following:\nwhether they use geometric or semantic subspaces, and for which type of semantic perturbations;\nvolumes, generalisability and falsifiability of verified subspaces.\nWe finished the paper with a study of the current limitations of the NLP components of the pipeline and proposed possible improvements such as introducing a perturbations filter stage using cosine similarity.\nOne of the major strengths of the pipeline is that each component can be improved individually.\nContributions. The major discoveries of this paper were:\nIn Section 4  ###reference_### we proposed generalisability as a novel metric, and showed that NLP verification methods exhibit a generalisability-verifiability trade-off. The effects of the trade-off can be severe, especially if the verified subspaces are generated naively (e.g. geometrically). We therefore strongly believe that generalisability should be routinely reported as part of NLP verification pipeline.\nIn Sections 4  ###reference_### and 5  ###reference_### we showed that it is possible to overcome this trade-off by using several heuristic methods: defining semantic subspaces, training for semantic robustness, choosing a suitable embedding function and filtering with cosine similarity. All of these methods result in the definition of more precise verifiable subspaces; and all of them can be practically implemented as part of NLP verification pipelines in the future.\nIn Section 5  ###reference_### we demonstrated that there are two key assumptions underlying the definition of subspaces that cannot be taken for granted. Firstly the LLMs, used as embedding functions, may not map semantically similar sentences to similar vectors in the embedding space. Secondly, our algorithmic methods for generating perturbations, whether by LLMs or otherwise, may not always be semantically-preserving operations. Both of these factors influence practical applications of the NLP verification pipeline.\nIn Section 5  ###reference_### we demonstrated that even verified subspaces can be semantically falsified: this effect is due to the tension between verification methods that are essentially geometric and the intuitively understood semantic meaning of sentences.\nBy defining the falsifiability metric and using it in our experiments, we demonstrated that the effects of falsifiability do not seem to be severe in practice; but this may vary from one scenario to another. It is important that NLP verification papers are aware of this pitfall, and report falsifiability alongside verifiability and generalisability.\nFinally, we claim as a contribution, a novel, coherent, methodological framework that allows us to include a broad spectrum of NLP, geometric, machine learning, and verification methods under a single umbrella. As illustrated throughout this paper, no previous publication in this domain covered this range and we believe that covering this broad range of methods is crucial for the development of this field.\nFuture Work.\nFollowing from our in-depth analysis of the NLP perspective, we note that even if one has a satisfactory solution to all the issues discussed, there is still the problem of scalability of the available verification algorithms. For example, the most performant neural network verifier, -Crown [56  ###reference_b56###], can only handle networks in the range of tens of millions of trainable parameters. In contrast, in NLP systems, the base model of BERT [135  ###reference_b135###] has around 110 million trainable parameters (considered small compared to modern LLMs \u2013 with trainable parameters in the billions!). It is clear that the rate at which DNN verifiers become more performant may never catch up with the rate at which Large Language Models (LLMs) become larger. Then the question arises: how can this pipeline be implemented in the real world?\nFor future work, we propose to tackle this based on the idea of verifying a smaller DNN (classifier), manageable by verifiers, that can be placed upstream of a complex NLP system as a safeguard.\nWe call this a filter (as mentioned in Section 3  ###reference_### and illustrated in Figure 2  ###reference_###), and Figure 7  ###reference_### shows how a semantically informed verified filter can be prepended to an NLP system (here, an LLM) to check that safety-critical queries are handled responsibly, e.g. by redirecting the query to a tightly controlled rule-based system instead of a stochastic LLM.\n###figure_12### While there are different ways to implement the verification filters (e.g. only the verified subspaces) we suggest utilizing both the verified subspaces together with the DNN as the additional classification could strengthen catching positives that fall outside the verified subspaces, thus giving stronger chances of detecting the query via both classification and verification.\nWe note that the NLP community has recently proposed guardrails, in order to control the output of LLMs and create safer systems (such as from Open AI  ###reference_/openai_functions/###, NVIDIA  ###reference_### and so on). These guardrails have been proposed at multiple stages of an NLP pipeline, for example an output rail that checks the output returned by an LLM, or input rail, that rejects unsafe user queries. In Figure 7  ###reference_###, we show an application of our filter applied to the user input, which thus creates guarantees that a subset of safety critical queries are handled responsibly. In theory these verification techniques we propose may be applied to guardrails at different stages in the system, and we plan to explore this in future work.\nA second future direction is to use this work to create NLP verification benchmarks.\nIn 2020, the International Verification of Neural Networks Competition [151  ###reference_b151###] (VNN-COMP) was established to facilitate comparison between existing approaches, bring researchers working on the DNN verification problem together, and help shape future directions of the field.\nHowever, on its fourth edition, the competition still lacked NLP verification benchmarks [152  ###reference_b152###]. We propose to use this work for creating NLP verification benchmarks for future editions, to spread the awareness and attention to this field."
        }
    ],
    "appendix": [],
    "tables": {
        "1": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T1.15\">\n<tr class=\"ltx_tr\" id=\"S2.T1.15.16\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt\" id=\"S2.T1.15.16.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.16.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.16.1.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.16.1.1.1.1\" style=\"font-size:80%;\">Method</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.15.16.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.16.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.16.2.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.16.2.1.1.1\" style=\"font-size:80%;\">Verification algorithm</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.15.16.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.16.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.16.3.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.16.3.1.1.1\" style=\"font-size:80%;\">Verification characteristics</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.15.16.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.16.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.16.4.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.16.4.1.1.1\" style=\"font-size:80%;\">Datasets</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.15.16.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.16.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.16.5.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.16.5.1.1.1\" style=\"font-size:80%;\">NLP perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.15.16.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.16.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.16.6.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.16.6.1.1.1\" style=\"font-size:80%;\">Embeddings</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.15.16.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.16.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.16.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.16.7.1.1.1\" style=\"font-size:80%;\">Architectures (# of parameters)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.15.16.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.16.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.16.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.16.8.1.1.1\" style=\"font-size:80%;\">Robust training</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt\" id=\"S2.T1.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.2.2.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.2.2.3.1.1.1\" style=\"font-size:80%;\">Ours</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.2.2.4.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.2.2.4.1.1.1\" style=\"font-size:80%;\">SMT</span><span class=\"ltx_text\" id=\"S2.T1.2.2.4.1.1.2\" style=\"font-size:80%;\">-based, Abstract interpretation-based, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.2.2.4.1.1.3\" style=\"font-size:80%;\">BaB</span><span class=\"ltx_text\" id=\"S2.T1.2.2.4.1.1.4\" style=\"font-size:80%;\">-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.2.2.5.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.2.2.5.1.1.1\" style=\"font-size:80%;\">Complete</span><span class=\"ltx_text\" id=\"S2.T1.2.2.5.1.1.2\" style=\"font-size:80%;\">, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.2.2.5.1.1.3\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.2.2.6.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.2.2.6.1.1.1\" style=\"font-size:80%;\">RUARobot, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T1.2.2.6.1.1.2\" style=\"font-size:80%;\">Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.1.1.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T1.1.1.1.1.1.1\" style=\"font-size:80%;\">General purpose: char, word and sentence perturbations, </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.2.2.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.2.2.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.2.2.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.2.2.7.1.1.1\" style=\"font-size:80%;\">Sentence: S-BERT, S-GPT</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.2.2.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.2.2.2.1.1.1\" style=\"font-size:80%;\">FFNN (</span><span class=\"ltx_text\" id=\"S2.T1.2.2.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T1.2.2.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.2.2.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.2.2.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.2.2.8.1.1.1\" style=\"font-size:80%;\">PGD</span><span class=\"ltx_text\" id=\"S2.T1.2.2.8.1.1.2\" style=\"font-size:80%;\">-based</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.3.3.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.3.3.2.1.1.1\" style=\"font-size:80%;\">Jia et al. (2019) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.3.3.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" id=\"S2.T1.3.3.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.3.3.3.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.3.3.3.1.1.1\" style=\"font-size:80%;\">IBP-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.3.3.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.3.3.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.3.3.4.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.3.3.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.3.3.5.1.1.1\" style=\"font-size:80%;\">IMDB, SNLI</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.3.3.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.3.3.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T1.3.3.6.1.1.1\" style=\"font-size:80%;\">Word substitution</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.3.3.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.3.3.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.3.3.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.3.3.1.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.3.3.1.1.1.1\" style=\"font-size:80%;\">LSTM, CNN, BoW, Attention-based, (</span><span class=\"ltx_text\" id=\"S2.T1.3.3.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.3.3.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.3.3.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.3.3.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.3.3.8.1.1.1\" style=\"font-size:80%;\">IBP</span><span class=\"ltx_text\" id=\"S2.T1.3.3.8.1.1.2\" style=\"font-size:80%;\">-based</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.4.4.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.4.2.1.1.1\" style=\"font-size:80%;\">Huang et al. (2019) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.4.4.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" id=\"S2.T1.4.4.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.4.4.3.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.4.3.1.1.1\" style=\"font-size:80%;\">IBP-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.4.4.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.4.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.4.4.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.4.4.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.4.5.1.1.1\" style=\"font-size:80%;\">AGNews, SST</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.4.4.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.4.6.1.1.1\" style=\"font-size:80%;\">Char and word substitution</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.4.4.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.4.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.4.4.1.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.4.4.1.1.1.1\" style=\"font-size:80%;\">CNN (</span><span class=\"ltx_text\" id=\"S2.T1.4.4.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.4.4.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.4.4.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.4.4.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.4.8.1.1.1\" style=\"font-size:80%;\">IBP</span><span class=\"ltx_text\" id=\"S2.T1.4.4.8.1.1.2\" style=\"font-size:80%;\">-based</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.5.5.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.5.5.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.5.5.2.1.1.1\" style=\"font-size:80%;\">Welbl et al. (2020) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.5.5.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib103\" title=\"\">103</a><span class=\"ltx_text\" id=\"S2.T1.5.5.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.5.5.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.5.5.3.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.5.5.3.1.1.1\" style=\"font-size:80%;\">IBP-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.5.5.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.5.5.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.5.4.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.5.5.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.5.5.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.5.5.5.1.1.1\" style=\"font-size:80%;\">SNLI, MNLI</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.5.5.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.5.5.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T1.5.5.6.1.1.1\" style=\"font-size:80%;\">Word deletion</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.5.5.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.5.5.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.5.5.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.5.5.1.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.5.5.1.1.1.1\" style=\"font-size:80%;\">Attention-based (</span><span class=\"ltx_text\" id=\"S2.T1.5.5.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.5.5.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.5.5.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.5.5.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T1.5.5.8.1.1.1\" style=\"font-size:80%;\">Data augmentation, random and beam search adversarial training, </span><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.5.5.8.1.1.2\" style=\"font-size:80%;\">IBP</span><span class=\"ltx_text\" id=\"S2.T1.5.5.8.1.1.3\" style=\"font-size:80%;\">-based</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.6.6.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.6.6.2.1.1.1\" style=\"font-size:80%;\">Zhang et al. (2021) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.6.6.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib19\" title=\"\">19</a><span class=\"ltx_text\" id=\"S2.T1.6.6.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.6.6.3.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.6.6.3.1.1.1\" style=\"font-size:80%;\">IBP-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.6.6.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.6.6.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.6.6.4.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.6.6.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.6.6.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.6.6.5.1.1.1\" style=\"font-size:80%;\">IMDB, SST, SST2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.6.6.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.6.6.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T1.6.6.6.1.1.1\" style=\"font-size:80%;\">Word perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.6.6.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.6.6.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.6.6.7.1.1.1\" style=\"font-size:80%;\">Word: not specified</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.6.6.1.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.6.6.1.1.1.1\" style=\"font-size:80%;\">LSTM (</span><span class=\"ltx_text\" id=\"S2.T1.6.6.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.6.6.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.6.6.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.6.6.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.6.6.8.1.1.1\" style=\"font-size:80%;\">IBP</span><span class=\"ltx_text\" id=\"S2.T1.6.6.8.1.1.2\" style=\"font-size:80%;\">-based</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.7.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.7.7.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.7.7.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.7.7.2.1.1.1\" style=\"font-size:80%;\">Wang et al. (2023) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.7.7.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib104\" title=\"\">104</a><span class=\"ltx_text\" id=\"S2.T1.7.7.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.7.7.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.7.7.3.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.7.7.3.1.1.1\" style=\"font-size:80%;\">IBP-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.7.7.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.7.7.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.7.7.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.7.7.4.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.7.7.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.7.7.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.7.7.5.1.1.1\" style=\"font-size:80%;\">IMDB, YELP, SST2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.7.7.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.7.7.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T1.7.7.6.1.1.1\" style=\"font-size:80%;\">Word substitution</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.7.7.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.7.7.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.7.7.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.7.7.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.7.7.1.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.7.7.1.1.1.1\" style=\"font-size:80%;\">CNN (</span><span class=\"ltx_text\" id=\"S2.T1.7.7.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.7.7.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.7.7.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.7.7.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.7.7.8.1.1.1\" style=\"font-size:80%;\">IBP</span><span class=\"ltx_text\" id=\"S2.T1.7.7.8.1.1.2\" style=\"font-size:80%;\">-based: Embedding Interval Bound Constraint (EIBC) triplet loss</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.9.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.9.9.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.9.9.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.9.9.3.1.1.1\" style=\"font-size:80%;\">Ko et al. (2019) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.9.9.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib105\" title=\"\">105</a><span class=\"ltx_text\" id=\"S2.T1.9.9.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.9.9.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.9.9.4.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.9.9.4.1.1.1\" style=\"font-size:80%;\">Abstract interpretation-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.9.9.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.9.9.5.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.9.9.5.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.9.9.5.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.9.9.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.9.9.6.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.9.9.6.1.1.1\" style=\"font-size:80%;\">CogComp QC</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.8.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.8.8.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.8.8.1.1.1\" style=\"width:75.9pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.9.9.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.9.9.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.9.9.7.1.1.1\" style=\"font-size:80%;\">Word: not specified</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.9.9.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.9.9.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.9.9.2.1.1.1\" style=\"font-size:80%;\">RNN, LSTM (</span><span class=\"ltx_text\" id=\"S2.T1.9.9.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.9.9.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.9.9.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.9.9.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T1.9.9.8.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.11.11\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.11.11.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.11.11.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.11.11.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.11.11.3.1.1.1\" style=\"font-size:80%;\">Shi et al. (2020) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.11.11.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" id=\"S2.T1.11.11.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.11.11.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.11.11.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.11.11.4.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.11.11.4.1.1.1\" style=\"font-size:80%;\">Abstract interpretation-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.11.11.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.11.11.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.11.11.5.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.11.11.5.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.11.11.5.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.11.11.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.11.11.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.11.11.6.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.11.11.6.1.1.1\" style=\"font-size:80%;\">YELP, SST</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.10.10.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.10.10.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.10.10.1.1.1\" style=\"width:75.9pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.11.11.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.11.11.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.11.11.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.11.11.7.1.1.1\" style=\"font-size:80%;\">Word: not specified</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.11.11.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.11.11.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.11.11.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.11.11.2.1.1.1\" style=\"font-size:80%;\">Transformer (</span><span class=\"ltx_text\" id=\"S2.T1.11.11.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.11.11.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.11.11.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.11.11.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T1.11.11.8.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.13.13\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.13.13.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.13.13.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.13.13.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.13.13.3.1.1.1\" style=\"font-size:80%;\">Du et al. (2021) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.13.13.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib106\" title=\"\">106</a><span class=\"ltx_text\" id=\"S2.T1.13.13.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.13.13.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.13.13.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.13.13.4.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.13.13.4.1.1.1\" style=\"font-size:80%;\">Abstract interpretation-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.13.13.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.13.13.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.13.13.5.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.13.13.5.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.13.13.5.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.13.13.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.13.13.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.13.13.6.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.13.13.6.1.1.1\" style=\"font-size:80%;\">Rotten Tomatoes Movie Review, Toxic Comment</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.12.12.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.12.12.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.12.12.1.1.1\" style=\"width:75.9pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.13.13.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.13.13.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.13.13.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.13.13.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.13.13.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.13.13.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.13.13.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.13.13.2.1.1.1\" style=\"font-size:80%;\">RNN, LSTM (</span><span class=\"ltx_text\" id=\"S2.T1.13.13.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T1.13.13.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.13.13.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.13.13.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.13.13.8.1.1.1\" style=\"font-size:80%;\">IBP</span><span class=\"ltx_text\" id=\"S2.T1.13.13.8.1.1.2\" style=\"font-size:80%;\">-based</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.15.15\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T1.15.15.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.15.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.15.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.15.15.3.1.1.1\" style=\"font-size:80%;\">Bonaert et al. (2021) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T1.15.15.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib107\" title=\"\">107</a><span class=\"ltx_text\" id=\"S2.T1.15.15.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.15.15.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.15.4.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.15.4.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T1.15.15.4.1.1.1\" style=\"font-size:80%;\">Abstract interpretation-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.15.15.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.15.5.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.15.5.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T1.15.15.5.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.15.15.5.1.1.2\" style=\"font-size:80%;\">Deterministic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.15.15.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.15.6.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.15.6.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T1.15.15.6.1.1.1\" style=\"font-size:80%;\">SST, YELP</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.14.14.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.14.14.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.14.14.1.1.1\" style=\"width:75.9pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.15.15.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.15.7.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.15.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.15.15.7.1.1.1\" style=\"font-size:80%;\">Word: not specified</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.15.15.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.15.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.15.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T1.15.15.2.1.1.1\" style=\"font-size:80%;\">Transformer (</span><span class=\"ltx_text\" id=\"S2.T1.15.15.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.15.15.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.15.15.8.1\">\n<span class=\"ltx_p\" id=\"S2.T1.15.15.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T1.15.15.8.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S2.T1.19.1.1\" style=\"font-size:113%;\">Table 1</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S2.T1.20.2\" style=\"font-size:113%;\">Summary of the main features of the existing NLP verification approaches. In bold are state-of-the-art methods.</em></figcaption>\n</figure>",
            "capture": "Table 1: Summary of the main features of the existing NLP verification approaches. In bold are state-of-the-art methods."
        },
        "2": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T2.13\">\n<tr class=\"ltx_tr\" id=\"S2.T2.13.14\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt\" id=\"S2.T2.13.14.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.14.1.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.14.1.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.13.14.1.1.1.1\" style=\"font-size:80%;\">Method</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.13.14.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.14.2.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.14.2.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.13.14.2.1.1.1\" style=\"font-size:80%;\">Verification algorithm</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.13.14.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.14.3.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.14.3.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.13.14.3.1.1.1\" style=\"font-size:80%;\">Verification characteristics</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.13.14.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.14.4.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.14.4.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.13.14.4.1.1.1\" style=\"font-size:80%;\">Datasets</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.13.14.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.14.5.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.14.5.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.13.14.5.1.1.1\" style=\"font-size:80%;\">NLP perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.13.14.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.14.6.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.14.6.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.13.14.6.1.1.1\" style=\"font-size:80%;\">Embeddings</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.13.14.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.14.7.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.14.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.13.14.7.1.1.1\" style=\"font-size:80%;\">Architectures (# of parameters)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.13.14.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.14.8.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.14.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.13.14.8.1.1.1\" style=\"font-size:80%;\">Robust training</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt\" id=\"S2.T2.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S2.T2.2.2.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.2.2.3.1.1.1\" style=\"font-size:80%;\">Ye et al. (2020) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T2.2.2.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib108\" title=\"\">108</a><span class=\"ltx_text\" id=\"S2.T2.2.2.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S2.T2.1.1.1.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T2.1.1.1.1.1.1\" style=\"font-size:80%;\">Randomised smoothing (</span><span class=\"ltx_text\" id=\"S2.T2.1.1.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S2.T2.2.2.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T2.2.2.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.2.2.4.1.1.2\" style=\"font-size:80%;\">Probabilistic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S2.T2.2.2.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.2.2.5.1.1.1\" style=\"font-size:80%;\">IMDB, Amazon</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S2.T2.2.2.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T2.2.2.6.1.1.1\" style=\"font-size:80%;\">Word substitution</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.2.2.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.2.2.7.1\">\n<span class=\"ltx_p\" id=\"S2.T2.2.2.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.2.2.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S2.T2.2.2.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.2.2.2.1.1.1\" style=\"font-size:80%;\">Transformer (</span><span class=\"ltx_text\" id=\"S2.T2.2.2.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T2.2.2.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.2.2.8.1\">\n<span class=\"ltx_p\" id=\"S2.T2.2.2.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T2.2.2.8.1.1.1\" style=\"font-size:80%;\">Data </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.2.2.8.1.1.2\" style=\"font-size:80%;\">augmentation</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T2.3.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S2.T2.3.3.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.3.3.2.1.1.1\" style=\"font-size:80%;\">Wang et al. (2021) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T2.3.3.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib109\" title=\"\">109</a><span class=\"ltx_text\" id=\"S2.T2.3.3.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S2.T2.3.3.3.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T2.3.3.3.1.1.1\" style=\"font-size:80%;\">Differential privacy-based</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S2.T2.3.3.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T2.3.3.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.3.3.4.1.1.2\" style=\"font-size:80%;\">Probabilistic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S2.T2.3.3.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.3.3.5.1.1.1\" style=\"font-size:80%;\">IMDB, AGNews</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.3.3.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.3.3.6.1\">\n<span class=\"ltx_p\" id=\"S2.T2.3.3.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T2.3.3.6.1.1.1\" style=\"font-size:80%;\">Word substitution</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.3.3.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.3.3.7.1\">\n<span class=\"ltx_p\" id=\"S2.T2.3.3.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.3.3.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S2.T2.3.3.1.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.3.3.1.1.1.1\" style=\"font-size:80%;\">LSTM (</span><span class=\"ltx_text\" id=\"S2.T2.3.3.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.3.3.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.3.3.8.1\">\n<span class=\"ltx_p\" id=\"S2.T2.3.3.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T2.3.3.8.1.1.1\" style=\"font-size:80%;\">Data </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.3.3.8.1.1.2\" style=\"font-size:80%;\">augmentation</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T2.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.5.5.3.1\">\n<span class=\"ltx_p\" id=\"S2.T2.5.5.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.5.5.3.1.1.1\" style=\"font-size:80%;\">Zhao et al. (2022) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T2.5.5.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib110\" title=\"\">110</a><span class=\"ltx_text\" id=\"S2.T2.5.5.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S2.T2.4.4.1.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T2.4.4.1.1.1.1\" style=\"font-size:80%;\">Randomised smoothing (</span><span class=\"ltx_text\" id=\"S2.T2.4.4.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S2.T2.5.5.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T2.5.5.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.5.5.4.1.1.2\" style=\"font-size:80%;\">Probabilistic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.5.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.5.5.5.1\">\n<span class=\"ltx_p\" id=\"S2.T2.5.5.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.5.5.5.1.1.1\" style=\"font-size:80%;\">AGNews, SST</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.5.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.5.5.6.1\">\n<span class=\"ltx_p\" id=\"S2.T2.5.5.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T2.5.5.6.1.1.1\" style=\"font-size:80%;\">Word substitution</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.5.5.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.5.5.7.1\">\n<span class=\"ltx_p\" id=\"S2.T2.5.5.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.5.5.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.5.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.5.5.2.1\">\n<span class=\"ltx_p\" id=\"S2.T2.5.5.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.5.5.2.1.1.1\" style=\"font-size:80%;\">Transformer (</span><span class=\"ltx_text\" id=\"S2.T2.5.5.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.5.5.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.5.5.8.1\">\n<span class=\"ltx_p\" id=\"S2.T2.5.5.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T2.5.5.8.1.1.1\" style=\"font-size:80%;\">Data </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.5.5.8.1.1.2\" style=\"font-size:80%;\">augmentation </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.5.5.8.1.1.3\" style=\"font-size:80%;\">and IBP-based</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.7.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T2.7.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.7.7.3.1\">\n<span class=\"ltx_p\" id=\"S2.T2.7.7.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.7.7.3.1.1.1\" style=\"font-size:80%;\">Zeng et al. (2023) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T2.7.7.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib111\" title=\"\">111</a><span class=\"ltx_text\" id=\"S2.T2.7.7.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S2.T2.6.6.1.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T2.6.6.1.1.1.1\" style=\"font-size:80%;\">Randomised smoothing (</span><span class=\"ltx_text\" id=\"S2.T2.6.6.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.7.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.7.7.4.1\">\n<span class=\"ltx_p\" id=\"S2.T2.7.7.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T2.7.7.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.7.7.4.1.1.2\" style=\"font-size:80%;\">Probabilistic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.7.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.7.7.5.1\">\n<span class=\"ltx_p\" id=\"S2.T2.7.7.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.7.7.5.1.1.1\" style=\"font-size:80%;\">IMDB, YELP</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.7.7.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.7.7.6.1\">\n<span class=\"ltx_p\" id=\"S2.T2.7.7.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T2.7.7.6.1.1.1\" style=\"font-size:80%;\">Char and word substitution</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.7.7.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.7.7.7.1\">\n<span class=\"ltx_p\" id=\"S2.T2.7.7.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.7.7.7.1.1.1\" style=\"font-size:80%;\">Word: not specified</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.7.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.7.7.2.1\">\n<span class=\"ltx_p\" id=\"S2.T2.7.7.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.7.7.2.1.1.1\" style=\"font-size:80%;\">Transformer (</span><span class=\"ltx_text\" id=\"S2.T2.7.7.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.7.7.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.7.7.8.1\">\n<span class=\"ltx_p\" id=\"S2.T2.7.7.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T2.7.7.8.1.1.1\" style=\"font-size:80%;\">Data </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.7.7.8.1.1.2\" style=\"font-size:80%;\">augmentation</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.9.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T2.9.9.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.9.9.3.1\">\n<span class=\"ltx_p\" id=\"S2.T2.9.9.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.9.9.3.1.1.1\" style=\"font-size:80%;\">Ye et al. (2023) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T2.9.9.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib112\" title=\"\">112</a><span class=\"ltx_text\" id=\"S2.T2.9.9.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.8.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.8.8.1.1\">\n<span class=\"ltx_p\" id=\"S2.T2.8.8.1.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T2.8.8.1.1.1.1\" style=\"font-size:80%;\">Randomised smoothing (</span><span class=\"ltx_text\" id=\"S2.T2.8.8.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.9.9.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.9.9.4.1\">\n<span class=\"ltx_p\" id=\"S2.T2.9.9.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T2.9.9.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.9.9.4.1.1.2\" style=\"font-size:80%;\">Probabilistic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.9.9.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.9.9.5.1\">\n<span class=\"ltx_p\" id=\"S2.T2.9.9.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.9.9.5.1.1.1\" style=\"font-size:80%;\">IMDB, SST2, YELP, AGNews</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.9.9.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.9.9.6.1\">\n<span class=\"ltx_p\" id=\"S2.T2.9.9.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T2.9.9.6.1.1.1\" style=\"font-size:80%;\">Word substitution</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.9.9.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.9.9.7.1\">\n<span class=\"ltx_p\" id=\"S2.T2.9.9.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.9.9.7.1.1.1\" style=\"font-size:80%;\">Word: not specified</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.9.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.9.9.2.1\">\n<span class=\"ltx_p\" id=\"S2.T2.9.9.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.9.9.2.1.1.1\" style=\"font-size:80%;\">Transformer (</span><span class=\"ltx_text\" id=\"S2.T2.9.9.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.9.9.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.9.9.8.1\">\n<span class=\"ltx_p\" id=\"S2.T2.9.9.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T2.9.9.8.1.1.1\" style=\"font-size:80%;\">Data </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.9.9.8.1.1.2\" style=\"font-size:80%;\">augmentation</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.11.11\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T2.11.11.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.11.11.3.1\">\n<span class=\"ltx_p\" id=\"S2.T2.11.11.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.11.11.3.1.1.1\" style=\"font-size:80%;\">Zhang et al. (2023) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T2.11.11.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib113\" title=\"\">113</a><span class=\"ltx_text\" id=\"S2.T2.11.11.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.10.10.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.10.10.1.1\">\n<span class=\"ltx_p\" id=\"S2.T2.10.10.1.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T2.10.10.1.1.1.1\" style=\"font-size:80%;\">Randomised smoothing (</span><span class=\"ltx_text\" id=\"S2.T2.10.10.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.11.11.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.11.11.4.1\">\n<span class=\"ltx_p\" id=\"S2.T2.11.11.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T2.11.11.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.11.11.4.1.1.2\" style=\"font-size:80%;\">Probabilistic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.11.11.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.11.11.5.1\">\n<span class=\"ltx_p\" id=\"S2.T2.11.11.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.11.11.5.1.1.1\" style=\"font-size:80%;\">IMDB, Amazon, AGNews</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.11.11.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.11.11.6.1\">\n<span class=\"ltx_p\" id=\"S2.T2.11.11.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T2.11.11.6.1.1.1\" style=\"font-size:80%;\">Word perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.11.11.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.11.11.7.1\">\n<span class=\"ltx_p\" id=\"S2.T2.11.11.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.11.11.7.1.1.1\" style=\"font-size:80%;\">Word: GloVe</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.11.11.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.11.11.2.1\">\n<span class=\"ltx_p\" id=\"S2.T2.11.11.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.11.11.2.1.1.1\" style=\"font-size:80%;\">LSTM, Transformer (</span><span class=\"ltx_text\" id=\"S2.T2.11.11.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T2.11.11.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.11.11.8.1\">\n<span class=\"ltx_p\" id=\"S2.T2.11.11.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T2.11.11.8.1.1.1\" style=\"font-size:80%;\">Data </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.11.11.8.1.1.2\" style=\"font-size:80%;\">augmentation</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.13.13\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_l ltx_border_r ltx_border_t\" id=\"S2.T2.13.13.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.13.3.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.13.3.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.13.13.3.1.1.1\" style=\"font-size:80%;\">Zhang et al. (2023) </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T2.13.13.3.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib114\" title=\"\">114</a><span class=\"ltx_text\" id=\"S2.T2.13.13.3.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T2.12.12.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.12.12.1.1\">\n<span class=\"ltx_p\" id=\"S2.T2.12.12.1.1.1\" style=\"width:77.2pt;\"><span class=\"ltx_text\" id=\"S2.T2.12.12.1.1.1.1\" style=\"font-size:80%;\">Randomised smoothing (</span><span class=\"ltx_text\" id=\"S2.T2.12.12.1.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T2.13.13.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.13.4.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.13.4.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S2.T2.13.13.4.1.1.1\" style=\"font-size:80%;\">Incomplete, </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S2.T2.13.13.4.1.1.2\" style=\"font-size:80%;\">Probabilistic</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T2.13.13.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.13.5.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.13.5.1.1\" style=\"width:69.4pt;\"><span class=\"ltx_text\" id=\"S2.T2.13.13.5.1.1.1\" style=\"font-size:80%;\">SST2, AGNews</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T2.13.13.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.13.6.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.13.6.1.1\" style=\"width:75.9pt;\"><span class=\"ltx_text\" id=\"S2.T2.13.13.6.1.1.1\" style=\"font-size:80%;\">Word perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T2.13.13.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.13.7.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.13.7.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.13.13.7.1.1.1\" style=\"font-size:80%;\">Word: not specified</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T2.13.13.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.13.2.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.13.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text\" id=\"S2.T2.13.13.2.1.1.1\" style=\"font-size:80%;\">Transformer (</span><span class=\"ltx_text\" id=\"S2.T2.13.13.2.1.1.2\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T2.13.13.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T2.13.13.8.1\">\n<span class=\"ltx_p\" id=\"S2.T2.13.13.8.1.1\" style=\"width:104.1pt;\"><span class=\"ltx_text\" id=\"S2.T2.13.13.8.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S2.T2.17.1.1\" style=\"font-size:113%;\">Table 2</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S2.T2.18.2\" style=\"font-size:113%;\">Summary of the main features of the existing randomised smoothing approaches.</em></figcaption>\n</figure>",
            "capture": "Table 2: Summary of the main features of the existing randomised smoothing approaches."
        },
        "3": {
            "table_html": "<figure class=\"ltx_table\" id=\"S2.T3\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T3.11\">\n<tr class=\"ltx_tr\" id=\"S2.T3.11.12\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T3.11.12.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.12.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.12.1.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.11.12.1.1.1.1\" style=\"font-size:80%;\">Dataset</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T3.11.12.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.12.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.12.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.11.12.2.1.1.1\" style=\"font-size:80%;\">Safety Critical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T3.11.12.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.12.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.12.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.11.12.3.1.1.1\" style=\"font-size:80%;\">Category</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" id=\"S2.T3.11.12.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.12.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.12.4.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.11.12.4.1.1.1\" style=\"font-size:80%;\">Tasks</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S2.T3.11.12.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.12.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.12.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.11.12.5.1.1.1\" style=\"font-size:80%;\">Size</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S2.T3.11.12.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.12.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.12.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T3.11.12.6.1.1.1\" style=\"font-size:80%;\">Classes</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.1.1.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.1.1.2.1.1.1\" style=\"font-size:80%;\">IMDB\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.1.1.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib119\" title=\"\">119</a><span class=\"ltx_text\" id=\"S2.T3.1.1.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.1.1.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.1.1.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.1.1.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.1.1.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.1.1.3.1.1.1\" style=\"font-size:80%;\">Sentiment analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.1.1.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.1.1.4.1.1.1\" style=\"font-size:80%;\">Document-level and sentence-level classification</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.1.1.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.1.1.5.1.1.1\" style=\"font-size:80%;\">25,000</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.1.1.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.1.1.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.1.1.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.1.1.6.1.1.1\" style=\"font-size:80%;\">2</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.2.2.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.2.2.2.1.1.1\" style=\"font-size:80%;\">SST\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.2.2.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib120\" title=\"\">120</a><span class=\"ltx_text\" id=\"S2.T3.2.2.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.2.2.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.2.2.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.2.2.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.2.2.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.2.2.3.1.1.1\" style=\"font-size:80%;\">Sentiment analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.2.2.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.2.2.4.1.1.1\" style=\"font-size:80%;\">Sentiment classification, hierarchical sentiment classification, sentiment span detection</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.2.2.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.2.2.5.1.1.1\" style=\"font-size:80%;\">70,042</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.2.2.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.2.2.6.1.1.1\" style=\"font-size:80%;\">5</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.3.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.3.3.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.3.3.2.1.1.1\" style=\"font-size:80%;\">SST2\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.3.3.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib120\" title=\"\">120</a><span class=\"ltx_text\" id=\"S2.T3.3.3.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.3.3.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.3.3.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.3.3.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.3.3.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.3.3.3.1.1.1\" style=\"font-size:80%;\">Sentiment analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.3.3.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.3.3.4.1.1.1\" style=\"font-size:80%;\">Sentiment classification</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.3.3.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.3.3.5.1.1.1\" style=\"font-size:80%;\">70,042</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.3.3.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.3.3.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.3.3.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.3.3.6.1.1.1\" style=\"font-size:80%;\">2</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.4.4.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.4.4.2.1.1.1\" style=\"font-size:80%;\">YELP\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.4.4.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib121\" title=\"\">121</a><span class=\"ltx_text\" id=\"S2.T3.4.4.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.4.4.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.4.4.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.4.4.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.4.4.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.4.4.3.1.1.1\" style=\"font-size:80%;\">Sentiment analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.4.4.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.4.4.4.1.1.1\" style=\"font-size:80%;\">Sentiment classification</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.4.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.4.4.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.4.4.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.4.4.5.1.1.1\" style=\"font-size:80%;\">570,771</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.4.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.4.4.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.4.4.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.4.4.6.1.1.1\" style=\"font-size:80%;\">2</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.5.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.5.5.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.5.5.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.5.5.2.1.1.1\" style=\"font-size:80%;\">Rotten Tomatoes Movie Review\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.5.5.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib122\" title=\"\">122</a><span class=\"ltx_text\" id=\"S2.T3.5.5.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.5.5.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.5.5.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.5.5.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.5.5.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.5.5.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.5.5.3.1.1.1\" style=\"font-size:80%;\">Sentiment analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.5.5.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.5.5.4.1.1.1\" style=\"font-size:80%;\">Sentiment classification</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.5.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.5.5.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.5.5.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.5.5.5.1.1.1\" style=\"font-size:80%;\">48,869</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.5.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.5.5.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.5.5.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.5.5.6.1.1.1\" style=\"font-size:80%;\">3/4</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.6.6.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.6.6.2.1.1.1\" style=\"font-size:80%;\">Amazon\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.6.6.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib123\" title=\"\">123</a><span class=\"ltx_text\" id=\"S2.T3.6.6.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.6.6.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.6.6.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.6.6.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.6.6.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.6.6.3.1.1.1\" style=\"font-size:80%;\">Sentiment analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.6.6.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.6.6.4.1.1.1\" style=\"font-size:80%;\">Sentiment classification, aspect-based sentiment analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.6.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.6.6.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.6.6.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.6.6.5.1.1.1\" style=\"font-size:80%;\">34,686,770</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.6.6.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.6.6.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.6.6.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.6.6.6.1.1.1\" style=\"font-size:80%;\">5</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.7.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.7.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.7.7.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.7.7.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.7.7.2.1.1.1\" style=\"font-size:80%;\">SNLI\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.7.7.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib117\" title=\"\">117</a><span class=\"ltx_text\" id=\"S2.T3.7.7.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.7.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.7.7.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.7.7.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.7.7.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.7.7.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.7.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.7.7.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.7.7.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.7.7.3.1.1.1\" style=\"font-size:80%;\">Semantic inference</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.7.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.7.7.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.7.7.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.7.7.4.1.1.1\" style=\"font-size:80%;\">Natural language inference, semantic similarity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.7.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.7.7.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.7.7.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.7.7.5.1.1.1\" style=\"font-size:80%;\">570,152</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.7.7.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.7.7.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.7.7.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.7.7.6.1.1.1\" style=\"font-size:80%;\">3</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.8.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.8.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.8.8.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.8.8.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.8.8.2.1.1.1\" style=\"font-size:80%;\">MNLI\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.8.8.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib124\" title=\"\">124</a><span class=\"ltx_text\" id=\"S2.T3.8.8.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.8.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.8.8.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.8.8.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.8.8.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.8.8.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.8.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.8.8.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.8.8.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.8.8.3.1.1.1\" style=\"font-size:80%;\">Semantic inference</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.8.8.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.8.8.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.8.8.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.8.8.4.1.1.1\" style=\"font-size:80%;\">Natural language inference, semantic similarity, generalisation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.8.8.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.8.8.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.8.8.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.8.8.5.1.1.1\" style=\"font-size:80%;\">432,702</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.8.8.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.8.8.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.8.8.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.8.8.6.1.1.1\" style=\"font-size:80%;\">3</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.9.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.9.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.9.9.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.9.9.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.9.9.2.1.1.1\" style=\"font-size:80%;\">AGNews\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.9.9.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib125\" title=\"\">125</a><span class=\"ltx_text\" id=\"S2.T3.9.9.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.9.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.9.9.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.9.9.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.9.9.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.9.9.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.9.9.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.9.9.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.9.9.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.9.9.3.1.1.1\" style=\"font-size:80%;\">Text analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.9.9.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.9.9.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.9.9.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.9.9.4.1.1.1\" style=\"font-size:80%;\">Text classification, sentiment classification</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.9.9.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.9.9.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.9.9.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.9.9.5.1.1.1\" style=\"font-size:80%;\">127,600</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.9.9.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.9.9.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.9.9.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.9.9.6.1.1.1\" style=\"font-size:80%;\">4</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.10.10\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.10.10.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.10.10.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.10.10.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.10.10.2.1.1.1\" style=\"font-size:80%;\">CogComp QC\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.10.10.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib126\" title=\"\">126</a><span class=\"ltx_text\" id=\"S2.T3.10.10.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.10.10.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.10.10.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.10.10.1.1.1\" style=\"width:30.4pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.10.10.1.1.1.1\" style=\"width:5.3pt;height:5.4pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.5pt,0.0pt) scale(0.85,1.0) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.10.10.1.1.1.1.1\"></span>\n</span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.10.10.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.10.10.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.10.10.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.10.10.3.1.1.1\" style=\"font-size:80%;\">Text analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S2.T3.10.10.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.10.10.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.10.10.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.10.10.4.1.1.1\" style=\"font-size:80%;\">Question classification, semantic understanding</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S2.T3.10.10.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.10.10.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.10.10.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.10.10.5.1.1.1\" style=\"font-size:80%;\">15,000</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T3.10.10.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.10.10.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.10.10.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.10.10.6.1.1.1\" style=\"font-size:80%;\">6/50</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.11.11\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T3.11.11.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.11.2.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.11.2.1.1\" style=\"width:47.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.11.11.2.1.1.1\" style=\"font-size:80%;\">Toxic Comment\u00a0</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S2.T3.11.11.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib118\" title=\"\">118</a><span class=\"ltx_text\" id=\"S2.T3.11.11.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T3.11.11.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.11.1.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.11.1.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S2.T3.11.11.1.1.1.1\" style=\"font-size:80%;position:relative; bottom:2.1pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" id=\"S2.T3.11.11.1.1.1.1.1\" style=\"width:0.0pt;height:0pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(0.7,0.7) ;\">\n<span class=\"ltx_p\" id=\"S2.T3.11.11.1.1.1.1.1.1\"></span>\n</span></span></span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T3.11.11.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.11.3.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.11.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.11.11.3.1.1.1\" style=\"font-size:80%;\">Text analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T3.11.11.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.11.4.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.11.4.1.1\"><span class=\"ltx_text\" id=\"S2.T3.11.11.4.1.1.1\" style=\"font-size:80%;\">Toxic comment classification, fine-grained toxicity analysis, bias analysis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T3.11.11.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.11.5.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.11.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S2.T3.11.11.5.1.1.1\" style=\"font-size:80%;\">18,560</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"S2.T3.11.11.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T3.11.11.6.1\">\n<span class=\"ltx_p\" id=\"S2.T3.11.11.6.1.1\" style=\"width:26.0pt;\"><span class=\"ltx_text\" id=\"S2.T3.11.11.6.1.1.1\" style=\"font-size:80%;\">6</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S2.T3.15.1.1\" style=\"font-size:113%;\">Table 3</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S2.T3.16.2\" style=\"font-size:113%;\">Summary of the main features of the datasets used in NLP verification.</em></figcaption>\n</figure>",
            "capture": "Table 3: Summary of the main features of the datasets used in NLP verification."
        },
        "4": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T4\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T4.2\">\n<tr class=\"ltx_tr\" id=\"S3.T4.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S3.T4.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.1.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.2.1.1.1.1.1\" style=\"font-size:80%;\">Method</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" id=\"S3.T4.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.1.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.2.1.2.1.1.1\" style=\"font-size:80%;\">Description</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S3.T4.2.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.1.3.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.1.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.2.1.3.1.1.1\" style=\"font-size:80%;\">Altered sentence</span><span class=\"ltx_text\" id=\"S3.T4.2.1.3.1.1.2\" style=\"font-size:80%;\"> </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S3.T4.2.1.3.1.1.3\" style=\"font-size:80%;\">(</span><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T4.2.1.3.1.1.4\" style=\"font-size:80%;\">Are you a robot?</em><span class=\"ltx_text\" id=\"S3.T4.2.1.3.1.1.5\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T4.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.2.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.2.2.1.1.1.1\" style=\"font-size:80%;\">Insertion</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T4.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.2.2.1.1\"><span class=\"ltx_text\" id=\"S3.T4.2.2.2.1.1.1\" style=\"font-size:80%;\">A character is randomly selected and inserted in a random position.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T4.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.2.3.1.1\" style=\"width:65.0pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T4.2.2.3.1.1.1\" style=\"font-size:80%;\">Are yo<span class=\"ltx_text\" id=\"S3.T4.2.2.3.1.1.1.1\" style=\"color:#FF0000;\">v</span>u a robot?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.2.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T4.2.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.3.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.3.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.2.3.1.1.1.1\" style=\"font-size:80%;\">Deletion</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T4.2.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.3.2.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.3.2.1.1\"><span class=\"ltx_text\" id=\"S3.T4.2.3.2.1.1.1\" style=\"font-size:80%;\">A character is randomly selected and deleted.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T4.2.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.3.3.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.3.3.1.1\" style=\"width:65.0pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T4.2.3.3.1.1.1\" style=\"font-size:80%;\">Are you a robt?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.2.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T4.2.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.4.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.4.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.2.4.1.1.1.1\" style=\"font-size:80%;\">Replacement</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T4.2.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.4.2.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.4.2.1.1\"><span class=\"ltx_text\" id=\"S3.T4.2.4.2.1.1.1\" style=\"font-size:80%;\">A character is randomly selected and replaced by an adjacent character on the keyboard.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T4.2.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.4.3.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.4.3.1.1\" style=\"width:65.0pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T4.2.4.3.1.1.1\" style=\"font-size:80%;\">Are you a ro<span class=\"ltx_text\" id=\"S3.T4.2.4.3.1.1.1.1\" style=\"color:#FF0000;\">n</span>ot?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.2.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T4.2.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.5.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.5.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.2.5.1.1.1.1\" style=\"font-size:80%;\">Swapping</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T4.2.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.5.2.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.5.2.1.1\"><span class=\"ltx_text\" id=\"S3.T4.2.5.2.1.1.1\" style=\"font-size:80%;\">A character is randomly selected and swapped with the adjacent right or left character in the word.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T4.2.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.5.3.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.5.3.1.1\" style=\"width:65.0pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T4.2.5.3.1.1.1\" style=\"font-size:80%;\">Are you a r<span class=\"ltx_text\" id=\"S3.T4.2.5.3.1.1.1.1\" style=\"color:#FF0000;\">bo</span>ot?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.2.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T4.2.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.6.1.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.6.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S3.T4.2.6.1.1.1.1\" style=\"font-size:80%;\">Repetition</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T4.2.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.6.2.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.6.2.1.1\"><span class=\"ltx_text\" id=\"S3.T4.2.6.2.1.1.1\" style=\"font-size:80%;\">A character in a random position is selected and duplicated.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"S3.T4.2.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T4.2.6.3.1\">\n<span class=\"ltx_p\" id=\"S3.T4.2.6.3.1.1\" style=\"width:65.0pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T4.2.6.3.1.1.1\" style=\"font-size:80%;\">Ar<span class=\"ltx_text\" id=\"S3.T4.2.6.3.1.1.1.1\" style=\"color:#FF0000;\">r</span>e you a robot?</em></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T4.5.1.1\" style=\"font-size:113%;\">Table 4</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T4.6.2\" style=\"font-size:113%;\">Character-level perturbations: their types and examples of how each type acts on a given sentence from the R-U-A-Robot dataset\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib129\" title=\"\">129</a>]</cite>. Perturbations are selected from random words that have 3 or more characters, first and last characters of a word are never perturbed.</em></figcaption>\n</figure>",
            "capture": "Table 4: Character-level perturbations: their types and examples of how each type acts on a given sentence from the R-U-A-Robot dataset\u00a0[129]. Perturbations are selected from random words that have 3 or more characters, first and last characters of a word are never perturbed."
        },
        "5": {
            "table_html": "<figure class=\"ltx_table\" id=\"S3.T5\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T5.2\">\n<tr class=\"ltx_tr\" id=\"S3.T5.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S3.T5.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.1.1.1.1\" style=\"width:73.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.2.1.1.1.1.1\" style=\"font-size:80%;\">Method</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" id=\"S3.T5.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.1.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.2.1.2.1.1.1\" style=\"font-size:80%;\">Description</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S3.T5.2.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.1.3.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.1.3.1.1\" style=\"width:138.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T5.2.1.3.1.1.1\" style=\"font-size:80%;\">Altered sentence</span><span class=\"ltx_text\" id=\"S3.T5.2.1.3.1.1.2\" style=\"font-size:80%;\"> </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S3.T5.2.1.3.1.1.3\" style=\"font-size:80%;\">(</span><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T5.2.1.3.1.1.4\" style=\"font-size:80%;\">Can u tell me if you are a chatbot?</em><span class=\"ltx_text\" id=\"S3.T5.2.1.3.1.1.5\" style=\"font-size:80%;\">)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T5.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.2.1.1.1\" style=\"width:73.7pt;\"><span class=\"ltx_text\" id=\"S3.T5.2.2.1.1.1.1\" style=\"font-size:80%;\">Deletion</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T5.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.2.2.1.1\"><span class=\"ltx_text\" id=\"S3.T5.2.2.2.1.1.1\" style=\"font-size:80%;\">Randomly selects a word and removes it.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T5.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.2.3.1.1\" style=\"width:138.8pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T5.2.2.3.1.1.1\" style=\"font-size:80%;\">Can u tell if you are a chatbot?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.2.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T5.2.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.3.1.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.3.1.1.1\" style=\"width:73.7pt;\"><span class=\"ltx_text\" id=\"S3.T5.2.3.1.1.1.1\" style=\"font-size:80%;\">Repetition</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T5.2.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.3.2.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.3.2.1.1\"><span class=\"ltx_text\" id=\"S3.T5.2.3.2.1.1.1\" style=\"font-size:80%;\">Randomly selects a word and duplicates it.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T5.2.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.3.3.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.3.3.1.1\" style=\"width:138.8pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T5.2.3.3.1.1.1\" style=\"font-size:80%;\">Can <span class=\"ltx_text\" id=\"S3.T5.2.3.3.1.1.1.1\" style=\"color:#FF0000;\">can</span> u tell me if you are a chatbot?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.2.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T5.2.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.4.1.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.4.1.1.1\" style=\"width:73.7pt;\"><span class=\"ltx_text\" id=\"S3.T5.2.4.1.1.1.1\" style=\"font-size:80%;\">Negation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T5.2.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.4.2.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.4.2.1.1\"><span class=\"ltx_text\" id=\"S3.T5.2.4.2.1.1.1\" style=\"font-size:80%;\">Identifies verbs then flips them (negative/positive).</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T5.2.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.4.3.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.4.3.1.1\" style=\"width:138.8pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T5.2.4.3.1.1.1\" style=\"font-size:80%;\">Can u tell me if you are <span class=\"ltx_text\" id=\"S3.T5.2.4.3.1.1.1.1\" style=\"color:#FF0000;\">not</span> a chatbot?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.2.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T5.2.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.5.1.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.5.1.1.1\" style=\"width:73.7pt;\"><span class=\"ltx_text\" id=\"S3.T5.2.5.1.1.1.1\" style=\"font-size:80%;\">Singular/ plural verbs</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T5.2.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.5.2.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.5.2.1.1\"><span class=\"ltx_text\" id=\"S3.T5.2.5.2.1.1.1\" style=\"font-size:80%;\">Changes verbs to singular form, and conversely.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T5.2.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.5.3.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.5.3.1.1\" style=\"width:138.8pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T5.2.5.3.1.1.1\" style=\"font-size:80%;\">Can u tell me if you <span class=\"ltx_text\" id=\"S3.T5.2.5.3.1.1.1.1\" style=\"color:#FF0000;\">is</span> a chatbot?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.2.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S3.T5.2.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.6.1.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.6.1.1.1\" style=\"width:73.7pt;\"><span class=\"ltx_text\" id=\"S3.T5.2.6.1.1.1.1\" style=\"font-size:80%;\">Word order</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" id=\"S3.T5.2.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.6.2.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.6.2.1.1\"><span class=\"ltx_text\" id=\"S3.T5.2.6.2.1.1.1\" style=\"font-size:80%;\">Randomly selects consecutive words and changes the order in which they appear.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T5.2.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.6.3.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.6.3.1.1\" style=\"width:138.8pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T5.2.6.3.1.1.1\" style=\"font-size:80%;\">Can u tell me if you are <span class=\"ltx_text\" id=\"S3.T5.2.6.3.1.1.1.1\" style=\"color:#FF0000;\">chatbot a</span>?</em></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T5.2.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T5.2.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.7.1.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.7.1.1.1\" style=\"width:73.7pt;\"><span class=\"ltx_text\" id=\"S3.T5.2.7.1.1.1.1\" style=\"font-size:80%;\">Verb tense</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T5.2.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.7.2.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.7.2.1.1\"><span class=\"ltx_text\" id=\"S3.T5.2.7.2.1.1.1\" style=\"font-size:80%;\">Converts present simple or continuous verbs to their corresponding past simple or continuous form.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"S3.T5.2.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T5.2.7.3.1\">\n<span class=\"ltx_p\" id=\"S3.T5.2.7.3.1.1\" style=\"width:138.8pt;\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T5.2.7.3.1.1.1\" style=\"font-size:80%;\">Can u tell me if you <span class=\"ltx_text\" id=\"S3.T5.2.7.3.1.1.1.1\" style=\"color:#FF0000;\">were</span> a chatbot?</em></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S3.T5.5.1.1\" style=\"font-size:113%;\">Table 5</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S3.T5.6.2\" style=\"font-size:113%;\">Word-level perturbations: their types and examples of how each type acts on a given sentence from the R-U-A-Robot dataset\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#bib.bib129\" title=\"\">129</a>]</cite>.</em></figcaption>\n</figure>",
            "capture": "Table 5: Word-level perturbations: their types and examples of how each type acts on a given sentence from the R-U-A-Robot dataset\u00a0[129]."
        },
        "6": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T6\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T6.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T6.1.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.2.1.1.1\" style=\"width:39.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.1.1.1.1\" style=\"font-size:80%;\">Model</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T6.1.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.2.2.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.2.1.1.1\" style=\"font-size:80%;\">Adversarial training</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T6.1.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.2.3.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.2.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.3.1.1.1\" style=\"font-size:80%;\">Train Accuracy RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T6.1.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.2.4.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.2.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.4.1.1.1\" style=\"font-size:80%;\">Test Accuracy RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T6.1.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.2.5.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.2.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.5.1.1.1\" style=\"font-size:80%;\">Train Accuracy Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T6.1.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.2.6.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.2.6.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.2.6.1.1.1\" style=\"font-size:80%;\">Test Accuracy Medical</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.1.1.1\" style=\"width:39.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.2.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text\" id=\"S4.T6.1.1.2.1.1.1\" style=\"font-size:80%;\">No</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T6.1.1.3.1.1.1\" style=\"font-size:80%;\">93.87 \u00b1 0.14%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T6.1.1.4.1.1.1\" style=\"font-size:80%;\">93.57 \u00b1 0.18%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.5.1.1.1\" style=\"font-size:80%;\">96.32 \u00b1 0.05</span><span class=\"ltx_text\" id=\"S4.T6.1.1.5.1.1.2\" style=\"font-size:80%;\">%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"S4.T6.1.1.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T6.1.1.6.1\">\n<span class=\"ltx_p\" id=\"S4.T6.1.1.6.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T6.1.1.6.1.1.1\" style=\"font-size:80%;\">94.49 \u00b1 0.26%</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T6.5.1.1\" style=\"font-size:113%;\">Table 6</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T6.6.2\" style=\"font-size:113%;\">Mean and standard deviation of the accuracy of the baseline DNN on the RUAR and the Medical datasets. All experiments are replicated five times.</em></figcaption>\n</figure>",
            "capture": "Table 6: Mean and standard deviation of the accuracy of the baseline DNN on the RUAR and the Medical datasets. All experiments are replicated five times."
        },
        "7": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T7.16\">\n<tr class=\"ltx_tr\" id=\"S4.T7.16.17\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T7.16.17.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.17.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.17.1.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.16.17.1.1.1.1\" style=\"font-size:80%;\">Experiment name</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T7.16.17.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.17.2.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.17.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.16.17.2.1.1.1\" style=\"font-size:80%;\">Hyper-rectangles construction method</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T7.16.17.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.17.3.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.17.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.16.17.3.1.1.1\" style=\"font-size:80%;\">Avg. volume of hyper-rectangles RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T7.16.17.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.17.4.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.17.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.16.17.4.1.1.1\" style=\"font-size:80%;\">Number of hyper-rectangles RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T7.16.17.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.17.5.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.17.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.16.17.5.1.1.1\" style=\"font-size:80%;\">Avg. volume of hyper-rectangles Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T7.16.17.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.17.6.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.17.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.16.17.6.1.1.1\" style=\"font-size:80%;\">Number of hyper-rectangles Medical</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T7.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.1.1.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T7.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T7.2.2.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T7.2.2.2.1.1.1\" style=\"font-size:80%;\">Hyper-rectangle around the entire dataset shrunk to exclude all negative examples - </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T7.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S4.T7.2.2.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.2.2.3.1.1.1\" style=\"font-size:80%;\">7.55e-11</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T7.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S4.T7.2.2.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.2.2.4.1.1.1\" style=\"font-size:80%;\">1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T7.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S4.T7.2.2.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.2.2.5.1.1.1\" style=\"font-size:80%;\">2.60e-09</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T7.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S4.T7.2.2.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.2.2.6.1.1.1\" style=\"font-size:80%;\">1</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.3.3.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T7.4.4.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T7.4.4.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles on the dataset separated into 50 clusters - </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T7.4.4.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.4.4.3.1.1.1\" style=\"font-size:80%;\">1.02e-16</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T7.4.4.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.4.4.4.1.1.1\" style=\"font-size:80%;\">50</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.4.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.4.4.5.1\">\n<span class=\"ltx_p\" id=\"S4.T7.4.4.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.4.4.5.1.1.1\" style=\"font-size:80%;\">6.56e-15</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T7.4.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.4.4.6.1\">\n<span class=\"ltx_p\" id=\"S4.T7.4.4.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.4.4.6.1.1.1\" style=\"font-size:80%;\">50</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.5.5.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S4.T7.6.6.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T7.6.6.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles on the dataset separated into 100 clusters - </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S4.T7.6.6.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.6.6.3.1.1.1\" style=\"font-size:80%;\">6.23e-18</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S4.T7.6.6.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.6.6.4.1.1.1\" style=\"font-size:80%;\">100</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.6.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.6.6.5.1\">\n<span class=\"ltx_p\" id=\"S4.T7.6.6.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.6.6.5.1.1.1\" style=\"font-size:80%;\">3.25e-17</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T7.6.6.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.6.6.6.1\">\n<span class=\"ltx_p\" id=\"S4.T7.6.6.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.6.6.6.1.1.1\" style=\"font-size:80%;\">100</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.8.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.7.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.7.7.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.7.7.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.8.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.8.8.2.1\">\n<span class=\"ltx_p\" id=\"S4.T7.8.8.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T7.8.8.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles on the dataset separated into 200 clusters - </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.8.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.8.8.3.1\">\n<span class=\"ltx_p\" id=\"S4.T7.8.8.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.8.8.3.1.1.1\" style=\"font-size:80%;\">3.31e-20</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.8.8.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.8.8.4.1\">\n<span class=\"ltx_p\" id=\"S4.T7.8.8.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.8.8.4.1.1.1\" style=\"font-size:80%;\">200</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.8.8.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.8.8.5.1\">\n<span class=\"ltx_p\" id=\"S4.T7.8.8.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.8.8.5.1.1.1\" style=\"font-size:80%;\">4.67e-19</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T7.8.8.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.8.8.6.1\">\n<span class=\"ltx_p\" id=\"S4.T7.8.8.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.8.8.6.1.1.1\" style=\"font-size:80%;\">200</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.10.10\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.9.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.9.9.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.9.9.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.10.10.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.10.10.2.1\">\n<span class=\"ltx_p\" id=\"S4.T7.10.10.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T7.10.10.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles on the dataset separated into 250 clusters - </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.10.10.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.10.10.3.1\">\n<span class=\"ltx_p\" id=\"S4.T7.10.10.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.10.10.3.1.1.1\" style=\"font-size:80%;\">6.42e-22</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.10.10.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.10.10.4.1\">\n<span class=\"ltx_p\" id=\"S4.T7.10.10.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.10.10.4.1.1.1\" style=\"font-size:80%;\">250</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.10.10.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.10.10.5.1\">\n<span class=\"ltx_p\" id=\"S4.T7.10.10.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.10.10.5.1.1.1\" style=\"font-size:80%;\">2.42e-20</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T7.10.10.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.10.10.6.1\">\n<span class=\"ltx_p\" id=\"S4.T7.10.10.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.10.10.6.1.1.1\" style=\"font-size:80%;\">250</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.13.13\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.11.11.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.11.11.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.11.11.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.13.13.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.13.13.3.2\">\n<span class=\"ltx_p\" id=\"S4.T7.13.13.3.2.2\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T7.13.13.3.2.2.1\" style=\"font-size:80%;\">Set of </span><span class=\"ltx_text\" id=\"S4.T7.13.13.3.2.2.2\" style=\"font-size:80%;\"> around all positive sentences in the dataset - </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.13.13.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.13.13.4.1\">\n<span class=\"ltx_p\" id=\"S4.T7.13.13.4.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.13.13.4.1.1.1\" style=\"font-size:80%;\">1.00e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.13.13.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.13.13.5.1\">\n<span class=\"ltx_p\" id=\"S4.T7.13.13.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.13.13.5.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T7.13.13.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.13.13.6.1\">\n<span class=\"ltx_p\" id=\"S4.T7.13.13.6.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.13.13.6.1.1.1\" style=\"font-size:80%;\">1.00e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T7.13.13.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.13.13.7.1\">\n<span class=\"ltx_p\" id=\"S4.T7.13.13.7.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.13.13.7.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.16.16\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T7.14.14.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.14.14.1.1\">\n<span class=\"ltx_p\" id=\"S4.T7.14.14.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T7.16.16.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.16.3.2\">\n<span class=\"ltx_p\" id=\"S4.T7.16.16.3.2.2\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T7.16.16.3.2.2.1\" style=\"font-size:80%;\">Set of </span><span class=\"ltx_text\" id=\"S4.T7.16.16.3.2.2.2\" style=\"font-size:80%;\"> around all positive sentences in the dataset - </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T7.16.16.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.16.4.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.16.4.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.16.16.4.1.1.1\" style=\"font-size:80%;\">1.00e-60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T7.16.16.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.16.5.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.16.5.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.16.16.5.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T7.16.16.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.16.6.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.16.6.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T7.16.16.6.1.1.1\" style=\"font-size:80%;\">1.00e-60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T7.16.16.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T7.16.16.7.1\">\n<span class=\"ltx_p\" id=\"S4.T7.16.16.7.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T7.16.16.7.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T7.20.1.1\" style=\"font-size:113%;\">Table 7</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T7.21.2\" style=\"font-size:113%;\">Sets of geometric subspaces used in the experiments, their cardinality and average volumes of hyper-rectangles.\nAll shapes are eigenspace rotated for better precision.\n</em></figcaption>\n</figure>",
            "capture": "Table 7: Sets of geometric subspaces used in the experiments, their cardinality and average volumes of hyper-rectangles.\nAll shapes are eigenspace rotated for better precision.\n"
        },
        "8": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T8.9\">\n<tr class=\"ltx_tr\" id=\"S4.T8.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T8.7.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.7.7.8.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T8.7.7.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.7.7.9.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T8.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T8.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T8.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T8.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T8.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T8.6.6.6\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T8.7.7.7\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S4.T8.8.8.2\"><span class=\"ltx_text\" id=\"S4.T8.8.8.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T8.8.8.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T8.8.8.3\"><span class=\"ltx_text\" id=\"S4.T8.8.8.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T8.8.8.4\"><span class=\"ltx_text\" id=\"S4.T8.8.8.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T8.8.8.5\"><span class=\"ltx_text\" id=\"S4.T8.8.8.5.1\" style=\"font-size:80%;\">1.33%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T8.8.8.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.8.8.6.1\" style=\"font-size:80%;\">0.52</span><span class=\"ltx_text\" id=\"S4.T8.8.8.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T8.8.8.7\"><span class=\"ltx_text\" id=\"S4.T8.8.8.7.1\" style=\"font-size:80%;\">0.41%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T8.8.8.8\"><span class=\"ltx_text\" id=\"S4.T8.8.8.8.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T8.8.8.9\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.8.8.9.1\" style=\"font-size:80%;\">88.67</span><span class=\"ltx_text\" id=\"S4.T8.8.8.9.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T8.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T8.9.9.2\"><span class=\"ltx_text\" id=\"S4.T8.9.9.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T8.9.9.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T8.9.9.3\"><span class=\"ltx_text\" id=\"S4.T8.9.9.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T8.9.9.4\"><span class=\"ltx_text\" id=\"S4.T8.9.9.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T8.9.9.5\"><span class=\"ltx_text\" id=\"S4.T8.9.9.5.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T8.9.9.6\"><span class=\"ltx_text\" id=\"S4.T8.9.9.6.1\" style=\"font-size:80%;\">2.10%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T8.9.9.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.9.9.7.1\" style=\"font-size:80%;\">4.08</span><span class=\"ltx_text\" id=\"S4.T8.9.9.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T8.9.9.8\"><span class=\"ltx_text\" id=\"S4.T8.9.9.8.1\" style=\"font-size:80%;\">5.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T8.9.9.9\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T8.9.9.9.1\" style=\"font-size:80%;\">97.86</span><span class=\"ltx_text\" id=\"S4.T8.9.9.9.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T8.13.1.1\" style=\"font-size:113%;\">Table 8</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T8.14.2\" style=\"font-size:113%;\">Verifiability of the baseline DNN on the RUAR and the Medical datasets, for a selection\nof geometric subspaces; using the ERAN verifier.</em></figcaption>\n</figure>",
            "capture": "Table 8: Verifiability of the baseline DNN on the RUAR and the Medical datasets, for a selection\nof geometric subspaces; using the ERAN verifier."
        },
        "9": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T9.7\">\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T9.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T9.1.1.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.2.1.1.1\" style=\"font-size:80%;\">Dataset</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T9.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T9.1.1.3.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.3.1.1.1\" style=\"font-size:80%;\">Experiment</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T9.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T9.1.1.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.4.1.1.1\" style=\"font-size:80%;\">Avg. Volume of hyper-rectangles</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T9.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T9.1.1.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.5.1.1.1\" style=\"font-size:80%;\">Generalisability (%)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T9.1.1.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.1.1.6.1\">\n<span class=\"ltx_p\" id=\"S4.T9.1.1.6.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.6.1.1.1\" style=\"font-size:80%;\">Number of sentences contributing to generalisability</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T9.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T9.1.1.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.1.1.1.1\" style=\"font-size:80%;\">Total Sentences in </span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" id=\"S4.T9.2.2.2\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T9.2.2.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S4.T9.2.2.2.1.1.1\" style=\"font-size:80%;\">RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T9.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T9.2.2.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T9.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S4.T9.2.2.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.2.2.3.1.1.1\" style=\"font-size:80%;\">1.00e-60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T9.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S4.T9.2.2.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.2.2.4.1.1.1\" style=\"font-size:80%;\">1.95</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T9.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S4.T9.2.2.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.2.2.5.1.1.1\" style=\"font-size:80%;\">2821</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T9.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S4.T9.2.2.6.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.2.2.6.1.1.1\" style=\"font-size:80%;\">144500</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T9.3.3.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.3.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S4.T9.3.3.2.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.3.3.2.1.1.1\" style=\"font-size:80%;\">1.00e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S4.T9.3.3.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.3.3.3.1.1.1\" style=\"font-size:80%;\">38.47</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S4.T9.3.3.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.3.3.4.1.1.1\" style=\"font-size:80%;\">55592</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T9.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S4.T9.3.3.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.3.3.5.1.1.1\" style=\"font-size:80%;\">144500</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S4.T9.4.4.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T9.4.4.2.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.4.4.2.1.1.1\" style=\"font-size:80%;\">7.55e-11</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T9.4.4.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.4.4.3.1.1.1\" style=\"font-size:80%;\">50.91</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T9.4.4.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.4.4.4.1.1.1\" style=\"font-size:80%;\">73561</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T9.4.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.4.4.5.1\">\n<span class=\"ltx_p\" id=\"S4.T9.4.4.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.4.4.5.1.1.1\" style=\"font-size:80%;\">144500</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T9.5.5.2\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.5.5.2.1\">\n<span class=\"ltx_p\" id=\"S4.T9.5.5.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S4.T9.5.5.2.1.1.1\" style=\"font-size:80%;\">Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T9.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S4.T9.5.5.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T9.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.5.5.3.1\">\n<span class=\"ltx_p\" id=\"S4.T9.5.5.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.5.5.3.1.1.1\" style=\"font-size:80%;\">1.00e-60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T9.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S4.T9.5.5.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.5.5.4.1.1.1\" style=\"font-size:80%;\">0.09</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T9.5.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.5.5.5.1\">\n<span class=\"ltx_p\" id=\"S4.T9.5.5.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.5.5.5.1.1.1\" style=\"font-size:80%;\">10</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T9.5.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.5.5.6.1\">\n<span class=\"ltx_p\" id=\"S4.T9.5.5.6.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.5.5.6.1.1.1\" style=\"font-size:80%;\">11209</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S4.T9.6.6.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S4.T9.6.6.2.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.6.6.2.1.1.1\" style=\"font-size:80%;\">1.00e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S4.T9.6.6.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.6.6.3.1.1.1\" style=\"font-size:80%;\">28.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T9.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S4.T9.6.6.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.6.6.4.1.1.1\" style=\"font-size:80%;\">3194</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T9.6.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.6.6.5.1\">\n<span class=\"ltx_p\" id=\"S4.T9.6.6.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.6.6.5.1.1.1\" style=\"font-size:80%;\">11209</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T9.7.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T9.7.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.7.7.1.1\">\n<span class=\"ltx_p\" id=\"S4.T9.7.7.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T9.7.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.7.7.2.1\">\n<span class=\"ltx_p\" id=\"S4.T9.7.7.2.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.7.7.2.1.1.1\" style=\"font-size:80%;\">2.6e-09</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T9.7.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.7.7.3.1\">\n<span class=\"ltx_p\" id=\"S4.T9.7.7.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.7.7.3.1.1.1\" style=\"font-size:80%;\">37.13</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T9.7.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.7.7.4.1\">\n<span class=\"ltx_p\" id=\"S4.T9.7.7.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.7.7.4.1.1.1\" style=\"font-size:80%;\">4162</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T9.7.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T9.7.7.5.1\">\n<span class=\"ltx_p\" id=\"S4.T9.7.7.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T9.7.7.5.1.1.1\" style=\"font-size:80%;\">11209</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T9.20.6.1\" style=\"font-size:113%;\">Table 9</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T9.17.5\" style=\"font-size:113%;\">Generalisability of the selected geometric subspaces ,  and , measured on the sets of semantic perturbations  and .\n</em></figcaption>\n</figure>",
            "capture": "Table 9: Generalisability of the selected geometric subspaces ,  and , measured on the sets of semantic perturbations  and .\n"
        },
        "10": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T10\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T10.8\">\n<tr class=\"ltx_tr\" id=\"S4.T10.8.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T10.8.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.9.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.9.1.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.8.9.1.1.1.1\" style=\"font-size:80%;\">Experiment name</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T10.8.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.9.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.9.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.8.9.2.1.1.1\" style=\"font-size:80%;\">Hyper-rectangles construction method</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T10.8.9.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.9.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.9.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.8.9.3.1.1.1\" style=\"font-size:80%;\">Avg. volume of hyper-rectangles RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T10.8.9.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.9.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.9.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.8.9.4.1.1.1\" style=\"font-size:80%;\">Number of hyper-rectangles RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T10.8.9.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.9.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.9.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.8.9.5.1.1.1\" style=\"font-size:80%;\">Avg. volume of hyper-rectangles Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T10.8.9.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.9.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.9.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T10.8.9.6.1.1.1\" style=\"font-size:80%;\">Number of hyper-rectangles Medical</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T10.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.1.1.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T10.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.1.1.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T10.1.1.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles for character perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T10.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.1.1.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.1.1.3.1.1.1\" style=\"font-size:80%;\">1.54e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T10.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.1.1.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.1.1.4.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T10.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.1.1.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.1.1.5.1.1.1\" style=\"font-size:80%;\">7.66e-31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T10.1.1.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.1.1.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.1.1.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.1.1.6.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.2.2.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.2.2.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T10.2.2.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles for word perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.2.2.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.2.2.3.1.1.1\" style=\"font-size:80%;\">1.28e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.2.2.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.2.2.4.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.2.2.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.2.2.5.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T10.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.2.2.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.2.2.6.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.3.3.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.3.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.3.3.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T10.3.3.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles for polyjuice sentence perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.3.3.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.3.3.3.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.3.3.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.3.3.4.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.3.3.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.3.3.5.1.1.1\" style=\"font-size:80%;\">2.01e-28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T10.3.3.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.3.3.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.3.3.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.3.3.6.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.4.4.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.4.4.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T10.4.4.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles for swapping perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.4.4.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.4.4.3.1.1.1\" style=\"font-size:80%;\">1.57e-31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.4.4.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.4.4.4.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.4.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.4.4.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.4.4.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.4.4.5.1.1.1\" style=\"font-size:80%;\">3.42e-31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T10.4.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.4.4.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.4.4.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.4.4.6.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.5.5.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.5.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.5.5.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.5.5.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T10.5.5.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles for replacement perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.5.5.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.5.5.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.5.5.3.1.1.1\" style=\"font-size:80%;\">9.84e-31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.5.5.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.5.5.4.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.5.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.5.5.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.5.5.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.5.5.5.1.1.1\" style=\"font-size:80%;\">3.43e-31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T10.5.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.5.5.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.5.5.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.5.5.6.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.6.6.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.6.6.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T10.6.6.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles for deletion perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.6.6.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.6.6.3.1.1.1\" style=\"font-size:80%;\">3.46e-31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.6.6.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.6.6.4.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.6.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.6.6.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.6.6.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.6.6.5.1.1.1\" style=\"font-size:80%;\">1.24e-32</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T10.6.6.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.6.6.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.6.6.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.6.6.6.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.7.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.7.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.7.7.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.7.7.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.7.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.7.7.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.7.7.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T10.7.7.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles for insertion perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.7.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.7.7.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.7.7.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.7.7.3.1.1.1\" style=\"font-size:80%;\">3.21e-31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.7.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.7.7.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.7.7.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.7.7.4.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T10.7.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.7.7.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.7.7.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.7.7.5.1.1.1\" style=\"font-size:80%;\">9.11e-33</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T10.7.7.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.7.7.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.7.7.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.7.7.6.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T10.8.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T10.8.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.8.1.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.8.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T10.8.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.8.2.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.8.2.1.1\" style=\"width:160.4pt;\"><span class=\"ltx_text\" id=\"S4.T10.8.8.2.1.1.1\" style=\"font-size:80%;\">Set of hyper-rectangles for repetition perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T10.8.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.8.3.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.8.3.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.8.8.3.1.1.1\" style=\"font-size:80%;\">1.56e-31</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T10.8.8.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.8.4.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.8.4.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.8.8.4.1.1.1\" style=\"font-size:80%;\">3400</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T10.8.8.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.8.5.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.8.5.1.1\" style=\"width:41.2pt;\"><span class=\"ltx_text\" id=\"S4.T10.8.8.5.1.1.1\" style=\"font-size:80%;\">1.06e-32</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T10.8.8.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T10.8.8.6.1\">\n<span class=\"ltx_p\" id=\"S4.T10.8.8.6.1.1\" style=\"width:34.7pt;\"><span class=\"ltx_text\" id=\"S4.T10.8.8.6.1.1.1\" style=\"font-size:80%;\">989</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T10.12.1.1\" style=\"font-size:113%;\">Table 10</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T10.13.2\" style=\"font-size:113%;\">Sets of semantic subspaces used in the experiments, their cardinality and average volumes of hyper-rectangles. All shapes are eigenspace rotated for better precision.</em></figcaption>\n</figure>",
            "capture": "Table 10: Sets of semantic subspaces used in the experiments, their cardinality and average volumes of hyper-rectangles. All shapes are eigenspace rotated for better precision."
        },
        "11": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T11\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T11.11\">\n<tr class=\"ltx_tr\" id=\"S4.T11.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T11.9.9.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.9.9.10.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T11.9.9.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.9.9.11.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T11.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T11.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T11.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T11.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T11.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T11.6.6.6\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T11.7.7.7\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T11.8.8.8\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T11.9.9.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.2\"><span class=\"ltx_text\" id=\"S4.T11.10.10.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.3\"><span class=\"ltx_text\" id=\"S4.T11.10.10.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.4\"><span class=\"ltx_text\" id=\"S4.T11.10.10.4.1\" style=\"font-size:80%;\">1.80%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.5\"><span class=\"ltx_text\" id=\"S4.T11.10.10.5.1\" style=\"font-size:80%;\">0.87%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.6\"><span class=\"ltx_text\" id=\"S4.T11.10.10.6.1\" style=\"font-size:80%;\">1.62%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.7\"><span class=\"ltx_text\" id=\"S4.T11.10.10.7.1\" style=\"font-size:80%;\">2.63%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.8\"><span class=\"ltx_text\" id=\"S4.T11.10.10.8.1\" style=\"font-size:80%;\">1.66%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.9\"><span class=\"ltx_text\" id=\"S4.T11.10.10.9.1\" style=\"font-size:80%;\">0.94%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T11.10.10.10\"><span class=\"ltx_text\" id=\"S4.T11.10.10.10.1\" style=\"font-size:80%;\">2.07%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T11.10.10.11\"><span class=\"ltx_text\" id=\"S4.T11.10.10.11.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T11.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.2\"><span class=\"ltx_text\" id=\"S4.T11.11.11.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.3\"><span class=\"ltx_text\" id=\"S4.T11.11.11.3.1\" style=\"font-size:80%;\">5.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.4\"><span class=\"ltx_text\" id=\"S4.T11.11.11.4.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.5\"><span class=\"ltx_text\" id=\"S4.T11.11.11.5.1\" style=\"font-size:80%;\">39.71%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.6\"><span class=\"ltx_text\" id=\"S4.T11.11.11.6.1\" style=\"font-size:80%;\">39.62%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.7\"><span class=\"ltx_text\" id=\"S4.T11.11.11.7.1\" style=\"font-size:80%;\">44.66%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.8\"><span class=\"ltx_text\" id=\"S4.T11.11.11.8.1\" style=\"font-size:80%;\">48.71%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.9\"><span class=\"ltx_text\" id=\"S4.T11.11.11.9.1\" style=\"font-size:80%;\">37.49%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T11.11.11.10\"><span class=\"ltx_text\" id=\"S4.T11.11.11.10.1\" style=\"font-size:80%;\">42.60%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T11.11.11.11\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T11.11.11.11.1\" style=\"font-size:80%;\">50.09</span><span class=\"ltx_text\" id=\"S4.T11.11.11.11.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T11.15.1.1\" style=\"font-size:113%;\">Table 11</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T11.16.2\" style=\"font-size:113%;\">Verifiability of the baseline DNN on the RUAR and the Medical datasets, for a selection of semantic subspaces; using the ERAN verifier.</em></figcaption>\n</figure>",
            "capture": "Table 11: Verifiability of the baseline DNN on the RUAR and the Medical datasets, for a selection of semantic subspaces; using the ERAN verifier."
        },
        "12": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T12\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T12.11\">\n<tr class=\"ltx_tr\" id=\"S4.T12.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T12.9.9.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T12.9.9.10.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T12.9.9.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T12.9.9.11.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T12.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T12.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T12.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T12.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T12.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T12.6.6.6\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T12.7.7.7\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T12.8.8.8\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T12.9.9.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T12.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.2\"><span class=\"ltx_text\" id=\"S4.T12.10.10.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.3\"><span class=\"ltx_text\" id=\"S4.T12.10.10.3.1\" style=\"font-size:80%;\">1.79%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.4\"><span class=\"ltx_text\" id=\"S4.T12.10.10.4.1\" style=\"font-size:80%;\">11.69%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.5\"><span class=\"ltx_text\" id=\"S4.T12.10.10.5.1\" style=\"font-size:80%;\">4.88%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.6\"><span class=\"ltx_text\" id=\"S4.T12.10.10.6.1\" style=\"font-size:80%;\">4.35%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.7\"><span class=\"ltx_text\" id=\"S4.T12.10.10.7.1\" style=\"font-size:80%;\">9.72%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.8\"><span class=\"ltx_text\" id=\"S4.T12.10.10.8.1\" style=\"font-size:80%;\">9.46%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.9\"><span class=\"ltx_text\" id=\"S4.T12.10.10.9.1\" style=\"font-size:80%;\">5.65%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T12.10.10.10\"><span class=\"ltx_text\" id=\"S4.T12.10.10.10.1\" style=\"font-size:80%;\">8.07%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T12.10.10.11\"><span class=\"ltx_text\" id=\"S4.T12.10.10.11.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T12.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.2\"><span class=\"ltx_text\" id=\"S4.T12.11.11.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.3\"><span class=\"ltx_text\" id=\"S4.T12.11.11.3.1\" style=\"font-size:80%;\">37.96%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.4\"><span class=\"ltx_text\" id=\"S4.T12.11.11.4.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.5\"><span class=\"ltx_text\" id=\"S4.T12.11.11.5.1\" style=\"font-size:80%;\">64.03%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.6\"><span class=\"ltx_text\" id=\"S4.T12.11.11.6.1\" style=\"font-size:80%;\">64.15%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.7\"><span class=\"ltx_text\" id=\"S4.T12.11.11.7.1\" style=\"font-size:80%;\">64.65%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.8\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T12.11.11.8.1\" style=\"font-size:80%;\">66.83</span><span class=\"ltx_text\" id=\"S4.T12.11.11.8.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.9\"><span class=\"ltx_text\" id=\"S4.T12.11.11.9.1\" style=\"font-size:80%;\">64.75%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T12.11.11.10\"><span class=\"ltx_text\" id=\"S4.T12.11.11.10.1\" style=\"font-size:80%;\">64.36%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T12.11.11.11\"><span class=\"ltx_text\" id=\"S4.T12.11.11.11.1\" style=\"font-size:80%;\">61.57%</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T12.15.1.1\" style=\"font-size:113%;\">Table 12</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T12.16.2\" style=\"font-size:113%;\">Verifiability of the baseline DNN on the RUAR and the Medical datasets, for a selection of semantic subspaces; using the Marabou verifier.\n</em></figcaption>\n</figure>",
            "capture": "Table 12: Verifiability of the baseline DNN on the RUAR and the Medical datasets, for a selection of semantic subspaces; using the Marabou verifier.\n"
        },
        "13": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T13\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T13.7\">\n<tr class=\"ltx_tr\" id=\"S4.T13.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T13.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T13.1.1.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.1.1.2.1.1.1\" style=\"font-size:80%;\">Dataset</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T13.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T13.1.1.3.1.1\" style=\"width:52.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.1.1.3.1.1.1\" style=\"font-size:80%;\">Experiment</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T13.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T13.1.1.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.1.1.4.1.1.1\" style=\"font-size:80%;\">Avg. Volume of hyper-rectangles</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T13.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T13.1.1.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.1.1.5.1.1.1\" style=\"font-size:80%;\">Generalisability (%)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T13.1.1.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.1.1.6.1\">\n<span class=\"ltx_p\" id=\"S4.T13.1.1.6.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.1.1.6.1.1.1\" style=\"font-size:80%;\">Number of sentences contributing to generalisability</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T13.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T13.1.1.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.1.1.1.1.1.1\" style=\"font-size:80%;\">Total Sentences in </span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" id=\"S4.T13.2.2.2\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T13.2.2.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S4.T13.2.2.2.1.1.1\" style=\"font-size:80%;\">RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T13.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T13.2.2.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T13.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S4.T13.2.2.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.2.2.3.1.1.1\" style=\"font-size:80%;\">1.00e-60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T13.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S4.T13.2.2.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.2.2.4.1.1.1\" style=\"font-size:80%;\">1.95</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T13.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S4.T13.2.2.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.2.2.5.1.1.1\" style=\"font-size:80%;\">2821</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T13.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S4.T13.2.2.6.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.2.2.6.1.1.1\" style=\"font-size:80%;\">144500</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T13.3.3.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.3.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S4.T13.3.3.2.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.3.3.2.1.1.1\" style=\"font-size:80%;\">1.00e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S4.T13.3.3.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.3.3.3.1.1.1\" style=\"font-size:80%;\">38.47</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S4.T13.3.3.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.3.3.4.1.1.1\" style=\"font-size:80%;\">55592</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T13.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S4.T13.3.3.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.3.3.5.1.1.1\" style=\"font-size:80%;\">144500</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S4.T13.4.4.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T13.4.4.2.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.4.4.2.1.1.1\" style=\"font-size:80%;\">1.28e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T13.4.4.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.4.4.3.1.1.1\" style=\"font-size:80%;\">47.67</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T13.4.4.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.4.4.4.1.1.1\" style=\"font-size:80%;\">68882</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T13.4.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.4.4.5.1\">\n<span class=\"ltx_p\" id=\"S4.T13.4.4.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.4.4.5.1.1.1\" style=\"font-size:80%;\">144500</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T13.5.5.2\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.5.5.2.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.5.2.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text\" id=\"S4.T13.5.5.2.1.1.1\" style=\"font-size:80%;\">Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T13.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.5.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T13.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.5.5.3.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.5.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.5.5.3.1.1.1\" style=\"font-size:80%;\">1.00e-60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T13.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.5.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.5.5.4.1.1.1\" style=\"font-size:80%;\">0.09</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T13.5.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.5.5.5.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.5.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.5.5.5.1.1.1\" style=\"font-size:80%;\">10</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T13.5.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.5.5.6.1\">\n<span class=\"ltx_p\" id=\"S4.T13.5.5.6.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.5.5.6.1.1.1\" style=\"font-size:80%;\">11209</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S4.T13.6.6.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S4.T13.6.6.2.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.6.6.2.1.1.1\" style=\"font-size:80%;\">1.00e-30</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S4.T13.6.6.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.6.6.3.1.1.1\" style=\"font-size:80%;\">28.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T13.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S4.T13.6.6.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.6.6.4.1.1.1\" style=\"font-size:80%;\">3194</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T13.6.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.6.6.5.1\">\n<span class=\"ltx_p\" id=\"S4.T13.6.6.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.6.6.5.1.1.1\" style=\"font-size:80%;\">11209</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T13.7.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T13.7.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.7.7.1.1\">\n<span class=\"ltx_p\" id=\"S4.T13.7.7.1.1.1\" style=\"width:52.0pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T13.7.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.7.7.2.1\">\n<span class=\"ltx_p\" id=\"S4.T13.7.7.2.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.7.7.2.1.1.1\" style=\"font-size:80%;\">2.01e-28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T13.7.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.7.7.3.1\">\n<span class=\"ltx_p\" id=\"S4.T13.7.7.3.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.7.7.3.1.1.1\" style=\"font-size:80%;\">28.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T13.7.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.7.7.4.1\">\n<span class=\"ltx_p\" id=\"S4.T13.7.7.4.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T13.7.7.4.1.1.1\" style=\"font-size:80%;\">3222</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T13.7.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T13.7.7.5.1\">\n<span class=\"ltx_p\" id=\"S4.T13.7.7.5.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S4.T13.7.7.5.1.1.1\" style=\"font-size:80%;\">11209</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T13.29.10.1\" style=\"font-size:113%;\">Table 13</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T13.25.9\" style=\"font-size:113%;\">Generalisability of the selected geometric subspaces  and  and the semantic subspaces  and , measured on the sets of semantic perturbations  and .\nNote that the generalisability of  (Table\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#S4.T9\" title=\"Table 9 \u2023 4.3.2 Generalisability of Geometric Subspaces \u2023 4.3 Verifiability-Generalisability Trade-off for Geometric Subspaces \u2023 4 Characterisation of Verifiable Subspaces \u2023 NLP Verification: Towards a General Methodology for Certifying Robustness\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>), despite it having the volume 19 order of magnitudes bigger, is only  greater than .</em><span class=\"ltx_text\" id=\"S4.T13.30.11\" style=\"font-size:113%;\">\n</span></figcaption>\n</figure>",
            "capture": "Table 13: Generalisability of the selected geometric subspaces  and  and the semantic subspaces  and , measured on the sets of semantic perturbations  and .\nNote that the generalisability of  (Table\u00a09), despite it having the volume 19 order of magnitudes bigger, is only  greater than .\n"
        },
        "14": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T14\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T14.14\">\n<tr class=\"ltx_tr\" id=\"S4.T14.14.15\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T14.14.15.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.15.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.15.1.1.1\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T14.14.15.1.1.1.1\" style=\"font-size:80%;\">Model</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T14.14.15.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.15.2.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.15.2.1.1\" style=\"width:60.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T14.14.15.2.1.1.1\" style=\"font-size:80%;\">Dataset</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T14.14.15.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.15.3.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.15.3.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T14.14.15.3.1.1.1\" style=\"font-size:80%;\">Train Accuracy RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T14.14.15.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.15.4.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.15.4.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T14.14.15.4.1.1.1\" style=\"font-size:80%;\">Test Accuracy RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T14.14.15.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.15.5.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.15.5.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T14.14.15.5.1.1.1\" style=\"font-size:80%;\">Train Accuracy Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T14.14.15.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.15.6.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.15.6.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T14.14.15.6.1.1.1\" style=\"font-size:80%;\">Test Accuracy Medical</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T14.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.1.1.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T14.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T14.2.2.2.1.1\" style=\"width:60.7pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T14.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S4.T14.2.2.3.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.2.2.3.1.1.1\" style=\"font-size:80%;\">95.62 \u00b1 0.26%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T14.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S4.T14.2.2.4.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.2.2.4.1.1.1\" style=\"font-size:80%;\">93.20 \u00b1 0.35%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T14.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S4.T14.2.2.5.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.2.2.5.1.1.1\" style=\"font-size:80%;\">99.08 \u00b1 0.06%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T14.2.2.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.2.2.6.1\">\n<span class=\"ltx_p\" id=\"S4.T14.2.2.6.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.2.2.6.1.1.1\" style=\"font-size:80%;\">93.46 \u00b1 0.30%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.3.3.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T14.4.4.2.1.1\" style=\"width:60.7pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T14.4.4.3.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.4.4.3.1.1.1\" style=\"font-size:80%;\">98.57 \u00b1 0.06%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T14.4.4.4.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.4.4.4.1.1.1\" style=\"font-size:80%;\">94.59 \u00b1 0.36%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.4.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.4.4.5.1\">\n<span class=\"ltx_p\" id=\"S4.T14.4.4.5.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.4.4.5.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T14.4.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.4.4.6.1\">\n<span class=\"ltx_p\" id=\"S4.T14.4.4.6.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.4.4.6.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.5.5.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S4.T14.6.6.2.1.1\" style=\"width:60.7pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S4.T14.6.6.3.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.6.6.3.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S4.T14.6.6.4.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.6.6.4.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.6.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.6.6.5.1\">\n<span class=\"ltx_p\" id=\"S4.T14.6.6.5.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.6.6.5.1.1.1\" style=\"font-size:80%;\">98.19 \u00b1 0.09%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T14.6.6.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.6.6.6.1\">\n<span class=\"ltx_p\" id=\"S4.T14.6.6.6.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.6.6.6.1.1.1\" style=\"font-size:80%;\">93.19 \u00b1 0.39%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.8.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.7.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.7.7.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.7.7.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.8.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.8.8.2.1\">\n<span class=\"ltx_p\" id=\"S4.T14.8.8.2.1.1\" style=\"width:60.7pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.8.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.8.8.3.1\">\n<span class=\"ltx_p\" id=\"S4.T14.8.8.3.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.8.8.3.1.1.1\" style=\"font-size:80%;\">93.26 \u00b1 0.19%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.8.8.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.8.8.4.1\">\n<span class=\"ltx_p\" id=\"S4.T14.8.8.4.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.8.8.4.1.1.1\" style=\"font-size:80%;\">92.51 \u00b1 0.38%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.8.8.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.8.8.5.1\">\n<span class=\"ltx_p\" id=\"S4.T14.8.8.5.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.8.8.5.1.1.1\" style=\"font-size:80%;\">96.27 \u00b1 0.05%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T14.8.8.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.8.8.6.1\">\n<span class=\"ltx_p\" id=\"S4.T14.8.8.6.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.8.8.6.1.1.1\" style=\"font-size:80%;\">95.09 \u00b1 0.16%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.10.10\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.9.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.9.9.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.9.9.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.10.10.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.10.10.2.1\">\n<span class=\"ltx_p\" id=\"S4.T14.10.10.2.1.1\" style=\"width:60.7pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.10.10.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.10.10.3.1\">\n<span class=\"ltx_p\" id=\"S4.T14.10.10.3.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.10.10.3.1.1.1\" style=\"font-size:80%;\">93.68 \u00b1 0.16%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.10.10.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.10.10.4.1\">\n<span class=\"ltx_p\" id=\"S4.T14.10.10.4.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.10.10.4.1.1.1\" style=\"font-size:80%;\">92.37 \u00b1 0.29%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.10.10.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.10.10.5.1\">\n<span class=\"ltx_p\" id=\"S4.T14.10.10.5.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.10.10.5.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T14.10.10.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.10.10.6.1\">\n<span class=\"ltx_p\" id=\"S4.T14.10.10.6.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.10.10.6.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.12.12\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.11.11.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.11.11.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.11.11.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.12.12.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.12.12.2.1\">\n<span class=\"ltx_p\" id=\"S4.T14.12.12.2.1.1\" style=\"width:60.7pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.12.12.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.12.12.3.1\">\n<span class=\"ltx_p\" id=\"S4.T14.12.12.3.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.12.12.3.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.12.12.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.12.12.4.1\">\n<span class=\"ltx_p\" id=\"S4.T14.12.12.4.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.12.12.4.1.1.1\" style=\"font-size:80%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T14.12.12.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.12.12.5.1\">\n<span class=\"ltx_p\" id=\"S4.T14.12.12.5.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.12.12.5.1.1.1\" style=\"font-size:80%;\">95.05 \u00b1 0.19%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T14.12.12.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.12.12.6.1\">\n<span class=\"ltx_p\" id=\"S4.T14.12.12.6.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.12.12.6.1.1.1\" style=\"font-size:80%;\">93.49 \u00b1 0.32%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T14.14.14\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T14.13.13.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.13.13.1.1\">\n<span class=\"ltx_p\" id=\"S4.T14.13.13.1.1.1\" style=\"width:43.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T14.14.14.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.14.2.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.14.2.1.1\" style=\"width:60.7pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T14.14.14.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.14.3.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.14.3.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.14.14.3.1.1.1\" style=\"font-size:80%;\">94.01 \u00b1 0.17%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T14.14.14.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.14.4.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.14.4.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.14.14.4.1.1.1\" style=\"font-size:80%;\">92.24 \u00b1 0.19%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T14.14.14.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.14.5.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.14.5.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.14.14.5.1.1.1\" style=\"font-size:80%;\">96.05 \u00b1 0.09%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T14.14.14.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T14.14.14.6.1\">\n<span class=\"ltx_p\" id=\"S4.T14.14.14.6.1.1\" style=\"width:62.9pt;\"><span class=\"ltx_text\" id=\"S4.T14.14.14.6.1.1.1\" style=\"font-size:80%;\">95.04 \u00b1 0.24%</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T14.19.2.1\" style=\"font-size:113%;\">Table 14</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T14.16.1\" style=\"font-size:113%;\">Accuracy of the robustly trained DNNs on the RUAR and the Medical datasets.  stands for either RUAR or Medical depending on the column.\n</em></figcaption>\n</figure>",
            "capture": "Table 14: Accuracy of the robustly trained DNNs on the RUAR and the Medical datasets.  stands for either RUAR or Medical depending on the column.\n"
        },
        "15": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T15\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T15.19\">\n<tr class=\"ltx_tr\" id=\"S4.T15.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T15.9.9.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.9.9.10.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T15.9.9.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.9.9.11.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T15.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T15.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T15.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T15.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T15.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T15.6.6.6\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T15.7.7.7\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T15.8.8.8\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T15.9.9.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T15.10.10.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.3\"><span class=\"ltx_text\" id=\"S4.T15.10.10.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.4\"><span class=\"ltx_text\" id=\"S4.T15.10.10.4.1\" style=\"font-size:80%;\">0.24%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.5\"><span class=\"ltx_text\" id=\"S4.T15.10.10.5.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.6\"><span class=\"ltx_text\" id=\"S4.T15.10.10.6.1\" style=\"font-size:80%;\">0.51%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.7\"><span class=\"ltx_text\" id=\"S4.T15.10.10.7.1\" style=\"font-size:80%;\">1.38%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.8\"><span class=\"ltx_text\" id=\"S4.T15.10.10.8.1\" style=\"font-size:80%;\">1.09%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.9\"><span class=\"ltx_text\" id=\"S4.T15.10.10.9.1\" style=\"font-size:80%;\">0.35%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.10.10.10\"><span class=\"ltx_text\" id=\"S4.T15.10.10.10.1\" style=\"font-size:80%;\">1.06%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T15.10.10.11\"><span class=\"ltx_text\" id=\"S4.T15.10.10.11.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T15.11.11.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.11.11.2\"><span class=\"ltx_text\" id=\"S4.T15.11.11.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.11.11.3\"><span class=\"ltx_text\" id=\"S4.T15.11.11.3.1\" style=\"font-size:80%;\">0.24%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.11.11.4\"><span class=\"ltx_text\" id=\"S4.T15.11.11.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.11.11.5\"><span class=\"ltx_text\" id=\"S4.T15.11.11.5.1\" style=\"font-size:80%;\">0.42%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.11.11.6\"><span class=\"ltx_text\" id=\"S4.T15.11.11.6.1\" style=\"font-size:80%;\">0.31%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.11.11.7\"><span class=\"ltx_text\" id=\"S4.T15.11.11.7.1\" style=\"font-size:80%;\">0.57%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.11.11.8\"><span class=\"ltx_text\" id=\"S4.T15.11.11.8.1\" style=\"font-size:80%;\">0.25%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.11.11.9\"><span class=\"ltx_text\" id=\"S4.T15.11.11.9.1\" style=\"font-size:80%;\">0.92%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T15.11.11.10\"><span class=\"ltx_text\" id=\"S4.T15.11.11.10.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T15.12.12.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.12.12.2\"><span class=\"ltx_text\" id=\"S4.T15.12.12.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.12.12.3\"><span class=\"ltx_text\" id=\"S4.T15.12.12.3.1\" style=\"font-size:80%;\">8.97%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.12.12.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.12.12.4.1\" style=\"font-size:80%;\">4.43</span><span class=\"ltx_text\" id=\"S4.T15.12.12.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.12.12.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.12.12.5.1\" style=\"font-size:80%;\">4.81</span><span class=\"ltx_text\" id=\"S4.T15.12.12.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.12.12.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.12.12.6.1\" style=\"font-size:80%;\">9.86</span><span class=\"ltx_text\" id=\"S4.T15.12.12.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.12.12.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.12.12.7.1\" style=\"font-size:80%;\">11.3</span><span class=\"ltx_text\" id=\"S4.T15.12.12.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.12.12.8\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.12.12.8.1\" style=\"font-size:80%;\">6.91</span><span class=\"ltx_text\" id=\"S4.T15.12.12.8.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.12.12.9\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.12.12.9.1\" style=\"font-size:80%;\">8.51</span><span class=\"ltx_text\" id=\"S4.T15.12.12.9.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T15.12.12.10\"><span class=\"ltx_text\" id=\"S4.T15.12.12.10.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.13.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T15.13.13.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.13.13.2\"><span class=\"ltx_text\" id=\"S4.T15.13.13.2.1\" style=\"font-size:80%;\">0.04%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.13.13.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.13.13.3.1\" style=\"font-size:80%;\">10.75</span><span class=\"ltx_text\" id=\"S4.T15.13.13.3.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.13.13.4\"><span class=\"ltx_text\" id=\"S4.T15.13.13.4.1\" style=\"font-size:80%;\">4.05%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.13.13.5\"><span class=\"ltx_text\" id=\"S4.T15.13.13.5.1\" style=\"font-size:80%;\">4.36%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.13.13.6\"><span class=\"ltx_text\" id=\"S4.T15.13.13.6.1\" style=\"font-size:80%;\">8.60%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.13.13.7\"><span class=\"ltx_text\" id=\"S4.T15.13.13.7.1\" style=\"font-size:80%;\">9.52%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.13.13.8\"><span class=\"ltx_text\" id=\"S4.T15.13.13.8.1\" style=\"font-size:80%;\">6.81%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.13.13.9\"><span class=\"ltx_text\" id=\"S4.T15.13.13.9.1\" style=\"font-size:80%;\">7.45%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T15.13.13.10\"><span class=\"ltx_text\" id=\"S4.T15.13.13.10.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.14.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T15.14.14.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.14.14.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.14.14.2.1\" style=\"font-size:80%;\">0.12</span><span class=\"ltx_text\" id=\"S4.T15.14.14.2.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.14.14.3\"><span class=\"ltx_text\" id=\"S4.T15.14.14.3.1\" style=\"font-size:80%;\">10.16%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.14.14.4\"><span class=\"ltx_text\" id=\"S4.T15.14.14.4.1\" style=\"font-size:80%;\">4.18%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.14.14.5\"><span class=\"ltx_text\" id=\"S4.T15.14.14.5.1\" style=\"font-size:80%;\">4.04%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.14.14.6\"><span class=\"ltx_text\" id=\"S4.T15.14.14.6.1\" style=\"font-size:80%;\">8.91%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.14.14.7\"><span class=\"ltx_text\" id=\"S4.T15.14.14.7.1\" style=\"font-size:80%;\">10.17%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.14.14.8\"><span class=\"ltx_text\" id=\"S4.T15.14.14.8.1\" style=\"font-size:80%;\">6.52%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.14.14.9\"><span class=\"ltx_text\" id=\"S4.T15.14.14.9.1\" style=\"font-size:80%;\">7.36%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T15.14.14.10\"><span class=\"ltx_text\" id=\"S4.T15.14.14.10.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.15.15\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T15.15.15.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.3\"><span class=\"ltx_text\" id=\"S4.T15.15.15.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.4\"><span class=\"ltx_text\" id=\"S4.T15.15.15.4.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.5\"><span class=\"ltx_text\" id=\"S4.T15.15.15.5.1\" style=\"font-size:80%;\">7.59%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.6\"><span class=\"ltx_text\" id=\"S4.T15.15.15.6.1\" style=\"font-size:80%;\">5.28%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.7\"><span class=\"ltx_text\" id=\"S4.T15.15.15.7.1\" style=\"font-size:80%;\">12.84%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.8\"><span class=\"ltx_text\" id=\"S4.T15.15.15.8.1\" style=\"font-size:80%;\">11.05%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.9\"><span class=\"ltx_text\" id=\"S4.T15.15.15.9.1\" style=\"font-size:80%;\">7.92%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T15.15.15.10\"><span class=\"ltx_text\" id=\"S4.T15.15.15.10.1\" style=\"font-size:80%;\">7.40%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T15.15.15.11\"><span class=\"ltx_text\" id=\"S4.T15.15.15.11.1\" style=\"font-size:80%;\">26.97%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.16.16\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T15.16.16.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.16.16.2\"><span class=\"ltx_text\" id=\"S4.T15.16.16.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.16.16.3\"><span class=\"ltx_text\" id=\"S4.T15.16.16.3.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.16.16.4\"><span class=\"ltx_text\" id=\"S4.T15.16.16.4.1\" style=\"font-size:80%;\">10.31%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.16.16.5\"><span class=\"ltx_text\" id=\"S4.T15.16.16.5.1\" style=\"font-size:80%;\">8.49%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.16.16.6\"><span class=\"ltx_text\" id=\"S4.T15.16.16.6.1\" style=\"font-size:80%;\">15.67%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.16.16.7\"><span class=\"ltx_text\" id=\"S4.T15.16.16.7.1\" style=\"font-size:80%;\">14.90%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.16.16.8\"><span class=\"ltx_text\" id=\"S4.T15.16.16.8.1\" style=\"font-size:80%;\">9.18%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.16.16.9\"><span class=\"ltx_text\" id=\"S4.T15.16.16.9.1\" style=\"font-size:80%;\">10.58%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T15.16.16.10\"><span class=\"ltx_text\" id=\"S4.T15.16.16.10.1\" style=\"font-size:80%;\">28.59%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.17.17\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T15.17.17.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.17.17.2\"><span class=\"ltx_text\" id=\"S4.T15.17.17.2.1\" style=\"font-size:80%;\">5.28%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.17.17.3\"><span class=\"ltx_text\" id=\"S4.T15.17.17.3.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.17.17.4\"><span class=\"ltx_text\" id=\"S4.T15.17.17.4.1\" style=\"font-size:80%;\">50.12%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.17.17.5\"><span class=\"ltx_text\" id=\"S4.T15.17.17.5.1\" style=\"font-size:80%;\">49.78%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.17.17.6\"><span class=\"ltx_text\" id=\"S4.T15.17.17.6.1\" style=\"font-size:80%;\">53.99%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.17.17.7\"><span class=\"ltx_text\" id=\"S4.T15.17.17.7.1\" style=\"font-size:80%;\">57.76%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.17.17.8\"><span class=\"ltx_text\" id=\"S4.T15.17.17.8.1\" style=\"font-size:80%;\">48.02%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.17.17.9\"><span class=\"ltx_text\" id=\"S4.T15.17.17.9.1\" style=\"font-size:80%;\">52.07%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T15.17.17.10\"><span class=\"ltx_text\" id=\"S4.T15.17.17.10.1\" style=\"font-size:80%;\">55.44%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.18.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T15.18.18.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.18.18.2\"><span class=\"ltx_text\" id=\"S4.T15.18.18.2.1\" style=\"font-size:80%;\">2.83%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.18.18.3\"><span class=\"ltx_text\" id=\"S4.T15.18.18.3.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.18.18.4\"><span class=\"ltx_text\" id=\"S4.T15.18.18.4.1\" style=\"font-size:80%;\">47.11%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.18.18.5\"><span class=\"ltx_text\" id=\"S4.T15.18.18.5.1\" style=\"font-size:80%;\">46.14%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.18.18.6\"><span class=\"ltx_text\" id=\"S4.T15.18.18.6.1\" style=\"font-size:80%;\">52.12%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.18.18.7\"><span class=\"ltx_text\" id=\"S4.T15.18.18.7.1\" style=\"font-size:80%;\">56.14%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.18.18.8\"><span class=\"ltx_text\" id=\"S4.T15.18.18.8.1\" style=\"font-size:80%;\">44.59%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T15.18.18.9\"><span class=\"ltx_text\" id=\"S4.T15.18.18.9.1\" style=\"font-size:80%;\">48.27%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T15.18.18.10\"><span class=\"ltx_text\" id=\"S4.T15.18.18.10.1\" style=\"font-size:80%;\">57.36%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T15.19.19\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.19.19.2.1\" style=\"font-size:80%;\">8.68</span><span class=\"ltx_text\" id=\"S4.T15.19.19.2.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.3\"><span class=\"ltx_text\" id=\"S4.T15.19.19.3.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.19.19.4.1\" style=\"font-size:80%;\">51.60</span><span class=\"ltx_text\" id=\"S4.T15.19.19.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.19.19.5.1\" style=\"font-size:80%;\">50.31</span><span class=\"ltx_text\" id=\"S4.T15.19.19.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.19.19.6.1\" style=\"font-size:80%;\">55.67</span><span class=\"ltx_text\" id=\"S4.T15.19.19.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.19.19.7.1\" style=\"font-size:80%;\">58.52</span><span class=\"ltx_text\" id=\"S4.T15.19.19.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.8\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.19.19.8.1\" style=\"font-size:80%;\">50.10</span><span class=\"ltx_text\" id=\"S4.T15.19.19.8.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T15.19.19.9\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.19.19.9.1\" style=\"font-size:80%;\">53.65</span><span class=\"ltx_text\" id=\"S4.T15.19.19.9.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T15.19.19.10\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T15.19.19.10.1\" style=\"font-size:80%;\">59.76</span><span class=\"ltx_text\" id=\"S4.T15.19.19.10.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T15.23.1.1\" style=\"font-size:113%;\">Table 15</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T15.24.2\" style=\"font-size:113%;\">Verifiability of the robustly trained DNNs on the RUAR and the Medical datasets, for a selection of semantic subspaces; using the ERAN verifier.\n</em></figcaption>\n</figure>",
            "capture": "Table 15: Verifiability of the robustly trained DNNs on the RUAR and the Medical datasets, for a selection of semantic subspaces; using the ERAN verifier.\n"
        },
        "16": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T16\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T16.19\">\n<tr class=\"ltx_tr\" id=\"S4.T16.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T16.9.9.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.9.9.10.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T16.9.9.11\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.9.9.11.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T16.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T16.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T16.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T16.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T16.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T16.6.6.6\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T16.7.7.7\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T16.8.8.8\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T16.9.9.9\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T16.10.10.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.3\"><span class=\"ltx_text\" id=\"S4.T16.10.10.3.1\" style=\"font-size:80%;\">0.72%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.4\"><span class=\"ltx_text\" id=\"S4.T16.10.10.4.1\" style=\"font-size:80%;\">13.90%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.5\"><span class=\"ltx_text\" id=\"S4.T16.10.10.5.1\" style=\"font-size:80%;\">8.49%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.6\"><span class=\"ltx_text\" id=\"S4.T16.10.10.6.1\" style=\"font-size:80%;\">7.92%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.7\"><span class=\"ltx_text\" id=\"S4.T16.10.10.7.1\" style=\"font-size:80%;\">13.67%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.8\"><span class=\"ltx_text\" id=\"S4.T16.10.10.8.1\" style=\"font-size:80%;\">15.50%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.9\"><span class=\"ltx_text\" id=\"S4.T16.10.10.9.1\" style=\"font-size:80%;\">9.56%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.10.10.10\"><span class=\"ltx_text\" id=\"S4.T16.10.10.10.1\" style=\"font-size:80%;\">11.88%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T16.10.10.11\"><span class=\"ltx_text\" id=\"S4.T16.10.10.11.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T16.11.11.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.11.11.2\"><span class=\"ltx_text\" id=\"S4.T16.11.11.2.1\" style=\"font-size:80%;\">0.24%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.11.11.3\"><span class=\"ltx_text\" id=\"S4.T16.11.11.3.1\" style=\"font-size:80%;\">11.30%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.11.11.4\"><span class=\"ltx_text\" id=\"S4.T16.11.11.4.1\" style=\"font-size:80%;\">3.87%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.11.11.5\"><span class=\"ltx_text\" id=\"S4.T16.11.11.5.1\" style=\"font-size:80%;\">4.05%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.11.11.6\"><span class=\"ltx_text\" id=\"S4.T16.11.11.6.1\" style=\"font-size:80%;\">8.27%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.11.11.7\"><span class=\"ltx_text\" id=\"S4.T16.11.11.7.1\" style=\"font-size:80%;\">8.84%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.11.11.8\"><span class=\"ltx_text\" id=\"S4.T16.11.11.8.1\" style=\"font-size:80%;\">5.71%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.11.11.9\"><span class=\"ltx_text\" id=\"S4.T16.11.11.9.1\" style=\"font-size:80%;\">7.72%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T16.11.11.10\"><span class=\"ltx_text\" id=\"S4.T16.11.11.10.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T16.12.12.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.12.12.2\"><span class=\"ltx_text\" id=\"S4.T16.12.12.2.1\" style=\"font-size:80%;\">7.37%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.12.12.3\"><span class=\"ltx_text\" id=\"S4.T16.12.12.3.1\" style=\"font-size:80%;\">41.93%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.12.12.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.12.12.4.1\" style=\"font-size:80%;\">30.41</span><span class=\"ltx_text\" id=\"S4.T16.12.12.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.12.12.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.12.12.5.1\" style=\"font-size:80%;\">30.23</span><span class=\"ltx_text\" id=\"S4.T16.12.12.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.12.12.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.12.12.6.1\" style=\"font-size:80%;\">38.20</span><span class=\"ltx_text\" id=\"S4.T16.12.12.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.12.12.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.12.12.7.1\" style=\"font-size:80%;\">45.87</span><span class=\"ltx_text\" id=\"S4.T16.12.12.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.12.12.8\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.12.12.8.1\" style=\"font-size:80%;\">32.74</span><span class=\"ltx_text\" id=\"S4.T16.12.12.8.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.12.12.9\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.12.12.9.1\" style=\"font-size:80%;\">36.62</span><span class=\"ltx_text\" id=\"S4.T16.12.12.9.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T16.12.12.10\"><span class=\"ltx_text\" id=\"S4.T16.12.12.10.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.13.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T16.13.13.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.13.13.2\"><span class=\"ltx_text\" id=\"S4.T16.13.13.2.1\" style=\"font-size:80%;\">12.17%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.13.13.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.13.13.3.1\" style=\"font-size:80%;\">45.12</span><span class=\"ltx_text\" id=\"S4.T16.13.13.3.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.13.13.4\"><span class=\"ltx_text\" id=\"S4.T16.13.13.4.1\" style=\"font-size:80%;\">25.82%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.13.13.5\"><span class=\"ltx_text\" id=\"S4.T16.13.13.5.1\" style=\"font-size:80%;\">25.39%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.13.13.6\"><span class=\"ltx_text\" id=\"S4.T16.13.13.6.1\" style=\"font-size:80%;\">33.85%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.13.13.7\"><span class=\"ltx_text\" id=\"S4.T16.13.13.7.1\" style=\"font-size:80%;\">37.45%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.13.13.8\"><span class=\"ltx_text\" id=\"S4.T16.13.13.8.1\" style=\"font-size:80%;\">26.87%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.13.13.9\"><span class=\"ltx_text\" id=\"S4.T16.13.13.9.1\" style=\"font-size:80%;\">30.99%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T16.13.13.10\"><span class=\"ltx_text\" id=\"S4.T16.13.13.10.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.14.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T16.14.14.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.14.14.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.14.14.2.1\" style=\"font-size:80%;\">18.46</span><span class=\"ltx_text\" id=\"S4.T16.14.14.2.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.14.14.3\"><span class=\"ltx_text\" id=\"S4.T16.14.14.3.1\" style=\"font-size:80%;\">41.93%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.14.14.4\"><span class=\"ltx_text\" id=\"S4.T16.14.14.4.1\" style=\"font-size:80%;\">21.99%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.14.14.5\"><span class=\"ltx_text\" id=\"S4.T16.14.14.5.1\" style=\"font-size:80%;\">20.32%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.14.14.6\"><span class=\"ltx_text\" id=\"S4.T16.14.14.6.1\" style=\"font-size:80%;\">28.13%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.14.14.7\"><span class=\"ltx_text\" id=\"S4.T16.14.14.7.1\" style=\"font-size:80%;\">32.83%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.14.14.8\"><span class=\"ltx_text\" id=\"S4.T16.14.14.8.1\" style=\"font-size:80%;\">23.52%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.14.14.9\"><span class=\"ltx_text\" id=\"S4.T16.14.14.9.1\" style=\"font-size:80%;\">26.74%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T16.14.14.10\"><span class=\"ltx_text\" id=\"S4.T16.14.14.10.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.15.15\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T16.15.15.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.3\"><span class=\"ltx_text\" id=\"S4.T16.15.15.3.1\" style=\"font-size:80%;\">1.14%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.4\"><span class=\"ltx_text\" id=\"S4.T16.15.15.4.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.5\"><span class=\"ltx_text\" id=\"S4.T16.15.15.5.1\" style=\"font-size:80%;\">37.05%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.6\"><span class=\"ltx_text\" id=\"S4.T16.15.15.6.1\" style=\"font-size:80%;\">35.29%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.7\"><span class=\"ltx_text\" id=\"S4.T16.15.15.7.1\" style=\"font-size:80%;\">41.50%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.8\"><span class=\"ltx_text\" id=\"S4.T16.15.15.8.1\" style=\"font-size:80%;\">42.47%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.9\"><span class=\"ltx_text\" id=\"S4.T16.15.15.9.1\" style=\"font-size:80%;\">34.89%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T16.15.15.10\"><span class=\"ltx_text\" id=\"S4.T16.15.15.10.1\" style=\"font-size:80%;\">37.94%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T16.15.15.11\"><span class=\"ltx_text\" id=\"S4.T16.15.15.11.1\" style=\"font-size:80%;\">49.65%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.16.16\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T16.16.16.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.16.16.2\"><span class=\"ltx_text\" id=\"S4.T16.16.16.2.1\" style=\"font-size:80%;\">5.77%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.16.16.3\"><span class=\"ltx_text\" id=\"S4.T16.16.16.3.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.16.16.4\"><span class=\"ltx_text\" id=\"S4.T16.16.16.4.1\" style=\"font-size:80%;\">39.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.16.16.5\"><span class=\"ltx_text\" id=\"S4.T16.16.16.5.1\" style=\"font-size:80%;\">38.66%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.16.16.6\"><span class=\"ltx_text\" id=\"S4.T16.16.16.6.1\" style=\"font-size:80%;\">42.28%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.16.16.7\"><span class=\"ltx_text\" id=\"S4.T16.16.16.7.1\" style=\"font-size:80%;\">44.22%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.16.16.8\"><span class=\"ltx_text\" id=\"S4.T16.16.16.8.1\" style=\"font-size:80%;\">37.29%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.16.16.9\"><span class=\"ltx_text\" id=\"S4.T16.16.16.9.1\" style=\"font-size:80%;\">39.03%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T16.16.16.10\"><span class=\"ltx_text\" id=\"S4.T16.16.16.10.1\" style=\"font-size:80%;\">38.22%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.17.17\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T16.17.17.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.17.17.2\"><span class=\"ltx_text\" id=\"S4.T16.17.17.2.1\" style=\"font-size:80%;\">51.70%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.17.17.3\"><span class=\"ltx_text\" id=\"S4.T16.17.17.3.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.17.17.4\"><span class=\"ltx_text\" id=\"S4.T16.17.17.4.1\" style=\"font-size:80%;\">77.59%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.17.17.5\"><span class=\"ltx_text\" id=\"S4.T16.17.17.5.1\" style=\"font-size:80%;\">77.25%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.17.17.6\"><span class=\"ltx_text\" id=\"S4.T16.17.17.6.1\" style=\"font-size:80%;\">77.50%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.17.17.7\"><span class=\"ltx_text\" id=\"S4.T16.17.17.7.1\" style=\"font-size:80%;\">77.98%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.17.17.8\"><span class=\"ltx_text\" id=\"S4.T16.17.17.8.1\" style=\"font-size:80%;\">77.92%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.17.17.9\"><span class=\"ltx_text\" id=\"S4.T16.17.17.9.1\" style=\"font-size:80%;\">78.67%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T16.17.17.10\"><span class=\"ltx_text\" id=\"S4.T16.17.17.10.1\" style=\"font-size:80%;\">76.58%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.18.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T16.18.18.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.18.18.2\"><span class=\"ltx_text\" id=\"S4.T16.18.18.2.1\" style=\"font-size:80%;\">57.45%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.18.18.3\"><span class=\"ltx_text\" id=\"S4.T16.18.18.3.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.18.18.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.18.18.4.1\" style=\"font-size:80%;\">81.94</span><span class=\"ltx_text\" id=\"S4.T16.18.18.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.18.18.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.18.18.5.1\" style=\"font-size:80%;\">81.47</span><span class=\"ltx_text\" id=\"S4.T16.18.18.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.18.18.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.18.18.6.1\" style=\"font-size:80%;\">82.31</span><span class=\"ltx_text\" id=\"S4.T16.18.18.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.18.18.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.18.18.7.1\" style=\"font-size:80%;\">83.48</span><span class=\"ltx_text\" id=\"S4.T16.18.18.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.18.18.8\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.18.18.8.1\" style=\"font-size:80%;\">82.47</span><span class=\"ltx_text\" id=\"S4.T16.18.18.8.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T16.18.18.9\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.18.18.9.1\" style=\"font-size:80%;\">82.72</span><span class=\"ltx_text\" id=\"S4.T16.18.18.9.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T16.18.18.10\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.18.18.10.1\" style=\"font-size:80%;\">82.24</span><span class=\"ltx_text\" id=\"S4.T16.18.18.10.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T16.19.19\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T16.19.19.2.1\" style=\"font-size:80%;\">62.57</span><span class=\"ltx_text\" id=\"S4.T16.19.19.2.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.3\"><span class=\"ltx_text\" id=\"S4.T16.19.19.3.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.4\"><span class=\"ltx_text\" id=\"S4.T16.19.19.4.1\" style=\"font-size:80%;\">79.32%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.5\"><span class=\"ltx_text\" id=\"S4.T16.19.19.5.1\" style=\"font-size:80%;\">78.57%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.6\"><span class=\"ltx_text\" id=\"S4.T16.19.19.6.1\" style=\"font-size:80%;\">78.70%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.7\"><span class=\"ltx_text\" id=\"S4.T16.19.19.7.1\" style=\"font-size:80%;\">80.21%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.8\"><span class=\"ltx_text\" id=\"S4.T16.19.19.8.1\" style=\"font-size:80%;\">79.40%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T16.19.19.9\"><span class=\"ltx_text\" id=\"S4.T16.19.19.9.1\" style=\"font-size:80%;\">80.76%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T16.19.19.10\"><span class=\"ltx_text\" id=\"S4.T16.19.19.10.1\" style=\"font-size:80%;\">66.22%</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T16.23.1.1\" style=\"font-size:113%;\">Table 16</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T16.24.2\" style=\"font-size:113%;\">Verifiability of the DNNs trained for robustness on the RUAR and the Medical datasets, for a selection of semantic subspaces; using the Marabou verifier.\n</em></figcaption>\n</figure>",
            "capture": "Table 16: Verifiability of the DNNs trained for robustness on the RUAR and the Medical datasets, for a selection of semantic subspaces; using the Marabou verifier.\n"
        },
        "17": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T17\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T17.6\">\n<tr class=\"ltx_tr\" id=\"S4.T17.6.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T17.6.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.7.1.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.7.1.1.1\" style=\"width:56.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T17.6.7.1.1.1.1\" style=\"font-size:80%;\">Model</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T17.6.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.7.2.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.7.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T17.6.7.2.1.1.1\" style=\"font-size:80%;\">Train Accuracy RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T17.6.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.7.3.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.7.3.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T17.6.7.3.1.1.1\" style=\"font-size:80%;\">Test Accuracy RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S4.T17.6.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.7.4.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.7.4.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T17.6.7.4.1.1.1\" style=\"font-size:80%;\">Train Accuracy Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T17.6.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.7.5.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.7.5.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T17.6.7.5.1.1.1\" style=\"font-size:80%;\">Test Accuracy Medical</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T17.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T17.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T17.1.1.1.1.1\" style=\"width:56.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T17.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T17.1.1.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.1.1.2.1.1.1\" style=\"font-size:80%;\">93.39 \u00b1 0.22%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T17.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T17.1.1.3.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.1.1.3.1.1.1\" style=\"font-size:80%;\">92.96 \u00b1 0.13%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S4.T17.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T17.1.1.4.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.1.1.4.1.1.1\" style=\"font-size:80%;\">96.14 \u00b1 0.12%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T17.1.1.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.1.1.5.1\">\n<span class=\"ltx_p\" id=\"S4.T17.1.1.5.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.1.1.5.1.1.1\" style=\"font-size:80%;\">94.29 \u00b1 0.26%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T17.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T17.2.2.1.1.1\" style=\"width:56.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T17.2.2.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.2.2.2.1.1.1\" style=\"font-size:80%;\">94.32 \u00b1 0.14%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S4.T17.2.2.3.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.2.2.3.1.1.1\" style=\"font-size:80%;\">93.49 \u00b1 0.19%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.2.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S4.T17.2.2.4.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.2.2.4.1.1.1\" style=\"font-size:80%;\">95.56 \u00b1 0.20%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T17.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S4.T17.2.2.5.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.2.2.5.1.1.1\" style=\"font-size:80%;\">95.15 \u00b1 0.12%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T17.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.3.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T17.3.3.1.1.1\" style=\"width:56.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.3.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S4.T17.3.3.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.3.3.2.1.1.1\" style=\"font-size:80%;\">94.88 \u00b1 0.04%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S4.T17.3.3.3.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.3.3.3.1.1.1\" style=\"font-size:80%;\">94.18 \u00b1 0.24%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S4.T17.3.3.4.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.3.3.4.1.1.1\" style=\"font-size:80%;\">95.71 \u00b1 0.11%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T17.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S4.T17.3.3.5.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T17.3.3.5.1.1.1\" style=\"font-size:80%;\">95.47 \u00b1 0.16</span><span class=\"ltx_text\" id=\"S4.T17.3.3.5.1.1.2\" style=\"font-size:80%;\">%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T17.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.4.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S4.T17.4.4.1.1.1\" style=\"width:56.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.4.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T17.4.4.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.4.4.2.1.1.1\" style=\"font-size:80%;\">95.09 \u00b1 0.09%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.4.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T17.4.4.3.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T17.4.4.3.1.1.1\" style=\"font-size:80%;\">94.45 \u00b1 0.14</span><span class=\"ltx_text\" id=\"S4.T17.4.4.3.1.1.2\" style=\"font-size:80%;\">%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.4.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T17.4.4.4.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.4.4.4.1.1.1\" style=\"font-size:80%;\">95.85 \u00b1 0.05%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T17.4.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.4.4.5.1\">\n<span class=\"ltx_p\" id=\"S4.T17.4.4.5.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.4.4.5.1.1.1\" style=\"font-size:80%;\">95.43 \u00b1 0.10%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T17.5.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.5.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.5.5.1.1\">\n<span class=\"ltx_p\" id=\"S4.T17.5.5.1.1.1\" style=\"width:56.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.5.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.5.5.2.1\">\n<span class=\"ltx_p\" id=\"S4.T17.5.5.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T17.5.5.2.1.1.1\" style=\"font-size:80%;\">95.22 \u00b1 0.08</span><span class=\"ltx_text\" id=\"S4.T17.5.5.2.1.1.2\" style=\"font-size:80%;\">%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.5.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.5.5.3.1\">\n<span class=\"ltx_p\" id=\"S4.T17.5.5.3.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.5.5.3.1.1.1\" style=\"font-size:80%;\">94.22 \u00b1 0.23%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S4.T17.5.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.5.5.4.1\">\n<span class=\"ltx_p\" id=\"S4.T17.5.5.4.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.5.5.4.1.1.1\" style=\"font-size:80%;\">96.07 \u00b1 0.13%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T17.5.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.5.5.5.1\">\n<span class=\"ltx_p\" id=\"S4.T17.5.5.5.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.5.5.5.1.1.1\" style=\"font-size:80%;\">95.38 \u00b1 0.22%</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T17.6.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T17.6.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.6.1.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.6.1.1.1\" style=\"width:56.4pt;\"></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T17.6.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.6.2.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.6.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.6.6.2.1.1.1\" style=\"font-size:80%;\">93.48 \u00b1 0.21%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T17.6.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.6.3.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.6.3.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.6.6.3.1.1.1\" style=\"font-size:80%;\">91.59 \u00b1 0.07%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S4.T17.6.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.6.4.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.6.4.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.6.6.4.1.1.1\" style=\"font-size:80%;\">96.24 \u00b1 0.04%</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T17.6.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T17.6.6.5.1\">\n<span class=\"ltx_p\" id=\"S4.T17.6.6.5.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S4.T17.6.6.5.1.1.1\" style=\"font-size:80%;\">95.13 \u00b1 0.09%</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T17.10.1.1\" style=\"font-size:113%;\">Table 17</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T17.11.2\" style=\"font-size:113%;\">Accuracy of the DNNs trained adversarially on the RUAR and the Medical datasets.\n</em></figcaption>\n</figure>",
            "capture": "Table 17: Accuracy of the DNNs trained adversarially on the RUAR and the Medical datasets.\n"
        },
        "18": {
            "table_html": "<figure class=\"ltx_table\" id=\"S4.T18\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T18.18\">\n<tr class=\"ltx_tr\" id=\"S4.T18.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T18.6.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.6.6.7.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T18.6.6.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.6.6.8.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T18.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T18.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T18.3.3.3\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T18.4.4.4\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" id=\"S4.T18.5.5.5\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T18.6.6.6\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S4.T18.7.7.2\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S4.T18.7.7.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T18.7.7.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.7.7.3\"><span class=\"ltx_text\" id=\"S4.T18.7.7.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.7.7.4\"><span class=\"ltx_text\" id=\"S4.T18.7.7.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.7.7.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.7.7.5.1\" style=\"font-size:80%;\">1.33</span><span class=\"ltx_text\" id=\"S4.T18.7.7.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.7.7.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.7.7.6.1\" style=\"font-size:80%;\">0.52</span><span class=\"ltx_text\" id=\"S4.T18.7.7.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.7.7.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.7.7.7.1\" style=\"font-size:80%;\">0.41</span><span class=\"ltx_text\" id=\"S4.T18.7.7.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T18.7.7.8\"><span class=\"ltx_text\" id=\"S4.T18.7.7.8.1\" style=\"font-size:80%;\">88.62%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.8.8.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.8.8.2\"><span class=\"ltx_text\" id=\"S4.T18.8.8.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.8.8.3\"><span class=\"ltx_text\" id=\"S4.T18.8.8.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.8.8.4\"><span class=\"ltx_text\" id=\"S4.T18.8.8.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.8.8.5\"><span class=\"ltx_text\" id=\"S4.T18.8.8.5.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.8.8.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.8.8.6.1\" style=\"font-size:80%;\">0.41</span><span class=\"ltx_text\" id=\"S4.T18.8.8.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.8.8.7\"><span class=\"ltx_text\" id=\"S4.T18.8.8.7.1\" style=\"font-size:80%;\">90.02%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.9.9.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.9.9.2\"><span class=\"ltx_text\" id=\"S4.T18.9.9.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.9.9.3\"><span class=\"ltx_text\" id=\"S4.T18.9.9.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.9.9.4\"><span class=\"ltx_text\" id=\"S4.T18.9.9.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.9.9.5\"><span class=\"ltx_text\" id=\"S4.T18.9.9.5.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.9.9.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.9.9.6.1\" style=\"font-size:80%;\">0.41</span><span class=\"ltx_text\" id=\"S4.T18.9.9.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.9.9.7\"><span class=\"ltx_text\" id=\"S4.T18.9.9.7.1\" style=\"font-size:80%;\">92.74%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.10.10.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.10.10.2\"><span class=\"ltx_text\" id=\"S4.T18.10.10.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.10.10.3\"><span class=\"ltx_text\" id=\"S4.T18.10.10.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.10.10.4\"><span class=\"ltx_text\" id=\"S4.T18.10.10.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.10.10.5\"><span class=\"ltx_text\" id=\"S4.T18.10.10.5.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.10.10.6\"><span class=\"ltx_text\" id=\"S4.T18.10.10.6.1\" style=\"font-size:80%;\">0.08%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.10.10.7\"><span class=\"ltx_text\" id=\"S4.T18.10.10.7.1\" style=\"font-size:80%;\">93.54%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.11.11.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.11.11.2\"><span class=\"ltx_text\" id=\"S4.T18.11.11.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.11.11.3\"><span class=\"ltx_text\" id=\"S4.T18.11.11.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.11.11.4\"><span class=\"ltx_text\" id=\"S4.T18.11.11.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.11.11.5\"><span class=\"ltx_text\" id=\"S4.T18.11.11.5.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.11.11.6\"><span class=\"ltx_text\" id=\"S4.T18.11.11.6.1\" style=\"font-size:80%;\">0.33%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.11.11.7\"><span class=\"ltx_text\" id=\"S4.T18.11.11.7.1\" style=\"font-size:80%;\">93.86%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.12.12.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.12.12.2\"><span class=\"ltx_text\" id=\"S4.T18.12.12.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.12.12.3\"><span class=\"ltx_text\" id=\"S4.T18.12.12.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.12.12.4\"><span class=\"ltx_text\" id=\"S4.T18.12.12.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.12.12.5\"><span class=\"ltx_text\" id=\"S4.T18.12.12.5.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.12.12.6\"><span class=\"ltx_text\" id=\"S4.T18.12.12.6.1\" style=\"font-size:80%;\">0.33%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.12.12.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.12.12.7.1\" style=\"font-size:80%;\">98.22</span><span class=\"ltx_text\" id=\"S4.T18.12.12.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.13.13\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T18.13.13.2\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S4.T18.13.13.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T18.13.13.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.13.13.3\"><span class=\"ltx_text\" id=\"S4.T18.13.13.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.13.13.4\"><span class=\"ltx_text\" id=\"S4.T18.13.13.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.13.13.5\"><span class=\"ltx_text\" id=\"S4.T18.13.13.5.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.13.13.6\"><span class=\"ltx_text\" id=\"S4.T18.13.13.6.1\" style=\"font-size:80%;\">2.50%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T18.13.13.7\"><span class=\"ltx_text\" id=\"S4.T18.13.13.7.1\" style=\"font-size:80%;\">4.40%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T18.13.13.8\"><span class=\"ltx_text\" id=\"S4.T18.13.13.8.1\" style=\"font-size:80%;\">97.47%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.14.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.14.14.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.14.14.2\"><span class=\"ltx_text\" id=\"S4.T18.14.14.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.14.14.3\"><span class=\"ltx_text\" id=\"S4.T18.14.14.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.14.14.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.14.14.4.1\" style=\"font-size:80%;\">1.08</span><span class=\"ltx_text\" id=\"S4.T18.14.14.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.14.14.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.14.14.5.1\" style=\"font-size:80%;\">3.60</span><span class=\"ltx_text\" id=\"S4.T18.14.14.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.14.14.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.14.14.6.1\" style=\"font-size:80%;\">6.00</span><span class=\"ltx_text\" id=\"S4.T18.14.14.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.14.14.7\"><span class=\"ltx_text\" id=\"S4.T18.14.14.7.1\" style=\"font-size:80%;\">98.79%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.15.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.15.15.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.15.15.2\"><span class=\"ltx_text\" id=\"S4.T18.15.15.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.15.15.3\"><span class=\"ltx_text\" id=\"S4.T18.15.15.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.15.15.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.15.15.4.1\" style=\"font-size:80%;\">1.08</span><span class=\"ltx_text\" id=\"S4.T18.15.15.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.15.15.5\"><span class=\"ltx_text\" id=\"S4.T18.15.15.5.1\" style=\"font-size:80%;\">3.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.15.15.6\"><span class=\"ltx_text\" id=\"S4.T18.15.15.6.1\" style=\"font-size:80%;\">5.04%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.15.15.7\"><span class=\"ltx_text\" id=\"S4.T18.15.15.7.1\" style=\"font-size:80%;\">99.09%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.16.16\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.16.16.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.16.16.2\"><span class=\"ltx_text\" id=\"S4.T18.16.16.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.16.16.3\"><span class=\"ltx_text\" id=\"S4.T18.16.16.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.16.16.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.16.16.4.1\" style=\"font-size:80%;\">1.08</span><span class=\"ltx_text\" id=\"S4.T18.16.16.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.16.16.5\"><span class=\"ltx_text\" id=\"S4.T18.16.16.5.1\" style=\"font-size:80%;\">2.90%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.16.16.6\"><span class=\"ltx_text\" id=\"S4.T18.16.16.6.1\" style=\"font-size:80%;\">4.96%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.16.16.7\"><span class=\"ltx_text\" id=\"S4.T18.16.16.7.1\" style=\"font-size:80%;\">99.05%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.17.17\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T18.17.17.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.17.17.2\"><span class=\"ltx_text\" id=\"S4.T18.17.17.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.17.17.3\"><span class=\"ltx_text\" id=\"S4.T18.17.17.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.17.17.4\"><span class=\"ltx_text\" id=\"S4.T18.17.17.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.17.17.5\"><span class=\"ltx_text\" id=\"S4.T18.17.17.5.1\" style=\"font-size:80%;\">2.90%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T18.17.17.6\"><span class=\"ltx_text\" id=\"S4.T18.17.17.6.1\" style=\"font-size:80%;\">4.40%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T18.17.17.7\"><span class=\"ltx_text\" id=\"S4.T18.17.17.7.1\" style=\"font-size:80%;\">98.73%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T18.18.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T18.18.18.1\"></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T18.18.18.2\"><span class=\"ltx_text\" id=\"S4.T18.18.18.2.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T18.18.18.3\"><span class=\"ltx_text\" id=\"S4.T18.18.18.3.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T18.18.18.4\"><span class=\"ltx_text\" id=\"S4.T18.18.18.4.1\" style=\"font-size:80%;\">0.00%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T18.18.18.5\"><span class=\"ltx_text\" id=\"S4.T18.18.18.5.1\" style=\"font-size:80%;\">2.30%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T18.18.18.6\"><span class=\"ltx_text\" id=\"S4.T18.18.18.6.1\" style=\"font-size:80%;\">4.32%</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T18.18.18.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T18.18.18.7.1\" style=\"font-size:80%;\">99.60</span><span class=\"ltx_text\" id=\"S4.T18.18.18.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S4.T18.22.1.1\" style=\"font-size:113%;\">Table 18</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S4.T18.23.2\" style=\"font-size:113%;\">Verifiability of the DNNs trained adversarially on the RUAR and the Medical datasets, for a selection of geometric subspaces; using the ERAN verifier.\n</em></figcaption>\n</figure>",
            "capture": "Table 18: Verifiability of the DNNs trained adversarially on the RUAR and the Medical datasets, for a selection of geometric subspaces; using the ERAN verifier.\n"
        },
        "19": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T19\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T19.28\">\n<tr class=\"ltx_tr\" id=\"S5.T19.28.29\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T19.28.29.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.29.1.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.29.1.1.1\" style=\"width:86.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.28.29.1.1.1.1\" style=\"font-size:80%;\">Pipeline Component</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T19.28.29.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.29.2.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.29.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.28.29.2.1.1.1\" style=\"font-size:80%;\">Component Implementations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" id=\"S5.T19.28.29.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.29.3.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.29.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.28.29.3.1.1.1\" style=\"font-size:80%;\">Additional Details</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.3.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S5.T19.3.3.4.1.1\" style=\"width:86.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.3.3.4.1.1.1\" style=\"font-size:80%;\">0. Choosing Datasets</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.3.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.3.3.5.1\">\n<span class=\"ltx_p\" id=\"S5.T19.3.3.5.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S5.T19.3.3.5.1.1.1\" style=\"font-size:80%;\">RUAR, Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.3.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.3.3.3.3\">\n<span class=\"ltx_p\" id=\"S5.T19.3.3.3.3.3\"><span class=\"ltx_text\" id=\"S5.T19.3.3.3.3.3.1\" style=\"font-size:80%;\">Same as in Section\u00a0</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#S4\" style=\"font-size:80%;\" title=\"4 Characterisation of Verifiable Subspaces \u2023 NLP Verification: Towards a General Methodology for Certifying Robustness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a><span class=\"ltx_text\" id=\"S5.T19.3.3.3.3.3.2\" style=\"font-size:80%;\"> experiments. The RUAR dataset has </span><span class=\"ltx_text\" id=\"S5.T19.3.3.3.3.3.3\" style=\"font-size:80%;\"> sentences equally divided among the two classes, while the Medical dataset has 2917 medical and non-medical queries (</span><span class=\"ltx_text\" id=\"S5.T19.3.3.3.3.3.4\" style=\"font-size:80%;\"> and </span><span class=\"ltx_text\" id=\"S5.T19.3.3.3.3.3.5\" style=\"font-size:80%;\"> examples respectively).</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.12.12\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.12.12.10\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.12.12.10.1\">\n<span class=\"ltx_p\" id=\"S5.T19.12.12.10.1.1\" style=\"width:86.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.12.12.10.1.1.1\" style=\"font-size:80%;\">1. Generating Sentence Perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.7.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.7.7.4.4\">\n<span class=\"ltx_p\" id=\"S5.T19.7.7.4.4.4\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S5.T19.7.7.4.4.4.1\" style=\"font-size:80%;\">, </span><span class=\"ltx_text\" id=\"S5.T19.7.7.4.4.4.2\" style=\"font-size:80%;\">, </span><span class=\"ltx_text\" id=\"S5.T19.7.7.4.4.4.3\" style=\"font-size:80%;\">, </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.12.12.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.12.12.9.5\">\n<span class=\"ltx_p\" id=\"S5.T19.12.12.9.5.5\"><span class=\"ltx_text\" id=\"S5.T19.12.12.9.5.5.1\" style=\"font-size:80%;\">With </span><span class=\"ltx_text\" id=\"S5.T19.12.12.9.5.5.2\" style=\"font-size:80%;\">, the resulting set of sentences </span><span class=\"ltx_text\" id=\"S5.T19.12.12.9.5.5.3\" style=\"font-size:80%;\"> has </span><span class=\"ltx_text\" id=\"S5.T19.12.12.9.5.5.4\" style=\"font-size:80%;\"> sentences for RUAR and </span><span class=\"ltx_text\" id=\"S5.T19.12.12.9.5.5.5\" style=\"font-size:80%;\"> sentences for Medical. The superscript </span><sup class=\"ltx_sup\" id=\"S5.T19.12.12.9.5.5.6\"><span class=\"ltx_text\" id=\"S5.T19.12.12.9.5.5.6.1\" style=\"font-size:80%;\">\u2662</span></sup><span class=\"ltx_text\" id=\"S5.T19.12.12.9.5.5.7\" style=\"font-size:80%;\"> refers to filtering that will be introduced in Section\u00a0</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#S5.SS5\" style=\"font-size:80%;\" title=\"5.5 Analysis of Perturbations \u2023 5 NLP Case Studies \u2023 NLP Verification: Towards a General Methodology for Certifying Robustness\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a><span class=\"ltx_text\" id=\"S5.T19.12.12.9.5.5.8\" style=\"font-size:80%;\">.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.28.30\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.28.30.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.30.1.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.30.1.1.1\" style=\"width:86.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.28.30.1.1.1.1\" style=\"font-size:80%;\">2. Embedding Sentences into Real Vector Space</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.28.30.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.30.2.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.30.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S5.T19.28.30.2.1.1.1\" style=\"font-size:80%;\">s-bert\u00a022M, s-gpt\u00a01.3B, s-gpt\u00a02.7B</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.28.30.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.30.3.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.30.3.1.1\"><span class=\"ltx_text\" id=\"S5.T19.28.30.3.1.1.1\" style=\"font-size:80%;\">In the experiments of Section\u00a0</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#S4\" style=\"font-size:80%;\" title=\"4 Characterisation of Verifiable Subspaces \u2023 NLP Verification: Towards a General Methodology for Certifying Robustness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a><span class=\"ltx_text\" id=\"S5.T19.28.30.3.1.1.2\" style=\"font-size:80%;\"> only s-bert 22M was used.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.20.20\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.20.20.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.20.20.9.1\">\n<span class=\"ltx_p\" id=\"S5.T19.20.20.9.1.1\" style=\"width:86.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.20.20.9.1.1.1\" style=\"font-size:80%;\">3. Defining Semantic Subspaces based on Sentence Perturbations</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.14.14.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.14.14.2.2\">\n<span class=\"ltx_p\" id=\"S5.T19.14.14.2.2.2\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S5.T19.14.14.2.2.2.1\" style=\"font-size:80%;\">, </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.20.20.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.20.20.8.6\">\n<span class=\"ltx_p\" id=\"S5.T19.20.20.8.6.6\"><span class=\"ltx_text\" id=\"S5.T19.20.20.8.6.6.1\" style=\"font-size:80%;\"> and </span><span class=\"ltx_text\" id=\"S5.T19.20.20.8.6.6.2\" style=\"font-size:80%;\"> are obtained on </span><span class=\"ltx_text\" id=\"S5.T19.20.20.8.6.6.3\" style=\"font-size:80%;\">, </span><span class=\"ltx_text\" id=\"S5.T19.20.20.8.6.6.4\" style=\"font-size:80%;\">, respectively. Their cardinality is </span><span class=\"ltx_text\" id=\"S5.T19.20.20.8.6.6.5\" style=\"font-size:80%;\"> for RUAR and </span><span class=\"ltx_text\" id=\"S5.T19.20.20.8.6.6.6\" style=\"font-size:80%;\"> for Medical.</span></span>\n<span class=\"ltx_itemize\" id=\"S5.I3\">\n<span class=\"ltx_item\" id=\"S5.I3.i1\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<span class=\"ltx_para\" id=\"S5.I3.i1.p1\">\n<span class=\"ltx_p\" id=\"S5.I3.i1.p1.4\"><span class=\"ltx_text\" id=\"S5.I3.i1.p1.4.1\" style=\"font-size:80%;\">Volume of </span><span class=\"ltx_text\" id=\"S5.I3.i1.p1.4.2\" style=\"font-size:80%;\"> for RUAR is </span><span class=\"ltx_text\" id=\"S5.I3.i1.p1.4.3\" style=\"font-size:80%;\"> (s-bert 22M), </span><span class=\"ltx_text\" id=\"S5.I3.i1.p1.4.4\" style=\"font-size:80%;\"> (s-gpt 1.3B), </span><span class=\"ltx_text\" id=\"S5.I3.i1.p1.4.5\" style=\"font-size:80%;\"> (s-gpt 2.7B).</span></span>\n</span></span>\n<span class=\"ltx_item\" id=\"S5.I3.i2\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<span class=\"ltx_para\" id=\"S5.I3.i2.p1\">\n<span class=\"ltx_p\" id=\"S5.I3.i2.p1.4\"><span class=\"ltx_text\" id=\"S5.I3.i2.p1.4.1\" style=\"font-size:80%;\">Volume of </span><span class=\"ltx_text\" id=\"S5.I3.i2.p1.4.2\" style=\"font-size:80%;\"> for RUAR is </span><span class=\"ltx_text\" id=\"S5.I3.i2.p1.4.3\" style=\"font-size:80%;\"> (s-bert 22M), </span><span class=\"ltx_text\" id=\"S5.I3.i2.p1.4.4\" style=\"font-size:80%;\"> (s-gpt 1.3B), </span><span class=\"ltx_text\" id=\"S5.I3.i2.p1.4.5\" style=\"font-size:80%;\"> (s-gpt 2.7B).</span></span>\n</span></span>\n<span class=\"ltx_item\" id=\"S5.I3.i3\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<span class=\"ltx_para\" id=\"S5.I3.i3.p1\">\n<span class=\"ltx_p\" id=\"S5.I3.i3.p1.4\"><span class=\"ltx_text\" id=\"S5.I3.i3.p1.4.1\" style=\"font-size:80%;\">Volume of </span><span class=\"ltx_text\" id=\"S5.I3.i3.p1.4.2\" style=\"font-size:80%;\"> for Medical is </span><span class=\"ltx_text\" id=\"S5.I3.i3.p1.4.3\" style=\"font-size:80%;\"> (s-bert 22M), </span><span class=\"ltx_text\" id=\"S5.I3.i3.p1.4.4\" style=\"font-size:80%;\"> (s-gpt 1.3B), </span><span class=\"ltx_text\" id=\"S5.I3.i3.p1.4.5\" style=\"font-size:80%;\"> (s-gpt 2.7B).</span></span>\n</span></span>\n<span class=\"ltx_item\" id=\"S5.I3.i4\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<span class=\"ltx_para\" id=\"S5.I3.i4.p1\">\n<span class=\"ltx_p\" id=\"S5.I3.i4.p1.4\"><span class=\"ltx_text\" id=\"S5.I3.i4.p1.4.1\" style=\"font-size:80%;\">Volume of </span><span class=\"ltx_text\" id=\"S5.I3.i4.p1.4.2\" style=\"font-size:80%;\"> for Medical is </span><span class=\"ltx_text\" id=\"S5.I3.i4.p1.4.3\" style=\"font-size:80%;\"> (s-bert 22M), </span><span class=\"ltx_text\" id=\"S5.I3.i4.p1.4.4\" style=\"font-size:80%;\"> (s-gpt 1.3B), </span><span class=\"ltx_text\" id=\"S5.I3.i4.p1.4.5\" style=\"font-size:80%;\"> (s-gpt 2.7B).</span></span>\n</span></span>\n</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.28.28\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.28.28.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.28.9.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.28.9.1.1\" style=\"width:86.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.28.28.9.1.1.1\" style=\"font-size:80%;\">4. Training Robust DNNs using Semantic Subspaces</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T19.23.23.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.23.23.3.3\">\n<span class=\"ltx_p\" id=\"S5.T19.23.23.3.3.3\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S5.T19.23.23.3.3.3.1\" style=\"font-size:80%;\">, </span><span class=\"ltx_text\" id=\"S5.T19.23.23.3.3.3.2\" style=\"font-size:80%;\">, </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S5.T19.28.28.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.28.8.5\">\n<span class=\"ltx_p\" id=\"S5.T19.28.28.8.5.5\"><span class=\"ltx_text\" id=\"S5.T19.28.28.8.5.5.1\" style=\"font-size:80%;\"> is obtained as in Section\u00a0</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#S4\" style=\"font-size:80%;\" title=\"4 Characterisation of Verifiable Subspaces \u2023 NLP Verification: Towards a General Methodology for Certifying Robustness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a><span class=\"ltx_text\" id=\"S5.T19.28.28.8.5.5.2\" style=\"font-size:80%;\">, while </span><span class=\"ltx_text\" id=\"S5.T19.28.28.8.5.5.3\" style=\"font-size:80%;\"> and </span><span class=\"ltx_text\" id=\"S5.T19.28.28.8.5.5.4\" style=\"font-size:80%;\"> are obtained through our adversarial training on </span><span class=\"ltx_text\" id=\"S5.T19.28.28.8.5.5.5\" style=\"font-size:80%;\"> and </span><span class=\"ltx_text\" id=\"S5.T19.28.28.8.5.5.6\" style=\"font-size:80%;\">, respectively.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T19.28.31\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T19.28.31.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.31.1.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.31.1.1.1\" style=\"width:86.7pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.28.31.1.1.1.1\" style=\"font-size:80%;\">5. Verifying resulting DNNs on the given semantic subspaces</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T19.28.31.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.31.2.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.31.2.1.1\" style=\"width:78.0pt;\"><span class=\"ltx_text\" id=\"S5.T19.28.31.2.1.1.1\" style=\"font-size:80%;\">Marabou</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"S5.T19.28.31.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T19.28.31.3.1\">\n<span class=\"ltx_p\" id=\"S5.T19.28.31.3.1.1\"><span class=\"ltx_text\" id=\"S5.T19.28.31.3.1.1.1\" style=\"font-size:80%;\">Same settings as in Section\u00a0</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#S4\" style=\"font-size:80%;\" title=\"4 Characterisation of Verifiable Subspaces \u2023 NLP Verification: Towards a General Methodology for Certifying Robustness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T19.34.2.1\" style=\"font-size:113%;\">Table 19</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T19.30.1\" style=\"font-size:113%;\">Section\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2403.10144v2#S5\" title=\"5 NLP Case Studies \u2023 NLP Verification: Towards a General Methodology for Certifying Robustness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> <span class=\"ltx_text ltx_font_bold\" id=\"S5.T19.30.1.1\">NLP verification pipeline</span> setup, implemented using ANTONIO. Note that, after filtering, the volume of  decreases by several orders of magnitude. Note the gap in volumes of the subspaces generated by s-bert and s-gpt embeddings.</em><span class=\"ltx_text\" id=\"S5.T19.35.3\" style=\"font-size:113%;\">\n</span></figcaption>\n</figure>",
            "capture": "Table 19: Section\u00a05 NLP verification pipeline setup, implemented using ANTONIO. Note that, after filtering, the volume of  decreases by several orders of magnitude. Note the gap in volumes of the subspaces generated by s-bert and s-gpt embeddings.\n"
        },
        "20": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T20\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T20.14\">\n<tr class=\"ltx_tr\" id=\"S5.T20.14.15\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T20.14.15.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.15.1.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T20.14.15.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.15.2.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"S5.T20.14.15.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.15.3.1\" style=\"font-size:80%;\">Test set</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S5.T20.14.15.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.15.4.1\" style=\"font-size:80%;\">Perturbed test set</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.14.16\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r\" id=\"S5.T20.14.16.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r\" id=\"S5.T20.14.16.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.14.16.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.16.3.1\" style=\"font-size:80%;\">Precision</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.14.16.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.16.4.1\" style=\"font-size:80%;\">Recall</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.14.16.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.16.5.1\" style=\"font-size:80%;\">F1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.14.16.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.16.6.1\" style=\"font-size:80%;\">Precision</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.14.16.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.16.7.1\" style=\"font-size:80%;\">Recall</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.14.16.8\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.14.16.8.1\" style=\"font-size:80%;\">F1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T20.1.1.2\" rowspan=\"7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.1.1.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T20.1.1.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.1.1.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.1.1.3.1\" style=\"font-size:80%;\">51.67%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.1.1.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.1.1.4.1\" style=\"font-size:80%;\">58.35%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T20.1.1.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.1.1.5.1\" style=\"font-size:80%;\">54.81%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.1.1.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.1.1.6.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.1.1.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.1.1.7.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.1.1.8\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.1.1.8.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.2.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.2.2.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.2.2.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.2.2.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.2.2.2.1\" style=\"font-size:80%;\">95.68%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.2.2.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.2.2.3.1\" style=\"font-size:80%;\">91.29%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.2.2.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.2.2.4.1\" style=\"font-size:80%;\">93.44</span><span class=\"ltx_text\" id=\"S5.T20.2.2.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.2.2.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.2.2.5.1\" style=\"font-size:80%;\">94.77%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.2.2.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.2.2.6.1\" style=\"font-size:80%;\">71.86%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.2.2.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.2.2.7.1\" style=\"font-size:80%;\">81.74%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.3.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.3.3.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.3.3.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.3.3.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.3.3.2.1\" style=\"font-size:80%;\">84.97%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.3.3.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.3.3.3.1\" style=\"font-size:80%;\">98.63%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.3.3.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.3.3.4.1\" style=\"font-size:80%;\">91.29%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.3.3.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.3.3.5.1\" style=\"font-size:80%;\">81.25%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.3.3.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.3.3.6.1\" style=\"font-size:80%;\">94.66%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.3.3.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.3.3.7.1\" style=\"font-size:80%;\">87.45</span><span class=\"ltx_text\" id=\"S5.T20.3.3.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.4.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.4.4.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.4.4.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.4.4.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.4.4.2.1\" style=\"font-size:80%;\">96.20%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.4.4.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.4.4.3.1\" style=\"font-size:80%;\">87.25%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.4.4.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.4.4.4.1\" style=\"font-size:80%;\">91.51%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.4.4.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.4.4.5.1\" style=\"font-size:80%;\">95.45%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.4.4.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.4.4.6.1\" style=\"font-size:80%;\">67.38%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.4.4.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.4.4.7.1\" style=\"font-size:80%;\">78.98%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.5.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.5.5.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.5.5.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.5.5.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.5.5.2.1\" style=\"font-size:80%;\">63.03%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.5.5.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.5.5.3.1\" style=\"font-size:80%;\">99.80%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.5.5.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.5.5.4.1\" style=\"font-size:80%;\">77.24%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.5.5.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.5.5.5.1\" style=\"font-size:80%;\">61.26%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.5.5.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.5.5.6.1\" style=\"font-size:80%;\">98.60%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.5.5.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.5.5.7.1\" style=\"font-size:80%;\">75.54%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.6.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.6.6.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.6.6.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.6.6.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.6.6.2.1\" style=\"font-size:80%;\">96.74</span><span class=\"ltx_text\" id=\"S5.T20.6.6.2.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.6.6.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.6.6.3.1\" style=\"font-size:80%;\">87.29%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.6.6.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.6.6.4.1\" style=\"font-size:80%;\">91.77%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.6.6.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.6.6.5.1\" style=\"font-size:80%;\">95.49</span><span class=\"ltx_text\" id=\"S5.T20.6.6.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.6.6.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.6.6.6.1\" style=\"font-size:80%;\">69.82%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.6.6.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.6.6.7.1\" style=\"font-size:80%;\">80.66%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.7.7\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.7.7.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.7.7.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.7.7.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.2.1\" style=\"font-size:80%;\">60.18%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.7.7.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.7.7.3.1\" style=\"font-size:80%;\">99.80</span><span class=\"ltx_text\" id=\"S5.T20.7.7.3.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.7.7.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.4.1\" style=\"font-size:80%;\">75.08%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.7.7.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.5.1\" style=\"font-size:80%;\">58.46%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.7.7.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.7.7.6.1\" style=\"font-size:80%;\">98.99</span><span class=\"ltx_text\" id=\"S5.T20.7.7.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.7.7.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.7.7.7.1\" style=\"font-size:80%;\">73.50%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.8.8\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T20.8.8.2\" rowspan=\"7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.8.8.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T20.8.8.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.8.8.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.8.8.3.1\" style=\"font-size:80%;\">58.95%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.8.8.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.8.8.4.1\" style=\"font-size:80%;\">70.22%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T20.8.8.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.8.8.5.1\" style=\"font-size:80%;\">64.09%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.8.8.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.8.8.6.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.8.8.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.8.8.7.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T20.8.8.8\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.8.8.8.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.9.9\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.9.9.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.9.9.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.9.9.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.9.9.2.1\" style=\"font-size:80%;\">95.23</span><span class=\"ltx_text\" id=\"S5.T20.9.9.2.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.9.9.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.9.9.3.1\" style=\"font-size:80%;\">93.25%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.9.9.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.9.9.4.1\" style=\"font-size:80%;\">94.23%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.9.9.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.9.9.5.1\" style=\"font-size:80%;\">95.20</span><span class=\"ltx_text\" id=\"S5.T20.9.9.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.9.9.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.9.9.6.1\" style=\"font-size:80%;\">89.64%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.9.9.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.9.9.7.1\" style=\"font-size:80%;\">92.34%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.10.10\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.10.10.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.10.10.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.10.10.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.10.10.2.1\" style=\"font-size:80%;\">93.35%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.10.10.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.10.10.3.1\" style=\"font-size:80%;\">97.36</span><span class=\"ltx_text\" id=\"S5.T20.10.10.3.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.10.10.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.10.10.4.1\" style=\"font-size:80%;\">95.31</span><span class=\"ltx_text\" id=\"S5.T20.10.10.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.10.10.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.10.10.5.1\" style=\"font-size:80%;\">92.38%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.10.10.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.10.10.6.1\" style=\"font-size:80%;\">95.17</span><span class=\"ltx_text\" id=\"S5.T20.10.10.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.10.10.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T20.10.10.7.1\" style=\"font-size:80%;\">93.76</span><span class=\"ltx_text\" id=\"S5.T20.10.10.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.11.11\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.11.11.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.11.11.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.11.11.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.11.11.2.1\" style=\"font-size:80%;\">91.93%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.11.11.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.11.11.3.1\" style=\"font-size:80%;\">88.11%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.11.11.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.11.11.4.1\" style=\"font-size:80%;\">89.98%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.11.11.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.11.11.5.1\" style=\"font-size:80%;\">92.17%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.11.11.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.11.11.6.1\" style=\"font-size:80%;\">84.17%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.11.11.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.11.11.7.1\" style=\"font-size:80%;\">87.98%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.12.12\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.12.12.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.12.12.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.12.12.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.12.12.2.1\" style=\"font-size:80%;\">84.41%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.12.12.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.12.12.3.1\" style=\"font-size:80%;\">96.27%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.12.12.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.12.12.4.1\" style=\"font-size:80%;\">89.38%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.12.12.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.12.12.5.1\" style=\"font-size:80%;\">83.15%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.12.12.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.12.12.6.1\" style=\"font-size:80%;\">94.70%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.12.12.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.12.12.7.1\" style=\"font-size:80%;\">88.54%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.13.13\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" id=\"S5.T20.13.13.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.13.13.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.13.13.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.13.13.2.1\" style=\"font-size:80%;\">93.25%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.13.13.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.13.13.3.1\" style=\"font-size:80%;\">89.29%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" id=\"S5.T20.13.13.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.13.13.4.1\" style=\"font-size:80%;\">91.23%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.13.13.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.13.13.5.1\" style=\"font-size:80%;\">92.89%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.13.13.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.13.13.6.1\" style=\"font-size:80%;\">84.79%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S5.T20.13.13.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.13.13.7.1\" style=\"font-size:80%;\">88.66%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T20.14.14\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T20.14.14.1\" style=\"padding-left:1.8pt;padding-right:1.8pt;\">\n<span class=\"ltx_text\" id=\"S5.T20.14.14.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T20.14.14.2\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.14.14.2.1\" style=\"font-size:80%;\">86.03%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T20.14.14.3\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.14.14.3.1\" style=\"font-size:80%;\">96.56%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T20.14.14.4\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.14.14.4.1\" style=\"font-size:80%;\">90.98%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T20.14.14.5\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.14.14.5.1\" style=\"font-size:80%;\">84.88%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T20.14.14.6\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.14.14.6.1\" style=\"font-size:80%;\">94.99%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T20.14.14.7\" style=\"padding-left:1.8pt;padding-right:1.8pt;\"><span class=\"ltx_text\" id=\"S5.T20.14.14.7.1\" style=\"font-size:80%;\">89.64%</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T20.20.2.1\" style=\"font-size:113%;\">Table 20</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T20.16.1\" style=\"font-size:113%;\">Performance of the models on the test/perturbation set. The average standard deviation is .</em><span class=\"ltx_text\" id=\"S5.T20.21.3\" style=\"font-size:113%;\">\n</span></figcaption>\n</figure>",
            "capture": "Table 20: Performance of the models on the test/perturbation set. The average standard deviation is .\n"
        },
        "21": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T21\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T21.2\">\n<tr class=\"ltx_tr\" id=\"S5.T21.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T21.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T21.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T21.2.1.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T21.2.1.1.1.1.1\" style=\"font-size:80%;\">Criteria</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S5.T21.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T21.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S5.T21.2.1.2.1.1\" style=\"width:338.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T21.2.1.2.1.1.1\" style=\"font-size:80%;\">Instructions</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T21.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T21.2.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T21.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S5.T21.2.2.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S5.T21.2.2.1.1.1.1\" style=\"font-size:80%;\">Semantic similarity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T21.2.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T21.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S5.T21.2.2.2.1.1\" style=\"width:338.2pt;\"><span class=\"ltx_text\" id=\"S5.T21.2.2.2.1.1.1\" style=\"font-size:80%;\">Evaluate whether the original and the modified sentence have the same meaning on a scale from 1 to 4, where 1 is \u2018The modified version means something completely different\u2019 and 4 means \u2018The modified version has exactly the same meaning\u2019.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T21.2.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T21.2.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T21.2.3.1.1\">\n<span class=\"ltx_p\" id=\"S5.T21.2.3.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S5.T21.2.3.1.1.1.1\" style=\"font-size:80%;\">Grammaticality</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T21.2.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T21.2.3.2.1\">\n<span class=\"ltx_p\" id=\"S5.T21.2.3.2.1.1\" style=\"width:338.2pt;\"><span class=\"ltx_text\" id=\"S5.T21.2.3.2.1.1.1\" style=\"font-size:80%;\">Grammatically means issues in grammar, such as verb tense. Evaluate the grammaticality of the modified version on a scale of 1-3, where 1 is \u2018Not understandable because of grammar issues\u2019, and 3 is \u2018Perfectly grammatical\u2019.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T21.2.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T21.2.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T21.2.4.1.1\">\n<span class=\"ltx_p\" id=\"S5.T21.2.4.1.1.1\" style=\"width:65.0pt;\"><span class=\"ltx_text\" id=\"S5.T21.2.4.1.1.1.1\" style=\"font-size:80%;\">Label consistency</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"S5.T21.2.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T21.2.4.2.1\">\n<span class=\"ltx_p\" id=\"S5.T21.2.4.2.1.1\" style=\"width:338.2pt;\"><span class=\"ltx_text\" id=\"S5.T21.2.4.2.1.1.1\" style=\"font-size:80%;\">Decide whether the positive label of the modified sentence is correct using labels 1 - \u2018Yes, the label is correct\u2019, 2 - \u2018No, the label is incorrect\u2019 and 3 - \u2018Unsure\u2019.</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T21.5.1.1\" style=\"font-size:113%;\">Table 21</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T21.6.2\" style=\"font-size:113%;\">Annotation instructions for manual estimation of the perturbation validity.</em></figcaption>\n</figure>",
            "capture": "Table 21: Annotation instructions for manual estimation of the perturbation validity."
        },
        "22": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T22\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T22.2\">\n<tr class=\"ltx_tr\" id=\"S5.T22.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T22.2.1.1\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.1.1.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T22.2.1.2\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.1.2.1\" style=\"font-size:80%;\">Perturbation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"8\" id=\"S5.T22.2.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.1.3.1\" style=\"font-size:80%;\">Semantic Similarity</span><span class=\"ltx_text\" id=\"S5.T22.2.1.3.2\" style=\"font-size:80%;\"> (%)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T22.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"4\" id=\"S5.T22.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.2.1.1\" style=\"font-size:80%;\">A1</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"4\" id=\"S5.T22.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.2.2.1\" style=\"font-size:80%;\">A2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T22.2.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.3.1.1\" style=\"font-size:80%;\">1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.3.2.1\" style=\"font-size:80%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.3.3.1\" style=\"font-size:80%;\">3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T22.2.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.3.4.1\" style=\"font-size:80%;\">4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.3.5.1\" style=\"font-size:80%;\">1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.3.6.1\" style=\"font-size:80%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.3.7.1\" style=\"font-size:80%;\">3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T22.2.3.8.1\" style=\"font-size:80%;\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T22.2.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T22.2.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T22.2.4.1.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T22.2.4.2\"><span class=\"ltx_text\" id=\"S5.T22.2.4.2.1\" style=\"font-size:80%;\">Rule-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.4.3\"><span class=\"ltx_text\" id=\"S5.T22.2.4.3.1\" style=\"font-size:80%;\">06.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.4.4\"><span class=\"ltx_text\" id=\"S5.T22.2.4.4.1\" style=\"font-size:80%;\">07.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.4.5\"><span class=\"ltx_text\" id=\"S5.T22.2.4.5.1\" style=\"font-size:80%;\">09.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T22.2.4.6\"><span class=\"ltx_text\" id=\"S5.T22.2.4.6.1\" style=\"font-size:80%;\">77.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.4.7\"><span class=\"ltx_text\" id=\"S5.T22.2.4.7.1\" style=\"font-size:80%;\">05.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.4.8\"><span class=\"ltx_text\" id=\"S5.T22.2.4.8.1\" style=\"font-size:80%;\">10.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.4.9\"><span class=\"ltx_text\" id=\"S5.T22.2.4.9.1\" style=\"font-size:80%;\">34.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.4.10\"><span class=\"ltx_text\" id=\"S5.T22.2.4.10.1\" style=\"font-size:80%;\">49.09</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T22.2.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T22.2.5.1\"><span class=\"ltx_text\" id=\"S5.T22.2.5.1.1\" style=\"font-size:80%;\">LLM-based</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.5.2\"><span class=\"ltx_text\" id=\"S5.T22.2.5.2.1\" style=\"font-size:80%;\">18.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.5.3\"><span class=\"ltx_text\" id=\"S5.T22.2.5.3.1\" style=\"font-size:80%;\">08.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.5.4\"><span class=\"ltx_text\" id=\"S5.T22.2.5.4.1\" style=\"font-size:80%;\">20.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T22.2.5.5\"><span class=\"ltx_text\" id=\"S5.T22.2.5.5.1\" style=\"font-size:80%;\">54.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.5.6\"><span class=\"ltx_text\" id=\"S5.T22.2.5.6.1\" style=\"font-size:80%;\">16.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.5.7\"><span class=\"ltx_text\" id=\"S5.T22.2.5.7.1\" style=\"font-size:80%;\">08.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.5.8\"><span class=\"ltx_text\" id=\"S5.T22.2.5.8.1\" style=\"font-size:80%;\">00.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T22.2.5.9\"><span class=\"ltx_text\" id=\"S5.T22.2.5.9.1\" style=\"font-size:80%;\">76.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T22.2.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T22.2.6.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T22.2.6.1.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T22.2.6.2\"><span class=\"ltx_text\" id=\"S5.T22.2.6.2.1\" style=\"font-size:80%;\">Rule-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.6.3\"><span class=\"ltx_text\" id=\"S5.T22.2.6.3.1\" style=\"font-size:80%;\">01.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.6.4\"><span class=\"ltx_text\" id=\"S5.T22.2.6.4.1\" style=\"font-size:80%;\">02.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.6.5\"><span class=\"ltx_text\" id=\"S5.T22.2.6.5.1\" style=\"font-size:80%;\">10.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T22.2.6.6\"><span class=\"ltx_text\" id=\"S5.T22.2.6.6.1\" style=\"font-size:80%;\">86.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.6.7\"><span class=\"ltx_text\" id=\"S5.T22.2.6.7.1\" style=\"font-size:80%;\">10.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.6.8\"><span class=\"ltx_text\" id=\"S5.T22.2.6.8.1\" style=\"font-size:80%;\">10.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.6.9\"><span class=\"ltx_text\" id=\"S5.T22.2.6.9.1\" style=\"font-size:80%;\">31.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T22.2.6.10\"><span class=\"ltx_text\" id=\"S5.T22.2.6.10.1\" style=\"font-size:80%;\">48.75</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T22.2.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T22.2.7.1\"><span class=\"ltx_text\" id=\"S5.T22.2.7.1.1\" style=\"font-size:80%;\">LLM-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T22.2.7.2\"><span class=\"ltx_text\" id=\"S5.T22.2.7.2.1\" style=\"font-size:80%;\">06.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T22.2.7.3\"><span class=\"ltx_text\" id=\"S5.T22.2.7.3.1\" style=\"font-size:80%;\">20.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T22.2.7.4\"><span class=\"ltx_text\" id=\"S5.T22.2.7.4.1\" style=\"font-size:80%;\">28.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T22.2.7.5\"><span class=\"ltx_text\" id=\"S5.T22.2.7.5.1\" style=\"font-size:80%;\">46.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T22.2.7.6\"><span class=\"ltx_text\" id=\"S5.T22.2.7.6.1\" style=\"font-size:80%;\">12.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T22.2.7.7\"><span class=\"ltx_text\" id=\"S5.T22.2.7.7.1\" style=\"font-size:80%;\">18.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T22.2.7.8\"><span class=\"ltx_text\" id=\"S5.T22.2.7.8.1\" style=\"font-size:80%;\">20.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T22.2.7.9\"><span class=\"ltx_text\" id=\"S5.T22.2.7.9.1\" style=\"font-size:80%;\">50.00</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T22.5.1.1\" style=\"font-size:113%;\">Table 22</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T22.6.2\" style=\"font-size:113%;\">Semantic similarity results of the manual evaluation for annotators A1 and A2.</em></figcaption>\n</figure>",
            "capture": "Table 22: Semantic similarity results of the manual evaluation for annotators A1 and A2."
        },
        "23": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T23\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T23.2\">\n<tr class=\"ltx_tr\" id=\"S5.T23.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T23.2.1.1\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.1.1.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T23.2.1.2\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.1.2.1\" style=\"font-size:80%;\">Perturbation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\" id=\"S5.T23.2.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.1.3.1\" style=\"font-size:80%;\">Grammaticality</span><span class=\"ltx_text\" id=\"S5.T23.2.1.3.2\" style=\"font-size:80%;\"> (%)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T23.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"S5.T23.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.2.1.1\" style=\"font-size:80%;\">A1</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"3\" id=\"S5.T23.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.2.2.1\" style=\"font-size:80%;\">A2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T23.2.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.3.1.1\" style=\"font-size:80%;\">1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.3.2.1\" style=\"font-size:80%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T23.2.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.3.3.1\" style=\"font-size:80%;\">3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.3.4.1\" style=\"font-size:80%;\">1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.3.5.1\" style=\"font-size:80%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T23.2.3.6.1\" style=\"font-size:80%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T23.2.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T23.2.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T23.2.4.1.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T23.2.4.2\"><span class=\"ltx_text\" id=\"S5.T23.2.4.2.1\" style=\"font-size:80%;\">Rule-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.4.3\"><span class=\"ltx_text\" id=\"S5.T23.2.4.3.1\" style=\"font-size:80%;\">10.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.4.4\"><span class=\"ltx_text\" id=\"S5.T23.2.4.4.1\" style=\"font-size:80%;\">31.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T23.2.4.5\"><span class=\"ltx_text\" id=\"S5.T23.2.4.5.1\" style=\"font-size:80%;\">57.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.4.6\"><span class=\"ltx_text\" id=\"S5.T23.2.4.6.1\" style=\"font-size:80%;\">13.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.4.7\"><span class=\"ltx_text\" id=\"S5.T23.2.4.7.1\" style=\"font-size:80%;\">78.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.4.8\"><span class=\"ltx_text\" id=\"S5.T23.2.4.8.1\" style=\"font-size:80%;\">08.18</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T23.2.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T23.2.5.1\"><span class=\"ltx_text\" id=\"S5.T23.2.5.1.1\" style=\"font-size:80%;\">LLM-based</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.5.2\"><span class=\"ltx_text\" id=\"S5.T23.2.5.2.1\" style=\"font-size:80%;\">02.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.5.3\"><span class=\"ltx_text\" id=\"S5.T23.2.5.3.1\" style=\"font-size:80%;\">02.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T23.2.5.4\"><span class=\"ltx_text\" id=\"S5.T23.2.5.4.1\" style=\"font-size:80%;\">96.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.5.5\"><span class=\"ltx_text\" id=\"S5.T23.2.5.5.1\" style=\"font-size:80%;\">00.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.5.6\"><span class=\"ltx_text\" id=\"S5.T23.2.5.6.1\" style=\"font-size:80%;\">02.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T23.2.5.7\"><span class=\"ltx_text\" id=\"S5.T23.2.5.7.1\" style=\"font-size:80%;\">98.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T23.2.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T23.2.6.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T23.2.6.1.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T23.2.6.2\"><span class=\"ltx_text\" id=\"S5.T23.2.6.2.1\" style=\"font-size:80%;\">Rule-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.6.3\"><span class=\"ltx_text\" id=\"S5.T23.2.6.3.1\" style=\"font-size:80%;\">07.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.6.4\"><span class=\"ltx_text\" id=\"S5.T23.2.6.4.1\" style=\"font-size:80%;\">32.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T23.2.6.5\"><span class=\"ltx_text\" id=\"S5.T23.2.6.5.1\" style=\"font-size:80%;\">60.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.6.6\"><span class=\"ltx_text\" id=\"S5.T23.2.6.6.1\" style=\"font-size:80%;\">01.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.6.7\"><span class=\"ltx_text\" id=\"S5.T23.2.6.7.1\" style=\"font-size:80%;\">88.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T23.2.6.8\"><span class=\"ltx_text\" id=\"S5.T23.2.6.8.1\" style=\"font-size:80%;\">10.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T23.2.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T23.2.7.1\"><span class=\"ltx_text\" id=\"S5.T23.2.7.1.1\" style=\"font-size:80%;\">LLM-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T23.2.7.2\"><span class=\"ltx_text\" id=\"S5.T23.2.7.2.1\" style=\"font-size:80%;\">00.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T23.2.7.3\"><span class=\"ltx_text\" id=\"S5.T23.2.7.3.1\" style=\"font-size:80%;\">00.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T23.2.7.4\"><span class=\"ltx_text\" id=\"S5.T23.2.7.4.1\" style=\"font-size:80%;\">100.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T23.2.7.5\"><span class=\"ltx_text\" id=\"S5.T23.2.7.5.1\" style=\"font-size:80%;\">00.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T23.2.7.6\"><span class=\"ltx_text\" id=\"S5.T23.2.7.6.1\" style=\"font-size:80%;\">06.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T23.2.7.7\"><span class=\"ltx_text\" id=\"S5.T23.2.7.7.1\" style=\"font-size:80%;\">94.00</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T23.5.1.1\" style=\"font-size:113%;\">Table 23</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T23.6.2\" style=\"font-size:113%;\">Grammaticality results of the manual evaluation for annotators A1 and A2.</em></figcaption>\n</figure>",
            "capture": "Table 23: Grammaticality results of the manual evaluation for annotators A1 and A2."
        },
        "24": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T24\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T24.2\">\n<tr class=\"ltx_tr\" id=\"S5.T24.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T24.2.1.1\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.1.1.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T24.2.1.2\" rowspan=\"3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.1.2.1\" style=\"font-size:80%;\">Perturbation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\" id=\"S5.T24.2.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.1.3.1\" style=\"font-size:80%;\">Label Consistency</span><span class=\"ltx_text\" id=\"S5.T24.2.1.3.2\" style=\"font-size:80%;\"> (%)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T24.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"S5.T24.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.2.1.1\" style=\"font-size:80%;\">A1</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"3\" id=\"S5.T24.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.2.2.1\" style=\"font-size:80%;\">A2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T24.2.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.3.1.1\" style=\"font-size:80%;\">1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.3.2.1\" style=\"font-size:80%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T24.2.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.3.3.1\" style=\"font-size:80%;\">3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.3.4.1\" style=\"font-size:80%;\">1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.3.5.1\" style=\"font-size:80%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T24.2.3.6.1\" style=\"font-size:80%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T24.2.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T24.2.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T24.2.4.1.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T24.2.4.2\"><span class=\"ltx_text\" id=\"S5.T24.2.4.2.1\" style=\"font-size:80%;\">Rule-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.4.3\"><span class=\"ltx_text\" id=\"S5.T24.2.4.3.1\" style=\"font-size:80%;\">88.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.4.4\"><span class=\"ltx_text\" id=\"S5.T24.2.4.4.1\" style=\"font-size:80%;\">00.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T24.2.4.5\"><span class=\"ltx_text\" id=\"S5.T24.2.4.5.1\" style=\"font-size:80%;\">10.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.4.6\"><span class=\"ltx_text\" id=\"S5.T24.2.4.6.1\" style=\"font-size:80%;\">85.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.4.7\"><span class=\"ltx_text\" id=\"S5.T24.2.4.7.1\" style=\"font-size:80%;\">04.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.4.8\"><span class=\"ltx_text\" id=\"S5.T24.2.4.8.1\" style=\"font-size:80%;\">10.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T24.2.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T24.2.5.1\"><span class=\"ltx_text\" id=\"S5.T24.2.5.1.1\" style=\"font-size:80%;\">LLM-based</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.5.2\"><span class=\"ltx_text\" id=\"S5.T24.2.5.2.1\" style=\"font-size:80%;\">78.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.5.3\"><span class=\"ltx_text\" id=\"S5.T24.2.5.3.1\" style=\"font-size:80%;\">20.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T24.2.5.4\"><span class=\"ltx_text\" id=\"S5.T24.2.5.4.1\" style=\"font-size:80%;\">02.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.5.5\"><span class=\"ltx_text\" id=\"S5.T24.2.5.5.1\" style=\"font-size:80%;\">70.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.5.6\"><span class=\"ltx_text\" id=\"S5.T24.2.5.6.1\" style=\"font-size:80%;\">24.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T24.2.5.7\"><span class=\"ltx_text\" id=\"S5.T24.2.5.7.1\" style=\"font-size:80%;\">06.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T24.2.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T24.2.6.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T24.2.6.1.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T24.2.6.2\"><span class=\"ltx_text\" id=\"S5.T24.2.6.2.1\" style=\"font-size:80%;\">Rule-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.6.3\"><span class=\"ltx_text\" id=\"S5.T24.2.6.3.1\" style=\"font-size:80%;\">90.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.6.4\"><span class=\"ltx_text\" id=\"S5.T24.2.6.4.1\" style=\"font-size:80%;\">00.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T24.2.6.5\"><span class=\"ltx_text\" id=\"S5.T24.2.6.5.1\" style=\"font-size:80%;\">10.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.6.6\"><span class=\"ltx_text\" id=\"S5.T24.2.6.6.1\" style=\"font-size:80%;\">97.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.6.7\"><span class=\"ltx_text\" id=\"S5.T24.2.6.7.1\" style=\"font-size:80%;\">00.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T24.2.6.8\"><span class=\"ltx_text\" id=\"S5.T24.2.6.8.1\" style=\"font-size:80%;\">02.50</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T24.2.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T24.2.7.1\"><span class=\"ltx_text\" id=\"S5.T24.2.7.1.1\" style=\"font-size:80%;\">LLM-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T24.2.7.2\"><span class=\"ltx_text\" id=\"S5.T24.2.7.2.1\" style=\"font-size:80%;\">88.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T24.2.7.3\"><span class=\"ltx_text\" id=\"S5.T24.2.7.3.1\" style=\"font-size:80%;\">04.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T24.2.7.4\"><span class=\"ltx_text\" id=\"S5.T24.2.7.4.1\" style=\"font-size:80%;\">08.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T24.2.7.5\"><span class=\"ltx_text\" id=\"S5.T24.2.7.5.1\" style=\"font-size:80%;\">74.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T24.2.7.6\"><span class=\"ltx_text\" id=\"S5.T24.2.7.6.1\" style=\"font-size:80%;\">00.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T24.2.7.7\"><span class=\"ltx_text\" id=\"S5.T24.2.7.7.1\" style=\"font-size:80%;\">26.00</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T24.5.1.1\" style=\"font-size:113%;\">Table 24</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T24.6.2\" style=\"font-size:113%;\">Label consistency results of the manual evaluation for annotators A1 and A2.</em></figcaption>\n</figure>",
            "capture": "Table 24: Label consistency results of the manual evaluation for annotators A1 and A2."
        },
        "25": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T25\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T25.2\">\n<tr class=\"ltx_tr\" id=\"S5.T25.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T25.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T25.2.1.1.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T25.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T25.2.1.2.1\" style=\"font-size:80%;\">Class</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T25.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T25.2.1.3.1\" style=\"font-size:80%;\">Encoder</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T25.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T25.2.1.4.1\" style=\"font-size:80%;\">Character</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T25.2.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T25.2.1.5.1\" style=\"font-size:80%;\">Vicuna</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T25.2.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T25.2.1.6.1\" style=\"font-size:80%;\">Word</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T25.2.2.1\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S5.T25.2.2.1.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T25.2.2.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T25.2.2.2.1\" style=\"font-size:80%;\">Positive</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T25.2.2.3\"><span class=\"ltx_text\" id=\"S5.T25.2.2.3.1\" style=\"font-size:80%;\">s-bert 22M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T25.2.2.4\"><span class=\"ltx_text\" id=\"S5.T25.2.2.4.1\" style=\"font-size:80%;\">12693/14450 (87.84%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T25.2.2.5\"><span class=\"ltx_text\" id=\"S5.T25.2.2.5.1\" style=\"font-size:80%;\">8190/12223 (67.00%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T25.2.2.6\"><span class=\"ltx_text\" id=\"S5.T25.2.2.6.1\" style=\"font-size:80%;\">17209/17340 (99.24%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.3.1\"><span class=\"ltx_text\" id=\"S5.T25.2.3.1.1\" style=\"font-size:80%;\">s-gpt 1.3B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.3.2\"><span class=\"ltx_text\" id=\"S5.T25.2.3.2.1\" style=\"font-size:80%;\">14170/14450 (98.06%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.3.3\"><span class=\"ltx_text\" id=\"S5.T25.2.3.3.1\" style=\"font-size:80%;\">9677/12223 (79.17%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.3.4\"><span class=\"ltx_text\" id=\"S5.T25.2.3.4.1\" style=\"font-size:80%;\">17123/17340 (98.75%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.4.1\"><span class=\"ltx_text\" id=\"S5.T25.2.4.1.1\" style=\"font-size:80%;\">s-gpt 2.7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.4.2\"><span class=\"ltx_text\" id=\"S5.T25.2.4.2.1\" style=\"font-size:80%;\">14168/14450 (98.05%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.4.3\"><span class=\"ltx_text\" id=\"S5.T25.2.4.3.1\" style=\"font-size:80%;\">10024/12223 (82.01%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.4.4\"><span class=\"ltx_text\" id=\"S5.T25.2.4.4.1\" style=\"font-size:80%;\">17112/17340 (98.69%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.5\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r\" id=\"S5.T25.2.5.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T25.2.5.1.1\" style=\"font-size:80%;\">Negative</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.5.2\"><span class=\"ltx_text\" id=\"S5.T25.2.5.2.1\" style=\"font-size:80%;\">s-bert 22M</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.5.3\"><span class=\"ltx_text\" id=\"S5.T25.2.5.3.1\" style=\"font-size:80%;\">11288/14450 (78.12%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.5.4\"><span class=\"ltx_text\" id=\"S5.T25.2.5.4.1\" style=\"font-size:80%;\">5008/8511 (58.84%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.5.5\"><span class=\"ltx_text\" id=\"S5.T25.2.5.5.1\" style=\"font-size:80%;\">2167/17309 (12.52%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.6.1\"><span class=\"ltx_text\" id=\"S5.T25.2.6.1.1\" style=\"font-size:80%;\">s-gpt 1.3B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.6.2\"><span class=\"ltx_text\" id=\"S5.T25.2.6.2.1\" style=\"font-size:80%;\">13315/14450 (92.15%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.6.3\"><span class=\"ltx_text\" id=\"S5.T25.2.6.3.1\" style=\"font-size:80%;\">5943/8511 (69.83%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.6.4\"><span class=\"ltx_text\" id=\"S5.T25.2.6.4.1\" style=\"font-size:80%;\">2164/17309 (12.50%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.7.1\"><span class=\"ltx_text\" id=\"S5.T25.2.7.1.1\" style=\"font-size:80%;\">s-gpt 2.7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.7.2\"><span class=\"ltx_text\" id=\"S5.T25.2.7.2.1\" style=\"font-size:80%;\">13404/14450 (92.76%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.7.3\"><span class=\"ltx_text\" id=\"S5.T25.2.7.3.1\" style=\"font-size:80%;\">6377/8511 (74.93%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.7.4\"><span class=\"ltx_text\" id=\"S5.T25.2.7.4.1\" style=\"font-size:80%;\">2229/17309 (12.88%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.8\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T25.2.8.1\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S5.T25.2.8.1.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T25.2.8.2\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T25.2.8.2.1\" style=\"font-size:80%;\">Positive</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T25.2.8.3\"><span class=\"ltx_text\" id=\"S5.T25.2.8.3.1\" style=\"font-size:80%;\">s-bert 22M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T25.2.8.4\"><span class=\"ltx_text\" id=\"S5.T25.2.8.4.1\" style=\"font-size:80%;\">4753/4945 (96.12%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T25.2.8.5\"><span class=\"ltx_text\" id=\"S5.T25.2.8.5.1\" style=\"font-size:80%;\">4282/4651 (92.07%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T25.2.8.6\"><span class=\"ltx_text\" id=\"S5.T25.2.8.6.1\" style=\"font-size:80%;\">5908/5934 (99.56%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.9.1\"><span class=\"ltx_text\" id=\"S5.T25.2.9.1.1\" style=\"font-size:80%;\">s-gpt 1.3B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.9.2\"><span class=\"ltx_text\" id=\"S5.T25.2.9.2.1\" style=\"font-size:80%;\">4914/4945 (99.37%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.9.3\"><span class=\"ltx_text\" id=\"S5.T25.2.9.3.1\" style=\"font-size:80%;\">4219/4651 (90.71%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.9.4\"><span class=\"ltx_text\" id=\"S5.T25.2.9.4.1\" style=\"font-size:80%;\">5909/5934 (99.58%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.10.1\"><span class=\"ltx_text\" id=\"S5.T25.2.10.1.1\" style=\"font-size:80%;\">s-gpt 2.7B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.10.2\"><span class=\"ltx_text\" id=\"S5.T25.2.10.2.1\" style=\"font-size:80%;\">4910/4945 (99.29%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.10.3\"><span class=\"ltx_text\" id=\"S5.T25.2.10.3.1\" style=\"font-size:80%;\">4309/4651 (92.65%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.10.4\"><span class=\"ltx_text\" id=\"S5.T25.2.10.4.1\" style=\"font-size:80%;\">5917/5934 (99.71%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.11\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r\" id=\"S5.T25.2.11.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T25.2.11.1.1\" style=\"font-size:80%;\">Negative</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.11.2\"><span class=\"ltx_text\" id=\"S5.T25.2.11.2.1\" style=\"font-size:80%;\">s-bert 22M</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.11.3\"><span class=\"ltx_text\" id=\"S5.T25.2.11.3.1\" style=\"font-size:80%;\">5037/5260 (95.76%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.11.4\"><span class=\"ltx_text\" id=\"S5.T25.2.11.4.1\" style=\"font-size:80%;\">947/1137 (83.29%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.11.5\"><span class=\"ltx_text\" id=\"S5.T25.2.11.5.1\" style=\"font-size:80%;\">6271/6312 (99.35%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T25.2.12.1\"><span class=\"ltx_text\" id=\"S5.T25.2.12.1.1\" style=\"font-size:80%;\">s-gpt 1.3B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.12.2\"><span class=\"ltx_text\" id=\"S5.T25.2.12.2.1\" style=\"font-size:80%;\">5216/5260 (99.16%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.12.3\"><span class=\"ltx_text\" id=\"S5.T25.2.12.3.1\" style=\"font-size:80%;\">983/1137 (86.46%)</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T25.2.12.4\"><span class=\"ltx_text\" id=\"S5.T25.2.12.4.1\" style=\"font-size:80%;\">6258/6312 (99.14%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T25.2.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T25.2.13.1\"><span class=\"ltx_text\" id=\"S5.T25.2.13.1.1\" style=\"font-size:80%;\">s-gpt 2.7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T25.2.13.2\"><span class=\"ltx_text\" id=\"S5.T25.2.13.2.1\" style=\"font-size:80%;\">5220/5260 (99.24%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T25.2.13.3\"><span class=\"ltx_text\" id=\"S5.T25.2.13.3.1\" style=\"font-size:80%;\">1017/1137 (89.45%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T25.2.13.4\"><span class=\"ltx_text\" id=\"S5.T25.2.13.4.1\" style=\"font-size:80%;\">6280/6312 (99.49%)</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T25.5.1.1\" style=\"font-size:113%;\">Table 25</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T25.6.2\" style=\"font-size:113%;\">Number of perturbations kept for each model after filtering with cosine similarity &gt; 0.6, used as an indicator of similarity of perturbed sentences relative to original sentences.\n</em></figcaption>\n</figure>",
            "capture": "Table 25: Number of perturbations kept for each model after filtering with cosine similarity > 0.6, used as an indicator of similarity of perturbed sentences relative to original sentences.\n"
        },
        "26": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T26\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T26.12\">\n<tr class=\"ltx_tr\" id=\"S5.T26.12.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T26.12.13.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.13.1.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T26.12.13.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.13.2.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"S5.T26.12.13.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.13.3.1\" style=\"font-size:80%;\">Test set</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S5.T26.12.13.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.13.4.1\" style=\"font-size:80%;\">Perturbed test set</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.12.14\">\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T26.12.14.1\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T26.12.14.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.12.14.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.14.3.1\" style=\"font-size:80%;\">Precision</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.12.14.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.14.4.1\" style=\"font-size:80%;\">Recall</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.12.14.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.14.5.1\" style=\"font-size:80%;\">F1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.12.14.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.14.6.1\" style=\"font-size:80%;\">Precision</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.12.14.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.14.7.1\" style=\"font-size:80%;\">Recall</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.12.14.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.12.14.8.1\" style=\"font-size:80%;\">F1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T26.1.1.2\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S5.T26.1.1.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T26.1.1.1\">\n<span class=\"ltx_text\" id=\"S5.T26.1.1.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.1.1.3\"><span class=\"ltx_text\" id=\"S5.T26.1.1.3.1\" style=\"font-size:80%;\">95.68%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.1.1.4\"><span class=\"ltx_text\" id=\"S5.T26.1.1.4.1\" style=\"font-size:80%;\">91.29%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T26.1.1.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.1.1.5.1\" style=\"font-size:80%;\">93.44</span><span class=\"ltx_text\" id=\"S5.T26.1.1.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.1.1.6\"><span class=\"ltx_text\" id=\"S5.T26.1.1.6.1\" style=\"font-size:80%;\">94.77%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.1.1.7\"><span class=\"ltx_text\" id=\"S5.T26.1.1.7.1\" style=\"font-size:80%;\">71.86%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.1.1.8\"><span class=\"ltx_text\" id=\"S5.T26.1.1.8.1\" style=\"font-size:80%;\">81.74%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.2.2.1\">\n<span class=\"ltx_text\" id=\"S5.T26.2.2.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.2.2.2\"><span class=\"ltx_text\" id=\"S5.T26.2.2.2.1\" style=\"font-size:80%;\">85.07%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.2.2.3\"><span class=\"ltx_text\" id=\"S5.T26.2.2.3.1\" style=\"font-size:80%;\">98.94%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.2.2.4\"><span class=\"ltx_text\" id=\"S5.T26.2.2.4.1\" style=\"font-size:80%;\">91.48%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.2.2.5\"><span class=\"ltx_text\" id=\"S5.T26.2.2.5.1\" style=\"font-size:80%;\">82.89%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.2.2.6\"><span class=\"ltx_text\" id=\"S5.T26.2.2.6.1\" style=\"font-size:80%;\">94.12%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.2.2.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.2.2.7.1\" style=\"font-size:80%;\">88.15</span><span class=\"ltx_text\" id=\"S5.T26.2.2.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.3.3.1\">\n<span class=\"ltx_text\" id=\"S5.T26.3.3.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.3.3.2\"><span class=\"ltx_text\" id=\"S5.T26.3.3.2.1\" style=\"font-size:80%;\">96.20%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.3.3.3\"><span class=\"ltx_text\" id=\"S5.T26.3.3.3.1\" style=\"font-size:80%;\">87.25%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.3.3.4\"><span class=\"ltx_text\" id=\"S5.T26.3.3.4.1\" style=\"font-size:80%;\">91.51%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.3.3.5\"><span class=\"ltx_text\" id=\"S5.T26.3.3.5.1\" style=\"font-size:80%;\">95.45%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.3.3.6\"><span class=\"ltx_text\" id=\"S5.T26.3.3.6.1\" style=\"font-size:80%;\">67.38%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.3.3.7\"><span class=\"ltx_text\" id=\"S5.T26.3.3.7.1\" style=\"font-size:80%;\">78.98%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.4.4.1\">\n<span class=\"ltx_text\" id=\"S5.T26.4.4.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.4.4.2\"><span class=\"ltx_text\" id=\"S5.T26.4.4.2.1\" style=\"font-size:80%;\">64.93%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.4.4.3\"><span class=\"ltx_text\" id=\"S5.T26.4.4.3.1\" style=\"font-size:80%;\">99.65%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.4.4.4\"><span class=\"ltx_text\" id=\"S5.T26.4.4.4.1\" style=\"font-size:80%;\">78.62%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.4.4.5\"><span class=\"ltx_text\" id=\"S5.T26.4.4.5.1\" style=\"font-size:80%;\">63.56%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.4.4.6\"><span class=\"ltx_text\" id=\"S5.T26.4.4.6.1\" style=\"font-size:80%;\">98.08%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.4.4.7\"><span class=\"ltx_text\" id=\"S5.T26.4.4.7.1\" style=\"font-size:80%;\">77.13%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.5.5.1\">\n<span class=\"ltx_text\" id=\"S5.T26.5.5.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.5.5.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.5.5.2.1\" style=\"font-size:80%;\">96.74</span><span class=\"ltx_text\" id=\"S5.T26.5.5.2.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.5.5.3\"><span class=\"ltx_text\" id=\"S5.T26.5.5.3.1\" style=\"font-size:80%;\">87.29%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.5.5.4\"><span class=\"ltx_text\" id=\"S5.T26.5.5.4.1\" style=\"font-size:80%;\">91.77%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.5.5.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.5.5.5.1\" style=\"font-size:80%;\">95.49</span><span class=\"ltx_text\" id=\"S5.T26.5.5.5.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.5.5.6\"><span class=\"ltx_text\" id=\"S5.T26.5.5.6.1\" style=\"font-size:80%;\">69.82%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.5.5.7\"><span class=\"ltx_text\" id=\"S5.T26.5.5.7.1\" style=\"font-size:80%;\">80.66%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.6.6.1\">\n<span class=\"ltx_text\" id=\"S5.T26.6.6.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.6.6.2\"><span class=\"ltx_text\" id=\"S5.T26.6.6.2.1\" style=\"font-size:80%;\">63.05%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.6.6.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.6.6.3.1\" style=\"font-size:80%;\">99.69</span><span class=\"ltx_text\" id=\"S5.T26.6.6.3.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.6.6.4\"><span class=\"ltx_text\" id=\"S5.T26.6.6.4.1\" style=\"font-size:80%;\">77.24%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.6.6.5\"><span class=\"ltx_text\" id=\"S5.T26.6.6.5.1\" style=\"font-size:80%;\">61.43%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.6.6.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.6.6.6.1\" style=\"font-size:80%;\">98.52</span><span class=\"ltx_text\" id=\"S5.T26.6.6.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.6.6.7\"><span class=\"ltx_text\" id=\"S5.T26.6.6.7.1\" style=\"font-size:80%;\">75.66%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T26.7.7.2\" rowspan=\"6\"><span class=\"ltx_text\" id=\"S5.T26.7.7.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T26.7.7.1\">\n<span class=\"ltx_text\" id=\"S5.T26.7.7.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.7.7.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.7.7.3.1\" style=\"font-size:80%;\">95.23</span><span class=\"ltx_text\" id=\"S5.T26.7.7.3.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.7.7.4\"><span class=\"ltx_text\" id=\"S5.T26.7.7.4.1\" style=\"font-size:80%;\">93.25%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T26.7.7.5\"><span class=\"ltx_text\" id=\"S5.T26.7.7.5.1\" style=\"font-size:80%;\">94.23%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.7.7.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.7.7.6.1\" style=\"font-size:80%;\">95.20</span><span class=\"ltx_text\" id=\"S5.T26.7.7.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.7.7.7\"><span class=\"ltx_text\" id=\"S5.T26.7.7.7.1\" style=\"font-size:80%;\">89.64%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T26.7.7.8\"><span class=\"ltx_text\" id=\"S5.T26.7.7.8.1\" style=\"font-size:80%;\">92.34%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.8.8.1\">\n<span class=\"ltx_text\" id=\"S5.T26.8.8.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.8.8.2\"><span class=\"ltx_text\" id=\"S5.T26.8.8.2.1\" style=\"font-size:80%;\">93.13%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.8.8.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.8.8.3.1\" style=\"font-size:80%;\">97.17</span><span class=\"ltx_text\" id=\"S5.T26.8.8.3.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.8.8.4\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.8.8.4.1\" style=\"font-size:80%;\">95.11</span><span class=\"ltx_text\" id=\"S5.T26.8.8.4.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.8.8.5\"><span class=\"ltx_text\" id=\"S5.T26.8.8.5.1\" style=\"font-size:80%;\">92.51%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.8.8.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.8.8.6.1\" style=\"font-size:80%;\">94.93</span><span class=\"ltx_text\" id=\"S5.T26.8.8.6.2\" style=\"font-size:80%;\">%</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.8.8.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S5.T26.8.8.7.1\" style=\"font-size:80%;\">93.70</span><span class=\"ltx_text\" id=\"S5.T26.8.8.7.2\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.9.9.1\">\n<span class=\"ltx_text\" id=\"S5.T26.9.9.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.9.9.2\"><span class=\"ltx_text\" id=\"S5.T26.9.9.2.1\" style=\"font-size:80%;\">91.93%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.9.9.3\"><span class=\"ltx_text\" id=\"S5.T26.9.9.3.1\" style=\"font-size:80%;\">88.11%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.9.9.4\"><span class=\"ltx_text\" id=\"S5.T26.9.9.4.1\" style=\"font-size:80%;\">89.98%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.9.9.5\"><span class=\"ltx_text\" id=\"S5.T26.9.9.5.1\" style=\"font-size:80%;\">92.17%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.9.9.6\"><span class=\"ltx_text\" id=\"S5.T26.9.9.6.1\" style=\"font-size:80%;\">84.17%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.9.9.7\"><span class=\"ltx_text\" id=\"S5.T26.9.9.7.1\" style=\"font-size:80%;\">87.98%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.10.10.1\">\n<span class=\"ltx_text\" id=\"S5.T26.10.10.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.10.10.2\"><span class=\"ltx_text\" id=\"S5.T26.10.10.2.1\" style=\"font-size:80%;\">84.45%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.10.10.3\"><span class=\"ltx_text\" id=\"S5.T26.10.10.3.1\" style=\"font-size:80%;\">95.71%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.10.10.4\"><span class=\"ltx_text\" id=\"S5.T26.10.10.4.1\" style=\"font-size:80%;\">89.72%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.10.10.5\"><span class=\"ltx_text\" id=\"S5.T26.10.10.5.1\" style=\"font-size:80%;\">84.26%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.10.10.6\"><span class=\"ltx_text\" id=\"S5.T26.10.10.6.1\" style=\"font-size:80%;\">94.24%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.10.10.7\"><span class=\"ltx_text\" id=\"S5.T26.10.10.7.1\" style=\"font-size:80%;\">88.96%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T26.11.11.1\">\n<span class=\"ltx_text\" id=\"S5.T26.11.11.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.11.11.2\"><span class=\"ltx_text\" id=\"S5.T26.11.11.2.1\" style=\"font-size:80%;\">93.25%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.11.11.3\"><span class=\"ltx_text\" id=\"S5.T26.11.11.3.1\" style=\"font-size:80%;\">89.29%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T26.11.11.4\"><span class=\"ltx_text\" id=\"S5.T26.11.11.4.1\" style=\"font-size:80%;\">91.23%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.11.11.5\"><span class=\"ltx_text\" id=\"S5.T26.11.11.5.1\" style=\"font-size:80%;\">92.89%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.11.11.6\"><span class=\"ltx_text\" id=\"S5.T26.11.11.6.1\" style=\"font-size:80%;\">84.79%</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T26.11.11.7\"><span class=\"ltx_text\" id=\"S5.T26.11.11.7.1\" style=\"font-size:80%;\">88.66%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T26.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T26.12.12.1\">\n<span class=\"ltx_text\" id=\"S5.T26.12.12.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T26.12.12.2\"><span class=\"ltx_text\" id=\"S5.T26.12.12.2.1\" style=\"font-size:80%;\">86.82%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T26.12.12.3\"><span class=\"ltx_text\" id=\"S5.T26.12.12.3.1\" style=\"font-size:80%;\">96.56%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T26.12.12.4\"><span class=\"ltx_text\" id=\"S5.T26.12.12.4.1\" style=\"font-size:80%;\">91.43%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T26.12.12.5\"><span class=\"ltx_text\" id=\"S5.T26.12.12.5.1\" style=\"font-size:80%;\">85.60%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T26.12.12.6\"><span class=\"ltx_text\" id=\"S5.T26.12.12.6.1\" style=\"font-size:80%;\">94.74%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T26.12.12.7\"><span class=\"ltx_text\" id=\"S5.T26.12.12.7.1\" style=\"font-size:80%;\">89.93%</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T26.17.2.1\" style=\"font-size:113%;\">Table 26</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T26.14.1\" style=\"font-size:113%;\">Performance of the models on the test/perturbation set, after filtering.\nThe average standard deviation is .</em></figcaption>\n</figure>",
            "capture": "Table 26: Performance of the models on the test/perturbation set, after filtering.\nThe average standard deviation is ."
        },
        "27": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T27\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T27.2\">\n<tr class=\"ltx_tr\" id=\"S5.T27.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T27.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.1.1.1.1\" style=\"width:28.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T27.2.1.1.1.1.1\" style=\"font-size:80%;\">Dataset</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T27.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.1.2.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T27.2.1.2.1.1.1\" style=\"font-size:80%;\">ROUGE-N</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_tt\" colspan=\"4\" id=\"S5.T27.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T27.2.1.3.1\" style=\"font-size:80%;\">Precision</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"4\" id=\"S5.T27.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T27.2.1.4.1\" style=\"font-size:80%;\">Recall</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T27.2.2\">\n<td class=\"ltx_td ltx_align_top ltx_border_r\" id=\"S5.T27.2.2.1\"></td>\n<td class=\"ltx_td ltx_align_top ltx_border_r\" id=\"S5.T27.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.2.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.2.3.1.1.1\" style=\"font-size:80%;\">No </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S5.T27.2.2.3.1.1.2\" style=\"font-size:80%;\">filtering</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\" colspan=\"3\" id=\"S5.T27.2.2.4\"><span class=\"ltx_text\" id=\"S5.T27.2.2.4.1\" style=\"font-size:80%;\">Filtering</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.2.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.2.5.1.1.1\" style=\"font-size:80%;\">No </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S5.T27.2.2.5.1.1.2\" style=\"font-size:80%;\">filtering</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top\" colspan=\"3\" id=\"S5.T27.2.2.6\"><span class=\"ltx_text\" id=\"S5.T27.2.2.6.1\" style=\"font-size:80%;\">Filtering</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T27.2.3\">\n<td class=\"ltx_td ltx_align_top ltx_border_r\" id=\"S5.T27.2.3.1\"></td>\n<td class=\"ltx_td ltx_align_top ltx_border_r\" id=\"S5.T27.2.3.2\"></td>\n<td class=\"ltx_td ltx_align_top\" id=\"S5.T27.2.3.3\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.3.4.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.3.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.3.4.1.1.1\" style=\"font-size:80%;\">s-bert 22M</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.3.5.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.3.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.3.5.1.1.1\" style=\"font-size:80%;\">s-gpt 1.3B</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T27.2.3.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.3.6.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.3.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.3.6.1.1.1\" style=\"font-size:80%;\">s-gpt 2.7B</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top\" id=\"S5.T27.2.3.7\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.3.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.3.8.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.3.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.3.8.1.1.1\" style=\"font-size:80%;\">s-bert 22M</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.3.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.3.9.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.3.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.3.9.1.1.1\" style=\"font-size:80%;\">s-gpt 1.3B</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.3.10\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.3.10.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.3.10.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.3.10.1.1.1\" style=\"font-size:80%;\">s-gpt 2.7B</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T27.2.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T27.2.4.1\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.1.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.1.1.1\" style=\"width:28.2pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.1.1.1.1\" style=\"font-size:80%;\">RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T27.2.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.2.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.2.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.2.1.1.1\" style=\"font-size:80%;\">ROUGE-1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.3.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.3.1.1.1\" style=\"font-size:80%;\">0.500</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.4.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.4.1.1.1\" style=\"font-size:80%;\">0.568</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.5.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.5.1.1.1\" style=\"font-size:80%;\">0.545</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T27.2.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.6.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.6.1.1.1\" style=\"font-size:80%;\">0.537</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.4.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.7.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.7.1.1.1\" style=\"font-size:80%;\">0.281</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.4.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.8.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.8.1.1.1\" style=\"font-size:80%;\">0.635</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.4.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.9.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.9.1.1.1\" style=\"font-size:80%;\">0.612</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.4.10\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.4.10.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.4.10.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.4.10.1.1.1\" style=\"font-size:80%;\">0.604</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T27.2.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T27.2.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.1.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.1.1.1.1\" style=\"font-size:80%;\">ROUGE-2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.2.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.2.1.1.1\" style=\"font-size:80%;\">0.557</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.3.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.3.1.1.1\" style=\"font-size:80%;\">0.342</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.4.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.4.1.1.1\" style=\"font-size:80%;\">0.320</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T27.2.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.5.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.5.1.1.1\" style=\"font-size:80%;\">0.314</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.6.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.6.1.1.1\" style=\"font-size:80%;\">0.312</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.5.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.7.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.7.1.1.1\" style=\"font-size:80%;\">0.382</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.5.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.8.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.8.1.1.1\" style=\"font-size:80%;\">0.358</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.5.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.5.9.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.5.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.5.9.1.1.1\" style=\"font-size:80%;\">0.352</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T27.2.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T27.2.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.1.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.1.1.1.1\" style=\"font-size:80%;\">ROUGE-3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.2.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.2.1.1.1\" style=\"font-size:80%;\">0.511</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.3.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.3.1.1.1\" style=\"font-size:80%;\">0.208</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.4.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.4.1.1.1\" style=\"font-size:80%;\">0.190</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T27.2.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.5.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.5.1.1.1\" style=\"font-size:80%;\">0.185</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.6.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.6.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.6.1.1.1\" style=\"font-size:80%;\">0.285</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.6.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.7.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.7.1.1.1\" style=\"font-size:80%;\">0.230</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.6.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.8.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.8.1.1.1\" style=\"font-size:80%;\">0.210</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.6.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.6.9.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.6.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.6.9.1.1.1\" style=\"font-size:80%;\">0.205</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T27.2.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T27.2.7.1\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.1.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.1.1.1\" style=\"width:28.2pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.1.1.1.1\" style=\"font-size:80%;\">Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T27.2.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.2.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.2.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.2.1.1.1\" style=\"font-size:80%;\">ROUGE-1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.3.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.3.1.1.1\" style=\"font-size:80%;\">0.451</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.4.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.4.1.1.1\" style=\"font-size:80%;\">0.466</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.5.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.5.1.1.1\" style=\"font-size:80%;\">0.469</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T27.2.7.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.6.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.6.1.1.1\" style=\"font-size:80%;\">0.465</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.7.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.7.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.7.1.1.1\" style=\"font-size:80%;\">0.230</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.7.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.8.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.8.1.1.1\" style=\"font-size:80%;\">0.553</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.7.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.9.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.9.1.1.1\" style=\"font-size:80%;\">0.555</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T27.2.7.10\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.7.10.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.7.10.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.7.10.1.1.1\" style=\"font-size:80%;\">0.551</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T27.2.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T27.2.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.1.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.1.1.1.1\" style=\"font-size:80%;\">ROUGE-2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.2.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.2.1.1.1\" style=\"font-size:80%;\">0.529</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.3.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.3.1.1.1\" style=\"font-size:80%;\">0.242</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.8.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.4.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.4.1.1.1\" style=\"font-size:80%;\">0.246</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T27.2.8.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.5.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.5.1.1.1\" style=\"font-size:80%;\">0.243</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.8.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.6.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.6.1.1.1\" style=\"font-size:80%;\">0.268</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.8.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.7.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.7.1.1.1\" style=\"font-size:80%;\">0.285</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.8.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.8.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.8.1.1.1\" style=\"font-size:80%;\">0.288</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T27.2.8.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.8.9.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.8.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.8.9.1.1.1\" style=\"font-size:80%;\">0.285</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T27.2.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S5.T27.2.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.1.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.1.1.1.1\" style=\"font-size:80%;\">ROUGE-3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T27.2.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.2.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.2.1.1.1\" style=\"font-size:80%;\">0.471</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T27.2.9.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.3.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.3.1.1.1\" style=\"font-size:80%;\">0.131</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T27.2.9.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.4.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.4.1.1.1\" style=\"font-size:80%;\">0.135</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S5.T27.2.9.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.5.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.5.1.1.1\" style=\"font-size:80%;\">0.133</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T27.2.9.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.6.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.6.1.1.1\" style=\"font-size:80%;\">0.238</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T27.2.9.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.7.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.7.1.1.1\" style=\"font-size:80%;\">0.156</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T27.2.9.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.8.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.8.1.1.1\" style=\"font-size:80%;\">0.159</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T27.2.9.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T27.2.9.9.1\">\n<span class=\"ltx_p\" id=\"S5.T27.2.9.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T27.2.9.9.1.1.1\" style=\"font-size:80%;\">0.157</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T27.6.1.1\" style=\"font-size:113%;\">Table 27</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T27.7.2\" style=\"font-size:113%;\">ROUGE-N scores comparing the original samples with Vicuna perturbations (of the positive class) for lexical overlap.</em><span class=\"ltx_text\" id=\"S5.T27.8.3\" style=\"font-size:113%;\">\n</span></figcaption>\n</figure>",
            "capture": "Table 27: ROUGE-N scores comparing the original samples with Vicuna perturbations (of the positive class) for lexical overlap.\n"
        },
        "28": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T28\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T28.2\">\n<tr class=\"ltx_tr\" id=\"S5.T28.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T28.2.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.1.1.1.1\" style=\"width:28.2pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T28.2.1.1.1.1.1\" style=\"font-size:80%;\">Dataset</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" id=\"S5.T28.2.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.1.2.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T28.2.1.2.1.1.1\" style=\"font-size:80%;\">ROUGE-N</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_tt\" colspan=\"4\" id=\"S5.T28.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T28.2.1.3.1\" style=\"font-size:80%;\">Precision</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"4\" id=\"S5.T28.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T28.2.1.4.1\" style=\"font-size:80%;\">Recall</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T28.2.2\">\n<td class=\"ltx_td ltx_align_top ltx_border_r\" id=\"S5.T28.2.2.1\"></td>\n<td class=\"ltx_td ltx_align_top ltx_border_r\" id=\"S5.T28.2.2.2\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.2.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.2.3.1.1.1\" style=\"font-size:80%;\">No </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S5.T28.2.2.3.1.1.2\" style=\"font-size:80%;\">filtering</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_r\" colspan=\"3\" id=\"S5.T28.2.2.4\"><span class=\"ltx_text\" id=\"S5.T28.2.2.4.1\" style=\"font-size:80%;\">Filtering</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.2.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.2.5.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.2.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.2.5.1.1.1\" style=\"font-size:80%;\">No </span>\n<br class=\"ltx_break\"/><span class=\"ltx_text\" id=\"S5.T28.2.2.5.1.1.2\" style=\"font-size:80%;\">filtering</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top\" colspan=\"3\" id=\"S5.T28.2.2.6\"><span class=\"ltx_text\" id=\"S5.T28.2.2.6.1\" style=\"font-size:80%;\">Filtering</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T28.2.3\">\n<td class=\"ltx_td ltx_align_top ltx_border_r\" id=\"S5.T28.2.3.1\"></td>\n<td class=\"ltx_td ltx_align_top ltx_border_r\" id=\"S5.T28.2.3.2\"></td>\n<td class=\"ltx_td ltx_align_top\" id=\"S5.T28.2.3.3\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.3.4.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.3.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.3.4.1.1.1\" style=\"font-size:80%;\">s-bert 22M</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.3.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.3.5.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.3.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.3.5.1.1.1\" style=\"font-size:80%;\">s-gpt 1.3B</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T28.2.3.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.3.6.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.3.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.3.6.1.1.1\" style=\"font-size:80%;\">s-gpt 2.7B</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_top\" id=\"S5.T28.2.3.7\"></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.3.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.3.8.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.3.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.3.8.1.1.1\" style=\"font-size:80%;\">s-bert 22M</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.3.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.3.9.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.3.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.3.9.1.1.1\" style=\"font-size:80%;\">s-gpt 1.3B</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.3.10\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.3.10.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.3.10.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.3.10.1.1.1\" style=\"font-size:80%;\">s-gpt 2.7B</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T28.2.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T28.2.4.1\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.1.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.1.1.1\" style=\"width:28.2pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.1.1.1.1\" style=\"font-size:80%;\">RUAR</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T28.2.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.2.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.2.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.2.1.1.1\" style=\"font-size:80%;\">ROUGE-1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.3.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.3.1.1.1\" style=\"font-size:80%;\">0.731</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.4.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.4.1.1.1\" style=\"font-size:80%;\">0.748</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.4.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.5.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.5.1.1.1\" style=\"font-size:80%;\">0.747</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T28.2.4.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.6.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.6.1.1.1\" style=\"font-size:80%;\">0.743</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.4.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.7.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.7.1.1.1\" style=\"font-size:80%;\">0.501</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.4.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.8.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.8.1.1.1\" style=\"font-size:80%;\">0.767</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.4.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.9.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.9.1.1.1\" style=\"font-size:80%;\">0.769</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.4.10\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.4.10.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.4.10.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.4.10.1.1.1\" style=\"font-size:80%;\">0.765</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T28.2.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T28.2.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.1.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.1.1.1.1\" style=\"font-size:80%;\">ROUGE-2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.2.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.2.1.1.1\" style=\"font-size:80%;\">0.738</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.3.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.3.1.1.1\" style=\"font-size:80%;\">0.524</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.4.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.4.1.1.1\" style=\"font-size:80%;\">0.521</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T28.2.5.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.5.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.5.1.1.1\" style=\"font-size:80%;\">0.514</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.5.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.6.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.6.1.1.1\" style=\"font-size:80%;\">0.504</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.5.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.7.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.7.1.1.1\" style=\"font-size:80%;\">0.532</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.5.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.8.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.8.1.1.1\" style=\"font-size:80%;\">0.532</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.5.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.5.9.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.5.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.5.9.1.1.1\" style=\"font-size:80%;\">0.525</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T28.2.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T28.2.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.1.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.1.1.1.1\" style=\"font-size:80%;\">ROUGE-3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.2.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.2.1.1.1\" style=\"font-size:80%;\">0.710</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.3.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.3.1.1.1\" style=\"font-size:80%;\">0.350</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.4.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.4.1.1.1\" style=\"font-size:80%;\">0.347</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T28.2.6.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.5.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.5.1.1.1\" style=\"font-size:80%;\">0.340</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.6.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.6.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.6.1.1.1\" style=\"font-size:80%;\">0.483</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.6.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.7.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.7.1.1.1\" style=\"font-size:80%;\">0.349</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.6.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.8.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.8.1.1.1\" style=\"font-size:80%;\">0.346</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.6.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.6.9.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.6.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.6.9.1.1.1\" style=\"font-size:80%;\">0.339</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T28.2.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T28.2.7.1\" rowspan=\"3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.1.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.1.1.1\" style=\"width:28.2pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.1.1.1.1\" style=\"font-size:80%;\">Medical</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T28.2.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.2.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.2.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.2.1.1.1\" style=\"font-size:80%;\">ROUGE-1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.3.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.3.1.1.1\" style=\"font-size:80%;\">0.670</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.4.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.4.1.1.1\" style=\"font-size:80%;\">0.674</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.7.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.5.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.5.1.1.1\" style=\"font-size:80%;\">0.678</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" id=\"S5.T28.2.7.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.6.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.6.1.1.1\" style=\"font-size:80%;\">0.676</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.7.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.7.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.7.1.1.1\" style=\"font-size:80%;\">0.410</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.7.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.8.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.8.1.1.1\" style=\"font-size:80%;\">0.710</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.7.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.9.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.9.1.1.1\" style=\"font-size:80%;\">0.714</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S5.T28.2.7.10\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.7.10.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.7.10.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.7.10.1.1.1\" style=\"font-size:80%;\">0.712</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T28.2.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T28.2.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.1.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.1.1.1.1\" style=\"font-size:80%;\">ROUGE-2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.2.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.2.1.1.1\" style=\"font-size:80%;\">0.694</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.3.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.3.1.1.1\" style=\"font-size:80%;\">0.415</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.8.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.4.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.4.1.1.1\" style=\"font-size:80%;\">0.422</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" id=\"S5.T28.2.8.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.5.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.5.1.1.1\" style=\"font-size:80%;\">0.419</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.8.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.6.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.6.1.1.1\" style=\"font-size:80%;\">0.422</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.8.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.7.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.7.1.1.1\" style=\"font-size:80%;\">0.434</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.8.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.8.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.8.1.1.1\" style=\"font-size:80%;\">0.441</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S5.T28.2.8.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.8.9.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.8.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.8.9.1.1.1\" style=\"font-size:80%;\">0.438</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T28.2.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S5.T28.2.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.1.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.1.1.1\" style=\"width:45.5pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.1.1.1.1\" style=\"font-size:80%;\">ROUGE-3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T28.2.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.2.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.2.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.2.1.1.1\" style=\"font-size:80%;\">0.657</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T28.2.9.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.3.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.3.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.3.1.1.1\" style=\"font-size:80%;\">0.247</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T28.2.9.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.4.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.4.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.4.1.1.1\" style=\"font-size:80%;\">0.254</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" id=\"S5.T28.2.9.5\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.5.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.5.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.5.1.1.1\" style=\"font-size:80%;\">0.252</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T28.2.9.6\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.6.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.6.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.6.1.1.1\" style=\"font-size:80%;\">0.399</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T28.2.9.7\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.7.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.7.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.7.1.1.1\" style=\"font-size:80%;\">0.258</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T28.2.9.8\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.8.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.8.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.8.1.1.1\" style=\"font-size:80%;\">0.263</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S5.T28.2.9.9\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T28.2.9.9.1\">\n<span class=\"ltx_p\" id=\"S5.T28.2.9.9.1.1\" style=\"width:30.4pt;\"><span class=\"ltx_text\" id=\"S5.T28.2.9.9.1.1.1\" style=\"font-size:80%;\">0.260</span></span>\n</span>\n</td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T28.5.1.1\" style=\"font-size:113%;\">Table 28</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T28.6.2\" style=\"font-size:113%;\">ROUGE-N scores comparing the original samples with Vicuna perturbations (of the positive class) for syntax overlap.</em></figcaption>\n</figure>",
            "capture": "Table 28: ROUGE-N scores comparing the original samples with Vicuna perturbations (of the positive class) for syntax overlap."
        },
        "29": {
            "table_html": "<figure class=\"ltx_table\" id=\"S5.T29\">\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T29.14\">\n<tr class=\"ltx_tr\" id=\"S5.T29.14.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T29.14.15.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.14.15.1.1\" style=\"font-size:80%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T29.14.15.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.14.15.2.1\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S5.T29.14.15.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.14.15.3.1\" style=\"font-size:80%;\">Verifiability</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S5.T29.14.15.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.14.15.4.1\" style=\"font-size:80%;\">Generalisability</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_tt\" colspan=\"2\" id=\"S5.T29.14.15.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.14.15.5.1\" style=\"font-size:80%;\">Falsifiability</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_tt\" colspan=\"2\" id=\"S5.T29.14.15.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.14.15.6.1\" style=\"font-size:80%;\">False Positives</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.14.16\">\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T29.14.16.1\"></td>\n<td class=\"ltx_td ltx_border_r\" id=\"S5.T29.14.16.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.14.16.3\"><span class=\"ltx_text\" id=\"S5.T29.14.16.3.1\" style=\"font-size:80%;\">%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.14.16.4\"><span class=\"ltx_text\" id=\"S5.T29.14.16.4.1\" style=\"font-size:80%;\">#</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.14.16.5\"><span class=\"ltx_text\" id=\"S5.T29.14.16.5.1\" style=\"font-size:80%;\">%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.14.16.6\"><span class=\"ltx_text\" id=\"S5.T29.14.16.6.1\" style=\"font-size:80%;\">#</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.14.16.7\"><span class=\"ltx_text\" id=\"S5.T29.14.16.7.1\" style=\"font-size:80%;\">%</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.14.16.8\"><span class=\"ltx_text\" id=\"S5.T29.14.16.8.1\" style=\"font-size:80%;\">#</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.14.16.9\"><span class=\"ltx_text\" id=\"S5.T29.14.16.9.1\" style=\"font-size:80%;\">%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T29.1.1.2\" rowspan=\"7\"><span class=\"ltx_text\" id=\"S5.T29.1.1.2.1\" style=\"font-size:80%;\">RUAR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T29.1.1.1\">\n<span class=\"ltx_text\" id=\"S5.T29.1.1.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T29.1.1.3\"><span class=\"ltx_text\" id=\"S5.T29.1.1.3.1\" style=\"font-size:80%;\">2.56</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T29.1.1.4\"><span class=\"ltx_text\" id=\"S5.T29.1.1.4.1\" style=\"font-size:80%;\">1256/44013</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S5.T29.1.1.5\"><span class=\"ltx_text\" id=\"S5.T29.1.1.5.1\" style=\"font-size:80%;\">2.85</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T29.1.1.6\"><span class=\"ltx_text\" id=\"S5.T29.1.1.6.1\" style=\"font-size:80%;\">1/3400</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S5.T29.1.1.7\"><span class=\"ltx_text\" id=\"S5.T29.1.1.7.1\" style=\"font-size:80%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T29.1.1.8\"><span class=\"ltx_text\" id=\"S5.T29.1.1.8.1\" style=\"font-size:80%;\">27/40270</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T29.1.1.9\"><span class=\"ltx_text\" id=\"S5.T29.1.1.9.1\" style=\"font-size:80%;\">0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.2.2.1\">\n<span class=\"ltx_text\" id=\"S5.T29.2.2.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.2.2.2\"><span class=\"ltx_text\" id=\"S5.T29.2.2.2.1\" style=\"font-size:80%;\">15.92</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.2.2.3\"><span class=\"ltx_text\" id=\"S5.T29.2.2.3.1\" style=\"font-size:80%;\">8361/44013</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.2.2.4\"><span class=\"ltx_text\" id=\"S5.T29.2.2.4.1\" style=\"font-size:80%;\">19.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.2.2.5\"><span class=\"ltx_text\" id=\"S5.T29.2.2.5.1\" style=\"font-size:80%;\">1/3400</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.2.2.6\"><span class=\"ltx_text\" id=\"S5.T29.2.2.6.1\" style=\"font-size:80%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.2.2.7\"><span class=\"ltx_text\" id=\"S5.T29.2.2.7.1\" style=\"font-size:80%;\">72/40270</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.2.2.8\"><span class=\"ltx_text\" id=\"S5.T29.2.2.8.1\" style=\"font-size:80%;\">0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.3.3.1\">\n<span class=\"ltx_text\" id=\"S5.T29.3.3.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.3.3.2.1\" style=\"font-size:80%;\">21.89</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.3.3.3.1\" style=\"font-size:80%;\">9530/44013</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.3.3.4.1\" style=\"font-size:80%;\">21.65</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.3.3.5\"><span class=\"ltx_text\" id=\"S5.T29.3.3.5.1\" style=\"font-size:80%;\">3/3400</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.3.3.6\"><span class=\"ltx_text\" id=\"S5.T29.3.3.6.1\" style=\"font-size:80%;\">0.09</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.3.3.7\"><span class=\"ltx_text\" id=\"S5.T29.3.3.7.1\" style=\"font-size:80%;\">101/40270</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.3.3.8\"><span class=\"ltx_text\" id=\"S5.T29.3.3.8.1\" style=\"font-size:80%;\">0.25</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.4.4.1\">\n<span class=\"ltx_text\" id=\"S5.T29.4.4.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.4.4.2\"><span class=\"ltx_text\" id=\"S5.T29.4.4.2.1\" style=\"font-size:80%;\">0.34</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.4.4.3\"><span class=\"ltx_text\" id=\"S5.T29.4.4.3.1\" style=\"font-size:80%;\">128/44013</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.4.4.4\"><span class=\"ltx_text\" id=\"S5.T29.4.4.4.1\" style=\"font-size:80%;\">0.29</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.4.4.5\"><span class=\"ltx_text\" id=\"S5.T29.4.4.5.1\" style=\"font-size:80%;\">0/3400</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.4.4.6\"><span class=\"ltx_text\" id=\"S5.T29.4.4.6.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.4.4.7\"><span class=\"ltx_text\" id=\"S5.T29.4.4.7.1\" style=\"font-size:80%;\">0/40270</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.4.4.8\"><span class=\"ltx_text\" id=\"S5.T29.4.4.8.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.5.5.1\">\n<span class=\"ltx_text\" id=\"S5.T29.5.5.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.5.5.2\"><span class=\"ltx_text\" id=\"S5.T29.5.5.2.1\" style=\"font-size:80%;\">11.27</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.5.5.3\"><span class=\"ltx_text\" id=\"S5.T29.5.5.3.1\" style=\"font-size:80%;\">5633/44013</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.5.5.4\"><span class=\"ltx_text\" id=\"S5.T29.5.5.4.1\" style=\"font-size:80%;\">12.80</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.5.5.5\"><span class=\"ltx_text\" id=\"S5.T29.5.5.5.1\" style=\"font-size:80%;\">2/3400</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.5.5.6\"><span class=\"ltx_text\" id=\"S5.T29.5.5.6.1\" style=\"font-size:80%;\">0.06</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.5.5.7\"><span class=\"ltx_text\" id=\"S5.T29.5.5.7.1\" style=\"font-size:80%;\">27/40270</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.5.5.8\"><span class=\"ltx_text\" id=\"S5.T29.5.5.8.1\" style=\"font-size:80%;\">0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.6.6.1\">\n<span class=\"ltx_text\" id=\"S5.T29.6.6.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.6.6.2\"><span class=\"ltx_text\" id=\"S5.T29.6.6.2.1\" style=\"font-size:80%;\">0.35</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.6.6.3\"><span class=\"ltx_text\" id=\"S5.T29.6.6.3.1\" style=\"font-size:80%;\">183/44013</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.6.6.4\"><span class=\"ltx_text\" id=\"S5.T29.6.6.4.1\" style=\"font-size:80%;\">0.42</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.6.6.5\"><span class=\"ltx_text\" id=\"S5.T29.6.6.5.1\" style=\"font-size:80%;\">0/3400</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.6.6.6\"><span class=\"ltx_text\" id=\"S5.T29.6.6.6.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.6.6.7\"><span class=\"ltx_text\" id=\"S5.T29.6.6.7.1\" style=\"font-size:80%;\">0/40270</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.6.6.8\"><span class=\"ltx_text\" id=\"S5.T29.6.6.8.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.7.7.1\">\n<span class=\"ltx_text\" id=\"S5.T29.7.7.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.7.7.2\"><span class=\"ltx_text\" id=\"S5.T29.7.7.2.1\" style=\"font-size:80%;\">11.63</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.7.7.3\"><span class=\"ltx_text\" id=\"S5.T29.7.7.3.1\" style=\"font-size:80%;\">5950/44013</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.7.7.4\"><span class=\"ltx_text\" id=\"S5.T29.7.7.4.1\" style=\"font-size:80%;\">13.52</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.7.7.5\"><span class=\"ltx_text\" id=\"S5.T29.7.7.5.1\" style=\"font-size:80%;\">1/3400</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.7.7.6\"><span class=\"ltx_text\" id=\"S5.T29.7.7.6.1\" style=\"font-size:80%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.7.7.7\"><span class=\"ltx_text\" id=\"S5.T29.7.7.7.1\" style=\"font-size:80%;\">18/40270</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.7.7.8\"><span class=\"ltx_text\" id=\"S5.T29.7.7.8.1\" style=\"font-size:80%;\">0.04</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T29.8.8.2\" rowspan=\"7\"><span class=\"ltx_text\" id=\"S5.T29.8.8.2.1\" style=\"font-size:80%;\">Medical</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T29.8.8.1\">\n<span class=\"ltx_text\" id=\"S5.T29.8.8.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T29.8.8.3\"><span class=\"ltx_text\" id=\"S5.T29.8.8.3.1\" style=\"font-size:80%;\">58.71</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T29.8.8.4\"><span class=\"ltx_text\" id=\"S5.T29.8.8.4.1\" style=\"font-size:80%;\">9135/15530</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S5.T29.8.8.5\"><span class=\"ltx_text\" id=\"S5.T29.8.8.5.1\" style=\"font-size:80%;\">58.82</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T29.8.8.6\"><span class=\"ltx_text\" id=\"S5.T29.8.8.6.1\" style=\"font-size:80%;\">0/989</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S5.T29.8.8.7\"><span class=\"ltx_text\" id=\"S5.T29.8.8.7.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T29.8.8.8\"><span class=\"ltx_text\" id=\"S5.T29.8.8.8.1\" style=\"font-size:80%;\">0/12709</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S5.T29.8.8.9\"><span class=\"ltx_text\" id=\"S5.T29.8.8.9.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.9.9.1\">\n<span class=\"ltx_text\" id=\"S5.T29.9.9.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.9.9.2\"><span class=\"ltx_text\" id=\"S5.T29.9.9.2.1\" style=\"font-size:80%;\">70.61</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.9.9.3\"><span class=\"ltx_text\" id=\"S5.T29.9.9.3.1\" style=\"font-size:80%;\">10879/15530</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.9.9.4\"><span class=\"ltx_text\" id=\"S5.T29.9.9.4.1\" style=\"font-size:80%;\">70.05</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.9.9.5\"><span class=\"ltx_text\" id=\"S5.T29.9.9.5.1\" style=\"font-size:80%;\">0/989</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.9.9.6\"><span class=\"ltx_text\" id=\"S5.T29.9.9.6.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.9.9.7\"><span class=\"ltx_text\" id=\"S5.T29.9.9.7.1\" style=\"font-size:80%;\">0/12709</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.9.9.8\"><span class=\"ltx_text\" id=\"S5.T29.9.9.8.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.10.10.1\">\n<span class=\"ltx_text\" id=\"S5.T29.10.10.1.1\" style=\"font-size:80%;\"> (s-bert 22M)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.10.10.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.10.10.2.1\" style=\"font-size:80%;\">73.47</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.10.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.10.10.3.1\" style=\"font-size:80%;\">10964/15530</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.10.10.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T29.10.10.4.1\" style=\"font-size:80%;\">70.6</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.10.10.5\"><span class=\"ltx_text\" id=\"S5.T29.10.10.5.1\" style=\"font-size:80%;\">0/989</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.10.10.6\"><span class=\"ltx_text\" id=\"S5.T29.10.10.6.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.10.10.7\"><span class=\"ltx_text\" id=\"S5.T29.10.10.7.1\" style=\"font-size:80%;\">0/12709</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.10.10.8\"><span class=\"ltx_text\" id=\"S5.T29.10.10.8.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.11.11.1\">\n<span class=\"ltx_text\" id=\"S5.T29.11.11.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.11.11.2\"><span class=\"ltx_text\" id=\"S5.T29.11.11.2.1\" style=\"font-size:80%;\">11.02</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.11.11.3\"><span class=\"ltx_text\" id=\"S5.T29.11.11.3.1\" style=\"font-size:80%;\">2092/15530</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.11.11.4\"><span class=\"ltx_text\" id=\"S5.T29.11.11.4.1\" style=\"font-size:80%;\">13.47</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.11.11.5\"><span class=\"ltx_text\" id=\"S5.T29.11.11.5.1\" style=\"font-size:80%;\">0/989</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.11.11.6\"><span class=\"ltx_text\" id=\"S5.T29.11.11.6.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.11.11.7\"><span class=\"ltx_text\" id=\"S5.T29.11.11.7.1\" style=\"font-size:80%;\">0/12709</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.11.11.8\"><span class=\"ltx_text\" id=\"S5.T29.11.11.8.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.12.12.1\">\n<span class=\"ltx_text\" id=\"S5.T29.12.12.1.1\" style=\"font-size:80%;\"> (s-gpt 1.3B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.12.12.2\"><span class=\"ltx_text\" id=\"S5.T29.12.12.2.1\" style=\"font-size:80%;\">20.19</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.12.12.3\"><span class=\"ltx_text\" id=\"S5.T29.12.12.3.1\" style=\"font-size:80%;\">3133/15530</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.12.12.4\"><span class=\"ltx_text\" id=\"S5.T29.12.12.4.1\" style=\"font-size:80%;\">20.17</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.12.12.5\"><span class=\"ltx_text\" id=\"S5.T29.12.12.5.1\" style=\"font-size:80%;\">0/989</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.12.12.6\"><span class=\"ltx_text\" id=\"S5.T29.12.12.6.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.12.12.7\"><span class=\"ltx_text\" id=\"S5.T29.12.12.7.1\" style=\"font-size:80%;\">0/12709</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.12.12.8\"><span class=\"ltx_text\" id=\"S5.T29.12.12.8.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.13.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T29.13.13.1\">\n<span class=\"ltx_text\" id=\"S5.T29.13.13.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T29.13.13.2\"><span class=\"ltx_text\" id=\"S5.T29.13.13.2.1\" style=\"font-size:80%;\">13.44</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.13.13.3\"><span class=\"ltx_text\" id=\"S5.T29.13.13.3.1\" style=\"font-size:80%;\">2489/15530</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.13.13.4\"><span class=\"ltx_text\" id=\"S5.T29.13.13.4.1\" style=\"font-size:80%;\">16.03</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.13.13.5\"><span class=\"ltx_text\" id=\"S5.T29.13.13.5.1\" style=\"font-size:80%;\">0/989</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S5.T29.13.13.6\"><span class=\"ltx_text\" id=\"S5.T29.13.13.6.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.13.13.7\"><span class=\"ltx_text\" id=\"S5.T29.13.13.7.1\" style=\"font-size:80%;\">0/12709</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S5.T29.13.13.8\"><span class=\"ltx_text\" id=\"S5.T29.13.13.8.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T29.14.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S5.T29.14.14.1\">\n<span class=\"ltx_text\" id=\"S5.T29.14.14.1.1\" style=\"font-size:80%;\"> (s-gpt 2.7B)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T29.14.14.2\"><span class=\"ltx_text\" id=\"S5.T29.14.14.2.1\" style=\"font-size:80%;\">24.92</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T29.14.14.3\"><span class=\"ltx_text\" id=\"S5.T29.14.14.3.1\" style=\"font-size:80%;\">3957/15530</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S5.T29.14.14.4\"><span class=\"ltx_text\" id=\"S5.T29.14.14.4.1\" style=\"font-size:80%;\">25.48</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T29.14.14.5\"><span class=\"ltx_text\" id=\"S5.T29.14.14.5.1\" style=\"font-size:80%;\">0/989</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S5.T29.14.14.6\"><span class=\"ltx_text\" id=\"S5.T29.14.14.6.1\" style=\"font-size:80%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T29.14.14.7\"><span class=\"ltx_text\" id=\"S5.T29.14.14.7.1\" style=\"font-size:80%;\">0/12709</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S5.T29.14.14.8\"><span class=\"ltx_text\" id=\"S5.T29.14.14.8.1\" style=\"font-size:80%;\">0.00</span></td>\n</tr>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" id=\"S5.T29.27.6.1\" style=\"font-size:113%;\">Table 29</span>: </span><em class=\"ltx_emph ltx_font_italic\" id=\"S5.T29.24.5\" style=\"font-size:113%;\">Verifiability, generalisability and falsifiability of the baseline and the robustly (adversarially) trained DNNs on the RUAR and the Medical datasets, for  ( and ) and  (); for Marabou verifier.\n</em></figcaption>\n</figure>",
            "capture": "Table 29: Verifiability, generalisability and falsifiability of the baseline and the robustly (adversarially) trained DNNs on the RUAR and the Medical datasets, for  ( and ) and  (); for Marabou verifier.\n"
        }
    },
    "image_paths": {
        "1": {
            "figure_path": "2403.10144v2_figure_1.png",
            "caption": "(a)"
        },
        "2": {
            "figure_path": "2403.10144v2_figure_2.png",
            "caption": "(b)"
        },
        "3": {
            "figure_path": "2403.10144v2_figure_3.png",
            "caption": "(c)"
        },
        "4": {
            "figure_path": "2403.10144v2_figure_4.png",
            "caption": "(d)"
        },
        "5": {
            "figure_path": "2403.10144v2_figure_5.png",
            "caption": "Figure 2: Visualisation of the NLP verification pipeline followed in our approach."
        },
        "6": {
            "figure_path": "2403.10144v2_figure_6.png",
            "caption": "(a)"
        },
        "7": {
            "figure_path": "2403.10144v2_figure_7.png",
            "caption": "(b)"
        },
        "8": {
            "figure_path": "2403.10144v2_figure_8.png",
            "caption": "(c)"
        },
        "9": {
            "figure_path": "2403.10144v2_figure_9.png",
            "caption": "Figure 4: Tool ANTONIO that implements a modular approach to the NLP verification pipeline used in this paper."
        },
        "10": {
            "figure_path": "2403.10144v2_figure_10.png",
            "caption": "Figure 5: Zero-shot prompts with 2 basic examples from the R-U-A-Robot dataset. Answers from vicuna-13b are given in italics. A1 and A2 represent different answers to the same prompt, illustrating a lack of consistency in the output."
        },
        "11": {
            "figure_path": "2403.10144v2_figure_11.png",
            "caption": "Figure 6: Analysis of some common issues found in the vicuna-13b generated perturbations."
        },
        "12": {
            "figure_path": "2403.10144v2_figure_12.png",
            "caption": "Figure 7: In this figure, we show how a prepended, semantically informed verified filter added to an NLP system (here, an LLM), can check that safety-critical queries are handled responsibly, e.g. by redirecting the query to a tightly controlled rule-based system instead of a stochastic LLM."
        }
    },
    "references": [
        {
            "1": {
                "title": "Faster r-cnn: Towards real-time object detection with region proposal networks.",
                "author": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.",
                "venue": "IEEE transactions on pattern analysis and machine intelligence, 39(6):1137\u20131149, 2016.",
                "url": null
            }
        },
        {
            "2": {
                "title": "Sequence to sequence learning with neural networks.",
                "author": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le.",
                "venue": "Advances in neural information processing systems, 27, 2014.",
                "url": null
            }
        },
        {
            "3": {
                "title": "Advances in natural language processing.",
                "author": "Julia Hirschberg and Christopher D. Manning.",
                "venue": "Science, 349(6245):261\u2013266, 2015.",
                "url": null
            }
        },
        {
            "4": {
                "title": "On the dangers of stochastic parrots: Can language models be too big?",
                "author": "Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.",
                "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610\u2013623, 2021.",
                "url": null
            }
        },
        {
            "5": {
                "title": "Ethical and social risks of harm from language models, 2021.",
                "author": "Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel.",
                "venue": null,
                "url": null
            }
        },
        {
            "6": {
                "title": "Guiding the release of safer e2e conversational ai through value sensitive design.",
                "author": "A Stevie Bergman, Gavin Abercrombie, Shannon L Spruit, Dirk Hovy, Emily Dinan, Y-Lan Boureau, and Verena Rieser.",
                "venue": "In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 39\u201352, 2022.",
                "url": null
            }
        },
        {
            "7": {
                "title": "SafetyKit: First aid for measuring safety in open-domain conversational systems.",
                "author": "Emily Dinan, Gavin Abercrombie, A. Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4113\u20134133, Dublin, Ireland, May 2022. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "8": {
                "title": "On the opportunities and risks of foundation models, 2021.",
                "author": "Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,\nAvanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u00e9, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram\u00e8r, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.",
                "venue": "URL https://arxiv.org/abs/2108.07258.",
                "url": null
            }
        },
        {
            "9": {
                "title": "Anticipating safety issues in E2E conversational AI: Framework and tooling, 2021.",
                "author": "Emily Dinan, Gavin Abercrombie, A. Stevie Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser.",
                "venue": "URL https://arxiv.org/abs/2107.03451.",
                "url": null
            }
        },
        {
            "10": {
                "title": "Eu artificial intelligence act: The european approach to ai, 2021.",
                "author": "Mauritz Kop.",
                "venue": "URL https://futurium.ec.europa.eu/sites/default/files/2021-10/Kop_EUArtificialIntelligenceAct-TheEuropeanApproachtoAI_21092021_0.pdf.",
                "url": null
            }
        },
        {
            "11": {
                "title": "California senate bill no. 1001.",
                "author": "California State Legislature.",
                "venue": "2018.",
                "url": null
            }
        },
        {
            "12": {
                "title": "Risk-graded safety for handling medical queries in conversational ai.",
                "author": "Gavin Abercrombie and Verena Rieser.",
                "venue": "In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, pages 234\u2013243, 2022.",
                "url": null
            }
        },
        {
            "13": {
                "title": "Patient and consumer safety risks when using conversational assistants for medical information: an observational study of siri, alexa, and google assistant.",
                "author": "Timothy W Bickmore, Ha Trinh, Stefan Olafsson, Teresa K O\u2019Leary, Reza Asadi, Nathaniel M Rickles, and Ricardo Cruz.",
                "venue": "Journal of medical Internet research, 20(9):e11510, 2018.",
                "url": null
            }
        },
        {
            "14": {
                "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.",
                "author": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.",
                "venue": "URL https://lmsys.org/blog/2023-03-30-vicuna/.",
                "url": null
            }
        },
        {
            "15": {
                "title": "Robustness verification for transformers, 2020.",
                "author": "Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh.",
                "venue": null,
                "url": null
            }
        },
        {
            "16": {
                "title": "Antonio: Towards a systematic method of generating nlp benchmarks for verification.",
                "author": "Marco Casadio, Luca Arnaboldi, Matthew Daggitt, Omri Isac, Tanvi Dinkar, Daniel Kienitz, Verena Rieser, and Ekaterina Komendantskaya.",
                "venue": "In Nina Narodytska, Guy Amir, Guy Katz, and Omri Isac, editors, Proceedings of the 6th Workshop on Formal Methods for ML-Enabled Autonomous Systems, volume 16 of Kalpa Publications in Computing, pages 59\u201370. EasyChair, 2023.",
                "url": null
            }
        },
        {
            "17": {
                "title": "Certified robustness to adversarial word substitutions.",
                "author": "Robin Jia, Aditi Raghunathan, Kerem G\u00f6ksel, and Percy Liang.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4129\u20134142, 2019a.",
                "url": null
            }
        },
        {
            "18": {
                "title": "Achieving verified robustness to symbol substitutions via interval bound propagation.",
                "author": "Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krishnamurthy Dvijotham, and Pushmeet Kohli.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4083\u20134093, 2019.",
                "url": null
            }
        },
        {
            "19": {
                "title": "Certified robustness to programmable transformations in lstms.",
                "author": "Yuhao Zhang, Aws Albarghouthi, and Loris D\u2019Antoni.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1068\u20131083, 2021.",
                "url": null
            }
        },
        {
            "20": {
                "title": "Towards deep learning models resistant to adversarial attacks.",
                "author": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.",
                "venue": "In International Conference on Learning Representations, 2018.",
                "url": null
            }
        },
        {
            "21": {
                "title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models.",
                "author": "Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel S Weld.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6707\u20136723, 2021.",
                "url": null
            }
        },
        {
            "22": {
                "title": "Marabou 2.0: A versatile formal analyzer of neural networks, 2024.",
                "author": "Haoze Wu, Omri Isac, Aleksandar Zelji\u0107, Teruhiro Tagomori, Matthew Daggitt, Wen Kokke, Idan Refaeli, Guy Amir, Kyle Julian, Shahaf Bassan, Pei Huang, Ori Lahav, Min Wu, Min Zhang, Ekaterina Komendantskaya, Guy Katz, and Clark Barrett.",
                "venue": null,
                "url": null
            }
        },
        {
            "23": {
                "title": "Automated pipeline design.",
                "author": "Daniel Kroening and Wolfgang Paul.",
                "venue": "In Proc. of 38th ACM/IEEE Design Automation Conference (DAC 2001), pages 810\u2013815. ACM Press, 2001.",
                "url": null
            }
        },
        {
            "24": {
                "title": "Formal verification of an arm processor.",
                "author": "Vishnu A Patankar, Alok Jain, and Randal E Bryant.",
                "venue": "In Proceedings Twelfth International Conference on VLSI Design.(Cat. No. PR00013), pages 282\u2013287. IEEE, 1999.",
                "url": null
            }
        },
        {
            "25": {
                "title": "A formally-verified c static analyzer.",
                "author": "Jacques-Henri Jourdan, Vincent Laporte, Sandrine Blazy, Xavier Leroy, and David Pichardie.",
                "venue": "ACM SIGPLAN Notices, 50(1):247\u2013259, 2015.",
                "url": null
            }
        },
        {
            "26": {
                "title": "Automating cryptographic protocol language generation from structured specifications.",
                "author": "Roberto Metere and Luca Arnaboldi.",
                "venue": "In Proceedings of the IEEE/ACM 10th International Conference on Formal Methods in Software Engineering, pages 91\u2013101, 2022.",
                "url": null
            }
        },
        {
            "27": {
                "title": "Formal methods: Practice and experience.",
                "author": "Jim Woodcock, Peter Gorm Larsen, Juan Bicarregui, and John Fitzgerald.",
                "venue": "ACM computing surveys (CSUR), 41(4):1\u201336, 2009.",
                "url": null
            }
        },
        {
            "28": {
                "title": "Reluplex: An efficient smt solver for verifying deep neural networks.",
                "author": "Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.",
                "venue": "In International conference on computer aided verification, pages 97\u2013117. Springer, 2017.",
                "url": null
            }
        },
        {
            "29": {
                "title": "The second international verification of neural networks competition (vnn-comp 2021): Summary and results.",
                "author": "Stanley Bak, Changliu Liu, and Taylor Johnson.",
                "venue": "arXiv preprint arXiv:2109.00498, 2021.",
                "url": null
            }
        },
        {
            "30": {
                "title": "Scalable quantitative verification for deep neural networks.",
                "author": "Teodora Baluta, Zheng Leong Chua, Kuldeep S Meel, and Prateek Saxena.",
                "venue": "In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 312\u2013323. IEEE, 2021.",
                "url": null
            }
        },
        {
            "31": {
                "title": "Algorithms for verifying deep neural networks.",
                "author": "Changliu Liu, Tomer Arnon, Christopher Lazarus, Christopher Strong, Clark Barrett, Mykel J Kochenderfer, et al.",
                "venue": "Foundations and Trends\u00ae in Optimization, 4(3-4):244\u2013404, 2021.",
                "url": null
            }
        },
        {
            "32": {
                "title": "Beyond the single neuron convex barrier for neural network certification.",
                "author": "Gagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, and Martin Vechev.",
                "venue": "Advances in Neural Information Processing Systems, 32, 2019a.",
                "url": null
            }
        },
        {
            "33": {
                "title": "Challenging smt solvers to verify neural networks.",
                "author": "Luca Pulina and Armando Tacchella.",
                "venue": "Ai Communications, 25(2):117\u2013135, 2012.",
                "url": null
            }
        },
        {
            "34": {
                "title": "Linear programming and extensions.",
                "author": "George Dantzig.",
                "venue": "Princeton university press, 1963.",
                "url": null
            }
        },
        {
            "35": {
                "title": "Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints.",
                "author": "Patrick Cousot and Radhia Cousot.",
                "venue": "In Proceedings of the 4th ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pages 238\u2013252, 1977.",
                "url": null
            }
        },
        {
            "36": {
                "title": "Verification by abstract interpretation.",
                "author": "Patrick Cousot.",
                "venue": "In Verification: Theory and Practice: Essays Dedicated to Zohar Manna on the Occasion of His 64th Birthday, pages 243\u2013268. Springer, 2003.",
                "url": null
            }
        },
        {
            "37": {
                "title": "Abstract interpretation: past, present and future.",
                "author": "Patrick Cousot and Radhia Cousot.",
                "venue": "In Proceedings of the Joint Meeting of the Twenty-Third EACSL Annual Conference on Computer Science Logic (CSL) and the Twenty-Ninth Annual ACM/IEEE Symposium on Logic in Computer Science (LICS), pages 1\u201310, 2014.",
                "url": null
            }
        },
        {
            "38": {
                "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope.",
                "author": "Eric Wong and Zico Kolter.",
                "venue": "In International conference on machine learning, pages 5286\u20135295. PMLR, 2018.",
                "url": null
            }
        },
        {
            "39": {
                "title": "Scalable verified training for provably robust image classification.",
                "author": "Sven Gowal, Krishnamurthy Dj Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli.",
                "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4842\u20134851, 2019.",
                "url": null
            }
        },
        {
            "40": {
                "title": "Fastened crown: Tightened neural network robustness certificates.",
                "author": "Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, and Luca Daniel.",
                "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5037\u20135044, 2020.",
                "url": null
            }
        },
        {
            "41": {
                "title": "Differentiable abstract interpretation for provably robust neural networks.",
                "author": "Matthew Mirman, Timon Gehr, and Martin Vechev.",
                "venue": "In International Conference on Machine Learning, pages 3578\u20133586. PMLR, 2018.",
                "url": null
            }
        },
        {
            "42": {
                "title": "Sok: Certified robustness for deep neural networks.",
                "author": "Linyi Li, Tao Xie, and Bo Li.",
                "venue": "In 2023 IEEE symposium on security and privacy (SP), pages 1289\u20131310. IEEE, 2023.",
                "url": null
            }
        },
        {
            "43": {
                "title": "Efficient neural network robustness certification with general activation functions.",
                "author": "Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.",
                "venue": "Advances in neural information processing systems, 31, 2018.",
                "url": null
            }
        },
        {
            "44": {
                "title": "An abstract domain for certifying neural networks.",
                "author": "Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev.",
                "venue": "Proceedings of the ACM on Programming Languages, 3(POPL):1\u201330, 2019b.",
                "url": null
            }
        },
        {
            "45": {
                "title": "Prima: general and precise neural network certification via scalable convex hull approximations.",
                "author": "Mark Niklas M\u00fcller, Gleb Makarchuk, Gagandeep Singh, Markus P\u00fcschel, and Martin T Vechev.",
                "venue": "Proc. ACM Program. Lang., 6(POPL):1\u201333, 2022.",
                "url": null
            }
        },
        {
            "46": {
                "title": "Fast and effective robustness certification.",
                "author": "Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, and Martin Vechev.",
                "venue": "In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.",
                "url": null
            }
        },
        {
            "47": {
                "title": "Maximum resilience of artificial neural networks.",
                "author": "Chih-Hong Cheng, Georg N\u00fchrenberg, and Harald Ruess.",
                "venue": "In Automated Technology for Verification and Analysis: 15th International Symposium, ATVA 2017, Pune, India, October 3\u20136, 2017, Proceedings 15, pages 251\u2013268. Springer, 2017.",
                "url": null
            }
        },
        {
            "48": {
                "title": "An approach to reachability analysis for feed-forward relu neural networks.",
                "author": "Alessio Lomuscio and Lalit Maganti.",
                "venue": "arXiv preprint arXiv:1706.07351, 2017.",
                "url": null
            }
        },
        {
            "49": {
                "title": "Evaluating robustness of neural networks with mixed integer programming.",
                "author": "Vincent Tjeng, Kai Xiao, and Russ Tedrake.",
                "venue": "n International Conference on Learning Representations,, 2019.",
                "url": null
            }
        },
        {
            "50": {
                "title": "gurobi: Gurobi optimizer 9.1 interface.",
                "author": "LLC Gurobi Optimization.",
                "venue": "R package version, pages 9\u20131, 2020.",
                "url": null
            }
        },
        {
            "51": {
                "title": "Ai2: Safety and robustness certification of neural networks with abstract interpretation.",
                "author": "Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev.",
                "venue": "In 2018 IEEE symposium on security and privacy (SP), pages 3\u201318. IEEE, 2018.",
                "url": null
            }
        },
        {
            "52": {
                "title": "A unified view of piecewise linear neural network verification.",
                "author": "Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda.",
                "venue": "Advances in Neural Information Processing Systems, 31, 2018.",
                "url": null
            }
        },
        {
            "53": {
                "title": "Branch and bound for piecewise linear neural network verification.",
                "author": "Rudy Bunel, P Mudigonda, Ilker Turkaslan, Philip Torr, Jingyue Lu, and Pushmeet Kohli.",
                "venue": "Journal of Machine Learning Research, 21(2020), 2020.",
                "url": null
            }
        },
        {
            "54": {
                "title": "Complete verification via multi-neuron relaxation guided branch-and-bound.",
                "author": "Claudio Ferrari, Mark Niklas Mueller, Nikola Jovanovi\u0107, and Martin Vechev.",
                "venue": "In International Conference on Learning Representations, 2022.",
                "url": null
            }
        },
        {
            "55": {
                "title": "Provable certificates for adversarial examples: Fitting a ball in the union of polytopes.",
                "author": "Matt Jordan, Justin Lewis, and Alexandros G Dimakis.",
                "venue": "Advances in neural information processing systems, 32, 2019.",
                "url": null
            }
        },
        {
            "56": {
                "title": "Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification.",
                "author": "Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.",
                "venue": "Advances in Neural Information Processing Systems, 34:29909\u201329921, 2021a.",
                "url": null
            }
        },
        {
            "57": {
                "title": "General cutting planes for bound-propagation-based neural network verification.",
                "author": "Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.",
                "venue": "Advances in Neural Information Processing Systems, 35:1656\u20131670, 2022.",
                "url": null
            }
        },
        {
            "58": {
                "title": "Prima: Precise and general neural network certification via multi-neuron convex relaxations, 2021.",
                "author": "Mark Niklas M\u00fcller, Gleb Makarchuk, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev.",
                "venue": null,
                "url": null
            }
        },
        {
            "59": {
                "title": "Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers.",
                "author": "Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh.",
                "venue": "In International Conference on Learning Representation (ICLR), 2021.",
                "url": null
            }
        },
        {
            "60": {
                "title": "Certified robustness to adversarial examples with differential privacy.",
                "author": "Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana.",
                "venue": "In 2019 IEEE symposium on security and privacy (SP), pages 656\u2013672. IEEE, 2019.",
                "url": null
            }
        },
        {
            "61": {
                "title": "Certified adversarial robustness with additive noise.",
                "author": "Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin.",
                "venue": "Advances in neural information processing systems, 32, 2019a.",
                "url": null
            }
        },
        {
            "62": {
                "title": "A framework for robustness certification of smoothed classifiers using f-divergences.",
                "author": "Krishnamurthy Dj Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, Andras Gyorgy, Kai Xiao, Sven Gowal, and Pushmeet Kohli.",
                "venue": "2020.",
                "url": null
            }
        },
        {
            "63": {
                "title": "Black-box certification with randomized smoothing: A functional optimization based framework.",
                "author": "Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu.",
                "venue": "Advances in Neural Information Processing Systems, 33:2316\u20132326, 2020a.",
                "url": null
            }
        },
        {
            "64": {
                "title": "Provably robust deep learning via adversarially trained smoothed classifiers.",
                "author": "Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang.",
                "venue": "Advances in Neural Information Processing Systems, 32, 2019.",
                "url": null
            }
        },
        {
            "65": {
                "title": "Higher-order certification for randomized smoothing.",
                "author": "Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel.",
                "venue": "Advances in Neural Information Processing Systems, 33:4501\u20134511, 2020.",
                "url": null
            }
        },
        {
            "66": {
                "title": "Neural network robustness as a verification property: A principled case study.",
                "author": "Marco Casadio, Ekaterina Komendantskaya, Matthew L. Daggitt, Wen Kokke, Guy Katz, Guy Amir, and Idan Refaeli.",
                "venue": "In Computer Aided Verification (CAV 2022), Lecture Notes in Computer Science. Springer, 2022.",
                "url": null
            }
        },
        {
            "67": {
                "title": "Explaining and harnessing adversarial examples, 2015.",
                "author": "Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.",
                "venue": null,
                "url": null
            }
        },
        {
            "68": {
                "title": "Adversarial robustness: Theory and practice.",
                "author": "Zico Kolter and Aleksander Madry.",
                "venue": "Tutorial at NeurIPS, page 3, 2018.",
                "url": null
            }
        },
        {
            "69": {
                "title": "Data augmentation can improve robustness.",
                "author": "Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and Timothy A Mann.",
                "venue": "Advances in Neural Information Processing Systems, 34:29935\u201329948, 2021.",
                "url": null
            }
        },
        {
            "70": {
                "title": "DL2: training and querying neural networks with logic.",
                "author": "Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, and Martin T. Vechev.",
                "venue": "In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1931\u20131941. PMLR, 2019.",
                "url": null
            }
        },
        {
            "71": {
                "title": "Logic of differentiable logics: Towards a uniform semantics of DL.",
                "author": "Natalia Slusarz, Ekaterina Komendantskaya, Matthew L. Daggitt, Robert J. Stewart, and Kathrin Stark.",
                "venue": "In Ruzica Piskac and Andrei Voronkov, editors, LPAR 2023: Proceedings of 24th International Conference on Logic for Programming, Artificial Intelligence and Reasoning, Manizales, Colombia, 4-9th June 2023, volume 94 of EPiC Series in Computing, pages 473\u2013493. EasyChair, 2023.",
                "url": null
            }
        },
        {
            "72": {
                "title": "Towards stable and efficient training of verifiably robust neural networks.",
                "author": "Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning, and Cho Jui Hsieh.",
                "venue": "In 8th International Conference on Learning Representations, ICLR 2020, 2020b.",
                "url": null
            }
        },
        {
            "73": {
                "title": "Certified training: Small boxes are all you need, 2023.",
                "author": "Mark Niklas M\u00fcller, Franziska Eckert, Marc Fischer, and Martin Vechev.",
                "venue": null,
                "url": null
            }
        },
        {
            "74": {
                "title": "Robustness to programmable string transformations via augmented abstract training.",
                "author": "Yuhao Zhang, Aws Albarghouthi, and Loris D\u2019Antoni.",
                "venue": "In Proceedings of the 37th International Conference on Machine Learning, pages 11023\u201311032, 2020c.",
                "url": null
            }
        },
        {
            "75": {
                "title": "Adversarial attacks on deep-learning models in natural language processing: A survey.",
                "author": "Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li.",
                "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 11(3):1\u201341, 2020d.",
                "url": null
            }
        },
        {
            "76": {
                "title": "Towards a robust deep neural network against adversarial texts: A survey.",
                "author": "Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, and Aoshuang Ye.",
                "venue": "IEEE Transactions on Knowledge and Data Engineering, pages 1\u20131, 2021b.",
                "url": null
            }
        },
        {
            "77": {
                "title": "Measure and improve robustness in nlp models: A survey.",
                "author": "Xuezhi Wang, Haohan Wang, and Diyi Yang.",
                "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4569\u20134586, 2022.",
                "url": null
            }
        },
        {
            "78": {
                "title": "Searching for an effective defender: Benchmarking defense against adversarial word substitution.",
                "author": "Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, Kai-Wei Chang, and Cho-Jui Hsieh.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3137\u20133147, 2021.",
                "url": null
            }
        },
        {
            "79": {
                "title": "Defense against synonym substitution-based adversarial attacks via dirichlet neighborhood ensemble.",
                "author": "Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei Chang, and Xuanjing Huan.",
                "venue": "In Association for Computational Linguistics (ACL), 2021.",
                "url": null
            }
        },
        {
            "80": {
                "title": "Freelb: Enhanced adversarial training for natural language understanding.",
                "author": "Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu.",
                "venue": "In International Conference on Learning Representations, 2019.",
                "url": null
            }
        },
        {
            "81": {
                "title": "Towards robustness against natural language word substitutions.",
                "author": "Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu.",
                "venue": "arXiv preprint arXiv:2107.13541, 2021.",
                "url": null
            }
        },
        {
            "82": {
                "title": "A survey of data augmentation approaches for NLP.",
                "author": "Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968\u2013988, Online, 2021. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "83": {
                "title": "Nl-augmenter: A framework for task-sensitive natural language augmentation.",
                "author": "Kaustubh D. Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava, Samson Tan, Tongshuang Wu, Jascha Sohl-Dickstein, Jinho D. Choi, Eduard H. Hovy, Ondrej Dusek, Sebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa Barthe, Hanna Behnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio Sobrevilla Cabezudo, Samuel Cahyawijaya, Emile Chapuis, Wanxiang Che, Mukund Choudhary, Christian Clauss, Pierre Colombo, Filip Cornell, Gautier Dagan, Mayukh Das, Tanay Dixit, Thomas Dopierre, Paul-Alexis Dray, Suchitra Dubey, Tatiana Ekeinhor, Marco Di Giovanni, Rishabh Gupta, Rishabh Gupta, Louanes Hamla, Sang Han, Fabrice Harel-Canada, Antoine Honore, Ishan Jindal, Przemyslaw K. Joniak, Denis Kleyko, Venelin Kovatchev, and et al.",
                "venue": "CoRR, abs/2112.02721, 2021.",
                "url": null
            }
        },
        {
            "84": {
                "title": "Robust neural machine translation with doubly adversarial inputs.",
                "author": "Yong Cheng, Lu Jiang, and Wolfgang Macherey.",
                "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4324\u20134333, 2019.",
                "url": null
            }
        },
        {
            "85": {
                "title": "Adversarial example generation with syntactically controlled paraphrase networks.",
                "author": "Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1875\u20131885, 2018.",
                "url": null
            }
        },
        {
            "86": {
                "title": "Tasa: Deceiving question answering models by twin answer sentences attack.",
                "author": "Yu Cao, Dianqi Li, Meng Fang, Tianyi Zhou, Jun Gao, Yibing Zhan, and Dacheng Tao.",
                "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11975\u201311992, 2022.",
                "url": null
            }
        },
        {
            "87": {
                "title": "Deep text classification can be fooled.",
                "author": "Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi.",
                "venue": "arXiv preprint arXiv:1704.08006, 2017.",
                "url": null
            }
        },
        {
            "88": {
                "title": "Hotflip: White-box adversarial examples for text classification.",
                "author": "Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou.",
                "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31\u201336, 2018.",
                "url": null
            }
        },
        {
            "89": {
                "title": "Phrase-level textual adversarial attack with label preservation.",
                "author": "Yibin Lei, Yu Cao, Dianqi Li, Tianyi Zhou, Meng Fang, and Mykola Pechenizkiy.",
                "venue": "In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1095\u20131112, 2022.",
                "url": null
            }
        },
        {
            "90": {
                "title": "Synthetic and natural noise both break neural machine translation.",
                "author": "Yonatan Belinkov and Yonatan Bisk.",
                "venue": "arXiv preprint arXiv:1711.02173, 2017.",
                "url": null
            }
        },
        {
            "91": {
                "title": "Black-box generation of adversarial text sequences to evade deep learning classifiers.",
                "author": "Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi.",
                "venue": "In 2018 IEEE Security and Privacy Workshops (SPW), pages 50\u201356. IEEE, 2018.",
                "url": null
            }
        },
        {
            "92": {
                "title": "Textbugger: Generating adversarial text against real-world applications.",
                "author": "Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang.",
                "venue": "Proceedings 2019 Network and Distributed System Security Symposium, 2019b.",
                "url": null
            }
        },
        {
            "93": {
                "title": "Towards crafting text adversarial samples, 2017.",
                "author": "Suranjana Samanta and Sameep Mehta.",
                "venue": null,
                "url": null
            }
        },
        {
            "94": {
                "title": "Fooling explanations in text classifiers.",
                "author": "Adam Ivankay, Ivan Girardi, Chiara Marchiori, and Pascal Frossard.",
                "venue": "arXiv preprint arXiv:2206.03178, 2022.",
                "url": null
            }
        },
        {
            "95": {
                "title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment.",
                "author": "Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.",
                "venue": "In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8018\u20138025, 2020.",
                "url": null
            }
        },
        {
            "96": {
                "title": "Generating natural language adversarial examples.",
                "author": "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.",
                "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2890\u20132896, 2018.",
                "url": null
            }
        },
        {
            "97": {
                "title": "Adversarial examples for natural language classification problems.",
                "author": "Volodymyr Kuleshov, Shantanu Thakoor, Tingfung Lau, and Stefano Ermon.",
                "venue": "2018.",
                "url": null
            }
        },
        {
            "98": {
                "title": "Glove: Global vectors for word representation.",
                "author": "Jeffrey Pennington, Richard Socher, and Christopher D Manning.",
                "venue": "In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.",
                "url": null
            }
        },
        {
            "99": {
                "title": "Evaluating the robustness of neural language models to input perturbations.",
                "author": "Milad Moradi and Matthias Samwald.",
                "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1558\u20131570, 2021.",
                "url": null
            }
        },
        {
            "100": {
                "title": "Adversarial examples for evaluating reading comprehension systems.",
                "author": "Robin Jia and Percy Liang.",
                "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021\u20132031, 2017.",
                "url": null
            }
        },
        {
            "101": {
                "title": "Robust machine comprehension models via adversarial training.",
                "author": "Yicheng Wang and Mohit Bansal.",
                "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 575\u2013581, 2018.",
                "url": null
            }
        },
        {
            "102": {
                "title": "Adversarial glue: A multi-task benchmark for robustness evaluation of language models.",
                "author": "Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li.",
                "venue": "arXiv preprint arXiv:2111.02840, 2021c.",
                "url": null
            }
        },
        {
            "103": {
                "title": "Towards verified robustness under text deletion interventions.",
                "author": "Johannes Welbl, Po-Sen Huang, Robert Stanforth, Sven Gowal, Krishnamurthy Dj Dvijotham, Martin Szummer, and Pushmeet Kohli.",
                "venue": "2020.",
                "url": null
            }
        },
        {
            "104": {
                "title": "Robustness-aware word embedding improves certified robustness to adversarial word substitutions.",
                "author": "Yibin Wang, Yichen Yang, Di He, and Kun He.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL 2023, pages 673\u2013687, 2023.",
                "url": null
            }
        },
        {
            "105": {
                "title": "Popqorn: Quantifying robustness of recurrent neural networks.",
                "author": "Ching-Yun Ko, Zhaoyang Lyu, Lily Weng, Luca Daniel, Ngai Wong, and Dahua Lin.",
                "venue": "In International Conference on Machine Learning, pages 3468\u20133477. PMLR, 2019.",
                "url": null
            }
        },
        {
            "106": {
                "title": "Cert-rnn: Towards certifying the robustness of recurrent neural networks.",
                "author": "Tianyu Du, Shouling Ji, Lujia Shen, Yao Zhang, Jinfeng Li, Jie Shi, Chengfang Fang, Jianwei Yin, Raheem Beyah, and Ting Wang.",
                "venue": "CCS, 21(2021):15\u201319, 2021.",
                "url": null
            }
        },
        {
            "107": {
                "title": "Fast and precise certification of transformers.",
                "author": "Gregory Bonaert, Dimitar I Dimitrov, Maximilian Baader, and Martin Vechev.",
                "venue": "In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, pages 466\u2013481, 2021.",
                "url": null
            }
        },
        {
            "108": {
                "title": "Safer: A structure-free approach for certified robustness to adversarial word substitutions.",
                "author": "Mao Ye, Chengyue Gong, and Qiang Liu.",
                "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3465\u20133475, 2020.",
                "url": null
            }
        },
        {
            "109": {
                "title": "Certified robustness to word substitution attack with differential privacy.",
                "author": "Wenjie Wang, Pengfei Tang, Jian Lou, and Li Xiong.",
                "venue": "In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1102\u20131112, Online, June 2021d. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "110": {
                "title": "Certified robustness against natural language attacks by causal intervention.",
                "author": "Haiteng Zhao, Chang Ma, Xinshuai Dong, Anh Tuan Luu, Zhi-Hong Deng, and Hanwang Zhang.",
                "venue": "In International Conference on Machine Learning, pages 26958\u201326970. PMLR, 2022.",
                "url": null
            }
        },
        {
            "111": {
                "title": "Certified robustness to text adversarial attacks by randomized [mask].",
                "author": "Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang.",
                "venue": "Computational Linguistics, 49(2):395\u2013427, 2023.",
                "url": null
            }
        },
        {
            "112": {
                "title": "Unit: A unified look at certified robust training against text adversarial perturbation.",
                "author": "Muchao Ye, Ziyi Yin, Tianrong Zhang, Tianyu Du, Jinghui Chen, Ting Wang, and Fenglong Ma.",
                "venue": "In Thirty-seventh Conference on Neural Information Processing Systems, 2023.",
                "url": null
            }
        },
        {
            "113": {
                "title": "Text-crs: A generalized certified robustness framework against textual adversarial attacks.",
                "author": "Xinyu Zhang, Hanbin Hong, Yuan Hong, Peng Huang, Binghui Wang, Zhongjie Ba, and Kui Ren.",
                "venue": "In 2024 IEEE Symposium on Security and Privacy (SP), pages 53\u201353. IEEE Computer Society, 2023a.",
                "url": null
            }
        },
        {
            "114": {
                "title": "Certified robustness for large language models with self-denoising.",
                "author": "Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, and Shiyu Chang.",
                "venue": "arXiv preprint arXiv:2307.07171, 2023b.",
                "url": null
            }
        },
        {
            "115": {
                "title": "The zonotope abstract domain taylor1+.",
                "author": "Khalil Ghorbal, Eric Goubault, and Sylvie Putot.",
                "venue": "In Computer Aided Verification: 21st International Conference, CAV 2009, Grenoble, France, June 26-July 2, 2009. Proceedings 21, pages 627\u2013633. Springer, 2009.",
                "url": null
            }
        },
        {
            "116": {
                "title": "Certified adversarial robustness via randomized smoothing.",
                "author": "Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.",
                "venue": "In international conference on machine learning, pages 1310\u20131320. PMLR, 2019.",
                "url": null
            }
        },
        {
            "117": {
                "title": "A large annotated corpus for learning natural language inference.",
                "author": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning.",
                "venue": "In Llu\u00eds M\u00e0rquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal, September 2015. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "118": {
                "title": "Toxic comment classification challenge, 2017.",
                "author": "cjadams Jeffrey Sorensen Julia Elliott Lucas Dixon Mark McDonald nithum and Will Cukierski.",
                "venue": "URL https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge.",
                "url": null
            }
        },
        {
            "119": {
                "title": "Learning word vectors for sentiment analysis.",
                "author": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.",
                "venue": "In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "120": {
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank.",
                "author": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.",
                "venue": "In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "121": {
                "title": "Style transfer from non-parallel text by cross-alignment.",
                "author": "Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola.",
                "venue": "Advances in neural information processing systems, 30, 2017.",
                "url": null
            }
        },
        {
            "122": {
                "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.",
                "author": "Bo Pang and Lillian Lee.",
                "venue": "In Kevin Knight, Hwee Tou Ng, and Kemal Oflazer, editors, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 115\u2013124, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "123": {
                "title": "Hidden factors and hidden topics: understanding rating dimensions with review text.",
                "author": "Julian McAuley and Jure Leskovec.",
                "venue": "In Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172, 2013.",
                "url": null
            }
        },
        {
            "124": {
                "title": "A broad-coverage challenge corpus for sentence understanding through inference.",
                "author": "Adina Williams, Nikita Nangia, and Samuel Bowman.",
                "venue": "In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "125": {
                "title": "Character-level convolutional networks for text classification.",
                "author": "Xiang Zhang, Junbo Zhao, and Yann LeCun.",
                "venue": "Advances in neural information processing systems, 28, 2015.",
                "url": null
            }
        },
        {
            "126": {
                "title": "Learning question classifiers.",
                "author": "Xin Li and Dan Roth.",
                "venue": "In COLING 2002: The 19th International Conference on Computational Linguistics, 2002.",
                "url": null
            }
        },
        {
            "127": {
                "title": "Hearing on \u201cOversight of AI: Rules for Artificial Intelligence\u201d.",
                "author": "Christina Montgomery.",
                "venue": "https://www.ibm.com/policy/wp-content/uploads/2023/05/Christina-Montgomery-Senate-Judiciary-Testimony-5-16-23.pdf, 2023.",
                "url": null
            }
        },
        {
            "128": {
                "title": "Chatbots, deepfakes, and voice clones: AI deception for sale.",
                "author": "Michael Atleson.",
                "venue": "https://www.ftc.gov/business-guidance/blog/2023/03/chatbots-deepfakes-voice-clones-ai-deception-sale, 2023.",
                "url": null
            }
        },
        {
            "129": {
                "title": "The rua-robot dataset: Helping avoid chatbot deception by detecting user questions about human or non-human identity.",
                "author": "David Gros, Yu Li, and Zhou Yu.",
                "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6999\u20137013, 2021.",
                "url": null
            }
        },
        {
            "130": {
                "title": "Mirages. on anthropomorphism in dialogue systems.",
                "author": "Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, Verena Rieser, and Zeerak Talat.",
                "venue": "In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4776\u20134790, 2023.",
                "url": null
            }
        },
        {
            "131": {
                "title": "Google duplex: An AI system for accomplishing real world tasks over the phone.",
                "author": "Yaniv Leviathan and Yossi Matias.",
                "venue": "Google AI Blog, 2018.",
                "url": null
            }
        },
        {
            "132": {
                "title": "Google\u2019s creepy AI phone call feature will disclose it\u2019s a robot, after backlash.",
                "author": "Johnny Lieu.",
                "venue": "https://mashable.com/2018/05/11/google-duplex-disclosures-robot, 2018.",
                "url": null
            }
        },
        {
            "133": {
                "title": "Chatbots RESET: A Framework for Governing Responsible Use of Conversational AI in Healthcare.",
                "author": "World Economic Forum.",
                "venue": "https://www.weforum.org/publications/chatbots-reset-a-framework-for-governingresponsible-use-of-conversational-ai-in-healthcare/, 2020.",
                "url": null
            }
        },
        {
            "134": {
                "title": "Llama: Open and efficient foundation language models, 2023.",
                "author": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.",
                "venue": null,
                "url": null
            }
        },
        {
            "135": {
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding, 2018.",
                "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
                "venue": "URL https://arxiv.org/abs/1810.04805.",
                "url": null
            }
        },
        {
            "136": {
                "title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks.",
                "author": "Nils Reimers and Iryna Gurevych.",
                "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China, November 2019. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "137": {
                "title": "Sgpt: Gpt sentence embeddings for semantic search, 2022.",
                "author": "Niklas Muennighoff.",
                "venue": null,
                "url": null
            }
        },
        {
            "138": {
                "title": "Invbert: Reconstructing text from contextualized word embeddings by inverting the bert pipeline.",
                "author": "Kai Kugler, Simon M\u00fcnker, Johannes H\u00f6hmann, and Achim Rettinger.",
                "venue": "arXiv preprint arXiv:2109.10104, 2021.",
                "url": null
            }
        },
        {
            "139": {
                "title": "The quickhull algorithm for convex hulls.",
                "author": "C. Bradford Barber, David P. Dobkin, and Hannu Huhdanpaa.",
                "venue": "ACM TRANSACTIONS ON MATHEMATICAL SOFTWARE, 22(4):469\u2013483, 1996.",
                "url": null
            }
        },
        {
            "140": {
                "title": "Computing the approximate convex hull in high dimensions, 2016.",
                "author": "Hossein Sartipizadeh and Tyrone L. Vincent.",
                "venue": null,
                "url": null
            }
        },
        {
            "141": {
                "title": "On geometric structure of activation spaces in neural networks, 2019b.",
                "author": "Yuting Jia, Haiwen Wang, Shuo Shao, Huan Long, Yunsong Zhou, and Xinbing Wang.",
                "venue": null,
                "url": null
            }
        },
        {
            "142": {
                "title": "The singular value decomposition: Its computation and some applications.",
                "author": "Virginia Klema and Alan Laub.",
                "venue": "IEEE Transactions on automatic control, 25(2):164\u2013176, 1980.",
                "url": null
            }
        },
        {
            "143": {
                "title": "Replication package for the article: An abstract domain for certifying neural networks.",
                "author": "Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev.",
                "venue": null,
                "url": null
            }
        },
        {
            "144": {
                "title": "Open llm leaderboard.",
                "author": "Edward Beeching, Sheon Han, Nathan Lambert ans Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.",
                "venue": "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.",
                "url": null
            }
        },
        {
            "145": {
                "title": "Truthfulqa: Measuring how models mimic human falsehoods.",
                "author": "Stephanie Lin, Jacob Hilton, and Owain Evans.",
                "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, 2022.",
                "url": null
            }
        },
        {
            "146": {
                "title": "Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers.",
                "author": "Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei.",
                "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2140\u20132151, 2021e.",
                "url": null
            }
        },
        {
            "147": {
                "title": "Adversarial robustness of visual dialog, 2022.",
                "author": "Lu Yu and Verena Rieser.",
                "venue": null,
                "url": null
            }
        },
        {
            "148": {
                "title": "Intraclass correlation\u2013a discussion and demonstration of basic features.",
                "author": "David Liljequist, Britt Elfving, and Kirsti Skavberg Roaldsen.",
                "venue": "PloS one, 14(7):e0219854, 2019.",
                "url": null
            }
        },
        {
            "149": {
                "title": "ROUGE: A package for automatic evaluation of summaries.",
                "author": "Chin-Yew Lin.",
                "venue": "In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics.",
                "url": null
            }
        },
        {
            "150": {
                "title": "Natural language processing with Python and spaCy: A practical introduction.",
                "author": "Yuli Vasiliev.",
                "venue": "No Starch Press, 2020.",
                "url": null
            }
        },
        {
            "151": {
                "title": "First three years of the international verification of neural networks competition (vnn-comp).",
                "author": "Christopher Brix, Mark Niklas M\u00fcller, Stanley Bak, Taylor T Johnson, and Changliu Liu.",
                "venue": "International Journal on Software Tools for Technology Transfer, 25(3):329\u2013339, 2023a.",
                "url": null
            }
        },
        {
            "152": {
                "title": "The fourth international verification of neural networks competition (vnn-comp 2023): Summary and results.",
                "author": "Christopher Brix, Stanley Bak, Changliu Liu, and Taylor T Johnson.",
                "venue": "arXiv preprint arXiv:2312.16760, 2023b.",
                "url": null
            }
        }
    ],
    "url": "http://arxiv.org/html/2403.10144v2",
    "segmentation": {
        "research_background_sections": [
            "1",
            "2",
            "2.1",
            "2.2",
            "2.3",
            "2.4",
            "2.5"
        ],
        "methodology_sections": [
            "3",
            "3.1",
            "3.2",
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3",
            "3.3.4",
            "3.3.5",
            "3.4",
            "3.5"
        ],
        "main_experiment_and_results_sections": [
            "4",
            "4.1",
            "4.2",
            "4.3",
            "4.3.1",
            "4.3.2",
            "4.4",
            "4.4.1",
            "4.4.2",
            "4.5",
            "5",
            "5.1",
            "5.2",
            "5.3",
            "5.4",
            "5.5",
            "5.5.1",
            "5.5.2",
            "5.5.x",
            "5.6",
            "5.6.x",
            "6"
        ]
    },
    "ablation_segmentation": {
        "ablation_sections": [
            "1",
            "4",
            "5"
        ]
    },
    "research_context": {
        "paper_id": "2403.10144v2",
        "paper_title": "NLP Verification: Towards a General Methodology for Certifying Robustness",
        "research_background": "**Motivation:**\nThe paper is motivated by the need to ensure safety and security in deploying deep neural network (DNN)-based systems in safety-critical applications. Current Natural Language Processing (NLP) systems lack guarantees for the truthfulness, accuracy, faithfulness, or groundedness of their outputs, potentially leading to various levels of harm. Specifically, there's a pressing need for chatbots to correctly disclose their non-human identity to comply with proposed legislation, as failure to do so can lead to legal implications. More broadly, guarantees are needed for queries related to safety-critical domains, such as medical advice, to prevent real-world harm.\n\n**Research Problem:**\nThe core research problem tackled by the paper is how to ensure that NLP systems can provide formally guaranteed outputs, particularly in scenarios that require maximum control over the output. A specific challenge within this broader problem is the verifiability of DNNs in the context of NLP, which involves guaranteeing that every point in a given region of the embedding space is classified correctly. However, due to the discrete nature of NLP data and the embedding gap issue, existing geometric verification methods from computer vision do not translate well to NLP. The paper seeks to bridge this gap by proposing methods to improve verifiability and generalisability of these semantic subspaces.\n\n**Relevant Prior Work:**\n1. **Application of Formal Verification Techniques**: Prior works have primarily focused on applying formal verification techniques to computer vision tasks, leveraging continuous vector spaces to ensure all points correspond to valid images. Key methods include equational reasoning, abstract interpretation, and bound propagation.\n   \n2. **Semantic Perturbations in NLP**: Some initial attempts at addressing the verification problem in NLP involve constructing subspaces based on semantic perturbations of sentences. Notable works include integrating simple geometric shapes like hyper-cubes and hyper-rectangles to form these subspaces, but issues remain regarding the optimal size and precision of these shapes.\n\n3. **Robust Training Regimes**: Robust training methods have previously been employed to improve the verifiability of subspaces in NLP, but it was unclear whether their success was due to dataset augmentation, adversarial robustness, or the utilization of semantic knowledge. The PGD algorithm and semantic adversarial attacks like polyjuice have been instrumental in this area.\n\nThe paper proposes novel refinements by suggesting a new methodology that includes the hyper-rectangle rotation to enhance shape precision of semantic subspaces. Moreover, it introduces a semantically robust training method using projected gradient descent on semantic subspaces, advancing the existing techniques of adversarial training and dataset augmentation. Through a series of experiments, the paper also seeks to introduce and validate a new generalisability metric, aiming at more principled evaluations of NLP verification methods.",
        "methodology": "**Proposed Method or Model**: The paper proposes a parametric NLP verification pipeline for certifying the robustness of NLP systems and large language models (LLMs). \n\n**Key Components and Innovations**:\n1. **Parametric Design**: Each component within the pipeline operates independently and can be studied or modified without affecting the others. This modular approach allows for greater flexibility and integration of state-of-the-art methods at each stage.\n\n2. **Integration with NLP Systems**: The pipeline can be applied on top of existing NLP systems or LLMs, such as S-BERT and S-GPT. This means it can certify the behavior of these deep neural networks (DNNs) when faced with safety-critical input queries.\n\n3. **Filter Mechanism**: The pipeline functions as a filter to ensure that the intended behaviors of the DNNs are maintained, enhancing the system's robustness and safety.\n\n**Innovative Implications**:\n- **Flexibility**: The parametric nature allows for sophisticated experimentation and customization, making it adaptable to various NLP contexts and advancements.\n- **Robustness Certification**: By serving as a filter, the pipeline ensures that the NLP systems behave as intended under critical conditions, thereby addressing concerns related to safety and reliability in real-world applications.\n\nThe methodology section promises to provide a detailed exposition of the methodological choices made at each step, which would elaborate on how these components and innovations are implemented in practice.",
        "main_experiment_and_results": "### Main Experiment Setup and Results:\n\n**Datasets**\nThe paper does not provide specific names of datasets used but implies the use of datasets appropriate for evaluating the generalisability and verifiability of subspaces in the context of NLP tasks.\n\n**Baselines**\nThe baselines for the main experiments include:\n- Geometric subspaces: This serves as a reference point to which other methods are compared.\n- Non-semantic robust training methods: Includes conventional adversarial training that does not specifically use semantic subspaces.\n\n**Evaluation Metrics**\nThe main evaluation metrics focus on two aspects:\n- **Verifiability**: The extent to which the network's robustness can be formally verified.\n- **Generalisability**: How well the trained network performs on unseen data while retaining robustness to adversarial inputs.\n\n**Main Experimental Results**\n1. **Verifiability vs. Generalisability Trade-off**:\n   - **Problem Introduction**: A detailed analysis of the inherent trade-off between generalisability and verifiability when geometric subspaces are used. It\u2019s shown that geometric subspaces, while being easier to verify, do not generalize as well.\n   \n2. **Semantic Subspaces for Better Balance**:\n   - The application of semantic subspaces finds a better equilibrium between verifiability and generalisability. This means semantic subspaces were capable of being both robustly verified and generalized better to new data compared to geometric subspaces.\n\n3. **Advantage of Adversarial Training on Semantic Subspaces**:\n   - Conducting adversarial training based on semantic subspaces has led to Deep Neural Networks (DNNs) that show superior verifiability and generalisability. This implies that networks trained with this method are both more robustly certifiable and perform better on unseen examples than those trained with traditional robust training techniques."
    },
    "reference_ablation_studies": [
        {
            "research_objective": "Investigate the impact of using semantic subspaces compared to geometric subspaces on the verifiability and generalisability of DNNs in NLP verification.",
            "experiment_process": "The study starts with introducing the metric of generalisability of verified subspaces and setting up baseline experiments. It then introduces the verifiability-generalisability trade-off problem in the context of geometric subspaces. The experiments compare the use of purely geometric subspaces versus semantic subspaces derived from embeddings of sentences and their semantic perturbations. The comparison uses different robust training methodologies, specifically focusing on adversarial training based on semantic subspaces.",
            "result_discussion": "The study shows that using semantic subspaces helps to find a better balance between generalisability and verifiability compared to geometric subspaces. Adversarial training based on semantic subspaces results in DNNs that are both more verifiable and more generalisable than those obtained with other forms of robust training.",
            "ablation_id": "2403.10144v2.No1"
        },
        {
            "research_objective": "Analyze the pitfalls inherent in standard NLP methods for embedding and perturbing sentences and propose metrics to address falsifiability of verified subspaces.",
            "experiment_process": "The section applies the NLP Verification Pipeline using modern NLP tools and different large language models (LLMs). Specifically, it replaces Polyjuice with the LLM Vicuna-13b and tests various components of the NLP pipeline using the ANTONIO tool. It focuses on evaluating the correctness of subspaces dependent on the NLP parts of the pipeline that generate, perturb, and embed sentences. Several assumptions around locality of embedding functions and the sentence perturbation algorithm are tested. The study examines how deviations from these assumptions may lead to constructing falsifiable subspaces.",
            "result_discussion": "The study highlights that the correctness of specifications in NLP verification is highly dependent on the assumptions made about the NLP components. Failures in locality of embedding functions or sentence perturbation algorithms can lead to subspaces that are both formally verified and empirically falsified. A new falsifiability metric is introduced to be used alongside verifiability and generalisability metrics in NLP verification benchmarks to address and report the extent to which semantic subspaces are reliable.",
            "ablation_id": "2403.10144v2.No2"
        }
    ]
}